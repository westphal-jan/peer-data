{"id": "1701.01675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Pareto Efficient Multi Objective Optimization for Local Tuning of Analogy Based Estimation", "abstract": "Analogy Based Effort Estimation (ABE) is one of the prominent methods for software effort estimation. The fundamental concept of ABE is closer to the mentality of expert estimation but with an automated procedure in which the final estimate is generated by reusing similar historical projects. The main key issue when using ABE is how to adapt the effort of the retrieved nearest neighbors. The adaptation process is an essential part of ABE to generate more successful accurate estimation based on tuning the selected raw solutions, using some adaptation strategy. In this study we show that there are three interrelated decision variables that have great impact on the success of adaptation method: (1) number of nearest analogies (k), (2) optimum feature set needed for adaptation, and (3) adaptation weights. To find the right decision regarding these variables, one need to study all possible combinations and evaluate them individually to select the one that can improve all prediction evaluation measures. The existing evaluation measures usually behave differently, presenting sometimes opposite trends in evaluating prediction methods. This means that changing one decision variable could improve one evaluation measure while it is decreasing the others. Therefore, the main theme of this research is how to come up with best decision variables that improve adaptation strategy and thus, the overall evaluation measures without degrading the others. The impact of these decisions together has not been investigated before, therefore we propose to view the building of adaptation procedure as a multi-objective optimization problem. The Particle Swarm Optimization Algorithm (PSO) is utilized to find the optimum solutions for such decision variables based on optimizing multiple evaluation measures", "histories": [["v1", "Tue, 29 Nov 2016 15:57:20 GMT  (2096kb)", "http://arxiv.org/abs/1701.01675v1", null]], "reviews": [], "SUBJECTS": "cs.SE cs.AI", "authors": ["mohammad azzeh", "ali bou nassif", "shadi banitaan", "fadi almasalha"], "accepted": false, "id": "1701.01675"}, "pdf": {"name": "1701.01675.pdf", "metadata": {"source": "CRF", "title": "Pareto Efficient Multi Objective Optimization for Local Tuning of Analogy Based Estimation", "authors": ["Mohammad Azzeh"], "emails": ["m.y.azzeh@asu.edu.jo", "abounassif@ieee.org", "banitash@udmercy.edu", "f_masalha@asu.edu.jo"], "sections": [{"heading": null, "text": "The concept of ABE is closer to the mentality of expert estimation, but with an automated process in which the final estimate is generated by reusing similar historical projects. The most important key question in using ABE is how to adapt the effort of the retrieved closest neighbors. The adaptation process is an essential part of ABE in order to generate a more accurate estimate based on the selected raw solutions, using some adaptation strategies. In this study, we show that there are three interlinked decision variables that have a major impact on the success of the adaptation method: (1) number of closest analogies (k), (2) optimal characteristics needed for adaptation, and (3) adaptation weights. To make the right decision regarding these variables, one needs to study and individually evaluate all possible combinations in order to select the one that can improve all valuation measures."}, {"heading": "1. Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2. Background & Related Work", "text": "In fact, it is so that most of them are able to surpass themselves, and that they are able to surpass themselves. In fact, it is so that they are able to surpass themselves. In fact, it is so that they are able to surpass themselves. In the third world, it is so that they are able to surpass themselves in the third world. In the third world, it is so that they are able to surpass themselves in the third world. In the third world, it is so that they are able to surpass themselves. In the third world, it is so that they are able to surpass themselves in the third world. In the third world, it is so that they are able to surpass themselves in the third world. In the third world, it is so that they are able to surpass themselves in the third world. In the third world, in the third world, in the third world, in the third world, in the third world, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third, in the third."}, {"heading": "3. Problem Representation", "text": "The proposed function of the ABE adaptation is represented in Eqs 3 and 4. The functions consist of three decision variables: 1) Number of closest analogies (k), 2) Distance weights (w) and 3) Characteristic set (v)."}, {"heading": "3 shows how each project is tuned whereas Eq. 4 is used to aggregate the adapted projects\u2019 efforts using", "text": "In the OWM, a method is marked with rank (k-i + 1) with a weight of 122 1ki, as shown in the Eq. 4wo k is the number of the closest analogies and i is the rank of a next project. For example, using 3 projects, one could consider a weight (4 / 7) for the best-placed project (1e), (2 / 7) for the next (2e) and (1 / 7) for the 3rd project (3e), so that the Eq. 4 would appear as follows: 3 7 1 2 2 1 7 4 e, (e), (e), (3), (3), (3), (3), (3), (3), (3), (3), (3), (4), (4), (4), (4), (4), (4), (4, (4), (4), (4, (4), (4), (4, 4, (4), (4), (4), (4, (4), (4), (4, (4), (4), (4, (4), (4), (4, (4), (4), (4), (4, (4), (4), (4, (4), (4), (4), (4, (4), (4), (4, (4), (4), (4), (4, (4), (4), (4), (4), (4, (4), (4), (4), (4), (4), (4), (4), (4), (4, (4), (4), (4), (4), (4), (4), (4), (4), (4), (4)."}, {"heading": "4. Objective functions", "text": "In fact, the fact is that most of them will be able to be in a position to be in a position, and that they will be able to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position."}, {"heading": "5. Multi-Objective Particle-Swarm Optimization algorithm", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "6. Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Using The Solutions Produced By MOPSO-CD in", "text": "The solutions generated by MOPSO-CD are used in ABE's adaptation strategy in two ways: local coordination and global coordination, as discussed in Section 6.2. These solutions are considered to be the most suitable Pareto solutions with the best MIBRE train, the best MBRE train and the best SA train. However, to show how the MOPSO CD algorithm works with ABE, we begin by describing the initialization process as shown in Figure 6. The pseudo-code shows how the particles are initialized at their initial velocity. As we have already seen, each particle represents a potential solution consisting of three variables: k, v and w. Each particle is initialized with random values for each variable. For example, the value of k can take the integer number from 1 to n-1, where n is the number of projects in the dataset."}, {"heading": "6.2 Local Tuning Vs. Global Tuning", "text": "The solutions produced by MOPSO-CD are used for ABE in two ways: local tuning (LT) and global tuning (GT). The main difference between them is that in LT each project is tuned exclusively with its own solution vector, i.e. with its own k-value, feature set and weight values. While in GT all projects in the data set share the same optimal solution, the goal is to increase accuracy and reduce the error rate. One point that needs to be clarified at this stage is how the objective functions are used in both types of tuning. As the rating measures for a project are completely different from the rating of the entire data set, we cannot apply the rating measures to a single project because they require the average of absolute errors for all projects. Therefore, we have a small modification to the type of accuracy that is used in both cases."}, {"heading": "7. Datasets", "text": "This year it is more than ever before."}, {"heading": "8. Experiment Setup", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "9. The Performance of MOPSO in finding Best k values", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "10. Local Tuning vs. Global Tuning", "text": "In fact, most of them are able to abide by the rules that they have imposed on themselves. (...) Most of them are able to abide by the rules. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...) Most of them are not able to achieve their goals. (...)"}, {"heading": "11. The importance of feature optimization", "text": "This year, it has reached the stage where it will be able to achieve the objectives mentioned without being able to achieve its objectives."}, {"heading": "12. The importance of weighting optimization in the adaptation strategy", "text": "This section focuses on answering question RQ4, which states that the use of GT-GT values contributes to improving the predictive accuracy of the adaptation technology (GT values without weight accuracy). It is already known that the use of weight mechanisms shows considerable performance when applied in project calls and some adaptation methods such as AQUA and GA [1]. Normally, software managers tend to use simple weight mechanisms that are feasible and easy to apply such as inversely weighted averages or similarities between projects. Although these mechanisms follow formal procedures, they are considered useful only when the data structure is rather simple and normally distributed. Weight values generated by LT or GT are randomly generated and do not follow a particular algorithm, and are then modified according to the best positions of particles. To better understand the importance of the weight strategy, we are not comparing the form of the adaptation strategy we are trying to use when comparing our weighted use."}, {"heading": "13. Further Analysis", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "14. Conclusions and Future Work", "text": "Finding the appropriate decisions to adapt the next analogies in the ABE method is relatively easy. Previously, we tried to use these decisions or parts of them manually based on expert opinions. This approach has some limitations, as the expert cannot identify all possible decision combinations that relate to different evaluation methods. In our study, we defined four research questions to examine the performance of the proposed adaptation methods and to examine the impact of the decision variables on the optimization process."}, {"heading": "15. Acknowledgements", "text": "The authors thank the Applied Science Private University, Amman, Jordan, for the financial support of this research project."}], "references": [{"title": "Optimal project feature weights in analogy-based cost estimation: Improvement and limitations", "author": ["M. Auer", "A. Trendowicz", "B. Graser", "E. Haunschmid", "S. Biffl"], "venue": "IEEE Transactions on Software Engineering vol. 32, no. 2, pp. 83\u201392", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Model tree Based Adaptation Strategy for software Effort estimation by Analogy", "author": ["M. Azzeh"], "venue": "11th IEEE International Conference on Computer and Information Technology, pp. 328-335", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "A replicated assessment and comparison of adaptation techniques for analogy-based effort estimation", "author": ["M. Azzeh"], "venue": "Journal of Empirical Software Engineering, vol. 17, no.1-2, 90-127", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "The PROMISE Repository of empirical software engineering data http://promisedata.googlecode.com", "author": ["T. Menzies", "B. Caglayan", "E. Kocaguneli", "J. Krall", "F. Peters", "B. Turhan"], "venue": "West Virginia University, Department of Computer Science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "The adapted analogy-based software effort estimation based on similarity distances", "author": ["N.H. Chiu", "S.J. Huang"], "venue": "Journal of Systems and Software, vol. 80, no. 4, pp.628\u2013640", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "A simulation study of the model evaluation criterion MMRE", "author": ["T. Foss", "E. Stensrud", "B. Kitchenham", "I. Myrtveit"], "venue": "IEEE Transactions on Software Engineering vol. 29, no. 11, pp.985\u2013995", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "How to find relevant data for effort estimation? In 5  th International Symposium on Empirical Software Engineering and Measurement (ESEM)", "author": ["E. Kocaguneli", "T. Menzies"], "venue": "pp. 255-264. IEEE, Banff, Canada", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "A review of studies on expert estimation of software development effort", "author": ["M. Jorgensen"], "venue": "Journal of Systems and Software, vol. 70, no. 1, pp. 37\u201360", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Software effort estimation by analogy and regression toward the mean", "author": ["M. Jorgensen", "U. Indahl", "D. Sjoberg"], "venue": "Journal of Systems and Software, vol. 68, no. 3, pp.253\u2013262", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Experiences using case based reasoning to predict software project effort", "author": ["G. Kadoda", "M. Cartwright", "L. Chen", "M. Shepperd"], "venue": "proceedings of EASE, Evaluation and Assessment in Software Engineering Conference, Keele, UK", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "An empirical analysis of linear adaptation techniques for case-based prediction", "author": ["C. Kirsopp", "E. Mendes", "R. Premraj", "M. Shepperd"], "venue": "5  th International Conference on Case Based Reasoning, pp.231\u2013245", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Linear combination of multiple case-based reasoning with optimized weight for software effort estimation", "author": ["D. Wu", "L. Jianping", "L. Yong"], "venue": "The Journal of Supercomputing: Vo. 64, no. 3, pp. 898-918", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining multiple learners induced on multiple datasets for software effort prediction", "author": ["E. Kocaguneli", "Y. Kultur", "A. Bener"], "venue": "20  th International Symposium on Software Reliability Engineering (ISSRE)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Exploiting the Essential Assumptions of Analogy-based Effort Estimation", "author": ["E. Kocaguneli", "T. Menzies", "A. Bener", "J. Keung"], "venue": "IEEE Transactions on Software Engineering, vol.38, no. 2, pp. 425-438", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "A flexible method for software effort estimation by analogy", "author": ["J.Z. Li", "G. Ruhe", "A. Al-Emran", "M. Richter"], "venue": "Journal of Empirical Software Engineering, vol. 12, no. 1, pp. 65\u2013106", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "A study of the non-linear adjustment for analogy based software cost estimation", "author": ["Y.F. Li", "M. Xie", "T.N. Goh"], "venue": "Journal of Empirical Software Engineering, vol. 14, no. 6, pp. 603\u2013643", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Selection of the optimal prototype subset for 1-NN classification", "author": ["U. Lipowezky"], "venue": "Journal of Pattern Recognition Letters, vol. 19, no. 10, pp. 907-918", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "A comparative study of cost estimation models for web hypermedia applications", "author": ["E. Mendes", "I. Watson", "C. Triggs", "N. Mosley", "S. Counsell"], "venue": "Journal of Empirical Software Engineering, vol. 8, no. 2, pp.163\u2013196", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Selecting Best Practices for Effort Estimation", "author": ["T. Menzies", "Z. Chen", "J. Hihn", "K. Lum"], "venue": "IEEE Transactions on Software Engineering, vol. 32, no. 11, pp. 883-895", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Reliability and validity in comparative studies of software prediction models", "author": ["I. Myrtveit", "E. Stensrud", "M. Shepperd"], "venue": "IEEE Transactions on Software Engineering, vol. 31, no. 5, pp. 380\u2013391", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Estimating software project effort using analogies", "author": ["M. Shepperd", "C. Schofield"], "venue": "IEEE Transactions on Software Engineering, vol. 23, no. 11 pp. 736\u2013743", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "A Replication of the Use of Regression towards the Mean (R2M) as an Adjustment to Effort Estimation Models", "author": ["M. Shepperd", "M. Cartwright"], "venue": "11th IEEE International Software Metrics Symposium (METRICS'05), pp. 38", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "An empirical study of analogy-based software effort Estimation", "author": ["F. Walkerden", "D.R. Jeffery"], "venue": "Journal of Empirical Software Engineering, vol. 4, no. 2, pp. 135\u2013158", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "On the Value of Ensemble Effort Estimation", "author": ["E. Kocaguneli", "T. Menzies", "J.W. Keung"], "venue": "IEEE Transactions on Software Engineering, vol.38, no.6, pp.1403-1416", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Data Mining Techniques for Software Effort Estimation: A Comparative Study", "author": ["K.K. Dejaeger", "D. Martens W. Verbeke", "B. Baesens"], "venue": "IEEE Transactions on Software Engineering,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Ranking and Clustering Software Cost Estimation Models through a Multiple Comparisons Algorithm", "author": ["N. Mittas", "L. Angelis"], "venue": "IEEE Transactions on Software Engineering, vol. 39, no. 4, pp. 537-551", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Software quality analysis by combining multiple projects and learners", "author": ["M. Khoshgoftaar", "P. Rebours", "N. Seliya"], "venue": "Journal Software Quality Control, vol. 17, no. 1, pp. 25\u201349", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning Best K analogies from Data Distribution for Case-Based Software Effort Estimation", "author": ["M. Azzeh", "Y. Elsheikh"], "venue": "The Seventh International Conference on Software Engineering Advances, pp. 341-347", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluating prediction systems in software project estimation", "author": ["M. Shepperd", "S. MacDonell"], "venue": "Journal of Information and Software Technology, vol. 54, no. 8, pp. 820-827", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Ensembles and locality: Insight on improving software effort estimation", "author": ["M. Leandro", "X. Yao"], "venue": "Journal of Information and Software Technology, vol. 55, no. 8, pp. 1512-1528", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Software effort estimation as a multiobjective learning problem, ACM Transactions on Software Engineering and Methodology", "author": ["Leandro", "X. Yao"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "The impact of parameter tuning on software effort estimation using learning machines", "author": ["L. Song", "M. Leandro", "Y. Xin"], "venue": "the 9th International Conference on Predictive Models in Software Engineering. ACM", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Analogy-X: Providing Statistical Inference to Analogy-Based Software Cost Estimation", "author": ["J. Keung", "B. Kitchenham", "D.R. Jeffery"], "venue": "IEEE Transactions on Software Engineering, Vol. 34, no. 4, pp. 471-484", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Stable rankings for different effort models", "author": ["T. Menzies", "O. Jalali", "J. Hihn", "D. Baker", "K. Lum"], "venue": "Journal of Automated Software Engineering, vol. 17, no. 4, pp. 409-437", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Handling multiple objectives with particle swarm optimization", "author": ["C.A.C Coello", "G.T. Pulido", "T. Pulido", "M.S. Lechuga"], "venue": "IEEE Transactions on Evolutionary Computation, 8.3, pp. 256-279", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "Particle swarm optimization", "author": ["K. James"], "venue": "Encyclopaedia of Machine Learning, Springer US, pp. 760-766", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Boundary handling approaches in particle swarm optimization", "author": ["N. Padhye", "K. Deb", "P. Mittal"], "venue": "the 7  th International Conference on Bio-Inspired Computing: Theories and Applications ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Particle Swarm Optimization", "author": ["J. Kennedy", "R.C. Eberhart"], "venue": "the 4  th IEEE International Conference on Neural Networks, pp. 1942\u2013 1948", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1995}, {"title": "Micro-MOPSO: a multi-objective particle swarm optimizer that uses a very small population size", "author": ["J.C.F Cabrera", "C.A.C Coello"], "venue": "Multi-Objective Swarm Intelligent Systems, Springer Berlin Heidelberg, 83-104", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Using crowding distance to improve multi-objective PSO with local search", "author": ["C.S. Tsou", "S.C. Chang", "P.W. Lai"], "venue": "Swarm Intelligence: Focus on Ant and Particle Swarm Optimization pp. 77-86", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2007}, {"title": "F.Thabta, L.McCluskey, Predicting phishing websites based on self-structuring neural network", "author": ["R. Mohammad"], "venue": "Journal of Neural Computing and Applications,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Artificial neural network for estimation of harbor oscillation in a cargo harbour basin", "author": ["M. Kankal", "O. Yuksek"], "venue": "Journal of Neural Computing and Applications, vol 25, no. 1, pp. 95-103", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "A Better Case Adaptation Method for Case-Based Effort Estimation Using Multi-Objective Optimization", "author": ["M. Azzeh", "A.B. Nassif", "S. Banitaan"], "venue": "The 13th International Conference on Machine Learning and Applications (ICMLA'14), Detroit, MI USA", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Software effort models should be assessed via leave-one-out validation", "author": ["E. Kocaguneli", "T. Menzies"], "venue": "Journal of Systems and Software,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}, {"title": "Analogy-based software effort estimation using Fuzzy numbers", "author": ["M. Azzeh", "D. Neagu", "P.I. Cowling"], "venue": "Journal of Systems and Software vol. 84, issue 2, pp. 270-284", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "An Empirical Analysis of Data Pre-processing for Machine Learning-based Software Cost Estimation", "author": ["J. Huang", "Y-F Li", "M. Xie"], "venue": "Information and Software Technology, Elsevier, In Press, doi:10.1016/j.infsof.2015.07.004", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Introduction One of the key challenges in software industry is how to obtain the accurate estimation of the development effort, which is particularly important for risk evaluation, resource scheduling as well as progress monitoring [3][15][28].", "startOffset": 232, "endOffset": 235}, {"referenceID": 13, "context": "Introduction One of the key challenges in software industry is how to obtain the accurate estimation of the development effort, which is particularly important for risk evaluation, resource scheduling as well as progress monitoring [3][15][28].", "startOffset": 235, "endOffset": 239}, {"referenceID": 26, "context": "Introduction One of the key challenges in software industry is how to obtain the accurate estimation of the development effort, which is particularly important for risk evaluation, resource scheduling as well as progress monitoring [3][15][28].", "startOffset": 239, "endOffset": 243}, {"referenceID": 45, "context": "This importance is clearly portrayed through proposing a vast variety of estimation models in the past years [47].", "startOffset": 109, "endOffset": 113}, {"referenceID": 29, "context": "In one hand the underestimation results in approval of projects that will exceed their planned budgets, while on the other hand the overestimation causes waste of resources and misses opportunities to offer funds for other projects in future [31].", "startOffset": 242, "endOffset": 246}, {"referenceID": 13, "context": "Software effort estimation has been extensively studied in literature since 70\u2019s but they have suffered from common problems such as very large performance deviations as well as being highly dataset dependent [15].", "startOffset": 209, "endOffset": 213}, {"referenceID": 1, "context": "The latter has two distinct advantages over the former such that they have capability to model complex set of relationships between dependent variable and the independent variables, and they are capable to learn from historical project data [2][27].", "startOffset": 241, "endOffset": 244}, {"referenceID": 25, "context": "The latter has two distinct advantages over the former such that they have capability to model complex set of relationships between dependent variable and the independent variables, and they are capable to learn from historical project data [2][27].", "startOffset": 244, "endOffset": 248}, {"referenceID": 12, "context": "In the recent years, a significant research effort was put into utilizing various machine learning (ML) algorithms as a complementary or as a replacement to previous methods [14][17][25][31].", "startOffset": 174, "endOffset": 178}, {"referenceID": 15, "context": "In the recent years, a significant research effort was put into utilizing various machine learning (ML) algorithms as a complementary or as a replacement to previous methods [14][17][25][31].", "startOffset": 178, "endOffset": 182}, {"referenceID": 23, "context": "In the recent years, a significant research effort was put into utilizing various machine learning (ML) algorithms as a complementary or as a replacement to previous methods [14][17][25][31].", "startOffset": 182, "endOffset": 186}, {"referenceID": 29, "context": "In the recent years, a significant research effort was put into utilizing various machine learning (ML) algorithms as a complementary or as a replacement to previous methods [14][17][25][31].", "startOffset": 186, "endOffset": 190}, {"referenceID": 13, "context": "they need to be tuned to local data for high accuracy values[15][33].", "startOffset": 60, "endOffset": 64}, {"referenceID": 31, "context": "they need to be tuned to local data for high accuracy values[15][33].", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "ML methods have an extremely large space of configuration possibilities [15][42][43].", "startOffset": 72, "endOffset": 76}, {"referenceID": 40, "context": "ML methods have an extremely large space of configuration possibilities [15][42][43].", "startOffset": 76, "endOffset": 80}, {"referenceID": 41, "context": "ML methods have an extremely large space of configuration possibilities [15][42][43].", "startOffset": 80, "endOffset": 84}, {"referenceID": 5, "context": "When we consider configuration possibilities of ML methods induced on different datasets, each method has its own characteristics, so it is not a surprise to see contradictory results [6][9][30][35].", "startOffset": 184, "endOffset": 187}, {"referenceID": 7, "context": "When we consider configuration possibilities of ML methods induced on different datasets, each method has its own characteristics, so it is not a surprise to see contradictory results [6][9][30][35].", "startOffset": 187, "endOffset": 190}, {"referenceID": 28, "context": "When we consider configuration possibilities of ML methods induced on different datasets, each method has its own characteristics, so it is not a surprise to see contradictory results [6][9][30][35].", "startOffset": 190, "endOffset": 194}, {"referenceID": 33, "context": "When we consider configuration possibilities of ML methods induced on different datasets, each method has its own characteristics, so it is not a surprise to see contradictory results [6][9][30][35].", "startOffset": 194, "endOffset": 198}, {"referenceID": 23, "context": "Finding the best estimation model was under a thorough investigation of many comparative studies that attempted to rank and categorize those models based on the quality of estimates they produce [25][27].", "startOffset": 195, "endOffset": 199}, {"referenceID": 25, "context": "Finding the best estimation model was under a thorough investigation of many comparative studies that attempted to rank and categorize those models based on the quality of estimates they produce [25][27].", "startOffset": 199, "endOffset": 203}, {"referenceID": 23, "context": "The most factors that contradict their findings seem to be the error measures [25], datasets preprocessing and their inherent characteristics [3], and finally, the experimental methodology [15].", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "The most factors that contradict their findings seem to be the error measures [25], datasets preprocessing and their inherent characteristics [3], and finally, the experimental methodology [15].", "startOffset": 142, "endOffset": 145}, {"referenceID": 13, "context": "The most factors that contradict their findings seem to be the error measures [25], datasets preprocessing and their inherent characteristics [3], and finally, the experimental methodology [15].", "startOffset": 189, "endOffset": 193}, {"referenceID": 20, "context": "Since the utility of a project cannot be evaluated directly, similarity between project descriptions is used as a heuristic approach to retrieve the projects\u2019 effort [22].", "startOffset": 166, "endOffset": 170}, {"referenceID": 32, "context": "We study ABE for several reasons: a) it reflects human reasoning, b) it works with spare data and complex domains, and c) it provides reasoning in a domain with a small body of knowledge [34].", "startOffset": 187, "endOffset": 191}, {"referenceID": 2, "context": "Previous research has reported that ABE is able to produce more successful results in comparison to traditional regression based methods [3][22].", "startOffset": 137, "endOffset": 140}, {"referenceID": 20, "context": "Previous research has reported that ABE is able to produce more successful results in comparison to traditional regression based methods [3][22].", "startOffset": 140, "endOffset": 144}, {"referenceID": 9, "context": "ABE has been favored over other methods when the dataset contains discontinuities [11].", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "Such decisions include selection of features and/or instances, deciding on the number of analogies to be used and the adaptation strategy [15][19].", "startOffset": 138, "endOffset": 142}, {"referenceID": 17, "context": "Such decisions include selection of features and/or instances, deciding on the number of analogies to be used and the adaptation strategy [15][19].", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "[15] stated that using different solutions for each parameter produce different ABE configuration, hence, different ABE models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Indeed, there is a direct evidence that the choice of right adaptation method has a big influence on the accuracy of ABE as confirmed in [3][13].", "startOffset": 137, "endOffset": 140}, {"referenceID": 11, "context": "Indeed, there is a direct evidence that the choice of right adaptation method has a big influence on the accuracy of ABE as confirmed in [3][13].", "startOffset": 140, "endOffset": 144}, {"referenceID": 20, "context": "The original ABE method [22], that is denoted as ABE0, does not use any kind of adaptation strategy, but it uses the mean of k nearest neighbors\u2019 efforts.", "startOffset": 24, "endOffset": 28}, {"referenceID": 30, "context": "Previous studies showed that applying different evaluation measures tend to behave differently in identifying best model [32], therefore finding these decisions should be based on improving all evaluation measures simultaneously.", "startOffset": 121, "endOffset": 125}, {"referenceID": 8, "context": "Moreover, the improved ABE models that use adaptation strategy such as regression towards the mean [10], genetics algorithm [5] and neural networks [17] still fail in specifying the appropriate number of the nearest analogies and do not take other decisions in their adaption process.", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "Moreover, the improved ABE models that use adaptation strategy such as regression towards the mean [10], genetics algorithm [5] and neural networks [17] still fail in specifying the appropriate number of the nearest analogies and do not take other decisions in their adaption process.", "startOffset": 124, "endOffset": 127}, {"referenceID": 15, "context": "Moreover, the improved ABE models that use adaptation strategy such as regression towards the mean [10], genetics algorithm [5] and neural networks [17] still fail in specifying the appropriate number of the nearest analogies and do not take other decisions in their adaption process.", "startOffset": 148, "endOffset": 152}, {"referenceID": 37, "context": "It has been proposed by Kennedy and Eberhart [39] to perform combination of random and neighborhood search.", "startOffset": 45, "endOffset": 49}, {"referenceID": 35, "context": "However, the conventional PSO can deal with problems that have only one objective function, but when the problem has many conflicting objectives as in our study we should use the extended version of PSO that can support multi-objective functions which is called multi-objective Particle Swarm Optimization (MOPSO) [37][40].", "startOffset": 314, "endOffset": 318}, {"referenceID": 38, "context": "However, the conventional PSO can deal with problems that have only one objective function, but when the problem has many conflicting objectives as in our study we should use the extended version of PSO that can support multi-objective functions which is called multi-objective Particle Swarm Optimization (MOPSO) [37][40].", "startOffset": 318, "endOffset": 322}, {"referenceID": 30, "context": "Since these measures tend to behave differently [32], the final outcome of MOPSO is not a single solution but a set of solutions that make a good trade-off between these objective functions.", "startOffset": 48, "endOffset": 52}, {"referenceID": 42, "context": "This paper is an extension to our previous works on using optimization for ABE adaptation [44].", "startOffset": 90, "endOffset": 94}, {"referenceID": 27, "context": "Previous studies suggest that adapting each project individually with its own decisions tend to be more accurate than adapting whole dataset with the same decision vector [29].", "startOffset": 171, "endOffset": 175}, {"referenceID": 20, "context": "Background & Related Work ABE generates a new estimation based on assumption that similar projects with respect to features description have similar efforts [22].", "startOffset": 157, "endOffset": 161}, {"referenceID": 22, "context": "In late 1990s, Walkerden and Jeffery [24] introduced the first adjustment method called Linear Size Extrapolation (LSE).", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "[12] extended Walkerden and Jeffery method to include all size related features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] based on a dataset collected from web projects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Unfortunately, the replication study on adjustment methods [3] revealed that MLFE is still less useful than LSE under certain experimental conditions.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "[10] proposed a different method called Regression Towards the Mean (RTM) to adjust and calibrate nearest projects based on the notion of project productivity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] remarked that the productivity distribution of estimated projects are narrower than that of actual projects which proves that the estimated efforts regress towards the mean effort in a particular dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Shepperd and Cartwright [23] replicated the work of Jorgensen et al.", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "[10] and advised that the dataset should be partitioned into groups of homogeneous projects so that the adjustment moves to a local productivity mean.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] demonstrate that the similarity degree between projects can play important role in adjusting selected projects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Previous analysis studies report that software datasets are characteristically noisy with complex structure [29].", "startOffset": 108, "endOffset": 112}, {"referenceID": 4, "context": "A typical example on this approach is the work of Chiu and Heung [5] who used GA to calibrate selected projects based on learning distances between them and reflect that difference on the predicted effort.", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "Likewise, Azzeh [2] used Model Tree to adjust and tune selected projects.", "startOffset": 16, "endOffset": 19}, {"referenceID": 15, "context": "[17] raised an important concern regarding structure of datasets as they claim that most software cost estimation datasets do not follow uniform distribution as presumed in linear methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The findings from the Li et al study are promising, but the replication study conducted by Azzeh [3] reported discouraging results where some linear adjustment methods produced more accurate results than neural networks.", "startOffset": 97, "endOffset": 100}, {"referenceID": 14, "context": "In recent years, various approaches have been proposed to specify this number such as k nearest neighbor algorithms and similarity cut off point [16][22].", "startOffset": 145, "endOffset": 149}, {"referenceID": 20, "context": "In recent years, various approaches have been proposed to specify this number such as k nearest neighbor algorithms and similarity cut off point [16][22].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "n-1 [1][2][11][15][19][24].", "startOffset": 4, "endOffset": 7}, {"referenceID": 1, "context": "n-1 [1][2][11][15][19][24].", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "n-1 [1][2][11][15][19][24].", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "n-1 [1][2][11][15][19][24].", "startOffset": 14, "endOffset": 18}, {"referenceID": 17, "context": "n-1 [1][2][11][15][19][24].", "startOffset": 18, "endOffset": 22}, {"referenceID": 22, "context": "n-1 [1][2][11][15][19][24].", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "[18] and Walkerden and Jeffery [24] who found k=1 was the most optimum number.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[18] and Walkerden and Jeffery [24] who found k=1 was the most optimum number.", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "[19] used k = 1, 2, 3 as optimum numbers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] proposes a policy that looks for only one prototype, which can be regarded as extreme when dealing with datasets as small as those in software effort estimation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "On the other hand, Azzeh [3] conducted an extensive replication study on various linear and non-linear adaptation strategies using many public datasets, and found that that k=1 was the most prominent number across all experimentations.", "startOffset": 25, "endOffset": 28}, {"referenceID": 10, "context": "[12] on the other hand proposes making predictions from the 2 nearest cases as it was found as the optimum value for the datasets of their study.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] have increased their accuracy values with case and feature subset selection strategies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Besides this approach, other researchers attempted to dynamically find the optimum number of nearest analogies such as [16] and [29].", "startOffset": 119, "endOffset": 123}, {"referenceID": 27, "context": "Besides this approach, other researchers attempted to dynamically find the optimum number of nearest analogies such as [16] and [29].", "startOffset": 128, "endOffset": 132}, {"referenceID": 14, "context": "[16] proposed a method to learn the k number based by optimizing similarity threshold.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Azzeh and Elsheikh [29] utilized bisecting k-medoid clustering to understand the structure of certain dataset and come up with best number of analogies for each test project individually.", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "Recent studies arise important concerns about using MRE because it is unbalanced and yields asymmetry distribution [6][21][30].", "startOffset": 115, "endOffset": 118}, {"referenceID": 19, "context": "Recent studies arise important concerns about using MRE because it is unbalanced and yields asymmetry distribution [6][21][30].", "startOffset": 118, "endOffset": 122}, {"referenceID": 28, "context": "Recent studies arise important concerns about using MRE because it is unbalanced and yields asymmetry distribution [6][21][30].", "startOffset": 122, "endOffset": 126}, {"referenceID": 28, "context": "In this paper we used the Standardized Accuracy (SA) measure that has been proposed by Shepperd and MacDonell [30] as shown in Eq.", "startOffset": 110, "endOffset": 114}, {"referenceID": 29, "context": "In addition to the above mentioned evaluation measures, we used other three reliable evaluation measures mentioned in literature that are considerably less vulnerable to bias or asymmetry distribution as in case of MMRE [31][32].", "startOffset": 220, "endOffset": 224}, {"referenceID": 30, "context": "In addition to the above mentioned evaluation measures, we used other three reliable evaluation measures mentioned in literature that are considerably less vulnerable to bias or asymmetry distribution as in case of MMRE [31][32].", "startOffset": 224, "endOffset": 228}, {"referenceID": 30, "context": "These measures were chosen because, even though all of them were initially designed to represent how well a model performs, they can behave very differently from each other as reported in [32].", "startOffset": 188, "endOffset": 192}, {"referenceID": 4, "context": "1 Basic Concepts Optimization algorithm is a typical solution for the sophisticated problems that have many interrelated design options as encountered in software engineering tasks [5][32].", "startOffset": 181, "endOffset": 184}, {"referenceID": 30, "context": "1 Basic Concepts Optimization algorithm is a typical solution for the sophisticated problems that have many interrelated design options as encountered in software engineering tasks [5][32].", "startOffset": 184, "endOffset": 188}, {"referenceID": 34, "context": "As there are many optimization algorithms in literature, we chose Particle Swarm Optimization (PSO) for two reasons: (1) The algorithm is simple and its implementation is straightforward, (2) it showed good performance against some well-known evolutionary algorithms such as Genetics algorithm and Simulated Annealing [36][37].", "startOffset": 318, "endOffset": 322}, {"referenceID": 35, "context": "As there are many optimization algorithms in literature, we chose Particle Swarm Optimization (PSO) for two reasons: (1) The algorithm is simple and its implementation is straightforward, (2) it showed good performance against some well-known evolutionary algorithms such as Genetics algorithm and Simulated Annealing [36][37].", "startOffset": 322, "endOffset": 326}, {"referenceID": 37, "context": "It was first developed in 1995 by Kennedy and Eberhart [39].", "startOffset": 55, "endOffset": 59}, {"referenceID": 35, "context": "The popularity of PSO stems from its simplicity in performing search and especially global search since it does not need many operators for creating new solution as in evolutionary algorithms, so its implementation is straightforward [37].", "startOffset": 234, "endOffset": 238}, {"referenceID": 38, "context": "But on the other hand, this algorithm suffers from two main problems: 1) slow convergence in refined search stage, and 2) Weak local search ability [40].", "startOffset": 148, "endOffset": 152}, {"referenceID": 37, "context": "Particles then fly through the problem space by following the current optimum Particles [39].", "startOffset": 88, "endOffset": 92}, {"referenceID": 36, "context": "are kept as members of the population through the course of the run [38].", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "Where 1 r and 2 r are random values [0, 1], j represents the index of the decision variable in ) (t xi \uf072 .", "startOffset": 36, "endOffset": 42}, {"referenceID": 37, "context": "It is interesting to note that a large inertia weight (W) facilitates a global search while a small inertia weight facilitates a local search [39].", "startOffset": 142, "endOffset": 146}, {"referenceID": 38, "context": "However, the classical PSO can deal efficiently when the problem has only one objective function, but when the problem has many conflicting objectives, we should use the extended version of PSO that can support multi-objective functions which is called multi-objective Particle Swarm Optimization (MOPSO) [40] as explained in the next section.", "startOffset": 305, "endOffset": 309}, {"referenceID": 38, "context": "3 Multi-Objective Particle Swarm Optimization (MOPSO) The MOSPO [40] is concerned with the problems that consists of one or more decisions and have many objectives to be optimized simultaneously.", "startOffset": 64, "endOffset": 68}, {"referenceID": 38, "context": "In this research, we used the more efficient MOPSO algorithm based on Crowding Distance (MOPSO-CD) [40].", "startOffset": 99, "endOffset": 103}, {"referenceID": 38, "context": "So instead of randomly selecting gbest from the whole solutions in the repository, it is randomly selected from the top 10% less crowded area of the repository for each Particle that is dominated by any solution located in this area [40] [41].", "startOffset": 233, "endOffset": 237}, {"referenceID": 39, "context": "So instead of randomly selecting gbest from the whole solutions in the repository, it is randomly selected from the top 10% less crowded area of the repository for each Particle that is dominated by any solution located in this area [40] [41].", "startOffset": 238, "endOffset": 242}, {"referenceID": 39, "context": "The non-dominated Particles are then selected and stored in a special repository (A) [41].", "startOffset": 85, "endOffset": 89}, {"referenceID": 39, "context": "The CD is calculated for non-dominated solution by first sorting the solutions in ascending order according to each objective function [41].", "startOffset": 135, "endOffset": 139}, {"referenceID": 39, "context": "The CD factor of first and last solution is usually equal to the maximum distance [41].", "startOffset": 82, "endOffset": 86}, {"referenceID": 38, "context": "Likewise, the value of pbest solution for each Particle is examined against current solution and the new pbest is determined based on one of three ways [40]: 1) if the pbest is dominated by current solution then the current solution is the new pbest for that Particle.", "startOffset": 152, "endOffset": 156}, {"referenceID": 39, "context": "During the update of Particles, it is important to mutate the current solution [41].", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "Where: \uf0b7 R is a randomly generated bit (zero and one both have a 50% probability of being generated) \uf0b7 t is the current iteration number \uf0b7 r is a random number generated from a uniform distribution in the range [0,1] \uf0b7 b is a tunable parameter that defines the non-uniformity level of the operator.", "startOffset": 211, "endOffset": 216}, {"referenceID": 38, "context": "In this approach, the b parameter is set to 5 as suggested in [40].", "startOffset": 62, "endOffset": 66}, {"referenceID": 35, "context": "When Particles fly to find better solutions the velocity and position of the Particle is updated based on its experience and that of neighborhoods [37].", "startOffset": 147, "endOffset": 151}, {"referenceID": 36, "context": "The typical solution is to truncate the location at the exceeded boundary at this iteration and reflect the velocity in the boundary so that the particle moves away at the next generation [38].", "startOffset": 188, "endOffset": 192}, {"referenceID": 36, "context": "This technique does not necessarily alter the direction of Particle, but permitting the particle to stay in the vicinity of the boundary [38].", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "Most of the methods in literature were tested on a single or a very limited number of datasets, thereby reducing the credibility of the proposed model [26].", "startOffset": 151, "endOffset": 155}, {"referenceID": 3, "context": "Specifically, these datasets come from two different sources namely, PROMISE [4] and ISBSG [7].", "startOffset": 77, "endOffset": 80}, {"referenceID": 23, "context": "Cocomo dataset enables the researchers to classify projects in terms of three different development modes: Organic, semi-detached and embedded [25].", "startOffset": 143, "endOffset": 147}, {"referenceID": 1, "context": "So we used the criteria that already utilized in our previous research which can be found in [2][29][46].", "startOffset": 93, "endOffset": 96}, {"referenceID": 27, "context": "So we used the criteria that already utilized in our previous research which can be found in [2][29][46].", "startOffset": 96, "endOffset": 100}, {"referenceID": 44, "context": "So we used the criteria that already utilized in our previous research which can be found in [2][29][46].", "startOffset": 100, "endOffset": 104}, {"referenceID": 24, "context": "The employed datasets typically contain a unique set of features that can be categorized according to four classes [26]: size features, development features, environment features and project data.", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "[12] in that all features that would introduce a zero into the denominator (for a particular case) are excluded from the calculation of the adaptation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "The principal reason for this selection, the leave-one-out cross validation has been used in deterministic procedure that can be exactly repeated by any other research with access to a particular dataset [45].", "startOffset": 204, "endOffset": 208}, {"referenceID": 43, "context": "Also, it generates higher variance estimates than n-Fold cross validation since leave-one-out cross validation conducts more tests [45].", "startOffset": 131, "endOffset": 135}, {"referenceID": 22, "context": "Moreover, the proposed adaptation functions are compared to some well-known adaptation strategies existing in the literature such as: LSE [24], RTM [10], AQUA [16] and GA [5] in addition to the ABE0.", "startOffset": 138, "endOffset": 142}, {"referenceID": 8, "context": "Moreover, the proposed adaptation functions are compared to some well-known adaptation strategies existing in the literature such as: LSE [24], RTM [10], AQUA [16] and GA [5] in addition to the ABE0.", "startOffset": 148, "endOffset": 152}, {"referenceID": 14, "context": "Moreover, the proposed adaptation functions are compared to some well-known adaptation strategies existing in the literature such as: LSE [24], RTM [10], AQUA [16] and GA [5] in addition to the ABE0.", "startOffset": 159, "endOffset": 163}, {"referenceID": 4, "context": "Moreover, the proposed adaptation functions are compared to some well-known adaptation strategies existing in the literature such as: LSE [24], RTM [10], AQUA [16] and GA [5] in addition to the ABE0.", "startOffset": 171, "endOffset": 174}, {"referenceID": 28, "context": "5 is considered better [30].", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "Presenting results without statistical significance is not convincing therefore we use win, tie, loss algorithm [25] to compare the predictive performance of the variants of adaptation methods, see Figure 8.", "startOffset": 112, "endOffset": 116}, {"referenceID": 23, "context": "Also, the win-tie-loss algorithm used Wilcoxon test in its procedure [25].", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "8 Pseudo code for win, tie, loss calculation between method Methodi and Methodj based on performance measure E [25].", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "Previous research studies [1][2][11][15][19][24] use limited number of analogies which is frequently less than or equal to 5 analogies.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "Previous research studies [1][2][11][15][19][24] use limited number of analogies which is frequently less than or equal to 5 analogies.", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "Previous research studies [1][2][11][15][19][24] use limited number of analogies which is frequently less than or equal to 5 analogies.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "Previous research studies [1][2][11][15][19][24] use limited number of analogies which is frequently less than or equal to 5 analogies.", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "Previous research studies [1][2][11][15][19][24] use limited number of analogies which is frequently less than or equal to 5 analogies.", "startOffset": 40, "endOffset": 44}, {"referenceID": 22, "context": "Previous research studies [1][2][11][15][19][24] use limited number of analogies which is frequently less than or equal to 5 analogies.", "startOffset": 44, "endOffset": 48}, {"referenceID": 30, "context": "[32] showed that using different evaluation measures behave differently therefore they could be useful to produce prediction models that present trade-off between these evaluation measures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "It is already recognized that using subset of features would perform better than using all features in terms of evaluation measures [8].", "startOffset": 132, "endOffset": 135}, {"referenceID": 0, "context": "The importance of weighting optimization in the adaptation strategy This section concentrates on answering question RQ4 which states that: does the use of weighting values contribute towards improving prediction accuracy of the adaptation technique? It is already known that using weighting mechanism shows considerable performance when it is applied in project retrieval and some adaptation methods such as AQUA and GA [1].", "startOffset": 420, "endOffset": 423}, {"referenceID": 23, "context": "26 To summarize the results we consult win, tie, loss algorithm [25] to show the complete picture of our analysis to compare the predictive performance of the variants of Adaptation methods as suggested by Kocaguneli et al.", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "[25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] results, any method is to be superior to others, should be ranked first and has a minimum number of changes in their ranks.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Analogy Based Effort Estimation (ABE) is one of the prominent methods for software effort estimation. The fundamental concept of ABE is closer to the mentality of expert estimation but with an automated procedure in which the final estimate is generated by reusing similar historical projects. The main key issue when using ABE is how to adapt the effort of the retrieved nearest neighbors. The adaptation process is an essential part of ABE to generate more successful accurate estimation based on tuning the selected raw solutions, using some adaptation strategy. In this study we show that there are three interrelated decision variables that have great impact on the success of adaptation method: (1) number of nearest analogies (k), (2) optimum feature set needed for adaptation, and (3) adaptation weights. To find the right decision regarding these variables, one need to study all possible combinations and evaluate them individually to select the one that can improve all prediction evaluation measures. The existing evaluation measures usually behave differently, presenting sometimes opposite trends in evaluating prediction methods. This means that changing one decision variable could improve one evaluation measure while it is decreasing the others. Therefore, the main theme of this research is how to come up with best decision variables that improve adaptation strategy and thus, the overall evaluation measures without degrading the others. The impact of these decisions together has not been investigated before, therefore we propose to view the building of adaptation procedure as a multi-objective optimization problem. The Particle Swarm Optimization Algorithm (PSO) is utilized to find the optimum solutions for such decision variables based on optimizing multiple evaluation measures. We evaluated the proposed approaches over 15 datasets and using 4 evaluation measures. After extensive experimentation we found that: (1) predictive performance of ABE has noticeably been improved, (2) optimizing all decision variables together is more efficient than ignoring any one of them. (3) Optimizing decision variables for each project individually yield better accuracy than optimizing them for the whole dataset.", "creator": "Microsoft\u00ae Word 2010"}}}