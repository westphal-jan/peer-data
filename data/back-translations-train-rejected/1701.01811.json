{"id": "1701.01811", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jan-2017", "title": "Structural Attention Neural Networks for improved sentiment analysis", "abstract": "We introduce a tree-structured attention neural network for sentences and small phrases and apply it to the problem of sentiment classification. Our model expands the current recursive models by incorporating structural information around a node of a syntactic tree using both bottom-up and top-down information propagation. Also, the model utilizes structural attention to identify the most salient representations during the construction of the syntactic tree. To our knowledge, the proposed models achieve state of the art performance on the Stanford Sentiment Treebank dataset.", "histories": [["v1", "Sat, 7 Jan 2017 09:58:49 GMT  (31kb,D)", "http://arxiv.org/abs/1701.01811v1", "Submitted to EACL2017 for review"]], "COMMENTS": "Submitted to EACL2017 for review", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["filippos kokkinos", "alexandros potamianos"], "accepted": false, "id": "1701.01811"}, "pdf": {"name": "1701.01811.pdf", "metadata": {"source": "CRF", "title": "Structural Attention Neural Networks for improved sentiment analysis", "authors": ["Filippos Kokkinos", "Alexandros Potamianos"], "emails": ["el11142@central.ntua.gr", "potam@central.ntua.gr"], "sections": [{"heading": "1 Introduction", "text": "This is a relatively recent field of research that has aroused great interest, as demonstrated by a number of common evaluation tasks, such as the analysis of tweets (Nakov et al., 2016). In (Turney and Littman, 2002), the affective evaluations of unknown words were predicted by analyzing the affective evaluations of a small set of words (seeds) and the semantic relationships between the seed words. An example of sentence analysis was proposed in (Malandrakis et al., 2013). Other fields of application include the measurement of public opinion and the prediction of election outcomes (seeds) and the semantic relationships between the seed words. An example of sentence analysis was proposed."}, {"heading": "2 Tree-Structured GRUs", "text": "Recursive GRUs (tree-gru) on tree structures are an extension of the sequential GRUs that allow information to be disseminated through network topologies. Similar to recursive LSTM networks on tree structures (Tai et al., 2015), the tree-GRU for each node of a tree has gating mechanisms that modulate the flow of information within the unit without requiring a separate memory cell. Activation hj of tree-GRU for node j is the interpolation of the previous calculated activation hjk of its kth child from the total number of N children and the activation of the candidate h-j. hj = zj - N = 1 hjk + (1 \u2212 zj) h - where zj is the update function that determines the degree of updating that will take place on the activation of the kth child from the total number of N children and candidate activation h."}, {"heading": "2.1 Bidirectional TreeGRU", "text": "A natural extension of the tree structure GRU is the addition of a bidirectional approach. TreeGRUs calculate an activation for the node j (similarly, a recalculated activation contains contents of both the children and the parents of a particular node. The bidirectional neural network can be trained in two separate phases: i) in the upward phase and ii) in the downward phase. During the upward phase, the network topology is similar to the topology of a TreeGru, each activation is based on the previously calculated activations found lower in a lower upward phase. If every activation of the node has been calculated, from the leaves to the root, then the root activation d is used as the input of the downward phase. The downward phase calculates the activations for each child of a lower upward phase."}, {"heading": "2.2 Structural Attention", "text": "We introduce Structural Attention, a generalization of the sequential attention model (Luong et al., 2015), which extracts informative nodes from a syntactic tree and aggregates the representation of these nodes to form the sentence vector. We introduce the representation of the node by a single-layer multilayer perceptron with the Ww-Rdxd weight matrix to obtain the hidden representation uj.uj = tanh (Ww \u0445 hj) (13) Using the Softmax function, weights aj for each node are obtained based on the similarity of the hidden representation uj and a global context vetor uw \u0445 Rd. Normalized weights ajare used to form the final sentence representation s-Rd, which is a weighted summation of all node representation hj.aj = u > j, uw-N i = 1 > i-i = 1 ihi (15) The structural representation of all the proposed node is applied during the training."}, {"heading": "3 Experiments", "text": "We use the Stanford Sentiment Treebank data set (Socher et al., 2013), which contains sentiment labels for each syntactically plausible phrase from the 8544 / 1101 / 2210 Train / Dev / Test records. Each phrase is labeled with a 5-class sentiment value, i.e. very negative, negative, neutral, positive, very positive. It can also be used for a binary classification subtask by excluding neutral phrases for the original splits. The binary classification subtask is evaluated to 6920 / 872 / 1821 Train / Dev / Test splits."}, {"heading": "3.1 Sentiment Classification", "text": "For all the aforementioned architectures on each node j, we use a softmax classifier to predict the sentiment label y-j. The unidirectional TreeGRU classifier uses the hidden state hj, which was created from recursive calculations to node j using a set xj of input nodes to predict the label as follows. The classifier for bidirectional TreeBiGRU architectures uses both the hidden state h-j (y-xj) = Softmax (Ws-hj) (16), where Ws-Rdxc and c are the number of sentiment classes. The classifier for bidirectional TreeBiGRU architectures uses both the hidden state h-j and h-j, which was produced from recursive computations to node j during the up and down phases."}, {"heading": "3.2 Results", "text": "The evaluation results are presented in Table 2 in terms of accuracy for several state-of-the-art models proposed in the literature, as well as for the TreeGRU and TreeBiGRU models proposed in this thesis. Of the approaches reported in the literature, the highest accuracy is achieved by DRNN and DMN for the binary scheme (88.6) and by DMN for the fine-grained scheme (52.1). We observe that the best performance is achieved by TreeBiGRU with attention to both binary (89.5) and fine-grained (52.4) evaluation metrics, which exceeds all previously reported results. Furthermore, the attention mechanism applied in the proposed TreeGRU and TreeBiGRU models improves performance for both evaluation metrics."}, {"heading": "4 Hyperparameters and Training Details", "text": "The evaluated models are trained using the AdaGrad algorithm (Duchi et al., 2010), using 0.01 learning rate and a minibatch of 25 sets. L2 regularization is performed using model parameters with a \u03bb value of 10 \u2212 4. We use dropouts with a 0.5 probability on both the input layer and the Softmax layer. Word embedding is initialized with publicly available glove vectors with a 300-dimensional dimension. The glove vectors provide a 95.5% coverage for the SST dataset. All initialized word vectors are fine-tuned during the training process along with each other parameter. Each matrix is multiplied by the identity matrix multiplied by 0.5 except for the matrices of the Softmax layer and the attention layer, which are randomly initialized from the normal Gaussian distribution. Each bias vector is initialized with zero. The training process takes 40 epochs."}, {"heading": "5 Conclusion", "text": "In this short paper, we propose an extension of Recursive Neural Networks, which includes a bidirectional approach with gated memory units and an attention model at the structural level. Proposed models were evaluated at both fine-grained and binary sentiment classification levels at the sentence level. Our results suggest that both the direction of calculation and attention at the structural level can improve the performance of neural networks at the sentiment analysis level."}], "references": [{"title": "Twitter mood predicts the stock market", "author": ["Johan Bollen", "Huina Mao", "Xiaojun Zeng."], "venue": "Journal of Computational Science, 2(1):1\u20138.", "citeRegEx": "Bollen et al\\.,? 2011", "shortCiteRegEx": "Bollen et al\\.", "year": 2011}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Technical report, EECS Department, University of California, Berkeley.", "citeRegEx": "Duchi et al\\.,? 2010", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput., 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Bidirectional recursive neural networks for token-level labeling with structure", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "CoRR, abs/1312.0493.", "citeRegEx": "Irsoy and Cardie.,? 2013", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "CoRR, abs/1404.2188.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "CoRR, abs/1408.5882.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher."], "venue": "CoRR, abs/1506.07285.", "citeRegEx": "Kumar et al\\.,? 2015", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "CoRR, abs/1508.04025.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Sail: A hybrid approach to sentiment analysis", "author": ["Nikolaos Malandrakis", "Abe Kazemzadeh", "Alexandros Potamianos", "Shrikanth Narayanan."], "venue": "Proceedings SemEval, pages 438\u2013442.", "citeRegEx": "Malandrakis et al\\.,? 2013", "shortCiteRegEx": "Malandrakis et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Semeval2016 task 4: Sentiment analysis in twitter", "author": ["Preslav Nakov", "Alan Ritter", "Sara Rosenthal", "Fabrizio Sebastiani", "Veselin Stoyanov."], "venue": "Proceedings of the 10th international workshop on semantic evaluation (SemEval 2016).", "citeRegEx": "Nakov et al\\.,? 2016", "shortCiteRegEx": "Nakov et al\\.", "year": 2016}, {"title": "Opinion mining and sentiment analysis", "author": ["Bo Pang", "Lillian Lee."], "venue": "Found. Trends Inf. Retr., 2(12):1\u2013135.", "citeRegEx": "Pang and Lee.,? 2008", "shortCiteRegEx": "Pang and Lee.", "year": 2008}, {"title": "Modeling indian general elections: sentiment analysis of political twitter data", "author": ["Kartik Singhal", "Basant Agrawal", "Namita Mittal."], "venue": "Information Systems Design and Intelligent Applications, pages 469\u2013477.", "citeRegEx": "Singhal et al\\.,? 2015", "shortCiteRegEx": "Singhal et al\\.", "year": 2015}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of the conference on", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "CoRR, abs/1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Unsupervised learning of semantic orientation from a hundred-billion-word corpus", "author": ["Peter Turney", "Michael L Littman"], "venue": null, "citeRegEx": "Turney and Littman.,? \\Q2002\\E", "shortCiteRegEx": "Turney and Littman.", "year": 2002}, {"title": "Deciphering word-of-mouth in social media: Text-based metrics of consumer reviews", "author": ["Zhu Zhang", "Xin Li", "Yubo Chen."], "venue": "ACM Trans. Manage. Inf. Syst., 3(1):5:1\u20135:23.", "citeRegEx": "Zhang et al\\.,? 2012", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 19, "context": "Sentiment analysis deals with the assessment of opinions, speculations, and emotions in text (Zhang et al., 2012; Pang and Lee, 2008).", "startOffset": 93, "endOffset": 133}, {"referenceID": 12, "context": "Sentiment analysis deals with the assessment of opinions, speculations, and emotions in text (Zhang et al., 2012; Pang and Lee, 2008).", "startOffset": 93, "endOffset": 133}, {"referenceID": 11, "context": ", analysis of tweets (Nakov et al., 2016).", "startOffset": 21, "endOffset": 41}, {"referenceID": 18, "context": "In (Turney and Littman, 2002), the affective ratings of unknown words were predicted utilizing the affective ratings of a small set of words (seeds) and the semantic relatedness between the unknown and the seed words.", "startOffset": 3, "endOffset": 29}, {"referenceID": 9, "context": "An example of sentence-level analysis was proposed in (Malandrakis et al., 2013).", "startOffset": 54, "endOffset": 80}, {"referenceID": 13, "context": "Other application areas include the detection of public opinion and prediction of election results (Singhal et al., 2015), correlation of mood states and stock market indices (Bollen et al.", "startOffset": 99, "endOffset": 121}, {"referenceID": 0, "context": ", 2015), correlation of mood states and stock market indices (Bollen et al., 2011).", "startOffset": 61, "endOffset": 82}, {"referenceID": 3, "context": "Recently, Recurrent Neural Network (RNN) with Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Units (GRU) (Chung et al.", "startOffset": 76, "endOffset": 110}, {"referenceID": 1, "context": "Recently, Recurrent Neural Network (RNN) with Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Units (GRU) (Chung et al., 2014) have been applied to various Natural Language Processing tasks.", "startOffset": 142, "endOffset": 162}, {"referenceID": 15, "context": "Tree structured neural networks, which are found in literature as Recursive Neural Networks, hold a linguistic interest due to their close relation to syntactic structures of sentences being able to capture distributed information of structure such as logical terms(Socher et al., 2012).", "startOffset": 265, "endOffset": 286}, {"referenceID": 4, "context": "This neural network is referred to as Bidirectional Recursive Network (Irsoy and Cardie, 2013).", "startOffset": 70, "endOffset": 94}, {"referenceID": 16, "context": "We evaluate our approach on the sentence-level sentiment classification task using one standard movie review dataset (Socher et al., 2013).", "startOffset": 117, "endOffset": 138}, {"referenceID": 17, "context": "Similar to Recursive LSTM network on tree structures (Tai et al., 2015), for every node of a tree, the Tree-GRU has gating mechanisms that modulate the flow of information inside the unit without the need of a separate memory cell.", "startOffset": 53, "endOffset": 71}, {"referenceID": 14, "context": "The candidate activation h\u0303j for a node j is computed similarly to that of a Recursive Neural Network as in (Socher et al., 2011):", "startOffset": 108, "endOffset": 129}, {"referenceID": 8, "context": "We introduce Structural Attention, a generalization of sequential attention model (Luong et al., 2015) which extracts informative nodes out of a syntactic tree and aggregates the representation of those nodes in order to form the sentence vector.", "startOffset": 82, "endOffset": 102}, {"referenceID": 16, "context": "We use the Stanford Sentiment Treebank (Socher et al., 2013) dataset which contains sentiment labels for every syntactically plausible phrase out of the 8544/1101/2210 train/dev/test sentences.", "startOffset": 39, "endOffset": 60}, {"referenceID": 2, "context": "The evaluated models are trained using the AdaGrad (Duchi et al., 2010) algorithm using 0.", "startOffset": 51, "endOffset": 71}, {"referenceID": 16, "context": "RNN, MV-RNN and RNTN (Socher et al., 2013).", "startOffset": 21, "endOffset": 42}, {"referenceID": 10, "context": "PVec: (Mikolov et al., 2013).", "startOffset": 6, "endOffset": 28}, {"referenceID": 17, "context": "TreeLSTM (Tai et al., 2015).", "startOffset": 9, "endOffset": 27}, {"referenceID": 4, "context": "DRNN (Irsoy and Cardie, 2013).", "startOffset": 5, "endOffset": 29}, {"referenceID": 5, "context": "DCNN (Kalchbrenner et al., 2014).", "startOffset": 5, "endOffset": 32}, {"referenceID": 6, "context": "CNN-multichannel (Kim, 2014).", "startOffset": 17, "endOffset": 28}, {"referenceID": 7, "context": "DMN (Kumar et al., 2015)", "startOffset": 4, "endOffset": 24}], "year": 2017, "abstractText": "We introduce a tree-structured attention neural network for sentences and small phrases and apply it to the problem of sentiment classification. Our model expands the current recursive models by incorporating structural information around a node of a syntactic tree using both bottomup and top-down information propagation. Also, the model utilizes structural attention to identify the most salient representations during the construction of the syntactic tree. To our knowledge, the proposed models achieve state of the art performance on the Stanford Sentiment Treebank dataset.", "creator": "TeX"}}}