{"id": "1701.01854", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jan-2017", "title": "Neural Machine Translation on Scarce-Resource Condition: A case-study on Persian-English", "abstract": "Neural Machine Translation (NMT) is a new approach for Machine Translation (MT), and due to its success, it has absorbed the attention of many researchers in the field. In this paper, we study NMT model on Persian-English language pairs, to analyze the model and investigate the appropriateness of the model for scarce-resourced scenarios, the situation that exists for Persian-centered translation systems. We adjust the model for the Persian language and find the best parameters and hyper parameters for two tasks: translation and transliteration. We also apply some preprocessing task on the Persian dataset which yields to increase for about one point in terms of BLEU score. Also, we have modified the loss function to enhance the word alignment of the model. This new loss function yields a total of 1.87 point improvements in terms of BLEU score in the translation quality.", "histories": [["v1", "Sat, 7 Jan 2017 16:27:44 GMT  (711kb)", "http://arxiv.org/abs/1701.01854v1", "6 pages, Submitted in ICEE 2017"]], "COMMENTS": "6 pages, Submitted in ICEE 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mohaddeseh bastan", "shahram khadivi", "mohammad mehdi homayounpour"], "accepted": false, "id": "1701.01854"}, "pdf": {"name": "1701.01854.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation on Scarce-Resource Condition: A case-study on Persian-English", "authors": ["Shahram Khadivi", "Mohaddeseh Bastan", "Mohammad Mehdi Homayounpour"], "emails": ["homayoun}@aut.ac.ir"], "sections": [{"heading": null, "text": "In this article, we examine the NMT model for the Persian language and find the best parameters and hyperparameters for two tasks: translation and transcription. We also apply some pre-processor scenarios that exist for Persian-centric translation systems. Furthermore, we note that the model is able to achieve an increase from one point to the other in terms of processing ingredients."}, {"heading": "A. Statistical Machine Translation", "text": "A common SMT model results in the determination of the target sentence f: y1, y2,..., yT using the source sentence e: x1, x2,..., xS by maximizing the following term [7]: p (e | f) ~ p (e).p (f | e) In this equation, () is the language model that helps us generate natural and grammatical characteristics, and p (f | e) is the translation model that ensures that e is usually interpreted as f and not as something else [8]. Most MT systems use a log-linear model instead of the pure form to model additional characteristics in the final equation. Then, the model will look like this [8]: log () = () = 1 + () This equation shows the mth characteristic of the SMT system with the symbol and the corresponding weight with the. Z is a normalization that differs from the M1."}, {"heading": "B. Neural Machine Translation", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "C. Data preprocessing", "text": "The Persian language makes MT a difficult task because of its specific characteristics. Therefore, the input sentences should be pre-processed and then inserted into the NMT model. Here is the list of pre-processing tasks performed on Persian corpora: \u25cf All corpora are changed so that they have one sentence per string in one of the punctuations: \",.\" or!. \"\u25cf All words are separated by a single space. \u25cf All zero-width non-fractions have been removed. \u25cf If a word is an appendage with a symbol, punctuation mark or other character, it is tokenized in...,\"... and all associated words are tokenized. \u25cf All of these pre-processing tasks prepare the Persian data to use it for NMT."}, {"heading": "D. The alignment feature", "text": "One of the characteristics of the NMT model is that it does not need to define different characteristics, and each characteristic is tuned to maximize the probability function. In fact, it learns everything about a unique model and translates the initial theorem into the target via the trained model. On the other hand, the SMT defines different characteristics and calculates the corresponding cost for each of them and tries to maximize the probability function. Since each of them has its own merits, our model will use both of these characteristics and tries to increase the accuracy of the alignment model in NMT by using the alignment model in SMT model, we use the GIZA + tool to align the source and target sentences. This tool uses an EM algorithm to align words in the source and target sentences and shows which words of the source sentence we align with the word or words of the target sentence."}, {"heading": "E. System configurations", "text": "For our experiment, we used the NVIDIA GeForce GTX 780 GPU, which increases processing speed compared to the CPU. Also, the NVIDIA CUDA Toolkit v.7 is used specifically for its mathematical libraries and optimization routines. In addition, we used the cuDNN library v5 to increase training speed. We used the Tensroflow Framework v0.10 for programming and made our changes based on the vendor's suggested MT model."}, {"heading": "F. Dataset description", "text": "We used two data sets. The first data set is Verbmobil English-Persian translation record, which consists of some conversation sets in English and their translations into Persian. The second data set is a transliteration record. It consists of some separate words in Persian. In this data set, the sentence means a word with separated characters, and words mean characters. Table I provides information about training, development and test sets of Verbmobil and transliteration. Sent. Count means the number of sentences for each of the English and Persian data sets. Unique words count is the number of unique words available in each corpus. This number refers to the main data set without preprocessing task. Table II describes an example from each data set. One set for each data set is shown in Table II."}, {"heading": "G. Evaluating Measurements", "text": "For the evaluation of the proposed model, we use different measurements. For the translation, we use BLEU [33] measurement, which is fast and language-independent. This measurement is based on the accuracy of n-grams of the translated text compared to target references or references. For the transliteration task, we use four different measurements in addition to BLEU. The first measurement is accuracy. Accuracy means how many sentences have been completely and error-free transliterated in each position of the sentence. The next measurement is WHO (Word Error Rate), which counts the number of incorrectly translated words. The words should be translated in exactly the same order as source words. As we have already described, words here mean the character. So, WHO measures the number of characters that have been translated into the wrong character. The third measurement is PER (Position-Independent Word Error Rate) [34]. It is the same as WHO, but does not take into account the order of the words."}, {"heading": "H. Experiments and Results", "text": "We evaluate our model through 3 different experiments. First, we find the best configuration for each data set. The set parameters are the number of layers and the number of nodes in each layer of the RNN. After adjusting the parameters and hyperparameters for each data set, we evaluate our proposed model. First, the changes to the data set and then the cost function are evaluated. The results are shown in Tables III to VI. First, we describe the results of the translation task and then for the translation task.Table III shows different configurations of the NMT model for the translation task. As we see by increasing the number of hidden layer nodes, all measurements improve and TER, WHO, PER acceptance. But stops at a specific configuration where the number of hidden nodes is not improved."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "GE. Dahl", "AR. Mohamed", "N. Jaitly"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Multi-column deep neural networks for image classification.", "author": ["D. Ciregan", "U. Meier", "J. Schmidhuber"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Twitter mood predicts the stock market.", "author": ["J. Bollen", "H. Mao", "X. Zeng"], "venue": "Journal of Computational Science", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Machine translation using neural networks and finite-state models", "author": ["MA. Casta\u00f1o", "F. Casacuberta", "E. Vidal"], "venue": "Theoretical and Methodological Issues in Machine Translation, pp. 160-167, Jul. 1997.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C C. Citro"], "venue": "Preliminary White Paper, November 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Readings in machine translation", "author": ["HL S. Nirenburg", "Somers"], "venue": "MIT Press, 2003.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "A statistical approach to machine translation", "author": ["PF. Brown", "J. Cocke", "SA. Pietra", "VJ. Pietra", "F. Jelinek", "JD. Lafferty"], "venue": "Computational linguistics, Vol 16(2), pp. 79-85, Jun. 1990.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1990}, {"title": "Statistical machine translation", "author": ["Y. Al-Onaizan", "J. Curin", "M. Jahr", "K. Knight", "J. Lafferty", "D. Melamed"], "venue": "InFinal Report, JHU Summer Workshop, Vol. 30, 1999.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "The alignment template approach to statistical machine translation", "author": ["FJ. Och", "H. Ney"], "venue": "Computational linguistics, Vol. 30(4), pp. 417-49, Dec. 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["PF. Brown", "VJ. Pietra", "SA. Pietra", "RL. Mercer"], "venue": "Computational linguistics, Vol. 19(2), pp. 263-311, Jun. 1993.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Training and analysing deep recurrent neural networks", "author": ["M. Hermans", "B. Schrauwen"], "venue": "InAdvances in Neural Information Processing Systems, pp. 190-198, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "QV. Le"], "venue": "InAdvances in neural information processing systems, pp. 3104-3112, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing, pp. 1724-34, Jun. 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreite", "J. Schmidhuber"], "venue": "Neural computation, Vol. 9(8), pp. 1735-80, Nov. 1997.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "On the properties of neural machine translation: Encoder\u2013Decoder approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint, Sep 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics, pp. 400-407, Sep. 1951.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1951}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["MD. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, Dec. 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio"], "venue": "The Journal of Machine Learning Research, Vol. 3, pp. 1137-1155, 2003.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Continuous space language models for the IWSLT 2006 task", "author": ["S. Holger", "MR. Costa-Jussa", "J. AR Fonollosa"], "venue": "IWSLT, 2006.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "Presentation at Google, Mountain View, April 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Yvon, \"Continuous space translation models with neural networks.\" Proceedings of the 2012 conference of the north american chapter of the association for computational linguistics: Human language technologies", "author": ["LH. Son", "A. Allauzen", "Fr"], "venue": "Association for Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Comparison of feedforward and recurrent neural network language models.\" Acoustics, Speech and Signal Processing (ICASSP)", "author": ["M. Sundermeyer", "I. Oparin", "JL. Gauvain", "B. Freiberg", "R. Schl\u00fcter", "H. Ney"], "venue": "IEEE International Conference on", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Recurrent Continuous Translation Models", "author": ["N. Kalchbrenner", "B. Phil"], "venue": "EMNLP, Vol. 3, p. 413, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint Language and Translation Modeling with Recurrent Neural Networks", "author": ["M. Auli", "M. Galley", "C. Quirk", "G. Zweig"], "venue": "EMNLP. Vol. 3, pp. 1044-54, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Overcoming the curse of sentence length for neural machine translation using automatic segmentation", "author": ["J. Pouget-Abadie", "D. Bahdanau", "B. van Merri\u00ebnboer", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1257. Sep. 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Continuous Space Translation Models for Phrase-Based Statistical Machine Translation", "author": ["H. Schwenk"], "venue": "COLING (Posters), pp. 1071-1080, Dec. 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast and Robust Neural Network Joint Models for Statistical Machine Translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "RM. Schwartz", "J. Makhoul"], "venue": "InACL Vol. 1, pp. 1370-1380, Jun. 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "KK. Paliwal"], "venue": "IEEE Transactions on Signal Processing, Vol. 45, pp. 2673-81, Nov. 1997.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "Translation Modeling with Bidirectional Recurrent Neural Networks", "author": ["M. Sundermeyer", "T. Alkhouli", "J. Wuebker", "H. Ney"], "venue": "InEMNLP 2014, pp. 14-25, Oct. 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Guided Alignment Training for Topic-Aware Neural Machine Translation", "author": ["W. Chen", "E. Matusov", "S. Khadivi", "JT. Peter"], "venue": "arXiv preprint arXiv:1607.01628, Jul 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "WJ. Zhu"], "venue": "InProceedings of the 40th annual meeting on association for computational linguistics, pp. 311-318, Jul. 2002, Association for Computational Linguistics.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Accelerated DP based search for statistical translation", "author": ["C. Tillmann", "S. Vogel", "H. Ney", "A. Zubiaga", "H. Sawaf"], "venue": "InEurospeech, Sep 1997", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["M. Snover", "B. Dorr", "R. Schwartz", "L. Micciulla", "J. Makhoul"], "venue": "InProceedings of association for machine translation in the Americas, Vol. 200, No. 6, pp. 223-31, Aug. 2006.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "These networks have recently been used in many applications such as speech recognition [1], image processing [2], and natural language processing [3] and achieved remarkable results.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "These networks have recently been used in many applications such as speech recognition [1], image processing [2], and natural language processing [3] and achieved remarkable results.", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "These networks have recently been used in many applications such as speech recognition [1], image processing [2], and natural language processing [3] and achieved remarkable results.", "startOffset": 146, "endOffset": 149}, {"referenceID": 3, "context": "MT which is a subcategory of natural language processing was firstly processed using neural networks by Casta\u00f1o in 1997 [4].", "startOffset": 120, "endOffset": 123}, {"referenceID": 4, "context": "We use Tensorflow MT model [5] which was released by Google in 2015.", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "MT is the automation of the translation between human languages [6].", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": ", xS by maximizing the following term [7]:", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "In this equation, p(e)is the language model which helps our output to be natural and grammatical, and p(f|e) is the translation model which ensures that e is normally interpreted as f, and not some other thing [8].", "startOffset": 210, "endOffset": 213}, {"referenceID": 7, "context": "Then the model will be as follow [8]:", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "Alignment is one of the features for MT and the same alignment is used as what described in [10] for estimating parameters of SMT in this paper", "startOffset": 92, "endOffset": 96}, {"referenceID": 8, "context": "Architecture of Translation approach based on log-linear model [9]", "startOffset": 63, "endOffset": 66}, {"referenceID": 10, "context": "DNNs are like pipeline processing in which each layer solves part of the issue and the result is fed into the next layer and at the end the last layer generates the output [11].", "startOffset": 172, "endOffset": 176}, {"referenceID": 11, "context": "DNNs are powerful because the ability to perform parallel computations for several steps [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "Most of the NMT models consist of two parts including an encoder which encodes the input sequence to a fixed length array, and a decoder which decodes the context vector into the output sequence [13].", "startOffset": 195, "endOffset": 199}, {"referenceID": 13, "context": "f is an activation function which can be simple as a sigmoid function or complicated as an LSTM [14].", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "To map a sequence of input words to sequence of output words with different length, the first attempt is done by [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "But as the length of the sentence increases, the context vector cannot encode all of the source sentences and the performance decreases significantly [15].", "startOffset": 150, "endOffset": 154}, {"referenceID": 15, "context": "Paper [16] proposes a model which does not encode the whole source sentence into a fixed length array.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "In contrast of the other NMT, the alignment is not a hidden variable here and is computed as a soft alignment [16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "For training the model we use Stochastic Gradient Descent [17].", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "The learning rate is a parameter which controls how large a step should be in the direction of the negative gradient [18].", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "In 2003 neural network language models generally introduced in [19].", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "In machine translation, some researchers used these models for rescoring the translation [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "For example, [12] used neural networks for rescoring the translation candida sentences and [13] used neural networks for translation scores in phrase table.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "For example, [12] used neural networks for rescoring the translation candida sentences and [13] used neural networks for translation scores in phrase table.", "startOffset": 91, "endOffset": 95}, {"referenceID": 20, "context": "One of the simplest and most impressive works in NMT is [21] which used neural networks for rescoring the n-best list in an MT system.", "startOffset": 56, "endOffset": 60}, {"referenceID": 21, "context": "In 2012, Li proposed an MT model using feedforward neural networks which used an output layer for classification and a short list for rescoring [22].", "startOffset": 144, "endOffset": 148}, {"referenceID": 22, "context": "For language model and machine translation, it has been shown that RNNs empirically work better than feedforward neural networks [23].", "startOffset": 129, "endOffset": 133}, {"referenceID": 23, "context": "encoder-decoder models for MT first used in [24].", "startOffset": 44, "endOffset": 48}, {"referenceID": 24, "context": "A newer model for encoder-decoder was presented in [25], where the decoder was conditioned on the source sentences.", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "In [13] another encoder-decoder was introduced which used an LSTM for encoding and decoding.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Bahdanau used an attention model to address the problem of translating long sentences [16].", "startOffset": 86, "endOffset": 90}, {"referenceID": 25, "context": "In [26] a model was proposed to address this problem.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "In [27] a model for scoring phrases was proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "Devlin [28] proposed an NMT model using feedforward neural network.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "Bidirectional LSTM first proposed by [29] and used for speech recognition task.", "startOffset": 37, "endOffset": 41}, {"referenceID": 29, "context": "These networks were used for MT in [30] and created a strong model which used next and previous input words for translation.", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "The idea of guided alignment first proposed in [31].", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "As the problem of the length of the sentence is decreased after using techniques described in [16], we decided to decrease the number of unique words and increase the length of the sentences.", "startOffset": 94, "endOffset": 98}, {"referenceID": 31, "context": "For translating we use BLEU [33] measurement, which is quick and language independent.", "startOffset": 28, "endOffset": 32}, {"referenceID": 32, "context": "The third measure is PER (Position-independent word Error Rate) [34].", "startOffset": 64, "endOffset": 68}, {"referenceID": 33, "context": "Finally, the last measure is TER (Translation Error Rate) [35] which counts the number of edits required to change a system output into one of the given translation references.", "startOffset": 58, "endOffset": 62}], "year": 2017, "abstractText": "Neural Machine Translation (NMT) is a new approach for Machine Translation (MT), and due to its success, it has absorbed the attention of many researchers in the field. In this paper, we study NMT model on Persian-English language pairs, to analyze the model and investigate the appropriateness of the model for scarce-resourced scenarios, the situation that exist for Persian-centered translation systems. We adjust the model for the Persian language and find the best parameters and hyper parameters for two tasks: translation and transliteration. We also apply some preprocessing task on the Persian dataset which yields to increase for about one point in terms of BLEU score. Also, we have modified the loss function to enhance the word alignment of the model. This new loss function yields a total of 1.87 point improvements in terms of BLEU score in the translation quality. Keywords-component; neural machine translation; cost function; alignment model; text preprocessing", "creator": "Microsoft\u00ae Word 2016"}}}