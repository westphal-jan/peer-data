{"id": "1701.01887", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jan-2017", "title": "Deep Learning for Time-Series Analysis", "abstract": "In many real-world application, e.g., speech recognition or sleep stage classification, data are captured over the course of time, constituting a Time-Series. Time-Series often contain temporal dependencies that cause two otherwise identical points of time to belong to different classes or predict different behavior. This characteristic generally increases the difficulty of analysing them. Existing techniques often depended on hand-crafted features that were expensive to create and required expert knowledge of the field. With the advent of Deep Learning new models of unsupervised learning of features for Time-series analysis and forecast have been developed. Such new developments are the topic of this paper: a review of the main Deep Learning techniques is presented, and some applications on Time-Series analysis are summaried. The results make it clear that Deep Learning has a lot to contribute to the field.", "histories": [["v1", "Sat, 7 Jan 2017 21:44:04 GMT  (63kb,D)", "http://arxiv.org/abs/1701.01887v1", "Written as part of the Seminar on Collaborative Intelligence in the TU Kaiserslautern. January 2016"]], "COMMENTS": "Written as part of the Seminar on Collaborative Intelligence in the TU Kaiserslautern. January 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["john cristian borges gamboa"], "accepted": false, "id": "1701.01887"}, "pdf": {"name": "1701.01887.pdf", "metadata": {"source": "CRF", "title": "Deep Learning for Time-Series Analysis", "authors": ["John Gamboa"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Artificial Neural Networks, Deep Learning, Time Series"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive by themselves if they are not able to play by the rules. (...) Most of them are able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...) Most of them are not able to play by the rules. (...)"}, {"heading": "1.1 Artificial Neural Network", "text": "The type of networks described here is by no means the only type of ANN architecture to be found in the literature; the reader is referred to a detailed description of architectural alternatives, such as the implementation of an LSTM concept, which is a detailed description of the various approaches. Furthermore, we point out that the reader is referring in a way to the implementation of an LSTM, as well as to details of CNN.A systems, which are basically a network of computing units connected by directed connections. Each computing unit performs a calculation and prints a value that then spreads out into other units. Connections usually have weights that correspond to how strongly two units are interconnected. Typically, computing is divided into two stages: aggregation and activation."}, {"heading": "2 Literature Review", "text": "In fact, most of them are able to survive on their own."}, {"heading": "3 Deep Learning for Time-Series Modeling", "text": "In this section, the work presented in [47] is reviewed by a factor of 2 \u2212 1 along the time dimension. As discussed above, FCNs are a modification of the CNN architecture, which, as required by some problems related to the TimeSeries, allows the input and output signals to have the same dimensions. Mittelman [47] argues that the architecture of the FCN is similar to the application of a Wavelet transformation, and that, for this reason, it can have strong variations when the input signal is subject to small translations. To solve this problem, and inspired by the non-decimated Wavelet transform, which is translation invariant, they propose the non-decimated fully convolutionary neural network (UFCNN), which is also translation unpredictable. The only difference between an FCN and a UFCNN is that the UFCNN Wavelet transform, which is translation invariant, removes both the uppooling architecture and the uppooling architecture."}, {"heading": "4 Deep Learning for Time-Series Classification", "text": "Wang and Oates [59] presented an approach to time series classification using CNN-like networks. In order to benefit from the high accuracy CNNs have achieved in image classification in recent years, the authors propose the idea of converting a time series into an image. Two approaches are presented: the first generates a Gramian Angular Field (GAF), while the second generates a Markov Transition Field (MTF). In both cases, the generation of the images increases the size of the time series, making the images potentially prohibitively large. Therefore, the authors propose strategies to reduce their size without losing too much information. Finally, the two image types are combined into a two-channel image, which is then used to achieve better results than those obtained when using each image separately. In the next sections, GAF and MTF are described. In the following equations, we assume that m = 1. Therefore, the time series is only composed by real observations (x refers to i)."}, {"heading": "4.1 Gramian Angular Field", "text": "The first step to generating a GAF consists of converting the entire time series into values between [\u2212 1, 1]. In the equation 1 represent max (X) and min (X) the maximum and minimum real values that are present in the time series X: x (i) = (x (i) \u2212 max (X) + (i) \u2212 max (X) = (X) \u2212 max (X) \u2212 min (X) (1) The next step consists of converting the newly generated time series X into polar coordinates. The angle is coded by x (i) + (i) \u2212 max (X) = max (X) \u2212 min (X) \u2212 min (X). The next step is to convert the newly created time series X into polar coordinates. The angle is???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "4.2 Markov Transition Field", "text": "The creation of the Markov Transition Field is based on the ideas proposed in [8] for the definition of the so-called Markov Transition Matrix (MTM). In a time series X, the first step is the definition of Q-quantile containers. Each x (i) is then assigned to the corresponding bin qj. The Markov Transition Matrix is the matrix W, which consists of elements composed in such a way that each pixel of M corresponds to the normalized \"frequency\" with which a point in the quantile qj is followed by a point in the quantile qi. \"This is a Q-Q matrix. The MTF is the n-n matrix M. Each pixel of M contains a value of W. The value in the pixel ij is the probability (as calculated in the construction of W) to proceed from the container in which the pixel i is in the containers: M = wij | x1, q1 \u00b7 q \u00b7 q \u00b7 qi, xqi, xj \u00b2, wij \u00b7 qiqi | x2"}, {"heading": "4.3 Performing Classification with the Generated Images", "text": "The authors use tiled CNNs to make classifications based on the images. In the reported experiments, both methods are evaluated separately in 12 \"hard\" data sets, \"where the classification error rate with the state-of-the-art SAX-BoP approach is above 0.1\" [39], i.e. 50Words, Adiac, Beef, Coffee, ECG200, Face (all), Lightning-2, Lightning-7, OliveOil, OSU Leaf, Swedish Leaf and Yoga [11]. Subsequently, the authors propose the use of both methods as \"colors\" of the images. The performance of the resulting classifier competes with many of the state-of-the-art classification methods reported by the authors."}, {"heading": "5 Deep Learning for Time-Series Anomaly Detection", "text": "Anomaly detection can easily be transformed into a task where the goal is to model the time series and, considering this model, find regions where the predicted values differ too much from the actual values (or, in other words, where the probability of the observed region is too low).This is the idea implemented in the paper reviewed in this section [44].The learned model does not require predicting all the m input variables. At any given time step, the learned model predicts l vectors with d input variables, with 1 \u2264 d \u2264 m.The modeling of the time series is done by applying a stacked LSTM architecture. The network has m input neurons (one for each input variable) and d \u00d7 l output neurons (one neuron for each of the d input variables, with 1 \u2264 m.The modeling of the time series is done using a stacked LSTM architecture."}, {"heading": "6 Conclusion", "text": "When applying deep learning, one tries to stack several independent neural network layers that together produce better results than the already existing flat structures. In this paper, we reviewed some of these modules, as well as the recent work done through their use, which can be found in the literature. Furthermore, we discussed some of the most important tasks normally performed in manipulating time series data using deep neural network structures. Finally, a more specific focus was placed on a work that performs each of these tasks. In these cases, the use of deep learning for time series analysis has yielded results that are better than the previously available techniques, which is evidence that this is a promising field for improvement. I would like to thank Ahmed Sheraz and Mohsin Munir for their guidance and contribution to this paper."}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["O. Abdel-Hamid", "Mohamed", "A.r.", "H. Jiang", "G. Penn"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. pp. 4277\u20134280. IEEE", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-time malaysian sign language translation using colour segmentation and neural network", "author": ["R. Akmeliawati", "M.P.L. Ooi", "Y.C. Kuang"], "venue": "Instrumentation and Measurement Technology Conference Proceedings, 2007. IMTC 2007. IEEE. pp. 1\u20136. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Fpgabased stochastic echo state networks for time-series forecasting", "author": ["M.L. Alomar", "V. Canals", "N. Perez-Mora", "V. Mart\u0301\u0131nez-Moll", "J.L. Rossell\u00f3"], "venue": "Computational Intelligence and Neuroscience 501, 537267", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep machine learning-a new frontier in artificial intelligence research [research frontier", "author": ["I. Arel", "D.C. Rose", "T.P. Karnowski"], "venue": "Computational Intelligence Magazine, IEEE 5(4), 13\u201318", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural network time series forecasting of financial markets", "author": ["E.M. Azoff"], "venue": "John Wiley & Sons, Inc.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H Larochelle"], "venue": "Advances in neural information processing systems 19, 153", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on 5(2), 157\u2013166", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1994}, {"title": "Duality between time series and networks", "author": ["A. Campanharo", "M.I. Sirer", "R.D. Malmgren", "F.M. Ramos", "L.A. Amaral"], "venue": "PloS one 6(8), e23378", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Forecasting the behavior of multivariate time series using neural networks", "author": ["K. Chakraborty", "K. Mehrotra", "C.K. Mohan", "S. Ranka"], "venue": "Neural networks 5(6), 961\u2013970", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Cooperative coevolution of elman recurrent neural networks for chaotic time series prediction", "author": ["R. Chandra", "M. Zhang"], "venue": "Neurocomputing 86, 116\u2013123", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "The ucr time series classification archive (July 2015), www.cs.ucr.edu/~eamonn/time_ series_data", "author": ["Y. Chen", "E. Keogh", "B. Hu", "N. Begum", "A. Bagnall", "A. Mueen", "G. Batista"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Multi-scale internet traffic forecasting using neural networks and time series methods", "author": ["P. Cortez", "M. Rio", "M. Rocha", "P. Sousa"], "venue": "Expert Systems 29(2), 143\u2013155", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Fuzzy time series forecasting with a novel hybrid approach combining fuzzy c-means and neural networks", "author": ["E. Egrioglu", "C.H. Aladag", "U. Yolcu"], "venue": "Expert Systems with Applications 40(3), 854\u2013857", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Time-series data mining", "author": ["P. Esling", "C. Agon"], "venue": "ACM Computing Surveys (CSUR) 45(1), 12", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "A hybrid neural network and arima model for water quality time series prediction", "author": ["D.\u00d6. Faruk"], "venue": "Engineering Applications of Artificial Intelligence 23(4), 586\u2013594", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "A novel approach for trajectory feature representation and anomalous trajectory detection", "author": ["W. Feng", "C. Han"], "venue": "Information Fusion (Fusion), 2015 18th International Conference on. pp. 1093\u20131099. IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Non-linear independent component analysis using series reversion and weierstrass network", "author": ["P. Gao", "W. Woo", "S. Dlay"], "venue": "IEE Proceedings-Vision, Image and Signal Processing 153(2), 115\u2013131", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep learning for tactile understanding from visual and haptic data", "author": ["Y. Gao", "L.A. Hendricks", "K.J. Kuchenbecker", "T. Darrell"], "venue": "arXiv preprint arXiv:1511.06065", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised sequence labelling with recurrent neural networks, vol", "author": ["A Graves"], "venue": "385. Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "A deep hybrid model for weather forecasting", "author": ["A. Grover", "A. Kapoor", "E. Horvitz"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 379\u2013386. ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Time series analysis, vol", "author": ["J.D. Hamilton"], "venue": "2. Princeton university press Princeton", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation 18(7), 1527\u20131554", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8), 1735\u20131780", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "A hybrid forecasting approach applied to wind speed time series", "author": ["J. Hu", "J. Wang", "G. Zeng"], "venue": "Renewable Energy 60, 185\u2013194", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast and robust fixed-point algorithms for independent component analysis", "author": ["A. Hyv\u00e4rinen"], "venue": "Neural Networks, IEEE Transactions on 10(3), 626\u2013634", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "Topographic independent component analysis", "author": ["A. Hyv\u00e4rinen", "P. Hoyer", "M. Inki"], "venue": "Neural computation 13(7), 1527\u20131558", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Independent component analysis: algorithms and applications", "author": ["A. Hyv\u00e4rinen", "E. Oja"], "venue": "Neural networks 13(4), 411\u2013430", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Human activity recognition using wearable sensors by deep convolutional neural networks", "author": ["W. Jiang", "Z. Yin"], "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia Conference. pp. 1307\u20131310. ACM", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Designing a neural network for forecasting financial and economic time series", "author": ["I. Kaastra", "M. Boyd"], "venue": "Neurocomputing 10(3), 215\u2013236", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1996}, {"title": "Application of time series and artificial neural network models in short-term forecasting of pv power generation", "author": ["E.G. Kardakos", "M.C. Alexiadis", "S. Vagropoulos", "C.K. Simoglou", "P.N. Biskas", "Bakirtzis", "A.G"], "venue": "Power Engineering Conference (UPEC), 2013 48th International Universities\u2019. pp. 1\u20136. IEEE", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "A novel hybridization of artificial neural networks and arima models for time series forecasting", "author": ["M. Khashei", "M. Bijari"], "venue": "Applied Soft Computing 11(2), 2664\u2013 2675", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural network architectures for robotic applications", "author": ["S.Y. King", "J.N. Hwang"], "venue": "Robotics and Automation, IEEE Transactions on 5(5), 641\u2013657", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1989}, {"title": "Time series forecasting using a deep belief network with restricted boltzmann machines", "author": ["T. Kuremoto", "S. Kimura", "K. Kobayashi", "M. Obayashi"], "venue": "Neurocomputing 137, 47\u201356", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling time-series with deep networks", "author": ["M. L\u00e4ngkvist"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Sleep stage classification using unsupervised feature learning", "author": ["M. L\u00e4ngkvist", "L. Karlsson", "A. Loutfi"], "venue": "Advances in Artificial Neural Systems 2012, 5", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11), 2278\u20132324", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1998}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "P. Pham", "Y. Largman", "A.Y. Ng"], "venue": "Advances in neural information processing systems. pp. 1096\u20131104", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Forecasting tourism demand using time series, artificial neural networks and multivariate adaptive regression splines: evidence from taiwan", "author": ["C.J. Lin", "H.F. Chen", "T.S. Lee"], "venue": "International Journal of Business Administration 2(2), p14", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Rotation-invariant similarity in time series using bag-ofpatterns representation", "author": ["J. Lin", "R. Khade", "Y. Li"], "venue": "Journal of Intelligent Information Systems 39(2), 287\u2013315", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural network modeling for big data weather forecasting", "author": ["J.N. Liu", "Y. Hu", "Y. He", "P.W. Chan", "L. Lai"], "venue": "Information Granularity, Big Data, and Computational Intelligence, pp. 389\u2013408. Springer", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural network based feature representation for weather forecasting", "author": ["J.N. Liu", "Y. Hu", "J.J. You", "P.W. Chan"], "venue": "Proceedings on the International Conference on Artificial Intelligence (ICAI). p. 1. The Steering Committee of The World Congress in Computer Science, Computer Engineering and Applied Computing (WorldComp)", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "arXiv preprint arXiv:1411.4038", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Traffic flow prediction with big data: a deep learning approach", "author": ["Y. Lv", "Y. Duan", "W. Kang", "Z. Li", "F.Y. Wang"], "venue": "Intelligent Transportation Systems, IEEE Transactions on 16(2), 865\u2013873", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp. 1033\u20131040", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "A logical calculus of the ideas immanent in nervous activity", "author": ["W.S. McCulloch", "W. Pitts"], "venue": "The bulletin of mathematical biophysics 5(4), 115\u2013133", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1943}, {"title": "Time-series modeling with undecimated fully convolutional neural networks", "author": ["R. Mittelman"], "venue": "arXiv preprint arXiv:1508.00317", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Tiled convolutional neural networks", "author": ["J. Ngiam", "Z. Chen", "D. Chia", "P.W. Koh", "Q.V. Le", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems. pp. 1279\u2013 1287", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural networks: a systematic introduction", "author": ["R. Rojas"], "venue": "Springer Science & Business Media", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "Time-series forecasting of indoor temperature using pre-trained deep neural networks", "author": ["P. Romeu", "F. Zamora-Mart\u0301\u0131nez", "P. Botella-Rocamora", "J. Pardo"], "venue": "Artificial Neural Networks and Machine Learning\u2013ICANN 2013, pp. 451\u2013458. Springer", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling 5, 3", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1988}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks 61, 85\u2013117", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Artificial neural networks for modeling time series of beach litter in the southern north sea", "author": ["M. Schulz", "M. Matthies"], "venue": "Marine environmental research 98, 14\u201320", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding convolutional neural networks", "author": ["D. Stutz"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "Fast and precise independent component analysis for high field fmri time series tailored using prior information on spatiotemporal structure", "author": ["K. Suzuki", "T. Kiryu", "T. Nakada"], "venue": "Human brain mapping 15(1), 54\u201366", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2002}, {"title": "Deep neural networks for object detection", "author": ["C. Szegedy", "A. Toshev", "D. Erhan"], "venue": "Advances in Neural Information Processing Systems. pp. 2553\u20132561", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "Practical issues in temporal difference learning", "author": ["G. Tesauro"], "venue": "Springer", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1992}, {"title": "Time series analysis using deep feed forward neural networks", "author": ["J.T. Turner"], "venue": "Ph.D. thesis, University of Maryland, Baltimore County", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "Encoding time series as images for visual inspection and classification using tiled convolutional neural networks", "author": ["Z. Wang", "T. Oates"], "venue": "Workshops at the TwentyNinth AAAI Conference on Artificial Intelligence", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2015}, {"title": "Time series classification using multi-channels deep convolutional neural networks", "author": ["Y. Zheng", "Q. Liu", "E. Chen", "Y. Ge", "J.L. Zhao"], "venue": "Web-Age Information Management, pp. 298\u2013310. Springer", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 44, "context": "Artificial Neural Networks (ANN), since their origin in 1943 [46], have been used to solve a large range of problems as diverse as robotic processing [32], object recognition [56], speech and handwriting recognition [19], and even real time sign-language translation [2].", "startOffset": 61, "endOffset": 65}, {"referenceID": 31, "context": "Artificial Neural Networks (ANN), since their origin in 1943 [46], have been used to solve a large range of problems as diverse as robotic processing [32], object recognition [56], speech and handwriting recognition [19], and even real time sign-language translation [2].", "startOffset": 150, "endOffset": 154}, {"referenceID": 54, "context": "Artificial Neural Networks (ANN), since their origin in 1943 [46], have been used to solve a large range of problems as diverse as robotic processing [32], object recognition [56], speech and handwriting recognition [19], and even real time sign-language translation [2].", "startOffset": 175, "endOffset": 179}, {"referenceID": 18, "context": "Artificial Neural Networks (ANN), since their origin in 1943 [46], have been used to solve a large range of problems as diverse as robotic processing [32], object recognition [56], speech and handwriting recognition [19], and even real time sign-language translation [2].", "startOffset": 216, "endOffset": 220}, {"referenceID": 1, "context": "Artificial Neural Networks (ANN), since their origin in 1943 [46], have been used to solve a large range of problems as diverse as robotic processing [32], object recognition [56], speech and handwriting recognition [19], and even real time sign-language translation [2].", "startOffset": 267, "endOffset": 270}, {"referenceID": 55, "context": "Despite the intuition that deeper architectures would yield better results than the then more commonly used shallow ones, empirical tests with deep networks had found similar or even worse results when compared to networks with only one or two layers [57] (for more details, see [6]).", "startOffset": 251, "endOffset": 255}, {"referenceID": 5, "context": "Despite the intuition that deeper architectures would yield better results than the then more commonly used shallow ones, empirical tests with deep networks had found similar or even worse results when compared to networks with only one or two layers [57] (for more details, see [6]).", "startOffset": 279, "endOffset": 282}, {"referenceID": 5, "context": "Additionally, training was found to be difficult and often inefficient [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 33, "context": "L\u00e4ngkvist [34] argues that this scenario started to change with the proposal of greedy layer-wise unsupervised learning [22], which allowed for the fast learning of Deep Belief Networks, while also solving the vanishing gradients problem [7].", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "L\u00e4ngkvist [34] argues that this scenario started to change with the proposal of greedy layer-wise unsupervised learning [22], which allowed for the fast learning of Deep Belief Networks, while also solving the vanishing gradients problem [7].", "startOffset": 120, "endOffset": 124}, {"referenceID": 6, "context": "L\u00e4ngkvist [34] argues that this scenario started to change with the proposal of greedy layer-wise unsupervised learning [22], which allowed for the fast learning of Deep Belief Networks, while also solving the vanishing gradients problem [7].", "startOffset": 238, "endOffset": 241}, {"referenceID": 13, "context": "These kinds of problems are addressed in the literature by a range of different approches (for a recent review of the main techniques applied to perform tasks such as Classification, Segmentation, Anomaly Detection and Prediction, see [14]).", "startOffset": 235, "endOffset": 239}, {"referenceID": 47, "context": "The reader is referred to [49] for a thorough description of architectural alternatives such as Restricted Boltzmann Machines (RBM), Hopfield Networks and Auto-Encoders, as well as for a detailed explanation of the Backpropagation algorithm.", "startOffset": 26, "endOffset": 30}, {"referenceID": 18, "context": "Additionally, we refer the reader to [19] for applications of RNN as well as more details on the implementation of a LSTM, and to [54] for details on CNN.", "startOffset": 37, "endOffset": 41}, {"referenceID": 52, "context": "Additionally, we refer the reader to [19] for applications of RNN as well as more details on the implementation of a LSTM, and to [54] for details on CNN.", "startOffset": 130, "endOffset": 134}, {"referenceID": 18, "context": "Each node of a layer is connected to all nodes of the next layer; (c) A LSTM block (adapted from [19]).", "startOffset": 97, "endOffset": 101}, {"referenceID": 45, "context": "2: (a) The proposed \u201cStacked Architecture\u201d for performing Anomaly Detection (adapted from [44]); (b) The architecture of a UFCNN (adapted from [47]).", "startOffset": 143, "endOffset": 147}, {"referenceID": 49, "context": "A popular learning algorithm is the Backpropagation algorithm [51], whereby the gradient of an error function is calculated and the weights are iteratively set so as to minimize the error.", "startOffset": 62, "endOffset": 66}, {"referenceID": 35, "context": "Also trained with the Backpropagation algorithm, CNNs [36] are common for image processing tasks and reduce the number of parameters to be learned by limiting the number of connections of the neurons in the hidden layer to only some of the input neurons (i.", "startOffset": 54, "endOffset": 58}, {"referenceID": 46, "context": "However, \u201cit prevents the pooling units from capturing more complex invariances, such as scale and rotation invariance\u201d [48].", "startOffset": 120, "endOffset": 124}, {"referenceID": 41, "context": "FCNs [42] allow for the input and output layers to have the same dimensions by introducing \u201ca decoder stage that is consisted of upsampling, convolution, and rectified linear units layers, to the CNN architecture\u201d [47].", "startOffset": 5, "endOffset": 9}, {"referenceID": 45, "context": "FCNs [42] allow for the input and output layers to have the same dimensions by introducing \u201ca decoder stage that is consisted of upsampling, convolution, and rectified linear units layers, to the CNN architecture\u201d [47].", "startOffset": 214, "endOffset": 218}, {"referenceID": 49, "context": "It is possible to adapt the Backpropagation algorithm to train a recurrent network, by \u201cunfolding\u201d the network through time and constraining some of the connections to always hold the same weights [51].", "startOffset": 197, "endOffset": 201}, {"referenceID": 6, "context": "This is called the vanishing gradients problem [7].", "startOffset": 47, "endOffset": 50}, {"referenceID": 22, "context": "A type of network architecture that solves this problem is the LSTM [23].", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "Traditional techniques on manipulating such data can be found in [21], and the application of traditional ANN techniques on this kind of data is described in [5].", "startOffset": 65, "endOffset": 69}, {"referenceID": 4, "context": "Traditional techniques on manipulating such data can be found in [21], and the application of traditional ANN techniques on this kind of data is described in [5].", "startOffset": 158, "endOffset": 161}, {"referenceID": 8, "context": "As an early attempt on using ANN for such tasks, [9] modelled flour prices over the range of 8 years.", "startOffset": 49, "endOffset": 52}, {"referenceID": 28, "context": "Still in the 90\u2019s, [29] delineated eight steps on \u201cdesigning a neural network forecast model using economic time series data\u201d.", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "More recent approaches include usage of Elman RNNs to predict chaotic TimeSeries [10], employing ANN ensemble methods for forecasting Internet traffic [12], using simple Multilayer Perceptrons for modeling the amount of littering in the North Sea [53], and implementing in FPGA a prediction algorithm using Echo State Networks for \u201cexploiting the inherent parallelism of these systems\u201d [3].", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "More recent approaches include usage of Elman RNNs to predict chaotic TimeSeries [10], employing ANN ensemble methods for forecasting Internet traffic [12], using simple Multilayer Perceptrons for modeling the amount of littering in the North Sea [53], and implementing in FPGA a prediction algorithm using Echo State Networks for \u201cexploiting the inherent parallelism of these systems\u201d [3].", "startOffset": 151, "endOffset": 155}, {"referenceID": 51, "context": "More recent approaches include usage of Elman RNNs to predict chaotic TimeSeries [10], employing ANN ensemble methods for forecasting Internet traffic [12], using simple Multilayer Perceptrons for modeling the amount of littering in the North Sea [53], and implementing in FPGA a prediction algorithm using Echo State Networks for \u201cexploiting the inherent parallelism of these systems\u201d [3].", "startOffset": 247, "endOffset": 251}, {"referenceID": 2, "context": "More recent approaches include usage of Elman RNNs to predict chaotic TimeSeries [10], employing ANN ensemble methods for forecasting Internet traffic [12], using simple Multilayer Perceptrons for modeling the amount of littering in the North Sea [53], and implementing in FPGA a prediction algorithm using Echo State Networks for \u201cexploiting the inherent parallelism of these systems\u201d [3].", "startOffset": 386, "endOffset": 389}, {"referenceID": 30, "context": "[31] presents a model for Time-Series forecasting using ANN and ARIMA models, and [15] applies the same kinds of models to water quality time series prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[31] presents a model for Time-Series forecasting using ANN and ARIMA models, and [15] applies the same kinds of models to water quality time series prediction.", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "In still other examples of the same ideas, [30] compares the performance of ARIMA models and ANNs to make short-term predictions on photovoltaic power generators, while [38] compares both models with the performance of Multivariate Adaptive Regression Splines.", "startOffset": 43, "endOffset": 47}, {"referenceID": 37, "context": "In still other examples of the same ideas, [30] compares the performance of ARIMA models and ANNs to make short-term predictions on photovoltaic power generators, while [38] compares both models with the performance of Multivariate Adaptive Regression Splines.", "startOffset": 169, "endOffset": 173}, {"referenceID": 12, "context": "[13] performs Time-Series forecasting by using a hybrid fuzzy model: while the Fuzzy C-means method is utilized for fuzzification, ANN are employed for defuzzification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Finally, [24] forecasts the speed of the wind using a hybrid of Support Vector Machines, Ensemble Empirical Mode Decomposition and Partial Autocorrelation Function.", "startOffset": 9, "endOffset": 13}, {"referenceID": 50, "context": "A very thorough review of the entire history of developments that led the field to its current state can be found in [52], while a higher focus on the novelties from the last decade is given in [4].", "startOffset": 117, "endOffset": 121}, {"referenceID": 3, "context": "A very thorough review of the entire history of developments that led the field to its current state can be found in [52], while a higher focus on the novelties from the last decade is given in [4].", "startOffset": 194, "endOffset": 197}, {"referenceID": 33, "context": "The advantage of CNNs is that they can learn such features by themselves, reducing the need for human experts [34].", "startOffset": 110, "endOffset": 114}, {"referenceID": 36, "context": "An example of the application of such unsupervised feature learning for the classification of audio signals is presented in [37].", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "In [1], the features learned by the CNN are used as input to a Hidden Markov Model, achieving a drop at the error rate of over 10%.", "startOffset": 3, "endOffset": 6}, {"referenceID": 58, "context": "An architecture that solves this constraint is presented in [60].", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "In [18] the performance of CNNs is compared with that of LSTM for the classification of Visual and Haptic Data in a robotics setting, and in [28] the signals produced by wearable sensors are transformed into images so that Deep CNNs can be used for classification.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "In [18] the performance of CNNs is compared with that of LSTM for the classification of Visual and Haptic Data in a robotics setting, and in [28] the signals produced by wearable sensors are transformed into images so that Deep CNNs can be used for classification.", "startOffset": 141, "endOffset": 145}, {"referenceID": 26, "context": "Relevant to Tiled CNNs was the development of Independent Component Analysis (ICA) [27].", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": ", [17], [55] or [25]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 53, "context": ", [17], [55] or [25]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": ", [17], [55] or [25]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "Tiled CNNs are normally trained with a variation of such technique that looses the assumption that each component is statistically independent and tries to find a topographic order between them: the Topographic ICA [26].", "startOffset": 215, "endOffset": 219}, {"referenceID": 32, "context": "For example, Deep Belief Networks are used in the work of [33] along with RBM.", "startOffset": 58, "endOffset": 62}, {"referenceID": 56, "context": "[58] also compares the performance of Deep Belief Networks with that of Stacked Denoising Autoencoders.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "This last type of network is also employed by [50] to predict the temperature of an indoor environment.", "startOffset": 46, "endOffset": 50}, {"referenceID": 42, "context": "Another application of Time-Series forecasting can be found in [43], which uses Stacked Autoencoders to predict the flow of traffic from a Big Data dataset.", "startOffset": 63, "endOffset": 67}, {"referenceID": 40, "context": "In [41], some preliminary predictions on weather data provided by The Hong Kong Observatory are made through the usage of Stacked Autoencoders.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "In a follow up work, the authors use similar ideas to perform predictions on Big Data [40].", "startOffset": 86, "endOffset": 90}, {"referenceID": 19, "context": "Instead of Autoencoders, [20] uses Deep Belief Networks for constructing a hybrid model in which the ANN models the joint distribution between the weather predictors variables.", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "It is still difficult to find works such as [16], that uses Stacked Denoising Autoencoders to perform Anomaly Detection of trajectories obtained from low level tracking algorithms.", "startOffset": 44, "endOffset": 48}, {"referenceID": 34, "context": "For example, identifying an anomaly could be transformed into a Classification task, as was done in [35].", "startOffset": 100, "endOffset": 104}, {"referenceID": 45, "context": "In this section the work presented in [47] is reviewed.", "startOffset": 38, "endOffset": 42}, {"referenceID": 45, "context": "Mittelman [47] argues that the architecture of the FCN resembles the application of a wavelet transform, and that for this reason, it can present strong variations when the input signal is subject to small translations.", "startOffset": 10, "endOffset": 14}, {"referenceID": 43, "context": "In both cases, the UFCNN outperforms the competing networks: a RNN, a Hessian-Free optimization-RNN [45], and a LSTM.", "startOffset": 100, "endOffset": 104}, {"referenceID": 57, "context": "Wang and Oates [59] presented an approach for Time-Series Classification using CNN-like networks.", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "The creation of the Markov Transition Field is based on the ideas proposed in [8] for the definition of the so-called Markov Transition Matrix (MTM).", "startOffset": 78, "endOffset": 81}, {"referenceID": 38, "context": "1 with the state-of-theart SAX-BoP approach\u201d [39], which are 50Words, Adiac, Beef, Coffee, ECG200, Face (all), Lightning-2, Lightning-7, OliveOil, OSU Leaf, Swedish Leaf and Yoga [11].", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "1 with the state-of-theart SAX-BoP approach\u201d [39], which are 50Words, Adiac, Beef, Coffee, ECG200, Face (all), Lightning-2, Lightning-7, OliveOil, OSU Leaf, Swedish Leaf and Yoga [11].", "startOffset": 179, "endOffset": 183}], "year": 2017, "abstractText": "In many real-world application, e.g., speech recognition or sleep stage classification, data are captured over the course of time, constituting a Time-Series. Time-Series often contain temporal dependencies that cause two otherwise identical points of time to belong to different classes or predict different behavior. This characteristic generally increases the difficulty of analysing them. Existing techniques often depended on hand-crafted features that were expensive to create and required expert knowledge of the field. With the advent of Deep Learning new models of unsupervised learning of features for Time-series analysis and forecast have been developed. Such new developments are the topic of this paper: a review of the main Deep Learning techniques is presented, and some applications on Time-Series analysis are summaried. The results make it clear that Deep Learning has a lot to contribute to the field.", "creator": "LaTeX with hyperref package"}}}