{"id": "1701.02185", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2017", "title": "Crowdsourcing Ground Truth for Medical Relation Extraction", "abstract": "Cognitive computing systems require human labeled data for evaluation, and often for training. The standard practice used in gathering this data minimizes disagreement between annotators, and we have found this results in data that fails to account for the ambiguity inherent in language. We have proposed the CrowdTruth method for collecting ground truth through crowdsourcing, that reconsiders the role of people in machine learning based on the observation that disagreement between annotators provides a useful signal for phenomena such as ambiguity in the text. We report on using this method to build an annotated data set for medical relation extraction for the $cause$ and $treat$ relations, and how this data performed in a supervised training experiment. We demonstrate that by modeling ambiguity, labeled data gathered from crowd workers can (1) reach the level of quality of domain experts for this task while reducing the cost, and (2) provide better training data at scale than distant supervision. We further propose and validate new weighted measures for precision, recall, and F-measure, that account for ambiguity in both human and machine performance on this task.", "histories": [["v1", "Mon, 9 Jan 2017 14:13:23 GMT  (1584kb,D)", "http://arxiv.org/abs/1701.02185v1", "In review for ACM Transactions on Interactive Intelligent Systems (TiiS) Special Issue on Human-Centered Machine Learning"], ["v2", "Tue, 3 Oct 2017 15:04:43 GMT  (1605kb,D)", "http://arxiv.org/abs/1701.02185v2", "Accepted for publication in ACM Transactions on Interactive Intelligent Systems (TiiS) Special Issue on Human-Centered Machine Learning"]], "COMMENTS": "In review for ACM Transactions on Interactive Intelligent Systems (TiiS) Special Issue on Human-Centered Machine Learning", "reviews": [], "SUBJECTS": "cs.CL cs.HC", "authors": ["anca dumitrache", "lora aroyo", "chris welty"], "accepted": false, "id": "1701.02185"}, "pdf": {"name": "1701.02185.pdf", "metadata": {"source": "CRF", "title": "A Crowdsourcing Ground Truth for Medical Relation Extraction", "authors": ["ANCA DUMITRACHE", "LORA AROYO", "CHRIS WELTY"], "emails": [], "sections": [{"heading": null, "text": "A Crowdsourcing Ground Truth for Medical Relation ExtractionANCA DUMITRACHE, Vrije Universiteit Amsterdam, CAS IBM Netherlands LORA AROYO, Vrije Universiteit Amsterdam CHRIS WELTY, Google ResearchCognitive Computing systems require human labeled data for evaluation, and often for training. Standard practice in collecting this data minimizes discrepancies between annotators, and we have found that this results in data that does not take into account the ambiguity of inherent language. We have proposed the CrowdTruth method for collecting basic truths through crowdsourcing, which relies on observing that discrepancies between ancies between anotators provide a useful signal for phenomena such as ambiguity in text. We report on the use of this method to gather an annotated amount of data for medical relations."}, {"heading": "1. INTRODUCTION", "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own."}, {"heading": "2. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Medical crowdsourcing", "text": "[Mortensen et al. 2013] use crowdsourcing to verify relationship hierarchies in biomedical ontologies. In 14 relationships from the SNOMED CT CORE Problem List Subset, the authors report on the accuracy of crowdsourcing relationships in medical literature analysis and in the Retrieval System Online (MEDLINE) abstracts. In the field of biomedical NLP, [Burger et al. 2012] report a weighted accuracy of 82% in a corpus of 250 MEDLINE abstracts. Finally, [Li et al. 2015] conducted a study that uncovered ambiguities in a gold standard for relationships between drug diseases and crowdsourcing. They found that across a corpus of 60 judgments, a medical match between crowdsourcing approaches and similar approaches was conducted in a comprehensive clinical study called crowdsourcing."}, {"heading": "2.2. Crowdsourcing ground truth", "text": "Crowdsourcing Ground Truth has shown promising results in a variety of other areas. [Snow et al. 2008] have shown that aggregating the responses of an increasing number of unskilled majority-voting crowdworkers can lead to high-quality NLP training data. [Hovy et al. 2014] has compared crowdsourcing to experts for the task of part-of-speech tagging, and the authors also show that models based on crowdsourcing annotations can work just as well as expert-trained models. [Kondreddi et al. 2014] has examined crowdsourcing for relationship extraction in the general field, comparing its efficiency to that of fully automated information extraction approaches. TheirACM Transactions on Interactive Intelligent Systems, Vol. V, No. N, Pub. date: January YYY. The results showed that crowdsourcing was particularly suitable for identifying subtle formulations of relationships that do not appear frequently enough to be picked up by statistical methods."}, {"heading": "2.3. Disagreement and ambiguity in crowdsourcing", "text": "In addition to our own work [Aroyo and Welty 2013a], the role of ambiguity in building a gold standard has already been discussed by [Lau et al. 2014] The authors suggest a method of crowdsourcing ambiguity in the grammatical correctness of text by giving workers the ability to select different degrees of correctness, but disagreement between commentators is not discussed as a factor in measuring this ambiguity. Having empirically examined subtext datasets in 2014, Plank et al. found that disagreement between commentators is uniform across domains, even across languages. Moreover, most disagreement is an indication of questionable cases in linguistic theory rather than faulty annotation. We believe that these results manifest even more strongly for NLP tasks where semantic ambiguity plays a role, such as in decoding relationships."}, {"heading": "3. EXPERIMENTAL SETUP", "text": "The goal of our experiments is to evaluate the quality of our non-consensual crowdsourcing data when training a model for extracting medical relationships. We use a binary classifier [Wang and Fan 2014] that uses a set of sentences and two terms from the sentence as input, and provides a score that reflects the likelihood that a particular relationship between the terms is expressed in the sentence. This multi-layered learning classifier was one of the first to accept weighted values for each training instance, although it still requires a discreet positive or negative label. This property seemed to make it suitable for our experiments, as we expected that the ambiguity of a sentence would affect its suitability as a training instance (in other words, we reduced the weight of training cases that had ambiguity). We are investigating the performance of the classifier through two medi-ACM transactions on interactive intelligent systems, Volume V, N, Article A, Pub."}, {"heading": "3.1. Data selection", "text": "The mentioned for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five. \"The mentioned for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five for the five."}, {"heading": "3.2. CrowdTruth metrics", "text": "In recent years it has become clear that the problem is not only a problem, but also a problem that cannot be solved, \"he told the German Press Agency.\" The problem is that it cannot be solved, \"he said.\" But it is not that we can solve it. \"He added:\" It is not that we have to solve it. \"He stressed:\" It is not that we have to. \"He added:\" But it is not that we have to manage it. \""}, {"heading": "3.3. Training the model", "text": "The sentences together with the relations annotations were then used to form a varied model for the relation extraction [Wang and Fan 2014]. This model was developed for the medical field and tested for the relation we use. It is trained for each individual relation, bringing in both positive and negative data. It provides support for the performance of crowddatasets, with positive values in (0, 1] and negative values in [\u2212 1, 0]. With this system, we will train several models for five-fold validation to evaluate the performance of crowddatasets. Training was carried out separately for the treatment and cause of relationships."}, {"heading": "3.4. Evaluation data", "text": "To make a meaningful comparison between the crowd and the expert model, the assessment set must be carefully reviewed. If both the expert and the crowd agree that a sentence is either a positive or a negative example, we can automatically use it as part of the test set. Such a sentence has been labeled the Expert Scale. Interesting cases occur when crowd and expert disagree. To ensure a fair comparison, our team decided that each of them can decide whether the relationship exists in the sentence or not. Sentences where no decision could be made were subsequently removed from the assessment, which consisted of confusing and ambiguous judgments that our team could not agree on, often because there was a lexical problem with the sentence (such as enumerations, term tension issues, etc.)."}, {"heading": "3.5. CrowdTruth-weighted evaluation", "text": "We have also studied how to include CrowdTruth in the evaluation process. The rationale of our approach is that the ambiguity of a sentence should also be taken into account in the evaluation - i.e. sentences that do not express a unique relationship should not count as much as clear sentences. In this case, the sentence-relation score provides a real value that measures the degree to which a particular sentence expresses a specific relationship between two terms. Therefore, we propose a series of evaluation metrics weighted using the sentence-relation score for a particular relationship. TheACM Transactions on Interactive Intelligent Systems, Vol. V, No. N, Article A, Pub. date: January YYY.Metrics have previously been weighted using a subset of our basic truth data as described in [Dumitrache et al. 2015a]. We collect true and false positives and negatives by default, so that tp (s) = 1 iff precision is a true, positive and weighted in a similar manner."}, {"heading": "4. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. CrowdTruth vs. medical experts", "text": "In this context, it should be noted that the solution to problems that have arisen in the past is not a solution, but a solution that is a solution."}, {"heading": "4.2. CrowdTruth vs. distant supervision", "text": "We did not have enough time with the experts to collect a larger dataset from them, but the quantity is always available, so after we found that the performance of the quantity matches with the medical experts, we expanded the experiments to 3,984 sets. The quantity dataset in this experiment used a fixed set-relation score threshold equal to 0.5, as this is the value where the mass in the previous experiment performed best, for both of the relationships. As in the previous experiment, we used a fivefold cross-validation to train the model. The test sets were kept the same as in the previous experiment, using the test partition labels as the gold standard. The ACM transactions on interactive intelligent systems, Vol. V, No. N, Article A, Pub. Date: January YYYY. The goal was to compare the quantity with the remote supervision baseline while scaling the number of training examples."}, {"heading": "5. DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. CrowdTruth vs. medical experts", "text": "It's the way it is, that it's able to put itself at the top, \"he said.\" It's the way it is, \"he said.\" It's the way it is, \"he said.\" It's the way it is, \"he said."}, {"heading": "5.2. CrowdTruth vs. distant supervision", "text": "The results for both relationships (Fig.9 & Fig.10) show that the quantity stabilized the remote supervision baseline after the learning curves, thus justifying its cost in terms of money. Consequently, we conclude that not only does the mass produce higher quality data than the automated baseline, but the formation of the model with weights, as opposed to binary labels, has a positive impact on the performance of the model. CrowdTruth weighted F1 results \"consistently exceed simple F1, both for baseline and for crowd across both relationships, consolidating our assumption that ambiguity has an impact on classification performance, and the weighting of test data can be taken into account with ambiguity."}, {"heading": "6. CONCLUSION", "text": "We propose the CrowdTruth method of collecting basic truths through crowdsourcing, which is rethinking the role of humans in machine learning. In this work, we used CrowdTruth to create a gold standard of 3,984 sentences for extracting medical relationships, focusing on the cause and treatment relationships, and using crowd data to train a classification model. We have shown that mass works just as well in processing ambiguity as medical experts in terms of the quality and effectiveness of annotations, while they are cheaper and more readily available."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Dr. Chang Wang for his assistance in using the Medical Relationship Extraction Classifier and Anthony Levas for his help in collecting the expert comments. The authors, Dr. Wang and Mr. Levas, were all employees of IBM Research when the expert data collection was conducted, and we are grateful to IBM for subsequently making the data freely available."}], "references": [{"title": "Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program", "author": ["Alan R Aronson"], "venue": "In Proceedings of the AMIA Symposium. American Medical Informatics Association,", "citeRegEx": "Aronson.,? \\Q2001\\E", "shortCiteRegEx": "Aronson.", "year": 2001}, {"title": "Crowd Truth: Harnessing disagreement in crowdsourcing a relation extraction gold standard", "author": ["Lora Aroyo", "Chris Welty."], "venue": "Web Science 2013. ACM (2013).", "citeRegEx": "Aroyo and Welty.,? 2013a", "shortCiteRegEx": "Aroyo and Welty.", "year": 2013}, {"title": "Measuring crowd truth for medical relation extraction", "author": ["Lora Aroyo", "Chris Welty."], "venue": "AAAI 2013 Fall Symposium on Semantics for Big Data.", "citeRegEx": "Aroyo and Welty.,? 2013b", "shortCiteRegEx": "Aroyo and Welty.", "year": 2013}, {"title": "The Three Sides of CrowdTruth", "author": ["Lora Aroyo", "Chris Welty."], "venue": "Journal of Human Computation 1 (2014), 31\u201334. Issue 1. DOI:http://dx.doi.org/10.15346/hc.v1i1.3", "citeRegEx": "Aroyo and Welty.,? 2014", "shortCiteRegEx": "Aroyo and Welty.", "year": 2014}, {"title": "What Determines Inter-coder Agreement in Manual Annotations? A Meta-analytic Investigation", "author": ["Petra Saskia Bayerl", "Karsten Ingmar Paul."], "venue": "Comput. Linguist. 37, 4 (Dec. 2011), 699\u2013725. DOI:http://dx.doi.org/10.1162/COLI a 00074", "citeRegEx": "Bayerl and Paul.,? 2011", "shortCiteRegEx": "Bayerl and Paul.", "year": 2011}, {"title": "The unified medical language system (UMLS): integrating biomedical terminology", "author": ["Olivier Bodenreider."], "venue": "Nucleic acids research 32, suppl 1 (2004), D267\u2013D270.", "citeRegEx": "Bodenreider.,? 2004", "shortCiteRegEx": "Bodenreider.", "year": 2004}, {"title": "Validating candidate gene-mutation relations in MEDLINE abstracts via crowdsourcing", "author": ["John D Burger", "Emily Doughty", "Sam Bayer", "David Tresner-Kirsch", "Ben Wellner", "John Aberdeen", "Kyungjoon Lee", "Maricel G Kann", "Lynette Hirschman."], "venue": "Data Integration in the Life Sciences. Springer, 83\u201391.", "citeRegEx": "Burger et al\\.,? 2012", "shortCiteRegEx": "Burger et al\\.", "year": 2012}, {"title": "Overcoming barriers to NLP for clinical text: the role of shared tasks and the need for additional creative solutions", "author": ["Wendy W Chapman", "Prakash M Nadkarni", "Lynette Hirschman", "Leonard W D\u2019Avolio", "Guergana K Savova", "Ozlem Uzuner"], "venue": "Journal of the American Medical Informatics Association 18,", "citeRegEx": "Chapman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapman et al\\.", "year": 2011}, {"title": "Conference v2", "author": ["Michelle Cheatham", "Pascal Hitzler."], "venue": "0: An uncertain version of the OAEI Conference benchmark. In The Semantic Web\u2013ISWC 2014. Springer, 33\u201348.", "citeRegEx": "Cheatham and Hitzler.,? 2014", "shortCiteRegEx": "Cheatham and Hitzler.", "year": 2014}, {"title": "Building a persistent workforce on mechanical turk for multilingual data collection", "author": ["David L Chen", "William B Dolan."], "venue": "Proceedings of The 3rd Human Computation Workshop (HCOMP 2011).", "citeRegEx": "Chen and Dolan.,? 2011", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Cascade: crowdsourcing taxonomy creation", "author": ["Lydia B. Chilton", "Greg Little", "Darren Edge", "Daniel S. Weld", "James A. Landay."], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI \u201913). ACM, New York, NY, USA, 1999\u20132008. DOI:http://dx.doi.org/10.1145/2470654.2466265", "citeRegEx": "Chilton et al\\.,? 2013", "shortCiteRegEx": "Chilton et al\\.", "year": 2013}, {"title": "Achieving Expert-Level Annotation Quality with CrowdTruth: the Case of Medical Relation Extraction", "author": ["Anca Dumitrache", "Lora Aroyo", "Chris Welty."], "venue": "Proceedings of Biomedical Data Mining, Modeling, and Semantic Integration (BDM2I) Workshop, International Semantic Web Conference (ISWC) 2015.", "citeRegEx": "Dumitrache et al\\.,? 2015a", "shortCiteRegEx": "Dumitrache et al\\.", "year": 2015}, {"title": "CrowdTruth Measures for Language Ambiguity: the Case of Medical Relation Extraction", "author": ["Anca Dumitrache", "Lora Aroyo", "Chris Welty."], "venue": "Proceedings of Linked data for Information Extraction (LD4IE) Workshop, International Semantic Web Conference (ISWC) 2015.", "citeRegEx": "Dumitrache et al\\.,? 2015b", "shortCiteRegEx": "Dumitrache et al\\.", "year": 2015}, {"title": "Annotating named entities in Twitter data with crowdsourcing", "author": ["Tim Finin", "Will Murnane", "Anand Karandikar", "Nicholas Keller", "Justin Martineau", "Mark Dredze."], "venue": "In Proc. NAACL HLT (CSLDAMT \u201910). Association for Computational Linguistics, 80\u201388.", "citeRegEx": "Finin et al\\.,? 2010", "shortCiteRegEx": "Finin et al\\.", "year": 2010}, {"title": "Experiments with crowdsourced re-annotation of a POS tagging data set", "author": ["Dirk Hovy", "Barbara Plank", "Anders S\u00f8gaard."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, Baltimore, Maryland, 377\u2013382.", "citeRegEx": "Hovy et al\\.,? 2014", "shortCiteRegEx": "Hovy et al\\.", "year": 2014}, {"title": "CrowdTruth: Machine-Human Computation Framework for Harnessing Disagreement in Gathering Annotated Data", "author": ["Oana Inel", "Khalid Khamkham", "Tatiana Cristea", "Anca Dumitrache", "Arne Rutjes", "Jelle van der Ploeg", "Lukasz Romaszko", "Lora Aroyo", "Robert-Jan Sips."], "venue": "The Semantic Web\u2013ISWC 2014. Springer, 486\u2013504.", "citeRegEx": "Inel et al\\.,? 2014", "shortCiteRegEx": "Inel et al\\.", "year": 2014}, {"title": "Combining information extraction and human computing for crowdsourced knowledge acquisition", "author": ["Sarath Kumar Kondreddi", "Peter Triantafillou", "Gerhard Weikum."], "venue": "30th International Conference on Data Engineering. IEEE, 988\u2013999.", "citeRegEx": "Kondreddi et al\\.,? 2014", "shortCiteRegEx": "Kondreddi et al\\.", "year": 2014}, {"title": "Measuring gradience in speakers grammaticality judgements", "author": ["Jey Han Lau", "Alexander Clark", "Shalom Lappin."], "venue": "Proceedings of the 36th Annual Conference of the Cognitive Science Society. 821\u2013826.", "citeRegEx": "Lau et al\\.,? 2014", "shortCiteRegEx": "Lau et al\\.", "year": 2014}, {"title": "Hybrid entity clustering using crowds and data", "author": ["Jongwuk Lee", "Hyunsouk Cho", "Jin-Woo Park", "Young-rok Cha", "Seung-won Hwang", "Zaiqing Nie", "Ji-Rong Wen."], "venue": "The VLDB Journal 22, 5 (2013), 711\u2013726. DOI:http://dx.doi.org/10.1007/s00778-013-0328-8", "citeRegEx": "Lee et al\\.,? 2013", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Exposing ambiguities in a relation-extraction gold standard with crowdsourcing", "author": ["Tong Shu Li", "Benjamin M Good", "Andrew I Su."], "venue": "arXiv preprint arXiv:1505.06256 (2015).", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Note on the sampling error of the difference between correlated proportions or percentages", "author": ["Quinn McNemar."], "venue": "Psychometrika 12, 2 (1947), 153\u2013157.", "citeRegEx": "McNemar.,? 1947", "shortCiteRegEx": "McNemar.", "year": 1947}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2. Association for Computational Linguistics, 1003\u20131011.", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Crowdsourcing the verification of relationships in biomedical ontologies", "author": ["Jonathan M Mortensen", "Mark A Musen", "Natalya F Noy."], "venue": "AMIA Annual Symposium Proceedings. American Medical Informatics Association, 1020.", "citeRegEx": "Mortensen et al\\.,? 2013", "shortCiteRegEx": "Mortensen et al\\.", "year": 2013}, {"title": "The meaning of meaning", "author": ["Charles Kay Ogden", "I.A. Richards."], "venue": "Trubner & Co, London.", "citeRegEx": "Ogden and Richards.,? 1923", "shortCiteRegEx": "Ogden and Richards.", "year": 1923}, {"title": "Linguistically debatable or just plain wrong", "author": ["Barbara Plank", "Dirk Hovy", "Anders S\u00f8gaard."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, Baltimore, Maryland, 507\u2013511.", "citeRegEx": "Plank et al\\.,? 2014", "shortCiteRegEx": "Plank et al\\.", "year": 2014}, {"title": "Resolvable vs", "author": ["Mike Schaekermann", "Edith Law", "Alex C. Williams", "William Callaghan."], "venue": "Irresolvable Ambiguity: A New Hybrid Framework for Dealing with Uncertain Ground Truth. In 1st Workshop on Human-Centered Machine Learning at SIGCHI 2016.", "citeRegEx": "Schaekermann et al\\.,? 2016", "shortCiteRegEx": "Schaekermann et al\\.", "year": 2016}, {"title": "Cheap and Fast\u2014but is It Good?: Evaluating Non-expert Annotations for Natural Language Tasks", "author": ["Rion Snow", "Brendan O\u2019Connor", "Daniel Jurafsky", "Andrew Y. Ng"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP \u201908)", "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "Medical Relation Extraction with Manifold Models", "author": ["Chang Wang", "James Fan."], "venue": "52nd Annual Meeting of the ACL, vol. 1. Association for Computational Linguistics, 828\u2013838.", "citeRegEx": "Wang and Fan.,? 2014", "shortCiteRegEx": "Wang and Fan.", "year": 2014}, {"title": "The multidimensional wisdom of crowds", "author": ["Peter Welinder", "Steve Branson", "Pietro Perona", "Serge J Belongie."], "venue": "Advances in neural information processing systems. 2424\u20132432.", "citeRegEx": "Welinder et al\\.,? 2010", "shortCiteRegEx": "Welinder et al\\.", "year": 2010}, {"title": "Large Scale Relation Detection", "author": ["Chris Welty", "James Fan", "David Gondek", "Andrew Schlaikjer."], "venue": "Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading (FAM-LbR \u201910). Association for Computational Linguistics, Stroudsburg, PA, USA, 24\u201333. http://dl.acm.org/citation.cfm?id=1866775.1866779", "citeRegEx": "Welty et al\\.,? 2010", "shortCiteRegEx": "Welty et al\\.", "year": 2010}, {"title": "Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise", "author": ["Jacob Whitehill", "Ting fan Wu", "Jacob Bergsma", "Javier R. Movellan", "Paul L. Ruvolo."], "venue": "Advances in Neural Information Processing Systems 22, Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta (Eds.). Curran Associates, Inc., 2035\u20132043. http://papers.nips.cc/paper/ 3644-whose-vote-should-count-more-optimal-integration-of-labels-from-labelers-of-unknown-expertise.", "citeRegEx": "Whitehill et al\\.,? 2009", "shortCiteRegEx": "Whitehill et al\\.", "year": 2009}, {"title": "Web 2.0-based crowdsourcing for high-quality gold standard development in clinical natural language processing", "author": ["Haijun Zhai", "Todd Lingren", "Louise Deleger", "Qi Li", "Megan Kaiser", "Laura Stoutenborough", "Imre Solti"], "venue": "JMIR 15,", "citeRegEx": "Zhai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhai et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "Note to reviewers: The parts of this paper describing the background, motivation, experimental methodology, and some of the experimental results (comparing the crowdsourced cause relation with expert annotation over 902 sentences), have been previously published in workshop papers [Dumitrache et al. 2015a; 2015b].", "startOffset": 282, "endOffset": 314}, {"referenceID": 7, "context": "The lack of annotated datasets for training and benchmarking is considered one of the big challenges of clinical NLP [Chapman et al. 2011].", "startOffset": 117, "endOffset": 138}, {"referenceID": 25, "context": "However, [Schaekermann et al. 2016] and [Bayerl and Paul 2011] criticize this approach by investigating the role of inter-annotator disagreement as a possible indicator of ambiguity inherent in text.", "startOffset": 9, "endOffset": 35}, {"referenceID": 21, "context": "Specifically, in the case of relation extraction from natural text, distant supervision [Mintz et al. 2009; Welty et al. 2010] is a well-established semi-supervised method that uses pairs of entities known to form a relation (e.", "startOffset": 88, "endOffset": 126}, {"referenceID": 29, "context": "Specifically, in the case of relation extraction from natural text, distant supervision [Mintz et al. 2009; Welty et al. 2010] is a well-established semi-supervised method that uses pairs of entities known to form a relation (e.", "startOffset": 88, "endOffset": 126}, {"referenceID": 22, "context": "[Mortensen et al. 2013] use crowdsourcing to verify relation hierarchies in biomedical ontologies.", "startOffset": 0, "endOffset": 23}, {"referenceID": 6, "context": "In the field of Biomedical NLP, [Burger et al. 2012] used crowdsourcing to extract the gene-mutation relations in Medical Literature Analysis and Retrieval System Online (MEDLINE) abstracts.", "startOffset": 32, "endOffset": 52}, {"referenceID": 19, "context": "Finally, [Li et al. 2015] performed a study exposing ambiguities in a gold standard for drug-disease relations with crowdsourcing.", "startOffset": 9, "endOffset": 25}, {"referenceID": 31, "context": "To our knowledge, the most extensive study of medical crowdsourcing was performed by [Zhai et al. 2013], who describe a method for crowdsourcing a ground truth for medical named entity recognition and entity linking.", "startOffset": 85, "endOffset": 103}, {"referenceID": 26, "context": "[Snow et al. 2008] have shown that aggregating the answers of an increasing number of unskilled crowd workers with majority vote can lead to high quality NLP training data.", "startOffset": 0, "endOffset": 18}, {"referenceID": 14, "context": "[Hovy et al. 2014] compared the crowd versus experts for the task of part-of-speech tagging.", "startOffset": 0, "endOffset": 18}, {"referenceID": 16, "context": "[Kondreddi et al. 2014] studied crowdsourcing for relation extraction in the general domain, comparing its efficiency to that of fully automated information extraction approaches.", "startOffset": 0, "endOffset": 23}, {"referenceID": 18, "context": "Other research for crowdsourcing ground truth includes: entity clustering and disambiguation [Lee et al. 2013], Twitter entity extraction [Finin et al.", "startOffset": 93, "endOffset": 110}, {"referenceID": 13, "context": "2013], Twitter entity extraction [Finin et al. 2010], multilingual entity extraction and paraphrasing [Chen and Dolan 2011], and taxonomy creation [Chilton et al.", "startOffset": 33, "endOffset": 52}, {"referenceID": 10, "context": "2010], multilingual entity extraction and paraphrasing [Chen and Dolan 2011], and taxonomy creation [Chilton et al. 2013].", "startOffset": 100, "endOffset": 121}, {"referenceID": 30, "context": "[Whitehill et al. 2009] and [Welinder et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 28, "context": "2009] and [Welinder et al. 2010] have used a latent variable model for task difficulty, as well as latent variables to measure the skill of each annotator, to optimize crowdsourcing for image labels.", "startOffset": 10, "endOffset": 32}, {"referenceID": 17, "context": "In addition to our own work [Aroyo and Welty 2013a], the role of ambiguity when building a gold standard has previously been discussed by [Lau et al. 2014].", "startOffset": 138, "endOffset": 155}, {"referenceID": 24, "context": "After empirically studying part-of-speech datasets, [Plank et al. 2014] found that interannotator disagreement is consistent across domains, even across languages.", "startOffset": 52, "endOffset": 71}, {"referenceID": 25, "context": "This position is shared by [Schaekermann et al. 2016], who propose a framework for dealing with uncertainty in ground truth that acknowledges the notion of ambiguity, and uses disagreement in crowdsourcing for modeling this ambiguity.", "startOffset": 27, "endOffset": 53}, {"referenceID": 21, "context": "Wang & Fan collected the sentences with distant supervision [Mintz et al. 2009; Welty et al. 2010], a method that picks positive sentences from a corpus based on whether known arguments of the seed relation appear together in the sentence (e.", "startOffset": 60, "endOffset": 98}, {"referenceID": 29, "context": "Wang & Fan collected the sentences with distant supervision [Mintz et al. 2009; Welty et al. 2010], a method that picks positive sentences from a corpus based on whether known arguments of the seed relation appear together in the sentence (e.", "startOffset": 60, "endOffset": 98}, {"referenceID": 15, "context": "The crowd output was processed with the use of CrowdTruth metrics \u2013 a set of generalpurpose crowdsourcing metrics [Inel et al. 2014], that have been successfully used to model ambiguity in annotations for relation extraction, event extraction, sounds, images, and videos [Aroyo and Welty 2014].", "startOffset": 114, "endOffset": 132}, {"referenceID": 11, "context": "metrics have been previously tested on a subset of our ground truth data, as detailed in [Dumitrache et al. 2015a].", "startOffset": 89, "endOffset": 114}, {"referenceID": 31, "context": "Our first goal was to demonstrate that, like the crowdsourced medical entity recognition work by [Zhai et al. 2013], the CrowdTruth approach of having multiple annotators with precise quality scores can be harnessed to create gold standard data with a quality that rivals annotated data created by medical experts.", "startOffset": 97, "endOffset": 115}], "year": 2017, "abstractText": "Cognitive computing systems require human labeled data for evaluation, and often for training. The standard practice used in gathering this data minimizes disagreement between annotators, and we have found this results in data that fails to account for the ambiguity inherent in language. We have proposed the CrowdTruth method for collecting ground truth through crowdsourcing, that reconsiders the role of people in machine learning based on the observation that disagreement between annotators provides a useful signal for phenomena such as ambiguity in the text. We report on using this method to build an annotated data set for medical relation extraction for the cause and treat relations, and how this data performed in a supervised training experiment. We demonstrate that by modeling ambiguity, labeled data gathered from crowd workers can (1) reach the level of quality of domain experts for this task while reducing the cost, and (2) provide better training data at scale than distant supervision. We further propose and validate new weighted measures for precision, recall, and F-measure, that account for ambiguity in both human and machine performance on this task. CCS Concepts: rInformation systems \u2192 Crowdsourcing; rComputing methodologies \u2192 Language resources; Natural language processing;", "creator": "TeX"}}}