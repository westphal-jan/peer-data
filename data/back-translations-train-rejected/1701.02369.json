{"id": "1701.02369", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2017", "title": "Reinforcement Learning based Embodied Agents Modelling Human Users Through Interaction and Multi-Sensory Perception", "abstract": "This paper extends recent work in interactive machine learning (IML) focused on effectively incorporating human feedback. We show how control and feedback signals complement each other in systems which model human reward. We demonstrate that simultaneously incorporating human control and feedback signals can improve interactive robotic systems' performance on a self-mirrored movement control task where an RL-agent controlled right arm attempts to match the preprogrammed movement pattern of the left arm. We illustrate the impact of varying human feedback parameters on task performance by investigating the probability of giving feedback on each time step and the likelihood of given feedback being correct. We further illustrate that varying the temporal decay with which the agent incorporates human feedback has a significant impact on task performance. We found that 'smearing' human feedback over time steps improves performance and we show varying the probability of feedback at each time step, and an increased likelihood of those feedbacks being 'correct' can impact agent performance. We conclude that understanding latent variables in human feedback is crucial for learning algorithms acting in human-machine interaction domains.", "histories": [["v1", "Mon, 9 Jan 2017 22:03:18 GMT  (1274kb)", "http://arxiv.org/abs/1701.02369v1", "4 pages, 2 figures, Accepted at the 2017 AAAI Spring Symposium on Interactive Multi-Sensory Object Perception for Embodied Agents"], ["v2", "Fri, 20 Jan 2017 17:44:52 GMT  (388kb,D)", "http://arxiv.org/abs/1701.02369v2", "4 pages, 2 figures, Accepted at the 2017 AAAI Spring Symposium on Interactive Multi-Sensory Object Perception for Embodied Agents"], ["v3", "Thu, 26 Jan 2017 18:37:52 GMT  (388kb,D)", "http://arxiv.org/abs/1701.02369v3", "4 pages, 2 figures, Accepted at the 2017 AAAI Spring Symposium on Interactive Multi-Sensory Object Perception for Embodied Agents"]], "COMMENTS": "4 pages, 2 figures, Accepted at the 2017 AAAI Spring Symposium on Interactive Multi-Sensory Object Perception for Embodied Agents", "reviews": [], "SUBJECTS": "cs.HC cs.AI cs.RO", "authors": ["kory w mathewson", "patrick m pilarski"], "accepted": false, "id": "1701.02369"}, "pdf": {"name": "1701.02369.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning based Embodied Agents Modelling Human Users Through Interaction and Multi-Sensory Perception", "authors": ["Kory Mathewson", "Patrick M. Pilarski"], "emails": [], "sections": [{"heading": "Introduction", "text": "Human teachers are a unique component of the environment that can provide control signals and context information through feedback. As human-robot interaction becomes more complex due to rapid advances in actuator and sensor technology, there is a significant gap between the number of possible control signals that a human can provide and the number of controllable actuators in a robotic system. There is often a limited number of control signals that a human can provide and a large number of controllable robot functions. The boundary of human-provided control signals is of particular interest in the field of robotic prostheses - artificial limbs that are attached to the body to expand and / or replace skills that are lost through injury or illness."}, {"heading": "Background", "text": "In this work, we use a continuous actor (AC system) that over time takes action to maximize the expected return - defined as a cumulative future reward signal received by the agent [Sutton and Barto, 1998]. An agent's control policy is iteratively improved by selecting actions that maximize return. RL problems are often described as sequential decision problems that are modeled as Markov decision processes that define tuples: () () ()))) () The full details of the MDPs are left out for space and can be found in the tuples. () ()) ()) ())) The ultimate goal of an RL agent ()) is the ability to recognize, learn. \""}, {"heading": "Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Aldebaran Nao and Myo EMG Data", "text": "The experimental setup is shown in Figure 1. It consists of the Aldebaran Nao robotics platform (Aldebaran Robotics), a Myo EMG wireless wristband (Thalmic Labs) and a MacBook Air (Apple, 2.2 GHz Intel Core i7, 8GB RAM) for human feedback and learning device operation. Experiments in this thesis are performed using a simulated Nao platform, a simulated EMG signal and a simulated human feedback model. We have previously shown that the performance of this experimental setup is comparable between simulation and real experiments [Mathewson et al., 2016]. By simulating human feedback, we are able to characterize and vary important latents that are hidden from the active ingredient that influence system learning."}, {"heading": "Experiments", "text": "We are interested in the modeling of the time, in which we get a grip on the effects of the different model parameters of human being on human being and being on human being and being on human being and human being on human being and being. (...) We are able to define an optimal policy, which closely follows the pre-programmed arm. (...) We are able to derive the MDP reward, which recognizes an anecular error threshold. (...) We are able to define an optimal policy, which closely follows the pre-programmed arm. (...) We are able to derive the MDP reward, which is an anecular error threshold. (...) If the RL-controlled elbow threshold of elbow threshold follows the pre-programmed elbow threshold of the pre-programmed elbow threshold of the pre-programmed elbow connection then a reward of the Self-Self-Self-Self-Self, the Self-Self-Self-Self-Self-Self-Self"}, {"heading": "Results", "text": "The results are presented in Figure 2. The results indicate that human interaction improves the performance of the agent in a self-reflecting motion task in which the performance is measured by means of the mean angular error in the last 5k time steps. Figure 2A shows that a lower probability of potentially false feedback offers a better performance. Figure 2B shows that there may not be a significant performance difference if the probability of the correctness of human feedback varies, given values of P (feedback). This can also be attributed to the values tested, all of which were greater than a 50% chance of correctness. Figure 2C shows that there is a benefit to choose a smear feedback value that is appropriate for the task and the robotic control system of P # # # # amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp; amp # 160 amp # 160 amp # 160 amp # 160 amp # 160 amp; amp # 160 amp # 160 amp; amp # 160 amp # 160 amp; amp # 160 amp # 160 amp # 160 amp; amp # 160 amp # 160 amp # 160 amp; 160 amp # 160 amp # 160 amp; 160 amp # 160 amp # 160 amp; 160 amp # 160 amp # 160 amp # 160 amp; 160 amp # 160 amp; 160 amp # 160 amp; 160 amp; 160 amp # 160 amp # 160 amp # 160 amp # 160 amp; 160 amp # 160 amp; 160 amp; 160 amp; 160 amp # 160 amp; 160 amp # 160 amp; 160 amp # 160 amp; 160 amp # 160 amp; 160 amp # 160 amp # 160 amp # 160 amp; 160 amp; 160 amp # 160 amp; 160 amp # 160 amp; 160 amp # 160 amp; 160 amp # 160 amp; 160 amp; 160 amp # 160 amp; 160 amp; 160 amp # 160 amp # 160 amp # 160 amp # 160 amp; 160 amp # 160 amp; 160 amp # 160 amp # 160 amp; 160 amp; amp; 160 amp; 160 amp; 160 amp; 160 amp # 160 amp # 160 amp; 160 amp; 160 amp # 160 amp # 160 amp; 160 amp; 160 amp # 160 amp # 160 amp; 160"}, {"heading": "Discussion", "text": "The experiments in this paper are carried out with a simulated Nao, a simulated EMG signal and a simulated human feedback. Previously, the performance of this experimental system was shown to be comparable between simulation and real experiments. [Mathewson et al., 2016] In this context, we are investigating the degree to which the learning system is affected by real human feedback. Working in simulation enables rapid iteration and allows testing of many different algorithmic properties. Simulation is often a simple learning problem caused by simplified physics and reduced noise."}, {"heading": "Conclusions", "text": "This work contributes a series of results from experiments that incorporate simulated human feedback and simultaneous human control into the training of a semi-autonomous robot. These results suggest that the performance of the task increases with the incorporation of human feedback into existing learning algorithms to amplify actors and critics. These results support the idea that human interaction can improve the performance of complex robot tasks if human feedback is delivered correctly, uniformly and on a timescale consistent with the original learning problem. This work supports an emerging viewpoint around the human training of a closely linked robot system. By improving the performance of the RL agent, this work further supports the division of autonomy between human and machine."}, {"heading": "Acknowledgements", "text": "This work is supported by the National Sciences & Engineering Research Council of Canada (NSERC), Alberta Innovates - Technology Futures (AITF), the Canada Research Chairs Program, the Canada Foundation for Innovation and the Alberta Machine Intelligence Institute (Amii) / Alberta Innovates Centre for Machine Learning (AICML)."}], "references": [{"title": "Upper limb prosthesis use and abandonment: a survey of the last 25 years", "author": ["EA Biddiss", "TT Chau"], "venue": "J Prosthet Orthot Intl.", "citeRegEx": "Biddiss and Chau. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Proc", "author": ["C Castellini", "P Artemiadis", "M Wininger", "A Ajoudani", "M Alimusaj", "A Bicchi", "B Caputo", "W Craelius", "S Dosen", "K Englehart", "D Farina"], "venue": "Of 1 Workshop on Peripheral Machine Interfaces.", "citeRegEx": "Castellini et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Lecs", "author": ["Chernova S", "Thomaz AL. Robot learning from human teachers"], "venue": "AIML.", "citeRegEx": "Chernova and Tomaz 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Interactively shaping agents via human reinforcement: The TAMER framework", "author": ["Knox", "Stone", "2009] Knox WB", "Stone P"], "venue": "In Proc of 5 Intl. Conf. on Knowledge Capture", "citeRegEx": "Knox et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Knox et al\\.", "year": 2009}, {"title": "Combining manual feedback with subsequent MDP reward signals for reinforcement learning", "author": ["Knox", "WB Stone 2010] Knox", "P. Stone"], "venue": "In Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: volume 1-Volume", "citeRegEx": "Knox et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Knox et al\\.", "year": 2010}, {"title": "Reinforcement learning from human reward: Discounting in episodic tasks", "author": ["WB Knox", "P Stone"], "venue": "IEEE RO-MAN.", "citeRegEx": "Knox and Stone. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Framing reinforcement learning from human reward: Reward positivity", "author": ["WB Knox", "P Stone"], "venue": "temporal discounting, episodicity, and performance. AI.", "citeRegEx": "Knox and Stone. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning", "author": ["R Loftin", "B Peng", "J MacGlashan", "ML Littman", "ME Taylor", "J Huang", "DL Roberts"], "venue": "AAMAS.", "citeRegEx": "Loftin et al.. 2015", "shortCiteRegEx": null, "year": 2016}, {"title": "Taylor ME", "author": ["J MacGlashan", "ML Littman", "DL Roberts", "R Loftin", "B Peng"], "venue": "Convergent Actor Critic by Humans.", "citeRegEx": "MacGlashan et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Simultaneous control and human feedback in the training of robotic agent with actor-critic reinforcement training", "author": ["K Mathewson", "PM Pilarski"], "venue": "IJCAI - Interactive Machine Learning Workshop.", "citeRegEx": "Mathewson et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Control of hand prostheses using peripheral information", "author": ["S Micera", "J Carpaneto", "S Raspopovic"], "venue": "IEEE Reviews in Biomedical Engineering.", "citeRegEx": "Micera et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Conf", "author": ["PM Pilarski", "MR Dawson", "T Degris", "F Fahimi", "JP Carey", "Intl Sutton RS. Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning. IEEE"], "venue": "on Rehabilitation Robotics.", "citeRegEx": "Pilarski et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Rehab", "author": ["PM Pilarski", "TB Dick", "IC Sutton RS. Real-time prediction learning for the simultaneous actuation of multiple prosthetic joints. IEEE"], "venue": "Rob.", "citeRegEx": "Pilarski et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Electromyogram pattern recognition for control of powered upper-limb prostheses: State of the art and challenges for clinical use", "author": ["E Scheme", "K Englehart"], "venue": "J. of Rehab Res and Dev.", "citeRegEx": "Scheme and Englehart. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "The behavior of organisms: an experimental analysis", "author": ["BF Skinner"], "venue": "Appleton-Century, Oxford.", "citeRegEx": "Skinner. 1938", "shortCiteRegEx": null, "year": 1938}, {"title": "Reinforcement learning: An introduction", "author": ["RS Sutton", "AG Barto"], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners", "author": ["AL Thomaz", "C Breazeal"], "venue": "Artificial Intelligence.", "citeRegEx": "Thomaz and Breazeal. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning via human feedback in continuous state and action spaces", "author": ["NA Vien", "W Ertel", "TC Chung"], "venue": "Applied Intelligence.", "citeRegEx": "Vien and Ertel. 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "Prosthetic limbs with many degrees-of-freedom have been developed [Castellini et al., 2014].", "startOffset": 66, "endOffset": 91}, {"referenceID": 14, "context": "RL is a learning framework inspired by behaviorism [Skinner, 1938] which describes how agents improve over time by taking actions in an environment with a goal of maximizing expected return\u2014defined as the cumulative future reward signal received by the agent [Sutton and Barto, 1998].", "startOffset": 51, "endOffset": 66}, {"referenceID": 15, "context": "RL is a learning framework inspired by behaviorism [Skinner, 1938] which describes how agents improve over time by taking actions in an environment with a goal of maximizing expected return\u2014defined as the cumulative future reward signal received by the agent [Sutton and Barto, 1998].", "startOffset": 259, "endOffset": 283}, {"referenceID": 15, "context": "AC methods can reduce variance in gradient estimation through the use of two learning systems: a policy-focused actor (selects the best action) and a critic (estimate of value function, criticizes actor) [Sutton and Barto, 1998].", "startOffset": 204, "endOffset": 228}, {"referenceID": 16, "context": "While there are many ways to incorporate human knowledge into a learning system, before and during learning [Thomaz and Breazeal, 2008; Chernova and Tomaz 2014], this paper focuses on incorporating human feedback directly alongside MDP derived reward.", "startOffset": 108, "endOffset": 160}, {"referenceID": 2, "context": "While there are many ways to incorporate human knowledge into a learning system, before and during learning [Thomaz and Breazeal, 2008; Chernova and Tomaz 2014], this paper focuses on incorporating human feedback directly alongside MDP derived reward.", "startOffset": 108, "endOffset": 160}, {"referenceID": 17, "context": "This work builds on the work of Vien and Ertel, who showed that the human feedback model can be generalized to address the problems associated with periods of noisy, and/or inconsistent, human feedback [Vien and Ertel, 2013].", "startOffset": 202, "endOffset": 224}, {"referenceID": 7, "context": "Recent advancements in modelling human feedback with a Bayesian approach have improved on the work of Knox and Stone in discrete environments [Loftin et al., 2015].", "startOffset": 142, "endOffset": 163}, {"referenceID": 8, "context": "show that human feedback may be better modelled as an advantage function to handle changes in a human\u2019s feedback strategy over time [MacGlashan et al., 2016].", "startOffset": 132, "endOffset": 157}, {"referenceID": 9, "context": "We have previously shown the performance of this experimental set-up to be comparable between simulation and real-world experiments [Mathewson et al., 2016].", "startOffset": 132, "endOffset": 156}, {"referenceID": 9, "context": "We extend on the results in [Mathewson et al., 2016] by exploring the impacts of varying model parameters of human trainer feedback on the RL system during the performance of a self-mirrored movement control task.", "startOffset": 28, "endOffset": 52}, {"referenceID": 11, "context": "With a decay parameter we are able to smear the human feedback forward in time, it has been shown that the limited human feedback can be applied across near-optimal state-action pairs, and support the agent learning an optimal solution [Pilarski et al., 2011].", "startOffset": 236, "endOffset": 259}, {"referenceID": 9, "context": "It has been previously shown the performance of this experimental set-up to be comparable between simulation and real-world experiments [Mathewson et al., 2016].", "startOffset": 136, "endOffset": 160}, {"referenceID": 7, "context": "While we have not optimized for the human feedback characteristics, these results indicate that some human reward paradigms may be preferable to others [Loftin et al., 2015].", "startOffset": 152, "endOffset": 173}, {"referenceID": 8, "context": "This idea is explored in [MacGlashan et al., 2016] where modelling the user feedback as an advantage function, we can understand positive feed back as \u2018yes, that was good\u2019 and negative feedback as \u2018no, that was bad\u2019.", "startOffset": 25, "endOffset": 50}], "year": 2016, "abstractText": "This paper extends recent work in interactive machine learning (IML) focused on effectively incorporating human feedback. We show how control and feedback signals complement each other in systems which model human reward. We demonstrate that simultaneously incorporating human control and feedback signals can improve interactive robotic systems\u2019 performance on a self-mirrored movement control task where a RL-agent controlled right arm attempts to match the preprogrammed movement pattern of the left arm. We illustrate the impact of varying human feedback parameters on task performance by investigating the probability of giving feedback on each time step and the likelihood of given feedback being correct. We further illustrate that varying the temporal decay with which the agent incorporates human feedback has a significant impact on task performance. We found that \u2018smearing\u2019 human feedback over time steps improves performance and we show varying the probability of feedback at each time step, and an increased likelihood of those feedbacks being \u2018correct\u2019 can impact agent performance. We conclude that understanding latent variables in human feedback is crucial for learning algorithms acting in human-machine interaction domains.", "creator": "Word"}}}