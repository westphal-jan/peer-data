{"id": "1701.02377", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2017", "title": "The principle of cognitive action - Preliminary experimental analysis", "abstract": "In this document we shows a first implementation and some preliminary results of a new theory, facing Machine Learning problems in the frameworks of Classical Mechanics and Variational Calculus. We give a general formulation of the problem and then we studies basic behaviors of the model on simple practical implementations.", "histories": [["v1", "Mon, 9 Jan 2017 22:29:08 GMT  (2257kb)", "http://arxiv.org/abs/1701.02377v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marco gori", "marco maggini", "alessandro rossi"], "accepted": false, "id": "1701.02377"}, "pdf": {"name": "1701.02377.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 1.Table of contents"}, {"heading": "1 Introduction 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Formulation of the problem 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Construction of Cost Functional 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 First Application 5", "text": "4.1 First Order Operator........................ 54.1.1 Experimental Results......................................... 54.1.1 Experimental Results...... 194.2 Second Order Operator.................................................. 194.2 Second Order Operator.................................................................................................................."}, {"heading": "5 First application on ANNs 37", "text": ""}, {"heading": "6 Conclusions 40", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Appendix 41", "text": "7.1 General solution and coefficients........................................................................................................................................................................."}, {"heading": "1 Introduction", "text": "Many real-world phenomena could be interpreted in an online scenario within the theory of machine learning. In the case of long-term learning problems, various approaches have been developed to deal with the large amounts of data and the use of their time correlation. Normally, some difficulties arise in storing data and in tracking the intrinsic information that comes from data over time. These aspects may suggest a more natural approach to learning, in a common theory between mechanics, calculus of variation and statistics. In fact, we are postponing an in-depth analysis of these ideas at the theoretical level, focusing on an initial practical implementation to establish a link between these new ideas and some applications to existing structures that might be useful in these preliminary steps. We briefly present basic ideas of this theory, first formulated in [1]. The concept of dissipation is well formulated in [2], whereas a summary of this report and an experimental analysis on standard benchmark can be found."}, {"heading": "2 Formulation of the problem", "text": "We examine the case in which we want to learn a function f that aims to represent the behavior of a characteristic representation u of the spatio-temporal domains D. If we assume the temporal domain T = [0, \u221e) and X RD, we have D = T \u00b7 X and u: D \u2192 Rd, so that f has an input u and depends on a series of weights W. For example, if f is described by an artificial neural network, the problem is to learn the parameters W under a certain assumption, i.e. by minimizing a cost functionality L, which consists of a penal term and a regulation term. Since the weights must be learned over time, we have W depend on time and we write f = f (u (t, x), W (t)). In the classical approach, the penal term imposes an acconsistency w.r.t., while the regulation term imposes the parameter norm, we could see a change in the course of the next physics, which could be too small."}, {"heading": "3 Construction of Cost Functional", "text": "In a classical learning problem, we have to replace a cost functionality L w.r.t. W (t). Functionality consists of a penalty term and a regularization term. The penalty term is calculated on each monitored example, namely via a training set P = {(uk, f \u00b2 k) l k = 1, through a loss function V = f (t, x), f \u00b2 k), where V could be the square function V (f, f \u00b2 k) = 1 2 (f \u2212 f \u00b2 k) 2 (where f = f (t, x), W (t), W (t), W (t), W (t), f \u00b2 k), for example, where V could be the square function V (f \u00b2 k) = 2 (f \u00b2 k) 2 (where f = f (t, x), W (t), the physics). Since the examples are displayed in time, when tk is the time when the pair (uk, f \u00b2 k) is available, we can replace the energy system (k \u00b2), V (k) (V) n, V (k = 1)."}, {"heading": "4 First Application", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 First Order Operator", "text": "In our first application of this theoretical framework, we analyze the simple case in which a linear function of a single real variable is f: R \u2192 R and f = yu + b, in which u = u (t, x (t)) = x (t). In this context, we want to learn the two weights y, b. The problem can be easily formulated by solving the second-order linear differential equation: y \u00bc + Phenomeny equation and analogous formula apply to b. We begin with the case T = \u03b10 + \u03b11D. By applying the equation of EuleroLagrange, we must solve the second-order linear differential equation: y \u00bc + Phenomeny equation + \u03b2y \u2212 \u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441fliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifli"}, {"heading": "4.1.1 Experimental results", "text": "A first implementation (in MatLab) of our first implementation is in the simple case where we have an approximate linear function in an interval (a, b] = [\u2212 1, 1] of the real axis. We assume that the examples on the training set are indeed different by a factor. This allows us to use a discretization of (4) to calculate the evolution of the system, as you can see in section (7,3). We also consider an even subdivision of our interval, which we cover forward and backward, i.e. we move from one to a b and vice versa to guarantee a time correlation between the examples. We assign each point a target f = 2 \u00b7 uk \u2212 1, so we wanted y (t) \u2192 2 and b (t) \u2192 1 after some epochs."}, {"heading": "4.1.2 Choice of solutions", "text": "All these parameters contribute to modelling our system, but we have to choose how. Since we want the system to react immediately to the stimuli, we want an impulse response that reaches its maximum as quickly as possible. Furthermore, it is useful that the system is broad enough to see all the examples beforehand to forget about them. Once we have selected a suitable \u03b8 (see Section 4.3), we can model the parameters \u03b1j to fulfil this requirement. Often, it is more convenient to select the solutions for building our system directly (see Section 7.2). In Fig.14, we can see that if we choose one solution close to 0 and then the other close to \u03b8 (since \u03b8 = (\u03bb1 + \u03bb2)), we have a longer g (green diagram), while we go the other way if the solutions are similar (blue diagram)."}, {"heading": "4.2 Second Order Operator", "text": "Under the practical assumptions of the previous section, we examine the implementation of the case in which the term K is composed by the second-order linear differential operator T = \u03b10 + \u03b11D + \u03b12D2. This time, the Eulero-Lagrange equation results in a fourth-order linear differential equation: D4y + \u03b23D 3y + \u03b22D 2y + \u03b21Dy + \u03b20y + \u03b20y + \u03b3\u00b522l \u2211 k = 1 (ukyk + bk \u2212 f k) uk \u00b7 \u03b4 (t \u2212 tk) = 0 (5), where: \u03b20 = \u03b10\u03b12\u03b82 \u2212 \u03b10\u03b11\u03b8 + \u03b120a22\u03b21 = \u03b12reig2 + (2\u03b10\u03b12 \u2212 21) \u03b122\u03b22 = \u03b122\u03b8 2 + \u03b12\u03b12\u03b12 \u2212 \u03b12 \u2212 \u03b12 (6) This time, the Routh-Hurwitz conditions require a stable order."}, {"heading": "4.2.1 Experimental results", "text": "This time we start with the observation that there are different possibilities in the type of solutions of (5): (1) Four different real solutionsWe begin with the case \u03b8 = 4, \u03b10 = 0.8, \u03b11 = 1.2, \u03b12 = 0.8. As we can see in Fig.15, the system is divergent for \u03b3 = \u2212 1. Also in Fig.16 we have a divergence, but we can see a different oscillation of weights. We can see in (8) that the second term is multiplied by the factor \u03b3 / (\u00b5\u03b122). Since \u03b12 < 1 we can apply some of the considerations made in Section 4.1.1 on the parameter \u00b5. This divergence may be due to a higher gradient if we actually have convergence to the desired values."}, {"heading": "4.2.2 Choice of solutions", "text": "As in the case of the first order, we are allowed to select a suitable set of solutions and then find the parameters for our model (Section 7.2). In this case, we have four solutions which are related to \u03b8 by the relation 2\u03b8 = \u2212 (\u03bb1 + \u03bb2 + \u03bb3 + \u03bb4). In this case, too, we need a solution close to 0 in order to remember. It is also useful not to choose one or two other small (w.r.t. \u03b8) solutions, as this causes the g to grow too much (Fig.25)."}, {"heading": "4.3 Sampling-step \u03c4 , Parameter \u03b8 and number of Impulses", "text": "In our first experiments, we look at a fully supervised training set, where the examples are evenly distributed, both in time and in space. The first example arrives at t1 = \u03c4, then the first supervision arrives at 3 2\u043d (see Section 7.3) and the system receives an impulse. The next example comes \u03c4 seconds after the first and so on. Since the memory is related to the saturation time of the impulse response, the learning process of the system shows all the examples that appear in the interval of the time before this saturation. This means that the system must be constructed in the same way as the saturation time comes after the entire training set, it has been seen more times, since the functional process of the system contains the term e\u03b8t, the parameters that must be such that e\u03b8t0 = 1 is not too much smaller than esuccesst0 (tl moment in which the last example ul comes). Another important feature that we must take into consideration is the delay of the impulse response. If the supervisions (and then the impulses themselves are too frequent), then we are accumulses."}, {"heading": "5 First application on ANNs", "text": "As the first simple practical application we try an experiment to optimize a simple ANN. We use a network with a hidden layer and an output layer, the identity as output function and the rectifier function: f (x) = {x if x > 0 0 other activation function. In this model we simply have to extend the update formulas for the weights y, b to the weights of the two layers. In this first application we try a different setting of parameters, with different number of units and for both the differential operator of the first and the second order. In the next list of experiments we refer to some results obtained by using \u03b8 = 1 and 20 units in the hidden layer."}, {"heading": "5.1 One dimension functions", "text": "First we attack the practical model of the first sections, i.e. a regression problem on a set with 100 points uk [\u2212 1, 1], which are sorted and evenly distributed. All points are labeled with the target yk = 2 \u00b7 uk \u2212 1, but only 10 points give supervision. We have the MSE = 1.77 \u00b7 10 \u2212 3 after 2 \u00b7 104 iterations. In Fig.29 we can see the trend of the MSE in other 2 \u00b7 105 epochs when we turn off supervision (only labeled points for the MSE evaluation). We then try the same parameters for a classification task on the same set. We assign the target class to the points in [\u2212 0.5, 0.5] and the class false (f \u0412k = [0 1]). Again we use only 10 points for supervision. After 5 \u00b7 104 iterations we have MSE = 0.03 and Accuracy = 0.97. Again we try to keep the agent on track and both go to the MSE with almost the same accuracy."}, {"heading": "5.2 Two dimensions functions", "text": "We choose our point in [\u2212 1, 1] \u00b7 [\u2212 1, 1]. We use two different trajectories to cover the training set, a spiral and a flower. We obtain the points of the spiral as: u (t) = {(t / 100) cos (t / 100) sin (t), while the trajectory of the flower is through u (t) = {cos (10t) \u00b7 cos (t) cos (10t) \u00b7 sin (t) We take 100 monitored points coming from each trajectory with t = 1,..., 100 (26 and 40 for the trajectory of the flower or spiral). The points in {(x, y) \u2022 R2: | x | + | y | \u2264 0.5} represent the class, the others are false. We divide our experiments into two different phases. In the first phase we train the network with the monitored points for 105 trajectories achieved with one trajectory. In the second phase (validation of the system) we will compare the results of the two groups in 0.5 and 0.5 respectively."}, {"heading": "5.3 Vowels Classifications", "text": "We record some tracks with the sequential pronunciation of the five Italian vowels. We process the files with Matlab and record the auditory spectral coefficients derived from the RASTA PLP algorithm. We obtain a sampling of the tracks with dots in 40 dimensions with spaces. Sentence 1 results from a 20-second track with sequential pronunciation of the vowels (2053 samples, 700 labeled). Sentence 2 (2144 samples) results from a 20-second track with sequential pronunciation of the vowels repeated in time (600 labeled points). Sentence 3 results from 5 tracks, each containing the pronunciation of a vowel (14934 labeled points). We continue the experiments with the same approach of the previous section. In the first phase we train the network with a few monitored samples. Also in the second phase we switch off the monitoring and examine the performance of the system over time. After some epochs we evaluate the agents on each set."}, {"heading": "6 Conclusions", "text": "As already mentioned, the applications studied do not fit perfectly with our theory, but the positive results showed that they could support our hypothesis and help us to better understand the meaning of the various aspects. This led us to look for a deeper analysis from many theoretical points of view. We are talking about the investigation of other differential operators, cost functionalities and functionalities f. At the same time, we would like to investigate the behavior of the current model in applications where a diverse regulation plays a fundamental role over time, such as computer vision problems."}, {"heading": "7 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 General solution and coefficients", "text": "In this section, we report on some practical calculations and assumptions to solve the differential equation of our theoretical framework = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "7.2 From solution to parameters", "text": "In section 4.2 we saw that the convergence for the case of second order depends not only on \u03b3, but also on the type of solutions of the characteristic polynomial, i.e. \u03b2j. Moreover, the most important parameter of our model \u03b8, which makes it possible to choose the memory width of the system, is related to the function \u043a (t) = e\u03b8t, which represents the weight that the model assigns to each sample over time. The smaller (but always > 0) the larger is the memory of our model. We are interested in finding suitable values of \u03b1j, which allow convergence when it is small. In practice, we can build our model only by the solutions of the characteristic poly, since the parameters influence the updating formulas (4), (8) only by the last sidej, which can be absorbed into the term \u00b5. Then we can directly select suitable solutions and check whether they are related to meaningful parameter values."}, {"heading": "7.2.1 First Order", "text": "If we have the solutions \u03bb1, \u03bb2, the characteristic poly of (3) is\u03bb2 + \u03b8\u03bb + \u03b2 = (\u03bb \u2212 \u03bb1) (\u03bb \u2212 \u03bb2) results. Thus, the value \u03b8 is given by \u03b8 = \u2212 (\u03bb2 + \u03bb1), and the rate \u03b10 \u03b11 can be calculated from \u03b2. That is, if we put \u03bd = \u03b10\u03b11, we can find the appropriate value of \u03b1j from the solutions of \u03bd2 \u2212 \u03b8\u03bd + \u03b2 = 0. (23)"}, {"heading": "7.2.2 Second Order", "text": "In this case, the coefficient \u03b23 is still given by the opposite of the summation between the solutions, and since \u03b23 = 2\u03b8 it is still easy to determine the rates below \u03b1j. To find the rates below \u03b1j, it is convenient to work with \u03bd0 = \u03b12 and \u03bd1 = \u03b11 \u03b12, so that: \u03b20 = \u03b10\u03bd2 \u2212 \u03b10\u03bd2 + 2\u03bd2 \u2212 \u03b10\u03b8 + \u03b1 2 0a22 = \u03bd0\u04452 \u2212 \u03bd2 \u2212 \u03bd0\u03bd2 \u2212 empirical solutions can be found. (24) \u03b21 = empirical solutions can be found. (2) \u03b21 = empirical 2 \u2212 empirical 2 \u2212 empirical 2 \u2212 empirical 2 \u2212 empirical solutions can be found. (25) \u03b22 = empirical solutions can be found. (2 + empirical 2 + empirical 2 + empirical 2 \u2212 empirical 2 \u2212 empirical solutions can be found."}, {"heading": "7.3 From continuos to discrete model", "text": "In practical implementation, it is not convenient to update the continuities of formulas (4), (8), (8), since we need to store too much value of the gradient, i.e., the greater is the memory of the system, the greater is the number of elements that we need to remember. In our case, it is convenient to use a discretization of the system (K + 2), if we have a linear differential equation of the order greater than one, we can transform it into a system of the same order with only linear differential equations of order one. In our case, inD4y + 3y + \u03b22D 2y + \u03b21Dy + \u03b21Dy + \u03b21Dy 0 \u2212 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s (t \u2212 tk) = 0we can replace y0 = y \u00b2 s, y1 = Dy,... and we assume that we rule u (t) =."}], "references": [{"title": "Variational foundations of online backpropagation", "author": ["Salvatore Frandina", "Marco Gori", "Marco Lippi", "Marco Maggini", "Stefano Melacci"], "venue": "In Artificial Neural Networks and Machine Learning - ICANN 2013 - 23rd International Conference on Artificial Neural Networks, Sofia, Bulgaria,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "The principle of least cognitive action", "author": ["Alessandro Betti", "Marco Gori"], "venue": "Theoretical Computer Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Neural network training as a dissipative process", "author": ["Marco Gori", "Marco Maggini", "Alessandro Rossi"], "venue": "Neural Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "On-line Learning on Temporal Manifolds, pages 321\u2013333", "author": ["Marco Maggini", "Alessandro Rossi"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "We briefly introduce basics ideas of this theory, first formulated in [1].", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "The concept of dissipation is well formulated in [2],whereas an summing up of this report and an experimental analysis on standard benchmark can be find in [3].", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "The concept of dissipation is well formulated in [2],whereas an summing up of this report and an experimental analysis on standard benchmark can be find in [3].", "startOffset": 156, "endOffset": 159}, {"referenceID": 3, "context": "A further theoretical abstraction applied to similar environment is proposed in [4], In this document, we will give a slightly theoretical formulation in order to allow us to go straight to the practical implementation issues.", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "We assign the target class true (f\u0304k=[1 0] ) to the points in [\u22120.", "startOffset": 37, "endOffset": 42}, {"referenceID": 0, "context": "5] and the class false (f\u0304k=[0 1] ) to the others.", "startOffset": 28, "endOffset": 33}], "year": 2017, "abstractText": null, "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}