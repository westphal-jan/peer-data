{"id": "1701.02388", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2017", "title": "Stoic Ethics for Artificial Agents", "abstract": "We present a position paper advocating the notion that Stoic philosophy and ethics can inform the development of ethical A.I. systems. This is in sharp contrast to most work on building ethical A.I., which has focused on Utilitarian or Deontological ethical theories. We relate ethical A.I. to several core Stoic notions, including the dichotomy of control, the four cardinal virtues, the ideal Sage, Stoic practices, and Stoic perspectives on emotion or affect. More generally, we put forward an ethical view of A.I. that focuses more on internal states of the artificial agent rather than on external actions of the agent. We provide examples relating to near-term A.I. systems as well as hypothetical superintelligent agents.", "histories": [["v1", "Mon, 9 Jan 2017 23:25:43 GMT  (28kb,D)", "http://arxiv.org/abs/1701.02388v1", "Submitted to Canadian A.I. 2017 conference"], ["v2", "Tue, 28 Mar 2017 23:59:25 GMT  (14kb,D)", "http://arxiv.org/abs/1701.02388v2", "Final accepted version submitted to Canadian A.I. 2017 conference"]], "COMMENTS": "Submitted to Canadian A.I. 2017 conference", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gabriel murray"], "accepted": false, "id": "1701.02388"}, "pdf": {"name": "1701.02388.pdf", "metadata": {"source": "CRF", "title": "Stoic Ethics for Artificial Agents", "authors": ["Gabriel Murray"], "emails": ["gabriel.murray@ufv.ca"], "sections": [{"heading": null, "text": "Keywords: Ethics A.I., Virtue, Stoicism, Superintelligence"}, {"heading": "1 Introduction", "text": "Stoics is a philosophy that emerged during the Hellenistic period and in the era of the Roman Empire. Their ethical perspective is a form of virtue, and both Stoics and virtuoso ethicists have experienced a resurgence of actions and their consequences in recent decades. Ethics is today one of the three most important ethical perspectives, alongside utilitarianism and ethics of ethics of ethics of ethics of ethics of ethics of ethics of ethics of ethics of ethics of ethics of virtues [3]. Utilitarianism examines the usefulness of actions and their consequences, and virtuoso ethics examines duties and duties, virtuality focuses on the virtuality and happiness, and the good life [4]."}, {"heading": "2 Related Work", "text": "The recent discussion of ethical A.I. can be divided into what focuses primarily on the ethical implications of current and short-term A.I. systems, and what focuses primarily on hypothetical super-intelligence, i.e. systems with intelligence and capabilities beyond human level. As an example of the former, Amodei et al. [4] describe concrete problems related to A.I. security, and Arkin [5] discusses ethics for autonomous systems used in warfare. Wallach and Allen [6] discuss morality for agents of varying levels of sophistication. Bostrom [7], Dewey [8], Christiano [9], and Yudkowsky [10] focus on A.I. ethics with a particular emphasis on super-intelligent systems. Much of this work, whether to look at short-term or long-term A.I., is based on reinforcing learning, where an artificial agent learns how to act in an environment by collecting rewards (which could be negative, i.e. punishments)."}, {"heading": "3 Stoicism and A.I.", "text": "In this section, we discuss how stoic philosophy and ethics can be relevant to the development of ethical A.I. systems."}, {"heading": "3.1 Internal States Matter", "text": "As mentioned in Section 2, most existing work on ethical A.I. is actually action-centered. For example, we can design an intelligent agent who tries to maximize a reward function in relation to an aspect of human well-being. There is a clear parallel between such systems and the Utilitarian Ethical Perspective, which states that an action is good if its positive consequences outweigh its negative consequences. Alternatively, an action-centered system could be informed by Deontological Ethics so that it takes measures that do not violate its obligations, and that it does not use people or other agents as a means to an end (i.e. it is in accordance with the category of imperative). In both cases, these actions and consequences are outwardly important to the agent. Stoicism and Virtual Ethics are more to do with the internal states of the agent."}, {"heading": "3.2 Control", "text": "A central concept of stoicism is the dichotomy of control, or the realization that some things are in our control and others are not. A person should not be too preoccupied with things outside his control, and more importantly, things that are outside a person's control are neither good nor bad, morally speaking. Only virtue is good, and virtue is always within a person's control. And even when dealing with events that are outside our control, we still control our reactions to these events. There are several applications of this idea to artificial intelligence. Clearly, it is important for an A.I. agent to know what is under his control and what is not under his control, i.e. what his abilities are and what the environment is like. And in an uncertain environment, the agent still controls his reactions. Modern stoic thinkers have used the idea of a trichotomy of control [16] by introducing a third category of things that we partially control, and this category can be demonstrated by two examples from artificial intelligence."}, {"heading": "3.3 Affective Computing", "text": "The stoic notion of control is closely related to its perspective on emotions (or \"passions\"). Since the word stoically has penetrated into the general vocabulary, meaning lack of emotion, stoics have been mistakenly regarded as emotionless or contemptuous emotion. Indeed, their perspective on emotion is that we control our emotional responses to events, even when we do not control the events themselves, and that there is no point in worrying about things outside our control. [15] A person who fixates on things outside his control suffers unnecessarily. Let's remember that affective abilities for an intelligent system include the ability to recognize, depict, and / or simulate emotions [15]. Let's call an agent with such abilities an affective agent as Affective Agent. Just as a stoic person can monitor and control his emotional responses to events, so an affective agent can monitor and control the way he simulates emotional states and does not just interact with the agent, but with the agent."}, {"heading": "3.4 Stoic Virtues", "text": "In fact, it is the case that one will be able to move to a place where one is able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "3.5 Moral Progress and the Ideal Sage", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is not a country, but a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which is a country, in which is a country, in which is a country,"}, {"heading": "3.6 Stoic Practices", "text": "The first practice is premeditatio malorum, roughly translated as anticipating the bad things to come. [22] For stoic practitioners, this is often a morning meditation in which they anticipate potential misfortunes, ranging from dealing with unkind people to losing a loved one. [22] The idea is not to disregard things that have not yet happened (which would be unstoic), but to deprive these events of their ability to shock or control us. For an artificial agent, such planning could be used to consider worst-case scenarios, minimize maximum potential losses, and determine whether other agents are competitive or cooperative. Another stoic practice is to attach the phrase \"destiny allows\" to all plans and endeavors that constitute an acknowledgment that the person controls his or her own actions, but not the consequences."}, {"heading": "4 Criticisms and Responses", "text": "A general criticism of stoicism and other forms of virtue is that they do not offer a strong normative ethics for situations in the real world where a person has to make a decision that contains the information available to him [2]. A response to this accusation is to accept it and to point out that both utilitarianism and deontological ethics are more precise than they appear at first glance, stoicism sets out a much vague principle: namely, what a virtuous person would do to accept it and to point out that both utilitarianism and deontological ethics are unenforceable."}, {"heading": "4.1 Paramedic Ethics for Artificial Agents", "text": "Collins and Miller [25] offer a \"paramedical ethics\" for computer scientists, who may have to operate in a variety of scenarios in which time and information are limited and in which decision-makers may not have a sound knowledge of ethical theories. They propose a syncretic algorithm that combines elements of utilitarianism, deontological ethics and social contract theory. It is possible to develop a similar algorithm that also includes virtue ethics. For example, a very simplified and general algorithm could be structured as follows: - Collect data. - Determine the measures available. - Does an action satisfy the obligations of the agent (towards a human principle, other agents, the law, etc.)? (the deontological step) 2. Does a conformity with the cardinal virtues (or would it be approved by the supervisor)? (the stoic step 3. What is the expected benefit of an action by maximizing the dexological step?"}, {"heading": "5 Conclusion", "text": "In this position paper we tried to show how stoic ethics could be applied to the development of ethical A.I. systems. We argued that inner states are important for ethical A.I. agents and that inner states can be analyzed by describing the four stoic cardinal virtues in terms of the characteristics of an intelligent system. We also briefly described other stoic practices and how they can be implemented by an A.I. agent. We briefly outlined how to start developing stoic A.I. systems by creating approval-oriented agents with stoic overseers and / or using a syncretic paramedic ethical algorithm with a step with stoic limitations. Although it may be advantageous to analyze the ethics of an A.I. agent from different perspectives, including consequentialist perspectives, we argued for the importance of also conducting a stoic ethical analysis of A.I. agents, in which the internal states of an A.I. agent are analyzed from different perspectives, including consequentialist perspectives, and not based on moral judgments."}], "references": [{"title": "Stoicism http://www.iep.utm.edu/stoicism/, (Accessed on 2January-2017)", "author": ["M. Pigliucci"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Virtue ethics (2016) https://plato.stanford.edu/ entries/ethics-virtue", "author": ["R. Hursthouse", "G. Pettigrove"], "venue": "(Accessed on 2-January-2017)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Justice: What\u2019s the right thing to do", "author": ["M. Sandel"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Concrete problems in ai safety", "author": ["D. Amodei", "C. Olah", "J. Steinhardt", "P. Christiano", "J. Schulman", "D. Man\u00e9"], "venue": "arXiv preprint arXiv:1606.06565", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Governing lethal behavior in autonomous robots", "author": ["R. Arkin"], "venue": "CRC Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Moral machines: Teaching robots right from wrong", "author": ["W. Wallach", "C. Allen"], "venue": "Oxford University Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Superintelligence: Paths, dangers, strategies", "author": ["N. Bostrom"], "venue": "Oxford University Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning what to value", "author": ["D. Dewey"], "venue": "International Conference on Artificial General Intelligence, Springer", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Approval-directed agents (2014) https://medium.com/ai-control/ model-free-decisions-6e6609f5d99e#.hpdm6kwee, (Accessed on 2-January-2017)", "author": ["P. Christiano"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Coherent extrapolated volition (2004) https://intelligence.org/ files/CEV.pdf", "author": ["E. Yudkowsky"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Android arete: Toward a virtue ethic for computational agents", "author": ["K. Coleman"], "venue": "Ethics and Information Technology 3(4)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Virtue ethics for robots (2014) http://jefais.tumblr.com/post/ 89164919838/virtue-ethics-for-robots, (Accessed on 2-January-2017)", "author": ["D. Hicks"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Corrigibility", "author": ["N. Soares", "B. Fallenstein", "S. Armstrong", "E. Yudkowsky"], "venue": "AAAI 2015 Workshop on AI & Ethics.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "The Oxford handbook of affective computing", "author": ["R. Calvo", "S. D\u2019Mello", "J. Gratch", "A. Kappas"], "venue": "Oxford University Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A guide to the good life: The ancient art of stoic joy", "author": ["W. Irvine"], "venue": "Oxford University Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Algorithms to Live by: The Computer Science of Human Decisions", "author": ["B. Christian", "T. Griffiths"], "venue": "Macmillan", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Stoic ethics http://www.iep.utm.edu/stoiceth/, (Accessed on 2January-2017)", "author": ["W. Stephens"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Tay, the neo-nazi millennial chatbot, gets autopsied (2016) http://arstechnica.com/information-technology/2016/03/ tay-the-neo-nazi-millennial-chatbot-gets-autopsied/, (Accessed on 9-January2017)", "author": ["P. Bright"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Microsoft\u2019s tay is an example of bad  design  (2016)  https://medium.com/@carolinesinders/ microsoft-s-tay-is-an-example-of-bad-design-d4e65bb2569f#.x27uitx3u, (Accessed on 9-January-2017)", "author": ["C. Sinders"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Rational choice and the structure of the environment", "author": ["H. Simon"], "venue": "Psychological review 63(2)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1956}, {"title": "Stoicism and the Art of Happiness", "author": ["D. Robertson"], "venue": "Teach Yourself", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Concrete approval-directed agents (2015) https://medium", "author": ["P. Christiano"], "venue": "com/ai-control/concrete-approval-directed-agents-89e247df7f1b#.u2e59x2os,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2017}, {"title": "An Introduction to Multiagent Systems, Second Edition", "author": ["M. Wooldridge"], "venue": "John Wiley & Sons", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Paramedic ethics for computer professionals", "author": ["W. Collins", "K. Miller"], "venue": "Journal of Systems and Software 17(1)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 0, "context": "Stoicism is a philosophy that was prominent during the Hellenistic period and into the era of the Roman Empire [1].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "Virtue Ethics is now one of the three major ethical perspectives, alongside Utilitarianism and Deontological Ethics [2].", "startOffset": 116, "endOffset": 119}, {"referenceID": 2, "context": "Whereas Utilitarianism examines the utility of actions and their consequences, and Deontological Ethics studies duties and obligations, Virtue Ethics focuses on virtuous or moral character, happiness, and the good life [3].", "startOffset": 219, "endOffset": 222}, {"referenceID": 3, "context": "[4] describe concrete problems relating to A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "safety, and Arkin [5] discusses ethics for autonomous systems used in warfare.", "startOffset": 18, "endOffset": 21}, {"referenceID": 5, "context": "Wallach and Allen [6] discuss morality for agents of varying levels of sophistication.", "startOffset": 18, "endOffset": 21}, {"referenceID": 6, "context": "Bostrom [7], Dewey [8], Christiano [9], and Yudkowsky [10] focus on A.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "Bostrom [7], Dewey [8], Christiano [9], and Yudkowsky [10] focus on A.", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "Bostrom [7], Dewey [8], Christiano [9], and Yudkowsky [10] focus on A.", "startOffset": 35, "endOffset": 38}, {"referenceID": 9, "context": "Bostrom [7], Dewey [8], Christiano [9], and Yudkowsky [10] focus on A.", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "Coleman [11] describes characteristics that can help an A.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "Hicks [12] discusses drawbacks to designing ethical A.", "startOffset": 6, "endOffset": 10}, {"referenceID": 12, "context": "be corrigible, or amenable to correction in order to improve behaviour and reduce mistakes [14].", "startOffset": 91, "endOffset": 95}, {"referenceID": 13, "context": "Affective computing involves the ability of a system to recognize, represent, and/or simulate affective states or emotions [15].", "startOffset": 123, "endOffset": 127}, {"referenceID": 6, "context": "It may also use emotional manipulation to persuade human operators to unbox it [7].", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "If the human simulations are truly conscious, this would constitute massive genocide [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 14, "context": "Modern Stoic thinkers have used the idea of a trichotomy of control [16], introducing a third category of things we partly control, and that category can be demonstrated with two examples from within artificial intelligence.", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "Recall that affective capabilities for an intelligent system involve the ability to recognize, represent, and/or simulate emotions or affective states [15].", "startOffset": 151, "endOffset": 155}, {"referenceID": 15, "context": "Christian and Griffiths [17] introduce the notion of Computational Stoicism to refer to the peace of mind that comes with using optimal algorithms in our everyday lives.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "could lack wisdom becomes evident when we examine the virtues that are categorized under wisdom: good sense, good calculation, quick-wittedness, discretion, and resourcefulness [1,18].", "startOffset": 177, "endOffset": 183}, {"referenceID": 16, "context": "could lack wisdom becomes evident when we examine the virtues that are categorized under wisdom: good sense, good calculation, quick-wittedness, discretion, and resourcefulness [1,18].", "startOffset": 177, "endOffset": 183}, {"referenceID": 0, "context": "Justice: The virtues categorized under justice include piety, honesty, equity, and fair dealing [1,18].", "startOffset": 96, "endOffset": 102}, {"referenceID": 16, "context": "Justice: The virtues categorized under justice include piety, honesty, equity, and fair dealing [1,18].", "startOffset": 96, "endOffset": 102}, {"referenceID": 0, "context": "The virtues categorized under courage include endurance, confidence, highmindedness, cheerfulness, and industriousness [1,18].", "startOffset": 119, "endOffset": 125}, {"referenceID": 16, "context": "The virtues categorized under courage include endurance, confidence, highmindedness, cheerfulness, and industriousness [1,18].", "startOffset": 119, "endOffset": 125}, {"referenceID": 0, "context": "Temperance: The virtues included under temperance include good discipline, seemliness, modesty, and self-control [1, 18] .", "startOffset": 113, "endOffset": 120}, {"referenceID": 16, "context": "Temperance: The virtues included under temperance include good discipline, seemliness, modesty, and self-control [1, 18] .", "startOffset": 113, "endOffset": 120}, {"referenceID": 17, "context": "known as Tay, a Twitter chatbot launched by Microsoft that began tweeting racist and graphic content [19, 20].", "startOffset": 101, "endOffset": 109}, {"referenceID": 18, "context": "known as Tay, a Twitter chatbot launched by Microsoft that began tweeting racist and graphic content [19, 20].", "startOffset": 101, "endOffset": 109}, {"referenceID": 6, "context": "agent to accomplish its goals while minimizing the impact that it has on the world [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": "[4] similarly propose either defining or learning an impact regularizer for an agent.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Alternatively, satisficing rather than maximizing models can be explored [21].", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "While it is usually assumed that the Ideal Sage is hypothetical and that there is no perfectly virtuous person, ancient Stoics used Socrates and Cato the Younger as two figures to consider when looking for guidance [22].", "startOffset": 215, "endOffset": 219}, {"referenceID": 8, "context": "That second notion leads us towards Christiano\u2019s proposal for \u201capprovaldirected agents,\u201d instead of goal-directed agents [9, 23].", "startOffset": 121, "endOffset": 128}, {"referenceID": 21, "context": "That second notion leads us towards Christiano\u2019s proposal for \u201capprovaldirected agents,\u201d instead of goal-directed agents [9, 23].", "startOffset": 121, "endOffset": 128}, {"referenceID": 8, "context": "The following quote from Christiano [9] nicely parallels the idea that the Ideal Sage (in this case, a perfect overseer) represents an unattainable state but that we can start simply and scale up:", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "in humans by killing all humans [7].", "startOffset": 32, "endOffset": 35}, {"referenceID": 6, "context": "Bostrom [7] has proposed several variants of what he calls the Hail Mary problem, wherein the A.", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "Yudkowsky [10] (further discussed by Bostrom [7]) proposes that an A.", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "Yudkowsky [10] (further discussed by Bostrom [7]) proposes that an A.", "startOffset": 45, "endOffset": 48}, {"referenceID": 20, "context": "The first practice is the premeditatio malorum, roughly translated as an anticipation of bad things to come [22].", "startOffset": 108, "endOffset": 112}, {"referenceID": 1, "context": "A general criticism of Stoicism and other forms of Virtue Ethics is that they do not provide a strong normative ethics for real-world situations in which a person needs to make a decision given their available information [2].", "startOffset": 222, "endOffset": 225}, {"referenceID": 6, "context": "agent that is trying to maximize a reward relating to human well-being could wind up directly stimulating the pleasure centres of the human brain in order to bring about a sense of well-being [7].", "startOffset": 192, "endOffset": 195}, {"referenceID": 3, "context": "that is trying to maximize some reward function can engage in reward hacking, also known as wireheading [4, 7].", "startOffset": 104, "endOffset": 110}, {"referenceID": 6, "context": "that is trying to maximize some reward function can engage in reward hacking, also known as wireheading [4, 7].", "startOffset": 104, "endOffset": 110}, {"referenceID": 22, "context": "This relates to the idea of adjustable autonomy in multi-agent systems, and the opportunities and challenges associated with an agent being able to dynamically change its autonomy so that it defers to a person more often or less often [24].", "startOffset": 235, "endOffset": 239}, {"referenceID": 23, "context": "Collins and Miller [25] provide a \u201cparamedic ethics\u201d for Computer Science professionals who may need to take action in a variety of scenarios where time and information are limited, and where the decision-makers may not have in-depth knowledge of ethical theories.", "startOffset": 19, "endOffset": 23}], "year": 2017, "abstractText": "We present a position paper advocating the notion that Stoic philosophy and ethics can inform the development of ethical A.I. systems. This is in sharp contrast to most work on building ethical A.I., which has focused on Utilitarian or Deontological ethical theories. We relate ethical A.I. to several core Stoic notions, including the dichotomy of control, the four cardinal virtues, the ideal Sage, Stoic practices, and Stoic perspectives on emotion or affect. More generally, we put forward an ethical view of A.I. that focuses more on internal states of the artificial agent rather than on external actions of the agent. We provide examples relating to near-term A.I. systems as well as hypothetical superintelligent", "creator": "LaTeX with hyperref package"}}}