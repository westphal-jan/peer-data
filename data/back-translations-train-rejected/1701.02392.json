{"id": "1701.02392", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2017", "title": "Reinforcement Learning via Recurrent Convolutional Neural Networks", "abstract": "Deep Reinforcement Learning has enabled the learning of policies for complex tasks in partially observable environments, without explicitly learning the underlying model of the tasks. While such model-free methods achieve considerable performance, they often ignore the structure of task. We present a natural representation of to Reinforcement Learning (RL) problems using Recurrent Convolutional Neural Networks (RCNNs), to better exploit this inherent structure. We define 3 such RCNNs, whose forward passes execute an efficient Value Iteration, propagate beliefs of state in partially observable environments, and choose optimal actions respectively. Backpropagating gradients through these RCNNs allows the system to explicitly learn the Transition Model and Reward Function associated with the underlying MDP, serving as an elegant alternative to classical model-based RL. We evaluate the proposed algorithms in simulation, considering a robot planning problem. We demonstrate the capability of our framework to reduce the cost of replanning, learn accurate MDP models, and finally re-plan with learnt models to achieve near-optimal policies.", "histories": [["v1", "Mon, 9 Jan 2017 23:36:05 GMT  (1448kb,D)", "http://arxiv.org/abs/1701.02392v1", "Accepted at the International Conference on Pattern Recognition, ICPR 2016"]], "COMMENTS": "Accepted at the International Conference on Pattern Recognition, ICPR 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["tanmay shankar", "santosha k dwivedy", "prithwijit guha"], "accepted": false, "id": "1701.02392"}, "pdf": {"name": "1701.02392.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning via Recurrent Convolutional Neural Networks", "authors": ["Tanmay Shankar", "Santosha K Dwivedy", "Prithwijit Guha"], "emails": ["tanmay.shankar@gmail.com", "dwivedy@iitg.ac.in", "pguha@iitg.ac.in"], "sections": [{"heading": null, "text": "In fact, it is difficult to understand the reasoning behind DRL, as it is often the underlying structure of the tasks.In contrast, model-based methods are used to generalize performance across different tasks, it is difficult to understand the reasoning behind DRL approaches, since they use the underlying structure of tasks.5] - [8] We use this inherent structure to make decisions based on domain knowledge, and the estimates of the transition model and reward function associated with the underlying Markov Decision Process (MDP) are set against the underlying structures and insights."}, {"heading": "II. THE VALUE ITERATION RCNN", "text": "In this section, we formulate the value iteration as a recurring convolution, representing the value iteration RCNN (VI RCNN) to perform the value iteration. We consider a standard Markov decision process consisting of a two-dimensional state space S of size Nd \u00b7 Nd, state s S, action space A of size Na, actions a, transition model T (s, s \"), reward function R (s, a) and discount factor. Value iteration, which is typically used to solve optimal policies in an MDP, refers to the Bellman update equation as Vk + 1 (s) = maxa R (s), a reward function T (s)."}, {"heading": "III. THE BELIEF PROPAGATION RCNN", "text": "In this section, we present the RCNN (BP RCNN) belief propagation to represent the Bayes filter faith actualization within the architecture of an RCNN and thus to learn MDP transition models. In order to make an optimal selection of actions in partially observable environments, a precise belief in the state b (s) is required. If we take a measure and obtain observations, we can use the Bayes filter to develop b (s) in relation to an observational model O (s), a, z), associated with the probability p (s), a). The discrete Bayes filter is represented in (2). The denominator is equivalent to p (z), b), and can be implemented as a normalization factor. b (s) = O (s) (s) filter is a (s) filter (s)."}, {"heading": "IV. THE QMDP RCNN", "text": "Finally, we present the QMDP RCNN as a combination of VI RCNN and BP RCNN to retrieve optimal action options and learn the underlying reward function of the POMDP (in addition to the transition model).The results of VI RCNN and BP RCNN decisions are presented as action value function, Q (s, a), and belief in the underlying reward function of the state, b (s), which we use at each step of the time to use these outputs to calculate faith space Q values Q (s), a) using the QMDP approximation. We have Q (s), a), resulting in the highest faith space Q values (s, a), which are alternately presented as Frobenius Inner products < b (s), Q (s, a) > F (s, a) > action options yat, corresponding to the highest faith space Q values, and can be retrieved from the software station (Q)."}, {"heading": "V. EXPERIMENTAL RESULTS AND DISCUSSIONS", "text": "In this section, the results are presented individually in relation to each of the three RCNNs that are defined."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we have defined three RCNN-like architectures, namely Value Iteration RCNN, Belief Propagation RCNN, and QMDP RCNN, to provide a more natural representation of model-based reinforcement learning solutions. Together, these contributions speed up the planning process in a partially observable environment and reduce the cost of redesigning model-based approaches. Given the access to agent observation and action options over time, the BP RCNN learns the transition model, and the QMDP RCNN learns the reward function, and then redesigns using learned models to come up with approximately optimal action options. The proposed architectures also outperform existing model-based approaches in terms of speed and model accuracy. The natural symbiotic representation of planning and learning algorithms allows these approaches to expand to more complex tasks by integrating them with sophisticated perception modules."}], "references": [{"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "NIPS Deep Learning Workshop, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep reinforcement learning with double q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "CoRR, vol. abs/1509.06461, 2015. [Online]. Available: http://arxiv.org/abs/1509.06461", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["M.J. Hausknecht", "P. Stone"], "venue": "CoRR, vol. abs/1507.06527, 2015. [Online]. Available: http://arxiv.org/abs/1507.06527", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D. Mcallester", "S. Singh", "Y. Mansour"], "venue": "In Advances in Neural Information Processing Systems 12. MIT Press, 2000, pp. 1057\u20131063.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "A comparison of direct and model-based reinforcement learning", "author": ["C.G. Atkeson", "J.C. Santamar\u0131\u0301a"], "venue": "Proceedings of the 1997 IEEE International Conference on Robotics and Automation, vol. 4. IEEE Press, 1997, pp. 3557\u20133564. [Online]. Available: ftp://ftp.cc.gatech.edu/pub/people/cga/rl-compare.ps.gz", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Reinforcement learning in robotics: A survey", "author": ["J. Kober", "J.A. Bagnell", "J. Peters"], "venue": "I. J. Robotic Res., vol. 32, no. 11, pp. 1238\u20131274, 2013. [Online]. Available: http://dx.doi.org/10.1177/0278364913495721", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Quasi-online reinforcement learning for robots", "author": ["B. Bakker", "V. Zhumatiy", "G. Gruener", "J. Schmidhuber"], "venue": "Proceedings of the 2006 IEEE International Conference on Robotics and Automation ICRA, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Generalized model learning for reinforcement learning on a humanoid robot.", "author": ["T. Hester", "M. Quinlan", "P. Stone"], "venue": "in ICRA. IEEE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Inverse reinforcement learning in partially observable environments", "author": ["J. Choi", "K.-E. Kim"], "venue": "J. Mach. Learn. Res., vol. 12, pp. 691\u2013 730, Jul. 2011. [Online]. Available: http://dl.acm.org/citation.cfm?id= 1953048.2021028", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Introduction to Reinforcement Learning, 1st ed", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "In Proceedings of the Twenty-first International Conference on Machine Learning. ACM Press, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Value iteration networks", "author": ["A. Tamar", "S. Levine", "P. Abbeel"], "venue": "CoRR, vol. abs/1602.02867, 2016. [Online]. Available: http://arxiv.org/abs/ 1602.02867", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning", "author": ["A.C. Ian Goodfellow", "Yoshua Bengio"], "venue": "2016, book in preparation for MIT Press. [Online]. Available: http: //www.deeplearningbook.org", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["L.-J. Lin"], "venue": "Machine Learning, vol. 8, no. 3\u20134, pp. 293\u2013321, 1992. [Online]. Available: http://www.cs.ualberta.ca/ \u223csutton/lin-92.pdf", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1992}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["M. Kearns", "S. Singh"], "venue": "Mach. Learn., vol. 49, no. 2-3, pp. 209\u2013232, Nov. 2002. [Online]. Available: http://dx.doi.org/10.1023/A:1017984413808", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["B.C. Stadie", "S. Levine", "P. Abbeel"], "venue": "CoRR, vol. abs/1507.00814, 2015. [Online]. Available: http://arxiv.org/abs/1507. 00814", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning with long short-term memory", "author": ["B. Bakker"], "venue": "In NIPS. MIT Press, 2002, pp. 1475\u20131482.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S.P. Singh"], "venue": "CoRR, vol. abs/1507.08750, 2015. [Online]. Available: http://arxiv.org/abs/1507. 08750", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonparametric model-based reinforcement learning", "author": ["C.G. Atkeson"], "venue": "Advances in Neural Information Processing Systems 10, [NIPS Conference, Denver, Colorado, USA, 1997], 1997, pp. 1008\u20131014. [Online]. Available: http://papers.nips.cc/paper/ 1476-nonparametric-model-based-reinforcement-learning", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "Longterm planning by short-term prediction", "author": ["S. Shalev-Shwartz", "N. Ben-Zrihem", "A. Cohen", "A. Shashua"], "venue": "CoRR, vol. abs/1602.01580, 2016. [Online]. Available: http://arxiv.org/abs/1602.01580", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Deep Reinforcement Learning (DRL) algorithms exploit model-free Reinforcement Learning (RL) techniques, to achieve high levels of performance on a variety of tasks, often on par with human experts in the same domain [1].", "startOffset": 216, "endOffset": 219}, {"referenceID": 0, "context": "These DRL methods use deep networks to either approximate the actionvalue functions as in Deep Q Networks [1]\u2013[3], or directly parametrizing the policy, as in policy gradient methods [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "These DRL methods use deep networks to either approximate the actionvalue functions as in Deep Q Networks [1]\u2013[3], or directly parametrizing the policy, as in policy gradient methods [4].", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "These DRL methods use deep networks to either approximate the actionvalue functions as in Deep Q Networks [1]\u2013[3], or directly parametrizing the policy, as in policy gradient methods [4].", "startOffset": 183, "endOffset": 186}, {"referenceID": 4, "context": "In contrast, model-based methods [5]\u2013[8] exploit this inherent structure to make decisions, based on domain knowledge.", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "In contrast, model-based methods [5]\u2013[8] exploit this inherent structure to make decisions, based on domain knowledge.", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "The estimates of the transition model and reward function associated with the underlying Markov Decision Process (MDP) are transferable across environments and agents [9],", "startOffset": 167, "endOffset": 170}, {"referenceID": 9, "context": "A significant deterrent from model-based RL is the indirect nature of learning with the estimated model; subsequent planning is required to obtain the optimal policy [10].", "startOffset": 166, "endOffset": 170}, {"referenceID": 10, "context": "Backpropagation through the QMDP RCNN learns the reward function of an expert agent, in a Learning from Demonstration via Inverse Reinforcement Learning (LfD-IRL) setting [11].", "startOffset": 171, "endOffset": 175}, {"referenceID": 11, "context": "We note that [12] follows an approach mathematically similar to the VI RCNN; however it differs in being a modelfree approach.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "more naturally than the fully connected layer used in [12].", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "The gradient updates of the QMDP RCNN adopt an intuitive form, that further contributes to an intuitive understanding of the action choices of the QMDP RCNN, as compared to [12].", "startOffset": 173, "endOffset": 177}, {"referenceID": 12, "context": "Teacher forcing [13] is adopted to handle this uncertainty; thus the target belief, rather than the network", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "Experience replay [14] is used to randomize over the expert trajectories and transitions while training the QMDP RCNN.", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "A comparison is provided against the models learnt using a counting style algorithm analogous to that used in [15], as well as a weighted-counting style algorithm that updates model-estimates by counting over belief values.", "startOffset": 110, "endOffset": 114}], "year": 2017, "abstractText": "Deep Reinforcement Learning has enabled the learning of policies for complex tasks in partially observable environments, without explicitly learning the underlying model of the tasks. While such model-free methods achieve considerable performance, they often ignore the structure of task. We present a natural representation of to Reinforcement Learning (RL) problems using Recurrent Convolutional Neural Networks (RCNNs), to better exploit this inherent structure. We define 3 such RCNNs, whose forward passes execute an efficient Value Iteration, propagate beliefs of state in partially observable environments, and choose optimal actions respectively. Backpropagating gradients through these RCNNs allows the system to explicitly learn the Transition Model and Reward Function associated with the underlying MDP, serving as an elegant alternative to classical model-based RL. We evaluate the proposed algorithms in simulation, considering a robot planning problem. We demonstrate the capability of our framework to reduce the cost of re-planning, learn accurate MDP models, and finally re-plan with learnt models to achieve near-optimal policies.", "creator": "LaTeX with hyperref package"}}}