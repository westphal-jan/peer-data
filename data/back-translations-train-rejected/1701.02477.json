{"id": "1701.02477", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2017", "title": "Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic Speech Recognition", "abstract": "Multi-task learning (MTL) involves the simultaneous training of two or more related tasks over shared representations. In this work, we apply MTL to audio-visual automatic speech recognition(AV-ASR). Our primary task is to learn a mapping between audio-visual fused features and frame labels obtained from acoustic GMM/HMM model. This is combined with an auxiliary task which maps visual features to frame labels obtained from a separate visual GMM/HMM model. The MTL model is tested at various levels of babble noise and the results are compared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate that MTL is especially useful at higher level of noise. Compared to base-line, upto 7\\% relative improvement in WER is reported at -3 SNR dB", "histories": [["v1", "Tue, 10 Jan 2017 08:47:56 GMT  (35kb,D)", "http://arxiv.org/abs/1701.02477v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.LG", "authors": ["abhinav thanda", "shankar m venkatesan"], "accepted": false, "id": "1701.02477"}, "pdf": {"name": "1701.02477.pdf", "metadata": {"source": "CRF", "title": "MULTI-TASK LEARNING OF DEEP NEURAL NETWORKS FOR AUDIO VISUAL AUTOMATIC SPEECH RECOGNITION", "authors": ["Abhinav Thanda", "Shankar M Venkatesan"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "2.1. Feature Extraction", "text": "The sampling rate of the audio data is converted to 16kHz. For each frame of the speech signal of 25ms duration, filter bank features of 40 dimensions are extracted. Mean and variance normalization are performed. The video frame rate is increased to match the rate of the audio images by interpolation. The area of interest (ROI) corresponding to the area around the speaker's mouth is extracted as follows: Each frame is converted to grayscale and facial recognition is performed using the Viola Jones algorithm. The 64x64 lip region is extracted by detecting 68 boundaries [20] on the speaker's face, and the ROI surrounding the speaker is truncated. 100-dimensional DCT features are extracted from the ROI. Similar to audio features, we perform middle and variance normalization."}, {"heading": "2.2. MTL-DNN", "text": "It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it. (it.) It. (it. (it.) It. (it.) It. (it. (it.) is. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) is. (it. (it.) It. (it. (it.) It. (it. (it.) is. (it. (it.) It. (it. (it. It. (it.) It. (it. (it.) It. (it. (it. (it. It.) It. (it. (it. It. (it.) It. (it. (it.) It. (it. (it. It. It. It. (it. It.) is. (it. (it.) It. (it. (it. It. It. (it.) It. (it.) It. (it. It. (it. (it.) is. It. (it. It. It. It. It. (it. It. (it. It. It. It.) is. (it. It. (it.) is. It. It. It. It. It. It. It. It. It. (it. (it. (it.) is. (it. It. It. It."}], "references": [{"title": "Hearing lips and seeing voices", "author": ["Harry McGurk", "John MacDonald"], "venue": "Nature, vol. 264, pp. 746\u2013748, 1976.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1976}, {"title": "Audio-visual speech modeling for continuous speech recognition", "author": ["St\u00e9phane Dupont", "Juergen Luettin"], "venue": "IEEE transactions on multimedia, vol. 2, no. 3, pp. 141\u2013 151, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Multimodal speech processing using asynchronous hidden markov models", "author": ["Samy Bengio"], "venue": "Information Fusion, vol. 5, no. 2, pp. 81\u201389, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Improving audio-visual speech recognition with an infrared headset", "author": ["Jing Huang", "Gerasimos Potamianos", "Chalapathy Neti"], "venue": "AVSP 2003-International Conference on Audio-Visual Speech Processing, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Dynamic bayesian networks for audio-visual speech recognition", "author": ["Ara V Nefian", "Luhong Liang", "Xiaobo Pi", "Xiaoxing Liu", "Kevin Murphy"], "venue": "EURASIP Journal on Advances in Signal Processing, vol. 2002, no. 11, pp. 1\u201315, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["George E Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Audio-visual deep learning for noise robust speech recognition", "author": ["Jing Huang", "Brian Kingsbury"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 7596\u20137599.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep multimodal learning for audio-visual speech recognition", "author": ["Youssef Mroueh", "Etienne Marcheret", "Vaibhava Goel"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 2130\u20132134.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 689\u2013696.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Audio-visual speech recognition using deep learning", "author": ["Kuniaki Noda", "Yuki Yamaguchi", "Kazuhiro Nakadai", "Hiroshi G Okuno", "Tetsuya Ogata"], "venue": "Applied Intelligence, vol. 42, no. 4, pp. 722\u2013737, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Recent advances in the automatic recognition of audiovisual speech", "author": ["Gerasimos Potamianos", "Chalapathy Neti", "Guillaume Gravier", "Ashutosh Garg", "Andrew W Senior"], "venue": "Proceedings of the IEEE, vol. 91, no. 9, pp. 1306\u20131326, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Audiovisual fusion: Challenges and new approaches", "author": ["Aggelos K Katsaggelos", "Sara Bahaadini", "Rafael Molina"], "venue": "Proceedings of the IEEE, vol. 103, no. 9, pp. 1635\u20131653, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Multitask learning", "author": ["Rich Caruana"], "venue": "Learning to learn, pp. 95\u2013133. Springer, 1998.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 160\u2013167.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-task Learning Deep Neural Networks for Automatic Speech Recognition", "author": ["Dongpeng Chen"], "venue": "Ph.D. thesis, The Hong Kong University of Science and Technology,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Multilingual acoustic models using distributed deep neural networks", "author": ["Georg Heigold", "Vincent Vanhoucke", "Alan Senior", "Patrick Nguyen", "M Ranzato", "Matthieu Devin", "Jeffrey Dean"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 8619\u20138623.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "A study on multilingual acoustic modeling for large vocabulary asr", "author": ["Hui Lin", "Li Deng", "Dong Yu", "Yi-fan Gong", "Alex Acero", "Chin-Hui Lee"], "venue": "2009 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2009, pp. 4333\u20134336.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-task learning for speech recognition: An overview", "author": ["Gueorgui Pironkov", "St\u00e9phane Dupont", "Thierry Dutoit"], "venue": ".", "citeRegEx": "19", "shortCiteRegEx": null, "year": 0}, {"title": "One millisecond face alignment with an ensemble of regression trees", "author": ["Vahid Kazemi", "Josephine Sullivan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 1867\u2013 1874.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "An audio-visual corpus for speech perception and automatic speech recognition", "author": ["Martin Cooke", "Jon Barker", "Stuart Cunningham", "Xu Shao"], "venue": "The Journal of the Acoustical Society of America, vol. 120, no. 5, pp. 2421\u20132424, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "The kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society, 2011, number EPFL-CONF-192584.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Kaldi+ pdnn: building dnn-based asr systems with kaldi and pdnn", "author": ["Yajie Miao"], "venue": "arXiv preprint arXiv:1401.6984, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis", "author": ["Zhizheng Wu", "Cassia Valentini-Botinhao", "Oliver Watts", "Simon King"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4460\u20134464.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "This was inspired by the fact that human perception of speech is dependent on both auditory and visual senses as demonstrated by the famous McGurk effect[1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "Traditionally, AV-ASR systems were implemented using GMM/HMM models [2, 3, 4, 5].", "startOffset": 68, "endOffset": 80}, {"referenceID": 2, "context": "Traditionally, AV-ASR systems were implemented using GMM/HMM models [2, 3, 4, 5].", "startOffset": 68, "endOffset": 80}, {"referenceID": 3, "context": "Traditionally, AV-ASR systems were implemented using GMM/HMM models [2, 3, 4, 5].", "startOffset": 68, "endOffset": 80}, {"referenceID": 4, "context": "Traditionally, AV-ASR systems were implemented using GMM/HMM models [2, 3, 4, 5].", "startOffset": 68, "endOffset": 80}, {"referenceID": 5, "context": "1 September 12, 2016 version, submitted to ICASSP 2017 With the advancement of deep learning based techniques in speech recognition[6], corresponding AV-ASR systems based on deep learning have been proposed [7, 8, 9, 10].", "startOffset": 131, "endOffset": 134}, {"referenceID": 6, "context": "1 September 12, 2016 version, submitted to ICASSP 2017 With the advancement of deep learning based techniques in speech recognition[6], corresponding AV-ASR systems based on deep learning have been proposed [7, 8, 9, 10].", "startOffset": 207, "endOffset": 220}, {"referenceID": 7, "context": "1 September 12, 2016 version, submitted to ICASSP 2017 With the advancement of deep learning based techniques in speech recognition[6], corresponding AV-ASR systems based on deep learning have been proposed [7, 8, 9, 10].", "startOffset": 207, "endOffset": 220}, {"referenceID": 8, "context": "1 September 12, 2016 version, submitted to ICASSP 2017 With the advancement of deep learning based techniques in speech recognition[6], corresponding AV-ASR systems based on deep learning have been proposed [7, 8, 9, 10].", "startOffset": 207, "endOffset": 220}, {"referenceID": 9, "context": "1 September 12, 2016 version, submitted to ICASSP 2017 With the advancement of deep learning based techniques in speech recognition[6], corresponding AV-ASR systems based on deep learning have been proposed [7, 8, 9, 10].", "startOffset": 207, "endOffset": 220}, {"referenceID": 10, "context": "Also GMM/HMM systems require uncorrelated inputs and do not benefit from multiple frames of input whereas, this is not the case for a DNN[11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "Similar to GMM/HMM feature fusion and decision fusion are possible for DNN-HMM systems[12, 13, 7].", "startOffset": 86, "endOffset": 97}, {"referenceID": 12, "context": "Similar to GMM/HMM feature fusion and decision fusion are possible for DNN-HMM systems[12, 13, 7].", "startOffset": 86, "endOffset": 97}, {"referenceID": 6, "context": "Similar to GMM/HMM feature fusion and decision fusion are possible for DNN-HMM systems[12, 13, 7].", "startOffset": 86, "endOffset": 97}, {"referenceID": 13, "context": "This is called multi-task learning(MTL)[14] and has been successfully applied to various problems of NLP[15] and speech recognition[16, 17, 18, 19] such as speech synthesis and multilingual acoustic modeling.", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "This is called multi-task learning(MTL)[14] and has been successfully applied to various problems of NLP[15] and speech recognition[16, 17, 18, 19] such as speech synthesis and multilingual acoustic modeling.", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "This is called multi-task learning(MTL)[14] and has been successfully applied to various problems of NLP[15] and speech recognition[16, 17, 18, 19] such as speech synthesis and multilingual acoustic modeling.", "startOffset": 131, "endOffset": 147}, {"referenceID": 16, "context": "This is called multi-task learning(MTL)[14] and has been successfully applied to various problems of NLP[15] and speech recognition[16, 17, 18, 19] such as speech synthesis and multilingual acoustic modeling.", "startOffset": 131, "endOffset": 147}, {"referenceID": 17, "context": "This is called multi-task learning(MTL)[14] and has been successfully applied to various problems of NLP[15] and speech recognition[16, 17, 18, 19] such as speech synthesis and multilingual acoustic modeling.", "startOffset": 131, "endOffset": 147}, {"referenceID": 18, "context": "This is called multi-task learning(MTL)[14] and has been successfully applied to various problems of NLP[15] and speech recognition[16, 17, 18, 19] such as speech synthesis and multilingual acoustic modeling.", "startOffset": 131, "endOffset": 147}, {"referenceID": 19, "context": "The 64x64 lip region is extracted by detecting 68 landmark points[20] on the speakers face, and cropping the ROI surrounding speakers mouth and chin.", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "The system was trained and tested on GRID audio-visual corpus[21].", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "The syntactic structures of all sentences are similar[21].", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "Models were trained and tested using Kaldi speech recognition tool kit[22] and Kaldi+PDNN[23].", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "Models were trained and tested using Kaldi speech recognition tool kit[22] and Kaldi+PDNN[23].", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "Base-line The base-line system is a single task learning feature fusion model(STL-DNN) somewhat similar to the network in [7].", "startOffset": 122, "endOffset": 125}, {"referenceID": 18, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 74, "endOffset": 82}, {"referenceID": 15, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 74, "endOffset": 82}, {"referenceID": 14, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 17, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 147, "endOffset": 155}, {"referenceID": 16, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 147, "endOffset": 155}, {"referenceID": 15, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 183, "endOffset": 191}, {"referenceID": 18, "context": "Multi-task learning(MTL) has been successfully applied to various problems[19, 16] in NLP[15], speech synthesis[24], multilingual acoustic modeling[18, 17] and other ASR applications [16, 19].", "startOffset": 183, "endOffset": 191}, {"referenceID": 15, "context": "The application of MTL to AV-ASR was suggested in [16] although no results were reported.", "startOffset": 50, "endOffset": 54}, {"referenceID": 16, "context": "Our training procedure to some extent similar to [17].", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "Like [17] we separate the inputs and outputs of two tasks.", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "Our work is inspired by [9] and [7].", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "Our work is inspired by [9] and [7].", "startOffset": 32, "endOffset": 35}, {"referenceID": 8, "context": "In [9] the authors employ unsupervised learning methods to obtain a shared representations of the audio and visual modalities which are then used in a separate supervised training step.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In contrast to [7] we employ low-level feature fusion.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "ported on GRID corpus which includes digits, alphabets and words different from the continuous digit dataset used in [7].", "startOffset": 117, "endOffset": 120}], "year": 2017, "abstractText": "Multi-task learning (MTL) involves the simultaneous training of two or more related tasks over shared representations. In this work, we apply MTL to audio-visual automatic speech recognition(AV-ASR). Our primary task is to learn a mapping between audio-visual fused features and frame labels obtained from acoustic GMM/HMM model. This is combined with an auxiliary task which maps visual features to frame labels obtained from a separate visual GMM/HMM model. The MTL model is tested at various levels of babble noise and the results are compared with a base-line hybrid DNN-HMM AVASR model. Our results indicate that MTL is especially useful at higher level of noise. Compared to base-line, upto 7% relative improvement in WER is reported at -3 SNR dB1.", "creator": "LaTeX with hyperref package"}}}