{"id": "1701.02481", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2017", "title": "Implicitly Incorporating Morphological Information into Word Embedding", "abstract": "In this paper, we implicitly incorporate morpheme information into word embedding. Based on the strategy we utilize the morpheme information, three models are proposed. To test the performances of our models, we conduct the word similarity and syntactic analogy. The results demonstrate the effectiveness of our methods. Our models beat the comparative baselines on both tasks to a great extent. On the golden standard Wordsim-353 and RG-65, our models approximately outperform CBOW for 5 and 7 percent, respectively. In addition, 7 percent advantage is also achieved by our models on syntactic analysis. According to parameter analysis, our models can increase the semantic information in the corpus and our performances on the smallest corpus are similar to the performance of CBOW on the corpus which is five times ours. This property of our methods may have some positive effects on NLP researches about the corpus-limited languages.", "histories": [["v1", "Tue, 10 Jan 2017 08:59:38 GMT  (1446kb,D)", "http://arxiv.org/abs/1701.02481v1", "7 pages, 7 figures"], ["v2", "Thu, 26 Jan 2017 01:35:53 GMT  (1865kb,D)", "http://arxiv.org/abs/1701.02481v2", "7 pages, 7 figures"], ["v3", "Mon, 8 May 2017 03:19:20 GMT  (1865kb,D)", "http://arxiv.org/abs/1701.02481v3", "7 pages, 7 figures"]], "COMMENTS": "7 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["yang xu", "jiawei liu"], "accepted": false, "id": "1701.02481"}, "pdf": {"name": "1701.02481.pdf", "metadata": {"source": "CRF", "title": "Implicitly Incorporating Morphological Information into Word Embedding", "authors": ["Yang Xu", "Jiawei Liu"], "emails": ["smallant@mail.ustc.edu.cn", "ustcljw@mail.ustc.edu.cn"], "sections": [{"heading": null, "text": "In fact, most of them are able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "II. RELATED WORK", "text": "By displaying words in low dimensions vector space, word embedding models can extract semantic information from context words that have been a hot topic in the field of NLP. Many word embedding models are proposed for their effectiveness in handling some tasks such as text classification and sentiment analysis, but some of them are based on neural networks while others are based on matrix factorization. Of all word embedding models, CBOW and Skipgram [5] are the most widely used and receive some state-of-the-art services. CBOW uses the context to predict the target word, while Skip-gram predicts the context by using the target word. Both suffer from the limitation of computational capability until Mikolov has proposed two efficient solutions, hierarchical softmax and negative sampling [10] to solve this problem. To combine the advantage of the distribution bureau presentation with the prerepresentation of the word with the advantage of combining the Penntorix with the word representation of the more famous word."}, {"heading": "III. MORPHEME-ENHANCED WORD EMBEDDING", "text": "In this section, we have proposed three refined morpheme-enhanced embedding models for words: Based on our models, morpheme-like words not only tend to group, but also sit closed to the meanings of morphemes. First, we present some backgrounds of CBOW, the basic model of our methods."}, {"heading": "A. CBOW with Negative Sampling", "text": "In the previous section, the basic idea of the CBOW was discussed. With a slide window, the CBOW uses the context words in the window to predict the target words. In view of a sequence of symbols T = {t1, t2, \u00b7 \u00b7, tn}, the goal of the CBOW is to maximize the following average log probability equation. L = 1n n \u2211 i = 1 log p (ti | context (ti)), (1) where context (ti) means the context words of ti in the slide window. Based on Softmax, p (ti | context (ti)) is defined as a follow probability. Exp (vec \u2032 (ti) T \u2211 \u2212 k \u2264 k, j \u00b7 6 = 0 Vec (ti + j)))). Vx = 1 Exp (vec \u2032 (tx) T = negative vector size (k) refers to the vector size as a follow probability."}, {"heading": "B. Morpheme-Enhanced Word Embedding-Average (MWE-A)", "text": "In this model, we assume that all meanings of token ti have the same meaning for ti. In view of a sequence of characters T = {t1, t2, \u00b7 \u00b7, tn}, we assume that the meaning of the morpheme of ti, (i, n]) is Mi. The modified embedding of ti can be divided into three parts Pi, Ri, and Si, which mean the meaning of the prefix of ti, (i, n). Thus, if ti is the context word of tj, the modified embedding of ti can be defined as a sequence. (vti, + 1Ni, w, w) is the embedding of ti, (4) where vti is the original word embedding ti, Ni denotes the length of Mi, and vw the vector of w."}, {"heading": "D. Morpheme-Enhanced Word Embedding-Max (MWE-M)", "text": "From the accumulations of morphemes and their meanings, it is clear that the corresponding meanings of the prefix, root and suffix of a token, apart from multiple meanings of a morpheme, are usually different. However, some meanings are not close to the target word. In order to better improve the representation of the target word, we only use the meaning of the morpheme whose distance from the target is the maximum value. In Figure 4, the meaning of the suffix for \"anthropologist\" is \"who\" and \"what.\" According to our description, the word \"the\" in red is not used for its shorter distance from the \"anthropologist.\" Further details are shown in Figure 4. For clarification, we also use the spelling in MWE-A. For a single token ti, the meaning of the new morpheme is defined as M imax = {P imax, Rimax, Simax, Simax} where P imax, Simax, Simax, etc."}, {"heading": "E. Strategy to Match the Morpheme", "text": "First, it is obvious that a number of words contain more than one prefix, root, and suffix. For example, \"anthropologist\" has the prefix \"a, an, anthrop.\" The first question is which word should be used. Second, some words \"morphemes\" have nothing to do with the word. For example, although \"apple\" has the prefix \"a,\" it does not have the meaning of \"without, not.\" To solve these problems, some rules are defined as follows. \u2022 If the morphemes of the word ti match, the long string is considered morphemous. For example, the prefix of the word \"anthropologist\" is \"not\" a or an. \"\u2022 We define a threshold. If the cosinal distance between the word ti and its morpheme mi, (mi-Mi) is greater than that of the word, mi will remain or otherwise be abandoned."}, {"heading": "IV. EXPERIMENTAL SETUP", "text": "In order to evaluate the performance of our models, we test them together with the comparative baselines on some semantic and syntactical tasks. From the front sections, our models, which include morphological information, are considered very advantageous for handling the morphemous languages. To demonstrate the assumption, a medium-sized English corpus is selected to evaluate our models."}, {"heading": "A. Corpus and Morphemes", "text": "The corpus comes from the ACL Workshop on Machine Translation2 of 2013 and is used in [11]. The website lists a number of corpses sorted by year when they are published. We select the 2009 news corpus, which is about 1.7 GB in size. It contains about 500 million tokens and 600 thousand vocabulary. To improve the quality of all word embeddings, we filter all numbers and some punctuations out of the corpus. All morphemes2http: / / www.statmt.org / wmt13 / translation-task.htmlused in this essay and their meanings are both collected from the site3. We get 90 prefixes, 241 roots and 64 suffixes."}, {"heading": "B. Baselines", "text": "For comparison, we select three competitive baselines, including CBOW, Skip-gram [5] and Glove [6]. All baselines provide some state-of-the-art services to the NLP tasks. CBOW maximizes the likelihood of the target word by providing its context information, while Skip-gram maximizes the product of the context probabilities by specifying the target word. Both models can be solved by negative sampling and hierarchical Softmax [10]. CBOW and Skip-gram function as baselines in a number of papers [12]. Unlike CBOW and Skip-gram, in this paper we combine not only the context information, but also the advantage of global matrix factoring. It is reported that Glove CBOW and Skip-gram perform better than CBOW and Skip-gram [6] in some NLP tasks. In this paper, we use the source code of wordc, CBOW-BOW and Skip-BOp-BOp."}, {"heading": "C. Parameter Settings", "text": "To speed up the training process, CBOW, Skipgram, together with our models, are trained through negative samples. It is reported that the number of negative samples in the range of 5-20 is useful for small bodies. If a huge body is used, the number of negative samples should be chosen from 2 to 5 [10]. Depending on the body used, the number of negative samples in this essay is set to 20. The dimension of word embedding is set to 200 as in [12]. We set the context window size to 5, corresponding to the setting in [10]."}, {"heading": "D. Evaluation Benchmarks", "text": "In this section, we test all word embedding from several aspects, including word similarity, syntactic analogics1) Word similarity: This experiment is used to evaluate the ability of word embedding to capture semantic information from the corpus.1 Each data set is divided into three columns; the first two columns represent pairs of words and the last column is human score. At the task, we have to solve two problems: One is how to calculate the distance between two words; the other is how to evaluate the similarity between our results and human scores. For the first problem, we use the cosmic distance to measure the distance between two words. This strategy is used in many papers [10], [6]. The second problem is solved via Spearman's rank correlation coefficient. higher meaning means better performance. For the English word similarity, there are two gold standard data sets, including Wordsim-353 [13] and RG-65 [14]."}, {"heading": "V. RESULTS AND ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Word Similarity", "text": "Table 2 shows that our models outperform the comparative baselines on five sets of data. It is noteworthy that our models roughly outperform the CBOW, which is our base model for 5 percent and 7 percent, respectively, on the gold standard Wordsim-353 and RG-65, respectively. On WS-353-REL, the difference between CBOW and MEW-S is as much as 8 percent. The advantage of our models is the validity and effectiveness of our methods. We provide some empirical explanations for the enormous funding. Based on our strategy, more information is captured in the corpus. As in the example in Figure 2, the semantic information captured by CBOW is based only on the sequence \"he is a good anthropologist.\" However, our model captures not only the semantic information in the original sequence, but also the information in the sequence \"he is a good person\" and \"he is a good person.\""}, {"heading": "B. Syntactic Analogy", "text": "In this paper, we report on the performance of the entire data set. In Table 2, all of our models beat the comparison lines to a large extent. Compared to CBOW, the advantage of MEW-A reaches as much as 7 percent. The result is in line with our expectations. In Mikolov's data set, we observe that the suffix \"b\" is usually the same as the suffix \"d\" when answering the question \"a is to b as c is to d.\" Based on our strategy, morpheme-like words will not only accumulate, but also tend to group near the meanings of the morph, giving our embeddings the advantage of dealing with syntactical analogy problems. However, it is abnormal that all results are lower than the state of the art in [18]."}, {"heading": "C. Parameter Analysis", "text": "It is obvious that parameter settings will affect the performance of the word similarity. For example, the larger symbol size corpus contains more semantic information that can improve the performance of the word similarity. In this essay, we analyze the effects of symbol size and window size on the performance of word embedding. In the analysis of symbol size, we set the same parameter settings as in the previous section. The size of the corpora used in this analysis are the 1 / 5, 2 / 5, 3 / 5, 4 / 5 and 5 / 5 of the aforementioned corpus. We use the result of word similarity on Wordsim-353 as the evaluation criterion. From Figure 5, we observe several phenomena. First, the performance of the CBOW is sensitive to the symbol size and 5 / 5 of the aforementioned corpus. However, our models seem more stable than that of the CBOW."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we proposed a new method for implicitly incorporating the morpheme information into the Word embedding. In our models, the meanings of the morpheme are inserted into the input layer of the CBOW, not into the morpheme itself. On the basis of this strategy, morpheme-like words are not only merged, but also tend to cluster near the meanings of their morphemes.Based on the different strategy of adding the meanings of the morphemes, three models were built called MWEA, MWE-S and MWE-M. To test the performance of our embedding, we used three comparative baselines and tested them for word similarity and syntactical analogy tasks. Results show the effectiveness of models called MWEA, MWE-S and MWE-M. Based on the gold standard Wordsim-353 and RG-65, our models will exceed the CBOW models by 5 percent and 7 percent respectively."}], "references": [{"title": "Topical word embeddings", "author": ["Y. Liu", "Z. Liu", "T.-S. Chua", "M. Sun"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "Cambridge university press Cambridge,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Lexicon integrated cnn models with attention for sentiment analysis", "author": ["B. Shin", "T. Lee", "J.D. Choi"], "venue": "2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast semantic extraction using a novel neural network architecture", "author": ["R. Collobert", "J. Weston"], "venue": "ACL 2007, Proceedings of the Meeting of the Association for Computational Linguistics, June 23-30, 2007, Prague, Czech Republic, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation.", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in EMNLP, vol", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["M. Faruqui", "J. Dodge", "S.K. Jauhar", "C. Dyer", "E. Hovy", "N.A. Smith"], "venue": "arXiv preprint arXiv:1411.4166, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint learning of character and word embeddings", "author": ["X. Chen", "L. Xu", "Z. Liu", "M. Sun", "H. Luan"], "venue": "International Conference on Artificial Intelligence, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["T. Luong", "R. Socher", "C.D. Manning"], "venue": "Conference, 2013, pp. 104\u2013113.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "J. Dean"], "venue": "Advances in neural information processing systems, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "Computer Science, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Eigenwords: Spectral word embeddings", "author": ["P.S. Dhillon", "D.P. Foster", "L.H. Ungar"], "venue": "The Journal of Machine Learning Research, vol. 16, no. 1, pp. 3035\u20133078, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "Proceedings of the 10th international conference on World Wide Web. ACM, 2001, pp. 406\u2013414.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Contextual correlates of synonymy", "author": ["H. Rubenstein", "J.B. Goodenough"], "venue": "Communications of the ACM, vol. 8, no. 10, pp. 627\u2013633, 1965.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1965}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "Meeting of the Association for Computational Linguistics: Long Papers, 2012, pp. 873\u2013882.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal distributional semantics.", "author": ["E. Bruni", "N.-K. Tran", "M. Baroni"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A study on similarity and relatedness using distributional and wordnetbased approaches", "author": ["E. Agirre", "E. Alfonseca", "K. Hall", "J. Kravalova", "M. Pa\u015fca", "A. Soroa"], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2009, pp. 19\u201327.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.T. Yih", "G. Zweig"], "venue": "HLT-NAACL, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "For example, we can simply average the vectors of all tokens in a document and utilize some classifiers to do text classification [1].", "startOffset": 130, "endOffset": 133}, {"referenceID": 1, "context": "In addition, word embedding are also used for information retrieval [2], sentiment analysis [3], etc.", "startOffset": 68, "endOffset": 71}, {"referenceID": 2, "context": "In addition, word embedding are also used for information retrieval [2], sentiment analysis [3], etc.", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "There are many classic word embedding methods including the Semantic Extraction using a Neural Network Architecture (SENNA) [4], Continuous Bag-of-Word (CBOW), Skip-gram [5], Global Vector (Glove) [6], etc.", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "There are many classic word embedding methods including the Semantic Extraction using a Neural Network Architecture (SENNA) [4], Continuous Bag-of-Word (CBOW), Skip-gram [5], Global Vector (Glove) [6], etc.", "startOffset": 170, "endOffset": 173}, {"referenceID": 5, "context": "There are many classic word embedding methods including the Semantic Extraction using a Neural Network Architecture (SENNA) [4], Continuous Bag-of-Word (CBOW), Skip-gram [5], Global Vector (Glove) [6], etc.", "startOffset": 197, "endOffset": 200}, {"referenceID": 6, "context": "These methods adjust the locations of word embeddings in vector space based on lexica like WordNet [7].", "startOffset": 99, "endOffset": 102}, {"referenceID": 7, "context": "For example, Chen [8] proposed a character-enhanced Chinese word embedding", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "In Luong\u2019s paper [9], a word is divided into the morpheme part and the basic part.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "Among all of word embedding models, CBOW and Skipgram[5] are the most widely used and get some state-of-theart performances.", "startOffset": 53, "endOffset": 56}, {"referenceID": 9, "context": "Both of them suffer from the limitation of computational ability until Mikolov proposed two efficient solution, hierarchical softmax and negative sampling [10], to solve this problem.", "startOffset": 155, "endOffset": 159}, {"referenceID": 5, "context": "In order to combine the advantage of distributional word representation with the advantage of matrix factorization, Pennington proposed another famous word embedding model named Glove [6], which is reported to outperform the word2vec on some tasks.", "startOffset": 184, "endOffset": 187}, {"referenceID": 7, "context": "Chen proposed a character-enhanced Chinese word embedding model [8].", "startOffset": 64, "endOffset": 67}, {"referenceID": 8, "context": "Based on recursive neural network, Luong split a word into morpheme part and basic part [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 10, "context": "Recently, Kim has proposed a character-level language model [11].", "startOffset": 60, "endOffset": 64}, {"referenceID": 10, "context": "The corpus stems from the 2013 ACL Workshop on Machine Translation2 and is used in [11].", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "For comparison, we choose three competitive baselines including CBOW, Skip-gram [5] and Glove [6].", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "For comparison, we choose three competitive baselines including CBOW, Skip-gram [5] and Glove [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 9, "context": "Both of the models can be solved via negative sampling and hierarchical softmax [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "CBOW and Skipgram acts as the baselines in a number of papers [12].", "startOffset": 62, "endOffset": 66}, {"referenceID": 5, "context": "It is reported that Glove outperforms CBOW and Skip-gram in some NLP tasks [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "If huge-sized corpus is used, the number of negative samples should be chosen from 2 to 5 [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 11, "context": "The dimension of word embedding is set as 200 like that in [12].", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "We set the context window size as 5 which is equal to the setting in [10].", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "This strategy is used in many papers [10], [6].", "startOffset": 37, "endOffset": 41}, {"referenceID": 5, "context": "This strategy is used in many papers [10], [6].", "startOffset": 43, "endOffset": 46}, {"referenceID": 12, "context": "For English word similarity, there are two golden standard datasets including Wordsim-353 [13] and RG-65 [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "For English word similarity, there are two golden standard datasets including Wordsim-353 [13] and RG-65 [14].", "startOffset": 105, "endOffset": 109}, {"referenceID": 8, "context": "Rare-Word [9], SCWS [15], Men-3k [16] and WS-353-Related [17].", "startOffset": 10, "endOffset": 13}, {"referenceID": 14, "context": "Rare-Word [9], SCWS [15], Men-3k [16] and WS-353-Related [17].", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "Rare-Word [9], SCWS [15], Men-3k [16] and WS-353-Related [17].", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "Rare-Word [9], SCWS [15], Men-3k [16] and WS-353-Related [17].", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "This dataset with size of 8000 is created by Mikolov [18].", "startOffset": 53, "endOffset": 57}, {"referenceID": 5, "context": "Specially, because of medium-size corpus and experimental settings, Glove doesn\u2019t perform as well as it in other papers [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 17, "context": "In [18], they divided the dataset into adjectives, nouns and verbs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "However, it is abnormal that all results are lower than the state-of-the-art results in [18].", "startOffset": 88, "endOffset": 92}], "year": 2017, "abstractText": "In this paper, we implicitly incorporate morpheme information into word embedding. Based on the strategy we utilize the morpheme information, three models are proposed. To test the performances of our models, we conduct the word similarity and syntactic analogy. The results demonstrate the effectiveness of our methods. Our models beat the comparative baselines on both tasks to a great extent. On the golden standard Wordsim-353 and RG-65, our models approximately outperform CBOW for 5 and 7 percent, respectively. In addition, 7 percent advantage is also achieved by our models on syntactic analysis. According to parameter analysis, our models can increase the semantic information in the corpus and our performances on the smallest corpus are similar to the performance of CBOW on the corpus which is five times ours. This property of our methods may have some positive effects on NLP researches about the corpus-limited languages.", "creator": "LaTeX with hyperref package"}}}