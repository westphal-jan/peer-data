{"id": "1701.02490", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2017", "title": "Real-Time Bidding by Reinforcement Learning in Display Advertising", "abstract": "The majority of online display ads are served through real-time bidding (RTB) --- each ad display impression is auctioned off in real-time when it is just being generated from a user visit. To place an ad automatically and optimally, it is critical for advertisers to devise a learning algorithm to cleverly bid an ad impression in real-time. Most previous works consider the bid decision as a static optimization problem of either treating the value of each impression independently or setting a bid price to each segment of ad volume. However, the bidding for a given ad campaign would repeatedly happen during its life span before the budget runs out. As such, each bid is strategically correlated by the constrained budget and the overall effectiveness of the campaign (e.g., the rewards from generated clicks), which is only observed after the campaign has completed. Thus, it is of great interest to devise an optimal bidding strategy sequentially so that the campaign budget can be dynamically allocated across all the available impressions on the basis of both the immediate and future rewards. In this paper, we formulate the bid decision process as a reinforcement learning problem, where the state space is represented by the auction information and the campaign's real-time parameters, while an action is the bid price to set. By modeling the state transition via auction competition, we build a Markov Decision Process framework for learning the optimal bidding policy to optimize the advertising performance in the dynamic real-time bidding environment. Furthermore, the scalability problem from the large real-world auction volume and campaign budget is well handled by state value approximation using neural networks.", "histories": [["v1", "Tue, 10 Jan 2017 09:30:29 GMT  (796kb,D)", "http://arxiv.org/abs/1701.02490v1", "WSDM 2017"], ["v2", "Thu, 12 Jan 2017 01:37:39 GMT  (796kb,D)", "http://arxiv.org/abs/1701.02490v2", "WSDM 2017"]], "COMMENTS": "WSDM 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.GT", "authors": ["han cai", "kan ren", "weinan zhang", "kleanthis malialis", "jun wang", "yong yu", "defeng guo"], "accepted": false, "id": "1701.02490"}, "pdf": {"name": "1701.02490.pdf", "metadata": {"source": "CRF", "title": "Real-Time Bidding by Reinforcement Learning in Display Advertising", "authors": ["\u2021Jun Wang", "\u2020Yong Yu", "Defeng Guo"], "emails": ["hcai@apex.sjtu.edu.cn,", "kren@apex.sjtu.edu.cn,", "wnzhang@apex.sjtu.edu.cn,", "j.wang@cs.ucl.ac.uk", "permissions@acm.org."], "sections": [{"heading": null, "text": "Permission to make digital or printed copies of all or part of this work for personal or internal use is granted free of charge, provided that copies are not made or distributed for profit or commercial purposes, and that copies bear this notice and the complete quotation on the first page. Copyrights for components of this work other than ACM must be recognized. Credit abstraction is permitted. Any other reproduction or re-publication, publication on servers, or redistribution to lists requires prior express permission and / or a fee."}, {"heading": "1. INTRODUCTION", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "2. BACKGROUND AND RELATED WORK", "text": "In this context, it should be noted that this is a very complex matter, a political project, which is first and foremost a political project."}, {"heading": "3. PROBLEM AND FORMULATION", "text": "In an RTB ad auction, each bidder acts on behalf of their advertiser and generates bids to achieve the specific goal of the campaign. Our main goal is to derive the optimal bidding policy based on a reinforced learning methodology. In most success-oriented campaigns, the optimization goal is to maximize users \"responses to the ads displayed when the bid results in winning the auction. Without losing universality, we consider clicks as our goal of targeted user response, while other KPIs can be similarly adopted. The graph of interactions between the bidder and the environment is shown in Figure 1."}, {"heading": "3.1 Problem Definition", "text": "Mathematically, bidding in ad advertising is considered an episodic process [1]; each episode includes T-auctions that are sent sequentially to the bidder. Each auction is represented by a high-dimensional feature vector x, indexed using a unified binary encryption. Each entry of x corresponds to a category in a field, such as the category London in the City field and the category Friday in the Weekday field. The fields consist of the campaign advertising information (e.g. creative ID and campaign ID) and the auctioned contextual information (e.g. user cookie ID, location, time, publishing domain and URL). Initially, the agent is initialized with a budget B, and the advertising target will receive as many clicks as possible during the subsequent T-auctions. Three main pieces of information are considered by the agent (i), the remaining auction number t, \u00b7 \u00b7 \u00b7, \u00b7 T} (iene)."}, {"heading": "3.2 MDP Formulation of RTB", "text": "A Markov Decision Process (MDP) provides a framework that is widely used for modeling agent-environment interactions (21). Our notations are listed in Table 1. \u2212 An MDP can be represented by a tuple (S, \"As,\" \"P ass,\" \"Rass,\" \"As\" and \"As\" are two sets of all states and all possible actions in state s \"S,\" respectively \"P ass\" represents the likelihood of transition from state s \"S\" to another state s, \"S if we take measures designated by\" As \"(a, s,\" s \"). Similarly, Rass\" is the reward function designated by r (a, s \"s\"), which represents the reward that is drawn after taking action in state s \"S\" and then the transition to state s. \"We consider (t, b\" xt \") as state s 1 and assume that the function vector is drawn."}, {"heading": "4. DYNAMIC PROGRAMMING SOLUTION", "text": "On a small scale, Eq. (3), the definition of V (t), b), b =, b =, b =, as defined, can have the optimal value function V (t, b, x) (max), where (t, b, x) \u2212 \u2212 \u2212 the state s is represented. In the meantime, we consider the situations in which we do not pay attention to the attribute vector x; another optimal function is thus V (t, b), b), b (b), x (x): the expected total reward if we start in (t, b) without paying attention to the attribute vector x. V (t, b) =, X px (x) V (t, b), x) dx is sufficient. Also, that we have the optimal political reward and express it as the optimal action a (t, b, x). From the definition, we have V (0, b, x) V = (0), b), if there are no auctions remaining, b)."}, {"heading": "4.1 Handling Large-Scale Issues", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "5. EXPERIMENTAL SETUP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets", "text": "In our experimental study, two real-world datasets are used, iPinYou and YOYI.iPinYou is one of the established RTB ad companies in China. The entire dataset includes 19.5M impressions, 14.79K clicks and 16.0K CNY spending on 9 different campaigns within 10 days in 2013. We follow [31] for the breakdown of train / test kits and feature engineering.YOYI is a leading RTB company focused on multi-device ad advertising in China. The YOYI dataset includes 441.7 M impressions, 416.9K clicks and 319.5K CNY costs on 8 days in January 2016. The first 7 days are set as training dates, while the last day is set as test datas.For experimental reproducibility, we publish our Code3. In the paper, we mainly report the results on iPinYou dataset and verify our algorithms using the YOYI supplement."}, {"heading": "5.2 Evaluation Methods", "text": "Valuation is done from the perspective of an advertising campaign operator with a predefined budget and lifespan (episode length).Valuation metrics The main goal of the bidder is to optimize the KPI (e.g. clicks, conversions, revenue, etc.) in light of the campaign budget. In our work, we consider the number of clicks purchased as a KPI, which is the primary valuation metric in our experiments. We also analyze other statistics such as profit rate, cost per million impressions (CPM) and effective cost per click (eCPC).Valuation flow. We largely follow the structure of the valuation flow, except for dividing the test data into episodes. Specifically, the test data is a list of records consisting of the offer proposal and the user's response to the user rating (click).We divide the test data into episodes."}, {"heading": "5.3 Compared Methods", "text": "The following bidding guidelines are compared with the same CTR estimation component LB LB, which is a logistic regression model and the same bidding landscape forecasting component, which is a non-parametric method as described in Section 2: SS-MDP is based on [1], taking into account the bidding landscape, but ignoring the feature vector of the bidding request when submitting the bid price. Although we consider this model to be state-of-the-art, it is proposed to work on keyword level bidding in the sponsored search, which means it is not fine-grained enough to compare with RTB display strategies. Mcpc gives its bidding strategy as aMcPC (t, b, x) = CPC \u00d7 \u03b8 (x), which corresponds to the demand of some advertisers for maximum CPC (cost per click). Lin is a linear bidding strategy w.r.Das pr, Tx (Lx)."}, {"heading": "6. EXPERIMENTAL RESULTS", "text": "In this section we present the experimental results for small and large data settings."}, {"heading": "6.1 Small-Scale Evaluation", "text": "The comparison of performance to iPinYou dataset under T = 1000 and various budget conditions is presented in Figure 7. Compared to the total clicks (upper left plot) we find that (i) our proposed model RLB yields the best results under all budget conditions. (iii) Mc-nbsp; Lin has the second-best performance, which represents a widely used bidding strategy in the industry. (iii) Mc-nbsp; n Mcnbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; c; nbsp; nbsp; nbsp; nbsp; nbsp; c; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; c; nbsp; c; nbsp; c; nbsp; c; nbsp; c; nbsp; c; nbsp; c; nbsp; c; c; nbsp; c; c; nbsp; c; nbsp; c; nbsp; c; c; nbsp; c; c; nbsp; nbsp; c; c; nbsp; c; c; nbsc; nbsc; c; nbsp; c; c; nbsp; nbsp; c; nbsp; c; nbsc;"}, {"heading": "6.2 Large-Scale Evaluation", "text": "In this section, we first update the value function in algorithm 1 under T0 = 10, 000 and B0 = CPMtrain \u00b7 10 \u2212 3 \u00b7 T0 \u00b7 1 / 2, then we train a neural network with the obtained data (t, b, D (t, b))) (with the exception of (t, b) 0, \u00b7 \u00b7, T0} \u00b7 {0, \u00b7 \u00b7 \u00b7, B0}). Here, we use a fully connected neural network with two hidden layers using the Tanh activation function, the first hidden layer has 30 hidden nodes and the second has 15 hidden nodes. Next, we apply the neural network to perform bending under T = 100, 000 and B = CPMtrain \u00b7 10 \u2212 3 \u00b7 T \u00b7 c0. Furthermore, SS-MDP is not tested in this experiment because it suffers from scalability problems and has a similar low performance as in the small-scale evaluation."}, {"heading": "7. ONLINE DEPLOYMENT AND A/B TEST", "text": "The deployment environment is based on HP ProLiant DL360p Gen8 servers. A 5-node cluster is used for the bidder, with each node being in CentOS release 6.3, with 6 core Intel Xeon CPU E5-2620 (2.10 GHz) and 64 GB RAM. The model is compared in Lua with Nginx.The bidding strategy is Lin, as discussed in Section 5.3. The optimization goal is to click. The two compared methods get the same budget, which is allocated further episodes. Unlike the online reviews, the online evaluation process only stops when the budget is exhausted. Within one episode, a maximum bid number T is set for each strategy to prevent excessive spending. Specifically, T is determined by the allocated budget for episode B, previous CPM and win rate."}, {"heading": "8. CONCLUSIONS", "text": "In this paper, we proposed a model-based reinforcement learning model (RLB) to learn the auction strategy in RTB advertising. Naturally, the auction strategy is defined as the policy of conducting an auction action in view of the state of campaign parameters and input request information. An MDP formulation was used to capture the transition and reward function of the state by modelling the auction competition or by clicking the user. The optimal auction policy is then derived by means of dynamic programming. Furthermore, to cope with the large auction volume and campaign budget, we proposed neural network models to take into account the difference in values between two consecutive countries. Experimental results on two large data sets in the real world and online A / B tests showed the superiority of our RLB solutions over several strong baseline and state-of-the-art methods, as well as their high efficiency in handling large-scale data sets, which we will use for future work such as optimizing the RLB policy."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank the engineers of YOYI DSP for providing the offline experiment data set and the engineers of Vlion DSP for their support in conducting online A / B tests."}, {"heading": "9. REFERENCES", "text": "[1] K. Amin, M. Kearns, P. Key, and A. Schwaighofer.Budget optimization for sponsored search: Censored learning in mdps. UAI, 2012. [2] J. Boyan and A. W. Moore. Generalization in reinforcement learning: Safely approximating the value function. NIPS, pages 369-376, 1995. [3] O. Chapelle. Modeling delay feedback in display advertising. In KDD, 2014. [4] Y. Chen, P. Berkhin, B. Anderson, and N. R. Devanur. Real-time bidding algorithms for performance-based display ad allocation. In KDD, 2011. Y. Cui, R. Zhang, W. Li, and J. Mao. Bid landscape prognosting in online ad exchange marketplace. In KDD, 2011. [6] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet advertising and the second price auction."}], "references": [{"title": "Budget optimization for sponsored search: Censored learning in mdps", "author": ["K. Amin", "M. Kearns", "P. Key", "A. Schwaighofer"], "venue": "UAI", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Generalization in reinforcement learning: Safely approximating the value function", "author": ["J. Boyan", "A.W. Moore"], "venue": "NIPS, pages 369\u2013376", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Modeling delayed feedback in display advertising", "author": ["O. Chapelle"], "venue": "KDD", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Real-time bidding algorithms for performance-based display ad allocation", "author": ["Y. Chen", "P. Berkhin", "B. Anderson", "N.R. Devanur"], "venue": "KDD", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Bid landscape forecasting in online ad exchange marketplace", "author": ["Y. Cui", "R. Zhang", "W. Li", "J. Mao"], "venue": "KDD", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords", "author": ["B. Edelman", "M. Ostrovsky", "M. Schwarz"], "venue": "Technical report, National Bureau of Economic Research", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Stable function approximation in dynamic programming", "author": ["G.J. Gordon"], "venue": "ICML, pages 261\u2013268", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft\u2019s bing search engine", "author": ["T. Graepel", "J.Q. Candela", "T. Borchert", "R. Herbrich"], "venue": "ICML", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "et al", "author": ["X. He", "J. Pan", "O. Jin", "T. Xu", "B. Liu", "T. Xu", "Y. Shi", "A. Atallah", "R. Herbrich", "S. Bowers"], "venue": "Practical lessons from predicting clicks on ads at facebook. In ADKDD", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimal bidding in stochastic budget constrained slot auctions", "author": ["K. Hosanagar", "V. Cherepanov"], "venue": "EC", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Auction theory", "author": ["V. Krishna"], "venue": "Academic press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Real time bid optimization with smooth budget delivery in online advertising", "author": ["K.-C. Lee", "A. Jalali", "A. Dasdan"], "venue": "ADKDD", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Estimating conversion rate in display advertising from past erformance data", "author": ["K.-c. Lee", "B. Orten", "A. Dasdan", "W. Li"], "venue": "In KDD,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Programmatic buying bidding  strategies with win rate and winning price estimation in real time mobile advertising", "author": ["X. Li", "D. Guan"], "venue": "In PAKDD", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "et al", "author": ["H.B. McMahan", "G. Holt", "D. Sculley", "M. Young", "D. Ebner", "J. Grady", "L. Nie", "T. Phillips", "E. Davydov", "D. Golovin"], "venue": "Ad click prediction: a view from the trenches. In KDD", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Predicting response in mobile advertising with hierarchical importance-aware factorization machine", "author": ["R.J. Oentaryo", "E.-P. Lim", "J.-W. Low", "D. Lo", "M. Finegold"], "venue": "WSDM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Bid optimizing and inventory scoring in targeted online advertising", "author": ["C. Perlich", "B. Dalessandro", "R. Hook", "O. Stitelman", "T. Raeder", "F. Provost"], "venue": "KDD", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "G", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre"], "venue": "van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Pac model-free reinforcement learning", "author": ["A.L. Strehl", "L. Li", "E. Wiewiora", "J. Langford", "M.L. Littman"], "venue": "ICML, pages 881\u2013888. ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Introduction to reinforcement learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "volume 135. MIT Press Cambridge", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Kernelized value function approximation for reinforcement learning", "author": ["G. Taylor", "R. Parr"], "venue": "ICML, pages 1017\u20131024. ACM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Real-time bidding: A new frontier of computational advertising research", "author": ["J. Wang", "S. Yuan"], "venue": "WSDM", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Display advertising with real-time bidding (RTB) and behavioural targeting", "author": ["J. Wang", "W. Zhang", "S. Yuan"], "venue": "arXiv preprint arXiv:1610.03013", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Functional bid landscape forecasting for display advertising", "author": ["Y. Wang", "K. Ren", "W. Zhang", "J. Wang", "Y. Yu"], "venue": "ECML-PKDD, pages 115\u2013131", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine learning, 8(3-4):279\u2013292", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1992}, {"title": "Predicting winning price in real time bidding with censored data", "author": ["W.C.-H. Wu", "M.-Y. Yeh", "M.-S. Chen"], "venue": "KDD", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Smart pacing for effective online ad campaign optimization", "author": ["J. Xu", "K.-c. Lee", "W. Li", "H. Qi", "Q. Lu"], "venue": "In KDD,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Sequential selection of correlated ads by pomdps", "author": ["S. Yuan", "J. Wang"], "venue": "CIKM", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-time bidding for online advertising: measurement and analysis", "author": ["S. Yuan", "J. Wang", "X. Zhao"], "venue": "ADKDD", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Statistical arbitrage mining for display advertising", "author": ["W. Zhang", "J. Wang"], "venue": "KDD", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal real-time bidding for display advertising", "author": ["W. Zhang", "S. Yuan", "J. Wang"], "venue": "KDD", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 17, "context": "In particular, the successful application of reinforcement learning in certain settings such as gaming control [19] has demonstrated that machines not only can predict, but also have a potential of achieving comparable humanlevel control and decision making.", "startOffset": 111, "endOffset": 115}, {"referenceID": 28, "context": "Auctions, particularly real-time bidding (RTB), have been a major trading mechanism for online display advertising [30, 7, 23].", "startOffset": 115, "endOffset": 126}, {"referenceID": 21, "context": "Auctions, particularly real-time bidding (RTB), have been a major trading mechanism for online display advertising [30, 7, 23].", "startOffset": 115, "endOffset": 126}, {"referenceID": 0, "context": "Unlike the keyword-level bid decision in sponsored search [1], the advertiser needs to make the impression-level bid decision in RTB, i.", "startOffset": 58, "endOffset": 61}, {"referenceID": 16, "context": ", bidding for every single ad impression in real time when it is just being generated by a user visit [18, 30] .", "startOffset": 102, "endOffset": 110}, {"referenceID": 28, "context": ", bidding for every single ad impression in real time when it is just being generated by a user visit [18, 30] .", "startOffset": 102, "endOffset": 110}, {"referenceID": 9, "context": ", to calculate the strategic amount that the advertiser would like to pay for an ad opportunity, constitutes a core component that drives the campaigns\u2019 ROI [11, 32].", "startOffset": 157, "endOffset": 165}, {"referenceID": 30, "context": ", to calculate the strategic amount that the advertiser would like to pay for an ad opportunity, constitutes a core component that drives the campaigns\u2019 ROI [11, 32].", "startOffset": 157, "endOffset": 165}, {"referenceID": 5, "context": "A straightforward bidding strategy in RTB display advertising is mainly based on the truthfulness of second-price auctions [6], which means the bid price for each ad impression should be equal to its true value, i.", "startOffset": 123, "endOffset": 126}, {"referenceID": 12, "context": ", clickthrough rate) [14].", "startOffset": 21, "endOffset": 25}, {"referenceID": 29, "context": "However, for budgeted bidding in repeated auctions, the optimal bidding strategy may not be truthful but depends on the market competition, auction volume and campaign budget [31].", "startOffset": 175, "endOffset": 179}, {"referenceID": 16, "context": "In [18, 32], researchers have proposed to seek the optimal bidding function that directly maximizes the campaign\u2019s key performance indicator (KPI), e.", "startOffset": 3, "endOffset": 11}, {"referenceID": 30, "context": "In [18, 32], researchers have proposed to seek the optimal bidding function that directly maximizes the campaign\u2019s key performance indicator (KPI), e.", "startOffset": 3, "endOffset": 11}, {"referenceID": 3, "context": "Nevertheless, such static bid optimization frameworks may still not work well in practice because the RTB market competition is highly dynamic and it is fairly common that the true data distribution heavily deviates from the assumed one during the model training [4], which requires additional control step such as budget pacing [28] to constrain the budget spending.", "startOffset": 263, "endOffset": 266}, {"referenceID": 26, "context": "Nevertheless, such static bid optimization frameworks may still not work well in practice because the RTB market competition is highly dynamic and it is fairly common that the true data distribution heavily deviates from the assumed one during the model training [4], which requires additional control step such as budget pacing [28] to constrain the budget spending.", "startOffset": 329, "endOffset": 333}, {"referenceID": 19, "context": "An MDP provides a mathematical framework which is widely used for modelling the dynamics of an environment under different actions, and is useful for solving reinforcement learning problems [21].", "startOffset": 190, "endOffset": 194}, {"referenceID": 6, "context": "For large-scale situations, it is difficult to experience the whole state space, which leads to the use of function approximation that constructs an approximator of the entire function [8, 22].", "startOffset": 185, "endOffset": 192}, {"referenceID": 20, "context": "For large-scale situations, it is difficult to experience the whole state space, which leads to the use of function approximation that constructs an approximator of the entire function [8, 22].", "startOffset": 185, "endOffset": 192}, {"referenceID": 30, "context": "In the RTB process [32], the advertiser receives the bid request of an ad impression with its real-time information and the very first thing to do is to estimate the utility, i.", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": ", the market price [1, 5], which is the highest bid price from other competitors, is also forecasted by the bid landscape forecasting component.", "startOffset": 19, "endOffset": 25}, {"referenceID": 4, "context": ", the market price [1, 5], which is the highest bid price from other competitors, is also forecasted by the bid landscape forecasting component.", "startOffset": 19, "endOffset": 25}, {"referenceID": 28, "context": "Given the estimated utility and cost factors, the bidding strategy [30] decides the final bid price with accessing the information of the remaining budget and auction volume.", "startOffset": 67, "endOffset": 71}, {"referenceID": 22, "context": "A recent comprehensive study on the data science of RTB display advertising is posted in [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": ", click or conversion, and can be modeled as a probability estimation task [14].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": ", click-through rate (CTR) [16], conversion rate (CVR) [14] and post-click conversion delay patterns [3].", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": ", click-through rate (CTR) [16], conversion rate (CVR) [14] and post-click conversion delay patterns [3].", "startOffset": 55, "endOffset": 59}, {"referenceID": 2, "context": ", click-through rate (CTR) [16], conversion rate (CVR) [14] and post-click conversion delay patterns [3].", "startOffset": 101, "endOffset": 104}, {"referenceID": 12, "context": "For modeling, linear models such as logistic regression [14] and nonlinear models such as boosted trees [10] and factorization machines [17] are widely used in practice.", "startOffset": 56, "endOffset": 60}, {"referenceID": 8, "context": "For modeling, linear models such as logistic regression [14] and nonlinear models such as boosted trees [10] and factorization machines [17] are widely used in practice.", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "For modeling, linear models such as logistic regression [14] and nonlinear models such as boosted trees [10] and factorization machines [17] are widely used in practice.", "startOffset": 136, "endOffset": 140}, {"referenceID": 7, "context": "There are also online learning models that immediately perform updating when observing each data instance, such as Bayesian probit regression [9], FTRL learning in logistic regression [16].", "startOffset": 142, "endOffset": 145}, {"referenceID": 14, "context": "There are also online learning models that immediately perform updating when observing each data instance, such as Bayesian probit regression [9], FTRL learning in logistic regression [16].", "startOffset": 184, "endOffset": 188}, {"referenceID": 12, "context": "In our paper, we follow [14, 32] and adopt the most widely used logistic regression for utility estimation to model the reward on agent actions.", "startOffset": 24, "endOffset": 32}, {"referenceID": 30, "context": "In our paper, we follow [14, 32] and adopt the most widely used logistic regression for utility estimation to model the reward on agent actions.", "startOffset": 24, "endOffset": 32}, {"referenceID": 4, "context": "is the winning probability given each specific bid price [5].", "startOffset": 57, "endOffset": 60}, {"referenceID": 13, "context": "The authors in [15, 32, 5] presented some hypothetical winning functions and learned the parameters.", "startOffset": 15, "endOffset": 26}, {"referenceID": 30, "context": "The authors in [15, 32, 5] presented some hypothetical winning functions and learned the parameters.", "startOffset": 15, "endOffset": 26}, {"referenceID": 4, "context": "The authors in [15, 32, 5] presented some hypothetical winning functions and learned the parameters.", "startOffset": 15, "endOffset": 26}, {"referenceID": 4, "context": "For example, a log-normal market price distribution with the parameters estimated by gradient boosting decision trees was proposed in [5].", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "Since advertisers only observe the winning impressions, the problem of censored data [1, 33] is critical.", "startOffset": 85, "endOffset": 92}, {"referenceID": 25, "context": "Authors in [27] proposed to leverage censored linear regression to jointly model the likelihood of observed market prices in winning cases and censored ones with losing bids.", "startOffset": 11, "endOffset": 15}, {"referenceID": 23, "context": "Recently, the authors in [25] proposed to combine survival analysis and decision tree models, where each tree leaf maintains a non-parametric survival model to fit the censored market prices.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "In this paper, we follow [1, 33] and use a non-parametric method to model the market price distribution.", "startOffset": 25, "endOffset": 32}, {"referenceID": 16, "context": "As has been discussed above, bidding strategy optimization is the key component within the decision process for the advertisers [18].", "startOffset": 128, "endOffset": 132}, {"referenceID": 10, "context": "The auction theory [12] proved that truthful bidding is the optimal strategy under the second-price auction.", "startOffset": 19, "endOffset": 23}, {"referenceID": 30, "context": "However, truthful bidding may perform poorly when considering the multiple auctions and the budget constraint [32].", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "In real-world applications, the linear bidding function [18] is widely used.", "startOffset": 56, "endOffset": 60}, {"referenceID": 30, "context": "The authors in [32] empirically showed that there existed non-linear bidding functions better than the linear ones under variant budget constraints.", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "When the data changes, however, the heuristic model [18] or hypothetical bidding functions [31, 32] cannot depict well the real data distribution.", "startOffset": 52, "endOffset": 56}, {"referenceID": 29, "context": "When the data changes, however, the heuristic model [18] or hypothetical bidding functions [31, 32] cannot depict well the real data distribution.", "startOffset": 91, "endOffset": 99}, {"referenceID": 30, "context": "When the data changes, however, the heuristic model [18] or hypothetical bidding functions [31, 32] cannot depict well the real data distribution.", "startOffset": 91, "endOffset": 99}, {"referenceID": 0, "context": "The authors in [1, 29] proposed the model-based MDPs to derive the optimal policy for bidding in sponsored search or ad selection in contextual advertising, where the decision is made on keyword level.", "startOffset": 15, "endOffset": 22}, {"referenceID": 27, "context": "The authors in [1, 29] proposed the model-based MDPs to derive the optimal policy for bidding in sponsored search or ad selection in contextual advertising, where the decision is made on keyword level.", "startOffset": 15, "endOffset": 22}, {"referenceID": 0, "context": "In our work, we investigate the most challenging impression-level bid decision problem in RTB display advertising that is totally different from [1, 29].", "startOffset": 145, "endOffset": 152}, {"referenceID": 27, "context": "In our work, we investigate the most challenging impression-level bid decision problem in RTB display advertising that is totally different from [1, 29].", "startOffset": 145, "endOffset": 152}, {"referenceID": 0, "context": "We also tackle the scalability problem, which remains unsolved in [1], and demonstrate the efficiency and effectiveness of our method in a variety of experiments.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "Mathematically, we consider bidding in display advertising as an episodic process [1]; each episode comprises T auctions which are sequentially sent to the bidding agent.", "startOffset": 82, "endOffset": 85}, {"referenceID": 19, "context": "A Markov Decision Process (MDP) provides a framework that is widely used for modeling agent-environment interactions [21].", "startOffset": 117, "endOffset": 121}, {"referenceID": 24, "context": "One may consider the possibility of model-free approaches [26, 20] to directly learn the bidding policy from experience.", "startOffset": 58, "endOffset": 66}, {"referenceID": 18, "context": "One may consider the possibility of model-free approaches [26, 20] to directly learn the bidding policy from experience.", "startOffset": 58, "endOffset": 66}, {"referenceID": 30, "context": "(6), we consider an approximation m(\u03b4,x) \u2248 m(\u03b4) by following the dependency assumption x\u2192 \u03b8 \u2192 a\u2192 w(winning rate) in [32].", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "the pCTR with a static parameter [18], such as Mcpc and Lin discussed in Section 5.", "startOffset": 33, "endOffset": 37}, {"referenceID": 30, "context": "(7), we take the approximation m(\u03b4,x) \u2248 m(\u03b4) by following the dependency assumption x\u2192 \u03b8 \u2192 a\u2192 w(winning rate) in [32] and consequently get an approximation of the optimal value function V (t, b) in Eq.", "startOffset": 113, "endOffset": 117}, {"referenceID": 1, "context": "As a widely used solution [2, 21], here we take a fully connected neural network with several hidden layers as a nonlinear approximator.", "startOffset": 26, "endOffset": 33}, {"referenceID": 19, "context": "As a widely used solution [2, 21], here we take a fully connected neural network with several hidden layers as a nonlinear approximator.", "startOffset": 26, "endOffset": 33}, {"referenceID": 29, "context": "We follow [31] for splitting the train/test sets and feature engineering.", "startOffset": 10, "endOffset": 14}, {"referenceID": 30, "context": "We mostly follow [32] when building the evaluation flow, except that we divide the test data into episodes.", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "Following previous work [32, 31], we run the evaluation with c0 = 1/32, 1/16, 1/8, 1/4, 1/2.", "startOffset": 24, "endOffset": 32}, {"referenceID": 29, "context": "Following previous work [32, 31], we run the evaluation with c0 = 1/32, 1/16, 1/8, 1/4, 1/2.", "startOffset": 24, "endOffset": 32}, {"referenceID": 0, "context": "SS-MDP is based on [1], considering the bid landscape but ignoring the feature vector of bid request when giving the bid price.", "startOffset": 19, "endOffset": 22}, {"referenceID": 16, "context": "using the training data [18].", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "(ii) Lin has the second best performance, which is a widely used bidding strategy in industry [18].", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "Also, since RLB naturally tackles the problem of budget over- or under-spending across the campaign lifetime, we will compare our RLB solutions with the explicit budget pacing techniques [13, 28].", "startOffset": 187, "endOffset": 195}, {"referenceID": 26, "context": "Also, since RLB naturally tackles the problem of budget over- or under-spending across the campaign lifetime, we will compare our RLB solutions with the explicit budget pacing techniques [13, 28].", "startOffset": 187, "endOffset": 195}], "year": 2017, "abstractText": "The majority of online display ads are served through realtime bidding (RTB) \u2014 each ad display impression is auctioned off in real-time when it is just being generated from a user visit. To place an ad automatically and optimally, it is critical for advertisers to devise a learning algorithm to cleverly bid an ad impression in real-time. Most previous works consider the bid decision as a static optimization problem of either treating the value of each impression independently or setting a bid price to each segment of ad volume. However, the bidding for a given ad campaign would repeatedly happen during its life span before the budget runs out. As such, each bid is strategically correlated by the constrained budget and the overall effectiveness of the campaign (e.g., the rewards from generated clicks), which is only observed after the campaign has completed. Thus, it is of great interest to devise an optimal bidding strategy sequentially so that the campaign budget can be dynamically allocated across all the available impressions on the basis of both the immediate and future rewards. In this paper, we formulate the bid decision process as a reinforcement learning problem, where the state space is represented by the auction information and the campaign\u2019s real-time parameters, while an action is the bid price to set. By modeling the state transition via auction competition, we build a Markov Decision Process framework for learning the optimal bidding policy to optimize the advertising performance in the dynamic real-time bidding environment. Furthermore, the scalability problem from the large real-world auction volume and campaign budget is well handled by state value approximation using neural networks. The empirical study on two large-scale real-world datasets and the live A/B testing on a commercial platform have demonstrated the superior performance and high efficiency compared to state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}