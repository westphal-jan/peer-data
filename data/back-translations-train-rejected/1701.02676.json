{"id": "1701.02676", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2017", "title": "Unsupervised Image-to-Image Translation with Generative Adversarial Networks", "abstract": "It's useful to automatically transform an image from its original form to some synthetic form (style, partial contents, etc.), while keeping the original structure or semantics. We define this requirement as the \"image-to-image translation\" problem, and propose a general approach to achieve it, based on deep convolutional and conditional generative adversarial networks (GANs), which has gained a phenomenal success to learn mapping images from noise input since 2014. In this work, we develop a two step (unsupervised) learning method to translate images between different domains by using unlabeled images without specifying any correspondence between them, so that to avoid the cost of acquiring labeled data. Compared with prior works, we demonstrated the capacity of generality in our model, by which variance of translations can be conduct by a single type of model. Such capability is desirable in applications like bidirectional translation", "histories": [["v1", "Tue, 10 Jan 2017 16:43:03 GMT  (1484kb,D)", "http://arxiv.org/abs/1701.02676v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["hao dong", "paarth neekhara", "chao wu", "yike guo"], "accepted": false, "id": "1701.02676"}, "pdf": {"name": "1701.02676.pdf", "metadata": {"source": "META", "title": "Unsupervised Image-to-Image Translation with Generative Adversarial Networks", "authors": ["Hao Dong", "Chao Wu", "Yike Guo"], "emails": ["hao.dong11@imperial.ac.uk", "pniitucs@iitr.ac.in", "chao.wu@imperial.ac.uk", "y.guo@imperial.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution that meets the needs of the individual."}, {"heading": "2. Background", "text": "In this section we discuss the related work and the techniques used in our work. A generative adversarial network (GAN) consists of a generator G and a discriminator D [5]. The discriminator is trained to maximize the likelihood that it will generate both training samples and samples. The generator is trained to minimize the likelihood of assigning samples from the generator to generate plausible samples based on the random vector z. Subsequently, it learns to synthesize images from unnamed data, then it can match an image from the generator."}, {"heading": "3. Method", "text": "Our image translation model consists of both Deep Convolutional Encoder and Decoder. It includes input of images from the source domain and output of images from the target domain (s) tied to a specific class / domain label. The training process consists of two steps: the first step for the image generator G for all domains and the second step for the image encoder E for all domains. During the translation process, the conditional class label is applied to the input image from the source domain and passes through the trained network for the output image in the target domain."}, {"heading": "3.1. Learning shared feature", "text": "In order to take advantage of both the unattended feature learning capability of GANs and the representation capability of Convolutional Encoders, we divided the learning process into two stages. In a first step, as shown on the left side of Figure 2, we used the auxiliary classifier GAN [15] to learn the global common characteristics of images sampled from different domains. The common characteristics are presented as a latent variable (noise) vector z. To narrow the variables into a specific range, we defined them as z-R-U (\u2212 1, 1). The idea is based on the fact that class-independent information contains a global structure about the synthesized image [15]. Thus, if the domains are semantically identical or similar (such as different faces of peoples, different weather of the environment), we expect that the common characteristics in the domains (such as smile, face, background, etc.) can be captured by the same latent representation in the domains."}, {"heading": "3.2. Learning image encoder", "text": "After the first step, the generator is able to generate corresponding images for different areas by keeping the latent variables fixed and changing the class name. In the second step, we learn how to map the image to the global latent variable representation. Instead of applying the trained generator G to the image code E [21], and except that the output image looks the same as the input image, we introduce a new method that E applies after the G from the first step, as shown in the middle of Figure 2. We, apart from the network, output the same variables with the input variables. The reason for this method works well: since the trained generator is able to synthesize images from arbitrary noise - this method is not only able to reconstruct the detailed feature, but it also accelerates image tracking. The reason for this is that this method works well: since the trained generator is able to synthesize images from arbitrary noise - we use the images from the image-synthesized image generator."}, {"heading": "3.3. Translation", "text": "After completing the training of the generator and image encoder, as shown on the right in Figure 2, the network first maps the image to the global latent representation z and then synthesizes the image of the target domain using a conditional generator."}, {"heading": "4. Experiments", "text": "In this section, we first present the results of the celebA face dataset on gender transformation and the video of the presidential debate on face swap. [11] The celebA face dataset contains 84434 male images and 118165 female images with different backgrounds and faces. For the second dataset, we identified and extracted face images of Obama and Hillary from two short videos of the presidential debate, there are 8452 images for Obama and 5065 images for Hillary that have different facial expressions. We used the same GAN and image encoder architecture for both datasets. The image size was 64 x 64, with 3 channel colors. We set the number of latent variables z to 100 and embedded the class designation on a vector of 5 values. As in the auxiliary classifier GAN [15], we used the learning rate of 0.0002 and used the Adam Optimizer [8] with a dynamic of 0.5 for step 1 and 2. During step 100, during step 64 training, we used the stack size for 1 and step 64, we trained the zoom for step 1 and step 2."}, {"heading": "4.1. Gender transformation", "text": "As Figure 3 shows, the proposed method has not only learned the features and expressions of the human face, but has also learned to reconstruct the background to a certain extent. On the one hand, the translation has achieved the desired performance, with the synthetic image having a similar image quality to the input images. On the other hand, the network can successfully translate facial images for both women and men and women, which means that our method is bidirectional."}, {"heading": "4.2. Face swapping", "text": "Figure 4 shows that the proposed method learns expressions and facial orientation very efficiently so that it does not violate the context. This function is very useful when we want to swap faces in the video."}, {"heading": "5. Conclusion", "text": "In this paper, we proposed a simple and effective two-step learning method for universal, unattended picture-to-picture translation. In the next step, we plan to further scale up2https: / / github.com / zsdonghao / tensorlayerthe network for higher resolution and try out more data sets. In the future, it may be interesting to combine symbolic features (such as edge recognition) with the original data to further improve translation performance. We will also try to make the method more general, make it more indicative of a learning algorithm and apply it to various other applications. We are considering proposing a paradigm of \"modular learning\" that performs continuous recursive self-improvement in terms of usability (reward system)."}], "references": [{"title": "TensorFlow: Large-Scale Machine", "author": ["A. Agarwal", "P. Barham", "e. a. Brevdo", "Eugene"], "venue": "Learning on Heterogeneous Distributed Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K. Weinberger", "F. Sha"], "venue": "arXiv preprint arXiv:1206.4683", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Sketch2photo: internet image montage", "author": ["T. Chen", "M.-M. Cheng", "P. Tan", "A. Shamir", "S.-M. Hu"], "venue": "ACM Transactions on Graphics (TOG), 28(5):124", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Domainadversarial training of neural networks", "author": ["Y. Ganin", "E. Ustinova", "H. Ajakan", "P. Germain", "H. Larochelle", "F. Laviolette", "M. Marchand", "V. Lempitsky"], "venue": "arXiv preprint arXiv:1505.07818", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "On intelligence", "author": ["J. Hawkins", "S. Blakeslee"], "venue": "Macmillan", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Imageto-image translation with conditional adversarial networks", "author": ["P. Isola", "J.-Y. Zhu", "T. Zhou", "A.A. Efros"], "venue": "arXiv preprint arXiv:1611.07004", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "International Conference on Learning Representations", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast faceswap using convolutional neural networks", "author": ["I. Korshunova", "W. Shi", "J. Dambre", "L. Theis"], "venue": "arXiv preprint arXiv:1611.09577", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Transient attributes for high-level understanding and editing of outdoor scenes", "author": ["P.-Y. Laffont", "Z. Ren", "X. Tao", "C. Qian", "J. Hays"], "venue": "ACM Transactions on Graphics (TOG), 33(4):149", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "Proceedings of International Conference on Computer Vision (ICCV)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": "arXiv preprint arXiv:1411.1784", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised learning with generative adversarial networks", "author": ["A. Odena"], "venue": "arXiv preprint arXiv:1606.01583", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "author": ["A. Odena", "C. Olah", "J. Shlens"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Krahenbuhl", "J. Donahue", "T. Darrell", "A.A. Efros"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial text to image synthesis", "author": ["S. Reed", "Z. Akata", "X. Yan", "L. Logeswaran", "B. Schiele", "H. Lee"], "venue": "ICML", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Data-driven hallucination of different times of day from a single outdoor photo", "author": ["Y. Shih", "S. Paris", "F. Durand", "W.T. Freeman"], "venue": "ACM Transactions on Graphics (TOG), 32(6):200", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised crossdomain image generation", "author": ["Y. Taigman", "A. Polyak", "L. Wolf"], "venue": "arXiv preprint arXiv:1611.02200", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["J.-Y. Zhu", "P. Kr\u00e4henb\u00fchl", "E. Shechtman", "A.A. Efros"], "venue": "European Conference on Computer Vision, pages 597\u2013613. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "face swapping [9], changing time of day for outdoor image [19], changing weather of outdoor image [10], comFigure 1.", "startOffset": 14, "endOffset": 17}, {"referenceID": 18, "context": "face swapping [9], changing time of day for outdoor image [19], changing weather of outdoor image [10], comFigure 1.", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "face swapping [9], changing time of day for outdoor image [19], changing weather of outdoor image [10], comFigure 1.", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "posed photo from sketch [3], but each of these tasks has been tackled with separate, special-purpose machinery.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "Fortunately, based on Generative Adversarial Networks (GANs) [5], a study [7] develop a common framework suitable for different problems, the generator in [7] is an encoder-decoder network which tries to synthesize fake image conditioned on a given image to fool discriminator, while the discriminator tries to identify the fake image by comparing with the corresponding target image.", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "Fortunately, based on Generative Adversarial Networks (GANs) [5], a study [7] develop a common framework suitable for different problems, the generator in [7] is an encoder-decoder network which tries to synthesize fake image conditioned on a given image to fool discriminator, while the discriminator tries to identify the fake image by comparing with the corresponding target image.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "Fortunately, based on Generative Adversarial Networks (GANs) [5], a study [7] develop a common framework suitable for different problems, the generator in [7] is an encoder-decoder network which tries to synthesize fake image conditioned on a given image to fool discriminator, while the discriminator tries to identify the fake image by comparing with the corresponding target image.", "startOffset": 155, "endOffset": 158}, {"referenceID": 19, "context": "Recently, another work [20] demonstrated an unsupervised approach for image-to-image translation, the training images do not need to have a specific input and target.", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": "Therefore, it can also be used for unsupervised domain adaption [2, 4].", "startOffset": 64, "endOffset": 70}, {"referenceID": 3, "context": "Therefore, it can also be used for unsupervised domain adaption [2, 4].", "startOffset": 64, "endOffset": 70}, {"referenceID": 5, "context": "According to one learning algorithm hypothesis, popularized by Jeff Hawkins [6], a universal learning machine is a powerful and general model for intelligent agents like human, in which the same network structure can serve for different learning purposes.", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "A generative adversarial network (GAN) consists of a generator G and a discriminator D [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 16, "context": "Following this, deep convolutional GAN [17] learns to synthesize images from unlabeled data, then an image can be represented by a fixed number of latent variables, and each of variable reflects a certain feature [17] e.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "Following this, deep convolutional GAN [17] learns to synthesize images from unlabeled data, then an image can be represented by a fixed number of latent variables, and each of variable reflects a certain feature [17] e.", "startOffset": 213, "endOffset": 217}, {"referenceID": 17, "context": "Following this, mass of studies show that images can be synthesized based on conditions such as sentence [18], class label [13], image inpainting [16] and image manipulation by constraints [21] etc.", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "Following this, mass of studies show that images can be synthesized based on conditions such as sentence [18], class label [13], image inpainting [16] and image manipulation by constraints [21] etc.", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "Following this, mass of studies show that images can be synthesized based on conditions such as sentence [18], class label [13], image inpainting [16] and image manipulation by constraints [21] etc.", "startOffset": 146, "endOffset": 150}, {"referenceID": 20, "context": "Following this, mass of studies show that images can be synthesized based on conditions such as sentence [18], class label [13], image inpainting [16] and image manipulation by constraints [21] etc.", "startOffset": 189, "endOffset": 193}, {"referenceID": 14, "context": "To synthesize image conditioned on a given class label c, auxiliary classifier GAN [15] demonstrate how to synthesize high quality image by using a new structure and loss function.", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "In the generator G, input the class label c and the noise vector z, then a deconvolutional network [17] performs image generation conditioned on both class label c and the noise vector z.", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "The discriminator is a convolutional network, which outputs the class label for the training data, that except requiring generate plausible image, the generator also trained to maximize the log-likelihood of the synthesize image to the correct class [14].", "startOffset": 250, "endOffset": 254}, {"referenceID": 20, "context": "In [21], author introduced a method to train a feedforward network for projecting an image to latent variables.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "The one learning algorithm is a concept in machine learning and neuroscience that has been studied extensively by Je Hawkins [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 14, "context": "In first step, as the left hand side of Figures 2 shows, we used auxiliary classifier GAN [15] to learn the global shared features of the images sampled from different domains.", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "The idea is based on the fact that class-independent information contains global structure about the synthesized image [15].", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "Conditional GANs usually use an embedding layer [18], we found same performance between embedding layer and one-hot format input.", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "If there are increasing size of domains (along with their relationships as word2vec [12]), such embedding layer can contribute more to the performance, compared with one-hot format.", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "Instead of applying trained generator G on top of image encoder E [21] and except that the output image looks the same with the input image, we introduce a new method that applies E after the G from the first step as the middle of Figure 2 shows.", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "The celebA face dataset [11] contains 84434 male images and 118165 female images, which has variety of backgrounds and faces.", "startOffset": 24, "endOffset": 28}, {"referenceID": 14, "context": "As in auxiliary classifier GAN [15], we used the learning rate of", "startOffset": 31, "endOffset": 35}, {"referenceID": 7, "context": "0002, and used the Adam optimizer [8] with momentum of 0.", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "The implementation was built with TensorLayer2 library on top of TensorFlow [1].", "startOffset": 76, "endOffset": 79}], "year": 2017, "abstractText": "It\u2019s useful to automatically transform an image from its original form to some synthetic form (style, partial contents, etc.), while keeping the original structure or semantics. We define this requirement as the \u201dimage-to-image translation\u201d problem, and propose a general approach to achieve it, based on deep convolutional and conditional generative adversarial networks (GANs), which has gained a phenomenal success to learn mapping images from noise input since 2014. In this work, we develop a two step (unsupervised) learning method to translate images between different domains by using unlabeled images without specifying any correspondence between them, so that to avoid the cost of acquiring labeled data. Compared with prior works, we demonstrated the capacity of generality in our model, by which variance of translations can be conduct by a single type of model. Such capability is desirable in applications like bidirectional translation", "creator": "LaTeX with hyperref package"}}}