{"id": "1701.02870", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "Context-aware Captions from Context-agnostic Supervision", "abstract": "We introduce a technique to produce discriminative context-aware image captions (captions that describe differences between images or visual concepts) using only generic context-agnostic training data (captions that describe a concept or an image in isolation). For example, given images and captions of \"siamese cat\" and \"tiger cat\", our system generates language that describes the \"siamese cat\" in a way that distinguishes it from \"tiger cat\". We start with a generic language model that is context-agnostic and add a listener to discriminate between closely-related concepts. Our approach offers two key advantages over previous work: 1) our listener does not need separate training, and 2) allows joint inference to decode sentences that satisfy both the speaker and listener -- yielding an introspective speaker. We first apply our introspective speaker to a justification task, i.e. to describe why an image contains a particular fine-grained category as opposed to another closely related category in the CUB-200-2011 dataset. We then study discriminative image captioning to generate language that uniquely refers to one out of two semantically similar images in the COCO dataset. Evaluations with discriminative ground truth for justification and human studies for discriminative image captioning reveal that our approach outperforms baseline generative and speaker-listener approaches for discrimination.", "histories": [["v1", "Wed, 11 Jan 2017 07:42:58 GMT  (8500kb,D)", "http://arxiv.org/abs/1701.02870v1", "16 pages, 10 figures"], ["v2", "Tue, 20 Jun 2017 08:59:56 GMT  (8699kb,D)", "http://arxiv.org/abs/1701.02870v2", "Accepted to CVPR 2017"], ["v3", "Mon, 31 Jul 2017 23:29:36 GMT  (8699kb,D)", "http://arxiv.org/abs/1701.02870v3", "Accepted to CVPR 2017 (Spotlight)"]], "COMMENTS": "16 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["ramakrishna vedantam", "samy bengio", "kevin murphy", "devi parikh", "gal chechik"], "accepted": false, "id": "1701.02870"}, "pdf": {"name": "1701.02870.pdf", "metadata": {"source": "CRF", "title": "Context-aware Captions from Context-agnostic Supervision", "authors": ["Ramakrishna Vedantam", "Virginia Tech", "Samy Bengio", "Kevin Murphy", "Gal Chechik"], "emails": ["vrama91@vt.edu", "bengio@google.com", "kpmurphy@google.com", "parikh@gatech.edu", "gal@google.com"], "sections": [{"heading": "1. Introduction", "text": "In fact, most people who are able to determine themselves are not able to decide what they want and what they want to do."}, {"heading": "2. Related Work", "text": "In recent years, overcapacity has become apparent, not only in the US, but also in other parts of the world."}, {"heading": "3. Approach", "text": "We will now describe our approach to creating a discriminatory language that describes an image of a target class in relation to its context for two tasks: justification, in which the context consists of another class, and discriminatory subtitling, in which the context consists of a semantically similar image. For the sake of simplicity, we will first describe the wording for justification, and then discuss a simple modification to apply it to discriminatory subtitling. In the justification task (fig. 1 above), we would like to create a discriminatory sentence consisting of a sequence of words {si}, based on a certain image I of a target term ct in the context of a distract term cd. Note that images of the distractor class are not available to the algorithm. The discriminatory sentence (justification) produced should capture aspects of the image that distinguish between the target and the distract concepts. We will first form a generic context-agnostic image 38, which Reed refers to as a better image caption model for access (28)."}, {"heading": "3.1. Reasoning Speaker", "text": "In order to create discrimination in the utterances of a speech model, it is natural to consider the use of a generator or speaker who models p (s | I, ct) in conjunction with a listener function f (s, ct, cd), which evaluates how discriminatory an utterance is. Thus, the task of a pragmatic speaker RS is to select utterances that are good sentences according to the generative model p and discriminatory per f: RS (I, ct, cd) = arg max s suffp (s | I, ct) + (1 \u2212 \u03bb) f (s, ct, cd) (1), where 0 \u2264 1 controls the compromise between linguistic appropriateness of the sentence and discriminability. A similar model of the speaker forms the core of the approach of [2], where p and f can be implemented by means of multi-layered perception (MLPs)."}, {"heading": "3.2. Introspective Speaker", "text": "Next, we explain our model for the inclusion of contextual behavior, based on a simple modification of the listener f (Eq. 1). In view of generator p, we construct a listener module to distinguish between ct, and cd, using the following log-livelihood ratio: f (s, ct, cd) = log p (s | ct, I) p (s | cd, I). (2) This listener depends only on a generative model, p (s | c, I), for the two classes, and cd. We call it \"introspective\" to emphasize that this step reuses the generative model and does not form an explicit listener model."}, {"heading": "3.3. Emitter-Suppressor (ES) Beam Search for RNNs", "text": "We will now describe a search algorithm for implementing the maximization in Eq. 3, which we call emitter suppressor (ES) beam search. We will use the beam search algorithm [22], which is a heuristic graph search algorithm commonly used for conclusions in recursive neural networks [16, 35]. We will first factorize the posterior log probability terms in the introspective speaker equation (Eq. 3) p (s | ct, I) = V-sentences commonly used for conclusions in recursive neural networks [16, 35]. We will first consider the posterior log probability terms in the introspective speaker equation (s1: 0 corresponds to a zero string) p (s | ct, I) = V-sentences (s1: s1, ct, I). We will then combine terms from Eq."}, {"heading": "3.4. Discriminative Image Captioning", "text": "In the task of discriminatory caption, we obtain two images It (target image) and Id (distractor image), which we would like to distinguish between, similar to the two classes for the justification task. We construct a loudspeaker (or generator) for this task by training an image caption model that generates a sentence conditioned on the image. Considering this speaker, we construct an emitter-suppressor equation (as in Equation 4): \"(It, Id) = arg max s T ently = 1 Logemitter p (s2, It) p (s2, Id) p (s2, Id) 1 Suppressor. (5) We use the mechanics of emitter-suppressor beam search from Sec. 3.3, which conditions the emitter to the target image It and the suppressor to the distractor image Id."}, {"heading": "4. Experimental Setup", "text": "We first discuss the data set and setup of the speaker training for the justification task. Afterwards, we explain our new CUBJustify data set for the evaluation of the justification task. Finally, we discuss the experimental protocols for the discriminatory subtitling task."}, {"heading": "4.1. Justification", "text": "\"We have a similar image to the other species of birds.\" \"This is a white spotted bird with a long black beak.\" \"This is a white spotted bird with a long black beak.\" \"This is a white spotted bird with a long black beak.\" \"This is a white spotted bird with a long black beak.\" \"We initially dated a subset of (about) 15 images from the CUB-200-2011 test group.\" \"We are collecting a new dataset (CUBJustify) with down-to-earth justifications to evaluate our model.\" \"We first select a subset of (about) 15 images from the CUB-200-2011 test group.\" For each of the 200 classes, we form a test group, and use the rest for speaker training. \"\" We then use the target and distractor classes from a hypercategory based on the folklore names of the birds."}, {"heading": "4.2. Discriminative Image Captioning", "text": "To construct a series of confusing image pairs, we follow two strategies. First, simple confusion: For each image in the validation (test) set, we find its closest neighbor in the FC7 room of a pre-trained VGG-16 CNN [31], and repeat this process of neighborhood identification for 1000 randomly selected source images. Second, hard confusion: To limit ourselves further to a list of semantically similar confusing images, we then run the loudspeaker model on the closest neighbor images and calculate worded overlaps (cross section over union) of their generated sentences. Interestingly, the captions for the two images are the same for 539 out of 5,000 pairs of the next confusing images (determined by exact match), reflecting the output of caption models without diversity and seemingly seductive [8, 36]."}, {"heading": "5. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Justification", "text": "Methods and basics: We evaluate the following models: 1. IS (\u03bb): Introspective speaker from Eq. 3; 2. IS (1): standard literal speaker, which generates a caption tied to the image and the target class, but which potentially ignores the distractor class; 3. semi-blind IS (\u03bb): introspective samples, in which the listener does not have access to the image, but the speaker does; 4. blind-IS (\u03bb): Introspective speaker without access to images, conditional only on classes; 5. RS (\u03bb): Our implementation of the approach [2], but using our (more powerful) language model, and Eq. 3 with a listener modeling models p (s | cd) (similar to semi-blind IS) for ranking samples (as opposed to a trained MLP [2], in order to stay thingscomparable."}, {"heading": "5.2. Discriminative Image Captioning", "text": "As explained in Section 4.2, we create two sets of semantically similar target and deflection images: simple confusion based on FC7 characteristics alone and hard confusion based on FC7 and sentences generated by the speaker (caption model). We are interested in understanding whether emitter-suppressor inferences help better identify the target image than the generative speaker base. Therefore, the two approaches are Speaker (S) (baseline) and Introspective Speaker (IS) (our approach). We use \u03bb = 0.3 based on our results on the CUB dataset. We perform both approaches at a beam size of 2, which is typically best for the caption on COCO [17]. Human studies: We set up a two-step forced selection (2AFC), where we show an image caption to raters with a request to \"select an image that is considered to be more likely to cause a discordant result.\""}, {"heading": "6. Discussion", "text": "There are many interesting directions to explore in order to generate context-conscious captions from context-nostic surveillance, including describing the absence of concepts (\"That's a dog, not a cat, because there are no whiskers\") and learning comparative language skills (\"The giraffe is bigger than the horse.\") Beyond pragmatism, the justification task also has interesting relationships to human learning. In fact, we all experience that we learn better when someone takes the time to justify or explain their point of view. One can imagine that such justifications are helpful for \"machine teaching,\" when a teacher (machine) can provide justification for a human learner who explains the rationale for an image that belongs to a particular fine-grained category, as opposed to another, possibly erroneous or confusing fine-grained category.Our approach to formulate context-conscious image descriptions from a contextagnostic perspective has certain limitations."}, {"heading": "7. Conclusion", "text": "We present a new technique for deriving pragmatic language from recurring neural network language models, a captioning model that takes into account the context of a distraction class or distraction image. Our technique can be used at a specific time to better distinguish concepts without having seen discriminatory training data. We examine two tasks in the field of vision and language that require pragmatic thinking: justification - which explains why an image belongs to one category and another, and discriminatory captions - the description of an image so that it can be distinguished from a closely related image. Our experiments demonstrate the strength of our method against generative baselines, as well as adaptations of previous work to our environment. We make the code and data sets available online."}, {"heading": "Acknowledgements", "text": "We thank David Rolnick for his invaluable expertise, both as a birdwatcher who led various design decisions for the justification task, and for brainstorming on the technical details of this work. We thank Tom Duerig for his support and guidance in designing this project. We thank Vahid Kazemi for his help with the user interface for the CUB Justify data collection, Bharadwaja Ghali for his help in collecting CUB-Justify, and Ashwin Kalyan for their valuable input at various stages of this work. This work was partially funded by an NSF CAREER Prize, ONR YIP Prize, Sloan Fellowship, Appinguished Investigator Award, Virginia Tech for their valuable input at various stages of this work."}, {"heading": "1. COCO Qualitative Results", "text": "COCO Qualitative Examples: Fig. 8 shows higher quality results in discriminating against captions on the hard confusion split of the COCO dataset. Note how our introspective speaker labels (referred to as IS), which explicitly model the context (distract image), are often more discriminatory, which helps to identify the target image more clearly than the base speaker's approach (referred to as S). In the second row, for example, our IS model generates the caption \"a Delta passenger jet flying through a clear blue sky,\" which is a more discriminatory (and more precise) caption than the base caption \"a large passenger jet flying through a blue sky,\" which applies to both the target and the distort image. Effect of increasing distance: We illustrate how the quality of discriminatory captions from the introspective speaker IS (target image) is less relevant than the differentiation approach (9)."}, {"heading": "2. Comparison to previous work on Generating", "text": "This is a kind of distraction from the predicted class and the original image. While Hendricks et al. aim to provide a rationale for a classification, we focus on a related but different problem of conceptual justification. We want to explain why an image contains a target class as opposed to a specific deflection class, while Hendircks et al. why a classifying thought contains a particular class. Unlike the visual explanation task, the justification task requires explicit consideration of the context. We verify this hypothesis by first adapting the work of the [15] justification task by using it as a speaker, and then using the speaker with our approach to construct an introversive speakerm."}, {"heading": "3. Architectures for Show, Attend, and Tell", "text": "It is the only way in which we can convert the class name (1 of 200 classes for CUB) to a1https: / / github.com / karpathy et al. [36] Our changes can be understood as three simple modifications aimed at using class information in the model. We first embed the class name (1 of 200 classes for CUB) in a1https: / / github.com / karpathy et al."}, {"heading": "4. Optimization Details", "text": "Our CUB subtitling network is trained using RmProp [33] with a batch size of 32 and a learning rate of 0.001. We have lowered the learning rate to all 5 cycling eras through training data. Our word E embeds words in a 512-dimensional vector and we set the LSTM hide and cell state variables (h0, c0) to 1800, similar to the \"show, attack, and tell\" model on COCO. The rest of our design decisions largely reflect the original work of [40] based on their implementation at https: / / github.com / kelvinxu / arctic-captions. We will make our tensorflow implementation of Show, Attend, and Tell publicly available."}, {"heading": "5. Metrics for Justification", "text": "In this section, we will deepen our discussion of the choice of metrics to evaluate justification (ICE value) by 0.1. In addition to the metrics we report in the paper, namely CIDEr-D [34] and METEOR [5], we are also considering the use of the recently introduced SPICE [1]. SPICE metrics uses a dependency saver to extract a scenario chart representation for the candidate and reference sets, and calculates an F-measure between the scenario charts. Given that the metric uses a dependency saver as an intermediate step, it is unclear how well it would scale our justification task: some of the sentences from our model may be good justifications, but not exactly grammatical. This is because our discriminatory justifications occur as a result of a compromise between highly probable propositions and discrimination (Eq. 3)."}, {"heading": "6. CUB-Justify Dataset Interface", "text": "We presented the workers with a target image from a selected target class, along with a set of six deflection images, all of which belonged to a different deflection class, and the deflection images were randomly selected from the validation and test division of the CUB data set we created to justify it. Lay people are unlikely to be given an explicit visual model of a particular deflection category, Indigo Bunting says. Thus, the deflection images were shown to justify the concept of the deflection class. As explained in Section 4.1, the choice of the deflection classes is based on the hierarchy we create using the popular names of the birds. In view of the target class and the deflection class images, the workers were asked to describe the target image in such a way that the sentence is not confusing the deflection images. Furthermore, the workers were instructed that anyone reading the sentence explicitly should be able to distinguish the deflection class from the target image (so that the image was unique)."}, {"heading": "7. Reasoning Speaker Performance Analysis", "text": "In this section, we explain in more detail how the performance of our Andreas and Klein adaptation [2], namely the RS (\u03bb) approach, differs when we take the number of samples from the model for \u03bb = 0.3, \u03bb = 0.5 and \u03bb = 0.7. Interestingly, we note that the RS (\u03bb) approach for \u03bb = 0.5 approaches the best performance of our IS (\u03bb) approach, since we get 100 samples from the model (Fig. 11). Interestingly, our IS (\u03bb) model will only work with a beam size of 10. Therefore, our model is able to search for discriminatory rates more efficiently than a sample and reranking-based approach like RS (\u03bb). It is easy to conclude that if we were willing to spend time enumerating exponentially on all sets, we would find the optimal solution in the worst case - the most approximate inference techniques in such a setting offer an optimum time versus optimum."}], "references": [{"title": "Spice: Semantic propositional image caption evaluation", "author": ["P. Anderson", "B. Fernando", "M. Johnson", "S. Gould"], "venue": "CoRR, abs/1607.08822,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Reasoning about pragmatics with neural listeners and speakers", "author": ["J. Andreas", "D. Klein"], "venue": "EMNLP,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": "pages 65\u201372,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "A computational account of comparative implicatures for a spoken dialogue agent", "author": ["L. Benotti", "D.R. Traum"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Simultaneous active learning of classifiers & attributes via relative feedback", "author": ["A. Biswas", "D. Parikh"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploring nearest neighbor approaches for image captioning", "author": ["J. Devlin", "S. Gupta", "R.B. Girshick", "M. Mitchell", "C.L. Zitnick"], "venue": "CoRR, abs/1505.04467,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CoRR, abs/1411.4389,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F.N. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "CoRR, abs/1411.4952,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Compact bilinear pooling", "author": ["Y. Gao", "O. Beijbom", "N. Zhang", "T. Darrell"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences, 112(12):3618\u20133623,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Logic and conversation", "author": ["H. Grice"], "venue": "Syntax and Semantics. Academic Press,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1975}, {"title": "Generating visual explanations", "author": ["L.A. Hendricks", "Z. Akata", "M. Rohrbach", "J. Donahue", "B. Schiele", "T. Darrell"], "venue": "ECCV,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9:1735\u20131780,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Referitgame: Referring to objects in photographs of natural scenes", "author": ["S. Kazemzadeh", "V. Ordonez", "M. Matten", "T.L. Berg"], "venue": "EMNLP,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["J. Li", "M. Galley", "C. Brockett", "J. Gao", "W.B. Dolan"], "venue": "HLT-NAACL,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S.J. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "The harpy speech understanding system", "author": ["B.T. Lowerre", "D.R. Reddy"], "venue": "Trends in Speech Recognition,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1980}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Generation and comprehension of unambiguous object", "author": ["J. Mao", "J. Huang", "A. Toshev", "O. Camburu", "A.L. Yuille", "K. Murphy"], "venue": "descriptions. 2016", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Modeling context between objects for referring expression understanding", "author": ["V.K. Nagaraja", "V.I. Morariu", "L.S. Davis"], "venue": "ECCV,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Attributes for classifier feedback", "author": ["A. Parkash", "D. Parikh"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Learning deep representations of fine-grained visual descriptions", "author": ["S. Reed", "Z. Akata", "B. Schiele", "H. Lee"], "venue": "IEEE Computer Vision and Pattern Recognition,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Image description with a goal: Building efficient discriminating expressions for images", "author": ["A. Sadovnik", "Y.-I. Chiu", "N. Snavely", "S. Edelman", "T. Chen"], "venue": "CVPR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Not everybody\u2019s special: Using neighbors in referring expressions with uncertain attributes", "author": ["A. Sadovnik", "A.C. Gallagher", "T. Chen"], "venue": "CVPR Workshops,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Asking for help using inverse semantics", "author": ["S. Tellex", "R.A. Knepper", "A. Li", "D. Rus", "N. Roy"], "venue": "Robotics: Science and Systems,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Diverse beam search: Decoding diverse solutions from neural sequence models", "author": ["A.K. Vijayakumar", "M. Cogswell", "R.R. Selvaraju", "Q. Sun", "S. Lee", "D.J. Crandall", "D. Batra"], "venue": "CoRR, abs/1610.02424,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CoRR, abs/1411.4555,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Emergence of gricean maxims from multi-agent decision theory", "author": ["A. Vogel", "M. Bodoia", "C. Potts", "D. Jurafsky"], "venue": "HLT- NAACL,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "The Caltech-UCSD Birds-200-2011 Dataset", "author": ["C. Wah", "S. Branson", "P. Welinder", "P. Perona", "S. Belongie"], "venue": "Technical Report CNS-TR-2011-001, California Institute of Technology,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning language games through interaction", "author": ["S.I. Wang", "P. Liang", "C.D. Manning"], "venue": "CoRR, abs/1606.02447,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "J.R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual madlibs: Fill in the blank description generation and question answering", "author": ["L. Yu", "E. Park", "A.C. Berg", "T.L. Berg"], "venue": "ICCV,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual7W: Grounded Question Answering in Images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C.L. Zitnick", "D. Parikh"], "venue": "CVPR,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "At a high level, we are motivated by similar considerations as recent work by Andreas, and Klein [2].", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "Furthermore, while [2] was applied to an abstract scene dataset [43], we apply our model to two qualitatively different real-image datasets: the fine-grained birds dataset CUB-200-2011 [38], and the COCO [21] dataset which contains real-life scenes with common objects.", "startOffset": 19, "endOffset": 22}, {"referenceID": 39, "context": "Furthermore, while [2] was applied to an abstract scene dataset [43], we apply our model to two qualitatively different real-image datasets: the fine-grained birds dataset CUB-200-2011 [38], and the COCO [21] dataset which contains real-life scenes with common objects.", "startOffset": 64, "endOffset": 68}, {"referenceID": 34, "context": "Furthermore, while [2] was applied to an abstract scene dataset [43], we apply our model to two qualitatively different real-image datasets: the fine-grained birds dataset CUB-200-2011 [38], and the COCO [21] dataset which contains real-life scenes with common objects.", "startOffset": 185, "endOffset": 189}, {"referenceID": 18, "context": "Furthermore, while [2] was applied to an abstract scene dataset [43], we apply our model to two qualitatively different real-image datasets: the fine-grained birds dataset CUB-200-2011 [38], and the COCO [21] dataset which contains real-life scenes with common objects.", "startOffset": 204, "endOffset": 208}, {"referenceID": 12, "context": "Pragmatics: The study of pragmatics \u2013 how context influences usage of language, stems from the foundational work of Grice [14] who analyzed how cooperative multiagent linguistic agents could model each others behavior to achieve a common objective.", "startOffset": 122, "endOffset": 126}, {"referenceID": 5, "context": "Consequently, a lot of pragmatics literature has studied higher-level behavior in agents including conversational implicature [6] and the Gricean maxims [37].", "startOffset": 126, "endOffset": 129}, {"referenceID": 33, "context": "Consequently, a lot of pragmatics literature has studied higher-level behavior in agents including conversational implicature [6] and the Gricean maxims [37].", "startOffset": 153, "endOffset": 157}, {"referenceID": 35, "context": "Other works model ideas from pragmatics to learn language via games played online [39] or for human-robot collaboration [32].", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "Other works model ideas from pragmatics to learn language via games played online [39] or for human-robot collaboration [32].", "startOffset": 120, "endOffset": 124}, {"referenceID": 1, "context": "Most relevant to our work is the recent work on deriving pragmatic behavior in abstract scenes made with clipart, by Andreas, and Klein [2].", "startOffset": 136, "endOffset": 139}, {"referenceID": 8, "context": "Beyond Image Captioning: Image captioning, the task of generating natural language description for an image, has seen quick progress [10, 11, 36, 40].", "startOffset": 133, "endOffset": 149}, {"referenceID": 9, "context": "Beyond Image Captioning: Image captioning, the task of generating natural language description for an image, has seen quick progress [10, 11, 36, 40].", "startOffset": 133, "endOffset": 149}, {"referenceID": 32, "context": "Beyond Image Captioning: Image captioning, the task of generating natural language description for an image, has seen quick progress [10, 11, 36, 40].", "startOffset": 133, "endOffset": 149}, {"referenceID": 36, "context": "Beyond Image Captioning: Image captioning, the task of generating natural language description for an image, has seen quick progress [10, 11, 36, 40].", "startOffset": 133, "endOffset": 149}, {"referenceID": 2, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 104, "endOffset": 119}, {"referenceID": 11, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 104, "endOffset": 119}, {"referenceID": 20, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 104, "endOffset": 119}, {"referenceID": 38, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 104, "endOffset": 119}, {"referenceID": 16, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 153, "endOffset": 169}, {"referenceID": 21, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 153, "endOffset": 169}, {"referenceID": 22, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 153, "endOffset": 169}, {"referenceID": 27, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 153, "endOffset": 169}, {"referenceID": 37, "context": "Recently, research has shifted beyond image captioning, addressing tasks like visual question answering [3, 13, 23, 42], referring expression generation [19, 24, 25, 30], and fill-in-the-blanks [41].", "startOffset": 194, "endOffset": 198}, {"referenceID": 26, "context": "[29] first studied a discriminative image description task, with the goal of distinguishing one image from a set of images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "The reference game from [2] can also be seen as a discriminative", "startOffset": 24, "endOffset": 27}, {"referenceID": 21, "context": "[24] generates discriminative captions which refer to particular objects in an image given context-aware supervision.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Rationales: Several works have studied how machines can understand human rationales, including enriching classification by asking explanations from humans [9], and incorporating human rationales in active learning [7, 26].", "startOffset": 214, "endOffset": 221}, {"referenceID": 23, "context": "Rationales: Several works have studied how machines can understand human rationales, including enriching classification by asking explanations from humans [9], and incorporating human rationales in active learning [7, 26].", "startOffset": 214, "endOffset": 221}, {"referenceID": 13, "context": "Other recent work [15] looks at post-hoc explanations for classification decisions.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "Instead of explaining why a model thinks an image is a particular class, [15] describes why an image is of a class predicted by the classifier.", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "We show a comparison to [15] in the appendix, demonstrating the importance of context for justification.", "startOffset": 24, "endOffset": 28}, {"referenceID": 31, "context": "This is similar in spirit to recent works on inducing diversity in beam search [35], and maximum mutual information inference for sequence-to-sequence models [20].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "This is similar in spirit to recent works on inducing diversity in beam search [35], and maximum mutual information inference for sequence-to-sequence models [20].", "startOffset": 158, "endOffset": 162}, {"referenceID": 25, "context": "[28] who collected captions describing bird images on the CUB-200-2011 [38] dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[28] who collected captions describing bird images on the CUB-200-2011 [38] dataset.", "startOffset": 71, "endOffset": 75}, {"referenceID": 32, "context": "Our language models are recurrent neural networks which represent the state-of-the-art for language modeling across a range of popular tasks like image captioning [36, 40], machine translation [4] etc.", "startOffset": 163, "endOffset": 171}, {"referenceID": 36, "context": "Our language models are recurrent neural networks which represent the state-of-the-art for language modeling across a range of popular tasks like image captioning [36, 40], machine translation [4] etc.", "startOffset": 163, "endOffset": 171}, {"referenceID": 3, "context": "Our language models are recurrent neural networks which represent the state-of-the-art for language modeling across a range of popular tasks like image captioning [36, 40], machine translation [4] etc.", "startOffset": 193, "endOffset": 196}, {"referenceID": 1, "context": "A similar model of the reasoning speaker forms the core of the approach of [2], where p, and f are implemented using multi-layer perceptrons (MLPs), for contextual reasoning on abstract images.", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "As noted in [2], selecting utterances from such a reasoning speaker poses several challenges.", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "Thus, Andreas, and Klein [2] adopt a sampling based strategy, where p is considered as the proposal distribution whose samples are ranked by a linear combination of p, and f (Eq.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "A careful inspection of the introspective speaker model reveals two desirable properties over previous work [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 19, "context": "We use the beam search [22] algorithm, which is a heuristic graph-search algorithm commonly used for inference in Recurrent Neural Networks [16, 35].", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "We use the beam search [22] algorithm, which is a heuristic graph-search algorithm commonly used for inference in Recurrent Neural Networks [16, 35].", "startOffset": 140, "endOffset": 148}, {"referenceID": 31, "context": "We use the beam search [22] algorithm, which is a heuristic graph-search algorithm commonly used for inference in Recurrent Neural Networks [16, 35].", "startOffset": 140, "endOffset": 148}, {"referenceID": 34, "context": "CUB dataset: The Caltech UCSD birds (CUB) dataset [38] contains 11788 images for 200 species of North American birds.", "startOffset": 50, "endOffset": 54}, {"referenceID": 25, "context": "[28].", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": ") 15 images from the test set of the CUB-200-2011 dataset [38] for each of the 200 classes to form a test set, and use the rest for speaker training.", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "[28]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[40], modifying the original model to also provide the class as input, similar in spirit to [15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[40], modifying the original model to also provide the class as input, similar in spirit to [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 25, "context": "Recall that this just has context-agnostic captions from [28].", "startOffset": 57, "endOffset": 61}, {"referenceID": 30, "context": "To evaluate the quality of our speaker model, we report numbers here using the CIDEr-D metric [34] commonly used for image captioning [18, 36] computed on the context-agnostic captions from [28].", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "To evaluate the quality of our speaker model, we report numbers here using the CIDEr-D metric [34] commonly used for image captioning [18, 36] computed on the context-agnostic captions from [28].", "startOffset": 134, "endOffset": 142}, {"referenceID": 32, "context": "To evaluate the quality of our speaker model, we report numbers here using the CIDEr-D metric [34] commonly used for image captioning [18, 36] computed on the context-agnostic captions from [28].", "startOffset": 134, "endOffset": 142}, {"referenceID": 25, "context": "To evaluate the quality of our speaker model, we report numbers here using the CIDEr-D metric [34] commonly used for image captioning [18, 36] computed on the context-agnostic captions from [28].", "startOffset": 190, "endOffset": 194}, {"referenceID": 13, "context": "The scores are in a simlar ballpark to [15], although our numbers are not exactly comparable, given we train on a different split of the CUB dataset.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "There is however a key difference: we condition the model on the ground truth class since we are eventually interested in discriminative justifications between concepts, while [15] conditions the model on the predicted class, since they are interested in explanation of a classification decision.", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "Justification evaluation: We measure performance of the (context-aware) justification captions on the CUB-Justify test set using the CIDEr-D metric, similar to previous work on evaluating image captions of the CUB dataset [15].", "startOffset": 222, "endOffset": 226}, {"referenceID": 28, "context": "First, easy confusion: For each image in the validation (test) set, we find its nearest neighbor in the FC7 space of a pre-trained VGG-16 CNN [31], and repeat this process of neighbor finding for 1000 randomly chosen source images.", "startOffset": 142, "endOffset": 146}, {"referenceID": 7, "context": "This reflects the issue of the output of image captioning models lacking diversity, and seeming templated [8, 36].", "startOffset": 106, "endOffset": 113}, {"referenceID": 32, "context": "This reflects the issue of the output of image captioning models lacking diversity, and seeming templated [8, 36].", "startOffset": 106, "endOffset": 113}, {"referenceID": 32, "context": "Speaker Training and Evaluation: We train our generative speaker for use in emitter-suppressor beam search using the model from [36] implemented in the neuraltalk2 project (github.", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "We use the train/val/test splits from [18].", "startOffset": 38, "endOffset": 42}, {"referenceID": 1, "context": "RS(\u03bb): Our implementation of the approach [2], but using our (more powerful) language model, and Eq.", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "3 with a listener that models p(s|ct) p(s|cd) (similar to semi-blind-IS(\u03bb)) for ranking samples (as opposed to a trained MLP [2], to keep things 0 4 8 12 16 20", "startOffset": 125, "endOffset": 128}, {"referenceID": 13, "context": "2: Generating visual explanations [15] adapted to the justification task.", "startOffset": 34, "endOffset": 38}, {"referenceID": 36, "context": "3: Architectural changes to the \u201cShow, Attend, and Tell\u201d image captioning model [40] for justification.", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "Comparison to previous work on Generating Visual Explanations [15]", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "[15] propose a method to explain classification decisions to an end user by providing post-hoc rationalizations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "We verify this hypothesis, by first adapting the work of [15] to our justification task, using it as a speaker, and then augmenting the speaker with our approach to construct an intropsective speakerm which accounts for context.", "startOffset": 57, "endOffset": 61}, {"referenceID": 13, "context": "Interestingly, we find that our introspective speaker approach helps improve the performance of generating visual explanations [15] on justification.", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "[15] differs from our setup in two important ways.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Firstly, uses a stronger CNN, namely the fine-grained compact-bilinear pooling CNN [12] which provides state-of-the-art performance on the CUB dataset.", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": "vis-exp [15] 20.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "Table 3: CUB-Justify test results: We compare vis-exp [15] and our emitter-suppressor beam search implemented on top of vis-exp, namely vis-exp-IS.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "We modify the inference setup of [15] slightly to condition the caption generation on the target class for justification, as opposed to the predicted class for explanation.", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "We then apply the emittersuppressor beam search (at a beam size of 1, to be consistent with [15]) to account for context, giving us an introspective visual explanation model (vis-exp-IS).", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "Note that this CUB-Justify test set is a strict subset of the test set from [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "However, as mentioned before, the approach of [15], similar to a baseline speaker S, cannot explicitly model context from a specific distractor class at inference.", "startOffset": 46, "endOffset": 50}, {"referenceID": 36, "context": "We explain some minor modifications to the \u201cShow, Attend and Tell\u201d [40] image captioning model to condition it on the class label in addition to the image, for our experiments on CUB.", "startOffset": 67, "endOffset": 71}, {"referenceID": 32, "context": "[36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "The three changes then, on top of the Show, Attend, and Tell model [40] are as follows:", "startOffset": 67, "endOffset": 71}, {"referenceID": 36, "context": "1, 2, 3 from [40]): \uf8ec\uf8ec\uf8ed it ft ot gt \uf8f7\uf8f7\uf8f8 = \uf8ec\uf8ec\uf8ed \u03c3 \u03c3", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "\u2022 Adding class information to the deep output layer: \u201cShow, Attend and Tell\u201d uses a deep output layer [27] to compute the output word distribution at every timestep, incorporating signals from the LSTM hidden state ht, context vector \u1e91t and the input word yt:", "startOffset": 102, "endOffset": 106}, {"referenceID": 36, "context": "The rest of our design choices closely mirror the original work of [40], based on their implementation available at https:// github.", "startOffset": 67, "endOffset": 71}, {"referenceID": 30, "context": "In addition to the metrics we report in the paper, namely CIDEr-D [34] and METEOR [5], we also considered using the recently introduced SPICE [1].", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "In addition to the metrics we report in the paper, namely CIDEr-D [34] and METEOR [5], we also considered using the recently introduced SPICE [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "In addition to the metrics we report in the paper, namely CIDEr-D [34] and METEOR [5], we also considered using the recently introduced SPICE [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "In this section, we provide more details on how the performance of our adaptation of Andreas, and Klein [2], namely the RS(\u03bb) approach varies as we sweep over the number of samples we draw from the model for \u03bb = 0.", "startOffset": 104, "endOffset": 107}], "year": 2017, "abstractText": "We introduce a technique to produce discriminative context-aware image captions (captions that describe differences between images or visual concepts) using only generic context-agnostic training data (captions that describe a concept or an image in isolation). For example, given images and captions of \u201csiamese cat\u201d and \u201ctiger cat\u201d, our system generates language that describes the \u201csiamese cat\u201d in a way that distinguishes it from \u201ctiger cat\u201d. We start with a generic language model that is context-agnostic and add a listener to discriminate between closely-related concepts. Our approach offers two key advantages over previous work: 1) our listener does not need separate training, and 2) allows joint inference to decode sentences that satisfy both the speaker and listener \u2013 yielding an introspective speaker. We first apply our introspective speaker to a justification task, i.e. to describe why an image contains a particular fine-grained category as opposed to another closely related category in the CUB-2002011 dataset. We then study discriminative image captioning to generate language that uniquely refers to one out of two semantically similar images in the COCO dataset. Evaluations with discriminative ground truth for justification and human studies for discriminative image captioning reveal that our approach outperforms baseline generative and speaker-listener approaches for discrimination.", "creator": "LaTeX with hyperref package"}}}