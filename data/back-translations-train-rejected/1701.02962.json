{"id": "1701.02962", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "Distinguishing Antonyms and Synonyms in a Pattern-based Neural Network", "abstract": "Distinguishing between antonyms and synonyms is a key task to achieve high performance in NLP systems. While they are notoriously difficult to distinguish by distributional co-occurrence models, pattern-based methods have proven effective to differentiate between the relations. In this paper, we present a novel neural network model AntSynNET that exploits lexico-syntactic patterns from syntactic parse trees. In addition to the lexical and syntactic information, we successfully integrate the distance between the related words along the syntactic path as a new pattern feature. The results from classification experiments show that AntSynNET improves the performance over prior pattern-based methods.", "histories": [["v1", "Wed, 11 Jan 2017 13:11:48 GMT  (89kb,D)", "http://arxiv.org/abs/1701.02962v1", "EACL 2017, 10 pages"]], "COMMENTS": "EACL 2017, 10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kim anh nguyen", "sabine schulte im walde", "ngoc thang vu"], "accepted": false, "id": "1701.02962"}, "pdf": {"name": "1701.02962.pdf", "metadata": {"source": "CRF", "title": "Distinguishing Antonyms and Synonyms in a Pattern-based Neural Network", "authors": ["Kim Anh Nguyen"], "emails": ["nguyenkh@ims.uni-stuttgart.de", "schulte@ims.uni-stuttgart.de", "thangvu@ims.uni-stuttgart.de"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people who are able to understand themselves and their environment put themselves in a situation in which they can no longer help themselves and others, in which they can no longer help themselves, in which they can no longer help themselves, in which they can no longer see themselves, in which they can no longer see themselves, in which they can no longer move, in which they can no longer move, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can not live, in which they can not live, in which they do not want to live, in which they can not live, in which they do not want to do it, in which they do not want to do it, in which they do not want to do it, in which they do not want to do it, in which they can not want to do it, that they do not want to do it, in which they want to do not want to do it, in which they do not want to do it, that they do not want to do it, in which they can not want to do it, that they want to do it, in which they want to do not want to do it, in which they can not want to do it, in which they want to do it, that they do not want to do it, in which they can not want to do it, that they do not want to do not want to do it, that they want to do it in which they want to do it, in which they want to do not want to do it, in which they can not want to do it, that they want to do not want to do it, that they want to do it in which they can not want to do it."}, {"heading": "2 Related Work", "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to play by the rules they have set themselves in order to play by the rules."}, {"heading": "3 AntSynNET: LSTM-based Antonym-Synonym Distinction", "text": "In this section, we describe the AntSynNET model using a pattern-based LSTM to distinguish antinyms from synonyms. First, we present the induction of patterns from an analyzed corpus (Section 3.1), then Section 3.2 describes how to use the recurrent neural network with long-term short-term memory units to encode the patterns as vector representation, and finally, we present the AntSynNET model and two approaches to classifying antinyms and synonyms (Section 3.3)."}, {"heading": "3.1 Induction of Patterns", "text": "Following the characterization of a tree in graph theory, any two nodes (depressions) of a tree are connected by a simple path (or a unique path), which is the shortest path between two nodes in a tree and does not contain repeated nodes. In the example, the lexico-syntactic tree pattern of the antonymous pair is old-redefined by finding the simple path (in red) from the old to the new path."}, {"heading": "3.2 Recurrent Neural Network with Long Short-Term Memory Units", "text": "A recursive neural network (RNN) is suitable for modeling sequential data using a vector representation. In our methods, we use a long-term short-term memory (LSTM) network, a variant of a recursive neural network, to encode patterns for the following reasons. Given a word sequence p = [n1, n2,..., nk] as input data, an RNN processes each word nt at a time and returns a vector of the state hk for the complete input sequence. For each time step, the RNN updates an internal memory state that is dependent on the current input and the previous state \u2212 1. But if sequential input is a long-term dependency, an RNN faces the problem of a gradient disappearing or exploding, which causes difficulties in training the model.LSTM units address these problems."}, {"heading": "3.3 The Proposed AntSynNET Model", "text": "In this section, we present two models for distinguishing antagonyms from synonyms: the first model uses patterns to classify antagonyms and synonyms by encoding patterns as vector representations using an LSTM and then feeding these vectors to a logistic regression layer (Section 3.3.1); the second model generates combined vector representations of word pairs that concatenate the vectors of words and patterns (Section 3.3.2)."}, {"heading": "3.3.1 Pattern-based AntSynNET", "text": "In this model, we use a recurrent neural network with LSTM units to encode patterns that contain a sequence of nodes. Figure 2 illustrates the AntSynNET model. In a word pair (x, y), we induce patterns for (x, y) from a corpus, each pattern representing a path from x to y (cf. Section 3.1), and then feed each pattern p of the word pair (x, y) into an LSTM to get ~ vp, the vector representation of the pattern p (cf. Section 3.2). For each word pair (x, y), the vector representation of (x, y) is calculated as follows: ~ vxy = \u2211 p-P (x, y) ~ vp-p-p (x, y) ~ cp-p (x, y) cp-p (x, y) cp (1) ~ vxy-references the vector of the word pair (x, y); P (x), the pair is assigned to the vxy, the prediction of the pair (x) is the vxy-pattern."}, {"heading": "3.3.2 Combined AntSynNET", "text": "Inspired by the method of supervised distribution concatenation in Baroni et al. (2012) and the integrated path-based and distribution method for hypernymy recognition in Shwartz et al. (2016), we consider the patterns and distribution of the target pairs to create their combined vector representations. In a word pair (x, y), the combined vector representation of the pair (x, y) is determined by both the co-occurrence distribution of the words and the syntactic path patterns: ~ vcomb (x, y) = [~ vx-vxy-vxy-vy] (2) ~ vcomb (x, y) refers to the combined vector of the word pair (x, y); ~ vx and ~ vy are the vectors of the word x and the word y, respectively; ~ vxy is the vector of the pattern corresponding to the pair (x, y), cf."}, {"heading": "4 Baseline Models", "text": "To compare AntSynNET with basic models for pattern-based classification of antonyms and synonyms, we present two pattern-based basic methods: the distribution method (Section 4.1) and the distributed method (Section 4.2)."}, {"heading": "4.1 Distributional Baseline", "text": "As a first starting point, we apply the approach of Roth and Schulte im Walde (2014), which is henceforth called R & SiW: They used a vector space model to represent pairs of words using a combination of standard lexical syntactic patterns and discourse markers; in addition to the patterns, the discourse markers added information to express discourse relationships, which in turn can indicate the specific semantic relationship between the two words in a word pair; for example, contrast relationships might indicate antagonism, while elaborations may indicate synonymy or hyponymy. Michael Roth, the lead author of R & SiW, kindly calculated the classification results of the pattern discourse model for our test sets: the weights between marker-based and pattern-based models were matched to the validation sets; other hyperparameters were determined exactly as described by the R & SiW method."}, {"heading": "4.2 Distributed Baseline", "text": "The SP method proposed by Schwartz et al. (2015) uses symmetrical patterns to generate word embeddings. In this paper, the authors applied an unattended algorithm to automatically extract symmetrical patterns from plain text. Symmetrical patterns were defined as a sequence of 3-5 characters consisting of exactly two wildcards and 1-3 words. Patterns were filtered by their frequency, so that the resulting pattern set contained 11 patterns. To generate word embeddings, a matrix of coexistence between patterns and words in the vocabulary was calculated using positive point-by-point mutual information. The problem of the rarity of vector representations was addressed by smoothing out. For antonym representation, the authors relied on two patterns proposed by Lin et al. (2003) to construct word embeddings that contain an antonym parameter that can be turned on to represent antagmas similarly."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Dataset", "text": "To form the models, neural networks require a large amount of training data. We use the existing large-area pairs of antonyms and synonyms previously used by Nguyen et al. (2016). Originally, the data pairs were created by WordNet (Miller, 1995) and Wordnik4.To create patterns for the word pairs in the data set, we identify the sentences in the corpus that contain the word pair. Then, we extract all patterns for the word pair. We filter out any pattern that occurs less than five times; and we only consider word pairs that contain at least five patterns for training, validation, and testing. For the proportion of positive and negative pairs, we keep a ratio of 1: 1 positive (antonyms) to negative (synonyms) pairs in the data set. To create the sets of training, test, and validation data, we perform random splits with 70% pull, 25% test, and 5% validation sets by the final word pair of each word set containing the number of the word pairs."}, {"heading": "5.2 Experimental Settings", "text": "We use the English Wikipedia Dump5 of June 2016 as the corpus resource for our methods and baselines. For the analysis of the corpus, we rely on spaCy6. For the embedding of the problem, we rely on the word embedding of dLCE4 http: / / www.wordnik.com 5 https: / / dumps.wikimedia.org / enwiki / latest / enwiki-latest-pages-articles.xml.bz2 6 https: / / spacy.iomodel7 (Nguyen et al., 2016), which is the state of vector representation for distinguishing synonyms. We have re-implemented this groundbreaking model on Wikipedia with 100 dimensions and then use the dLCE word embedding to initialize the Lemma embedding. The embedding of the POS tags, the distance labels, the distance labels and the vocabulary we rely on for the training of the 10."}, {"heading": "5.3 Overall Results", "text": "In terms of adjectives, the two proposed models achieve an improvement of >.06. In terms of nouns, the improvement of the new methods is just mal.02 F1 compared to the 7 https: / / github.com / nguyenkh / AntSynDistinction 8t-Test, * p < 0.05, * p < 0.1R & SiW baseline, but we achieve a much better performance compared to SP baseline, an increase of.37 F1. In terms of verbs, we do not exceed the more advanced R & SiW baseline in terms of the F1 score, but we get higher recall scores. Compared to SP baseline, our models still have a clear amplification."}, {"heading": "5.4 Effect of the Distance Feature", "text": "In our models, the novel distance attribute is successfully integrated along the syntactic path to represent lexico-syntactic patterns. Intuition behind the distance attribute exploits tree properties in graph theory that show that there are differences in the degree of relationship between the parent node and the child nodes (distance = 1) and in the degree of relationship between the ancestor node and the subsequent nodes (distance > 1). Therefore, we use the distance attribute to effectively capture these relationships. To evaluate the effect of our novel distance attribute, we compare the distance attribute with the directional attribute proposed by Shwartz et al. (2016). In their approach, the authors combined Lemma, POS, dependency, and directional attribute for the task of hypernym detection. The directional attribute represented the direction of the dependency label between two nodes in a path from X to Y.For the evaluation, we then replace the same information about the dependency attribute in the 5.3 and section of the word with the two directions."}, {"heading": "5.5 Effect of Word Embeddings", "text": "Our methods are based on the word embeddings of the dLCE model, state-of-the-art word embeddings to differentiate antonym from synonym. However, the word embeddings of the dLCE model, i.e. the supervised word embeddings, represent information gathered from lexical resources. To evaluate the effect of these word embeddings on the performance of our models, we replace them with the pre-formed GloVe word embeddings 9 with 100 dimensions and compare the effects of the GloVe word embeddings and the dLCE word embeddings on the performance of the two proposed models. Table 5 illustrates the performance of our two models in all word classes. The table shows that the dLCE word embeddings are better than the 9 http: / www-nlp.stanford.edu / projects / glove / pre-trained GloVCE word embeddings in order to match the VCE model with the pattern-based NET and VCE model in the VCE synthesis 1."}, {"heading": "6 Conclusion", "text": "In this paper, we presented a novel pattern-based neural method, AntSynNET, to distinguish antinyms from synonyms. We assumed that antonyms are more common in lexical-syntactic patterns within a sentence than synonymous pairs of words. Patterns were derived from the simple paths between semantically related words in a syntactic parse tree. In addition to lexical and syntactic information, we proposed a novel path removal feature. The AntSynNET model consists of two approaches for classifying antinyms and synonyms. In the first approach, we used a recursive neural network with long-term storage units to encode the patterns as vector representations; in the second approach, we used the distribution and coded patterns of the target pairs to create combined vector representations."}, {"heading": "Acknowledgements", "text": "We would like to thank Michael Roth for helping us to calculate the results of the R & SiW model on our dataset, which was supported by the Ministry of Education and Training of the Socialist Republic of Vietnam (Scholarship 977 / QD-BGDDT; KimAnh Nguyen), the DFG Collaborative Research Centre 732 (Kim-Anh Nguyen, Ngoc Thang Vu) and the DFG Heisenberg Scholarship SCHU2580 / 1 (Sabine Schulte im Walde)."}], "references": [{"title": "Entailment above the word level in distributional semantics", "author": ["Baroni et al.2012] Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung chieh Shan"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association for Compu-", "citeRegEx": "Baroni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "The Structure of Associations in Language and Thought", "author": ["James Deese"], "venue": null, "citeRegEx": "Deese.,? \\Q1965\\E", "shortCiteRegEx": "Deese.", "year": 1965}, {"title": "Cooccurrence and antonymy", "author": ["Christiane Fellbaum"], "venue": "International Journal of Lexicography,", "citeRegEx": "Fellbaum.,? \\Q1995\\E", "shortCiteRegEx": "Fellbaum.", "year": 1995}, {"title": "Papers in Linguistics 1934-51", "author": ["John R. Firth"], "venue": null, "citeRegEx": "Firth.,? \\Q1957\\E", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["Marti Hearst"], "venue": "Proceedings of the 14th International Conference on Computational Linguistics (COLING),", "citeRegEx": "Hearst.,? \\Q1992\\E", "shortCiteRegEx": "Hearst.", "year": 1992}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Co-occurrences of antonymous adjectives and their contexts", "author": ["Justeson", "Katz1991] John S. Justeson", "Slava M. Katz"], "venue": "Computational Linguistics,", "citeRegEx": "Justeson et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Justeson et al\\.", "year": 1991}, {"title": "Identifying synonyms among distributionally similar words", "author": ["Lin et al.2003] Dekang Lin", "Shaojun Zhao", "Lijuan Qin", "Ming Zhou"], "venue": "In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: A lexical database for English", "author": ["George A. Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Integrating distributional lexical contrast into word embeddings for antonym-synonym distinction", "author": ["Sabine Schulte im Walde", "Ngoc Thang Vu"], "venue": "In Proceedings of the 54th Annual Meeting", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Word embedding-based antonym detection using thesauri and distributional information", "author": ["Ono et al.2015] Masataka Ono", "Makoto Miwa", "Yutaka Sasaki"], "venue": "In Proceedings of the Conference of the North American Chapter of the Association", "citeRegEx": "Ono et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ono et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Combining word patterns and discourse markers for paradigmatic relation classification", "author": ["Roth", "Sabine Schulte im Walde"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Roth et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2014}, {"title": "Chasing hypernyms in vector spaces with entropy", "author": ["Santus et al.2014] Enrico Santus", "Alessandro Lenci", "Qin Lu", "Sabine Schulte im Walde"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Lin-", "citeRegEx": "Santus et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Uncovering distributional differences between synonyms and antonyms in a word space model", "author": ["Sabine Schulte im Walde", "Sylvia Springorum"], "venue": "In Proceedings of the 6th International Joint Conference on Natu-", "citeRegEx": "Scheible et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Scheible et al\\.", "year": 2013}, {"title": "Pattern-based distinction of paradigmatic relations for german nouns, verbs, adjectives", "author": ["Schulte im Walde", "Maximilian K\u00f6per"], "venue": "In Proceedings of the 25th International Conference of the German Society", "citeRegEx": "Walde et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Walde et al\\.", "year": 2013}, {"title": "Symmetric pattern based word embeddings for improved word similarity prediction", "author": ["Roi Reichart", "Ari Rappoport"], "venue": "In Proceedings of the 19th Conference on Computational Language Learning (CoNLL),", "citeRegEx": "Schwartz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schwartz et al\\.", "year": 2015}, {"title": "Improving hypernymy detection with an integrated path-based and distributional method", "author": ["Yoav Goldberg", "Ido Dagan"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Shwartz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shwartz et al\\.", "year": 2016}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Combining recurrent and convolutional neural networks for relation classification", "author": ["Vu et al.2016] Ngoc Thang Vu", "Heike Adel", "Pankaj Gupta", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Asso-", "citeRegEx": "Vu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vu et al\\.", "year": 2016}, {"title": "Polarity inducing latent semantic analysis", "author": ["Yih et al.2012] Wen-tau Yih", "Geoffrey Zweig", "John C. Platt"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Yih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2012}, {"title": "ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "similar in meaning (Deese, 1965; Lyons, 1977).", "startOffset": 19, "endOffset": 45}, {"referenceID": 3, "context": "Both make use of distributional vector representations, relying on the distributional hypothesis (Harris, 1954; Firth, 1957), that words with similar distributions have related meanings: co-occurrence models and pattern-based models.", "startOffset": 97, "endOffset": 124}, {"referenceID": 8, "context": "gram model (Mikolov et al., 2013), or use matrix factorization (Pennington et al.", "startOffset": 11, "endOffset": 33}, {"referenceID": 12, "context": ", 2013), or use matrix factorization (Pennington et al., 2014) that builds word embeddings by factorizing word-context cooccurrence matrices.", "startOffset": 37, "endOffset": 62}, {"referenceID": 8, "context": "gram model (Mikolov et al., 2013), or use matrix factorization (Pennington et al., 2014) that builds word embeddings by factorizing word-context cooccurrence matrices. In comparison to standard co-occurrence vector representations, word embeddings address the problematic sparsity of word vectors and have achieved impressive results in many NLP tasks such as word similarity (e.g., Pennington et al. (2014)), relation classification (e.", "startOffset": 12, "endOffset": 408}, {"referenceID": 8, "context": "gram model (Mikolov et al., 2013), or use matrix factorization (Pennington et al., 2014) that builds word embeddings by factorizing word-context cooccurrence matrices. In comparison to standard co-occurrence vector representations, word embeddings address the problematic sparsity of word vectors and have achieved impressive results in many NLP tasks such as word similarity (e.g., Pennington et al. (2014)), relation classification (e.g., Vu et al. (2016)), and antonym-synonym distinction (e.", "startOffset": 12, "endOffset": 458}, {"referenceID": 8, "context": "gram model (Mikolov et al., 2013), or use matrix factorization (Pennington et al., 2014) that builds word embeddings by factorizing word-context cooccurrence matrices. In comparison to standard co-occurrence vector representations, word embeddings address the problematic sparsity of word vectors and have achieved impressive results in many NLP tasks such as word similarity (e.g., Pennington et al. (2014)), relation classification (e.g., Vu et al. (2016)), and antonym-synonym distinction (e.g., Nguyen et al. (2016)).", "startOffset": 12, "endOffset": 520}, {"referenceID": 4, "context": "Hearst (1992) determined surface patterns, e.", "startOffset": 0, "endOffset": 14}, {"referenceID": 4, "context": "Hearst (1992) determined surface patterns, e.g., X such as Y, to identify nominal hypernyms. Lin et al. (2003) proposed two textual patterns indicating semantic incompatibility, from X to Y and either X or Y, to distinguish opposites from semantically similar ar X iv :1 70 1.", "startOffset": 0, "endOffset": 111}, {"referenceID": 16, "context": "Recently, Schwartz et al. (2015) used two prominent patterns from Lin et al.", "startOffset": 10, "endOffset": 33}, {"referenceID": 7, "context": "(2015) used two prominent patterns from Lin et al. (2003) to learn word embeddings that distinguished antonyms from similar words in determining degrees of similarity and word analogy.", "startOffset": 40, "endOffset": 58}, {"referenceID": 9, "context": "Among others, Charles and Miller (1989) suggested that ad-", "startOffset": 26, "endOffset": 40}, {"referenceID": 2, "context": "jectival opposites co-occur in patterns; Fellbaum (1995) stated that nominal and verbal opposites co-occur in the same sentence significantly more often than chance; Lin et al.", "startOffset": 41, "endOffset": 57}, {"referenceID": 2, "context": "jectival opposites co-occur in patterns; Fellbaum (1995) stated that nominal and verbal opposites co-occur in the same sentence significantly more often than chance; Lin et al. (2003) argued that if two words appear in clear antonym patterns, they", "startOffset": 41, "endOffset": 184}, {"referenceID": 7, "context": "Lin et al. (2003) used bilingual dependency triples and patterns to extract distributionally similar words.", "startOffset": 0, "endOffset": 18}, {"referenceID": 17, "context": "More recently, Schwartz et al. (2015) presented a symmetric pattern-based model for word vector representation in which antonyms are assigned to dissimilar vector representations.", "startOffset": 15, "endOffset": 38}, {"referenceID": 21, "context": "Vector representation methods: Yih et al. (2012) introduced a new vector representation where antonyms lie on opposite sides of a sphere.", "startOffset": 31, "endOffset": 49}, {"referenceID": 14, "context": "Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "Santus et al. (2014) proposed a different method to distinguish antonyms from synonyms by identifying the most salient dimensions of meaning in vector representations and reporting a new average-precision-based dis-", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "Ono et al. (2015) trained supervised word embeddings for the task of identifying antonymy.", "startOffset": 0, "endOffset": 18}, {"referenceID": 10, "context": "More recently, Nguyen et al. (2016) proposed two methods to distinguish antonyms from synonyms: in the first method, the authors improved the qual-", "startOffset": 15, "endOffset": 36}, {"referenceID": 8, "context": "In the second method, the lexical contrast information was integrated into the skip-gram model (Mikolov et al., 2013) to learn word embeddings.", "startOffset": 95, "endOffset": 117}, {"referenceID": 0, "context": "Inspired by the supervised distributional concatenation method in Baroni et al. (2012) and the in-", "startOffset": 66, "endOffset": 87}, {"referenceID": 18, "context": "tegrated path-based and distributional method for hypernymy detection in Shwartz et al. (2016), we take into account the patterns and distribution of target pairs to create their combined vector representations.", "startOffset": 73, "endOffset": 95}, {"referenceID": 17, "context": "The SP method proposed by Schwartz et al. (2015) uses symmetric patterns for generating word embeddings.", "startOffset": 26, "endOffset": 49}, {"referenceID": 7, "context": "For antonym representation, the authors relied on two patterns suggested by Lin et al. (2003) to construct word embeddings containing an antonym parameter that can be turned on in order to represent antonyms as dissimilar, and that can be turned off to represent antonyms as similar.", "startOffset": 76, "endOffset": 94}, {"referenceID": 9, "context": "Originally, the data pairs were collected from WordNet (Miller, 1995) and Wordnik4.", "startOffset": 55, "endOffset": 69}, {"referenceID": 9, "context": "We use the existing large-scale antonym and synonym pairs previously used by Nguyen et al. (2016). Originally, the data pairs were collected from WordNet (Miller, 1995) and Wordnik4.", "startOffset": 77, "endOffset": 98}, {"referenceID": 10, "context": "model7 (Nguyen et al., 2016) which is the stateof-the-art vector representation for distinguishing antonyms from synonyms.", "startOffset": 7, "endOffset": 28}, {"referenceID": 22, "context": "date rule (Zeiler, 2012).", "startOffset": 10, "endOffset": 24}, {"referenceID": 18, "context": "In order to evaluate the effect of our novel distance feature, we compare the distance feature to the direction feature proposed by Shwartz et al. (2016). In their approach, the authors combined", "startOffset": 132, "endOffset": 154}], "year": 2017, "abstractText": "Distinguishing between antonyms and synonyms is a key task to achieve high performance in NLP systems. While they are notoriously difficult to distinguish by distributional co-occurrence models, pattern-based methods have proven effective to differentiate between the relations. In this paper, we present a novel neural network model AntSynNET that exploits lexico-syntactic patterns from syntactic parse trees. In addition to the lexical and syntactic information, we successfully integrate the distance between the related words along the syntactic path as a new pattern feature. The results from classification experiments show that AntSynNET improves the performance over prior pattern-based methods.", "creator": "LaTeX with hyperref package"}}}