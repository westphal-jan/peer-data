{"id": "1701.03038", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "Decoding with Finite-State Transducers on GPUs", "abstract": "Weighted finite automata and transducers (including hidden Markov models and conditional random fields) are widely used in natural language processing (NLP) to perform tasks such as morphological analysis, part-of-speech tagging, chunking, named entity recognition, speech recognition, and others. Parallelizing finite state algorithms on graphics processing units (GPUs) would benefit many areas of NLP. Although researchers have implemented GPU versions of basic graph algorithms, limited previous work, to our knowledge, has been done on GPU algorithms for weighted finite automata. We introduce a GPU implementation of the Viterbi and forward-backward algorithm, achieving decoding speedups of up to 5.2x over our serial implementation running on different computer architectures and 6093x over OpenFST.", "histories": [["v1", "Wed, 11 Jan 2017 16:07:27 GMT  (73kb,D)", "https://arxiv.org/abs/1701.03038v1", "accepted at EACL 2017"], ["v2", "Tue, 17 Jan 2017 14:48:24 GMT  (72kb,D)", "http://arxiv.org/abs/1701.03038v2", "accepted at EACL 2017"]], "COMMENTS": "accepted at EACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.DC", "authors": ["arturo argueta", "david chiang"], "accepted": false, "id": "1701.03038"}, "pdf": {"name": "1701.03038.pdf", "metadata": {"source": "CRF", "title": "Decoding with Finite-State Transducers on GPUs", "authors": ["Arturo Argueta", "David Chiang"], "emails": ["aargueta@nd.edu", "dchiang@nd.edu"], "sections": [{"heading": "1 Introduction", "text": "Weighted finite automata (Mohri, 2009), including hidden Markov models and contingent random fields (Lafferty et al., 2001), are used to solve a wide range of natural language processing (NLP) problems, including phonology and morphology, part-of-speech tagging, chunking, named entity recognition, and others. Even speech recognition and phrase translation models can be thought of as extensions of finite automata (Mohri et al., 2002; Kumar et al., 2005).Although the use of graphics processing units (GPUs) can now be made easier in neural network and phrase translation applications, such as Theano (Theano Development Team, 2016; Kumar et al., 2005), although the use of graphics processing units (GPUs) for deterring neural networks and the easy use of toolkits such as Theano (Theano et al Development Team, 2016), there has been little acceleration of previous work and acceleration."}, {"heading": "2 Graphics Processing Units", "text": "Current standard CPUs contain 8-16 cores, while 1500-2500 GPUs contain simple CUDA cores built into the card. General Purpose GPUs (GPGPU) contain cores capable of performing calculations that are not limited to image processing. GPGPUs are now used in many scientific fields to increase the performance of various applications."}, {"heading": "2.1 Architecture", "text": "The number of cores per SM varies depending on the microarchitecture of the GPU, ranging from 8 cores per SM (Tesla) to 192 (Kepler).The total number of SM on the chip varies, and it can range from 15 (Kepler) to 24 (Maxwell).Streaming multiprocessors are composed of the following components: \u2022 Special functional units (SFUs) These allow the calculation of functions such as sine, cosmos, etc. \u2022 Shared memory and L1 cache The size of memory varies on the GPU model. \u2022 Warp scheduler identifies threads in an SM that are executed in a particular warpu."}, {"heading": "2.2 Optimizations", "text": "In this section we have to consider the number of kernels on the device; the number of kernels on the KPU is immense."}, {"heading": "3 Weighted Finite Automata", "text": "In this section, we check weighted finite automata by using a matrix formula. A weighted finite automata is a tuple M = (Q, \u03a3, s, F, \u03b4), where \u2022 Q is a finite set of states. \u2022 \u03a3 is a finite input alphabet. \u2022 s-RQ is a uniform vector: If M can begin in state q, then f [q] > 0 is the resulting weight; otherwise f [q] = 0. \u2022 \u043c-plication is the transition function: If M is in state q and the next input symbol is a vector of finite weights: If M can accept in state q, then f [q] > 0 is the resulting weight."}, {"heading": "4 Serial Algorithm", "text": "Applications of finite automata use a variety of algorithms, but the most common are the Viterbi, forward and backward algorithms. Several of these automata algorithms are related to each other and are used for learning and conclusion.The acceleration of these algorithms allows for faster training and development of large machine learning systems.Their intermediate values are used to calculate the expected numbers during training by maximizing expectation (Eisner, 2002).They can be calculated by Algorithm 2.Algorithm 1 to calculate Viterbi with Equation (1).It is a simple algorithm, but the data structures require a short explanation."}, {"heading": "P 0.48 0.08 1 1 1 1", "text": "\u2022 O contains the output symbols for the transitions from state S [k] to state T [k] \u2022 P contains the probabilities for the transitions from state S [k] to state T [k] The vector f of the final weights is stored as a sparse vector: for each k, S f [k] is a final state with weight P f [k].Algorithm 1 Serial Viterbi algorithm (using the CSR / COO representation).1: for q Q do 2: \u03b1 [0] [q] = 0 3: \u03b1 [0] [s] = 1 4: for t = 1,.., n do 5: a \u2190 wt 6: for k = R [a],..., R [a + 1] \u2212 1 do 7: p \u2190 [t \u2212 1] [S [k] [s] = 1: for t = 1,."}, {"heading": "5 Parallel Algorithm", "text": "Our parallel implementation is based on algorithm 1 for viterbi and algorithm 2 for q q \u00b2 backwards, but in parallel, the loop passes over t, i.e. over the transitions on the wt \u00b2 symbol. The transitions algorithm 2 forward-backward algorithm (q \u00b2).1: forward [0] [s] \u2190 1. Pass forward 2: for t = 0,. \u2212 1 do 3: for q \u00b2 Q do 4: for q \u00b2 Q so that \u03b4 [wt, q \u00b2] > 0 do 5: p = forward [t] [q] go backwards [t] [wt] [q, q \u2212 1] 6: forward [t \u2212 1] [q \u2032 Q] + = p 7: do for q \u00b2 Q. Pass backwards 8: [n] [q] [f] [wq] = 9: for t = n \u2212 1,."}, {"heading": "5.1 Viterbi", "text": "If the probability (at line 8 in algorithm 1) is higher than \u03b1 [t] [q \u2032], we store the probability in \u03b1 [t] [q \u2032]. Since this update is potentially involved, we read and write in the same location, we use a maximum atomic operation (defined as atomicMax on the NVIDIA toolkit), but atomicMax is not defined for floating point values. Additionally, this update must store a backpointer (k), which is then used to reconstruct the path with the highest probability. The problem is that the AtomicMax provided by NVIDIA can only atomically update a single value. We solve both problems with one trick: Pack the Viterbi probability and the backpointer back into a single 64-bit integer, with the probability in the higher 32 bits and the backpointer in the lower 32 bits."}, {"heading": "5.2 Forward-Backward", "text": "The forward and backward algorithms 2 are similar to the Viterbi algorithm, but do not have to hold backwards pointers. In the forward algorithm, when a transition \u03b4 [wt] [q, q \u2032] is processed, we update the sum of probabilities that reach the state q \"in forward [t + 1] [q\"]. Likewise, in the backward algorithm, we update the sum of probabilities that proceed from q in backward [t] [q]. Both passes require atomic addition, but since we use protocol probabilities to avoid an underflow, the atomic addition must be implemented as follows: log (exp a + exp b) = b + log1p (exp (a \u2212 b)), (2) assuming a \u2264 b and where log1p (x) = log (1 + x), a common function in mathematical libraries that are numerically more stable for small x.We implemented a uatomatic version of this pre-protocol exloading, but the two are the DA-rapid transcendents."}, {"heading": "6 Other Approaches", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Parallel prefix sum", "text": "We have already mentioned a number of works started by (Ladner and Fischer, 1980) for unweighted, non-deterministic finite automata and continued by (Hillis and Steele, 1986) and (Mytkowicz et al., 2014) for unweighted, deterministic finite automata. These approaches use parallel prefix sums to calculate weight (1), multiply each adjacent matrix pair in parallel and repeat until all matrices are multiplied with each other. This approach could be combined with ours; we will leave this to future work. One possible problem is that matrix vector products are replaced by slower matrix matrix products. Another is that prefix sums may not be applicable in a more general environment - for example, if an FST consists of an input grid and not an input string."}, {"heading": "6.2 Matrix libraries", "text": "The formulation of the viterbi and forwardbackward algorithms as a result of matrix multiplications suggests two possible simple implementation strategies. Firstly, a neural network toolkit could be used to perform this calculation on a GPU. However, in practice, this approach is likely to be inefficient due to the small number of our transition matrices. Secondly, standard libraries exist for sparse matrix / vector operations, such as cuSPARSE.1 However, such libraries do not allow redefinition of addition and multiplication operations, making it difficult to implement the Viterbi algorithm or use protocol probabilities. Also, the parallelization of sparse matrix / vector operations depends heavily on the austerity pattern (Bell and Garland, 2008), making it possible to find a solution to infinity under these conditions."}, {"heading": "7 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Setup", "text": "To test our algorithm, we constructed an FST for rudimentary translations from French to English. We trained various unsmoothed Bigram Language1http: / / docs.nvidia.com / cuda / cusparse / models on 1k / 10k / 100k / 150k lines of French English parallel data from the Europarl corpus and converted it into a finite automaton (see Figure 3a for a toy example).GIZA + + was used to align the same data verbatim and to generate text translation tables P (f | e) from the word alignments as in lexical weighting (Koehn et al., 2003).We converted this table into a national FST (Figure 3b).The language model automaton and the translation table converter P (f | e) were separated to create a converter similar to Figure 1."}, {"heading": "7.2 Baselines", "text": "We compared ourselves to the following basics: Carmel is an FST toolkit developed at USC / ISI.2OpenFST, developed by Google as an open source successor to the AT & T Finite State Machine Library (Allauzen et al., 2007). For compatibility, our implementations read the OpenFST / AT & T text file format. 2https: / / github.com / graehl / carmelOur serial implementation algorithm 1 for viterbi and algorithm 2 for forward-back. cuSPARSE was used to implement the forward algorithm, using CSR format instead of COO for transition matrices. As we cannot redefine addition and multiplication, we could not implement the Viterbi algorithm."}, {"heading": "7.3 Results", "text": "Table 2 shows the overall performance of our Viterbi algorithm and basic algorithms. Our parallel implementation performs worse than our serial implementation if the converter used is small (presumably due to the overhead for kernel startup and memory copies), but the accelerations increase with the size of the converter and reach an acceleration of 5x. The forward-backward algorithm with expected counting achieves a 5-fold acceleration over the serial code of the largest converter (see Table 3). CuSPARSE performs significantly worse than even our serial implementation; probably it would have worked better if the transition matrices of our converters had been economical. Figure 4 shows decryption times for three algorithms (our serial and parallel Viterbi as well as cuSPARSE forward) on individual sets. It is evident that all three algorithms are roughly linear in set length."}, {"heading": "7.4 Comparison across GPU architectures", "text": "Table 2 compares the performance of the Kepler-based K40, on which we conducted most of our experiments, with the Maxwell-based Titan X and the Pascal-based Tesla P100. The performance improvement is due to a number of factors, such as greater number of active thread blocks per streaming multiprocessor on a GPU architecture, the network and block size selected to operate the kernel, and memory management on the GPU. Following the release of the Kepler architecture, the Maxwell architecture introduced improved workload balancing, reduced latency, and faster atomic operations. Moreover, the Pascal architecture enables acceleration over all other architectures by providing higher floating point performance, faster data movement performance (NVLink), larger and more efficient shared memory, and improved atomic operations. In addition, SMs on the pascal architecture are more efficient and allow greater acceleration than their predecessors. Our parallel implementations have been merged under the flagship architectural advantages of X."}, {"heading": "7.5 Comparison against a multi-core implementation", "text": "The results show that multicore implementation of the algorithm results in slower performance than serial code due to communication and synchronization efforts. Multiple cores often need to transfer information and synchronize all messages on a single core. In this case, GPUs work better than multicore because all memory is already on the graphics card and the cost of using global memory on the GPU is lower than synchronizing and exchanging data between cores."}, {"heading": "8 Conclusion", "text": "We have shown that our algorithm outperforms several serial implementations (our own serial implementation on Intel Core i7 and Xeon E machines, Carmel and OpenFST) as well as a GPU implementation using cuSPARSE. A system with newer and faster cores can achieve higher acceleration than a GPU on smaller data sets. However, building a multicore system that outperforms a GPU setup can be more expensive. Thus, a 16-core Intel Xeon E5-2698 V3 can cost $3,500 (Bogoychev and Lopez, 2016). Newer GPU models offer users the opportunity to get acceleration for an earlier generation at a lower price (Titan X GPUs sell cheaper than Xeon E5 setups at $1,200)."}, {"heading": "Acknowledgements", "text": "This research was supported in part by a gift from NVIDIA Corporation to a Tesla K40c GPU card."}], "references": [{"title": "OpenFst: A general and efficient weighted finite-state transducer library", "author": ["Mohri."], "venue": "Proc. International Conference on Implementation and Application of Automata (CIAA 2007), pages 11\u201323.", "citeRegEx": "Mohri.,? 2007", "shortCiteRegEx": "Mohri.", "year": 2007}, {"title": "Efficient sparse matrix-vector multiplication on CUDA", "author": ["Bell", "Garland2008] Nathan Bell", "Michael Garland"], "venue": "Technical Report NVIDIA Technical Report NVR-2008-004,", "citeRegEx": "Bell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bell et al\\.", "year": 2008}, {"title": "N-gram language models for massively parallel devices", "author": ["Bogoychev", "Lopez2016] Nikolay Bogoychev", "Adam Lopez"], "venue": "In Proc. ACL,", "citeRegEx": "Bogoychev et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bogoychev et al\\.", "year": 2016}, {"title": "A multi-teraflop constituency parser using GPUs", "author": ["Canny et al.2013] John Canny", "David Hall", "Dan Klein"], "venue": "In Proc. EMNLP,", "citeRegEx": "Canny et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Canny et al\\.", "year": 2013}, {"title": "A fully data parallel WFST-based large vocabulary continuous speech recognition on a graphics processing unit", "author": ["Chong et al.2009] Jike Chong", "Ekaterina Gonina", "Youngmin Yi", "Kurt Keutzer"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Chong et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chong et al\\.", "year": 2009}, {"title": "Parameter estimation for probabilistic finite-state transducers", "author": ["Jason Eisner"], "venue": "In Proc. ACL,", "citeRegEx": "Eisner.,? \\Q2002\\E", "shortCiteRegEx": "Eisner.", "year": 2002}, {"title": "Sparser, better, faster GPU parsing", "author": ["Hall et al.2014] David Hall", "Taylor Berg-Kirkpatrick", "Dan Klein"], "venue": "In Proc. ACL,", "citeRegEx": "Hall et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2014}, {"title": "Massively parallel suffix array queries and on-demand phrase extraction for statistical machine translation using gpus", "author": ["He et al.2013] Hua He", "Jimmy Lin", "Adam Lopez"], "venue": "In Proc. NAACL HLT,", "citeRegEx": "He et al\\.,? \\Q2013\\E", "shortCiteRegEx": "He et al\\.", "year": 2013}, {"title": "Efficient on-the-fly hypothesis rescoring in a hybrid gpu/cpu-based large vocabulary continuous speech recognition engine", "author": ["Kim et al.2012] Jungsuk Kim", "Jike Chong", "Ian R Lane"], "venue": "In Proc. INTERSPEECH,", "citeRegEx": "Kim et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2012}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In Proc. NAACL HLT,", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "A weighted finite state transducer translation template model for statistical machine translation", "author": ["Kumar et al.2005] Shankar Kumar", "Yonggang Deng", "William Byrne"], "venue": "J. Natural Language Engineering,", "citeRegEx": "Kumar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2005}, {"title": "Parallel prefix computation", "author": ["Ladner", "Fischer1980] Richard E. Ladner", "Michael J. Fischer"], "venue": "J. ACM,", "citeRegEx": "Ladner et al\\.,? \\Q1980\\E", "shortCiteRegEx": "Ladner et al\\.", "year": 1980}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando C.N. Pereira"], "venue": "In Proc. ICML,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Efficient parallel implementation of threepoint viterbi decoding algorithm on CPU, GPU, and FPGA. Concurrency and Computation: Practice and Experience, 26(3):821\u2013840", "author": ["Li et al.2014] Rongchun Li", "Yong Dou", "Dan Zou"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Scalable GPU graph traversal", "author": ["Michael Garland", "Andrew Grimshaw"], "venue": "In Proc. 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP),", "citeRegEx": "Merrill et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Merrill et al\\.", "year": 2012}, {"title": "Weighted finitestate transducers in speech recognition", "author": ["Mohri et al.2002] Mehryar Mohri", "Fernando C.N. Pereira", "Michael Riley"], "venue": "Computer Speech and Language,", "citeRegEx": "Mohri et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2002}, {"title": "Weighted automata algorithms", "author": ["Mehryar Mohri"], "venue": "Handbook of Weighted Automata,", "citeRegEx": "Mohri.,? \\Q2009\\E", "shortCiteRegEx": "Mohri.", "year": 2009}, {"title": "Dataparallel finite-state machines", "author": ["Madanlal Musuvathi", "Wolfram Schulte"], "venue": "In Proc. Architectural Support for Programming Languages and Operating Systems (ASPLOS),", "citeRegEx": "Mytkowicz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mytkowicz et al\\.", "year": 2014}, {"title": "Improving GPU performance via large warps and two-level warp scheduling", "author": ["Michael Shebanow", "Chang Joo Lee", "Rustam Miftakhutdinov", "Onur Mutlu", "Yale N Patt"], "venue": null, "citeRegEx": "Narasiman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Narasiman et al\\.", "year": 2011}, {"title": "A Gb/s parallel block-based viterbi decoder for convolutional codes on gpu", "author": ["Peng et al.2016] Hao Peng", "Rongke Liu", "Yi Hou", "Ling Zhao"], "venue": "arXiv preprint arXiv:1608.00066", "citeRegEx": "Peng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "Weighted finite automata (Mohri, 2009), including hidden Markov models and conditional random fields (Lafferty et al.", "startOffset": 25, "endOffset": 38}, {"referenceID": 12, "context": "Weighted finite automata (Mohri, 2009), including hidden Markov models and conditional random fields (Lafferty et al., 2001), are used to solve a wide range of natural language processing (NLP) problems, including phonology and morphology, part-of-speech tagging, chunking, named entity recognition, and others.", "startOffset": 101, "endOffset": 124}, {"referenceID": 15, "context": "Even models for speech recognition and phrase-based translation can be thought of as extensions of finite automata (Mohri et al., 2002; Kumar et al., 2005).", "startOffset": 115, "endOffset": 155}, {"referenceID": 10, "context": "Even models for speech recognition and phrase-based translation can be thought of as extensions of finite automata (Mohri et al., 2002; Kumar et al., 2005).", "startOffset": 115, "endOffset": 155}, {"referenceID": 18, "context": "Although the use of graphics processing units (GPUs) is now de rigeur in applications of neural networks and made easy through toolkits like Theano (Theano Development Team, 2016), there has been little previous work, to our knowledge, on acceleration of weighted finite-state computations on GPUs (Narasiman et al., 2011; Li et al., 2014; Peng et al., 2016; Chong et al., 2009).", "startOffset": 298, "endOffset": 378}, {"referenceID": 13, "context": "Although the use of graphics processing units (GPUs) is now de rigeur in applications of neural networks and made easy through toolkits like Theano (Theano Development Team, 2016), there has been little previous work, to our knowledge, on acceleration of weighted finite-state computations on GPUs (Narasiman et al., 2011; Li et al., 2014; Peng et al., 2016; Chong et al., 2009).", "startOffset": 298, "endOffset": 378}, {"referenceID": 19, "context": "Although the use of graphics processing units (GPUs) is now de rigeur in applications of neural networks and made easy through toolkits like Theano (Theano Development Team, 2016), there has been little previous work, to our knowledge, on acceleration of weighted finite-state computations on GPUs (Narasiman et al., 2011; Li et al., 2014; Peng et al., 2016; Chong et al., 2009).", "startOffset": 298, "endOffset": 378}, {"referenceID": 4, "context": "Although the use of graphics processing units (GPUs) is now de rigeur in applications of neural networks and made easy through toolkits like Theano (Theano Development Team, 2016), there has been little previous work, to our knowledge, on acceleration of weighted finite-state computations on GPUs (Narasiman et al., 2011; Li et al., 2014; Peng et al., 2016; Chong et al., 2009).", "startOffset": 298, "endOffset": 378}, {"referenceID": 17, "context": "Most previous work on parallel processing of finite automata (Ladner and Fischer, 1980; Hillis and Steele, 1986; Mytkowicz et al., 2014) uses dense representations of finite automata, which is only appropriate if the automata are not too sparse (that is, most states can transition to most other states).", "startOffset": 61, "endOffset": 136}, {"referenceID": 7, "context": "Previous work has been done on accelerating particular NLP tasks on GPUs: in machine translation, phrase-pair retrieval (He et al., 2013) and language model querying (Bogoychev and Lopez, 2016); parsing (Hall et al.", "startOffset": 120, "endOffset": 137}, {"referenceID": 6, "context": ", 2013) and language model querying (Bogoychev and Lopez, 2016); parsing (Hall et al., 2014; Canny et al., 2013); and speech recognition (Kim et al.", "startOffset": 73, "endOffset": 112}, {"referenceID": 3, "context": ", 2013) and language model querying (Bogoychev and Lopez, 2016); parsing (Hall et al., 2014; Canny et al., 2013); and speech recognition (Kim et al.", "startOffset": 73, "endOffset": 112}, {"referenceID": 8, "context": ", 2013); and speech recognition (Kim et al., 2012).", "startOffset": 32, "endOffset": 50}, {"referenceID": 14, "context": "Our work uses concepts from the work of Merrill et al. (2012), who show that GPUs can be used to accelerate breadth-first search in sparse graphs.", "startOffset": 40, "endOffset": 62}, {"referenceID": 5, "context": "Their intermediate values are used to compute expected counts during training by expectation-maximization (Eisner, 2002).", "startOffset": 106, "endOffset": 120}, {"referenceID": 17, "context": "We have already mentioned a line of work begun by (Ladner and Fischer, 1980) for unweighted, nondeterministic finite automata, and continued by (Hillis and Steele, 1986) and (Mytkowicz et al., 2014) for unweighted, deterministic finite automata.", "startOffset": 174, "endOffset": 198}, {"referenceID": 9, "context": "GIZA++ was used to word-align the same data and generate word-translation tables P( f | e) from the word alignments, as in lexical weighting (Koehn et al., 2003).", "startOffset": 141, "endOffset": 161}], "year": 2017, "abstractText": "Weighted finite automata and transducers (including hidden Markov models and conditional random fields) are widely used in natural language processing (NLP) to perform tasks such as morphological analysis, part-of-speech tagging, chunking, named entity recognition, speech recognition, and others. Parallelizing finite state algorithms on graphics processing units (GPUs) would benefit many areas of NLP. Although researchers have implemented GPU versions of basic graph algorithms, limited previous work, to our knowledge, has been done on GPU algorithms for weighted finite automata. We introduce a GPU implementation of the Viterbi and forward-backward algorithm, achieving decoding speedups of up to 5.2x over our serial implementation running on different computer architectures and 6093x over OpenFST.", "creator": "LaTeX with hyperref package"}}}