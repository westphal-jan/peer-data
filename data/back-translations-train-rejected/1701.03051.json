{"id": "1701.03051", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "Efficient Twitter Sentiment Classification using Subjective Distant Supervision", "abstract": "As microblogging services like Twitter are becoming more and more influential in today's globalised world, its facets like sentiment analysis are being extensively studied. We are no longer constrained by our own opinion. Others opinions and sentiments play a huge role in shaping our perspective. In this paper, we build on previous works on Twitter sentiment analysis using Distant Supervision. The existing approach requires huge computation resource for analysing large number of tweets. In this paper, we propose techniques to speed up the computation process for sentiment analysis. We use tweet subjectivity to select the right training samples. We also introduce the concept of EFWS (Effective Word Score) of a tweet that is derived from polarity scores of frequently used words, which is an additional heuristic that can be used to speed up the sentiment classification with standard machine learning algorithms. We performed our experiments using 1.6 million tweets. Experimental evaluations show that our proposed technique is more efficient and has higher accuracy compared to previously proposed methods. We achieve overall accuracies of around 80% (EFWS heuristic gives an accuracy around 85%) on a training dataset of 100K tweets, which is half the size of the dataset used for the baseline model. The accuracy of our proposed model is 2-3% higher than the baseline model, and the model effectively trains at twice the speed of the baseline model.", "histories": [["v1", "Wed, 11 Jan 2017 16:39:04 GMT  (19kb)", "http://arxiv.org/abs/1701.03051v1", null]], "reviews": [], "SUBJECTS": "cs.SI cs.CL cs.IR", "authors": ["tapan sahni", "chinmay chandak", "naveen reddy chedeti", "manish singh"], "accepted": false, "id": "1701.03051"}, "pdf": {"name": "1701.03051.pdf", "metadata": {"source": "CRF", "title": "Efficient Twitter Sentiment Classification using Subjective Distant Supervision", "authors": ["Tapan Sahni", "Chinmay Chandak", "Naveen Reddy", "Manish Singh"], "emails": ["msingh}@iith.ac.in"], "sections": [{"heading": null, "text": "Most of these techniques use machine learning algorithms with features such as Unigrams, n-grams, Part-OfSpeech (POS) tags. However, the training data sets are often very large, and therefore this process requires a lot of computing power and time. The following question arises: What to do if we do not have resources that provide such large computing power? The existing solution to this problem is to use a smaller sample of the data sets. For sentiment analysis, when we train the model using smaller randomly selected keywords, we get less accuracy."}, {"heading": "II. RELATED WORK", "text": "There has been a great deal of previous research in the sentiment analysis of tweets. Read [10] shows that using emoticons as labels for positive and sentiment is effective in reducing dependencies in machine learning techniques. Alec Go [1] used Naive Bayes, SVM and MaxEnt classification techniques to train her model, which, as already mentioned, is our base model. Our model builds on this and achieves greater accuracy on a much smaller training dataset. Ayushi Dalmia [6] proposed a model with a more involved pre-processing phase and used features such as the scores from Bing LiusOpinion Lexicon and the number of positive, negative POS tags. This model achieved considerably high accuracy, considering that its features were not the conventional bagof words or any N-grams. The idea of using polarity values of commonly used tweet-tags as described in our Whayu-Words model (as WS)."}, {"heading": "III. SUBJECTIVITY", "text": "An objective perspective is one that is not influenced by emotions, opinions or personal feelings - it is a perspective that is actually based on quantifiable and measurable things. A subjective perspective is open to a larger interpretation that is based on personal feelings, emotions, aesthetics, etc. Subjectivity classification is another issue in the field of text classification that is becoming more and more interesting in the field of sentiment analysis. Since a single sentence can contain several opinions and subjective and factual clauses, this problem is not as simple as it seems. Below are some examples of subjective and objective sentences without feeling: Thus the earth revolves around the sun. Objective sentences with sentiment: The drug has relieved my pain. Subjective sentences without feeling: I think he went home yesterday. Subjective sentences with feeling: I am so happy that you have the erudition. Classifying a sentence as subjective or certain sentences with feeling: The drug does not relieve my pain and sentences are not subjective in general."}, {"heading": "A. Corpus", "text": "Our training dataset 1 has 1.6 million tweets and 5000 tweets in the test dataset. As the provided test dataset only contained 500 tweets, we used some of the training data (exactly 5000 tweets, as opposed to the training dataset) as a test dataset. We remove emoticons from our training and test data. The following table shows some sample tweets.Tweet Sentiment @ MrZeroo00 Yes! tks man Positive oh so bored... stayed at home Negative Nice night and I feel too sick Negative"}, {"heading": "B. Subjectivity Filtering", "text": "We use TextBlob to classify each tweet as subjective or objective. We then remove all tweets with a subjectivity level / score (score is between 0 and 1) below a specified threshold, and the remaining tweets are used for training purposes. We observe that a significant number of tweets are removed as the subjectivity threshold rises. We show the effect of this approach on overall accuracy in the evaluation section of the paper."}, {"heading": "C. Preprocessing", "text": "We also use the following properties to reduce the feature space. However, most of the pre-processing steps are common in most previous work in the field. However, we have added a few more steps to this stage of our model. 1The URL is http: / / twittersentiment.appspot.com /. However, this page has a link to our training data and test data. It is also a public tool that other researchers can use to build their own data sets. 1) Basic Steps: We first extract the emoticons from the data. Users often include Twitter usernames in their tweets to direct their messages. We also extract usernames (e.g. @ chinmay) and URLs that are present in tweets because they do not help us classify them. Apart from full stops that are covered in the next item, other punctuation and special symbols are removed."}, {"heading": "D. Baseline model", "text": "The basic model for our experiments is explained in the thesis of Alec Go [1]. The model uses the Naive Bayes, SVM and Maximum Entropy classifiers for their experiment. Their feature vector consists of either Unigrams, Bigrams, Unigrams + Bigrams or Unigrams + POS tags. This work achieved the following maximum accuracies: a) 82.2 for the Unigram feature vector using the SVM classifier, b) 83.0 for the Unigram + Bigram feature vector using the MaxEnt classifier and 82.7 using the Naive Bayes classifier. c) 81.9 for the Unigram + POS feature vector using the SVM classifier. These basic accuracies are based on a training data set of 1.6 million tweets and a test data set of 500 tweets. We use the same training data set for our experiments. Later we present the basic accuracies on a training data set of 200K, a data set and a data set of 5000."}, {"heading": "E. Effective Word Score (EFWS) Heuristic", "text": "We have described our baseline model above. The feature vectors for which we collect results are Unigram, Unigram + Bigram and Unigram + POS. We have already made two major changes before the training on our data set begins, compared to our baseline model. Firstly, our training data sets are filtered according to the subjectivity threshold. Secondly, our pre-processing is much more robust compared to their work. Now, let's consider an additional heuristic value that we use to obtain terms for our test data. Along with dictionaries for stopwords and acronyms, we also maintain a dictionary of a list of frequently used words and their polarity values. This dictionary has about 2500 words and their polarity values, ranging from -5 to 5. In runtime, we also use all the synonyms of a word (from WordNet) that exist in a tweet, as well as the dictionary, and assign them the same score as a dictionary."}, {"heading": "F. Training Model", "text": "We use the following classifiers for our model. 1) Naive Bayes: Naive Bayes: Naive Bayes is a simple model that works well on text categorization. We use a Naive Bayes model. Class c * is associated with Tweet d, where c * = argmax P (c | d).PNB (c | d) = P (c) m \u2211 i = 1P (f | c) ni (d) and PNB (c | d) are calculated using the Bayes rule. In this formula, f represents one feature and ni (d) the number of features found in Tweet d. There are a total of m features. The parameters P (c) and P (f | c) are obtained by maximum probability estimations.2) Supported vector machines: Supported vector machines are based on the structural risk composition principle found in Tweet d. There are a total of m features. The parameters P (maximum) and P (maximum) are calculated by probability (P)."}, {"heading": "V. EVALUATION", "text": "To show that our model achieves a higher accuracy than the baseline model and on a smaller training dataset, we first fixed the test datasets. Our test dataset, as already mentioned, consists of 5000 tweets. We conducted our experiments on an Intel Core i5 machine (4 cores), with 8GB of RAM. Following are the accuracies of the baseline model on a training platform of 200K tweets. We performed our experiments on an Intel Core i5 machine (4 cores), with 8GB of RAM. The following are the accuracies of the baseline model on a training platform of 71.8% 79.7% We filtered the training platform with a subjectivity threshold of 0.5%. Thus, we saw that the number of tweets was reduced to approximately 0.6 million tweets from a previous total of 1.6 million. We then randomly selected our model for 100K tweets in earlier sections."}, {"heading": "VI. CONCLUSION", "text": "We show that higher accuracy can be achieved in the sentiment classification of Twitter messages, practiced on a smaller dataset and with a much faster computing time, solving the problem of limiting computing power to a certain degree. This can be achieved by using a subjectivity threshold to selectively filter training data, including a more complex pre-processing phase, and using additional heuristics for sentiment classification in addition to traditional machine learning techniques. Since Twitter data is abundant, our subjectivity filtering process can provide a better generalized model for sentiment classification."}], "references": [{"title": "Twitter Sentiment Classification using Distant Supervision", "author": ["Alec Go", "Richa Bhayani", "Lei Huang"], "venue": "CS224N Project Report,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "A Survey of Opinion Mining and Sentiment Analysis", "author": ["Bing Liu", "Lei Zhang"], "venue": "Mining Text Data,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Sentiment Analysis and Subjectivity", "author": ["Bing Liu"], "venue": "In Handbook of Natural Language Processing, Second Edition. Taylor and Francis Group,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "NRC- Canada: Building the State-of-the-Art in Sentiment Analysis of tweets", "author": ["Saif M. Mohammad", "Svetlana Kiritchenko", "Xiaodan Zhu"], "venue": "In Proceedings of the 7th International Workshop on Semantic Evaluation Exercises", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Twitter Sentiment Analysis The good, the bad and the neutral! IIIT-H at SemEval", "author": ["Ayushi Dalmia", "Manish Gupta", "Vasudeva Varma"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Twitter as a Corpus for Sentiment Analysis and Opinion Mining", "author": ["Alexander Pak", "Patrick Paroubek"], "venue": "In Proceedings of LREC,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Sentiment Analysis of Twitter Data: A Survey of Techniques", "author": ["Vishal A. Kharde", "S.S. Sonawane"], "venue": "International Journal of Computer Applications", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Sentiment Analysis of Twitter Data: A Survey of Techniques", "author": ["Hassan Saif", "Yulan He", "Harith Alani"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Using emoticons to reduce dependency in machine learning techniques for sentiment classification", "author": ["J. Read"], "venue": "In Proceedings of ACL-05,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Sentiment analysis of twitter data", "author": ["A. Agarwal", "B. Xie", "I. Vovsha", "O. Rambow", "R. Passonneau"], "venue": "In Proc. ACL 2011 Workshop on Languages in Social Media,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Sentiment classification using machine learning techniques", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "A survey on Sentiment Analysis Algorithms for opinion mining", "author": ["Vidisha M. Pradhan", "Jay Vala", "Prem Balani"], "venue": "International Journal of Computer Applications,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Enhanced twitter sentiment classification using contextual information. In 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA", "author": ["Soroush Vosoughi", "Helen Zhou", "Deb Roy"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Twitter Sentiment Analysis: The Good the Bad and the OMG", "author": ["Efthymios Kouloumpis", "Theresa Wilson", "Johanna Moore"], "venue": "In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Robust sentiment detection on twitter from biased and noisy data", "author": ["Luciano Barbosa", "Junlan Feng"], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics: Posters,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Evaluation datasets for Twitter sentiment analysis: a survey and a new dataset, the STS-Gold. In: 1st International Workshop on Emotion and Sentiment in Social and Expressive Media: Approaches and Perspectives from AI (ESSEM", "author": ["Hassan Saif", "Miriam Fernndez", "Yulan He", "Harith. Alani"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}], "referenceMentions": [{"referenceID": 15, "context": "For sentiment analysis, if we train the model using a smaller randomly chosen sample, then we get low accuracy [16, 17].", "startOffset": 111, "endOffset": 119}, {"referenceID": 16, "context": "For sentiment analysis, if we train the model using a smaller randomly chosen sample, then we get low accuracy [16, 17].", "startOffset": 111, "endOffset": 119}, {"referenceID": 9, "context": "Read [10] shows that using emoticons as labels for positive and sentiment is effective for reducing dependencies in machine learning techniques.", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "Alec Go [1] used Naive Bayes, SVM, and MaxEnt classifiers to train their model.", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "Ayushi Dalmia [6] proposed a model with a more involved preprocessing stage, and used features like scores from Bing Lius", "startOffset": 14, "endOffset": 17}, {"referenceID": 13, "context": "[14] created prior probabilities using the datasets for the average sentiment of tweets in different spatial, temporal and authorial contexts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Another significant effort in sentiment analysis on Twitter data is by Barbosa [16].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "(Davidov, Tsur, and Rappoport 2010) also use hashtags for creating training data, but they limit their experiments to sentiment/non-sentiment classification, rather than 3-way polarity classification, as [15] does.", "startOffset": 204, "endOffset": 208}, {"referenceID": 8, "context": "Hassan Saif [9] introduced a novel approach of adding semantics as additional features into the training set for sentiment analysis.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "The baseline model for our experiments is explained in the paper by Alec Go [1].", "startOffset": 76, "endOffset": 79}], "year": 2017, "abstractText": "As microblogging services like Twitter are becoming more and more influential in today\u2019s globalized world, its facets like sentiment analysis are being extensively studied. We are no longer constrained by our own opinion. Others\u2019 opinions and sentiments play a huge role in shaping our perspective. In this paper, we build on previous works on Twitter sentiment analysis using Distant Supervision. The existing approach requires huge computation resource for analyzing large number of tweets. In this paper, we propose techniques to speed up the computation process for sentiment analysis. We use tweet subjectivity to select the right training samples. We also introduce the concept of EFWS (Effective Word Score) of a tweet that is derived from polarity scores of frequently used words, which is an additional heuristic that can be used to speed up the sentiment classification with standard machine learning algorithms. We performed our experiments using 1.6 million tweets. Experimental evaluations show that our proposed technique is more efficient and has higher accuracy compared to previously proposed methods. We achieve overall accuracies of around 80% (EFWS heuristic gives an accuracy around 85%) on a training dataset of 100K tweets, which is half the size of the dataset used for the baseline model. The accuracy of our proposed model is 2-3% higher than the baseline model, and the model effectively trains at twice the speed of the baseline model.", "creator": "LaTeX with hyperref package"}}}