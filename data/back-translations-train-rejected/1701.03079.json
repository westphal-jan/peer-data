{"id": "1701.03079", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems", "abstract": "Open-domain human-computer conversation has been attracting increasing attention over the past few years. However, there does not exist a standard automatic evaluation metric for open-domain dialog systems; researchers usually resort to human annotation for model evaluation, which is time- and labor-intensive. In this paper, we propose RUBER, a Referenced metric and Unreferenced metric Blended Evaluation Routine, which evaluates a reply by taking into consideration both a groundtruth reply and a query (previous user utterance). Our metric is learnable, but its training does not require labels of human satisfaction. Hence, RUBER is flexible and extensible to different datasets and languages. Experiments on both retrieval and generative dialog systems show that RUBER has high correlation with human annotation.", "histories": [["v1", "Wed, 11 Jan 2017 17:43:57 GMT  (508kb,D)", "http://arxiv.org/abs/1701.03079v1", null], ["v2", "Sun, 16 Jul 2017 16:43:08 GMT  (4482kb,D)", "http://arxiv.org/abs/1701.03079v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.HC cs.IR", "authors": ["chongyang tao", "lili mou", "dongyan zhao", "rui yan"], "accepted": false, "id": "1701.03079"}, "pdf": {"name": "1701.03079.pdf", "metadata": {"source": "CRF", "title": "RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems", "authors": ["Chongyang Tao", "Lili Mou", "Dongyan Zhao", "Rui Yan"], "emails": ["chongyangtao@pku.edu.cn", "zhaody@pku.edu.cn", "ruiyan@pku.edu.cn", "doublepower.mou@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In these studies, however, researchers will be able to evaluate their models, which is costly and time-consuming, so automated evaluation metrics are particularly needed to ease the burden of comparing models and encourage further research."}, {"heading": "2 Methodology", "text": "Figure 1 shows the general design methodology of our RUBER metrics. We introduce the referenced and unreferenced metrics in sections 2.1 and 2.2 respectively. Section 2.3 describes how they are combined."}, {"heading": "2.1 Referenced Metric", "text": "\"It's not as if we're able to sow, as if we're able to sow, as if we're able to sow, as if we're able to hide.\" And, \"It's not as if we're able to sow, as if we're able to sow.\" And, \"It's not as if we're able to hide, as if we're able to sow.\""}, {"heading": "2.2 Unreferenced Metric", "text": "We then measure the relationship between the generated response r and its query q. This metric is unreferenced and referred to as sU (q, r) because it does not refer to a basic truth. (Figure 2) To predict the appropriateness of an answer in relation to a cross-sectional statement, each word is included in a query q and an answer r in an embedding. (Figure 2) To predict the appropriateness of an answer in relation to a cross-sectional statement, each word is included in a query q and an answer r in an embedding. (Figure 2) The bidirectional recurrent neural network structure with gated recurrent units (BiGRU RNN) collects information along the word sequence. The forward RNN takes the formrt = 1 (Wrxt + Urh) (3) zt."}, {"heading": "2.3 Hybrid Evaluation", "text": "We combine the above two metrics using simple heuristics, resulting in a hybrid RUBER method for evaluating open domain dialog systems. First, we normalize each metric to the range (0, 1), so that they are generally on the same scale. Specifically, normalization is given by Bys = s \u2212 min (s) max (s) \u2212 min (s) (8), with min (s) and max (s) referring to the maximum and minimum values of a given metric, respectively. Then, we combine s \u00b2 R and s \u00b2 U as our ultimate RUBER metric by heuristics, including min, max, geometric averaging and arithmetic averaging. As we will see in Section 3.2, different strategies lead to similar results that consistently exceed baseline."}, {"heading": "3 Experiments", "text": "In this section, we will examine the correlation between our RUBER metric and human annotation, which is the ultimate goal of automatic metrics. Due to the cultural background, the experiment was performed on a Chinese corpus, as human aspects play a major role in this work. However, we believe that our evaluation routine could be applied to different languages."}, {"heading": "3.1 Setup", "text": "We combed through massive data from an online Chinese forum Douban.1 The training set contains 1,449,218 samples. We performed Chinese word segmentation and received Chinese terms as primitive tokens. The RUBER metric (along with baselines) is evaluated on two dialog systems: one is a property-based retrieval and reranking system in Song et al. (2016), which first retrieves a coarse-grained candidate by keyword comparison and then re-encodes the candidates by human-made characteristics; the best results are selected for evaluation; the other is a sequence-based neural network (Bahdanau et al., 2015), which encodes a query as a vector with an RNN and decodes the Vector1http: / www.douban.com to a response with another RNN; the attention mechanism is also used to improve the query as a whole with an RNN. We had 9 volunteers to lend their human satisfaction to a response to a 1 or a poor response to a 1."}, {"heading": "3.2 Results", "text": "The results of the study show that the reference metric sR, which is based on mebeddings, is more correlated with human annotations than existing metrics, including BLEU and ROUGE, which are based on overlapping information, implying that the basic truth alone is useful for evaluating a response to BER. However, the exact word overlaps are too strict in the dialogue environment; the embedding of based methods measures sentence density in a \"soft\" way. The referenced metrics sU also achieve a high correlation and show that quantity alone is informative2 and that negative sampling values are useful for the formation of evaluation metrics, although a dialogue generator is aware of quantity."}, {"heading": "4 Conclusion", "text": "In this paper, we proposed an evaluation methodology for open domain dialog systems. Our metric is called RUBER (a Referenced Metric and Unreferenced Metric Blended Evaluation Routine) because it takes into account both the basic truth and its query. Experiments show that RUBER, although unattended, exhibits a strong correlation with human annotations."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Representation Learning.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments", "author": ["Satanjeev Banerjee", "Alon Lavie."], "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine", "citeRegEx": "Banerjee and Lavie.,? 2005", "shortCiteRegEx": "Banerjee and Lavie.", "year": 2005}, {"title": "Dialog system using real-time crowdsourcing and twitter large-scale corpus", "author": ["Fumihiro Bessho", "Tatsuya Harada", "Yasuo Kuniyoshi."], "venue": "Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue.", "citeRegEx": "Bessho et al\\.,? 2012", "shortCiteRegEx": "Bessho et al\\.", "year": 2012}, {"title": "Establishing and maintaining long-term humancomputer relationships", "author": ["Timothy W Bickmore", "Rosalind W Picard."], "venue": "ACM Transactions on Computer-Human Interaction 12(2):293\u2013327.", "citeRegEx": "Bickmore and Picard.,? 2005", "shortCiteRegEx": "Bickmore and Picard.", "year": 2005}, {"title": "Bootstrapping dialog systems with word embeddings", "author": ["Gabriel Forgues", "Joelle Pineau", "Jean-Marie Larchev\u00eaque", "R\u00e9al Tremblay."], "venue": "NIPS ML-NLP Workshop.", "citeRegEx": "Forgues et al\\.,? 2014", "shortCiteRegEx": "Forgues et al\\.", "year": 2014}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "ROUGE: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Text Summarization Branches out: Proceedings of the ACL-04 Workshop. pages 74\u201381. http://aclweb.org/anthology/W04-1013.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian Serban", "Mike Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "Proceedings", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Towards an automatic turing test: Learning to evaluate dialogue responses", "author": ["Ryan Lowe", "Michael Noseworthy", "Iulian V. Serban", "Nicolas Angelard-Gontier", "Yoshua Bengio", "Joelle Pineau."], "venue": "https://openreview.net/pdf?id=HJ5PIaseg.", "citeRegEx": "Lowe et al\\.,? 2017", "shortCiteRegEx": "Lowe et al\\.", "year": 2017}, {"title": "Natural language inference by tree-based convolution and heuristic", "author": ["Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin"], "venue": null, "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. pages 583\u2013593. http://aclweb.org/anthology/D11-1054.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Quantitative evaluation of user simulation techniques for spoken dialogue systems", "author": ["Jost Schatzmann", "Kallirroi Georgila", "Steve Young."], "venue": "Proceedings of the 6th Sigdial Workshop on Discourse an Dialogue. pages 45\u201354.", "citeRegEx": "Schatzmann et al\\.,? 2005", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2005}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["Aliaksei Severyn", "Alessandro Moschitti."], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. pages", "citeRegEx": "Severyn and Moschitti.,? 2015", "shortCiteRegEx": "Severyn and Moschitti.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Two are better than one: An ensemble of retrieval-and generation-based dialog systems", "author": ["Yiping Song", "Rui Yan", "Xiang Li", "Dongyan Zhao", "Ming Zhang."], "venue": "arXiv preprint arXiv:1610.07149 .", "citeRegEx": "Song et al\\.,? 2016", "shortCiteRegEx": "Song et al\\.", "year": 2016}, {"title": "Results of the wmt15 metrics shared task", "author": ["Milo\u0161 Stanojevi\u0107", "Amir Kamran", "Philipp Koehn", "Ond\u0159ej Bojar."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation. pages 256\u2013273. http://aclweb.org/anthology/W15-3031.", "citeRegEx": "Stanojevi\u0107 et al\\.,? 2015", "shortCiteRegEx": "Stanojevi\u0107 et al\\.", "year": 2015}, {"title": "PARADISE: A framework for evaluating spoken dialogue agents", "author": ["Marilyn A Walker", "Diane J Litman", "Candace A Kamm", "Alicia Abella."], "venue": "Proceedings of the 8th Conference on European Chapter of the Association", "citeRegEx": "Walker et al\\.,? 1997", "shortCiteRegEx": "Walker et al\\.", "year": 1997}, {"title": "Quantitative and qualitative evaluation of darpa communicator spoken dialogue systems", "author": ["Marilyn A Walker", "Rebecca Passonneau", "Julie E Boland."], "venue": "Proceedings of the 39th Annual Meeting on Association for Computational Linguistics. pages", "citeRegEx": "Walker et al\\.,? 2001", "shortCiteRegEx": "Walker et al\\.", "year": 2001}, {"title": "Learning natural language inference with lstm", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pages 1442\u20131451.", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Learning to rrespond with deep neural networks for retrievalbased human-computer conversation system", "author": ["Rui Yan", "Yiping Song", "Hua Wu."], "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Infor-", "citeRegEx": "Yan et al\\.,? 2016", "shortCiteRegEx": "Yan et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Nowadays, open-domain conversation is attracting increasing attention because of its wide applications (Bickmore and Picard, 2005; Bessho et al., 2012; Shang et al., 2015; Yan et al., 2016).", "startOffset": 103, "endOffset": 189}, {"referenceID": 2, "context": "Nowadays, open-domain conversation is attracting increasing attention because of its wide applications (Bickmore and Picard, 2005; Bessho et al., 2012; Shang et al., 2015; Yan et al., 2016).", "startOffset": 103, "endOffset": 189}, {"referenceID": 14, "context": "Nowadays, open-domain conversation is attracting increasing attention because of its wide applications (Bickmore and Picard, 2005; Bessho et al., 2012; Shang et al., 2015; Yan et al., 2016).", "startOffset": 103, "endOffset": 189}, {"referenceID": 20, "context": "Nowadays, open-domain conversation is attracting increasing attention because of its wide applications (Bickmore and Picard, 2005; Bessho et al., 2012; Shang et al., 2015; Yan et al., 2016).", "startOffset": 103, "endOffset": 189}, {"referenceID": 12, "context": "In early years, traditional vertical-domain dialog systems use automatic metrics like slot-filling accuracy and goal-completion rate (Walker et al., 1997, 2001; Schatzmann et al., 2005).", "startOffset": 133, "endOffset": 185}, {"referenceID": 10, "context": ", BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) for machine translation, and ROUGE (Lin, 2004) for summarization.", "startOffset": 7, "endOffset": 30}, {"referenceID": 1, "context": ", 2002) and METEOR (Banerjee and Lavie, 2005) for machine translation, and ROUGE (Lin, 2004) for summarization.", "startOffset": 19, "endOffset": 45}, {"referenceID": 6, "context": ", 2002) and METEOR (Banerjee and Lavie, 2005) for machine translation, and ROUGE (Lin, 2004) for summarization.", "startOffset": 81, "endOffset": 92}, {"referenceID": 11, "context": "For conversation systems, researchers occassionally adopt these metrics for evaluation (Ritter et al., 2011; Li et al., 2015; Song et al., 2016).", "startOffset": 87, "endOffset": 144}, {"referenceID": 5, "context": "For conversation systems, researchers occassionally adopt these metrics for evaluation (Ritter et al., 2011; Li et al., 2015; Song et al., 2016).", "startOffset": 87, "endOffset": 144}, {"referenceID": 15, "context": "For conversation systems, researchers occassionally adopt these metrics for evaluation (Ritter et al., 2011; Li et al., 2015; Song et al., 2016).", "startOffset": 87, "endOffset": 144}, {"referenceID": 4, "context": ", in BLEU and ROUGE), we measure the similarity by pooling of word embeddings (Forgues et al., 2014); it is more suited to dialog systems due to the diversity of replies.", "startOffset": 78, "endOffset": 100}, {"referenceID": 1, "context": ", 2002) and METEOR (Banerjee and Lavie, 2005) for machine translation, and ROUGE (Lin, 2004) for summarization. For conversation systems, researchers occassionally adopt these metrics for evaluation (Ritter et al., 2011; Li et al., 2015; Song et al., 2016). However, Liu et al. (2016) conduct extensive empirical experiments and show weak correlation between existing metrics and human annotation.", "startOffset": 20, "endOffset": 285}, {"referenceID": 1, "context": ", 2002) and METEOR (Banerjee and Lavie, 2005) for machine translation, and ROUGE (Lin, 2004) for summarization. For conversation systems, researchers occassionally adopt these metrics for evaluation (Ritter et al., 2011; Li et al., 2015; Song et al., 2016). However, Liu et al. (2016) conduct extensive empirical experiments and show weak correlation between existing metrics and human annotation. Very recently, Lowe et al. (2017) propose a neural network-based metric for conversation systems; it learns to predict a score of a reply given its query (previous user-issued utterance) and a groundtruth reply.", "startOffset": 20, "endOffset": 432}, {"referenceID": 6, "context": "We apply negative sampling to train the network. Our approach requires no manual annotation labels, and hence is more extensible than Lowe et al. (2017). \u2022 We propose to combine the referenced and unreferenced metrics to better make use both worlds.", "startOffset": 22, "endOffset": 153}, {"referenceID": 4, "context": "Also, such treatment is more robust than vector extrema (Forgues et al., 2014); it chooses either the largest positive or smallest negative value, where features are more vulnerable to signs.", "startOffset": 56, "endOffset": 78}, {"referenceID": 13, "context": "The above empirical structure is mainly inspired by several previous studies (Severyn and Moschitti, 2015; Yan et al., 2016).", "startOffset": 77, "endOffset": 124}, {"referenceID": 20, "context": "The above empirical structure is mainly inspired by several previous studies (Severyn and Moschitti, 2015; Yan et al., 2016).", "startOffset": 77, "endOffset": 124}, {"referenceID": 19, "context": "We may also apply other variants for utterance matching (Wang and Jiang, 2016; Mou et al., 2016); details are beyond the focus of this paper.", "startOffset": 56, "endOffset": 96}, {"referenceID": 9, "context": "We may also apply other variants for utterance matching (Wang and Jiang, 2016; Mou et al., 2016); details are beyond the focus of this paper.", "startOffset": 56, "endOffset": 96}, {"referenceID": 20, "context": "In previous work, researchers adopt negative sampling for utterance matching (Yan et al., 2016).", "startOffset": 77, "endOffset": 95}, {"referenceID": 8, "context": "Our study further verifies that negative sampling is useful for the evaluation task, which eases the burden of human annotation compared with fully supervised approaches that requirer manual labels for training their metrics (Lowe et al., 2017).", "startOffset": 225, "endOffset": 244}, {"referenceID": 0, "context": "The other is a sequence-to-sequence neural network (Bahdanau et al., 2015) that encodes a query as a vector with an RNN and decodes the vector", "startOffset": 51, "endOffset": 74}, {"referenceID": 5, "context": "The RUBER metric (along with baselines) is evaluated on two dialog systems. One is a featurebased retrieval-and-reranking system in Song et al. (2016), which first retrieves a coarse-grained candidate set by keyword matching and then reranks the candidates by human-engineered features; the top-ranked results are selected for evaluation.", "startOffset": 33, "endOffset": 151}, {"referenceID": 16, "context": "Pearson and Spearman correlation measures are widely used in other research of automatic metrics such as machine translation (Stanojevi\u0107 et al., 2015).", "startOffset": 125, "endOffset": 150}, {"referenceID": 8, "context": "Thus, RUBER does not requirer human labels; it is more flexible than the existing supervised metric (Lowe et al., 2017), and can be easily adapted to different datasets.", "startOffset": 100, "endOffset": 119}], "year": 2017, "abstractText": "Open-domain human-computer conversation has been attracting increasing attention over the past few years. However, there does not exist a standard automatic evaluation metric for open-domain dialog systems; researchers usually resort to human annotation for model evaluation, which is timeand labor-intensive. In this paper, we propose RUBER, a Referenced metric and Unreferenced metric Blended Evaluation Routine, which evaluates a reply by taking into consideration both a groundtruth reply and a query (previous user utterance). Our metric is learnable, but its training does not require labels of human satisfaction. Hence, RUBER is flexible and extensible to different datasets and languages. Experiments on both retrieval and generative dialog systems show that RUBER has high correlation with human annotation.", "creator": "LaTeX with hyperref package"}}}