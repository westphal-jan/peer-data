{"id": "1701.03092", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "Job Detection in Twitter", "abstract": "In this report, we propose a new application for twitter data called \\textit{job detection}. We identify people's job category based on their tweets. As a preliminary work, we limited our task to identify only IT workers from other job holders. We have used and compared both simple bag of words model and a document representation based on Skip-gram model. Our results show that the model based on Skip-gram, achieves a 76\\% precision and 82\\% recall.", "histories": [["v1", "Wed, 11 Jan 2017 18:42:09 GMT  (33kb,D)", "http://arxiv.org/abs/1701.03092v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["besat kassaie"], "accepted": false, "id": "1701.03092"}, "pdf": {"name": "1701.03092.pdf", "metadata": {"source": "CRF", "title": "Job Detection in Twitter", "authors": ["Besat Kassaie"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The use of data analysis techniques is also invaluable and can be derived from abundant data available on the Internet. Social networks, such as Twitter, are one of the most popular application classes that collect a lot of information in various formats such as text, image and video. So far, people have worked on Twitter data from interesting and diverse angles, extracting high-precision information related to feelings [5], collecting fine dust information [6], scaling back prediction [7], topic recognition [8], and so on. Our main contribution to this work is to recognize the work of Twitterers users based on the textual content of their tweets."}, {"heading": "2 Method and Data", "text": "Since there is no such data set, we had to create our own data set. Creating such a data set for all jobs would take a lot of time and resources. In this preparatory work, we focused on identifying people with IT jobs. Therefore, our data set included people who were identified as IT workers or non-IT workers. In this work, we will examine two architectures for classification based on different approaches to document rendering, the first based on the well-known document rendering dictionary model. For the second model, we will use a document rendering based on the term vectors extracted from word2vec. Word2vec is a tool for calculating vector rendering of words introduced by a research team at Google. In the next section, we will explain more about word2vec. We also provide more detailed explanations for data collection strategies as well as our document rendering and classification techniques in the next sections."}, {"heading": "2.1 Word2Vec", "text": "Although displaying words as indices in the vocabulary of the dataset has many advantages for use in NLP tasks, such as simplicity and fast modeling, it ignores possible and obvious similarities between words. For example, the simple techniques of word representation cannot detect the semantic similarity between \"King\" and \"Man,\" as well as the syntactical similarities between \"Flowers\" and \"Cats.\" The ideal word representation for many applications is a representation that is able to detect all possible similarities and also maintain regularities between vectors as much as possible. Regularities are observed as constant vector offsets between pairs of words that share a certain relationship [7]. Some examples of these regularities are listed below: vector (\"King\") + vector (\"Woman\") = vector (\"Queen\") = vector (\"Apple\") - vector (\"Vector\") = vector (\"car\" - \"-\" models to grasp this order \")."}, {"heading": "2.2 Data Preparation", "text": "We used two techniques to build our record. As our first approach, we tried to use Linkedin profiles to get some automatically tagged users. In this approach, we look for people in Linkedin who indicated that they were working in some IT-related jobs. Then, we could use their name and try to find a related Twitter ID to their name LinkedIn. There are some challenges in this approach. Initially, there is no dictionary of IT job titles. To deal with this problem, we have put together a number of job names to cover such jobs. This dictionary includes 183 job titles. The next challenge is that there is no free API in Linkedin to search people. To deal with this challenge, we used the free Bing search engine API. We used a search query such as: {jobtitle} +'site: ca.linkedin.com / in' to get names of people in Linkedin who mention one of our job titles in the description of Linkedin."}, {"heading": "2.3 Classification Architectures", "text": "In our work, we are not looking for signals at a single tweet level, but rather combine the user's most recent tweets from their timelines into a large document. In our first approach, we simply extract the document representations of the labeled record based on the presence of 5000 terms as characteristics. Then, a Naive Bayes model is used to classify these documents, resulting in a high-dimensional and sparse representation for documents. In the second approach, we use a more concise representation for documents. In this approach, we first train word2vec model over our set of unlabeled user lines. As described above, this is an unattended model that can provide some vectorized representations for each word, which are also semantically significant. With this word representations, we create an appropriate representation for documents in our labeled dataset.Different approaches could be considered for creating document representations based on the vectors derived from word2vec."}, {"heading": "3 Implementation and Results", "text": "We used Python together with the sklearn package as our main package for machine learning alongside the gensim package, which includes an implementation of word2vec in Python. We conducted two experiments in this work. In our first experiment, we used the sack-of-words model, in which we used the word vectors we had gained from a preschool phase using word2vec method in the second experiment. In the sack-of-word model, we presented each document as characteristics based on 5000 terms. We used 80 percent of the randomly selected data set for training and the rest as a test set. We used a Naive Bayes model of the training set for classification. The results are presented in Table 1.In the next experiment, we used the document representation based on word2vec in our classification. We used word2vec for pre-training and extracting word vectors from a series of Twitter737 timelines for classification."}, {"heading": "4 Conclusion and Future Works", "text": "Here we presented our work on capturing Twitter users jobs based on their tweets. This is a new application of higher quality Twitter data. Due to the large number of job titles and job categories, in this preparatory work we have tried to identify only people who have an IT-related job title. By using the automatic tagging method we have described in the section on data capture, we can add more labeled data for other jobs to recognize other job categories as well.The other post in this work uses deep learning to produce document representations and show that it can produce better results than word models for this specific application. We used word2vec to extract word vectors and proposed a new document representation based on these vectors. We have rather adopted a naive approach to combine the word vectors that can be improved."}], "references": [{"title": "Target-dependent churn classification in microblogs", "author": ["Amiri H", "III", "D. H"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Twitter sentiment analysis: The good the bad and the omg", "author": ["E. Kouloumpis", "T. Wilson", "J. Moore"], "venue": "In Proceedings of the Fifth International 6  Conference on Weblogs and Social Media,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Fine-grained location extraction from tweets with temporal awareness", "author": ["C. Li", "A. Sun"], "venue": "In Proceedings of the 37th International ACM SIGIR Conference on Research  Development in Information Retrieval (New York, NY, USA,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W. tau Yih", "G. Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2013)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Learning similarity functions for topic detection in online reputation monitoring", "author": ["D. Spina", "J. Gonzalo", "E. Amig\u00f3"], "venue": "In Proceedings of the 37th International ACM SIGIR Conference on Research  Development in Information Retrieval (New York, NY, USA,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "They could extract highly accurate information in terms of sentiments [5], fine grain location information [6], churn prediction [2] , topic detection [8] ,and so on.", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "They could extract highly accurate information in terms of sentiments [5], fine grain location information [6], churn prediction [2] , topic detection [8] ,and so on.", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "They could extract highly accurate information in terms of sentiments [5], fine grain location information [6], churn prediction [2] , topic detection [8] ,and so on.", "startOffset": 129, "endOffset": 132}, {"referenceID": 5, "context": "They could extract highly accurate information in terms of sentiments [5], fine grain location information [6], churn prediction [2] , topic detection [8] ,and so on.", "startOffset": 151, "endOffset": 154}, {"referenceID": 4, "context": "The regularities are observed as constant vector offsets between pairs of words sharing a particular relationship [7].", "startOffset": 114, "endOffset": 117}, {"referenceID": 4, "context": "Figure 1: The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word [7]", "startOffset": 148, "endOffset": 151}, {"referenceID": 1, "context": "Based on [3, 4] we think that using this approach may produce superior results.", "startOffset": 9, "endOffset": 15}], "year": 2017, "abstractText": "In this report, we propose a new application for twitter data called job detection. We identify people\u2019s job category based on their tweets. As a preliminary work, we limiteour task to identify only IT workers from other job holders. We have used and compared both simple bag of words model and a document representation based on Skip-gram model. Our results show that the model based on Skip-gram, achieves a 76% precision and 82% recall. 1", "creator": "LaTeX with hyperref package"}}}