{"id": "1701.03126", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "Attention-Based Multimodal Fusion for Video Description", "abstract": "Currently successful methods for video description are based on encoder-decoder sentence generation using recur-rent neural networks (RNNs). Recent work has shown the advantage of integrating temporal and/or spatial attention mechanisms into these models, in which the decoder net-work predicts each word in the description by selectively giving more weight to encoded features from specific time frames (temporal attention) or to features from specific spatial regions (spatial attention). In this paper, we propose to expand the attention model to selectively attend not just to specific times or spatial regions, but to specific modalities of input such as image features, motion features, and audio features. Our new modality-dependent attention mechanism, which we call multimodal attention, provides a natural way to fuse multimodal information for video description. We evaluate our method on the Youtube2Text dataset, achieving results that are competitive with current state of the art. More importantly, we demonstrate that our model incorporating multimodal attention as well as temporal attention significantly outperforms the model that uses temporal attention alone.", "histories": [["v1", "Wed, 11 Jan 2017 19:16:42 GMT  (286kb,D)", "http://arxiv.org/abs/1701.03126v1", "Submitted to CVPR 2017 for review, 8 pages, 4 figures"], ["v2", "Thu, 9 Mar 2017 22:57:10 GMT  (284kb,D)", "http://arxiv.org/abs/1701.03126v2", "Resubmitted to the rebuttal for CVPR 2017 for review, 8 pages, 4 figures"]], "COMMENTS": "Submitted to CVPR 2017 for review, 8 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.MM", "authors": ["chiori hori", "takaaki hori", "teng-yok lee", "kazuhiro sumi", "john r hershey", "tim k marks"], "accepted": false, "id": "1701.03126"}, "pdf": {"name": "1701.03126.pdf", "metadata": {"source": "CRF", "title": "Attention-Based Multimodal Fusion for Video Description", "authors": ["Chiori Hori", "Takaaki Hori", "Teng-Yok Lee", "Kazuhiro Sumi", "John R. Hershey", "Tim K. Marks"], "emails": ["tmarks}@merl.com", "sumi@it.aoyama.ac.jp"], "sections": [{"heading": "1. Introduction and Related Work", "text": "In fact, most of them are able to play by the rules that they have adopted in recent years."}, {"heading": "2. Encoder-decoder-based sentence generator", "text": "A basic approach to image description is based on sequence-to-sequence learning. The input sequence, i.e. the image sequence, is first encoded into a fixed-dimensional semantic vector, and then the output sequence, i.e., word sequence, is generated from the semantic vector. In this case, however, both the encoder and the decoder (or generator) are usually modeled as Long Short Term Memory (LSTM) networks. Figure 1 shows an example of the LSTM-based encoder decoder architecture. Faced with a sequence of images, X = x2, each image is first fed into a feature extractor that is a pre-programmed CNN for an image or video classification task such as GoogLeNet [15], VGNet [20], or C3D [22]. The sequence of image characteristics, X = 2, is the sequence."}, {"heading": "3. Attention-based sentence generator", "text": "Another approach to video description is an attention-based sequence generator [6] that allows the network to emphasize characteristics of specific times or spatial regions, depending on the current context, which allows the next word to be predicted more accurately. Compared to the basic approach described in Section 2, the attention-based generator can selectively utilize the input functions according to the input and output contexts. The effectiveness of attention models has been demonstrated in many tasks such as machine translation [1]. Figure 2 shows an example of the attention-based sentence generator from video that has a temporal attention mechanism via the input image sequence. The input sequence of feature vectors is achieved using one or more feature extractors. Generally, attention-based generators employ an encoder based on a bi-directional LSTM (BLSTM) or Gated Recurrent Untuits (GRU) for further conversion of each feature so that it contains its characteristic."}, {"heading": "4. Attention-based multimodal fusion", "text": "This section proposes an attention model for handling the fusion of multiple modalities, with each modality having its own sequence of feature vectors = 11. Multimodal inputs such as image characteristics, motion characteristics and audio characteristics are available for video description. In addition, the combination of multiple features from different feature extraction methods is often used effectively to improve descriptive accuracy. In [29] content vectors from VGGNet (image characteristics) and C3D (spatial emporal motion characteristics) are combined into a vector that is used to predict the next word, in the fusion layer where the following activation vectors are calculated instead of Eq. (19), gi = tanh (W (D) s \u2212 1 + di + b (D) s, where di + attention is used to predict the next word."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Dataset", "text": "The data set includes 1,970 video clips with multiple descriptions of natural language. Each video clip is provided with multiple parallel sentences provided by different Mechanical Turkers. There are a total of 80,839 sentences with approximately 41 annotated sentences per clip. Each sentence averages about 8 words. The words contained in each sentence form a vocabulary of 13,010 unique lexical entries. The data set is open domain and covers a wide range of topics including sports, animals and music. Subsequently [38] we divided the data set into a training set of 1,200 video clips, a validation set of 100 clips and a test set consisting of the remaining 670 clips."}, {"heading": "5.2. Video Preprocessing", "text": "The image data is extracted from each video clip, which consists of 24 frames per second, and enlarged to 224 x 224 pixels using the popular implementation in Caffe [11]. To extract image data, a pre-formed GoogLeNet [15] CNN is used to extract fixed-length representations. Features are extracted from the hidden layer pool 5 / 7x7 s1. We select an image from 16 frames from each video clip and feed it into CNN to obtain 1024-dimensional feature vectors in the image, and we also use a VGNet [20] preschooled on the ImageNet dataset [14]. Hidden activation vectors of the fully connected layer fc7 are used for the image functions that generate a sequence of 4096-dimensional feature vectors."}, {"heading": "5.3. Audio Processing", "text": "Unlike previous methods that use the YouTube2Text dataset [28, 18, 29], we also incorporate audio features into our attention-based feature fusion method. Since the YouTube2Text corpus does not contain an audio track, we extracted the audio data from the original video URLs. Although some of the videos were no longer available on YouTube, we were able to collect the audio data for 1649 video clips covering 84% of the corpus. 44 kHz sampled audio data is then sampled down to 16 kHz, and the Mel frequency cepstral coefficients (MFCCs) are extracted from each 50 ms time window with 25 ms shift. The sequence of the 13-dimensional MFCC characteristics is then linked to a vector from each group of 20 consecutive frames, resulting in a sequence of 260-dimensional vectors."}, {"heading": "5.4. Experimental Setup", "text": "The image generation model, i.e. the decoder network, is trained to minimize the cross-entropy criterion using the training set. Image functions are fed to the decoder network by a projection layer of 512 units, while audio functions, i.e. MFCCs, are fed to the BLSTM encoder network, followed by the decoder network. The encoder network has a projection layer of 512 units and bi-directional LSTM layers of 512 cells. The decoder network has an LSTM layer of 512 cells. Each word is embedded in a 256-dimensional vector when fed to the LSTM layer. We use the AdaDelta optimizer [30] to update the parameters often used to optimize attention models. The LSTM and the attention models have been evaluated using Chainer [21] [Video Engine Implementation] to reevaluate the basic metric similarity between the MEobesity and the metabolic parameters."}, {"heading": "5.5. Results and Discussion", "text": "Table 1 shows the evaluation results of the Youtube2text dataset. We compared the performance of our multimodal attention model, the temporal and multimodal attention mechanisms with a simple additive multimodal fusion (Simple Multimodal), unimmodal models with temporal attention (Unimodal) and base systems that used temporal attention, better than the unimmodal models. However, the proposed multimodal attention model outperformed Simple Multimodal. The audio function deteriorates the performance of the baseline because some YouTube data contains noise such as background music, which has nothing to do with the video content. As expected, the multimodal attention model mitigated the effects of the noise of the audio features. In addition, the combination of audio features using our proposed method Delta achieves the best performance of NIDEr for all experimental conditions."}, {"heading": "6. Conclusion", "text": "We have proposed a new modality-dependent attention mechanism, which we call multimodal attention, for video description based on the encoder decoder sentence generation using recursive neural networks (RNNs). In this approach, the attention model selectively takes into account not only certain times, but also certain input modalities such as image functions, spatio-temporal motion functions, and audio functions. This approach provides a natural way to merge multimodal information for video description. We evaluate our method based on the Youtube2Text dataset and achieve results that compete with current methods that use temporal attention models, where the decoder network predicts each word in the description by selectively placing more emphasis on coded features from specific time periods. More importantly, our model, which incorporates multimodal attention and temporal attention, exceeds the model that uses only temporal attention."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio"], "venue": "pages 4945\u20134949,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Microsoft COCO captions: Data collection and evaluation server", "author": ["X. Chen", "H. Fang", "T. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "CoRR, abs/1504.00325,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "On the properties of neural machine translation", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "Encoderdecoder approaches. CoRR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 577\u2013585. Curran Associates, Inc.,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["M.J. Denkowski", "A. Lavie"], "venue": "Proceedings of the Ninth Workshop on Statistical Machine  Translation, WMT@ACL 2014, June 26-27, 2014, Baltimore, Maryland, USA, pages 376\u2013380,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition", "author": ["S. Guadarrama", "N. Krishnamoorthy", "G. Malkarnenkar", "S. Venugopalan", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2712\u20132719,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Dialog state tracking with attention-based sequence-to-sequence learning", "author": ["T. Hori", "H. Wang", "C. Hori", "S. Watanabe", "B. Harsham", "J.L. Roux", "J. Hershey", "Y. Koji", "Y. Jing", "Z. Zhu", "T. Aikawa"], "venue": "2016 IEEE Spoken Language Technology Workshop, SLT 2016, San Diego, CA, USA, December 13-16,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Generating Natural Video Descriptions via Multimodal Processing", "author": ["Q. Jin", "J. Liang", "X. Lin"], "venue": "Interspeech,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1725\u20131732,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097\u20131105. Curran Associates, Inc.,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "CoRR, abs/1312.4400,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["R. Lowe", "N. Pow", "I. Serban", "J. Pineau"], "venue": "Proceedings of the SIGDIAL 2015 Conference, The 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 2-4 September 2015, Prague, Czech Republic, pages 285\u2013294,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (mrnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "CoRR, abs/1412.6632,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Y. Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui"], "venue": "CoRR, abs/1505.01861,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu"], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA., pages 311\u2013318,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Chainer: a nextgeneration open source framework for deep learning", "author": ["S. Tokui", "K. Oono", "S. Hido", "J. Clayton"], "venue": "Proceedings of Workshop on Machine Learning Systems (Learn-  ingSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["D. Tran", "L.D. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 4489\u20134497,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C.L. Zitnick", "D. Parikh"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 4566\u20134575,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R.J. Mooney", "K. Saenko"], "venue": "NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Denver, Colorado, USA, May 31 - June 5, 2015, pages 1494\u20131504,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 3156\u20133164,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Action Recognition by Dense Trajectories", "author": ["H. Wang", "A. Kl\u00e4ser", "C. Schmid", "C.-L. Liu"], "venue": "IEEE Conference on Computer Vision & Pattern Recognition, pages 3169\u20133176, Colorado Springs, United States, June", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 2048\u20132057,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C.J. Pal", "H. Larochelle", "A.C. Courville"], "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7- 13, 2015, pages 4507\u20134515,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["H. Yu", "J. Wang", "Z. Huang", "Y. Yang", "W. Xu"], "venue": "CoRR, abs/1510.07712,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "CoRR, abs/1212.5701,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 24, "context": "Sentence generation using an encoder-decoder architecture was originally used for neural machine translation (NMT), in which sentences in a source language are converted into sentences in a target language [25, 5].", "startOffset": 206, "endOffset": 213}, {"referenceID": 4, "context": "Sentence generation using an encoder-decoder architecture was originally used for neural machine translation (NMT), in which sentences in a source language are converted into sentences in a target language [25, 5].", "startOffset": 206, "endOffset": 213}, {"referenceID": 0, "context": "However, the fixed length of the feature vector limited performance, particularly on long input sentences, so [1] proposed to encode the input sentence as a sequence of feature vectors, employing a recurrent neural network (RNN)-based soft attention model to enable the decoder to pay attention to features derived from specific words of the input sentence when generating each output word.", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "The encoder-decoder based sequence to sequence framework has been applied not only to machine translation but also to other application areas including speech recognition [2], image captioning [25], and dialog management [16].", "startOffset": 171, "endOffset": 174}, {"referenceID": 24, "context": "The encoder-decoder based sequence to sequence framework has been applied not only to machine translation but also to other application areas including speech recognition [2], image captioning [25], and dialog management [16].", "startOffset": 193, "endOffset": 197}, {"referenceID": 15, "context": "The encoder-decoder based sequence to sequence framework has been applied not only to machine translation but also to other application areas including speech recognition [2], image captioning [25], and dialog management [16].", "startOffset": 221, "endOffset": 225}, {"referenceID": 16, "context": "Recent work on RNN-based image captioning includes [17, 25].", "startOffset": 51, "endOffset": 59}, {"referenceID": 24, "context": "Recent work on RNN-based image captioning includes [17, 25].", "startOffset": 51, "endOffset": 59}, {"referenceID": 26, "context": "To improve performance, [27] added an attention mechanism, to enable focusing on specific parts of the image when generating each word of the description.", "startOffset": 24, "endOffset": 28}, {"referenceID": 23, "context": "Encoder-decoder networks have also been applied to the task of video description [24].", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "In this task, the inputs to the encoder network are video information features that may include static image features extracted using convolutional neural networks (CNNs), temporal dynamics of videos extracted using spatiotemporal 3D CNNs [22], dense trajectories [26], optical flow, and audio features [12].", "startOffset": 239, "endOffset": 243}, {"referenceID": 25, "context": "In this task, the inputs to the encoder network are video information features that may include static image features extracted using convolutional neural networks (CNNs), temporal dynamics of videos extracted using spatiotemporal 3D CNNs [22], dense trajectories [26], optical flow, and audio features [12].", "startOffset": 264, "endOffset": 268}, {"referenceID": 11, "context": "In this task, the inputs to the encoder network are video information features that may include static image features extracted using convolutional neural networks (CNNs), temporal dynamics of videos extracted using spatiotemporal 3D CNNs [22], dense trajectories [26], optical flow, and audio features [12].", "startOffset": 303, "endOffset": 307}, {"referenceID": 8, "context": "sequences based on language models using recurrent neural networks (RNNs) based on long short-term memory (LSTM) units [9] or gated recurrent units (GRUs) [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "sequences based on language models using recurrent neural networks (RNNs) based on long short-term memory (LSTM) units [9] or gated recurrent units (GRUs) [4].", "startOffset": 155, "endOffset": 158}, {"referenceID": 0, "context": "However, attention mechanisms have been used to boost the network\u2019s ability to retrieve the relevant features from the corresponding parts of the input, in applications such as machine translation [1], speech recognition [2], image captioning [27], and dialog management [10].", "startOffset": 197, "endOffset": 200}, {"referenceID": 1, "context": "However, attention mechanisms have been used to boost the network\u2019s ability to retrieve the relevant features from the corresponding parts of the input, in applications such as machine translation [1], speech recognition [2], image captioning [27], and dialog management [10].", "startOffset": 221, "endOffset": 224}, {"referenceID": 26, "context": "However, attention mechanisms have been used to boost the network\u2019s ability to retrieve the relevant features from the corresponding parts of the input, in applications such as machine translation [1], speech recognition [2], image captioning [27], and dialog management [10].", "startOffset": 243, "endOffset": 247}, {"referenceID": 9, "context": "However, attention mechanisms have been used to boost the network\u2019s ability to retrieve the relevant features from the corresponding parts of the input, in applications such as machine translation [1], speech recognition [2], image captioning [27], and dialog management [10].", "startOffset": 271, "endOffset": 275}, {"referenceID": 27, "context": "In recent work, these attention mechanisms have been applied to video description [28, 29].", "startOffset": 82, "endOffset": 90}, {"referenceID": 28, "context": "In recent work, these attention mechanisms have been applied to video description [28, 29].", "startOffset": 82, "endOffset": 90}, {"referenceID": 14, "context": ", xL, each image is first fed to a feature extractor, which can be a pretrained CNN for an image or video classification task such as GoogLeNet [15], VGGNet [20], or C3D [22].", "startOffset": 144, "endOffset": 148}, {"referenceID": 19, "context": ", xL, each image is first fed to a feature extractor, which can be a pretrained CNN for an image or video classification task such as GoogLeNet [15], VGGNet [20], or C3D [22].", "startOffset": 157, "endOffset": 161}, {"referenceID": 21, "context": ", xL, each image is first fed to a feature extractor, which can be a pretrained CNN for an image or video classification task such as GoogLeNet [15], VGGNet [20], or C3D [22].", "startOffset": 170, "endOffset": 174}, {"referenceID": 5, "context": "Another approach to video description is an attentionbased sequence generator [6], which enables the network to emphasize features from specific times or spatial regions depending on the current context, enabling the next word to be predicted more accurately.", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "The efficacy of attention models has been shown in many tasks such as machine translation [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "The decoder network is an Attention-based Recurrent Sequence Generator (ARSG) [1][6] that generates an output label sequence with content vectors ci.", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "The decoder network is an Attention-based Recurrent Sequence Generator (ARSG) [1][6] that generates an output label sequence with content vectors ci.", "startOffset": 81, "endOffset": 84}, {"referenceID": 0, "context": "The attention weights are computed in the same manner as in [1]:", "startOffset": 60, "endOffset": 63}, {"referenceID": 28, "context": "In [29], content vectors from VGGNet (image features) and C3D (spatiotemporal motion features) are combined into one vector, which is used to predict the next word.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "We evaluated our proposed feature fusion using the Youtube2Text video corpus [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 14, "context": "For extracting image features, a pretrained GoogLeNet [15] CNN is used to extract fixed-length representation with the help of the popular implementation in Caffe [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "For extracting image features, a pretrained GoogLeNet [15] CNN is used to extract fixed-length representation with the help of the popular implementation in Caffe [11].", "startOffset": 163, "endOffset": 167}, {"referenceID": 19, "context": "We also use a VGGNet [20] that was pretrained on the ImageNet dataset [14].", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "We also use a VGGNet [20] that was pretrained on the ImageNet dataset [14].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "Furthermore, to model motion and short-term spatiotemporal activity, we use the pretrained C3D [22] (which was trained on the Sports-1M dataset [13]).", "startOffset": 95, "endOffset": 99}, {"referenceID": 12, "context": "Furthermore, to model motion and short-term spatiotemporal activity, we use the pretrained C3D [22] (which was trained on the Sports-1M dataset [13]).", "startOffset": 144, "endOffset": 148}, {"referenceID": 27, "context": "Unlike previous methods that use the YouTube2Text dataset [28, 18, 29], we also incorporate audio features, to use in our attention-based feature fusion method.", "startOffset": 58, "endOffset": 70}, {"referenceID": 17, "context": "Unlike previous methods that use the YouTube2Text dataset [28, 18, 29], we also incorporate audio features, to use in our attention-based feature fusion method.", "startOffset": 58, "endOffset": 70}, {"referenceID": 28, "context": "Unlike previous methods that use the YouTube2Text dataset [28, 18, 29], we also incorporate audio features, to use in our attention-based feature fusion method.", "startOffset": 58, "endOffset": 70}, {"referenceID": 27, "context": "TA [28] Temporal GoogLeNet 3D CNN 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "517 LSTM-E [18] VGGNet C3D 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 28, "context": "310 h-RNN [29] Temporal VGGNet C3D 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "We apply the AdaDelta optimizer [30] to update the parameters, which is widely used for optimizing attention models.", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "The LSTM and attention models were implemented using Chainer [21].", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "The similarity between ground truth and automatic video description results are evaluated using machine-translationmotivated metrics: BLEU [19], METEOR [7], and the newly proposed metric for image description, CIDEr [23].", "startOffset": 139, "endOffset": 143}, {"referenceID": 6, "context": "The similarity between ground truth and automatic video description results are evaluated using machine-translationmotivated metrics: BLEU [19], METEOR [7], and the newly proposed metric for image description, CIDEr [23].", "startOffset": 152, "endOffset": 155}, {"referenceID": 22, "context": "The similarity between ground truth and automatic video description results are evaluated using machine-translationmotivated metrics: BLEU [19], METEOR [7], and the newly proposed metric for image description, CIDEr [23].", "startOffset": 216, "endOffset": 220}, {"referenceID": 2, "context": "We used the publicly available evaluation script prepared for image captioning challenge [3].", "startOffset": 89, "endOffset": 92}, {"referenceID": 27, "context": "In contrast to the existing systems, our temporal attention system which used only static image features (Unimodal) outperformed TA using combination of static image and dynamic video features [28].", "startOffset": 193, "endOffset": 197}, {"referenceID": 17, "context": "Our proposed attention mechanisms outperformed LSTM-E [18] which does not use attention mechanisms.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "Our Simple Multimodal system using temporal attention is the same basic structure used by h-RNN as well as the same features extracted from VGGNet [20] and C3D [22].", "startOffset": 147, "endOffset": 151}, {"referenceID": 21, "context": "Our Simple Multimodal system using temporal attention is the same basic structure used by h-RNN as well as the same features extracted from VGGNet [20] and C3D [22].", "startOffset": 160, "endOffset": 164}], "year": 2017, "abstractText": "Currently successful methods for video description are based on encoder-decoder sentence generation using recurrent neural networks (RNNs). Recent work has shown the advantage of integrating temporal and/or spatial attention mechanisms into these models, in which the decoder network predicts each word in the description by selectively giving more weight to encoded features from specific time frames (temporal attention) or to features from specific spatial regions (spatial attention). In this paper, we propose to expand the attention model to selectively attend not just to specific times or spatial regions, but to specific modalities of input such as image features, motion features, and audio features. Our new modality-dependent attention mechanism, which we call multimodal attention, provides a natural way to fuse multimodal information for video description. We evaluate our method on the Youtube2Text dataset, achieving results that are competitive with current state of the art. More importantly, we demonstrate that our model incorporating multimodal attention as well as temporal attention significantly outperforms the model that uses temporal attention alone.", "creator": "LaTeX with hyperref package"}}}