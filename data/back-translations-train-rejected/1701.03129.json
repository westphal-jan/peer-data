{"id": "1701.03129", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "De-identification In practice", "abstract": "We report our effort to identify the sensitive information, subset of data items listed by HIPAA (Health Insurance Portability and Accountability), from medical text using the recent advances in natural language processing and machine learning techniques. We represent the words with high dimensional continuous vectors learned by a variant of Word2Vec called Continous Bag Of Words (CBOW). We feed the word vectors into a simple neural network with a Long Short-Term Memory (LSTM) architecture. Without any attempts to extract manually crafted features and considering that our medical dataset is too small to be fed into neural network, we obtained promising results. The results thrilled us to think about the larger scale of the project with precise parameter tuning and other possible improvements.", "histories": [["v1", "Wed, 11 Jan 2017 19:22:56 GMT  (207kb,D)", "http://arxiv.org/abs/1701.03129v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["besat kassaie"], "accepted": false, "id": "1701.03129"}, "pdf": {"name": "1701.03129.pdf", "metadata": {"source": "CRF", "title": "De-identification In practice", "authors": ["Besat Kassaie"], "emails": [], "sections": [{"heading": "1 Motivation", "text": "For example, the Optometry Clinic at the University of Waterloo's School of Optometry and Vision Science, which is one of the largest vision care centers in Canada, has adopted an EHR system. Indeed, Transparency Market Research has predicted that the value of global eHealth data will rise to $30.28 billion by the end of 2023. As well as supporting individual health care, such data has the potential to be used on a broader scale to improve the lives of multiple generations, and researchers from various fields such as public health, social science, and economics can gain invaluable insights from this data."}, {"heading": "2 Related Works", "text": "The current project falls into two different areas of work, namely automatic free text identification and the application of deep learning processes in natural language processing. Extracting specific information from texts such as company names and emotions is not a new topic. In fact, the de-identification task as a specific type of named entity recognition (NLP), in which the goal is to identify some coarse particle units such as places, data and people, is behind 25 years of research. [18] Due to the importance of automatic de-identification of text, an NLP challenge is considered as part of i2b2Informatics to integrate biology and the person. [30] The goal of this challenge is the same as ours, although the datasets used are different."}, {"heading": "2.1 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2 Word Representations", "text": "If we rely solely on naive representations like \"Bag of Words,\" i.e. using sparse vectors to represent words as atomic units, we ignore many levels of semantics and syntactics embedded in words and also in their relations to other words. On the other hand, relationships like \"similarity\" are not static and permanent concepts and vary based on context. Recently, some interesting work has come out that attempts to capture some levels of semantic and syntactic similarities between words by representing words as continuous vectors. We need a representation that can be changed based on context, and the semantics and syntactics that exist in a specific context. An approach that has been very well studied so far is the use of words as neighbors that represent a large number of words that exist in a document, or it can be a small window that surrounds the word."}, {"heading": "2.3 Conditional Random Fields", "text": "Among other things, CRFs and semiCRFs [6] have shown acceptable performance in many areas such as natural language processing and biomedical fields. CRFs are undirected graphical models that offer a discriminatory approach to predicting multiple, interdependent output variables [7]. Extracting suitable features and introducing them into the model as feature functions is an important labor-intensive step in implementing CRFs."}, {"heading": "2.4 LSTM", "text": "Wherever we are confronted with data that have a sequential character, such as text or spoken language, the need to capture the context for each data unit that can be a word, an audio signal or a sentence is critical. Previous methods for addressing these problems used hand-crafted systems to deal with the sequential nature of data. Recently, due to advances in neural networks, among others, Recurrent Neural Networks (RNN) have been extensively applied to machine learning tasks related to sequential data. Handwritten recognition [8], [9], machine translation [10] and voice modeling [11] are few. In theory, standard RNN architectures are able to capture the long-term dependencies, but in practice, the context for which they are able to maintain primary dependencies is very small. In fact, RNN's NN architectures are able to capture the long-term dependencies, however, the context for which they are able to maintain M's primary dependencies is the long-term ones."}, {"heading": "3 DataSet", "text": "We have used two different datasets for this project: the AMIA dataset to decrypt the challenge [29] and the optometry dataset. [29] The reader can see [29] for the description of their dataset. The optometry dataset contains 68,278 datasets belonging to patients for a period of one year. Each dataset contains seven points where the doctor can write comments and notes, such as DIAGNONSIS, CHIEF-COMPLAINT, COMMENT, NOTE, EXTERNAL among others. Our goal is to identify sensitive information from these seven points. 299 datasets were randomly selected to be commented on manually. We consider our problem as a sequence labeling problem. We commented on all 299 files in the same way as described in [29] to feed them into the CRFs model. We have performed some additional processes on datasets to train our LM model as we will later describe it."}, {"heading": "4 Experiments", "text": "The HIPAA Regulation requires health organizations to remove Protected Health Information (PHI) from data sets before sharing it with researchers. HIPAA provides a list of 18 identifiers for the purpose of entitiidentification. Although the majority of these categories have been observed in the optometry dataset, in this project we will focus on identifying six categories that were previously tested in the AMIA i2b2 entitlement challenge [30]. By focusing on this subset, we are able to compare our system performance to the best system that participated in the NLP challenge [29] [TODO: Need for a New i2b2 dataset 2014-2016]."}, {"heading": "4.1 Evaluation", "text": "We have used three standard metrics to evaluate our experiments: The metrics are as follows: Precision = TruePositiveTruePositive + FalsePositiveRecall = TruePositiveTruePositive + FalseNegativeF \u2212 measure = 2 \u0445 Precision \u043a RecallPrecision + RecallWe have calculated all of the above metrics at the token level. For example, let's assume the position PHI, which consists of three tokens, such as Canada National Library. The evaluation can be done at the instance or token level. The former means that if all the tokens in PHI are correctly tagged, then it is considered as true positives. Token level takes all three words separately. Note that, as we will explain later, we use a sliding window approach to generate fixed length segments to insert them into the LSTM model, so each token falls into more than one segment and takes different labels. However, the metrics will be comparable to the result merged on the LSTM model."}, {"heading": "4.2 Experiment Using CRFs", "text": "Since we found that the way the six PHIs in the optometry dataset are represented and the dataset provided by i2b2 is quite similar, we decided to try the tool used by [21] for de-identification on the optometry dataset. [21] Carafe is a sequence tagging system that was implemented for the phrase identification task and showed significant performance. In fact, Carafe implemented conditional random fields (CRFs) for the phrase identification task. [21] introduced some new and task-specific features to Carafe to enable the phrase identification task."}, {"heading": "4.3 Experiment Using Deep Learning", "text": "Since RNNs have shown significant performance in labeling sequences in recent studies, we opted for LSTM, a variant of RNN in our work. LSTM allows us to use automatically extracted features related to word embedding. Since distributed continuous word representation is a powerful technique to capture the semantics and syntactics of relationships between words, we used the CBOW architecture to learn word representation, as shown in Figure 2. We used all 68,278 optometry data sets to build the model. In this architecture, we set the window size to five. We ignore words with a frequency of less than two. We replace all rare words, i.e. once in the training set and unseen words in the test set, with UNK [32]. We replace all sequences of numbers with DIGIT [33]. For example, in 1990 and 2005 both are assigned to DIGITDIGITDIGITGITY."}, {"heading": "4.3.1 Evaluating Word2vec", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution that meets the needs of the individual."}, {"heading": "4.3.2 Data Preprocessing", "text": "For the LSTM model, we have labeled each word to show whether it is part of the PHI or not. We have assigned a code for each label. Table 10 shows some of the labels along with the assigned codes. Since we need a fixed length of strings to feed into the LSTM, we have created a sliding window over the training set. Sliding windows can break some of the labels. To deal with the broken labels, all labels are replaced by \"O\" in a broken tag and only if the label exists fully within the window, do we keep it. To feed each label into the LSTM model, it is replaced by the corresponding code, as in Table 10. Also, each word is replaced by a vector of dimension 200 that comes from the Word2Vec model."}, {"heading": "4.3.3 Sequence Labeling with LSTM for Optometry Dataset", "text": "The architecture of our model is illustrated in Figure 1. In this architecture, word sequences of length 15 are fed into the first network layer, which consists of 15 LSTM components. These word sequences are obtained by applying a sliding window over the previously described training set.Each word xi in these sequences is represented by a vector of length 200.This vector was obtained from the previously described Word2Vec model.The output of each LSTM component hi would be another vector of length 200.A dense neural network is attached to each of these output vectors.A dense neural network is simply a fully connected neural network. This network has an output vector of length 17 called yi, which represents a probability distribution over possible label values.We used drop-out of 0.2, binary cross entropy for the loss function and Adam [34] as an optimization algorithm in our experiment."}, {"heading": "4.3.4 LSTM for AMIA dataset", "text": "In this case, however, we do not have access to a variety of datasets to learn word representation based on the AMIA dataset. Therefore, we used a set of pre-formed word vectors based on message data provided by Google. Our strategy was to use the few datasets in the AMIA dataset to build a primary vector representation for words. We also used the vectors from the pre-trained model as a second source when we could not find word representation based on the primary model, Table 11 shows the distribution of words obtained from different models, and then applied floating window approach over the AMIA dataset, similar to what we did with the optometry dataset, to provide word sequences. We used the same training and test set as AMIA. We used the same LSTM network architecture that we used for optometric data. Results are provided in Table 4. In this experiment, we used this vector traced 300 as this model."}, {"heading": "5 Discussion", "text": "The absence of data is a determining factor in the use of deep neural networks. But in our case, we had a few instances described for the formation of our network. This problem is more obvious for rare tags in our data set, such as telephone or hospital names. The results in Table 6 show that our deep learning model was not able to detect all cases. In contrast, the model based on these tags was more successful, because they introduced very specific features based on regular expressions, and that they enriched their system by using external lessons."}], "references": [{"title": "A Neural Probabilistic Language Model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1137\u20131155, Mar. 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Neurocomputing: Foundations of Research", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "pp. 696\u2013699, Cambridge, MA, USA: MIT Press, 1988.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1988}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv:1301.3781 [cs], Jan. 2013. arXiv: 1301.3781.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["T. Mikolov", "S.W.-t. Yih", "G. Zweig"], "venue": "Microsoft Research, May 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficiently Inducing Features of Conditional Random Fields", "author": ["A. McCallum"], "venue": "Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence, UAI\u201903, (San Francisco, CA, USA), pp. 403\u2013410, Morgan Kaufmann Publishers Inc., 2003.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "An introduction to conditional random fields", "author": ["C. Sutton", "A. McCallum"], "venue": "Foundations and Trends in Machine Learning, vol. 4, no. 4, p. 267373, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast and robust training of recurrent neural networks for offline handwriting recognition", "author": ["P. Doetsch", "M. Kozielski", "H. Ney"], "venue": "International Conference on Frontiers in Handwriting Recognition, (Crete, Greece), pp. 279\u2013 284, Sept. 2014. 15", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "A Novel Connectionist System for Unconstrained Handwriting Recognition", "author": ["A. Graves", "M. Liwicki", "S. Fernndez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 31, pp. 855\u2013 868, May 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Addressing the Rare Word Problem in Neural Machine Translation", "author": ["T. Luong", "I. Sutskever", "Q.V. Le", "O. Vinyals", "W. Zaremba"], "venue": "2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent Neural Network Regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv:1409.2329 [cs], Sept. 2014. arXiv: 1409.2329.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised Sequence Labelling with Recurrent Neural Networks, vol", "author": ["A. Graves"], "venue": "Studies in Computational Intelligence. Springer,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, pp. 1735\u20131780, Nov. 1997.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, vol. 18, no. 5-6, pp. 602\u2013610, 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning to Forget: Continual Prediction with LSTM", "author": ["F.A. Gers", "J.A. Schmidhuber", "F.A. Cummins"], "venue": "Neural Comput., vol. 12, pp. 2451\u20132471, Oct. 2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "An empirical exploration of recurrent network architectures", "author": ["R. Jozefowicz", "W. Zaremba", "I. Sutskever"], "venue": "2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to Identify Emotions in Text", "author": ["C. Strapparava", "R. Mihalcea"], "venue": "Proceedings of the 2008 ACM Symposium on Applied Computing, SAC \u201908, (New York, NY, USA), pp. 1556\u20131560, ACM, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Extracting company names from text", "author": ["L.F. Rau"], "venue": ", Seventh IEEE Conference on Artificial Intelligence Applications, 1991. Proceedings, vol. i, pp. 29\u201332, Feb. 1991.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1991}, {"title": "Message Understanding Conference-6: A Brief History", "author": ["R. Grishman", "B. Sundheim"], "venue": "Proceedings of the 16th Conference on Computational Linguistics - Volume 1, COLING \u201996, (Stroudsburg, PA, USA), pp. 466\u2013 471, Association for Computational Linguistics, 1996.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1996}, {"title": "State-of-the-art anonymization of medical records using an iterative machine learning framework", "author": ["G. Szarvas", "R. Farkas", "R. Busa-Fekete"], "venue": "Journal of the American Medical Informatics Association : JAMIA, vol. 14, no. 5, pp. 574\u2013580, 2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Rapidly Retargetable Approaches to De-identification in Medical Records", "author": ["B. Wellner", "M. Huyck", "S. Mardis", "J. Aberdeen", "A. Morgan", "L. Peshkin", "A. Yeh", "J. Hitzeman", "L. Hirschman"], "venue": "Journal of the American Medical Informatics Association : JAMIA, vol. 14, no. 5, pp. 564\u2013573, 2007. 16", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Automated de-identification and categorization of medical records", "author": ["R. Guillen"], "venue": "2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Applying a SVM based chunker and a text classifier to the deid challenge", "author": ["K. Hara"], "venue": "2006.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Automatic De-identification of Electronic Medical Records using Token-level and Character-level Conditional Random Fields", "author": ["Z. Liu", "Y. Chen", "B. Tang", "X. Wang", "Q. Chen", "H. Li", "J. Wang", "Q. Deng", "S. Zhu"], "venue": "Journal of biomedical informatics, vol. 58, pp. S47\u2013S52, Dec. 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Opinion mining with deep recurrent neural networks", "author": ["O. \u0130rsoy", "C. Cardie"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 720\u2013728, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid speech recognition with deep bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "A. Mohamed"], "venue": "2013 IEEE Workshop on Automatic Speech Recognition and Understanding, Olomouc, Czech Republic, December 8-12, 2013, pp. 273\u2013278, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Named Entity Recognition with Long Short-term Memory", "author": ["J. Hammerton"], "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 - Volume 4, CONLL \u201903, (Stroudsburg, PA, USA), pp. 172\u2013175, Association for Computational Linguistics, 2003.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "De-identification of Patient Notes with Recurrent Neural Networks", "author": ["F. Dernoncourt", "J.Y. Lee", "O. Uzuner", "P. Szolovits"], "venue": "arXiv:1606.03475 [cs, stat], June 2016. arXiv: 1606.03475.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Evaluating the State-of-the-Art in Automatic De-identification", "author": ["z. Uzuner", "Y. Luo", "P. Szolovits"], "venue": "Journal of the American Medical Informatics Association : JAMIA, vol. 14, no. 5, pp. 550\u2013563, 2007.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Recurrent Neural Networks for Language Understanding", "author": ["K. Yao", "G. Zweig", "M.-Y. Hwang", "Y. Shi", "D. Yu"], "venue": "Microsoft Research, Aug. 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Natural Language Processing (almost) from Scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "arXiv:1103.0398 [cs], Mar. 2011. arXiv: 1103.0398.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv:1412.6980 [cs], Dec. 2014. arXiv: 1412.6980. 17", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "Extracting specific information from text such as ,in a wide range, from company names to emotions[17] is not a new topic.", "startOffset": 98, "endOffset": 102}, {"referenceID": 16, "context": "In fact if we consider the de-identification task as a specific kind of Named Entity Recognition (NER) in which the aim is to identify some coarse grain entities such as locations, dates and person it has 25 years of research behind [18],[19].", "startOffset": 233, "endOffset": 237}, {"referenceID": 17, "context": "In fact if we consider the de-identification task as a specific kind of Named Entity Recognition (NER) in which the aim is to identify some coarse grain entities such as locations, dates and person it has 25 years of research behind [18],[19].", "startOffset": 238, "endOffset": 242}, {"referenceID": 18, "context": "Some of the approaches that have been applied in this challenge includes classification[20], sequence labeling using CRFs [21], rulebased mechanisms[22] or a hybrid of other approaches [23].", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "Some of the approaches that have been applied in this challenge includes classification[20], sequence labeling using CRFs [21], rulebased mechanisms[22] or a hybrid of other approaches [23].", "startOffset": 122, "endOffset": 126}, {"referenceID": 20, "context": "Some of the approaches that have been applied in this challenge includes classification[20], sequence labeling using CRFs [21], rulebased mechanisms[22] or a hybrid of other approaches [23].", "startOffset": 148, "endOffset": 152}, {"referenceID": 21, "context": "Some of the approaches that have been applied in this challenge includes classification[20], sequence labeling using CRFs [21], rulebased mechanisms[22] or a hybrid of other approaches [23].", "startOffset": 185, "endOffset": 189}, {"referenceID": 22, "context": "Other works came out which essentially were founded on the challenge approaches such as [24] in which they proposed a hybrid system of rule-based and token/character-level CRFs.", "startOffset": 88, "endOffset": 92}, {"referenceID": 23, "context": "There are plenty of NLP tasks on which variants of deep neural network such as Recurrent Neural Networks have been applied successfully such as opinion mining [25], speech recognition [26], language modeling [11], Named Entity Recognition [27], and recently de idenntification of patient notes [28].", "startOffset": 159, "endOffset": 163}, {"referenceID": 24, "context": "There are plenty of NLP tasks on which variants of deep neural network such as Recurrent Neural Networks have been applied successfully such as opinion mining [25], speech recognition [26], language modeling [11], Named Entity Recognition [27], and recently de idenntification of patient notes [28].", "startOffset": 184, "endOffset": 188}, {"referenceID": 9, "context": "There are plenty of NLP tasks on which variants of deep neural network such as Recurrent Neural Networks have been applied successfully such as opinion mining [25], speech recognition [26], language modeling [11], Named Entity Recognition [27], and recently de idenntification of patient notes [28].", "startOffset": 208, "endOffset": 212}, {"referenceID": 25, "context": "There are plenty of NLP tasks on which variants of deep neural network such as Recurrent Neural Networks have been applied successfully such as opinion mining [25], speech recognition [26], language modeling [11], Named Entity Recognition [27], and recently de idenntification of patient notes [28].", "startOffset": 239, "endOffset": 243}, {"referenceID": 26, "context": "There are plenty of NLP tasks on which variants of deep neural network such as Recurrent Neural Networks have been applied successfully such as opinion mining [25], speech recognition [26], language modeling [11], Named Entity Recognition [27], and recently de idenntification of patient notes [28].", "startOffset": 294, "endOffset": 298}, {"referenceID": 26, "context": "The objective and methods used in our work is quite similar to [28] from an high level view.", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "Our way of building the word representation is slightly different as we use a relatively large dataset to learn them, whereas [28] exploits pre-trained word vectors.", "startOffset": 126, "endOffset": 130}, {"referenceID": 26, "context": "Our current LSTM architecture is also different from [28].", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "There have been designed various neural network based models to capture this representation [1],[2], [3].", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "There have been designed various neural network based models to capture this representation [1],[2], [3].", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "Among the proposed techniques we use the model proposed in [4].", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "Two quite similar architectures, are proposed in [4] called Continuous Bag Of Words and Continuous Skip-gram.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "In fact the learned word representations capture syntactic and semantic regularities[5].", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "CRFs and semiCRFs[6] have shown acceptable performance in many areas such as natural language processing, and biomedical domains among others.", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "CRFs are undirected graphical models which provides a discriminative approach to predict multiple output variables which depend on each other[7].", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "Handwriting recognition [8], [9], machine translation [10], and language modeling [11] are a few among others.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "Handwriting recognition [8], [9], machine translation [10], and language modeling [11] are a few among others.", "startOffset": 29, "endOffset": 32}, {"referenceID": 8, "context": "Handwriting recognition [8], [9], machine translation [10], and language modeling [11] are a few among others.", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "Handwriting recognition [8], [9], machine translation [10], and language modeling [11] are a few among others.", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "sensitivity of the network and subsequently the output to the inputs decrease over time [12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "LSTMs [13] are a type of RNNs, that are more effective at capturing long-term temporal dependencies without the optimization problems that comes with standard RNNs.", "startOffset": 6, "endOffset": 10}, {"referenceID": 12, "context": "Different architectures of LSTM such as Bidirectional Long ShortTerm Memory [14] have been designed in which LSTM get trained in both directions over inputs.", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "For the current project we have used an architecture quite similar to the primary LSTM architecture introduced in [15].", "startOffset": 114, "endOffset": 118}, {"referenceID": 14, "context": "To improve the performance of the LSTM, as recommended in [16], we add a bias value equal to 1 to the forget gate.", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "We have used two different datasets for this project: AMIA de-identification challenge dataset [29] and the Optometry dataset.", "startOffset": 95, "endOffset": 99}, {"referenceID": 27, "context": "The reader can look at [29] for the description of their dataset.", "startOffset": 23, "endOffset": 27}, {"referenceID": 27, "context": "We annotated all 299 files in the same way as described in [29] to be feed into the CRFs model.", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": "By focusing on this subset we are able to compare our system performance with the best system [21] participated in the NLP challenge [29] [TODO: need to find i2b2 new dataset 2014-2016].", "startOffset": 94, "endOffset": 98}, {"referenceID": 27, "context": "By focusing on this subset we are able to compare our system performance with the best system [21] participated in the NLP challenge [29] [TODO: need to find i2b2 new dataset 2014-2016].", "startOffset": 133, "endOffset": 137}, {"referenceID": 19, "context": "As we found that the way of representing the six PHIs in the optometry dataset and the dataset provided by i2b2 is quite similar, we decided to try the tool [31] used by [21] for de-identification on the optometry dataset.", "startOffset": 170, "endOffset": 174}, {"referenceID": 19, "context": "[21] defines the de-identification as a sequence labeling task for which graphical models can effectively predict the output variables.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] have introduced some new and task specific features to Carafe to make it applicable on the deidentification task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "A bit of discrepancies exist in our results in compare to the [21] due to the some parameter tuning.", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "On the other hand [29] created this dataset by combining some structured fields so we can see some regularities in each record.", "startOffset": 18, "endOffset": 22}, {"referenceID": 28, "context": "and unseen words in the test set with UNK [32].", "startOffset": 42, "endOffset": 46}, {"referenceID": 29, "context": "We replace all sequences of numbers with DIGIT [33].", "startOffset": 47, "endOffset": 51}, {"referenceID": 2, "context": "Figure 2: Continuous Bag of Words [4]", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "We used the test set presented in [4].", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "In [4] they designed a set of questions for two pairs of words holding the same similarity relations among.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "For pairs like (France, Paris) and (Canada, Ottawa), a possible question to be asked is What is the word that is similar to France in the same way as Canada is similar to Ottawa? For answering this question, [4] gets the corresponding representation vectors for Canada, Ottawa, France and by a simple linear transformation such as following X = vector(Ottawa)\u2212vector(Canada)+vector(France) , they look for a word with highest cosine similarity with vector X.", "startOffset": 208, "endOffset": 211}, {"referenceID": 2, "context": "embeddings was sufficient as others, such as [4], have used hundred of millions of words to build their representations.", "startOffset": 45, "endOffset": 48}, {"referenceID": 30, "context": "2, Binary Cross Entropy for loss function and Adam [34] as our optimization algorithm.", "startOffset": 51, "endOffset": 55}], "year": 2017, "abstractText": "We report our effort to identify the sensitive information, subset of data items listed by HIPAA(Health Insurance Portability and Accountability), from medical text using the recent advances in natural language processing and machine learning techniques. We represent the words with high dimensional continuous vectors learned by a variant of Word2Vec called Continous Bag Of Words(CBOW). We feed the word vectors into a simple neural network with a Long Short-Term Memory (LSTM) architecture. Without any attempts to extract manually crafted features and considering that our medical dataset is too small to be fed into neural network, we obtained promising results. The results thrilled us to think about the larger scale of the project with precise parameter tuning and other possible improvements. 1", "creator": "LaTeX with hyperref package"}}}