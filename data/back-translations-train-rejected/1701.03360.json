{"id": "1701.03360", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2017", "title": "Residual LSTM: Design of a Deep Recurrent Architecture for Distant Speech Recognition", "abstract": "In this paper, a novel architecture for a deep recurrent neural network, residual LSTM is introduced. A plain LSTM has an internal memory cell that can learn long term dependencies of sequential data. It also provides a temporal shortcut path to avoid vanishing or exploding gradients in the temporal domain. The proposed residual LSTM architecture provides an additional spatial shortcut path from lower layers for efficient training of deep networks with multiple LSTM layers. Compared with the previous work, highway LSTM, residual LSTM reuses the output projection matrix and the output gate of LSTM to control the spatial information flow instead of additional gate networks, which effectively reduces more than 10% of network parameters. An experiment for distant speech recognition on the AMI SDM corpus indicates that the performance of plain and highway LSTM networks degrades with increasing network depth. For example, 10-layer plain and highway LSTM networks showed 13.7% and 6.2% increase in WER over 3-layer baselines, respectively. On the contrary, 10-layer residual LSTM networks provided the lowest WER 41.0%, which corresponds to 3.3% and 2.8% WER reduction over 3-layer plain and highway LSTM networks, respectively. Training with both the IHM and SDM corpora, the residual LSTM architecture provided larger gain from increasing depth: a 10-layer residual LSTM showed 3.0% WER reduction over the corresponding 5-layer one.", "histories": [["v1", "Tue, 10 Jan 2017 20:03:37 GMT  (1206kb,D)", "http://arxiv.org/abs/1701.03360v1", null], ["v2", "Wed, 15 Mar 2017 00:23:45 GMT  (1636kb,D)", "http://arxiv.org/abs/1701.03360v2", null], ["v3", "Mon, 5 Jun 2017 18:51:08 GMT  (1527kb,D)", "http://arxiv.org/abs/1701.03360v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SD", "authors": ["jaeyoung kim", "mostafa el-khamy", "jungwon lee"], "accepted": false, "id": "1701.03360"}, "pdf": {"name": "1701.03360.pdf", "metadata": {"source": "CRF", "title": "Residual LSTM: Design of a Deep Recurrent Architecture for Distant Speech Recognition", "authors": ["Jaeyoung Kim", "Mostafa El-Khamy", "Jungwon Lee"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of them will be able to move to another world, in which they can move to another world, but in which they are not able to integrate."}, {"heading": "II. REVISITING HIGHWAY NETWORKS", "text": "In this section we give a brief overview of LSTM and three existing motorway architectures."}, {"heading": "A. Residual Network", "text": "The residual network [11] provides an identity mapping via abbreviation paths. As the identity mapping is always enabled, the function output only needs to learn the residual mapping. The formulation of this relationship can be expressed as follows: y = F (x; W) + x (1) y is a layer mapping, x is a layer mapping, and F (x; W) is a function with an internal parameter W. Without a shortcut path, F (x; W) y should represent input x, but with an identity mapping x, F (x; W) only the residual mapping needs to be learned, y \u2212 x. Since the layers stack up, a network can bypass identity mapping without training, which could greatly simplify the formation of a deep network."}, {"heading": "B. Highway Network", "text": "The motorway network [12] offers another possibility to implement a shortcut path for a deep neural network. Output of layer H (x; Wh) is multiplied by a transformation gate T (x; WT) and before the transition to the next layer a motorway path x \u00b7 (1 \u2212 T (x; WT)) is added. The formulation of a motorway network can be summarised as follows: y = H (x; Wh) \u00b7 T (x; WT) + x \u00b7 (1 \u2212 T (x; WT)))) (2) The transformation gate is defined as: T (x; WT) = \u03c3 (WTx + bT) (3) Unlike a residual network, a motorway path is not always switched on. Thus, a motorway network can ignore a motorway path if T (x; WT) = 1, or bypass an output of the layer if T (x; WT) = 0."}, {"heading": "C. Long Short-Term Memory (LSTM)", "text": "Long-term memory (LSTM) [15] has been proposed to resolve vanishing or exploding gradients for a recursive neural network. LSTM has an internal memory cell controlled by forget and input gate networks. A forget gate in an LSTM determines how much of the previous memory value should be transferred to the next time step. Similarly, an input gate scales a new input into a memory cell. Depending on the states of both gates, LSTM can gradually become a long-term or short-term dependence on sequential data. The formulation of an LSTM is as follows: ilt = \u03c3 (W l xix l t + W l l hih l t \u2212 1 + w l l l l l (l l l l l) l (l l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) (l) l (l)."}, {"heading": "D. Highway LSTM", "text": "Equations (4), (5), (7), (8) and (9) do not change for a highway LSTM. Equation (6) is updated to add a highway link: clt = d l t \u00b7 cl \u2212 1t + f l \u00b7 clt \u2212 1 + ilt \u00b7 tanh (W lxcxlt + W lhchlt \u2212 1 + blc) (10) dlt = \u03c3 (W l xdx l + W l cdc l \u2212 1 + w l \u00b7 cdc l \u2212 1 t + b l d) (11) Where dlt is a low gate that connects c l \u2212 1 t in the (l \u2212 1) th layer with clt in the lth layer. [13] showed that an acoustic model based on the highway LSTM improved speech recognition compared to a pure word LER that increases the error rate of LSTM [3] in the third layer."}, {"heading": "III. RESIDUAL LSTM", "text": "In this section, a novel architecture for a deep recursive neural network is used to reduce the number of scales. Residual LSTM starts with the intuition that separating a spatial shortcut path with a time-bound LSTM cell update can offer greater flexibility in dealing with disappearing or exploding gradients. In contrast to a highway LSTM, residual LSTM does not accumulate a highway path on internal memory. Instead, a shortcut path is added to an LSTM output level.For a suitable dimension, Equation (14) can be changed to: hlt = o l \u00b7 (mlt + xlt) (15) 6 Since a highway path is always enabled to a residual LSTM, there should be a scaling parameter on the main path output. For example, linear filters in the last CNN layer of a residual layer of a residual network path are reusable to the main path."}, {"heading": "A. Experimental Setup", "text": "The AMI corpus consists of 100 hours of session recordings. For each meeting, three to four people have free conversation in English. Often there is overlapping speech processes from multiple speakers, and in this case the training protocol always follows a keynote speaker. Multiple microphones are used to record conversations synchronously in different environments. Individual headset microphones (IHM) record clean close-up conversation and single remote microphones (SDM), bearing in mind that the conversations in the far field are noisy. In this paper, SDM is used to record a residual LSTM in Section IV-B and IV-C and combined SDM and IHM corporations. Kaldi [17] is a speech recognition toolkit used to train a contextual LDAMLLT-GMM-HMM system."}, {"heading": "B. Training Performance with increasing Depth", "text": "Figure 2 compares telephone error rates (PER) with increasing depth. However, cross-validation (CV) PER is measured with separate data that are not used for regression. Figure 2 shows both training and CV-PERs in the diagrams. Increasing the depth from 3 to 5 does not show much difference in both training and CV-PERs. However, the 10-shift highway LSTM always showed a significant deterioration. Despite increasing complexity, the convergent training PER is much higher than 3 or 5-shift models. As the CV PER was also degraded, the loss from training PER did not result from a better generalization. Therefore, the training residue loss is purely due to increased depth. In a 10-shift highway LSTM, it is 3.6% CV PER loss and 15% loss from training PER-3 shift-M a slightly higher loss for better training."}, {"heading": "C. WER Evaluation with SDM corpus", "text": "Table I compares WER for LSTM, Highway LSTM and Residual LSTM with increasing depth. All three networks were trained by the SDM AMI Corpus. Both overlapped and unoverlapped WERs are shown. For each layer, the internal memory cell size is set to 1024 and the size of the output node is set to 512. A simple LSTM performed worse with increasing layers. Specifically, the 10-layer LSTM deteriorated by up to 13.7% over the 3-layer LSTM for unoverlapped WER. A highway LSTM showed better performance over a flat LSTM, but could not prevent degradation with increasing depth. The 10-layer LSTM-LSTM presented a 6.2% increase in WER over the 3-layer network. On the contrary, a residual LSTM-LSTM improved with increasing layers."}, {"heading": "D. WER Evaluation with SDM and IHM corpora", "text": "Table II compares WHO of highways and LSTM residues trained with combined IHM and SDM corpora. However, as the body size increases, the most powerful configuration for a highway LSTM is converted to a 5-layer LSTM with 40.7% WER. However, the 10-layer highway LSTM still suffered from training losses due to increased depth: 6.6% increase in WER (non-overlapped). On the contrary, 10-layer LSTM residual corpus showed the best WER of 39.3%, corresponding to a 3.1% WER reduction (non-overlapped) compared to the 5-layer, while the previous experiment, which was trained only with SDM corpus, showed an improvement of 1%. Increasing training data provides greater benefits from a deeper network. Residual LSTM enabled to train a deeper LSTM network without training losses."}, {"heading": "V. CONCLUSION", "text": "In this paper, we proposed a novel architecture for a deep recursive neural network: Residual LSTM. A residual LSTM provides a shortcut path between the outputs of adjacent layers. Unlike a highway network, a residual LSTM does not assign dedicated gate networks for a 10-layer link. Instead, the projection matrix and output gate are reused for a shortcut, which reduces network parameters by about 10% compared to a highway LSTM. Experiments on the AMI corpus showed that a residual LSTM improved significantly with increasing depth, while 10-layer LSTMs suffered greatly from training losses in the flat and highway areas."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In this paper, a novel architecture for a deep recurrent neural network, residual LSTM is introduced.<lb>A plain LSTM has an internal memory cell that can learn long term dependencies of sequential data. It<lb>also provides a temporal shortcut path to avoid vanishing or exploding gradients in the temporal domain.<lb>The proposed residual LSTM architecture provides an additional spatial shortcut path from lower layers<lb>for efficient training of deep networks with multiple LSTM layers. Compared with the previous work,<lb>highway LSTM, residual LSTM reuses the output projection matrix and the output gate of LSTM to<lb>control the spatial information flow instead of additional gate networks, which effectively reduces more<lb>than 10% of network parameters. An experiment for distant speech recognition on the AMI SDM corpus<lb>indicates that the performance of plain and highway LSTM networks degrades with increasing network<lb>depth. For example, 10-layer plain and highway LSTM networks showed 13.7% and 6.2% increase in<lb>WER over 3-layer baselines, respectively. On the contrary, 10-layer residual LSTM networks provided<lb>the lowest WER 41.0%, which corresponds to 3.3% and 2.8% WER reduction over 3-layer plain and<lb>highway LSTM networks, respectively. Training with both the IHM and SDM corpora, the residual<lb>LSTM architecture provided larger gain from increasing depth: a 10-layer residual LSTM showed 3.0%<lb>WER reduction over the corresponding 5-layer one.", "creator": "LaTeX with hyperref package"}}}