{"id": "1701.03578", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2017", "title": "Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network", "abstract": "In this paper, we propose an efficient transfer leaning methods for training a personalized language model using a recurrent neural network with long short-term memory architecture. With our proposed fast transfer learning schemes, a general language model is updated to a personalized language model with a small amount of user data and a limited computing resource. These methods are especially useful for a mobile device environment while the data is prevented from transferring out of the device for privacy purposes. Through experiments on dialogue data in a drama, it is verified that our transfer learning methods have successfully generated the personalized language model, whose output is more similar to the personal language style in both qualitative and quantitative aspects.", "histories": [["v1", "Fri, 13 Jan 2017 07:26:00 GMT  (36kb)", "http://arxiv.org/abs/1701.03578v1", "AAAI workshop on Crowdsourcing, Deep Learning and Artificial Intelligence Agents, Feb 2017, San Francisco CA, USA"]], "COMMENTS": "AAAI workshop on Crowdsourcing, Deep Learning and Artificial Intelligence Agents, Feb 2017, San Francisco CA, USA", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["seunghyun yoon", "hyeongu yun", "yuna kim", "gyu-tae park", "kyomin jung"], "accepted": false, "id": "1701.03578"}, "pdf": {"name": "1701.03578.pdf", "metadata": {"source": "CRF", "title": "Efficient Transfer Learning Schemes for Personalized Language Modeling using Recurrent Neural Network", "authors": ["Seunghyun Yoon", "Hyeongu Yun", "Yuna Kim", "Gyu-tae Park", "Kyomin Jung"], "emails": ["kjung}@snu.ac.kr,", "hyeongu.yun.1989@gmail.com,", "gyutae.park}@samsung.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 1.03 578v 1 [cs.C L] 13 Jan 20"}, {"heading": "Introduction", "text": "In fact, it is the case that we will be able to look for a solution that is capable, that we are able, that we are able to find a solution that is capable of us, that we are able, that we are able to find a solution, that we are able to find a solution, that we are able, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution."}, {"heading": "Architecture for Personalized Language Model", "text": "Focusing on personalized speech modeling with the preservation of user data, we create two types of speech models: First, a sentence completion language model that can complete sentences with a given n-many word sequence; second, a message and response prediction language model that can generate a sentence for a particular message; and the output of both models implies user characteristics such as preferred vocabulary, sentence length, and other language-related patterns. To achieve this result, we trained the speech model with a large amount of general data in powerful computer environments and then applied transfer learning in relatively small computer environments. We expect this method to be applied to mobile devices. Considering privacy protections, the transferred model will be retrained within local environments such as mobile devices, and no personal data will be sent from the devices. This would have been achieved with the proposed transfer learning programs in the RNLM architecture."}, {"heading": "Sentence Completion Language Model", "text": "A sentence completion model completes a sentence with the default word sequence X = {x1, x2,.., xT}, where xN is a word (N = 1, 2,.., T).The model can predict the next word xN + 1 with the default word sequence x1: N. By repeating the prediction until the output word reaches the end of the sentence signal, \"< eos >,\" the whole sentence can be generated.The model is similar to that of (Graves 2013), and we put the 1000-dimensional word embedding layer directly behind the input layer. Subsequently, 3 deep LSTM layers, each containing 100 LSTM cells and without a peephole connection, are used to learn the sequence pattern of the sentences. The output probability of the input sequence X and the training target arc (Y | X) = T-t = 1p (yt | x1: t \u2212 1) L = 1-yt (the input value is 1)."}, {"heading": "Message-Reply Prediction Language Model", "text": "A predictive model for a message generates a response rate for a given message. It is similar to the language model for sentence completion, except that the message rate is encoded and used as context information when the model generates a word sequence for a response. Our approach is inspired by sequence-to-sequence learning research (Sutskever, Vinyals, and Le 2014), which is successfully applied to a machine translation task. The next sequence word X = {x1, x2,., xT} is fed into the model and the last hidden state is used as context information cT. With this context information, the next sequence word is predicted in a similar way as in the language model case for sentence completion. When implementing, we have 1000-dimensional word embedding and 3-depth LSTM layers with 100 LSTM cells in each layer. The output probability and the training target is (X | T = 1 sentence). (Arez \u2212 1)"}, {"heading": "Fast Transfer Learning Schemes", "text": "In order to generate a personalized language model with a small amount of user data and limited computing resources, transfer learning is indispensable. In the private data retention scenario, we examine three fast transfer learning schemes. Each scheme is described as follows: \u2022 Scheme 1, Learn the entire layer: As a starting point, we train the entire model only with private data and compare the result with the two subsequent schemes. Due to the retraining of the LSTM layers in their entirety, this scheme requires more computing power than the other two schemes. \u2022 Scheme 2, Excess layer: After training the model with general data, an excess layer is inserted between the output layer and the last of the deep LSTM layers. Subsequently, for private data, we update only the parameters of the excess layer in the transfer learning phase. We assume that the parameters of a user could be modelled by learning additional features in the user's private data. \u2022 More general model, 3."}, {"heading": "Measures", "text": "Perplexity is one of the popular measures of a language model. It measures how well the language model q predicts a sample. However, it is not good at measuring how well the output of the language model corresponds to a target language style. Another measure, the BLEU score algorithm (Papineni et al. 2002), has been widely used for the automatic evaluation of the model output. However, it cannot be applied directly to measuring the quality of the personalized model output because it takes into account the similarity between a language and the target language. Other research has been conducted to prove authorship and fraud in the literature, for example Jane Austen's leftover novel Partly Completed (Morton 1978). This research counted the occurrence of several words in the literature, compared their relative frequencies with those of words in the target literature, and concluded that the target literature was a forgery. This approach could be applied to a text evaluation, where a large amount of data is generally used as a response to certain words, and the distribution of the language is considered more common."}, {"heading": "Datasets", "text": "\u2022 WMT14 ENG Corpus: The WMT '14 data set contains several corpora. We use only one English part of the 109 French-English corpus. The data set was searched from the bilingual websites of international organizations (Callison-Burch et al. 2011), so it contains high-quality formal written language data. It consists of 21,000,000 sentences. \u2022 English Bible Corpus: The English Bible corpus is another type of written language data. It is useful data that differs from the WMT' 14 data set not only in terms of frequent vocabulary but also in average sentence length. It consists of 31,102 sentences. \u2022 Drama Corpus: To collect spoken language data, we use dramaturgy data from \"friends\" from opensubtitles2. We extracted 69,000 sentences from dialogues that we used to form a sentence completion language model."}, {"heading": "Experiments", "text": "In the first case, we train a general language model with literary-style data and apply a proposed transfer learning scheme with speaker-style data to achieve a personalized language model. With this setting, the difference between general and personalized language models can be measured quantitatively and qualitatively. In the second case, we use dialogue-like data such as drama scripts to train a general language model.From the drama scripts, the data of some characters are taken2Available from \"http: / / www.opensubtitles.org /\" and are used to train the personalized language model.With this setting, the output of the personalized model is compared with the original dialogue of the same character."}, {"heading": "Literary-Style to Spoken-Style Sentence Completion", "text": "We train a generic language model of literary style with the WMT '14 Corpus. We then apply a transfer learning scheme with \"Friends\" drama data to the model to learn the spoken style language. Training the generic language model took about 10 days, then we spent a further 4 hours training the personalized language model in each scheme. A \"titan-X GPU\" and a \"GeForce GT 730 GPU\" were used for these experiments, the latter GPU being one of the low-end GPU series whose computing power was similar to that of the latest mobile GPUs, such as \"Qualcomm Adreno 530\" in \"Samsung Galaxy S7\" or \"NVIDIA Tegra K1\" in \"Google Nexus 9.\" For a vocabulary setting, we construct our dictionary as 50,002 words, including \"eos >,\" to mark the endings and \"* unknown *\" to replace blank vocabulary in the data."}, {"heading": "General-Style to Personal-Style Message-Reply Prediction", "text": "In fact, most people are able to decide whether they will be able to play by the rules, or whether they will be able to break the rules."}, {"heading": "Related Work", "text": "The proposed language models were tested on web corpora (i.e. Wikipedia, news articles), while qualitative examples demonstrated their applicability. (Sutskever, Vinyals and Le 2014) A sequence-to-sequence learning algorithm with RNN and long-term short-term memory was proposed (Hochreiter and Schmidhuber 1997), and (Cho et al.) RNN encoder architecture was proposed. These studies were applied to the machine translation problem. Recently, the RNN translation approach was extended to the short-term memory problem (Sordoni et al. 2014)."}, {"heading": "Conclusion", "text": "We propose an efficient method of training a personalized model using the LSTM-RNN model. In order to maintain user performance, we propose various transfer learning programs so that the personalized language model can be generated within the user's local environment. The proposed programs \"Surplus Layer\" and \"Fixed-n Layer\" exhibit higher generalization performance while exercising only a lower number of parameters than the base model. Quantitative and qualitative test results indicate that the output of the model is similar to that of the user. It is certain that our proposed method demonstrates the applicability of the RNN-based language model in an application device while respecting privacy. Furthermore, our method can generate the personalized language model with a lower amount of application data than the enormous amount of training data normally required in the traditional discipline of deeper neural networks. In the future work, we aim to cover the neural relationship between the STN and the network that seems to be less specific to the neural network than the one needed previously."}], "references": [{"title": "Deep learners benefit more from out-of-distribution", "author": ["Y. Bengio", "F. Bastien", "A. Bergeron", "N. BoulangerLewandowski", "T.M. Breuel", "Y. Chherawala", "M. Cisse", "M. C\u00f4t\u00e9", "D. Erhan", "J Eustache"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2011}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Y. Bengio"], "venue": "Unsupervised and Transfer Learning Challenges in Machine Learning 7:19.", "citeRegEx": "Bengio,? 2012", "shortCiteRegEx": "Bengio", "year": 2012}, {"title": "Findings of the 2011 workshop on statistical machine translation", "author": ["C. Callison-Burch", "P. Koehn", "C. Monz", "O.F. Zaidan"], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation, 22\u201364. Association for Computational Linguistics.", "citeRegEx": "Callison.Burch et al\\.,? 2011", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2011}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Proceedings of the 34th annual meeting on Association for Computa-", "citeRegEx": "Chen and Goodman,? 1996", "shortCiteRegEx": "Chen and Goodman", "year": 1996}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850.", "citeRegEx": "Graves,? 2013", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems, 1135\u2013 1143.", "citeRegEx": "Han et al\\.,? 2015", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Scalable modified kneser-ney language model estimation", "author": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn"], "venue": "ACL (2), 690\u2013696.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y.-D. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "arXiv preprint arXiv:1511.06530.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Literary detection: How to prove authorship and fraud in literature and documents", "author": ["A.Q. Morton"], "venue": "Scribner.", "citeRegEx": "Morton,? 1978", "shortCiteRegEx": "Morton", "year": 1978}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, 311\u2013318. Association for Computational Linguistics.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li"], "venue": "ACL.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J.-Y. Nie", "J. Gao", "B. Dolan"], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, 3104\u2013 3112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model", "author": ["O. Vinyals", "Q. Le"], "venue": "arXiv preprint arXiv:1506.05869.", "citeRegEx": "Vinyals and Le,? 2015", "shortCiteRegEx": "Vinyals and Le", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "Among the RNN structures, a Long Short-Term Memory RNN (LSTM-RNN) and its variants are successfully used for language modeling tasks (Hochreiter and Schmidhuber 1997; Cho et al. 2014).", "startOffset": 133, "endOffset": 183}, {"referenceID": 4, "context": "Among the RNN structures, a Long Short-Term Memory RNN (LSTM-RNN) and its variants are successfully used for language modeling tasks (Hochreiter and Schmidhuber 1997; Cho et al. 2014).", "startOffset": 133, "endOffset": 183}, {"referenceID": 5, "context": "The model is similar to that of (Graves 2013), and we put the 1,000-dimension word-embedding layer right after the input layer.", "startOffset": 32, "endOffset": 45}, {"referenceID": 11, "context": "Another measure, the BLEU score algorithm (Papineni et al. 2002), has been widely used for the automatic evaluation of the model output.", "startOffset": 42, "endOffset": 64}, {"referenceID": 10, "context": "Other research was conducted on proving authorship and fraud in literature, for instance, Jane Austen\u2019s left-over novel with partially completed (Morton 1978).", "startOffset": 145, "endOffset": 158}, {"referenceID": 2, "context": "The dataset was crawled data from the bilingual web pages of the international organizations (Callison-Burch et al. 2011).", "startOffset": 93, "endOffset": 121}, {"referenceID": 3, "context": "An unpruned n-gram language models using modified Kneser-Ney smoothing are used for performance comparisons (Chen and Goodman 1996).", "startOffset": 108, "endOffset": 131}, {"referenceID": 7, "context": "The n-gram models were trained by using KenLM software package (Heafield et al. 2013).", "startOffset": 63, "endOffset": 85}, {"referenceID": 5, "context": "Researchers have proposed language models using RNN, which learns the probability of next sequence data at the character or word level (Sutskever, Martens, and Hinton 2011; Graves 2013).", "startOffset": 135, "endOffset": 185}, {"referenceID": 8, "context": "(Sutskever, Vinyals, and Le 2014) proposed a sequence-to-sequence learning algorithm with RNN and long short-term memory (LSTM) architecture (Hochreiter and Schmidhuber 1997), and (Cho et al.", "startOffset": 141, "endOffset": 174}, {"referenceID": 4, "context": "(Sutskever, Vinyals, and Le 2014) proposed a sequence-to-sequence learning algorithm with RNN and long short-term memory (LSTM) architecture (Hochreiter and Schmidhuber 1997), and (Cho et al. 2014) proposed RNN encoder-decoder architecture.", "startOffset": 180, "endOffset": 197}, {"referenceID": 13, "context": "Recently, the RNN machine translation approach was extended to the short message generation problem (Sordoni et al. 2015).", "startOffset": 100, "endOffset": 121}, {"referenceID": 16, "context": "There was another trial on the generation of responses in technical troubleshooting discourses (Vinyals and Le 2015).", "startOffset": 95, "endOffset": 116}, {"referenceID": 0, "context": "(Bengio et al. 2011; Bengio 2012) suggested that a base-trained model with general data could be transferred to another domain.", "startOffset": 0, "endOffset": 33}, {"referenceID": 1, "context": "(Bengio et al. 2011; Bengio 2012) suggested that a base-trained model with general data could be transferred to another domain.", "startOffset": 0, "endOffset": 33}, {"referenceID": 17, "context": "Recently, (Yosinski et al. 2014) showed, through experiments, that the lower layers tended to have general features whereas the higher layer tended to have specific features.", "startOffset": 10, "endOffset": 32}, {"referenceID": 9, "context": "To adapt a neural network model to an embedded system with limited resources, (Kim et al. 2015) (Han et al.", "startOffset": 78, "endOffset": 95}, {"referenceID": 6, "context": "2015) (Han et al. 2015) reduced the size of the model by pruning the unnecessary connections within it.", "startOffset": 6, "endOffset": 23}], "year": 2017, "abstractText": "In this paper, we propose an efficient transfer leaning methods for training a personalized language model using a recurrent neural network with long short-term memory architecture. With our proposed fast transfer learning schemes, a general language model is updated to a personalized language model with a small amount of user data and a limited computing resource. These methods are especially useful for a mobile device environment while the data is prevented from transferring out of the device for privacy purposes. Through experiments on dialogue data in a drama, it is verified that our transfer learning methods have successfully generated the personalized language model, whose output is more similar to the personal language style in both qualitative and quantitative aspects.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}