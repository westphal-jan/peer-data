{"id": "1701.03641", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2017", "title": "Symbolic Regression Algorithms with Built-in Linear Regression", "abstract": "Recently, several algorithms for symbolic regression (SR) emerged which employ a form of multiple linear regression (LR) to produce generalized linear models. The use of LR allows the algorithms to create models with relatively small error right from the beginning of the search; such algorithms are thus claimed to be (sometimes by orders of magnitude) faster than SR algorithms based on vanilla genetic programming. However, a systematic comparison of these algorithms on a common set of problems is still missing. In this paper we conceptually and experimentally compare several representatives of such algorithms (GPTIPS, FFX, and EFS). They are applied as off-the-shelf, ready-to-use techniques, mostly using their default settings. The methods are compared on several synthetic and real-world SR benchmark problems. Their performance is also related to the performance of three conventional machine learning algorithms --- multiple regression, random forests and support vector regression.", "histories": [["v1", "Fri, 13 Jan 2017 12:23:10 GMT  (188kb)", "http://arxiv.org/abs/1701.03641v1", "Submitted to Journal of Heuristics"], ["v2", "Thu, 9 Mar 2017 10:24:16 GMT  (0kb,I)", "http://arxiv.org/abs/1701.03641v2", "Withdrawn due to wrong journal data"], ["v3", "Fri, 10 Mar 2017 11:14:17 GMT  (188kb)", "http://arxiv.org/abs/1701.03641v3", null]], "COMMENTS": "Submitted to Journal of Heuristics", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jan \\v{z}egklitz", "petr po\\v{s}\\'ik"], "accepted": false, "id": "1701.03641"}, "pdf": {"name": "1701.03641.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["zegkljan@fel.cvut.cz", "petr.posik@fel.cvut.cz"], "sections": [{"heading": null, "text": "ar Xiv: 170 1.03 641v 1 [cs.L G] 13 Jan 20keywords symbolic regression \u00b7 genetic programming \u00b7 linear regression \u00b7 comparative study"}, {"heading": "1 Introduction", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "2 Compared Algorithms", "text": "This section briefly describes the selected algorithms and important aspects regarding the complexity of the models created by these algorithms."}, {"heading": "2.1 GPTIPS", "text": "GPTIPS [23,22] is an open source SR toolbox for MATLAB. It is an implementation of Multi-Gene Genetic Programming (MGGP) [8] and therefore has its roots in Vanilla GP. Each solution consists of several independent trees, called genes, and their results are combined linearly. Coefficients of this linear combination are optimally calculated in terms of the mean square error (MSE) of the resulting expression measured using the training data. MGGP (and GPTIPS in particular) are based on classical genetic programming, which means that they work with a population of fixed size, subtree mutation, subtree crossing, tournament selection, standard initialization procedures, and are able to handle the internal constants of the model (to a certain degree) using ephemeral random constants. The output of GPTIPS is the last population to select from models (not to parse them)."}, {"heading": "2.2 FFX", "text": "FFX, or Fast Function Extraction [14], is a deterministic algorithm for symbolic regression. It first generates a large number of basic functions, which are then combined linearly using Pathwise Regularized Learning [6,29] to produce sparse models. The algorithm generates a pareto-front of models in terms of their accuracy and complexity. Again, it is up to the user to choose the final model.There are two types of bases that are generated: univariate bases and bivariate bases. Univariate bases are: a variable that is increased to a power (selected from a fixed set of options) and (non-linear) functions that are applied to another univariate model.Bivariate bases are products of all pairs of univariate bases that exclude pairs where both bases are functional; the author argues that such products are \"considered too complex\" to include FFX, which also contains a learning trick that enables them to use a tricks."}, {"heading": "2.3 EFS", "text": "EFS, or Evolutionary Feature Synthesis [2], is the youngest of the three algorithms. In EFS, the population does not consist of complete models, but of functions that collectively form a single model. In this respect, EFS FFX is similar: in FFX, the individual characteristics are relatively simple and are generated systematically and exhaustively, while in EFS, characteristics can be more complex (depending on complexity limitations) and are generated stochastically. The initial population is formed by the original characteristics of the dataset. In each generation, a model is composed of the characteristics of the current population through pathologically regulated learning and is stored when it is best. The next step in a generation is the composition of new characteristics by applying non-uniform and binary functions to the characteristics already present in the current population."}, {"heading": "3 Benchmarks and Testing", "text": "This year, it has reached the stage where it will be able to take the lead in order to achieve the objectives I have mentioned."}, {"heading": "4 Results", "text": "In the following sections we will discuss the results per dataset, some global trends that we see in the results, and the differences between the individual countries. We define the number of nodes as the sum of the algorithms tested. We only count the express trees themselves, i.e. we do not count the additional coefficients and operators that are able to find a linear combination that is used in the tested algorithms. These coefficients and operators are not counted because they are completely dependent on the basics themselves."}, {"heading": "5 Conclusions and Future work", "text": "In this article we compared three current methods for symbolic regression, EFS, FFX and GPTIPS. All of them produce models from the class of Generalized Linear Models. Two of these methods, EFS and FFX, use Pathwise Regularized Learning, while GPTIPS uses classic (multiple) linear regression to determine the linear coefficients of the resulting model. EFS and GPTIPS are stochastic methods based on GP operators of mutation and crossover, while FFX is a completely deterministic method. We used the methods as standard tools, with their default settings and without modifications to their implementations. Since the standard GPTIPS has a very limited functionality gap, we added mGPTIPS with a function closer to that of EFS. The methods were compared with five artificial and four real algorithms, and the results show that none of the algorithms is exceptional or worse than the others."}], "references": [{"title": "Multiple regression genetic programming", "author": ["I. Arnaldo", "K. Krawiec", "U.M. O\u2019Reilly"], "venue": "Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation, GECCO \u201914, pp. 879\u2013886. ACM, New York, NY, USA", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Building predictive models via feature synthesis", "author": ["I. Arnaldo", "U.M. O\u2019Reilly", "K. Veeramachaneni"], "venue": "Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation, GECCO \u201915, pp. 983\u2013990. ACM, New York, NY, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Lichman, M.: UCI machine learning repository", "author": ["K. Bache"], "venue": "Http://archive.ics.uci.edu/ml", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Geometric semantic genetic programming with local search", "author": ["M. Castelli", "L. Trujillo", "L. Vanneschi", "S. Silva", "E. Z-Flores", "P. Legrand"], "venue": "Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation, GECCO \u201915, pp. 999\u2013 1006. ACM, New York, NY, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Statistical Software 33(1), 1\u201322", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A multi-gene genetic programming model for estimating stress-dependent soil water retention curves", "author": ["A. Garg", "A. Garg", "K. Tai"], "venue": "Computational Geosciences 18(1), 45\u201356", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Modelling chemical process systems using a multi-gene genetic programming algorithm", "author": ["M. Hinchliffe", "H. Hiden", "B. McKay", "M. Willis", "M. Tham", "G. Barton"], "venue": "Late Breaking Paper, GP\u201996, pp. 56\u201365. Stanford, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1996}, {"title": "Adaptation in Natural and Artificial Systems", "author": ["J.H. Holland"], "venue": "MIT Press, Cambridge, MA, USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Scaled symbolic regression", "author": ["M. Keijzer"], "venue": "Genetic Programming and Evolvable Machines 5(3), 259\u2013269", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Genetic Programming Theory and Practice IX, chap", "author": ["M.F. Korns"], "venue": "Accuracy in Symbolic Regression, pp. 129\u2013151. Springer New York, New York, NY", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Genetic Programming: On the Programming of Computers by Means of Natural Selection", "author": ["J.R. Koza"], "venue": "MIT Press, Cambridge, MA, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1992}, {"title": "Lexicographic parsimony pressure", "author": ["S. Luke", "L. Panait"], "venue": "Proceedings of the Genetic and Evolutionary Computation Conference, GECCO \u201902, pp. 829\u2013 836. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Ffx: Fast, scalable, deterministic symbolic regression technology", "author": ["T. McConaghy"], "venue": "R. Riolo, E. Vladislavleva, J.H. Moore (eds.) Genetic Programming Theory and Practice IX, Genetic and Evolutionary Computation, pp. 235\u2013 260. Springer New York", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Genetic programming needs better benchmarks", "author": ["J. McDermott", "D.R. White", "S. Luke", "L. Manzoni", "M. Castelli", "L. Vanneschi", "W. Jaskowski", "K. Krawiec", "R. Harper", "K. De Jong", "U.M. O\u2019Reilly"], "venue": "Proceedings of the 14th Annual Conference on Genetic and Evolutionary Computation, GECCO \u201912, pp. 791\u2013 798. ACM, New York, NY, USA", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Geometric semantic genetic programming", "author": ["A. Moraglio", "K. Krawiec", "C. Johnson"], "venue": "C. Coello, V. Cutello, K. Deb, S. Forrest, G. Nicosia, M. Pavone (eds.) Parallel Problem Solving from Nature - PPSN XII, Lecture Notes in Computer Science, vol. 7491, pp. 21\u201331. Springer Berlin Heidelberg", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research 12, 2825\u20132830", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Distilling free-form natural laws from experimental data", "author": ["M. Schmidt", "H. Lipson"], "venue": "Science 324(5923), 81\u201385", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Eureqa (Version 0.98 beta", "author": ["M. Schmidt", "H. Lipson"], "venue": "Www.nutonian.com", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "GPTIPS 2: An Open-Source Software Platform for Symbolic Data Mining, pp", "author": ["D.P. Searson"], "venue": "551\u2013573. Springer International Publishing, Cham", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "GPTIPS: an open source genetic programming toolbox for multigene symbolic regression", "author": ["D.P. Searson", "D.E. Leahy", "M.J. Willis"], "venue": "Proceedings of the International MultiConference of Engineers and Computer Scientists, vol. 1, pp. 77\u201380", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools", "author": ["A. Tsanas", "A. Xifara"], "venue": "Energy and Buildings 49, 560\u2013567", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Order of nonlinearity as a complexity measure for models generated by symbolic regression via pareto genetic programming", "author": ["E. Vladislavleva", "G. Smits", "D. den Hertog"], "venue": "Evolutionary Computation, IEEE Transactions on 13(2), 333\u2013349", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "The supervised learning no-free-lunch theorems", "author": ["D.H. Wolpert"], "venue": "In Proc. 6th Online World Conference on Soft Computing in Industrial Applications, pp. 25\u201342", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2001}, {"title": "Modeling of strength of high-performance concrete using artificial neural networks", "author": ["I.C. Yeh"], "venue": "Cement and Concrete Research 28(12), 1797\u20131808", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67(2), 301\u2013320", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 10, "context": "SR is a landmark application of Genetic Programming (GP) [12].", "startOffset": 57, "endOffset": 61}, {"referenceID": 7, "context": "GP is similar to Genetic Algorithms [9]: it uses a population of individuals (candidate solutions), a fitness function that evaluates the behavior of the solutions, a selection mechanism to promote better solutions over the worse ones, a crossover operator(s) that combines two (or more) individuals and a mutation operator(s) that (randomly) modifies individuals.", "startOffset": 36, "endOffset": 39}, {"referenceID": 10, "context": "For the rest of this article we will refer to the Koza\u2019s original GP [12] system as to \u2018vanilla GP\u2019.", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "Such a system may reach impressive results [20,21] when given good data and enough time, sometimes even recovering the true equations describing the underlying phenomenon which generated the observed data.", "startOffset": 43, "endOffset": 50}, {"referenceID": 17, "context": "Such a system may reach impressive results [20,21] when given good data and enough time, sometimes even recovering the true equations describing the underlying phenomenon which generated the observed data.", "startOffset": 43, "endOffset": 50}, {"referenceID": 14, "context": "A novel, revealing view of the SR problem is provided by Geometric Semantic Genetic Programming (GSGP) [18].", "startOffset": 103, "endOffset": 107}, {"referenceID": 3, "context": "A combination of GSGP with Local Search [5] proposed recently uses only the mutation, but the offspring is constructed as the optimal linear combination with respect to the parent and a random tree via multiple regression.", "startOffset": 40, "endOffset": 43}, {"referenceID": 19, "context": "Recently, several methods emerged [23,22,14,2,1] that explicitly restrict the class of models to generalized linear models, i.", "startOffset": 34, "endOffset": 48}, {"referenceID": 18, "context": "Recently, several methods emerged [23,22,14,2,1] that explicitly restrict the class of models to generalized linear models, i.", "startOffset": 34, "endOffset": 48}, {"referenceID": 12, "context": "Recently, several methods emerged [23,22,14,2,1] that explicitly restrict the class of models to generalized linear models, i.", "startOffset": 34, "endOffset": 48}, {"referenceID": 1, "context": "Recently, several methods emerged [23,22,14,2,1] that explicitly restrict the class of models to generalized linear models, i.", "startOffset": 34, "endOffset": 48}, {"referenceID": 0, "context": "Recently, several methods emerged [23,22,14,2,1] that explicitly restrict the class of models to generalized linear models, i.", "startOffset": 34, "endOffset": 48}, {"referenceID": 12, "context": "In [14], it is argued that (some of) these SR methods already have the status of a technology, i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "We chose 3 methods: (1) GPTIPS [22], a SR framework using multigene genetic programming, (2) Fast Function Extraction (FFX) [14], an example of non-evolutionary deterministic methods, and (3) Evolutionary Feature Synthesis (EFS) [2], a recent evolutionary method for fast creation of interpretable SR models.", "startOffset": 31, "endOffset": 35}, {"referenceID": 12, "context": "We chose 3 methods: (1) GPTIPS [22], a SR framework using multigene genetic programming, (2) Fast Function Extraction (FFX) [14], an example of non-evolutionary deterministic methods, and (3) Evolutionary Feature Synthesis (EFS) [2], a recent evolutionary method for fast creation of interpretable SR models.", "startOffset": 124, "endOffset": 128}, {"referenceID": 1, "context": "We chose 3 methods: (1) GPTIPS [22], a SR framework using multigene genetic programming, (2) Fast Function Extraction (FFX) [14], an example of non-evolutionary deterministic methods, and (3) Evolutionary Feature Synthesis (EFS) [2], a recent evolutionary method for fast creation of interpretable SR models.", "startOffset": 229, "endOffset": 232}, {"referenceID": 22, "context": "No Free Lunch theorems for supervised learning [27]); we are more interested in the types of differences we can expect from these algorithms when applied to the same regression problems.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "1 Another candidate for such a comparison would be system Eureqa [20,21].", "startOffset": 65, "endOffset": 72}, {"referenceID": 17, "context": "1 Another candidate for such a comparison would be system Eureqa [20,21].", "startOffset": 65, "endOffset": 72}, {"referenceID": 19, "context": "GPTIPS [23,22] is an open-source SR toolbox for MATLAB.", "startOffset": 7, "endOffset": 14}, {"referenceID": 18, "context": "GPTIPS [23,22] is an open-source SR toolbox for MATLAB.", "startOffset": 7, "endOffset": 14}, {"referenceID": 6, "context": "It is an implementation of Multi-Gene Genetic Programming (MGGP) [8] and thus has its roots in vanilla GP.", "startOffset": 65, "endOffset": 68}, {"referenceID": 11, "context": "To limit the complexity of the candidate models and to prefer simpler ones, GPTIPS by default uses Lexicographic Parsimony Pressure [13] using Expressional Complexity [26] of the models (genes).", "startOffset": 132, "endOffset": 136}, {"referenceID": 21, "context": "To limit the complexity of the candidate models and to prefer simpler ones, GPTIPS by default uses Lexicographic Parsimony Pressure [13] using Expressional Complexity [26] of the models (genes).", "startOffset": 167, "endOffset": 171}, {"referenceID": 6, "context": "MGGP was shown to be faster and more accurate than vanilla GP [8] and also a comparable or better alternative to classical methods like Support Vector Regression and Artificial Neural Networks [7].", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "MGGP was shown to be faster and more accurate than vanilla GP [8] and also a comparable or better alternative to classical methods like Support Vector Regression and Artificial Neural Networks [7].", "startOffset": 193, "endOffset": 196}, {"referenceID": 12, "context": "FFX, or Fast Function Extraction [14], is a deterministic algorithm for symbolic regression.", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": "It first exhaustively generates a massive set of basis functions, which are then linearly combined using Pathwise Regularized Learning [6,29] to produce sparse models.", "startOffset": 135, "endOffset": 141}, {"referenceID": 24, "context": "It first exhaustively generates a massive set of basis functions, which are then linearly combined using Pathwise Regularized Learning [6,29] to produce sparse models.", "startOffset": 135, "endOffset": 141}, {"referenceID": 12, "context": "The original paper [14] reports FFX to be more accurate than many classical methods including vanilla GP, neural networks and SVM.", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "EFS, or Evolutionary Feature Synthesis [2], is the most recent of the three algorithms.", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "The original paper [2] reports EFS being comparable to neural networks and similar or better than Multiple Regression Genetic Programming which itself was reported to outperform vanilla GP, multiple regression and Scaled Symbolic Regression (introduced in [10]).", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "The original paper [2] reports EFS being comparable to neural networks and similar or better than Multiple Regression Genetic Programming which itself was reported to outperform vanilla GP, multiple regression and Scaled Symbolic Regression (introduced in [10]).", "startOffset": 256, "endOffset": 260}, {"referenceID": 13, "context": "All the datasets except the last one were picked based on [17].", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "Using the notation from [17]:", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "Koza-1 [12] is a classical, easy-to-solve SR benchmark.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "Koza-1 f1(x) = x4 + x3 + x2 + x 1 [12] Korns-11 f2(x, y, z, v, w) = 6.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "23x3) 5 [11] S1 f3(x) = e\u2212xx3 sin(x) cos(x)(sin2(x) cos(x) \u2212 1) 1 [26] S2 f4(x, y) = (y \u2212 5)f3(x) 2 [26] UB f5(x1, x2, x3, x4, x5) = 10 5+ \u2211 5 i=1 (xi\u22123) 2 5 [26]", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "23x3) 5 [11] S1 f3(x) = e\u2212xx3 sin(x) cos(x)(sin2(x) cos(x) \u2212 1) 1 [26] S2 f4(x, y) = (y \u2212 5)f3(x) 2 [26] UB f5(x1, x2, x3, x4, x5) = 10 5+ \u2211 5 i=1 (xi\u22123) 2 5 [26]", "startOffset": 66, "endOffset": 70}, {"referenceID": 21, "context": "23x3) 5 [11] S1 f3(x) = e\u2212xx3 sin(x) cos(x)(sin2(x) cos(x) \u2212 1) 1 [26] S2 f4(x, y) = (y \u2212 5)f3(x) 2 [26] UB f5(x1, x2, x3, x4, x5) = 10 5+ \u2211 5 i=1 (xi\u22123) 2 5 [26]", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "23x3) 5 [11] S1 f3(x) = e\u2212xx3 sin(x) cos(x)(sin2(x) cos(x) \u2212 1) 1 [26] S2 f4(x, y) = (y \u2212 5)f3(x) 2 [26] UB f5(x1, x2, x3, x4, x5) = 10 5+ \u2211 5 i=1 (xi\u22123) 2 5 [26]", "startOffset": 158, "endOffset": 162}, {"referenceID": 9, "context": "Korns-11 [11] is specific in the fact that the output depends on only one of the 5 input features and also by the presence of internal constant.", "startOffset": 9, "endOffset": 13}, {"referenceID": 21, "context": "Salustowicz 1D (S1) [26] (called Vladislavleva-2 in [17]) is defined by a single, relatively complex term.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "Salustowicz 1D (S1) [26] (called Vladislavleva-2 in [17]) is defined by a single, relatively complex term.", "startOffset": 52, "endOffset": 56}, {"referenceID": 21, "context": "Salustowicz 2D (S2) [26] (called Vladislavleva-3 in [17]) has similar features as S1, but in two dimensions.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "Salustowicz 2D (S2) [26] (called Vladislavleva-3 in [17]) has similar features as S1, but in two dimensions.", "startOffset": 52, "endOffset": 56}, {"referenceID": 21, "context": "Unwrapped Ball 5D (UB) [26] is specific by the presence of a fraction and consists of 5 features which all influence the target value.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "ENC 8 768 [25,4] ENH 8 768 [25,4] CCS 8 1030 [28,4] ASN 5 1503 [4]", "startOffset": 10, "endOffset": 16}, {"referenceID": 2, "context": "ENC 8 768 [25,4] ENH 8 768 [25,4] CCS 8 1030 [28,4] ASN 5 1503 [4]", "startOffset": 10, "endOffset": 16}, {"referenceID": 20, "context": "ENC 8 768 [25,4] ENH 8 768 [25,4] CCS 8 1030 [28,4] ASN 5 1503 [4]", "startOffset": 27, "endOffset": 33}, {"referenceID": 2, "context": "ENC 8 768 [25,4] ENH 8 768 [25,4] CCS 8 1030 [28,4] ASN 5 1503 [4]", "startOffset": 27, "endOffset": 33}, {"referenceID": 23, "context": "ENC 8 768 [25,4] ENH 8 768 [25,4] CCS 8 1030 [28,4] ASN 5 1503 [4]", "startOffset": 45, "endOffset": 51}, {"referenceID": 2, "context": "ENC 8 768 [25,4] ENH 8 768 [25,4] CCS 8 1030 [28,4] ASN 5 1503 [4]", "startOffset": 45, "endOffset": 51}, {"referenceID": 2, "context": "ENC 8 768 [25,4] ENH 8 768 [25,4] CCS 8 1030 [28,4] ASN 5 1503 [4]", "startOffset": 63, "endOffset": 66}, {"referenceID": 20, "context": "Energy Efficiency (ENC, ENH) [25] are datasets regarding energy efficiency of cooling (ENC) and heating (ENH) of buildings, acquired from the UCI repository [4].", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "Energy Efficiency (ENC, ENH) [25] are datasets regarding energy efficiency of cooling (ENC) and heating (ENH) of buildings, acquired from the UCI repository [4].", "startOffset": 157, "endOffset": 160}, {"referenceID": 1, "context": "They were already used as benchmarks in [2], where the EFS method was introduced.", "startOffset": 40, "endOffset": 43}, {"referenceID": 23, "context": "Concrete Compressive Strength (CCS) [28] is a dataset representing a highly non-linear function of concrete age and ingredients, acquired from the UCI repository [4].", "startOffset": 36, "endOffset": 40}, {"referenceID": 2, "context": "Concrete Compressive Strength (CCS) [28] is a dataset representing a highly non-linear function of concrete age and ingredients, acquired from the UCI repository [4].", "startOffset": 162, "endOffset": 165}, {"referenceID": 2, "context": "Airfoil Self-Noise (ASN), acquired from the UCI repository [4], is a dataset regarding the sound pressure levels of airfoils based on measurements from a wind tunnel.", "startOffset": 59, "endOffset": 62}, {"referenceID": 15, "context": "The implementations of all three ML algorithms were grabbed from the Python machine learning package, scikit-learn [19,16].", "startOffset": 115, "endOffset": 122}, {"referenceID": 1, "context": "For details of the parameter settings, see the original paper [2].", "startOffset": 62, "endOffset": 65}], "year": 2017, "abstractText": "Recently, several algorithms for symbolic regression (SR) emerged which employ a form of multiple linear regression (LR) to produce generalized linear models. The use of LR allows the algorithms to create models with relatively small error right from the beginning of the search; such algorithms are thus claimed to be (sometimes by orders of magnitude) faster than SR algorithms based on vanilla genetic programming. However, a systematic comparison of these algorithms on a common set of problems is still missing. In this paper we conceptually and experimentally compare several representatives of such algorithms (GPTIPS, FFX, and EFS). They are applied as off-the-shelf, ready-to-use techniques, mostly using their default settings. The methods are compared on several synthetic and real-world SR benchmark problems. Their performance is also related to the performance of three conventional machine learning algorithms \u2014 multiple regression, random forests and support vector regression.", "creator": "LaTeX with hyperref package"}}}