{"id": "1701.03647", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2017", "title": "Restricted Boltzmann Machines with Gaussian Visible Units Guided by Pairwise Constraints", "abstract": "Restricted Boltzmann machines (RBMs) and their variants are usually trained by contrastive divergence (CD) learning, but the training procedure is an unsupervised learning approach, without any guidances of the background knowledge. To enhance the expression ability of traditional RBMs, in this paper, we propose pairwise constraints restricted Boltzmann machine with Gaussian visible units (pcGRBM) model, in which the learning procedure is guided by pairwise constraints and the process of encoding is conducted under these guidances. The pairwise constraints are encoded in hidden layer features of pcGRBM. Then, some pairwise hidden features of pcGRBM flock together and another part of them are separated by the guidances. In order to deal with real-valued data, the binary visible units are replaced by linear units with Gausian noise in the pcGRBM model. In the learning process of pcGRBM, the pairwise constraints are iterated transitions between visible and hidden units during CD learning procedure. Then, the proposed model is inferred by approximative gradient descent method and the corresponding learning algorithm is designed in this paper. In order to compare the availability of pcGRBM and traditional RBMs with Gaussian visible units, the features of the pcGRBM and RBMs hidden layer are used as input 'data' for K-means, spectral clustering (SP) and affinity propagation (AP) algorithms, respectively. A thorough experimental evaluation is performed with sixteen image datasets of Microsoft Research Asia Multimedia (MSRA-MM). The experimental results show that the clustering performance of K-means, SP and AP algorithms based on pcGRBM model are significantly better than traditional RBMs. In addition, the pcGRBM model for clustering task shows better performance than some semi-supervised clustering algorithms.", "histories": [["v1", "Fri, 13 Jan 2017 12:43:58 GMT  (2784kb)", "http://arxiv.org/abs/1701.03647v1", "13pages"]], "COMMENTS": "13pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jielei chu", "hongjun wang", "hua meng", "peng jin", "tianrui li"], "accepted": false, "id": "1701.03647"}, "pdf": {"name": "1701.03647.pdf", "metadata": {"source": "CRF", "title": "Restricted Boltzmann Machines with Gaussian Visible Units Guided by Pairwise Constraints", "authors": ["Jielei Chu", "Hongjun Wang", "Hua Meng", "Peng Jin"], "emails": ["jieleichu@home.swjtu.edu.cn,", "wanghongjun@swjtu.edu.cn,", "huameng@swjtu.edu.cn,", "trli@swjtu.edu.cn.", "jandp@pku.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 170 1,03 647v 1 [cs.L G] 13 Jan 20Index terms - restricted Blotzmann machine (RBM); pair constraints; contrastive divergence (CD); unsupervised clustering; semi-supervised clustering."}, {"heading": "1 INTRODUCTION", "text": "This year, it has come to the point that there is only one occasion when there is a scandal before there is a scandal."}, {"heading": "2 RELATED WORK", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "3 PRELIMINARIES", "text": "This section briefly summarizes the background of the pairwise constraints, RBM and Gaussian visible units."}, {"heading": "3.1 Pairwise Constraints", "text": "There are two types of pair-by-pair constraints at the instance level: one is cannotlink constraints that should not be combined, and the other is must-link constraints that should be combined. Must-link constraints and cannot-link constraints define a relationship at the instance level of transient binary data. Consequently, two types of constraints can be derived from background knowledge of records or marked data. In this essay, we randomly select selected data from different groups and ensure that each group has the same ratio of marked data to be selected. Then, the must-link constraints are generated by the same group-marked data, and the non-link constraints are generated by the selected different group-marked data."}, {"heading": "3.2 Restricted Boltzmann Machine", "text": "An RBM [2] [3] is a two-layer network in which the first layer consists of visible units and the second layer of hidden units. & < < > Data connections between visible and hidden layers exist neither with the visible units nor with the hidden units. A classic RBM model is shown in Figure 1. An energy function [56] of a common configuration (v, h) between the visible layer and the hidden layer is given by: E (v, h) = \u2212 Data connection between the visible units or the hidden units. \u2212 A classic RBM model is shown in Figure i, jvihjwij (1) of a common configuration (v, h \u00b7 \u00b7 \u00b7 vn) and h = (h1, h2 \u00b7 \u00b7 hm) are the visible and hidden vectors, ai and bj are the hidden vectors that are their prejudices, and are the visible or hidden dimensions of the layer."}, {"heading": "3.3 Gaussian Visible Units", "text": "To deal with real data such as natural images, one solution is to replace the binary visible units with linear units with independent Gaussian noise, but the hidden units remain binary, which is first suggested by [57]. Negative log probability results from the following energy function: \u2212 logp (v, h) = E (v, h) = \u2211 i \"visible (vi \u2212 ai) 22\u03c32i \u2212 \u2211 j\" hiddenbjhj \"i, jvi \u03c3i hjwij (10), where \u03c3i is the standard deviation of Gaussian noise for visible unit i. For each visible unit it is easy to learn the variance of noise, but CD1 is difficult because it takes a long time [50] [58]. Therefore, in many applications it is easy to normalize the data to have unit variance and zero mean [50] [59] [61] [61]."}, {"heading": "4 PCGRBM MODEL AND ITS LEARNING ALGORITHM", "text": "First we propose a pair-limited Boltzmann model with Gaussian visible units (pcGRBM), in which the binary visible units are replaced by noise-free linear units and the learning process is guided by pair-wise constraints. Then we give precise conclusions on the pcGRBM optimization. Finally, the corresponding learning algorithm is presented."}, {"heading": "4.1 pcGRBM Model", "text": "Suppose that V = {v1, v2, \u00b7 \u00b7, vn} is an original p-dimensional dataset normalized, H = {h1, h2, \u00b7 \u00b7 \u00b7, hn} is a q \u2212 dimensional hidden code. The pair must-link set of reconstruction data is defined by M = {(vs, vt) | vs, vt belongs to the same class, and in pairs the set of reconstruction data cannot be linked by C = {(vs, vt) | vs, vt belongs to the different classes. To learn the parameters of the pcGRBM model, the first goal is to maximize the log probability of RBM with the visible units Gauss, and the second goal is to maximize the removal of all paired vectors."}, {"heading": "4.2 pcGRBM Inference", "text": "For our first target, we can use the gradient descend to solve the optimal problem, but it is expensive to calculate the gradient of the log probability. Recently, we have shown that CD1 learning is easier than ML learning in RBMs with Gaussian linear units. Then, we apply the CD1 learning method to get an approximation of the log probability gradient. \u2212 \u2212 For our second target, we use the method of gradient descend to solve the optimization problem. The following main work is the calculation of the gradient of 1 \u2212 ph NM \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2"}, {"heading": "4.3 pcGRBM Learning Algorithm", "text": "According to the above conclusion, the learning algorithm for pcGRBM is summarized as follows: Algorithm 1 Learning for pcGRBMInput: \u03b5 is the learning rate; V is a p-dimensional dataset; \u03bb is a coefficient of scale; NM is the cardinality of the specified paired constraints that must be linked; NC is the cardinality of the specified pictorial constraints that cannot be linked; M is specified pictorial constraints that must be linked; C is specified pictorial constraints that must be linked. Output: \u03b8 = {a, b, W}, W are connecting weights, a is visible distortions, b is hidden bias. Starting point for each iteration doFor all hidden units j do compute j do compute p (hj = 1 | v}, b is hidden distortions."}, {"heading": "5 RESULTS AND DISCUSSION", "text": "In this section we present the data sets, define the experimental setup and discuss experimental results."}, {"heading": "5.1 DataSets", "text": "We used the Microsoft Research Asia Multimedia (MSRAMM) [63], which contains two partial data sets, e.g. a video data set and an image set. The image part contains 1,011,738 images and the video part 23,517 videos. To evaluate our pcGRBM model, we use 16 image sets (alphabet, ambulance, bed, beret, drinks, bicycle, billiards, blog, blood, bonsai, book, bread, breakfast, building, vegetables and virus) from the image part for our experiments. The summary of the data sets is listed in Table I."}, {"heading": "5.2 Experimental Setup", "text": "The aim of the experiments is to study the following aspects: \u2022 Dose the paired constraints guide the encoding process of traditional RBM? \u2022 How do unattended clustering algorithms based on pcGRBM model compare with their semi-monitored clustering algorithms? \u2022 How do unattended clustering algorithms based on pcGRBM model compare with these algorithms based on traditional RBM? To verify the characteristics of pcGRBM, we include conductive information on whether we use the output of pcGRBM as input of unattended clustering algorithms. In our experiments, we select K-Means, Affinity Propagation (AP) [64], SP Clustering algorithms as examples. Then we present three algorithms based on pcGRBM model for clustering task."}, {"heading": "5.3 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1 The pcGRBM for Clustering VS Unsupervised Algorithms", "text": "In this section we compare uncontrolled clustering of Kmeans, SP and AP with Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm, which are based on pcGRBM by evaluating average accuracy, average rank and average purity. From Table II, the average accuracy of K averages, SP and AP algorithms are 43.78%, 39.97% and 42.31% respectively, but the average accuracies of Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm algorithms increase to 47.48%, 47.13% and 47.39%, respectively. The average ranks of K averages, SP and AP algorithms are shown in Table III, their values are 97.5625, 154.0625 and 124.4375, but the average ranks of Kmeans.pcgrbm."}, {"heading": "5.3.2 The pcGRBM for Clustering VS Semi-supervised Algorithms", "text": "In this section we make another comparison between semi-supervised clustering of cop-kmeans, semi-SP and semi-AP using Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm by evaluation of average accuracy, average rank and average purity. In addition, the comparison of average accuracy is in Figs.2-4, respectively. From Table II, the average accuracies of cop-kmeans, semi-SP and semi-AP using Kmeans.pcgrbm algorithms are 43.85%, 40.26% and 42.53% grpcbm, respectively. The pcGRBM increase the average accuracies by 3.98%, 6.87% and 5.09%, respectively. From Table III, the average ranks of cop-kmeans, semi-SP and semi-AP bgorithms are better."}, {"heading": "5.3.3 The pcGRBM VS RBM with Gaussian Visible Units for Clustering", "text": "The pcGRBM and RBM visible with Gaussian have the ability to extract features, but which shows better performance for clustering task? To compare the representation capability between the pcGRBM and RBM without conducting paired constraints, we design a structure of the clustering algorithm in which the features of the RBM are used with the visible units of Gaussian as input of unattended clustering. In our experiment, we use three clustering algorithms based on this structure, referred to as Kmeans.grbm, SP.grbm and AP.grbm algorithms, to be compared with Kmeans.pcgrbm, SP.pcgrbm and AP.pcgrbm. From Table II are the average accuracies of kmeans.grbm, SP.grbm and AP.grbm bm algorithms, which are 43.321%, 43.387% and 43.11% respectively."}, {"heading": "5.3.4 The Rank", "text": "We compare twelve algorithms and sixteen datasets using the Aligned Friedman Test Statistics [67] given by T = (G \u2212 1) (G \u00b7 j = 1 R 2.j \u2212 (GD 2 / 4) (GD + 1) 2] {[GD (GD + 1) (2GD + 1)]] (1 / G) D = 1 R \u2212 2i. (22), where R \u2212 i is the ranking sum of the jth algorithm, R.j is the ranking sum of the ith dataset, D is the number of datasets and G is the number of algorithms, and G is the number of algorithms. In our experiments, all paediatric limitations come from labels information. We select 1% to 10% in pairs of constraints in steps of 1%. The average ranking is lower than the ranking sum of the ith dataset, D is the number of datasets, and G is the number of algorithms."}, {"heading": "6 CONCLUSION", "text": "In this paper, we proposed a novel pcGRBM model, whose learning process is guided by the pair-by-pair constraints and the coding process is conducted under instruction. Subsequently, some pair-by-pair hidden features of pcGRBM are merged, and another part of them is separated by the guidelines. In the pcGRBM learning process, CD learning is used to approximate ML learning, and pairwise hidden constraints are iterated transitions between visible and hidden units. Subsequently, the background of the picturesque constraints is encoded in hidden layer features of pcGRBM. To prove the availability of pcGRBM, the features of the hidden layer of pcGRBM are used as input \"data\" for cluster tasks. Experimental results showed that the performance of the Kmeans.pcgrbm, SP.pcgrbm, SP.cbans.Coaus assignments are better based on their classical GRSP and SP.cb.11 supercomputer-based algorithms."}, {"heading": "7 ACKNOWLEDGEMENT", "text": "This work was partially supported by the National Science Foundation of China (No. 61573292)."}], "references": [{"title": "Optimal perceptual inference", "author": ["G.E. Hinton", "T.J. Sejnowski"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. Citeseer, 1983, pp. 448\u2013453.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1983}, {"title": "Learning and releaming in boltzmann machines", "author": ["G. Hinton", "T. Sejnowski"], "venue": "Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1, pp. 282\u2013317, 1986.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1986}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural computation, vol. 14, no. 8, pp. 1771\u20131800, 2002.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "On contrastive divergence learning", "author": ["M.A. Carreira-Perpinan", "G.E. Hinton"], "venue": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics. Citeseer, 2005, pp. 33\u201340.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in Neural Information Processing Systems, vol. 19, p. 153, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "An efficient learning procedure for deep boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Neural Computation, vol. 24, no. 8, pp. 1967\u20132006, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1967}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1929}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1097\u20131105.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 3642\u2013 3649.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Fuzzy classification with restricted boltzman machines and echo-state networks for predicting potential railway door system failures", "author": ["O. Fink", "E. Zio", "U. Weidmann"], "venue": "Reliability, IEEE Transactions on, vol. 64, no. 3, pp. 861\u2013868, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Spectral\u2013spatial classification of hyperspectral data based on deep belief network", "author": ["Y. Chen", "X. Zhao", "X. Jia"], "venue": "Selected Topics in Applied Earth Observations and Remote Sensing, IEEE Journal of, vol. 8, no. 6, pp. 2381\u20132392, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Expected energy-based restricted boltzmann machine for classification", "author": ["S. Elfwing", "E. Uchibe", "K. Doya"], "venue": "Neural Networks, vol. 64, pp. 29\u201338, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1828}, {"title": "Rate-coded restricted boltzmann machines for face recognition", "author": ["Y.W. Teh", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, pp. 908\u2013914, 2001.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 3371\u20133408, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Replicated softmax: an undirected topic model", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems, 2009, pp. 1607\u20131614.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Application of deep belief networks for natural language understanding", "author": ["R. Sarikaya", "G.E. Hinton", "A. Deoras"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 22, no. 4, pp. 778\u2013784, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "A generative restricted boltzmann machine based method for high-dimensional motion data modeling", "author": ["S. Nie", "Z. Wang", "Q. Ji"], "venue": "Computer Vision and Image Understanding, vol. 136(C), pp. 14\u201322, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Voice conversion using rnn pre-trained by recurrent temporal restricted boltzmann machines", "author": ["T. Nakashika", "T. Takiguchi", "Y. Ariki"], "venue": "Audio, Speech, and Language Processing, IEEE/ACM Transactions on, vol. 23, no. 3, pp. 580\u2013587, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep belief networks for automatic music genre classification", "author": ["X. Yang", "Q. Chen", "S. Zhou", "X. Wang"], "venue": "Twelfth Annual Conference of the International Speech Communication Association, vol. 8, no. 11, 2011, pp. 13\u201316.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Real-time keypoint recognition using restricted boltzmann machine", "author": ["M. Yuan", "H. Tang", "H. Li"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 25, no. 11, pp. 2119\u20132126, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Periocular recognition using unsupervised convolutional rbm feature learning", "author": ["L. Nie", "A. Kumar", "S. Zhan"], "venue": "Pattern Recognition (ICPR), 2014 22nd International Conference on. IEEE, 2014, pp. 399\u2013 404.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Fuzzy restricted boltzmann machine for the enhancement of deep learning", "author": ["C. Chen", "C.-Y. Zhang", "L. Chen", "M. Gan"], "venue": "Fuzzy Systems, IEEE Transactions on, vol. 23, no. 6, pp. 2163\u20132173, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "R\u00e9nyi divergence based generalization for learning of classification restricted boltzmann machines", "author": ["Q. Yu", "Y. Hou", "X. Zhao", "G. Cheng"], "venue": "Data Mining Workshop (ICDMW), 2014 IEEE International Conference on. IEEE, 2014, pp. 692\u2013697.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "The spike-andslab rbm and extensions to discrete and sparse data distributions", "author": ["A. Courville", "G. Desjardins", "J. Bergstra", "Y. Bengio"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 36, no. 9, pp. 1874\u20131887, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1874}, {"title": "Gaussian-binary restricted boltzmann machines on modeling natural image statistics", "author": ["N. Wang", "J. Melchior", "L. Wiskott"], "venue": "arXiv preprint arXiv:1401.5900, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse deep belief net models for visual area v2", "author": ["C. Ekanadham", "S. Reader", "H. Lee"], "venue": "Advances in Neural Information Processing Systems, vol. 20, 2007.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Modeling documents with deep boltzmann machines", "author": ["N. Srivastava", "R.R. Salakhutdinov", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1309.6865, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "The recurrent temporal restricted boltzmann machine", "author": ["I. Sutskever", "G.E. Hinton", "G.W. Taylor"], "venue": "Advances in Neural Information Processing Systems, 2009, pp. 1601\u20131608.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised 3d local feature learning by circle convolutional restricted boltzmann machine", "author": ["Z. Han", "Z. Liu", "J. Han", "C.-M. Vong", "S. Bu", "X. Li"], "venue": "IEEE Transactions on Image Processing, vol. 25, no. 11, pp. 5331\u20135344, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Non-parallel training in voice conversion using an adaptive restricted boltzmann machine", "author": ["T. Nakashika", "T. Takiguchi", "Y. Minami"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 11, pp. 2032\u20132045, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning relevance restricted boltzmann machine for unstructured group activity and event understanding", "author": ["F. Zhao", "Y. Huang", "L. Wang", "T. Xiang", "T. Tan"], "venue": "International Journal of Computer Vision, pp. 1\u201317, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Theta-rbm: Unfactored gated restricted boltzmann machine for rotation-invariant representations", "author": ["M.V. Giuffrida", "S.A. Tsaftaris"], "venue": "arXiv preprint arXiv:1606.08805, 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Estimating 3d trajectories from 2d projections via disjunctive factored four-way conditional restricted boltzmann machines", "author": ["D.C. Mocanu", "H.B. Ammar", "L. Puig", "E. Eaton", "A. Liotta"], "venue": "arXiv preprint arXiv:1604.05865, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "A novel feature extraction method for scene recognition based on centered convolutional restricted boltzmann machines", "author": ["J. Gao", "J. Yang", "G. Wang", "M. Li"], "venue": "Neurocomputing, vol. 11, no. 2, pp. p14\u201319, 2016.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Social restricted boltzmann machine: Human behavior prediction in health social networks", "author": ["N. Phan", "D. Dou", "B. Piniewski", "D. Kil"], "venue": "2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM). IEEE, 2015, pp. 424\u2013431.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Temperature based restricted boltzmann machines.", "author": ["G. Li", "L. Deng", "Y. Xu", "C. Wen", "W. Wang", "J. Pei", "L. Shi"], "venue": "Scientific Reports,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Learning deep hierarchical visual feature coding", "author": ["H. Goh", "N. Thome", "M. Cord", "J.-H. Lim"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 25, no. 12, pp. 2212\u20132225, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Constrained kmeans clustering with background knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schr\u00f6dl"], "venue": "ICML, vol. 1, 2001, pp. 577\u2013584.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2001}, {"title": "Constrained 1-spectral clustering", "author": ["S.S. Rangapuram", "M. Hein"], "venue": "arXiv preprint arXiv:1505.06485, 2015.  12", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised clustering based on affinity propagation algorithm", "author": ["Y.J. XIAO Yu"], "venue": "Journal of Software, vol. 19, no. 11, pp. 2803\u20132813, 2008.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling image patches with a directed hierarchy of markov random fields", "author": ["S. Osindero", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2008, pp. 1121\u20131128.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning informative features from restricted boltzmann machines", "author": ["J.M. Tomczak"], "venue": "Neural Processing Letters, vol. 44, no. 3, pp. 1\u201316, 2015.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Incremental extreme learning machine based on deep feature embedded", "author": ["J. Zhang", "S. Ding", "N. Zhang", "Z. Shi"], "venue": "International Journal of Machine Learning and Cybernetics, vol. 7, no. 1, pp. 111\u2013120, 2016.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning multilevel distributed representations for high-dimensional sequences", "author": ["I. Sutskever", "G.E. Hinton"], "venue": "International Conference on Artificial Intelligence and Statistics, 2007, pp. 548\u2013555.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2007}, {"title": "Conditional restricted boltzmann machines for structured output prediction", "author": ["V. Mnih", "H. Larochelle", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1202.3748, 2012.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "2009.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "Improved learning of gaussian-bernoulli restricted boltzmann machines", "author": ["K. Cho", "A. Ilin", "T. Raiko"], "venue": "Artificial Neural Networks and Machine Learning\u2013ICANN 2011. Springer, 2011, pp. 10\u201317.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2011}, {"title": "Gaussian-bernoulli deep boltzmann machine", "author": ["K.H. Cho", "T. Raiko", "A. Ilin"], "venue": "Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1\u20137.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Two distributed-state models for generating high-dimensional time series", "author": ["G.W. Taylor", "G.E. Hinton", "S.T. Roweis"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 1025\u20131068, 2011.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}, {"title": "Supervised deep learning with auxiliary networks", "author": ["J. Zhang", "G. Tian", "Y. Mu", "W. Fan"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 353\u2013361.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep transductive semi-supervised maximum margin clustering", "author": ["G. Chen"], "venue": "arXiv preprint arXiv:1501.06237, 2015.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["J.J. Hopfield"], "venue": "Proceedings of the National Academy of Sciences, vol. 79, no. 8, pp. 2554\u20132558, 1982.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1982}, {"title": "Unsupervised learning of distributions of binary vectors using two layer networks", "author": ["Y. Freund", "D. Haussler"], "venue": "Computer Research Laboratory [University of California, Santa Cruz],", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1994}, {"title": "Convolutional deep belief networks on cifar-10", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Unpublished manuscript, vol. 40, 2010.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning deep generative models", "author": ["R. Salakhutdinov"], "venue": "Ph.D. dissertation, University of Toronto, 2009.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2009}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G. Hinton"], "venue": "Momentum, vol. 9, no. 1, p. 926, 2010.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2010}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamical analysis of contrastive divergence learning: Restricted boltzmann machines with gaussian visible units.", "author": ["R. Karakida", "M. Okada", "S.I. Amari"], "venue": "Neural Networks the Official Journal of the International Neural Network Society,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2016}, {"title": "Msra-mm 2.0: A large-scale web multimedia dataset", "author": ["H. Li", "M. Wang", "X.-S. Hua"], "venue": "Data Mining Workshops, 2009. ICDMW\u201909. IEEE International Conference on. IEEE, 2009, pp. 164\u2013169.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2009}, {"title": "Clustering by passing messages between data points", "author": ["B.J. Frey", "D. Dueck"], "venue": "Science, vol. 315, no. 5814, pp. 972\u2013976, 2007.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2007}, {"title": "Document clustering using locality preserving indexing", "author": ["D. Cai", "X. He", "J. Han"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 17, no. 12, pp. 1624\u20131637, 2005.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2005}, {"title": "Orthogonal nonnegative matrix tfactorizations for clustering", "author": ["C. Ding", "T. Li", "W. Peng", "H. Park"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006, pp. 126\u2013135.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Hinton and Sejnowski[1] proposed a learning algorithm for general Boltzmann machine which has hidden-to-hidden and visible-to-visible connections, but in practice it was too slow to be used.", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "Then, the restricted Blotzmann machine (RBM) was proposed by[2] in 1986, which has no lateral connections among nodes in each layer, so the learning procedure becomes much more efficient than general Blotzmann machine.", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "There has been extensive research into the RBM since Hinton proposed fast learning algorithms[3],", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "cn [4] by contrastive divergence (CD) learning algorithm.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "Several power and tractability deep networks was proposed, including deep belief networks[5], deep autoencoder[6], deep Boltzmann machine[7], deep dropout neural net[8].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "Several power and tractability deep networks was proposed, including deep belief networks[5], deep autoencoder[6], deep Boltzmann machine[7], deep dropout neural net[8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "Several power and tractability deep networks was proposed, including deep belief networks[5], deep autoencoder[6], deep Boltzmann machine[7], deep dropout neural net[8].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "Several power and tractability deep networks was proposed, including deep belief networks[5], deep autoencoder[6], deep Boltzmann machine[7], deep dropout neural net[8].", "startOffset": 165, "endOffset": 168}, {"referenceID": 8, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 16, "endOffset": 19}, {"referenceID": 9, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 61, "endOffset": 65}, {"referenceID": 14, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 114, "endOffset": 118}, {"referenceID": 16, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 159, "endOffset": 163}, {"referenceID": 18, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 195, "endOffset": 199}, {"referenceID": 19, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 216, "endOffset": 220}, {"referenceID": 20, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 246, "endOffset": 250}, {"referenceID": 21, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 268, "endOffset": 272}, {"referenceID": 22, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 302, "endOffset": 306}, {"referenceID": 23, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 339, "endOffset": 343}, {"referenceID": 24, "context": ", classification[9], [10], [11], [12], [13], feature learning[14], facial recognition[15], collaborative filtering[16], topic modelling[17], speech recognition[18], natural language understanding[19], computer vision[20], dimensionality reduction[21], voice conversion[22], musical genre categorization[23], real-time key point recognition[24] and periocular recognition[25].", "startOffset": 370, "endOffset": 374}, {"referenceID": 25, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 42, "endOffset": 46}, {"referenceID": 26, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 66, "endOffset": 70}, {"referenceID": 27, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 122, "endOffset": 126}, {"referenceID": 28, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 174, "endOffset": 178}, {"referenceID": 29, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 222, "endOffset": 226}, {"referenceID": 30, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 257, "endOffset": 261}, {"referenceID": 31, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 310, "endOffset": 314}, {"referenceID": 32, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 373, "endOffset": 377}, {"referenceID": 33, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 416, "endOffset": 420}, {"referenceID": 34, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 468, "endOffset": 472}, {"referenceID": 35, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 519, "endOffset": 523}, {"referenceID": 36, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 607, "endOffset": 611}, {"referenceID": 37, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 673, "endOffset": 677}, {"referenceID": 38, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 721, "endOffset": 725}, {"referenceID": 39, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 782, "endOffset": 786}, {"referenceID": 40, "context": ", fuzzy restricted Boltzmann machine(FRBM)[26], classification RBM[27], spikeand-slab restricted Boltzmann machine (ssRBM)[28], Gaussian restricted Boltzmann machines (GRBMs)[29], sparse restricted Boltzmann machine (SRBM)[30], over-replicated softmax model[31], temporal restricted Boltzmann machines (RTRBMs)[32], circle convolutional restricted Boltzmann machine (CCRBM)[33], adaptive restricted Boltzmann machine[34], relevance restricted Boltzmann machine (ReRBM)[35], theta-restricted Boltzmann machine (thetaRBM)[36], disjunctive factored four-way conditional restricted Boltzmann machine (DFFW-CRBM)[37], centered convolutional restricted Boltzmann machines (CCRBM)[38], social restricted Boltzmann machine (SRBM)[39], temperature based restricted Boltzmann machines (TRBMs)[40] and deep feature coding architecture[41].", "startOffset": 823, "endOffset": 827}, {"referenceID": 41, "context": "In addition, the pcGRBM model for clustering is better performance than some semi-supervised algorithms (Cop-Kmeans[42], SemiSpectral clustering (Semi-SP)[43] and semi-supervised affinity propagation (Semi-AP)[44]).", "startOffset": 115, "endOffset": 119}, {"referenceID": 42, "context": "In addition, the pcGRBM model for clustering is better performance than some semi-supervised algorithms (Cop-Kmeans[42], SemiSpectral clustering (Semi-SP)[43] and semi-supervised affinity propagation (Semi-AP)[44]).", "startOffset": 154, "endOffset": 158}, {"referenceID": 43, "context": "In addition, the pcGRBM model for clustering is better performance than some semi-supervised algorithms (Cop-Kmeans[42], SemiSpectral clustering (Semi-SP)[43] and semi-supervised affinity propagation (Semi-AP)[44]).", "startOffset": 209, "endOffset": 213}, {"referenceID": 20, "context": "There are several common methods to develop standard RBM such as adding connections information between the visible units and the hidden units, changing the value type of visible or hidden units, expanding the relationships of the units between visible layer and hidden layer from constant to variable by fuzzy mathematics, constructing deep network based on autoencoder[21] by pairwise constraints.", "startOffset": 370, "endOffset": 374}, {"referenceID": 44, "context": "Osindero and Hinton proposed a semi-restricted Boltzmann machines (SRBM)[45] which has lateral connections between the visible units, but these lateral connections are unit-level semi-supervised information.", "startOffset": 72, "endOffset": 76}, {"referenceID": 45, "context": "In order to enforce hidden units to be pairwise uncorrelated and to maximize entropy, Tomczak[46] proposed to add penalty term to the log-likelihood function.", "startOffset": 93, "endOffset": 97}, {"referenceID": 46, "context": "[47] built deep belief network based on SRBM for classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "Sutskever and Hinton proposed temporal restricted Boltzmann machine (TRBM)[48] by adding directed connections between previous and current states of the visible and hidden units.", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "Furthermore, they proposed the recurrent TRBM (RTRBM)[32].", "startOffset": 53, "endOffset": 57}, {"referenceID": 48, "context": "Mnih and Hinton proposed the conditional restricted Boltzmann machines (CRBMs)[49] by adding conditioning vector which determines increments to the biases of the visible and hidden layer of the traditional RBM.", "startOffset": 78, "endOffset": 82}, {"referenceID": 27, "context": "[28] developed the spike-and-slab restricted Boltzmann machine (ssRBM).", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] proposed a fuzzy restricted Boltzmann machine (FRBM) to enhance deep learning capability which can avoid the flaw.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] proposed to theoretically extend the conventional RBMs by introducing another term in the energy function to explicitly model the local spatial interactions in the input data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "Then one common way is to replace them by means of Gaussian linear units, that is Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs)[50].", "startOffset": 139, "endOffset": 143}, {"referenceID": 50, "context": "[51] proposed a novel method to improve their learning efficiency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "Moreover, the deep networks of Gaussian-Bernoulli deep Boltzmann machine (GDBM)[52], [53] has been developed by the GBRBM in recent years.", "startOffset": 79, "endOffset": 83}, {"referenceID": 52, "context": "Moreover, the deep networks of Gaussian-Bernoulli deep Boltzmann machine (GDBM)[52], [53] has been developed by the GBRBM in recent years.", "startOffset": 85, "endOffset": 89}, {"referenceID": 53, "context": "proposed a mixed model named as supervision guided autoencoder (SUGAR)[54] which includes three components: main network, auxiliary network and bridge.", "startOffset": 70, "endOffset": 74}, {"referenceID": 20, "context": "The main network is a sparsity-encouraging variant of the autoencoder[21], that is the unsupervised autoencoder.", "startOffset": 69, "endOffset": 73}, {"referenceID": 54, "context": "In the work of [55], Chen proposed a deep network structure based on RBMs which is the most related to our work.", "startOffset": 15, "endOffset": 19}, {"referenceID": 54, "context": "Both the work of [55] and our work aim to solve the similar problems, e.", "startOffset": 17, "endOffset": 21}, {"referenceID": 1, "context": "A RBM[2][3] is a two-layer network in which the first layer consists of visible units, and the second layer consists of hidden units.", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "A RBM[2][3] is a two-layer network in which the first layer consists of visible units, and the second layer consists of hidden units.", "startOffset": 8, "endOffset": 11}, {"referenceID": 55, "context": "An energy function[56] of a joint configuration (v, h) between the visible layer and the hidden layer is given by:", "startOffset": 18, "endOffset": 22}, {"referenceID": 2, "context": "Hinton[3] proposed a faster learning algorithm with the CD learning and the change of learning parameter is given by:", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "Original RBMs were developed by binary stochastic units for the hidden and visible layers[3].", "startOffset": 89, "endOffset": 92}, {"referenceID": 56, "context": "To deal with real-valued data such as natural images, one solution is that the binary visible units are replaced by linear units with independent Gaussian noise, but the hidden units remain binary, which is first suggested by[57].", "startOffset": 225, "endOffset": 229}, {"referenceID": 49, "context": "For each visible unit, it is easy to learn the variance of the noise, but it is difficult using CD1 because of taking long time[50][58].", "startOffset": 127, "endOffset": 131}, {"referenceID": 57, "context": "For each visible unit, it is easy to learn the variance of the noise, but it is difficult using CD1 because of taking long time[50][58].", "startOffset": 131, "endOffset": 135}, {"referenceID": 49, "context": "Therefore, in many applications, it is easy to normalise the data to have unit variance and zero mean[50][59][60][61].", "startOffset": 101, "endOffset": 105}, {"referenceID": 58, "context": "Therefore, in many applications, it is easy to normalise the data to have unit variance and zero mean[50][59][60][61].", "startOffset": 105, "endOffset": 109}, {"referenceID": 59, "context": "Therefore, in many applications, it is easy to normalise the data to have unit variance and zero mean[50][59][60][61].", "startOffset": 109, "endOffset": 113}, {"referenceID": 60, "context": "Therefore, in many applications, it is easy to normalise the data to have unit variance and zero mean[50][59][60][61].", "startOffset": 113, "endOffset": 117}, {"referenceID": 61, "context": "[62] demonstrated that CD1 learning is simpler than ML learning in RBMs with Gaussian linear units.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "We used the Microsoft Research Asia Multimedia (MSRAMM)[63] which contains two sub-datasets, e.", "startOffset": 55, "endOffset": 59}, {"referenceID": 63, "context": "In our experiments, we choose K-means, affinity propagation (AP)[64], SP clustering algorithms as examples.", "startOffset": 64, "endOffset": 68}, {"referenceID": 41, "context": "Secondly, the proposed algorithms are used to compare with Cop-Kmeans[42], Semi-SP[43] and Semi-AP[44], respectively.", "startOffset": 69, "endOffset": 73}, {"referenceID": 42, "context": "Secondly, the proposed algorithms are used to compare with Cop-Kmeans[42], Semi-SP[43] and Semi-AP[44], respectively.", "startOffset": 82, "endOffset": 86}, {"referenceID": 43, "context": "Secondly, the proposed algorithms are used to compare with Cop-Kmeans[42], Semi-SP[43] and Semi-AP[44], respectively.", "startOffset": 98, "endOffset": 102}, {"referenceID": 64, "context": "To evaluate the performance of the clustering algorithms, we adopt three widely used metrics: clustering accuracy[65], clustering purity[66] and clustering rank[67] as the evaluation measure.", "startOffset": 113, "endOffset": 117}, {"referenceID": 65, "context": "To evaluate the performance of the clustering algorithms, we adopt three widely used metrics: clustering accuracy[65], clustering purity[66] and clustering rank[67] as the evaluation measure.", "startOffset": 136, "endOffset": 140}], "year": 2017, "abstractText": "Restricted Boltzmann machines (RBMs) and their variants are usually trained by contrastive divergence (CD) learning, but the training procedure is an unsupervised learning approach, without any guidances of the background knowledge. To enhance the expression ability of traditional RBMs, in this paper, we propose pairwise constraints restricted Boltzmann machine with Gaussian visible units (pcGRBM) model, in which the learning procedure is guided by pairwise constraints and the process of encoding is conducted under these guidances. The pairwise constraints are encoded in hidden layer features of pcGRBM. Then, some pairwise hidden features of pcGRBM flock together and another part of them are separated by the guidances. In order to deal with real-valued data, the binary visible units are replaced by linear units with Gausian noise in the pcGRBM model. In the learning process of pcGRBM, the pairwise constraints are iterated transitions between visible and hidden units during CD learning procedure. Then, the proposed model is inferred by approximative gradient descent method and the corresponding learning algorithm is designed in this paper. In order to compare the availability of pcGRBM and traditional RBMs with Gaussian visible units, the features of the pcGRBM and RBMs hidden layer are used as input \u2018data\u2019 for K-means, spectral clustering (SP) and affinity propagation (AP) algorithms, respectively. A thorough experimental evaluation is performed with sixteen image datasets of Microsoft Research Asia Multimedia (MSRA-MM). The experimental results show that the clustering performance of K-means, SP and AP algorithms based on pcGRBM model are significantly better than traditional RBMs. In addition, the pcGRBM model for clustering task shows better performance than some semi-supervised clustering algorithms.", "creator": "LaTeX with hyperref package"}}}