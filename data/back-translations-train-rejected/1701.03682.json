{"id": "1701.03682", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2017", "title": "LIDE: Language Identification from Text Documents", "abstract": "The increase in the use of microblogging came along with the rapid growth on short linguistic data. On the other hand deep learning is considered to be the new frontier to extract meaningful information out of large amount of raw data in an automated manner. In this study, we engaged these two emerging fields to come up with a robust language identifier on demand, namely Language Identification Engine (LIDE). As a result, we achieved 95.12% accuracy in Discriminating between Similar Languages (DSL) Shared Task 2015 dataset, which is comparable to the maximum reported accuracy of 95.54% achieved so far.", "histories": [["v1", "Fri, 13 Jan 2017 14:20:06 GMT  (901kb,D)", "http://arxiv.org/abs/1701.03682v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["priyank mathur", "arkajyoti misra", "emrah budur"], "accepted": false, "id": "1701.03682"}, "pdf": {"name": "1701.03682.pdf", "metadata": {"source": "CRF", "title": "LIDE: Language Identification from Text Documents", "authors": ["Priyank Mathur", "Arkajyoti Misra", "Emrah Budur"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION Automatic language recognition is the first step in accomplishing a variety of tasks such as recognizing the source language for machine translation, improving the relevance of the search by personalizing search results according to the query language [1], [2], providing a unified search field for a multilingual dictionary [3], marking Twitter streams with a suitable language, etc. Although classifying languages belonging to different groups is not difficult, disambiguating languages originating from the same source and the same dialects is still a considerable challenge in the area of processing natural language. Regular classifiers based on word frequency are insufficient to accurately predict such similar languages, and the use of advanced machine learning tools to capture the structure of the language is necessary to increase classification performance."}, {"heading": "II. PREVIOUS WORK", "text": "In the past, a variety of methods such as Naive Bayes [4], SVM [5], n-gram [6], graph-based n-gram [7], prediction partial matching (PPM) [8], linear interpolation with post-independent weight optimization and majority decisions for combining multiple classifiers [9], etc. and the best accuracy achieved are still in the lower ninety percentage points. Researchers have worked on various critical tasks that call into question the dimensions of the issue, including, but not limited to, support for languages with limited resources, i.e. Nepali, Urdu and Icelandic [10], [11] dealing with user-generated unstructured short texts, i.e. microblogs [9] the construction of an agnostic machine [10]."}, {"heading": "III. DATASET DESCRIPTION", "text": "The data for this project comes from \"Discriminating between Similar Language (DSL) Shared Task 2015\" [18]. A set of 20,000 instances per language (18,000 training (train.txt) and 2000 evaluation (test.txt) was provided for 13 different world languages. The data set also consisted of a subset (develop.txt) of the general training data we used for hyperparameter tuning. Languages are grouped as shown in Table I. The names of the groups are frequently referenced in the following paragraphs. Each entry in the data set is a full sentence extracted from journalistic corpora and written in one of the languages and tagged with the language group and the country of origin. A similar set of mixed language instance was also provided for ar Xiv: 170 1.03 682v 1 [cs.C L] 13 January 2017http: / / SeeYourLanguage.infoadd noise to the data."}, {"heading": "IV. METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Multinomial Naive Bayes", "text": "We have not pre-processed the text commonly used in this area, such as word origin or word removal, because we believe that this could potentially remove important signatures of a particular language, especially if the same language is spoken by two geographically distinct groups of people (e.g. Portuguese, which is spoken in Portugal and Brazil).We have experimented with both word and letter-n-grams. The letter-n-grams proved to be particularly useful in distinguishing between two languages, which use largely different strings in their alphabets.The letter-n-gram level behaves very differently from the letter-n-grams, as shown in Fig. 2. Individual letters carry little information, and therefore performance for letter-n-grams is quite improved, as the number of characters is increased before they are saturated at n = 8. We have experimented with letter-n-grams, which carry little information, and therefore performance for letter-n-grams is quite limited, as the number of characters is increased before they are saturated at n = 8."}, {"heading": "B. Logistic Regression", "text": "Next, we tried regulated logistical regression, and here, too, the n-gram character level performed slightly better than the n-gram word. Fig. 3 shows that the model was able to complete the training set, but the performance on the validation set was close to 0.945. The best performance was achieved by a 9-gram character model, which shortened all n-grams to n = 9. These n-grams were shortened at the word boundaries, in other words, these n-grams did not capture two or more consecutive words. Loosening this criterion significantly increases the size of the Term Frequency Matrix and shifts the limit of the computer memory, but improves performance by a fraction of one percent."}, {"heading": "C. Recurrent Neural Network", "text": "This year, we have it in the hand in which we find ourselves, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand."}, {"heading": "V. RESULTS", "text": "Table II shows a comparison of the models we have experimented with. A surprising feature of the result is that individual RNN models were not able to outperform the performance of the MNN and LR models, although the latter have minimal knowledge of a language structure. However, when we created an ensemble of RNN models, it proved to be the best model, exceeding the 95% threshold for the first time. It should be noted that MNB and LR models for a particular n-gram model use all m-grams where 1 \u2264 m n is used. However, due to the nature of an RNN architecture, a combination of n-grams cannot be used, as this will result in an overlapping sequence of content being fed into the network. Since each given n-gram captures only limited information about a language, it was natural to try an ensemble of n-gram RNN models with different values of n, so that the structure can be captured on multiple levels."}, {"heading": "VI. DISCUSSION", "text": "In fact, most of them are able to survive themselves if they see themselves able to survive and survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "VII. CONCLUSION AND NEXT STEPS", "text": "We have presented a deep neural network-based speech recognition scheme that achieves near-perfect accuracy in classifying disparate languages and about 90% accuracy in very similar languages. West Slavic languages in particular posed the biggest challenge, and expanding the corpus of these languages using external sources did not help much, mainly because no n-grammes of words unique to specific languages were included in the extended part of the corpus. We have relied on the overall set of RNN models to discover the structure that is unique to a particular language, but we have not been able to develop additional features due to a lack of knowledge of these specific languages. At this point, we think that further improvements can only be achieved by designing rules-based features through conversations with language experts or native speakers."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank David Jurgens of Stanford University's Department of Computer Science for his help with the initial idea, data set and previous research; Junjie Qin of Stanford University's Department of Computational and Mathematical Engineering for his mentoring and insightful comments that polished up the study's outcome; AWS Educate Program for providing EC2 credits and computing resources; and Microsoft Azure for Research Program for providing Azure credits and full computing resources; and finally Google, Yandex and Basis Tech for providing free access to their voice recognition APIs for our benchmarking analysis."}], "references": [{"title": "Ambiguity of Queries and the Challenges for Query Language Detection", "author": ["J. Stiller", "M. G\u00e4de", "V. Petras"], "venue": "CLEF 2010 Labs and Workshops Notebook Papers, 2010. [Online]. Available: http://clef2010. org/resources/proceedings/clef2010labs{ }submission{ }41.pdf", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Query language determination using query terms and interface language", "author": ["F. Datta", "Ruchira S. Lopiano"], "venue": "Patent US 11/408 245, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Word Level Language Identification in Online Multilingual Communication", "author": ["D. Nguyen", "a. S. Do"], "venue": "vol. 23, no. October, pp. 857\u2013862, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "langid. py: An off-the-shelf language identification tool", "author": ["M. Lui", "T. Baldwin"], "venue": "Proceedings of the ACL 2012 System Demonstrations, no. July, pp. 25\u201330, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Language identification of names with SVMs", "author": ["A. Bhargava", "G. Kondrak"], "venue": "Computational Linguistics, no. June, pp. 693\u2013696, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "N-Gram-Based Text Categorization", "author": ["W.B. Cavnar", "J.M. Trenkle", "A.A. Mi"], "venue": "Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval, 1994.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1994}, {"title": "Graph-based N-gram language identification on short texts", "author": ["E. Tromp", "M. Pechenizkiy"], "venue": "\u201dProceedings of the 20th annual Belgian-Dutch Conference on Machine Learning\u201d, pp. 27\u201334, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Native Language Identification with PPM", "author": ["V. Bobicev"], "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, 2013, pp. 180\u2013187.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Microblog language identification: overcoming the limitations of short, unedited and idiomatic text", "author": ["S. Carter", "W. Weerkamp", "M. Tsagkias"], "venue": "Language Resources and Evaluation, vol. 47, no. 1, pp. 195\u2013215, 2012. [Online]. Available: http://www.springerlink.com/ index/10.1007/s10579-012-9195-y", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Language Identification for Creating Language-Specific Twitter Collections", "author": ["S. Bergsma", "P. Mcnamee", "M. Bagdouri", "C. Fink", "T. Wilson"], "venue": "\u201dProceedings of the 2nd Workshop on Language in Social Media at NAACL-HLT\u201912\u201d, no. Lsm 2012, pp. 65\u201374, 2012. [Online]. Available: http://www.aclweb.org/anthology-new/W/ W12/W12-2108.pdf", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Half of messages on Twitter are not in English Japanese is the second most used language", "author": ["Semicoast"], "venue": "Semiocast, p. 75005, 2010. [Online]. Available: http: //semiocast.com/downloads/Semiocast{ }Half{ }of{ }messages{ }on{ }Twitter{ }are{ }not{ }in{ }English{ }20100224.pdf", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Google Compact Language Detector 2", "author": ["D. Sites"], "venue": "2013. [Online]. Available: https://github.com/CLD2Owners/cld2", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Statistical Identification of Language", "author": ["T. Dunning"], "venue": "Computing, no. November, 1994.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "Automatic language identification using deep neural networks", "author": ["I. Lopez-Moreno", "J. Gonzalez-Dominguez", "O. Plchot", "D. Martinez", "J. Gonzalez-Rodriguez", "P.J. Moreno"], "venue": "Icassp-2014, pp. 5337\u20135341, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning for spoken language identification", "author": ["G. Montavon"], "venue": "NIPS Workshop on Deep Learning for Speech Recognition and Related Applications, pp. 1\u20134, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep bottleneck features for spoken language identification", "author": ["B. Jiang", "Y. Song", "S. Wei", "J.H. Liu", "I.V. McLoughlin", "L.R. Dai"], "venue": "PLoS ONE, vol. 9, no. 7, pp. 3012\u20133016, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic Language Identification using Long Short-Term Memory Recurrent Neural Networks", "author": ["J. Gonzalez-Dominguez", "I. Lopez-Moreno", "H. Sak", "J. Gonzalez- Rodriguez", "P.J. Moreno"], "venue": "Interspeech- 2014, 2014, pp. 2155\u20132159.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Accelerating t-SNE using tree-based algorithms", "author": ["L. Van Der Maaten"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 3221\u20133245, 1 2014. [Online]. Available: http://dl.acm.org/citation.cfm? id=2627435.2697068", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "International Conference on Machine Learning, no. 2, pp. 1310\u20131318, 2013. [Online]. Available: http://jmlr.org/proceedings/papers/v28/pascanu13.pdf", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory", "author": ["S.H. Schmidhuber", "Jrgen"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735 \u2013 1780, 1997.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning Phrase Representations using RNN Encoder- Decoder for Statistical Machine Translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv, pp. 1724\u20131734, 2014. [Online]. Available: http://arxiv.org/abs/1406.1078", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 1 2014. [Online]. Available: http://dl.acm.org/citation.cfm?id=2627435.2670313", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1929}], "referenceMentions": [{"referenceID": 0, "context": "Automatic language detection is the first step toward achieving a variety of tasks like detecting the source language for machine translation, improving the search relevancy by personalizing the search results according to the query language [1], [2], providing uniform search box for a multilingual dictionary [3], tagging data stream from Twitter with appropriate language etc.", "startOffset": 242, "endOffset": 245}, {"referenceID": 1, "context": "Automatic language detection is the first step toward achieving a variety of tasks like detecting the source language for machine translation, improving the search relevancy by personalizing the search results according to the query language [1], [2], providing uniform search box for a multilingual dictionary [3], tagging data stream from Twitter with appropriate language etc.", "startOffset": 247, "endOffset": 250}, {"referenceID": 2, "context": "Automatic language detection is the first step toward achieving a variety of tasks like detecting the source language for machine translation, improving the search relevancy by personalizing the search results according to the query language [1], [2], providing uniform search box for a multilingual dictionary [3], tagging data stream from Twitter with appropriate language etc.", "startOffset": 311, "endOffset": 314}, {"referenceID": 3, "context": "In the past, a variety of methods have been tried like Naive Bayes [4], SVM [5], n-gram [6], graph-based n-gram [7], prediction partial matching (PPM) [8], linear interpolation with post independent weight optimization and majority voting for combining multiple classifiers [9] etc.", "startOffset": 67, "endOffset": 70}, {"referenceID": 4, "context": "In the past, a variety of methods have been tried like Naive Bayes [4], SVM [5], n-gram [6], graph-based n-gram [7], prediction partial matching (PPM) [8], linear interpolation with post independent weight optimization and majority voting for combining multiple classifiers [9] etc.", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "In the past, a variety of methods have been tried like Naive Bayes [4], SVM [5], n-gram [6], graph-based n-gram [7], prediction partial matching (PPM) [8], linear interpolation with post independent weight optimization and majority voting for combining multiple classifiers [9] etc.", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "In the past, a variety of methods have been tried like Naive Bayes [4], SVM [5], n-gram [6], graph-based n-gram [7], prediction partial matching (PPM) [8], linear interpolation with post independent weight optimization and majority voting for combining multiple classifiers [9] etc.", "startOffset": 112, "endOffset": 115}, {"referenceID": 7, "context": "In the past, a variety of methods have been tried like Naive Bayes [4], SVM [5], n-gram [6], graph-based n-gram [7], prediction partial matching (PPM) [8], linear interpolation with post independent weight optimization and majority voting for combining multiple classifiers [9] etc.", "startOffset": 151, "endOffset": 154}, {"referenceID": 8, "context": "In the past, a variety of methods have been tried like Naive Bayes [4], SVM [5], n-gram [6], graph-based n-gram [7], prediction partial matching (PPM) [8], linear interpolation with post independent weight optimization and majority voting for combining multiple classifiers [9] etc.", "startOffset": 274, "endOffset": 277}, {"referenceID": 9, "context": "Icelandic [10], [11] handling user-generated unstructured short texts, i.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "Icelandic [10], [11] handling user-generated unstructured short texts, i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "microblogs [10], [9] building a domain agnostic engine [10], [7].", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "microblogs [10], [9] building a domain agnostic engine [10], [7].", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "microblogs [10], [9] building a domain agnostic engine [10], [7].", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "microblogs [10], [9] building a domain agnostic engine [10], [7].", "startOffset": 61, "endOffset": 64}, {"referenceID": 9, "context": "Existing benchmarking solutions approach the LID problem in different ways where LogR [10] adopts a discriminative approaches with regularized logistic regression, TextCat and Google CLD [12] recruits N-gram-based algorithm, langid.", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Existing benchmarking solutions approach the LID problem in different ways where LogR [10] adopts a discriminative approaches with regularized logistic regression, TextCat and Google CLD [12] recruits N-gram-based algorithm, langid.", "startOffset": 187, "endOffset": 191}, {"referenceID": 3, "context": "py [4] relies on a Naive Bayes classifier with a multinomial event model.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "The outstanding results, of the time, suggested by Cavnar and Trenkle became de facto standard of LID even today [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 12, "context": "The significant ingredient of their method is shown to use a rank order statistic called \u201dout of place\u201d distance measure [13].", "startOffset": 121, "endOffset": 125}, {"referenceID": 10, "context": "Considering that Japanese is the second most frequent language used in Twitter [11], there is a need for better approach to scale the solution to all languages.", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "As a solution to their problem, Dunning came up with a better approach with incorporating byte level n-grams of the whole string instead of char level n-grams of the words [13].", "startOffset": 172, "endOffset": 176}, {"referenceID": 13, "context": "On the other hand, there are a few number of studies that applied deep learning to identify the language of speech [14], [15], [16], [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 14, "context": "On the other hand, there are a few number of studies that applied deep learning to identify the language of speech [14], [15], [16], [17].", "startOffset": 121, "endOffset": 125}, {"referenceID": 15, "context": "On the other hand, there are a few number of studies that applied deep learning to identify the language of speech [14], [15], [16], [17].", "startOffset": 127, "endOffset": 131}, {"referenceID": 16, "context": "On the other hand, there are a few number of studies that applied deep learning to identify the language of speech [14], [15], [16], [17].", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "We applied t-SNE algorithm to visualize the instances in 3D euclidean space [19], [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 18, "context": "Until recently, RNNs were considered very difficult to train because of the problem of exploding or vanishing gradients [22] which makes it very difficult for them to learn long sequences of input.", "startOffset": 120, "endOffset": 124}, {"referenceID": 19, "context": "Recent architectures like Long Short Term Memory (LSTM) [23] and Gated Recurrent Unit (GRU) [24] were also specifically designed to get around this problem.", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "Recent architectures like Long Short Term Memory (LSTM) [23] and Gated Recurrent Unit (GRU) [24] were also specifically designed to get around this problem.", "startOffset": 92, "endOffset": 96}, {"referenceID": 21, "context": "dropping hidden units as each example propagates through the network and back [25].", "startOffset": 78, "endOffset": 82}, {"referenceID": 3, "context": "py is an off-the-shelf language identification tool and it is considered to be a cornerstone in the literature [4].", "startOffset": 111, "endOffset": 114}], "year": 2017, "abstractText": "The increase in the use of microblogging came along with the rapid growth on short linguistic data. On the other hand deep learning is considered to be the new frontier to extract meaningful information out of large amount of raw data in an automated manner. In this study, we engaged these two emerging fields to come up with a robust language identifier on demand, namely Language Identification Engine (LIDE). As a result, we achieved 95.12% accuracy in Discriminating between Similar Languages (DSL) Shared Task 2015 dataset, which is comparable to the maximum reported accuracy of 95.54% achieved so far.", "creator": "LaTeX with hyperref package"}}}