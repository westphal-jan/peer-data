{"id": "1701.03849", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2017", "title": "Deep Neural Networks for Czech Multi-label Document Classification", "abstract": "This paper is focused on automatic multi-label document classification of Czech text documents. The current approaches usually use some pre-processing which can have negative impact (loss of information, additional implementation work, etc). Therefore, we would like to omit it and use deep neural networks that learn from simple features. This choice was motivated by their successful usage in many other machine learning fields. Two different networks are compared: the first one is a standard multi-layer perceptron, while the second one is a popular convolutional network. The experiments on a Czech newspaper corpus show that both networks significantly outperform baseline method which uses a rich set of features with maximum entropy classifier. We have also shown that convolutional network gives the best results.", "histories": [["v1", "Fri, 13 Jan 2017 23:23:12 GMT  (135kb)", "http://arxiv.org/abs/1701.03849v1", "Dears, this paper has been already accepted for CICLing 2016 and presented at the conference in 04 2016. Unfortunately, the proceedings is not ready yet. Therefore, we have decided to publish this paper at ArXiv. -acceptance letter begin- Dear Professor Kral, We are happy to notify you that your paper 167: \"Deep Neural Networks for Czech Multi-label Document Classification\" has been ACCEPTED"], ["v2", "Wed, 18 Jan 2017 23:17:30 GMT  (135kb)", "http://arxiv.org/abs/1701.03849v2", "Accepted for CICLing 2016 and presented at the conference in 04 2016"]], "COMMENTS": "Dears, this paper has been already accepted for CICLing 2016 and presented at the conference in 04 2016. Unfortunately, the proceedings is not ready yet. Therefore, we have decided to publish this paper at ArXiv. -acceptance letter begin- Dear Professor Kral, We are happy to notify you that your paper 167: \"Deep Neural Networks for Czech Multi-label Document Classification\" has been ACCEPTED", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ladislav lenc", "pavel kr\\'al"], "accepted": false, "id": "1701.03849"}, "pdf": {"name": "1701.03849.pdf", "metadata": {"source": "CRF", "title": "Deep Neural Networks for Czech Multi-label Document Classification", "authors": ["Ladislav Lenc", "Pavel Kr\u00e1l"], "emails": ["llenc@kiv.zcu.cz", "pkral@kiv.zcu.cz"], "sections": [{"heading": null, "text": "ar Xiv: 170 1,03 849v 1 [cs.C L] 13 YesKeywords: Czech, Deep Neural Networks, document classification, multi-label"}, {"heading": "1 Introduction", "text": "The amount of electronic text documents is growing extremely fast and therefore the automatic classification of documents (or categorization) is becoming very important for information organization, storage and recovery. Multi-label classification is much more important than label classification because it usually better meets the needs of current applications. Modern approaches usually use several pre-processing tasks: feature selection / reduction [1]; the precise representation of documents (e.g. POS filtering, certain lexical and syntactic properties, lemmatization, etc.) to reduce the feature space with minimal negative effects on classification. However, this pre-processing has several disadvantages, such as loss of information, dependence on task / application, etc. Neural networks with deep learning are very popular in the field of machine learning today and it has been proven that they perform many state-of-the-art approaches."}, {"heading": "2 Related Work", "text": "Most of them are able to abide by the rules that they have imposed on themselves. (...) Most of them are able to abide by the rules. (...) Most of them are able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) \"(...)\" (...) \"(...)\" (...) (...) \"(...\" () (...) \"(...) () () () () () () () () () () () () ()) () () ()) () () () ()) () () () ()) () () () () () ()) () () () () () ()) () () () () () () ()) () () () () () () ()) () () () () () ()) () () () () () () ()) () () () () ()) () () () () () ()) () () () () () ()) () () ()) () () () () ()) () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () (() () () () () () () () () () () () () (() () () () () () () () () () () () (() () () () () () () ("}, {"heading": "3 Neural Nets for Multi-label Document Classification", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Baseline Classification", "text": "The feature set was created according to Brychc\u0131 \u0301 n et al. [21] and consists of words, strains and features created by S-LDA and COALS. They are used because the authors have experimentally proven that the additional unsupervised features significantly improve the classification outcomes. In multiple classification, we use an efficient approach outlined by Tsoumakas et al. in [22] This method uses n binary classifiers Cni = 1: d \u2192 l, \u00ac l (i.e. any binary classifier assigns document d to the label l if the label is included in the document, \u00ac l otherwise).The classification result is derived from the following equation: C (d) = 1: Ci (d) (1) The model of maximum entropy (ME) is used for classification."}, {"heading": "3.2 Standard Feed-forward Deep Neural Network (FDNN)", "text": "As we enter our network, we use the simple Bag of Words (BoW), a binary vector where value 1 means that the word with a specific index is present in the document. The size of this vector depends on the size of the dictionary, which is limited by N most commonly used words. The only pre-processing is the conversion of all characters to lowercase letters, and also the replacement of all numbers with a common token. Therefore, the size of the input layer depends on the size of the dictionary used to create feature vectors. The first hidden layer has 1024, while the second has 512 nodes 3. The output layer has the size of the number of categories, which in our case is 37. To handle multi-label classification, we limit the values of the nodes in the output layer. Only values greater than a given threshold are assigned to the labels."}, {"heading": "3.3 Convolutional Neural Network (CNN)", "text": "The input function of CNN is a sequence of words in the document. We use a similar document pre-processing and dictionary as in the previous approach, and the words are then represented by the indexes in the dictionary. The first important problem of this network for document classification is the variable length of the documents. It is usually solved by setting a fixed value and shortening longer documents, while shorter ones must be supplemented to ensure exactly the same length. Words that are not in the dictionary are assigned to a reserved index, and the cushion also has a reserved index.The architecture of our network is motivated by Kim in [14].However, we only use one size of the evolutionary core, not the combination of several sizes. Our cores have only one dimension (1D), while Kim has used larger 2-dimensional cores. This is mainly due to our preliminary experiments, where the simple 1-dimensional cores yielded better results than the larger ones. The input of our network is a vector."}, {"heading": "4 Experiments", "text": "In this section we will first describe the Czech document corpus that we used to evaluate our methods, then describe the experiments carried out and the final results. The results will be compared with already published results on the Czech document corpus."}, {"heading": "4.1 Tools and Corpus", "text": "For the implementation of all neural networks, we used Kera's toolkit [23], which is based on the Theano Deep Learning Library [24]. It was chosen mainly on the basis of its good performance and our previous experience with this tool. All experiments were calculated on GPU to achieve reasonable computing times. As already mentioned, the results of this work are used by C-TK. Therefore, for the following experiments, we used the Czech text documents provided by C-TK. This corpus contains 2,974,040 words, which belong to 11,955 documents. The documents are commented from a series of 60 categories, of which we used 37 most frequently. Category reduction was made to allow comparison with previously reported results in this corpus, where the same group of 37 categories was used. Figure 2 illustrates the distribution of documents depending on the number of labels. Figure 3 shows the distribution of document lengths (in word marks) This corpus is available for research purposes free at http: / fizz.cz for the rest of the results."}, {"heading": "4.2 Experimental Results", "text": "In fact, it is not as if we have been able to distinguish ourselves as we have in the past. It is not as if we have been able to trump each other. It is not as if we have been able to trump each other. It is not as if we have been able to trump ourselves. It is not as if we have been able to trump ourselves. It is not as if we have been able to trump ourselves. It is not as if we have been able to trump ourselves. It is not as if we have been able to trump ourselves. It is not as if we have been able to trump ourselves."}, {"heading": "5 Conclusions and Future Work", "text": "In this thesis we used two different neural networks for the multi-level classification of Czech text documents. Several experiments were conducted to determine optimal network topologies and parameters. An important contribution is the evaluation of the performance of neural networks using simple features. Therefore, we used the BoW representation for the FDNN and the sequence of word indexes for the CNN as input. On the basis of these experiments we can determine the following: - the two proposed network topologies together with the threshold of the output are efficient for multi-level classification tasks - Softmax activation function is better for FDNN, while the sigmoid activation function provides better results for CNN - CNN only slightly exceeds FDNN (\u0445F1 \u0445 + 0.6%) - most important is the fact that both neural networks with only basic preprocessing and without parameterization will significantly improve the maximum entropy method with a rich set of parameters."}, {"heading": "Acknowledgements", "text": "This work was supported by the LO1506 project of the Ministry of Education, Youth and Sport of the Czech Republic. We would also like to thank the Czech New Agency for the support and provision of the data."}], "references": [{"title": "A comparative study on feature selection in text categorization", "author": ["Y. Yang", "J.O. Pedersen"], "venue": "Proceedings of the Fourteenth International Conference on Machine Learning. ICML \u201997, San Francisco, CA, USA, Morgan Kaufmann Publishers Inc.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Multiple sets of features for automatic genre classification of web documents", "author": ["C.S. Lim", "K.J. Lee", "G.C. Kim"], "venue": "Information Processing and Management 41", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research 12", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A comparison between multi-layer perceptrons and convolutional neural networks for text image super-resolution", "author": ["C. Peyrard", "F. Mamalet", "C. Garcia"], "venue": "International Conference on Computer Vision Theory and Applications.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Inducing features of random fields", "author": ["S. Della Pietra", "V. Della Pietra", "J. Lafferty"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 19", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Optimizing text classification through efficient feature selection based on quality metric", "author": ["J.C. Lamirel", "P. Cuxac", "A.S. Chivukula", "K. Hajlaoui"], "venue": "Journal of Intelligent Information Systems", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Using syntactic information in document filtering: A comparative study of part-of-speech tagging and supertagging", "author": ["R. Chandrasekar", "B. Srinivas"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora", "author": ["D. Ramage", "D. Hall", "R. Nallapati", "C.D. Manning"], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1. EMNLP \u201909, Stroudsburg, PA, USA, Association for Computational Linguistics", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Partially labeled topic models for interpretable text mining", "author": ["D. Ramage", "C.D. Manning", "S. Dumais"], "venue": "Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD \u201911, New York, NY, USA, ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Pca document reconstruction for email classification", "author": ["J.C. Gomez", "M.F. Moens"], "venue": "Computer Statistics and Data Analysis 56", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "A multi-layer text classification framework based on two-level representation model", "author": ["J. Yun", "L. Jing", "Y.J.", "H. Huang"], "venue": "Expert Systems with Applications 39", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Text understanding from scratch", "author": ["X. Zhang", "Y. LeCun"], "venue": "arXiv preprint arXiv:1502.01710", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "arXiv preprint arXiv:1408.5882", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Proceedings of Workshop at ICLR.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "A tutorial survey of architectures, algorithms, and applications for deep learning", "author": ["L. Deng"], "venue": "APSIPA Transactions on Signal and Information Processing 3", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "One-class document classification via neural networks", "author": ["L. Manevitz", "M. Yousef"], "venue": "Neurocomputing 70", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Multilabel neural networks with applications to functional genomics and text categorization", "author": ["M.L. Zhang", "Z.H. Zhou"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 18", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Evaluation of the Document Classification Approaches", "author": ["M. Hrala", "P. Kr\u00e1l"], "venue": "8th International Conference on Computer Recognition Systems (CORES 2013), Milkow, Poland, Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-label document classification in Czech", "author": ["M. Hrala", "P. Kral"], "venue": "16th International conference on Text, Speech and Dialogue (TSD 2013), Pilsen, Czech Republic, Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Novel unsupervised features for Czech multi-label document classification", "author": ["T. Brychc\u0131\u0301n", "P. Kr\u00e1l"], "venue": "13th Mexican International Conference on Artificial Intelligence (MICAI 2014), Tuxtla Gutierrez, Chiapas, Mexic, Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-label classification: An overview", "author": ["G. Tsoumakas", "I. Katakis"], "venue": "International Journal of Data Warehousing and Mining (IJDWM) 3", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for scientific computing conference (SciPy). Volume 4., Austin, TX", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluation: From precision, recall and f-measure to roc., informedness, markedness & correlation", "author": ["D. Powers"], "venue": "Journal of Machine Learning Technologies", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Numerical recipes in C", "author": ["W.H. Press", "S.A. Teukolsky", "W.T. Vetterling", "B.P. Flannery"], "venue": "Volume 2. Citeseer", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "The modern approaches usually use several pre-processing tasks: feature selection/reduction [1]; precise document representation (e.", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": ") [2] to reduce the feature space with minimal negative impact on classification accuracy.", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": "This fact is particularly evident in image processing [3], however it was further showed that they are also superior in Natural Language Processing (NLP) including Part-Of-Speech (POS) tagging, chunking, named entity recognition or semantic role labelling [4].", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "This fact is particularly evident in image processing [3], however it was further showed that they are also superior in Natural Language Processing (NLP) including Part-Of-Speech (POS) tagging, chunking, named entity recognition or semantic role labelling [4].", "startOffset": 256, "endOffset": 259}, {"referenceID": 4, "context": "Note that we expect better performance of the CNNs as shown for instance in the OCR task [5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "Several classification methods have been successfully used [6], for instance Bayesian classifiers, Maximum Entropy (ME), Support Vector Machines (SVMs), etc.", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "Numerous feature selection/reduction approaches have been introduced [1,7] to solve this problem.", "startOffset": 69, "endOffset": 74}, {"referenceID": 6, "context": "Numerous feature selection/reduction approaches have been introduced [1,7] to solve this problem.", "startOffset": 69, "endOffset": 74}, {"referenceID": 1, "context": "using lexical and syntactic features as shown in [2].", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "further show in [8] that it is beneficial to use POS-tag filtration in order to represent a document more accurately.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "More recently, some interesting approaches based on Latent Dirichlet Allocation (LLDA) [9] have been introduced.", "startOffset": 87, "endOffset": 90}, {"referenceID": 9, "context": "Another method exploits partial labels to discover latent topics [10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "Principal Component Analysis [11] incorporating semantic concepts [12] has also been used for the document classification.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "Principal Component Analysis [11] incorporating semantic concepts [12] has also been used for the document classification.", "startOffset": 66, "endOffset": 70}, {"referenceID": 3, "context": "Recently, \u201cdeep\u201d Neural Nets (NN) have shown their superior performance in many natural language processing tasks including POS tagging, chunking, named entity recognition and semantic role labelling [4] without any parametrization.", "startOffset": 200, "endOffset": 203}, {"referenceID": 12, "context": "For instance, the authors of [13] propose two Convolutional Neural Nets (CNN) for ontology classification, sentiment analysis and single-label document classification.", "startOffset": 29, "endOffset": 33}, {"referenceID": 13, "context": "Another interesting work [14] uses in the first layer (i.", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "lookup table) pre-trained vectors from word2vec [15].", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "For additional information about architectures, algorithms, and applications of deep learning, please refer the survey [16].", "startOffset": 119, "endOffset": 123}, {"referenceID": 16, "context": "show in [17] that their simple feed-forward neural network with three layers (20 inputs, 6 neurons in hidden layer and 10 neurons in the output layer, i.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "Traditional multi-layer neural networks were also used for multi-label document classification in [18].", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "use in [19] lemmatization and Part-Of-Speech (POS) filtering for a precise representation of Czech documents.", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "In [20], three different multi-label classification approaches are compared and evaluated.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Another recent work proposes novel features based on the unsupervised machine learning [21].", "startOffset": 87, "endOffset": 91}, {"referenceID": 20, "context": "[21] and is composed of words, stems and features created by S-LDA and COALS.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "in [22].", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "The architecture of our network is motivated by Kim in [14].", "startOffset": 55, "endOffset": 59}, {"referenceID": 22, "context": "For implementation of all neural-nets we used Keras tool-kit [23] which is based on the Theano deep learning library [24].", "startOffset": 61, "endOffset": 65}, {"referenceID": 23, "context": "For implementation of all neural-nets we used Keras tool-kit [23] which is based on the Theano deep learning library [24].", "startOffset": 117, "endOffset": 121}, {"referenceID": 24, "context": "For evaluation of the document classification accuracy, we use the standard F-measure (F1) metric [25].", "startOffset": 98, "endOffset": 102}, {"referenceID": 25, "context": "95 [26].", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Summary of the Results Table 6 compares the results of our approaches with another efficient method [21].", "startOffset": 100, "endOffset": 104}, {"referenceID": 20, "context": "[21] 89.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "This paper is focused on automatic multi-label document classification of Czech text documents. The current approaches usually use some preprocessing which can have negative impact (loss of information, additional implementation work, etc). Therefore, we would like to omit it and use deep neural networks that learn from simple features. This choice was motivated by their successful usage in many other machine learning fields. Two different networks are compared: the first one is a standard multi-layer perceptron, while the second one is a popular convolutional network. The experiments on a Czech newspaper corpus show that both networks significantly outperform baseline method which uses a rich set of features with maximum entropy classifier. We have also shown that convolutional network gives the best results.", "creator": "gnuplot 4.6 patchlevel 6"}}}