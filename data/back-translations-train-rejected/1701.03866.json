{"id": "1701.03866", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2017", "title": "Long Timescale Credit Assignment in NeuralNetworks with External Memory", "abstract": "Credit assignment in traditional recurrent neural networks usually involves back-propagating through a long chain of tied weight matrices. The length of this chain scales linearly with the number of time-steps as the same network is run at each time-step. This creates many problems, such as vanishing gradients, that have been well studied. In contrast, a NNEM's architecture recurrent activity doesn't involve a long chain of activity (though some architectures such as the NTM do utilize a traditional recurrent architecture as a controller). Rather, the externally stored embedding vectors are used at each time-step, but no messages are passed from previous time-steps. This means that vanishing gradients aren't a problem, as all of the necessary gradient paths are short. However, these paths are extremely numerous (one per embedding vector in memory) and reused for a very long time (until it leaves the memory). Thus, the forward-pass information of each memory must be stored for the entire duration of the memory. This is problematic as this additional storage far surpasses that of the actual memories, to the extent that large memories on infeasible to back-propagate through in high dimensional settings. One way to get around the need to hold onto forward-pass information is to recalculate the forward-pass whenever gradient information is available. However, if the observations are too large to store in the domain of interest, direct reinstatement of a forward pass cannot occur. Instead, we rely on a learned autoencoder to reinstate the observation, and then use the embedding network to recalculate the forward-pass. Since the recalculated embedding vector is unlikely to perfectly match the one stored in memory, we try out 2 approximations to utilize error gradient w.r.t. the vector in memory.", "histories": [["v1", "Sat, 14 Jan 2017 01:47:54 GMT  (164kb,D)", "http://arxiv.org/abs/1701.03866v1", "Accepted into the NIPS 2016 workshop on Continual Learning"]], "COMMENTS": "Accepted into the NIPS 2016 workshop on Continual Learning", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["steven stenberg hansen"], "accepted": false, "id": "1701.03866"}, "pdf": {"name": "1701.03866.pdf", "metadata": {"source": "CRF", "title": "Long Timescale Credit Assignment in Neural Networks with External Memory", "authors": ["Steven S. Hansen"], "emails": ["sshansen@stanford.edu"], "sections": [{"heading": null, "text": "Long-term lending in neural networks with external memorySteven S. Hansen Department of PsychologyStanford University Stanford, CA 94303sshansen @ stanford.edu"}, {"heading": "1 Introduction", "text": "Neural networks with external memory (NNEM), such as the Neural Turing Machine [1] and Memory Networks [2], have often been compared to the human hippocampus in their ability to maintain episodic information over long time scales, but so far these networks have only been designed for tasks that require storage in memory of a few minutes, while the hippocampus stores information on the order of years. It is known that this type of long-term episodic storage is essential to overcome the problem of catastrophic interference [4], which is central to the challenge of continuous learning. While some of these discrepancies can be attributed to the short-term nature of current benchmark tasks, a major bottleneck is reliance on the use of back-propagation-through-time to learn the embedding function, which maps the high-dimensional observations to low-dimensional representations that are suitable for storage in external memory."}, {"heading": "2 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Long Timescale Credit Assignment in NNEMs", "text": "The length of this chain scales linearly with the number of time steps that the same network performs in each time step, creating many problems, such as the disappearance of gradients that have been well studied [7]. In contrast, the architecture of an NNEM is recurrence1 While its work uses a long time external memory, it does not address the issues raised here, as the embedding function has not been optimized for use in an external memory. Rather, it was either a random projection or prepared for a separate target. Indeed, the low performance of the latter suggests that end-to-end optimization may be necessary to pay off in this context."}, {"heading": "2.2 Synthetic Gradients", "text": "Synthetic gradients are a newer technique designed to deal with the situation of whether a part of a network is \"blocked\" for further use [6], as in the previously discussed case where information must be stored until all gradient calculations are completed. Your way around this problem is to treat the gradient signal as just another function that needs to be approximated, namely a function mapping of an activation vector and any context information on the error gradients of the activation vector. This auxiliary network can be used to calculate an estimate of the gradient before the true gradient is available. This synthetic gradient producing network can be trained in a purely supervised way by comparing its estimates with the true gradient. As shown in Figure 1B, we can apply this approach to NNEMs by updating the embedded network immediately after calculating the gradient with a synthetic gradient."}, {"heading": "2.3 Reinstatement for Credit Assignment", "text": "One way to circumvent the need to record forward gear information is to recalculate forward gear whenever gradient information is available. However, as mentioned above, even the observations are too large to store in our field of interest, preventing a direct recovery of forward gear. Instead, we rely on a learned auto encoder to reinstate the observation and then use the embedding network to recalculate the forward gear. As the recalculated embedding vector probably does not perfectly match the memory stored, it is not obvious how to use the error gradient w.r.t. of the vector in memory. There are two different ways: ignore the discrepancy and use the memory gradient directly, or use the fresh embedding instead of the memory during inference. These two methods are illustrated in Figure 1C and D, and although they are not mutually exclusive, they will be treated as such in order to be helpful during the initial interpretation."}, {"heading": "3 Results", "text": "To test our model, we used the standard MNIST benchmark, the goal of which is to correctly classify images of digits by the integers they represent. While this task is trivial for modern networks with the appropriate architecture, we are using it here because it is one of the simplest tasks that, under certain conditions, could still encounter the previously described problem of lending. Since our goal is to undo the embedding of functions within a network with an external memory, we constructed a simple NNEM consisting of an embedding network and an external memory that holds the embedding of the 5000 most recently encountered images together with their correct labels. Classifications are made by establishing the cosmic similarity between the embedding of the current image and all embedding in the memory."}, {"heading": "4 Future Directions", "text": "The absolute performance of this MNIST task is not useful in itself, as the performance has been deliberately sabotaged to isolate the novel mechanisms under discussion, but while the relative performance changes demonstrate the promise of restoring based lending, the invented domain requires further work to confirm these results on a large scale. One way would be to address amplification issues such as Atari, with a variant of the model-free episodic control network [5] modified so that the embedding function is learned online. While the exact modifications needed to align this attitude with this approach to lending are outside the scope of this paper, it is clear that this domain is one of the few in the current literature that requires long-term episodic storage. In a more complex environment, additional nuances emerge that could improve lending."}], "references": [{"title": "Hybrid computing using a neural network", "author": ["A. Graves", "G. Wayne", "M. Reynolds", "T. Harley", "I. Danihelka", "A. Grabska-Barwi\u0144ska", "A.P. Badia"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory", "author": ["J.L. McClelland", "B.L. McNaughton", "R.C. O\u2019Reilly"], "venue": "Psychological review,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Model-free episodic control.", "author": ["Blundell", "Charles"], "venue": "arXiv preprint arXiv:1606.04460", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Decoupled neural interfaces using synthetic gradients. arXiv preprint arXiv:1608.05343", "author": ["M. Jaderberg", "W.M. Czarnecki", "S. Osindero", "O. Vinyals", "A. Graves", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["S. Hochreiter"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Neural networks with external memories (NNEM),such as the Neural Turing Machine [1] and Memory Networks [2], have often been compared to the human hippocampus in their ability to maintain episodic information over long timescales [3].", "startOffset": 230, "endOffset": 233}, {"referenceID": 1, "context": "It is well known that this sort of long-term episodic storage is vital for overcoming the problem of catastrophic interference [4], which is central to the challenge of continual learning.", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "Indeed, Blundell et al [5] noted that even in the relatively modest Atari domain, storing a few million observations in pixel-space would require upwards of 300 gigabytes1.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "This creates many problems, such as vanishing gradients, that have been well studied [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": "Synthetic gradients are a recent technique created to deal with situation whether a part of a network is \u2018blocked\u2019 from further usage [6], such as in the case previously discussed whereby information must be stored until all gradient calculations are complete.", "startOffset": 134, "endOffset": 137}, {"referenceID": 2, "context": "One possibility would be tackle reinforcement learning problems like Atari, using a variant of the model-free episodic control network [5] modified such that the embedding function is learned on-line.", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "Variational autoencoders have been shown to be more robust [8] and also provide uncertainty estimates that could be useful when deciding which embeddings to discard.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "But in many setups, embeddings are discarded as a function of their usage [5] such that frequently needed embeddings are kept around long enough for staleness to become problematic.", "startOffset": 74, "endOffset": 77}], "year": 2017, "abstractText": "Neural networks with external memories (NNEM),such as the Neural Turing Machine [1] and Memory Networks [2], have often been compared to the human hippocampus in their ability to maintain episodic information over long timescales [3]. But so far, these networks have only been trained on tasks requiring memory storage comparable to a few minutes, whereas the hippocampus stores information on the order of years. It is well known that this sort of long-term episodic storage is vital for overcoming the problem of catastrophic interference [4], which is central to the challenge of continual learning.", "creator": "LaTeX with hyperref package"}}}