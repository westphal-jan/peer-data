{"id": "1701.03868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2017", "title": "Minimally Naturalistic Artificial Intelligence", "abstract": "The rapid advancement of machine learning techniques has re-energized research into general artificial intelligence. While the idea of domain-agnostic meta-learning is appealing, this emerging field must come to terms with its relationship to human cognition and the statistics and structure of the tasks humans perform. The position of this article is that only by aligning our agents' abilities and environments with those of humans do we stand a chance at developing general artificial intelligence (GAI). A broad reading of the famous 'No Free Lunch' theorem is that there is no universally optimal inductive bias or, equivalently, bias-free learning is impossible. This follows from the fact that there are an infinite number of ways to extrapolate data, any of which might be the one used by the data generating environment; an inductive bias prefers some of these extrapolations to others, which lowers performance in environments using these adversarial extrapolations. We may posit that the optimal GAI is the one that maximally exploits the statistics of its environment to create its inductive bias; accepting the fact that this agent is guaranteed to be extremely sub-optimal for some alternative environments. This trade-off appears benign when thinking about the environment as being the physical universe, as performance on any fictive universe is obviously irrelevant. But, we should expect a sharper inductive bias if we further constrain our environment. Indeed, we implicitly do so by defining GAI in terms of accomplishing that humans consider useful. One common version of this is need the for 'common-sense reasoning', which implicitly appeals to the statistics of physical universe as perceived by humans.", "histories": [["v1", "Sat, 14 Jan 2017 01:57:31 GMT  (8kb)", "http://arxiv.org/abs/1701.03868v1", "Accepted into the NIPS 2016 Workshop on Machine Intelligence (M.A.I.N.)"]], "COMMENTS": "Accepted into the NIPS 2016 Workshop on Machine Intelligence (M.A.I.N.)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["steven stenberg hansen"], "accepted": false, "id": "1701.03868"}, "pdf": {"name": "1701.03868.pdf", "metadata": {"source": "CRF", "title": "Minimally Naturalistic Artificial Intelligence", "authors": ["Steven Hansen"], "emails": ["sshansen@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 1.03 868v 1 [cs.A I] 1 4Ja n20 17Minimally Naturalistic Artificial IntelligenceSteven Hansen Department of PsychologyStanford University Stanford, CA 94303sshansen @ stanford.edu"}, {"heading": "1 Introduction", "text": "Although the idea of domain-agnostic meta-learning is appealing, this emerging field needs to revitalize its relationship to human cognition and the statistics and structure of the tasks that humans perform. [1] The position of this article is that we only have a chance to develop general artificial intelligence (GAI) if we align the abilities and environments of our agents with those of humans. [1] A broad reading of the famous \"No Free Lunch\" theory is that there is no universally optimal inductive bias or, accordingly, biased free learning, resulting from the fact that there are an infinite number of ways to extrapolate data, each of which could be the data-generating environment of the universe; an inductive bias favors some of this bias over others, which reduces performance in environments by using these contradictory expressions."}, {"heading": "2 The CommAI Environment", "text": "The CommAI environment is both too broad and too narrow. It is too broad for the tasks not to have to relate to humans, and it is unbound by the statistics it contains. Imagine a CommAI agent trying to follow the mapping in a simulated car. Most of the time, the teacher tries to direct the geometry in the direction of the teacher, but such an agent is dominated by non-Euclidean geometries in the real world."}, {"heading": "3 The Case of Reward Inference", "text": "A concrete case where general and minimally naturalistic approaches to artificial intelligence differ is the problem of inference. While reward is in some ways the simplest feedback, there are many cases where even a reward signal is difficult to achieve [5]. Indeed, social learning seems to be one of these cases [6]. Instead, we must include our agent's motivations in the form of a latent intention that the teacher has in mind but can communicate only indirectly (e.g. sending rewards, verbal instructions).The most common way to deal with such a situation is to take the reward signal for granted and treat other indications of latent intentions as additional state information. This was the approach in Branavan et al. [7] Although they did not address the problem of imprecise rewards, the attempt to take a reward signal for themselves is by treating other indications of latent intentions as additional state information."}], "references": [{"title": "No free lunch theorems for optimization", "author": ["D.H. Wolpert", "W.G. Macready"], "venue": "IEEE transactions on evolutionary computation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "The St. Thomas common sense symposium: designing architectures for human-level intelligence", "author": ["M.L. Minsky", "P. Singh", "A. Sloman"], "venue": "AI Magazine,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "A roadmap towards machine intelligence. arXiv preprint arXiv:1511.08130", "author": ["T. Mikolov", "A. Joulin", "M. Baroni"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "S. Petersen"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning", "author": ["R. Loftin", "B. Peng", "J. MacGlashan", "M.L. Littman", "M.E. Taylor", "J. Huang", "D.L. Roberts"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Help or hinder: Bayesian models of social goal inference. In Advances in neural information processing systems (pp. 1874-1882)", "author": ["T. Ullman", "C. Baker", "O. Macindoe", "O. Evans", "N. Goodman", "J.B. Tenenbaum"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Learning to win by reading manuals in a Monte- Carlo framework. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1 (pp. 268-277)", "author": ["S.R.K. Branavan", "D. Silver", "Barzilay", "June"], "venue": "Association for Computational Linguistics", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "A broad reading of the famous \u201cNo Free Lunch\u201d theorem[1] is that there is no universally optimal inductive bias or, equivalently, bias-free learning is impossible.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "One common version of this is need the for \u201ccommon-sense reasoning\u201d, which implicitly appeals to the statistics of physical universe as perceived by humans [2].", "startOffset": 156, "endOffset": 159}, {"referenceID": 2, "context": "The CommAI environment[3] is both too broad and too narrow.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "The authors attempt to address this criticism by the suggested use of object detection systems to convert image data into a format with a similarity structure akin to linguistic data, but this ignores the fact that much of the recent progress in machine learning has been due to systems that adapt their representations to the ultimate objective function, foregoing the modularization necessitated by an object detection system [4].", "startOffset": 428, "endOffset": 431}, {"referenceID": 4, "context": "While reward is in some sense the simplest possible feedback, there are many cases where even a reward signal is hard to come by[5].", "startOffset": 128, "endOffset": 131}, {"referenceID": 5, "context": "Indeed, social learning appears to be one of these cases[6].", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "This was the approach taken in the work of Branavan et al[7] (though they did not have to handle the issue of imprecise rewards).", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "Not only did this approach predict human performance on this task, related work has shown that this approach is also a computationally viable way to handle imprecise reward signals[6].", "startOffset": 180, "endOffset": 183}], "year": 2017, "abstractText": "The rapid advancement of machine learning techniques has re-energized research into general artificial intelligence. While the idea of domain-agnostic meta-learning is appealing, this emerging field must come to terms with its relationship to human cognition and the statistics and structure of the tasks humans perform. The position of this article is that only by aligning our agents\u2019 abilities and environments with those of humans do we stand a chance at developing general artificial intelligence (GAI).", "creator": "LaTeX with hyperref package"}}}