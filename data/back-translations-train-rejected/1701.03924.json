{"id": "1701.03924", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2017", "title": "QCRI Machine Translation Systems for IWSLT 16", "abstract": "This paper describes QCRI's machine translation systems for the IWSLT 2016 evaluation campaign. We participated in the Arabic-&gt;English and English-&gt;Arabic tracks. We built both Phrase-based and Neural machine translation models, in an effort to probe whether the newly emerged NMT framework surpasses the traditional phrase-based systems in Arabic-English language pairs. We trained a very strong phrase-based system including, a big language model, the Operation Sequence Model, Neural Network Joint Model and Class-based models along with different domain adaptation techniques such as MML filtering, mixture modeling and using fine tuning over NNJM model. However, a Neural MT system, trained by stacking data from different genres through fine-tuning, and applying ensemble over 8 models, beat our very strong phrase-based system by a significant 2 BLEU points margin in Arabic-&gt;English direction. We did not obtain similar gains in the other direction but were still able to outperform the phrase-based system. We also applied system combination on phrase-based and NMT outputs.", "histories": [["v1", "Sat, 14 Jan 2017 14:18:54 GMT  (31kb)", "http://arxiv.org/abs/1701.03924v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nadir durrani", "fahim dalvi", "hassan sajjad", "stephan vogel"], "accepted": false, "id": "1701.03924"}, "pdf": {"name": "1701.03924.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "Stephan Vogel"], "emails": ["ndurrani@qf.org.qa", "faimaduddin@qf.org.qa", "hsajjad@qf.org.qa", "svogel@qf.org.qa"], "sections": [{"heading": null, "text": "ar Xiv: 170 1.03 924v 1 [cs.C L] 14 Jan 2017"}, {"heading": "1. Introduction", "text": "It is up to us to reach an agreement, \"he told the German Press Agency."}, {"heading": "2. Data Settings and Pre/Post Processing", "text": "We trained our systems using data provided by the 2016 IWSLT campaign, which included two indomain data sets TED Talks and QED Corpus [14] and two outdomain data sets UN Corpus [4] and OPUS data [6]. Statistics are shown in Table 1. For the language model, we trained using the landing page of the parallel corpus and all available English data from the recent WMT campaign [15], as well as GigaWord and OPUS monocorpus for Arabic. We segmented Arabic data with both MADAMIRA and Farasa. We found that MADAMIRA [16] performed 0.1 BLEU points better than Farasa [17] (see Table 2) and decided to use them for the competition."}, {"heading": "3. Phrase-based System", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Baseline Settings", "text": "We trained the phrase-based Moses system with the settings described in [18]: a maximum sentence length of 80, FastAligner for text alignment [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNNJM model [23] with the basic settings described in [24]. We used standard distortion limit, 100 best translation options, phase length of 5, monotonous punctuation heuristic, cut cubes with a limit of 1000 during the vote 5000 during the test. We used k-best batch MIRA [25] for voting. We used packaged BLEU [26] to measure progress."}, {"heading": "3.2. Data Selection", "text": "Based on our experience from previous contests, we were skeptical that the mere addition of UN data would be harmful to the AR \u2192 MT system, so we selected data through MML filtering [8]. We selected 2.5%, 3.75%, 5%, 10% and 30% of UN data and trained the MT pipeline by linking the selected data to the in-domain data. We did not include opus data (40 million records) and NNJM in these experiments to get the results quickly. Table 3 shows the results. We found 3.75% (685,000 records) as the optimal threshold. As an alternative to data selection, we tried to train phrase tables in and out-domain phrase tables separately and to use the outdomain phrase table only as a back-off. The second-to-last series of Table 3 shows results. Although it yielded improvements over the base system, it was slightly behind the ML data filtering, and we then tried to select an optimal half of US data (we tried to select 20 million)."}, {"heading": "3.3. Language Model", "text": "We trained a larger language model by using all available English data from the recent WMT campaign1 and the landing page of the parallel data. On each subcorpus, a smoothed 5 gram Kneser-Ney language model was individually trained and then interpolated to minimize confusion in the target part of the monolingual data. We achieved a gain of + 0.5 with a larger language model. See Table 4."}, {"heading": "3.4. Interpolation of Operation Sequence Models", "text": "The OSM model is a regular feature of the phrase-based pipeline [27] in competition assessment systems. It is a common sequence translation model that integrates reordering. [10] Recently, it has been found that an OSM model trained in simple data concatenation is suboptimal and can be improved by individually training and interpolating OSM models in each domain by minimizing perplexity in the domain tune. Table 5 shows that using the interpolated OSM model (OSMi) instead of the OSMc model yields an average improvement of + 0.6 BLEU points."}, {"heading": "3.5. NNJM Adaptation", "text": "We also studied the award-winning Neural Network Joint Model (NNJM) in our pipeline and tried to adapt it to in-domain data. We trained an NNJM model on the UN and opus data for 25 epochs and then refined it using 1http: / / www.statmt.org / wmt16 / translation-task.htmlrunning for 25 more epochs on the in-domain data. As the data is huge, the entire training took 1.5 months of clock time. Table 6 shows results. The NNJM model brought significant improvements (+ 0.6) above the baseline in which it is not included."}, {"heading": "3.6. Class-based Models", "text": "We investigated the use of automatic word clusters in phrase-based models [11], using 50 classes gained by executing mkcls, including cluster IDs in the phrase table, and training the domain-internal language model with word classes and interpolated OSM with word classes, but we found very little improvement with word classes."}, {"heading": "3.7. Handling Unknown Words", "text": "We tried to deal with OOV words using drop-oov and transliteration [28, 29], the former worked a bit better and was used in the best system. Of course, the advantages of the two methods are additive, since they address different OOVs, but there is no good way to automatically figure out which word to drop and which to transliterate."}, {"heading": "3.8. Final System", "text": "Our best system included MML selected UN and Opus corpora, large language model, interpolated OSM, and fine-tuned NNJM models. We used drop-oov option to deal with unknown words."}, {"heading": "3.9. English-to-Arabic Systems", "text": "The base system (ID) was trained on the TED data and destination side of all permitted parallel data. In the second row, we added all parallel data except the UN. In the third row, we added the UN data we had selected in Arabic \u2192 English. Additional parallel data gives an average improvement of + 1.4 BLEU points. Then, we added an NNJM model trained on TED data in the domain to improve it by + 0.8. Adding GigaWord and monolingual OPUS data (another 20M sets outside the destination side of the parallel data) resulted in an improvement of + 0.3. Finally, we replaced the base model NNJM with the in-domain data trained and refined on OPUS data to get our best system."}, {"heading": "3.10. QED Systems", "text": "We simply replicated QED systems by replacing QED corpus with in-domain data instead of TED data. We used the same UN data we selected for our Arabic \u2192 English system, so our phrase tables remain the same. The most important changes occur when we train customized OSM and NNJM models. For NNJM, we simply refined the QED corpus instead of the TED corpus. For interpolated OSM, we combined TED and QED corpus and built OSM on it, which is then interpolated with the OSM models trained on the selected UN and Opus data. We used IWSLT tuning to get the interpolation weights. In this way, the OSM Sub Modelsystem customized-11-12 customized-13-14 Avg 30% FT + (TED) tuning to get the interpolation weights. In this way, the OSM Sub Modelsystem customized-13 customized-14 Avg 30% FT + (TED) tuned-1.4 + 330.8 models are best, we have the TED models."}, {"heading": "4. Neural Machine Translation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Pre/Post-processing", "text": "We used a similar pre- / post-processing pipeline for Neural MT as our phrase-based systems (Section 2), and additionally applied BPE [30] before we trained them. Our BPE models are trained separately for both Arabic and English records, rather than training them together because the character set differs between languages. We limited the number of operations to 59,500, as proposed in [30]. We experimented with BPE models trained on the TED data and the concatenation of TED and out-domain data. We saw no significant performance difference between these models, so we used the BPE model trained on the TED data for the experiments reported in this paper."}, {"heading": "4.2. Baseline", "text": "We used standard parameters in Nematus to train our systems: a stack size of 80, source and target vocabulary with 50K entries each, 1024 LSTM units and the embedding layer size of 500. Base systems were trained exclusively with TED corpus."}, {"heading": "4.3. Fine Tuning on Concatenation versus OD", "text": "We experimented with both strategies. In the interest of time, we selected 30% of the UN data using MML filtering (Table 3). We trained two systems, one by linking the in-domain data with the selected (30%) UN data and the other only on the selected data. We then refined both models with the in-domain TED data after running them for three eras. Table 11 shows that fine-tuning a system trained only on out-domain data outperforms the system designed for concatenation."}, {"heading": "4.4. Fine-tuning Variants and Dropouts", "text": "The intuition behind freezing a layer is not to let the weights in that layer change with additional data, which is sometimes useful when we can learn certain layers better from external data. A single layer in our case is the word embedding layer. We tried a variant where we do not freeze a layer, the latter variant exceeds the default setting (see Table 12). Dropouts are useful in NN training when the training data is small. We experimented with using dropouts in our experiments, but found no significant difference, so we decided to use them only in fine-tuning with in-domain data (TED / QED), as the other two data sets (UN and OPUS) were large and did not pose a risk of causing the problem of overadjustment."}, {"heading": "4.5. Data Selection", "text": "Since we found the data selection in the phrase-based system useful, we also trained our neural systems on 5%, 30% and 100% of the UN data. In these experiments, we combined the 5% and 30% of the UN data with the in-domain data. To evaluate the most promising models, we trained all models until the learning plateau was reached, and then refined these models with in-domain data. 2 The results are in Table 13. Using only 5% of the data proved harmful, and the system did not generalize as well as the other models. The model trained on 30% of the data better than the model trained on all the data by 0.7 BLEU points. In our subsequent experiments, we tried to verify whether this finding is true when we add the OPUS data. We therefore trained two systems by refining 30% of the selected UN data or the complete UN data using OPUS."}, {"heading": "4.6. Ensemble", "text": "We therefore experimented with several variations. we found best2Because we conducted experiments in parallel, we were not aware at that time that fine-tuning outside the range is a better strategic performance combination by fine-tuning the last eight models of the UN + OPUS system and then merging these eight fine-tuned models. Ensemble performance improvements are shown in Table 14. The second row shows systems when fine-tuning our best system in Table 13 with the domain's internal TED data. In the last row, we perform ensembles."}, {"heading": "4.7. Final System", "text": "After learning based on the OPUS data, we took the last eight models, which were very similar in performance, and refined each of them using TED data. We then combined these eight finely tuned models into an overall ensemble to form our final system. Progress is shown in Table 15. We used the same strategy for the QED systems by refining the last eight OPUS models with QED data and combining them into an overall ensemble."}, {"heading": "4.8. English-to-Arabic Systems", "text": "We used insights from our Arabic-English system experiments to train our English \u2192 Arabic systems. Our final model for TED and QED was first trained on all UN data, followed by the OPUS data, and finally refined with the In-Domain data. Progress is shown in Table 16."}, {"heading": "5. System Combination", "text": "We combined hypotheses derived from our best Phrase-based and Neural MT systems, using the MultiEngine MT System, or MEMT [13]. Results are shown in Table 17. We have not made any significant improvements by system combinations. Minor improvements were made in the Arabic-English Direction Test 2012, whereas significant improvements were made only in the English-Arabic Direction Test 2013. Table 18 shows results on the official test kits."}, {"heading": "6. Summary", "text": "We trained a very strong phrase system with SOTA features such as OSM, NNJM, and Big LM. The system improved greatly through the use of domain adaptations. To this end, we applied MML-based filtering, interpolated OSM, and fine-tuning of NNNJM models. Overall, our phrase system gained 4 BLEU points over the base system. We also applied data selection to train our NMT. However, the NMT systems were quickly overhauled and did not perform well. Our experiments showed that the NMT system was best trained on the full UN data, and the final NMT system used all available out-of-domain data. However, the training was done step by step, starting with UN data for 50k iterations, fine-tuning OPUS for 25k more iterations, and then fine-tuning the final model using TED conversations for a few iterations."}, {"heading": "7. References", "text": "[1] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst, \"Moses: Open source toolkit for statistical machine translation,\" in Proceedings of the Association for Computational Linguistics (ACL '07), Prague, Czech Republic, 2007. [2] D. Bahdanau, K. Cho, and Y. Bengio, \"Neural machine translation by align and translate to align and translate,\" in ICLR, 2015. Available: http: / / arxiv.org / pdf / 1409.0473v6.pdf [3] R. Senningurich, B. Haddow, and A. Birch, \"Edinburgh neural machine translation systems for wmt 16.\""}], "references": [{"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of the Association for Computational Linguistics (ACL\u201907), Prague, Czech Republic, 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR, 2015. [Online]. Available: http://arxiv.org/pdf/1409.0473v6.pdf", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Edinburgh neural machine translation systems for wmt 16", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "Proceedings of the First Conference on Machine Translation. Berlin, Germany: Association for Computational Linguistics, August 2016, pp. 371\u2013376. [Online]. Available: http://www.aclweb.org/anthology/W16-2323", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "The united nations parallel corpus v1.0", "author": ["M. Ziemski", "M. Junczys-Dowmunt", "B. Pouliquen"], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation LREC 2016, Portoro\u017e, Slovenia, May 23-28, 2016, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "QCRI at IWSLT 2013: Experiments in Arabic-English and English-Arabic spoken language translation", "author": ["H. Sajjad", "F. Guzmn", "P. Nakov", "A. Abdelali", "K. Murray", "F.A. Obaidli", "S. Vogel"], "venue": "Proceedings of the 10th International Workshop on Spoken Language Technology (IWSLT-13), December 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles", "author": ["P. Lison", "J. Tiedemann"], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA), may 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "The AMARA corpus: Building parallel language resources for the educational domain", "author": ["A. Abdelali", "F. Guzman", "H. Sajjad", "S. Vogel"], "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914), Reykjavik, Iceland, May 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain adaptation via pseudo in-domain data selection", "author": ["A. Axelrod", "X. He", "J. Gao"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, ser. EMNLP \u201911, Edinburgh, United Kingdom, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "A joint sequence translation model with integrated reordering", "author": ["N. Durrani", "H. Schmid", "A. Fraser"], "venue": "Proceedings of the Association for Computational  Linguistics: Human Language Technologies (ACL- HLT\u201911), Portland, OR, USA, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Using joint models for domain adaptation in statistical machine translation", "author": ["N. Durrani", "H. Sajjad", "S. Joty", "A. Abdelali", "S. Vogel"], "venue": "Proceedings of the Fifteenth Machine Translation Summit (MT Summit XV). Florida, USA: AMTA, November 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Investigating the usefulness of generalized word representations in smt", "author": ["N. Durrani", "P. Koehn", "H. Schmid", "A. Fraser"], "venue": "Proceedings of the 25th Annual Conference on Computational Linguistics, ser. COLING\u201914, Dublin, Ireland, 2014, pp. 421\u2013432.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Stanford neural machine translation systems for spoken language domain", "author": ["M.-T. Luong", "C.D. Manning"], "venue": "International Workshop on Spoken Language Translation, Da Nang, Vietnam, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "CMU system combination in WMT 2011", "author": ["K. Heafield", "A. Lavie"], "venue": "Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, Edinburgh, Scotland, United Kingdom, July 2011, pp. 145\u2013151. [Online]. Available: http://kheafield.com/professional/avenue/wmt 2011.pdf", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "The AMARA corpus: Building resources for translating the web\u2019s educational content", "author": ["F. Guzm\u00e1n", "H. Sajjad", "S. Vogel", "A. Abdelali"], "venue": "Proceedings of the 10th International Workshop on Spoken Language Technology (IWSLT-13), December 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Findings of the 2016 conference on machine translation", "author": ["O. Bojar", "R. Chatterjee", "C. Federmann", "Y. Graham", "B. Haddow", "M. Huck", "A. Jimeno Yepes", "P. Koehn", "V. Logacheva", "C. Monz", "M. Negri", "A. Neveol", "M. Neves", "M. Popel", "M. Post", "R. Rubino", "C. Scarton", "L. Specia", "M. Turchi", "K. Verspoor", "M. Zampieri"], "venue": "Proceedings of the First Conference on Machine Translation. Berlin, Germany: Association for Computational Linguistics, August 2016, pp. 131\u2013198. [Online]. Available: http://www.aclweb.org/anthology/W/W16/W16-2301", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "MADAMIRA: A fast, comprehensive tool for morphological analysis and disambiguation of Arabic", "author": ["A. Pasha", "M. Al-Badrashiny", "M. Diab", "A. El Kholy", "R. Eskander", "N. Habash", "M. Pooleery", "O. Rambow", "R.M. Roth"], "venue": "Proceedings of the Language Resources and Evaluation Conference, ser. LREC \u201914, Reykjavik, Iceland, 2014, pp. 1094\u20131101.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Farasa: A fast and furious segmenter for arabic", "author": ["A. Abdelali", "K. Darwish", "N. Durrani", "H. Mubarak"], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations. San  Diego, California: Association for Computational Linguistics, June 2016, pp. 11\u201316. [Online]. Available: http://www.aclweb.org/anthology/N16-3003", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Edinburgh SLT and MT system description for the IWSLT 2014 evaluation", "author": ["A. Birch", "M. Huck", "N. Durrani", "N. Bogoychev", "P. Koehn"], "venue": "Proceedings of the 11th International Workshop on Spoken Language Translation, ser. IWSLT \u201914, Lake Tahoe, CA, USA, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["C. Dyer", "V. Chahuneau", "N.A. Smith"], "venue": "Proceedings of NAACL\u201913, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "KenLM: Faster and Smaller Language Model Queries", "author": ["K. Heafield"], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, Scotland, United Kingdom, July 2011, pp. 187\u2013197. [Online]. Available: http://kheafield.com/professional/avenue/kenlm.pdf", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "A Simple and Effective Hierarchical Phrase Reordering Model", "author": ["M. Galley", "C.D. Manning"], "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Honolulu, Hawaii, October 2008, pp. 848\u2013856. [Online]. Available: http://www.aclweb.org/anthology/D08-1089", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "The Operation Sequence Model \u2013 Combining N-Gram-based and Phrase-based Statistical Machine Translation", "author": ["N. Durrani", "H. Schmid", "A. Fraser", "P. Koehn", "H. Sch\u00fctze"], "venue": "Computational Linguistics, vol. 41, no. 2, pp. 157\u2013186, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "How to Avoid Unwanted Pregnancies: Domain Adaptation using Neural Network Models", "author": ["S. Joty", "H. Sajjad", "N. Durrani", "K. Al-Mannai", "A. Abdelali", "S. Vogel"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, September 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["C. Cherry", "G. Foster"], "venue": "Proceedings of the 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, ser. NAACL-HLT \u201912, Montr\u00e9al, Canada, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th Annual  Meeting on Association for Computational Linguistics, ser. ACL \u201902, Morristown, NJ, USA, 2002, pp. 311\u2013 318.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "Can markov models over minimal translation units help phrase-based smt?\u201d in Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "author": ["N. Durrani", "A. Fraser", "H. Schmid", "H. Hoang", "P. Koehn"], "venue": "Sofia, Bulgaria: Association for Computational Linguistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "A statistical model for unsupervised and semi-supervised transliteration mining", "author": ["H. Sajjad", "A. Fraser", "H. Schmid"], "venue": "Proceedings of the Association for Computational Linguistics (ACL\u201912), Jeju, Korea, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Integrating an unsupervised transliteration model into statistical machine translation", "author": ["N. Durrani", "H. Sajjad", "H. Hoang", "P. Koehn"], "venue": "Proceedings of the 15th Conference of the European Chapter of the ACL (EACL 2014), Gothenburg, Sweden, April 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation of rare words with subword units", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Berlin, Germany: Association for Computational Linguistics, August 2016, pp. 1715\u20131725. [Online]. Available: http://www.aclweb.org/anthology/P16-1162", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Our translation engines have been historically based on the phrase-based system trained using the Moses toolkit [1], but during the course of this evaluation, we made a transition towards the newly emerged Neural MT framework [2], using Nematus, a toolkit used by the top performing team [3], during the recent WMT campaign.", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "Our translation engines have been historically based on the phrase-based system trained using the Moses toolkit [1], but during the course of this evaluation, we made a transition towards the newly emerged Neural MT framework [2], using Nematus, a toolkit used by the top performing team [3], during the recent WMT campaign.", "startOffset": 226, "endOffset": 229}, {"referenceID": 2, "context": "Our translation engines have been historically based on the phrase-based system trained using the Moses toolkit [1], but during the course of this evaluation, we made a transition towards the newly emerged Neural MT framework [2], using Nematus, a toolkit used by the top performing team [3], during the recent WMT campaign.", "startOffset": 288, "endOffset": 291}, {"referenceID": 3, "context": "The in-domain data based on TED talks is available in very little quantity compared to the out-domain UN corpus [4], which has been found to be harmful previously when simply concatenated to the training [5].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "The in-domain data based on TED talks is available in very little quantity compared to the out-domain UN corpus [4], which has been found to be harmful previously when simply concatenated to the training [5].", "startOffset": 204, "endOffset": 207}, {"referenceID": 5, "context": "In this year\u2019s IWSLT, two additional data resources Opus subtitles [6] and the QED corpus [7] were introduced.", "startOffset": 67, "endOffset": 70}, {"referenceID": 6, "context": "In this year\u2019s IWSLT, two additional data resources Opus subtitles [6] and the QED corpus [7] were introduced.", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "\u2022 We applied MML-based data selection [8] to the UN and Open Sub-title data, with the goals of filtering out harmful data.", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "\u2022 We trained OSM models [9] on separate corpora, and interpolated them [10] by optimizing perplexity on the tuning-set.", "startOffset": 24, "endOffset": 27}, {"referenceID": 9, "context": "\u2022 We trained OSM models [9] on separate corpora, and interpolated them [10] by optimizing perplexity on the tuning-set.", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "We also tried this on the OSM models trained on the word classes [11].", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "\u2022 We tried the fine-tuning method of training the NNJM model on the out-domain data and fine-tuning with the in-domain TED data [12].", "startOffset": 128, "endOffset": 132}, {"referenceID": 12, "context": "Finally we applied system combination over the outputs of best Neural MT and phrase-based systems using MEMT [13].", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "This contained two indomain data sets TED talks and QED corpus [14] and two out-domain data sets UN corpus [4] and OPUS data [6].", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "This contained two indomain data sets TED talks and QED corpus [14] and two out-domain data sets UN corpus [4] and OPUS data [6].", "startOffset": 107, "endOffset": 110}, {"referenceID": 5, "context": "This contained two indomain data sets TED talks and QED corpus [14] and two out-domain data sets UN corpus [4] and OPUS data [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 14, "context": "For language model we trained using the target side of the parallel corpus and all the available English data from the recent WMT campaign [15], and GigaWord and OPUS mono corpus for Arabic.", "startOffset": 139, "endOffset": 143}, {"referenceID": 15, "context": "We found MADAMIRA [16] performed 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 16, "context": "1 BLEU points better than Farasa [17] (See Table 2) and decided to use it for the competition.", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": "Before scoring the output, we normalized them and reference translations using the QCRI normalizer [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 17, "context": "We trained phrase-based Moses system, with the settings described in [18]: a maximum sentence length of 80, FastAligner for word-alignments [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNJM model [23], with the baseline settings described in [24].", "startOffset": 69, "endOffset": 73}, {"referenceID": 18, "context": "We trained phrase-based Moses system, with the settings described in [18]: a maximum sentence length of 80, FastAligner for word-alignments [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNJM model [23], with the baseline settings described in [24].", "startOffset": 140, "endOffset": 144}, {"referenceID": 19, "context": "We trained phrase-based Moses system, with the settings described in [18]: a maximum sentence length of 80, FastAligner for word-alignments [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNJM model [23], with the baseline settings described in [24].", "startOffset": 203, "endOffset": 207}, {"referenceID": 20, "context": "We trained phrase-based Moses system, with the settings described in [18]: a maximum sentence length of 80, FastAligner for word-alignments [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNJM model [23], with the baseline settings described in [24].", "startOffset": 238, "endOffset": 242}, {"referenceID": 21, "context": "We trained phrase-based Moses system, with the settings described in [18]: a maximum sentence length of 80, FastAligner for word-alignments [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNJM model [23], with the baseline settings described in [24].", "startOffset": 278, "endOffset": 282}, {"referenceID": 22, "context": "We trained phrase-based Moses system, with the settings described in [18]: a maximum sentence length of 80, FastAligner for word-alignments [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNJM model [23], with the baseline settings described in [24].", "startOffset": 305, "endOffset": 309}, {"referenceID": 23, "context": "We trained phrase-based Moses system, with the settings described in [18]: a maximum sentence length of 80, FastAligner for word-alignments [19], an interpolated KneserNey smoothed 5-gram language model [20], lexicalized reordering model [21], a 5-gram operation sequence model [22], a 14-gram NNJM model [23], with the baseline settings described in [24].", "startOffset": 351, "endOffset": 355}, {"referenceID": 24, "context": "We used k-best batch MIRA [25] for tuning.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "We used cased BLEU [26] to measure progress.", "startOffset": 19, "endOffset": 23}, {"referenceID": 7, "context": "Due to our experience from previous competitions, we were wary of the fact that simply adding the UN data is harmful for the AR\u2192 MT system, we therefore selected data through MML filtering [8].", "startOffset": 189, "endOffset": 192}, {"referenceID": 26, "context": "The OSM model has been a regular feature of the phrasebased pipeline [27] in the competition grade systems.", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "[10] recently found that an OSM model trained on plain concatenation of data is sub-optimal and can be improved by training OSM models on each domain individually and interpolating them by minimizing perplexity on the in-domain tune-set.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "We trained an NNJM models on the UN and Opus data for 25 epochs and then fine-tuned [12] it by", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "We explored the use of automatic word clusters in phrasebased models [11].", "startOffset": 69, "endOffset": 73}, {"referenceID": 27, "context": "We tried to handle OOV words using drop-oov and through transliteration [28, 29].", "startOffset": 72, "endOffset": 80}, {"referenceID": 28, "context": "We tried to handle OOV words using drop-oov and through transliteration [28, 29].", "startOffset": 72, "endOffset": 80}, {"referenceID": 29, "context": "We used a similar pre/post-processing pipeline for Neural MT as our phrase-based systems (Section 2), and additionally applied BPE [30] before training them.", "startOffset": 131, "endOffset": 135}, {"referenceID": 29, "context": "We limited the number of operations to 59,500, as suggested in [30].", "startOffset": 63, "endOffset": 67}, {"referenceID": 2, "context": "Ensembling models has shown to give a consistent boost in performance in past best performing systems [3].", "startOffset": 102, "endOffset": 105}, {"referenceID": 12, "context": "For this purpose we used MultiEngine MT system, or MEMT [13].", "startOffset": 56, "endOffset": 60}], "year": 2017, "abstractText": "This paper describes QCRI\u2019s machine translation systems for the IWSLT 2016 evaluation campaign. We participated in the Arabic\u2192English and English\u2192Arabic tracks. We built both Phrase-based and Neural machine translation models, in an effort to probe whether the newly emerged NMT framework surpasses the traditional phrase-based systems in Arabic-English language pairs. We trained a very strong phrase-based system including, a big language model, the Operation Sequence Model, Neural Network Joint Model and Class-based models along with different domain adaptation techniques such as MML filtering, mixture modeling and using fine tuning over NNJM model. However, a Neural MT system, trained by stacking data from different genres through fine-tuning, and applying ensemble over 8 models, beat our very strong phrase-based system by a significant 2 BLEU points margin in Arabic\u2192English direction. We did not obtain similar gains in the other direction but were still able to outperform the phrase-based system. We also applied system combination on phrase-based and NMT outputs.", "creator": "LaTeX with hyperref package"}}}