{"id": "1701.03937", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2017", "title": "Hedera: Scalable Indexing and Exploring Entities in Wikipedia Revision History", "abstract": "Much of work in semantic web relying on Wikipedia as the main source of knowledge often work on static snapshots of the dataset. The full history of Wikipedia revisions, while contains much more useful information, is still difficult to access due to its exceptional volume. To enable further research on this collection, we developed a tool, named Hedera, that efficiently extracts semantic information from Wikipedia revision history datasets. Hedera exploits Map-Reduce paradigm to achieve rapid extraction, it is able to handle one entire Wikipedia articles revision history within a day in a medium-scale cluster, and supports flexible data structures for various kinds of semantic web study.", "histories": [["v1", "Sat, 14 Jan 2017 15:47:06 GMT  (456kb,D)", "http://arxiv.org/abs/1701.03937v1", "Pubished via CEUR-WS.org/Vol-1272"]], "COMMENTS": "Pubished via CEUR-WS.org/Vol-1272", "reviews": [], "SUBJECTS": "cs.AI cs.IR", "authors": ["tuan tran", "tu ngoc nguyen"], "accepted": false, "id": "1701.03937"}, "pdf": {"name": "1701.03937.pdf", "metadata": {"source": "CRF", "title": "Hedera: Scalable Indexing and Exploring Entities in Wikipedia Revision History", "authors": ["Tuan Tran", "Ngoc Nguyen"], "emails": ["tunguyen}@L3S.de"], "sections": [{"heading": "1 Introduction", "text": "Over the decades, Wikipedia has become the backbone of sematic web research, with the proliferation of high-quality Big Knowledge Base (KBs) such as DBpedia [1], where information is derived from various public Wikipedia collections. Existing approaches often rely on an offline snapshot of data sets, treat knowledge as static, and ignore the temporal evolution of information in Wikipedia. For example, when a fact changes (e.g. the death of a celebrity) or entities themselves develop, they can only be reflected in the next version of the Knowledge Base (typically freshly extracted from a recent Wikipedia dump). This unwanted quality of KBs renders them unable to capture time dynamic relationships that are latent between the revisions of the Encyclopedia (e.g. participating in complex events) that are difficult to detect in a single Wikipedia snapshot."}, {"heading": "2 Extracting and Indexing Entities", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Preprocessing Dataset", "text": "Here we describe the Hedera architecture and workflow. As shown in Figure 1, Hedera core data entry is a Wikipedia revision dump 2. Hedera currently works with the raw XML dumps and supports access and extraction of information directly from compressed files. Hedera heavily uses the Hadoop framework. The preprocessor is responsible for repartitioning the raw files into independent units (also known as InputSplit in Hadoop) as required by the user. There are two levels of partitioning: Entity-wise and Document-wise. Entity-wise partitioning guarantees that revisions of the same unit are sent to an arithmetic node, while document-by-document content from revisions is arbitrarily sent to any node, and keeps references to its previous revisions in each revision for future use in the map reduction level in mind. The pre-processor accepts low-level filters defined by the 2012 article (for example, if the article needs to be limited to low-level maps)."}, {"heading": "2.2 Extracting Information", "text": "The main goal of the transformer is to consume the files and output (key, value) pairs suitable for input into a card function. Hedera provides several classes of transformers, each of which implements one operator specified in extraction1. Documentation and code of the project can be found at: https: / / github.com / antoine-tran / Hedera 2 http: / / dumps.wikimedia.orglayer. When these operators are pressed into transformers, the text volume sent over the network decreases. The extraction layer allows the user to write high-level extraction logic in programming languages such as Java or Pig 3 that can be used in other applications. The extraction layer also accepts user-defined filters, allowing the user to extract and index different parts of the same time."}, {"heading": "3 Indexing and Exploring Entity-based Evolutions in Wikipedia", "text": "In this section, we will illustrate the use of Hedera in an application - incremental indexing and visualization of Wikipedia revision history 01. Indexing large-scale longitudinal data collections, i.e. Wikipedia history is not an easy problem. Challenges in finding a scalable data structure and distributed storage that can exploit most of the data along the time dimension are still not fully addressed. In Hedera, we present a distributed approach in which the collection is processed, and then indexing is paralleled with the Wikipedia reduction paradigm. This approach (based on the documentary data structure of ElasticSearch) can be seen as the basis for further optimizations. The schema of the index is loosely structured, allowing for flexible updating and gradual indexing of new revisions."}, {"heading": "4 Conclusions and Future Work", "text": "In this post, we introduced Hedera, our ongoing work to support flexible and efficient access to the Wikipedia revision history dataset. Hedera can work directly with low-level raw data, it uses Map-Reduce to achieve high-performance computing, we open-source Hedera for future use in research communities and believe our system is the first of its kind. Future work will include deeper integration with knowledge databases, with more API and services to access the extraction layer more flexibly."}], "references": [{"title": "DBpedia: A nucleus for a web of open data", "author": ["S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z. Ives"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Wikipedia revision toolkit: efficiently accessing wikipedia\u2019s edit history", "author": ["O. Ferschke", "T. Zesch", "I. Gurevych"], "venue": "In HLT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "For over decades, Wikipedia has become a backbone of sematic Web research, with the proliferation of high-quality big knowledge bases (KBs) such as DBpedia [1], where information is derived from various Wikipedia public collections.", "startOffset": 156, "endOffset": 159}, {"referenceID": 1, "context": "In contrast to existing work that handle the dataset in centralized settings [2], Hedera employs the MapReduce paradigm to achieve the scalable performance, which is able to transfer raw data of 2.", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "Our preliminary evaluation showed that this approach outperformed the well-known centralized indexing method provided by [2].", "startOffset": 121, "endOffset": 124}], "year": 2017, "abstractText": "Much of work in semantic web relying on Wikipedia as the main source of knowledge often work on static snapshots of the dataset. The full history of Wikipedia revisions, while contains much more useful information, is still difficult to access due to its exceptional volume. To enable further research on this collection, we developed a tool, named Hedera, that efficiently extracts semantic information from Wikipedia revision history datasets. Hedera exploits Map-Reduce paradigm to achieve rapid extraction, it is able to handle one entire Wikipedia articles\u2019 revision history within a day in a medium-scale cluster, and supports flexible data structures for various kinds of semantic web study.", "creator": "LaTeX with hyperref package"}}}