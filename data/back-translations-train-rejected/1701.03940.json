{"id": "1701.03940", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2017", "title": "Scalable and Incremental Learning of Gaussian Mixture Models", "abstract": "This work presents a fast and scalable algorithm for incremental learning of Gaussian mixture models. By performing rank-one updates on its precision matrices and determinants, its asymptotic time complexity is of \\BigO{NKD^2} for $N$ data points, $K$ Gaussian components and $D$ dimensions. The resulting algorithm can be applied to high dimensional tasks, and this is confirmed by applying it to the classification datasets MNIST and CIFAR-10. Additionally, in order to show the algorithm's applicability to function approximation and control tasks, it is applied to three reinforcement learning tasks and its data-efficiency is evaluated.", "histories": [["v1", "Sat, 14 Jan 2017 16:15:44 GMT  (49kb,D)", "http://arxiv.org/abs/1701.03940v1", "13 pages, 1 figure, submitted for peer-review. arXiv admin note: substantial text overlap witharXiv:1506.04422"]], "COMMENTS": "13 pages, 1 figure, submitted for peer-review. arXiv admin note: substantial text overlap witharXiv:1506.04422", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rafael pinto", "paulo engel"], "accepted": false, "id": "1701.03940"}, "pdf": {"name": "1701.03940.pdf", "metadata": {"source": "CRF", "title": "Scalable and Incremental Learning of Gaussian Mixture Models", "authors": ["Rafael Coimbra Pinto", "Paulo Martins Engel"], "emails": ["rcpinto@inf.ufrgs.br", "engel@inf.ufrgs.br"], "sections": [{"heading": null, "text": "(NKD2) for N data points, KGaussian components and D dimensions. The resulting algorithm can be applied to high-dimensional tasks, and this is demonstrated by applying it to the classification data sets MNIST and CIFAR-10. In addition, it is applied to three reinforcing learning tasks and its data efficiency is evaluated to demonstrate the applicability of the algorithm to functional approximation and control tasks. Keywords Gaussian blending models \u00b7 Incremental learning"}, {"heading": "1 Introduction", "text": "It is a question of whether and to what extent it is a matter of a way in which the dissemination of information that has been used in the past is at all possible in the future. It is a question of whether and to what extent the dissemination of information is at all possible in the future. It is a question of whether the dissemination of information is at all possible in the future. It is a question of the extent to which the dissemination of information is at all possible in the future. It is a question of the extent to which the dissemination of information is at all possible in the future. It is a question of the extent to which the dissemination of information is at all possible in the future. It is a question of the extent to which the dissemination of data is at all possible in the future. It is a question of the extent to which the dissemination of information is at all possible in the future. It is a question of the extent to which the dissemination of information is at all possible in the future."}, {"heading": "2 Incremental Gaussian Mixture Network", "text": "In the next subsections we will describe the IGMN algorithm, a slightly improved version of the (16].2.1 LearningThe algorithm starts with no components, which are created as necessary (see sub-section 2.2). Given the input x (a single instantaneous data point), the IGMN algorithm processing step is as follows: First, the squared Mahalanobis distance d2 (x, j) for each component j is calculated: d2M (x, j) = (x \u2212 \u00b5j) T\u03a3 \u2212 1j (x \u2212 \u00b5j) (1), where \u00b5j is the jest component mean, and the complete covariance matrix. If any d 2 (x, j) is smaller than any other component 2D, 1 \u2212 \u03b2 (the 1 \u2212 \u03b2 percentile of a chi-squared distribution with D degrees-of freedom, where D is the input dimensionality and \u03b2 is defined user a, e.g. a shift or 0.1)."}, {"heading": "3 Fast IGMN", "text": "This section introduces the more scalable version of the IGMN algorithm, the Fast Incremental Gaussian Mixture Network (FIGMN), which is an improvement over the version presented in [28]. The main problem with the IGMN algorithm in terms of computational complexity lies in the fact that Equation 1 (square distance) requires a matrix inversion that has an asymptotic time complexity of O (D3), for D dimensions (O (Dlog27 + O (1))) for the street algorithms, or for bestO (D2.3728639) with the most up-to-date algorithms [24], which makes the entire IGMN algorithms impractical for high-dimensional tasks. Here we show how we work directly with the inversion of the covariance matrix (also referred to as precision or concentration) for the entire process."}, {"heading": "4 Experiments", "text": "The first experiment was meant to verify that both IGMN implementations produce exactly the same results, and the exact quantity for each package is shown. They were both applied to 7 standard datasets distributed with the Weka software (Table 1). Parameters were tested in 3 different ways. (The same parameters were used for all datasets.) Results were obtained from 10x cross-validation (resulting in training sets with 90% of the data and test sets with the remaining 10%) and statistical meanings came from paired t-tests with p = 0.05. As in Table 2, IGMN and FIGMN algorithms produced exactly the results, our expectations were confirmed. The number of clusters they created was also the exact quantity of the quantity."}, {"heading": "5 Conclusion", "text": "We have shown how to work directly with precision matrices in the IGMN algorithm and avoid costly matrix inversions by performing rank-one updates. The determining calculations have also been avoided using a similar method, eliminating virtually any source of cubic complexity from the learning algorithm. While previous work used two rank-one updates for covariance matrices and determinants, this work shows how such updates can be performed with single rank-one operations. These improvements have resulted in significant acceleration for high-dimensional datasets, making the IGMN a good option for this type of tasks. While the inference operation still has cubic complexity, we argue that it has a much smaller impact on the overall run-time of the algorithm, as the number of outputs is usually much lower than the number of inputs. This has been confirmed in the experiments to show that high-speed data amplification algorithms are also useful for achieving high-level FIX inputs."}], "references": [{"title": "Using a Gaussian mixture neural network for incremental learning and robotics", "author": ["MR Heinen", "PM Engel", "RC. Pinto"], "venue": "Neural Networks (IJCNN), The 2012 International Joint Conference on", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "An incremental gaussian mixture network that learns instantaneously from data flows", "author": ["MR Heinen", "PM Engel", "IGMN Pinto RC."], "venue": "Proc VIII Encontro Nacional de Intelig\u00eancia Artificial", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["AP Dempster", "NM Laird", "DB Rubin"], "venue": "Journal of the Royal Statistical Society Series B (Methodological)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1977}, {"title": "Incremental learning of multivariate gaussian mixture models", "author": ["P Engel", "M. Heinen"], "venue": "Advances in Artificial Intelligence\u2013SBIA", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Competitive learning: From interactive activation to adaptive resonance", "author": ["S. Grossberg"], "venue": "Cognitive science", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1987}, {"title": "A connectionist approach for incremental function approximation and on-line tasks", "author": ["Heinen MR"], "venue": "Universidade Federal do Rio Grande do Sul;", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Parallel distributed processing", "author": ["Rumelhart DE", "McClelland JL"], "venue": "MIT Pr.;", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1986}, {"title": "Supervised learning from incomplete data via an EM approach. In: Advances in neural information processing systems 6. Citeseer", "author": ["Ghahramani Z", "Jordan MI"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "An online algorithm for simultaneously learning forward and inverse kinematics", "author": ["B Damas", "J. Santos-Victor"], "venue": "Intelligent Robots and Systems (IROS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "IGMN: An incremental connectionist approach for concept formation, reinforcement learning and robotics", "author": ["Heinen MR", "Engel PM"], "venue": "Journal of Applied Computing Research", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Echo State Incremental Gaussian Mixture Network for Spatio-Temporal Pattern Processing", "author": ["RC Pinto", "PM Engel", "MR. Heinen"], "venue": "Proceedings of the IX ENIA-Brazilian Meeting on Artificial", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Recursive incremental gaussian mixture network for spatio-temporal pattern processing", "author": ["RC Pinto", "PM Engel", "MR. Heinen"], "venue": "Proc 10th Brazilian Congr Computational Intelligence", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Autocorrelation and partial autocorrelation functions to improve neural networks models on univariate time series forecasting", "author": ["JHF Flores", "PM Engel", "RC. Pinto"], "venue": "Neural Networks (IJCNN), The 2012 International Joint Conference on", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Dealing with continuous-state reinforcement learning for intelligent control of traffic signals", "author": ["MR Heinen", "AL Bazzan", "PM. Engel"], "venue": "Intelligent Transportation Systems (ITSC),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "One-shot learning in the road sign problem", "author": ["RC Pinto", "PM Engel", "MR. Heinen"], "venue": "Neural Networks (IJCNN), The 2012 International Joint Conference on", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Learning Abstract Behaviors with the Hierarchical Incremental Gaussian Mixture Network", "author": ["R de Pontes Pereira", "PM Engel", "RC. Pinto"], "venue": "Neural Networks (SBRN),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Location-Based Events Detection on Micro-Blogs", "author": ["Santos ADPd", "Wives LK", "Alvares LO"], "venue": "arXiv preprint arXiv:12104008", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Efficient update of the covariance matrix inverse in iterated linear discriminant analysis", "author": ["J Salmen", "M Schlipsing", "C. Igel"], "venue": "Pattern Recognition Letters", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Jointly Informative Feature Selection", "author": ["L Lefakis", "F. Fleuret"], "venue": "Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics;", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Extended mllt for gaussian mixture models", "author": ["Olsen PA", "Gopinath RA"], "venue": "Transactions in Speech and Audio Processing", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "A stochastic approximation method", "author": ["H Robbins", "S. Monro"], "venue": "The annals of mathematical statistics", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1951}, {"title": "INBC: An incremental algorithm for dataflow segmentation based on a probabilistic approach. INBC: an incremental algorithm for dataflow segmantation based on a probabilistic approach", "author": ["Engel PM"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Powers of tensors and fast matrix multiplication", "author": ["Gall FL"], "venue": "arXiv preprint arXiv:14017714", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Adjustment of an Inverse Matrix Corresponding to a Change in One Element of a Given Matrix", "author": ["Sherman J", "Morrison WJ"], "venue": "The Annals of Mathematical Statistics", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1950}, {"title": "Matrix algebra from a statistician\u2019s perspective", "author": ["Harville DA"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "The WEKA data mining software: an update", "author": ["M Hall", "E Frank", "G Holmes", "B Pfahringer", "P Reutemann", "IH. Witten"], "venue": "ACM SIGKDD explorations newsletter", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "A Fast Incremental Gaussian Mixture Model", "author": ["R.C. Pinto", "P.M. Engel"], "venue": "PloS one, 10(10),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Experiment Data for \u201dA Fast Incremental Gaussian Mixture Model", "author": ["Pinto", "RC"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2030}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1998}, {"title": "Learning multiple layers of features from tiny images", "author": ["A Krizhevsky", "G. Hinton"], "venue": "Computer Science Department,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["GE Hinton", "N Srivastava", "A Krizhevsky", "I Sutskever", "RR. Salakhutdinov"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Moritz", "Philipp", "Jordan", "Michael I", "Abbeel", "Pieter"], "venue": "arXiv preprint", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Dueling Network Architectures for Deep Reinforcement Learning", "author": ["Wang", "Ziyu", "de Freitas", "Nando", "Lanctot", "Marc"], "venue": "arXiv preprint", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The Incremental Gaussian Mixture Network (IGMN) [1,2] is a supervised algorithm which approximates the EM algorithm for Gaussian mixture models [3], as shown in [4].", "startOffset": 48, "endOffset": 53}, {"referenceID": 1, "context": "The Incremental Gaussian Mixture Network (IGMN) [1,2] is a supervised algorithm which approximates the EM algorithm for Gaussian mixture models [3], as shown in [4].", "startOffset": 48, "endOffset": 53}, {"referenceID": 2, "context": "The Incremental Gaussian Mixture Network (IGMN) [1,2] is a supervised algorithm which approximates the EM algorithm for Gaussian mixture models [3], as shown in [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 3, "context": "The Incremental Gaussian Mixture Network (IGMN) [1,2] is a supervised algorithm which approximates the EM algorithm for Gaussian mixture models [3], as shown in [4].", "startOffset": 161, "endOffset": 164}, {"referenceID": 4, "context": "New points are added directly to existing Gaussian components or new components are created when necessary, avoiding merge and split operations, much like what is seen in the Adaptive Resonance Theory (ART) algorithms [5].", "startOffset": 218, "endOffset": 221}, {"referenceID": 5, "context": "It has been previously shown in [6] that the algorithm is robust even when data is presented in random order, having similar performance and producing similar number of clusters in any order.", "startOffset": 32, "endOffset": 35}, {"referenceID": 3, "context": "Also, [4] has", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "In other words, any element can be used to predict any other element, like auto-associative neural networks [8] or missing data imputation [9].", "startOffset": 108, "endOffset": 111}, {"referenceID": 7, "context": "In other words, any element can be used to predict any other element, like auto-associative neural networks [8] or missing data imputation [9].", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "This feature is useful for simultaneous learning of forward and inverse kinematics [10], as well as for simultaneous learning of a value function and a policy in reinforcement learning [11].", "startOffset": 83, "endOffset": 87}, {"referenceID": 9, "context": "This feature is useful for simultaneous learning of forward and inverse kinematics [10], as well as for simultaneous learning of a value function and a policy in reinforcement learning [11].", "startOffset": 185, "endOffset": 189}, {"referenceID": 10, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 86, "endOffset": 93}, {"referenceID": 11, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 86, "endOffset": 93}, {"referenceID": 12, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 86, "endOffset": 93}, {"referenceID": 1, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 118, "endOffset": 125}, {"referenceID": 13, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 118, "endOffset": 125}, {"referenceID": 0, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 160, "endOffset": 170}, {"referenceID": 14, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 160, "endOffset": 170}, {"referenceID": 15, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 160, "endOffset": 170}, {"referenceID": 16, "context": "Previous successful applications of the IGMN algorithm include time-series prediction [12\u201314], reinforcement learning [2, 15], mobile robot control and mapping [1, 16,17] and outlier detection in data streams [18].", "startOffset": 209, "endOffset": 213}, {"referenceID": 5, "context": "One solution would be to use diagonal covariance matrices, but this decreases the quality of the results, as already reported in previous works [6, 12].", "startOffset": 144, "endOffset": 151}, {"referenceID": 10, "context": "One solution would be to use diagonal covariance matrices, but this decreases the quality of the results, as already reported in previous works [6, 12].", "startOffset": 144, "endOffset": 151}, {"referenceID": 26, "context": "In [28], we propose the use of rank-one updates for both inverse matrices and determinants applied to full covariance matrices, thus reducing the time complexity to O ( NKD ) for learning while keeping the quality of a full covariance matrix solution.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [19], rank-one updates were applied to an iterated linear discriminant analysis algorithm in order to decrease the complexity of the algorithm.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "Rank-one updates were also used in [20], where Gaussian models are employed for feature selection.", "startOffset": 35, "endOffset": 39}, {"referenceID": 19, "context": "Finally, in [21], the same kind of optimization was applied to Maximum Likelihood Linear Transforms (MLLT).", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "In the next subsections we describe the IGMN algorithm, a slightly improved version of the one described in [16].", "startOffset": 108, "endOffset": 112}, {"referenceID": 20, "context": "The equations are derived using the Robbins-Monro stochastic approximation [22] for maximizing the likelihood of the model.", "startOffset": 75, "endOffset": 79}, {"referenceID": 3, "context": "This derivation can be found in [4, 23].", "startOffset": 32, "endOffset": 39}, {"referenceID": 21, "context": "This derivation can be found in [4, 23].", "startOffset": 32, "endOffset": 39}, {"referenceID": 5, "context": "Since the removed components have small accumulated activations, it also implies that their removal has almost no negative impact on the model quality, often producing positive impact on generalization performance due to model simplification (a more throughout analysis of parameter sensibility for the IGMN algorithm can be found in [6]).", "startOffset": 334, "endOffset": 337}, {"referenceID": 26, "context": "It is an improvement over the version presented in [28].", "startOffset": 51, "endOffset": 55}, {"referenceID": 22, "context": "O ( D ) , for D dimensions (O ( D2 ( 1 )) for the Strassen algorithm or at best O ( D ) with the most recent algorithms to date [24]).", "startOffset": 128, "endOffset": 132}, {"referenceID": 23, "context": "This allows us to apply the Sherman-Morrison formula [25]:", "startOffset": 53, "endOffset": 57}, {"referenceID": 24, "context": "But computing the determinant itself is also a O ( D ) operation, so we will instead perform rank-one updates using the Matrix Determinant Lemma [26], which states the following:", "startOffset": 145, "endOffset": 149}, {"referenceID": 25, "context": "In fact, Weka (the data mining platform used in this work [27]) allows for only 1 output, leaving us with just scalar operations.", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "The Weka packages for both variations of the IGMN algorithm, as well as the datasets used in the experiments can be found at [29].", "startOffset": 125, "endOffset": 129}, {"referenceID": 28, "context": "Dataset Instances (N) Attributes (D) Classes breast-cancer 286 9 2 pima-diabetes 768 8 2 Glass 214 9 7 ionosphere 351 34 2 iris 150 4 3 labor-neg-data 57 16 2 soybean 683 35 19 MNIST [30] 70000 784 10 CIFAR-10 [31] 60000 3072 10", "startOffset": 183, "endOffset": 187}, {"referenceID": 29, "context": "Dataset Instances (N) Attributes (D) Classes breast-cancer 286 9 2 pima-diabetes 768 8 2 Glass 214 9 7 ionosphere 351 34 2 iris 150 4 3 labor-neg-data 57 16 2 soybean 683 35 19 MNIST [30] 70000 784 10 CIFAR-10 [31] 60000 3072 10", "startOffset": 210, "endOffset": 214}, {"referenceID": 30, "context": "The neural network is a parallel implementation of a state-of-the-art Dropout Neural Network [32] with 100 hidden neurons, 50% dropout for the hidden layer and 20% dropout for the input layer (this specific implementation can be found at https://github.", "startOffset": 93, "endOffset": 97}, {"referenceID": 28, "context": "The MNIST dataset was split into a training set with 60000 data points and a testing set containing 10000 data points, the standard procedure in the machine learning community [30].", "startOffset": 176, "endOffset": 180}, {"referenceID": 29, "context": "CIFAR-10 dataset was split into 50000 training data points and 10000 testing data points, also a standard procedure for this dataset [31].", "startOffset": 133, "endOffset": 137}, {"referenceID": 31, "context": "The FIGMN algorithm was compared to other 3 algorithms with high scores on OpenAI Gym: Sarsa(\u03bb), Trust Region Policy Optimization (TRPO; a policy gradient method, suitable to continuous states, actions and time, but which works in batch mode and has low data-efficiency) [33] and Dueling Double DQN (an improvement over the DQN algorithm, using two value function approximators with different update rates and generalizing between actions; it is restricted to discrete actions) [34].", "startOffset": 271, "endOffset": 275}, {"referenceID": 32, "context": "The FIGMN algorithm was compared to other 3 algorithms with high scores on OpenAI Gym: Sarsa(\u03bb), Trust Region Policy Optimization (TRPO; a policy gradient method, suitable to continuous states, actions and time, but which works in batch mode and has low data-efficiency) [33] and Dueling Double DQN (an improvement over the DQN algorithm, using two value function approximators with different update rates and generalizing between actions; it is restricted to discrete actions) [34].", "startOffset": 478, "endOffset": 482}], "year": 2017, "abstractText": "This work presents a fast and scalable algorithm for incremental learning of Gaussian mixture models. By performing rank-one updates on its precision matrices and determinants, its asymptotic time complexity is of O ( NKD ) for N data points, K Gaussian components and D dimensions. The resulting algorithm can be applied to high dimensional tasks, and this is confirmed by applying it to the classification datasets MNIST and CIFAR-10. Additionally, in order to show the algorithm\u2019s applicability to function approximation and control tasks, it is applied to three reinforcement learning tasks and its data-efficiency is evaluated.", "creator": "LaTeX with hyperref package"}}}