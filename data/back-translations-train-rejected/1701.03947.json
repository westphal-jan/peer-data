{"id": "1701.03947", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2017", "title": "Balancing Novelty and Salience: Adaptive Learning to Rank Entities for Timeline Summarization of High-impact Events", "abstract": "Long-running, high-impact events such as the Boston Marathon bombing often develop through many stages and involve a large number of entities in their unfolding. Timeline summarization of an event by key sentences eases story digestion, but does not distinguish between what a user remembers and what she might want to re-check. In this work, we present a novel approach for timeline summarization of high-impact events, which uses entities instead of sentences for summarizing the event at each individual point in time. Such entity summaries can serve as both (1) important memory cues in a retrospective event consideration and (2) pointers for personalized event exploration. In order to automatically create such summaries, it is crucial to identify the \"right\" entities for inclusion. We propose to learn a ranking function for entities, with a dynamically adapted trade-off between the in-document salience of entities and the informativeness of entities across documents, i.e., the level of new information associated with an entity for a time point under consideration. Furthermore, for capturing collective attention for an entity we use an innovative soft labeling approach based on Wikipedia. Our experiments on a real large news datasets confirm the effectiveness of the proposed methods.", "histories": [["v1", "Sat, 14 Jan 2017 16:47:51 GMT  (835kb,D)", "http://arxiv.org/abs/1701.03947v1", "Published via ACM to CIKM 2015"]], "COMMENTS": "Published via ACM to CIKM 2015", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["tuan tran", "claudia nieder\\'ee", "nattiya kanhabua", "ujwal gadiraju", "avishek anand"], "accepted": false, "id": "1701.03947"}, "pdf": {"name": "1701.03947.pdf", "metadata": {"source": "CRF", "title": "Balancing Novelty and Salience: Adaptive Learning to Rank Entities for Timeline Summarization of High-impact Events", "authors": ["Tuan Tran", "Claudia Nieder\u00e9e", "Nattiya Kanhabua", "Ujwal Gadiraju", "Avishek Anand", "Francois Hollande"], "emails": ["anand}@L3S.de", "permissions@acm.org."], "sections": [{"heading": null, "text": "Categories and Theme Descriptions H.3.3 [Information Storage and Retrieval]: Algorithms for Information Search and Retrieval General Terms, Experimental Keywords Entity Retrieval, Timeline Summarization, Wikipedia, Learning to Rank, News, Temporal Ranking"}, {"heading": "1. INTRODUCTION", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2. RELATED WORK", "text": "The task of summarizing messages has already been explored in various contexts, ranging from focusing on summarizing multiple documents to generating a timeline. [27] We propose a new method of summarizing complex stories using subway maps that explicitly capture the relationships between different aspects and show the chronological evolution of the stories. Instead of using documents or sentences as units of information, we provide a series of entities, such as supporting events in time."}, {"heading": "3. OVERVIEW", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Preliminaries", "text": "In fact, it is such that it is a matter of a way in which one sees oneself in a position to identify oneself, as if one were able to identify oneself. In fact, it is such as if one were able to identify oneself. In fact, it is such as if one were able to identify oneself. In fact, it is as if one were able to identify oneself, as if one were able to identify oneself. In fact, it is such as if one were able to identify oneself. In fact, it is such as if one were able to identify oneself, as if one were able to identify oneself. In order to be able to be able, to be able to be able to be able, to be able to be able to be able, to be able to be able to be able, to be able to be able to be able to be able, to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be in oneself. In fact, it is as if one were able to be able to identify oneself, as if one were able to be able to identify oneself, as if one were able to be able to be able to be able to identify oneself, to be able to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able in oneself."}, {"heading": "3.2 Approach Overview", "text": "We solve this problem by learning a unit ranking function that aims to optimize the trade-off between the meaning of units within a document and the informativeness of units within documents: y (q) t = f (E, \u03c9s, \u03c9i), f (1), where y (q) t is the vector of the ranking value of units in equation in time interval t, E is a matrix composed of characteristic vectors of units in equation extracted from their context of mention. \u03c9s are the unknown parameter vectors for unit ranking based on emphasis and informativeness, respectful. In our work, a ranking function is based on a learning and ranking technique [19]. A general approach to the learning goal is to optimize a defined loss function defined by manual annotation or judgments y (q) j of units for a series of training events within the Q time interval."}, {"heading": "3.3 Framework", "text": "Figure 2 provides an overview of our framework for the ranking of our units, which covers both the training and the application / testing phase. If we specify an event q, its reporting time axis and the set of documents Dq (in practice, Dq can be given a priori or retrieved using different retrieval models), we identify the entity set Eq using our entity extraction, which consists of designated entity recognition, co-manufacturing and context texture (Section 4.1). When the event is used for training (training phase), we associate a subset of Eq with Wikipedia concepts comprising the popular and emerging entities of the event. In order to facilitate the learning process, these entities are gently labeled on the basis of viewpoint statistics from Wikipedia (Section 4.2), which serve as training instances. Although we use popular entities for training, we design the common features, and the adoption of Wikipedia, the adoption of the adoption of Wikipedia, and the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of Wikipedia, the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption of the adoption"}, {"heading": "4. METHODOLOGY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Entity Extraction", "text": "First, we discuss how to extract entities, the key units within our framework. For each document in the set, we use a named entity identifier to identify mentions of entities in three categories (persons, places, and organizations). For each mention, we add the sentences contained as context. First, an intra-document co-reference system is used to identify all co-reference chains for entity mentions within a document. We include each reference in the chain with and from the contexts of the entity. First, an intra-document co-reference system is used to identify all entity mentions within a document."}, {"heading": "4.2 Mining Wikipedia for Soft Labeling", "text": "In the following, we explain how the training data is generated. In particular, in view of an event q, an interval t and an entity e, we aim to generate the score y (q) j so that y (e) > y (q) j (e), if the entity e \u00b2 is in time t in relation to the event q. This score can be used as an indicator of entity to learn the ranking functions mentioned in Equation 2. The use of soft entity names has already been suggested in [12], where user behavior is used in query logs as an indicator of the entity Salience Scores. Dunnietz et al proposed to treat entities in news headlines as outstanding, and propagated this validity to other entities via the PageRank algorithm. The limitation of these measures is that they restrict the assessment of the meaning of individual documents."}, {"heading": "4.3 Unified Ranking Function", "text": "We are now turning our attention to defining the ranking function in Equation 1. The intuition is that for each event qand time ti we place one unit e higher than others when: (1) e is more relevant for the central parts of documents in Dq, i where it appears (emphasis); and (2) the context of e for other contexts (of e or other units) in Dq, i \u2212 1. In addition, these two criteria should be unified in an adaptive way that depends on the query, i.e. event and time. For example, users interested in a festival would like to know more about prominent entities of the event, while those following a breaking story prefer entities with fresher information. Even for an event, the importance of emerging and information over time might be important. For example, informativeness is more important at the beginning if the event is frequently updated."}, {"heading": "4.4 Multi-criteria Learning Model", "text": "We will now discuss how to learn the above ranking function using Equation 2. An easy way is to use the two models q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q, which is expensive. Secondly, previous work has shown that a \"hard\" classification \"of the query itself poses a difficult problem and can damage ranking performance [14]. In this work, we use the dividing and overcoming (DAC) of the learning framework in [3] as follows: We define E \u0445 as the\" hard \"matrix\" of the (M + N) of the (M + N) matrix of the (M + N) dimensional extension vectors from the corresponding vectors of matrices and eices."}, {"heading": "4.5 Event-based Models Adaptation", "text": "The adaptive values S (q, t), I (q, t) and the decay function q (t) are critical to the adjustment of ratios and informativeness models. A verified approach to pre-defining categories for events (S, I) is not practical and detrimental to ranking performance if the training data is distorted. Instead, previous work on information-dependent ranking [14, 3] often uses the \"localization property\" of query spaces, i.e. the characteristics of queries in the same category are more similar than those of the different categories. Bian et al. [3] constructed query characteristics using query-dependent documents and grouped them over a blending model. However, the feature setting is the same for all query ranges, making it difficult to derive the semantics of the query categories. In this work, we inherit and adjust the approach in [3] as follows."}, {"heading": "5. ENTITY RANKING FEATURES", "text": "We will now discuss the highlighting and computerization characteristics for the entity ranking. Ranking characteristics are extracted from event documents in which the entity appears as follows: These characteristics, referred to as individual characteristics, are extracted at two different levels. First, characteristics are extracted at context level regardless of each mention and their contexts. Characteristics at this level include mention word offset, context length or meaning values of the context within the document using summary algorithms (SumBasic or SumFocus characteristics). Second, label level characteristics are extracted from all mentions, for example, aggregated term (document) frequencies of mentions. Based on the individual characteristics, the entity characteristics are constructed as follows. For each entity and feature dimension, we have the list of characteristic values (z1, z2, zn), where the characteristic is simple for each of the characterization or for the average."}, {"heading": "5.1 Salience Features", "text": "An important piece of evidence for the emphasis of entities is the context of mentioning entities. It is well known that text at the beginning of a document contains more significant information [12, 10]. In addition to the position, the content of sentences per se or in relation to other sentences also indicates the emphasis of entities. We use SumBasic [26] and LexRank [11] summarization algorithms to obtain the evaluation of contexts (attributes Sum-B or PR). We follow this direction and apply characteristics perceived by humans. Entity can be judged by the intentions or interests of the reader [12], and recent studies suggest that users \"interest in entities can be attracted by randomness in texts [6]. We apply the characteristics depicted in [6], namely setimentality, attitudes and readability of individual mention contexts. For readability, we also refer to two other standard metrics with a flesh index [Foincg-16] and [Foincaid]."}, {"heading": "5.2 Informativeness Features", "text": "Considering a mention m of entity e, the context-conscious informativeness value of m for event q is defined by ti: inf (m, i) = 1 Uq, i \u2212 1 = 1 Uq, i \u2212 1 \u00b2 c \u00b2, c (m)) s (dc \u00b2, q \u2212 1) | Uq, i \u2212 1 = 1 Uq, i \u2212 1 is the mention of contexts of e in Dq, i \u2212 1, dc \u00b2 is the document consisting of the context c \u00b2, c (m), i \u2212 1) | Uq, i \u2212 1 = 6 = Uq, i \u2212 1 is the mention of contexts in Dq, i \u2212 1, dc \u00b2 is the document containing the context c \u00b2, c \u00b2, c (m) is the sentence dis-similarity, and s (dc \u00b2, i \u2212 1) is the mention of c \u00b2, the mention of c \u00b2."}, {"heading": "6. EXPERIMENTS AND EVALUATIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Setup", "text": "We use KBA 2014 Filtered Stream Corpus (SC14) dataset, which is used for TREC 2014 Temporal Summarization Track2. We extract news and mainstream articles from the dataset, consisting of 7,592,062 documents. The dataset includes 15 long-running news events from December 2012 to April 2013. All events are highly effective, are widely discussed in the news media and have their own Wikipedia pages. Each event has a predefined time span (ranging from 4 to 18 days) and is represented by a text phrase that is used as the initial event. Based on the event type (available in the datasets) we group the events into 4 categories: Accident, Riot and Protest, Natural Disasters, Crime (Shooting, Bombing)."}, {"heading": "6.2 Evaluation", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "6.3 Results and Discussion", "text": "This year it is more than ever before."}, {"heading": "7. CONCLUSIONS", "text": "In this paper, we have presented a novel approach to summarizing the timeline of high-impact events, using entities as the main unit of the summary. We propose to dynamically adjust the properties of entity and informativity to enhance the user experience of the summary. In addition, we introduce adaptive learning to evaluate the framework that collectively learns the properties of entity and informativity in a consistent manner. To scale learning, we use Wikipedia page views as an implicit signal of user interest in entities associated with high-impact events. Our experiments have shown that the methods introduced can significantly improve user experience in selecting entities by using both small-scale expert reviews and large-scale crowdsourcing evaluations."}, {"heading": "8. REFERENCES", "text": "[1] P. Andre \u0301, J. Teevan, and S. T. Dumais. From x-rays to sillyputty via uranus: serendipity and its role in web search. In CHI, 2009. [2] D. Berntsen. Involuntary autobiographic memories: An introduction to the unbidden past. Cambridge University Press, 2009. [3] J. Bian, X. Li, F. Li, Z. Zheng, and H. Zha T. Ranking specialization for web search: a divide-and-conquer approach by using topical ranksvm. In WWW, 2010. [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 2003. [5] B. Boguraev and C. Kennedy. Salience-based content characterization of text documents. ACL, 1997. [6] I. Bordino, Y."}], "references": [{"title": "From x-rays to silly putty via uranus: serendipity and its role in web search", "author": ["P. Andr\u00e9", "J. Teevan", "S.T. Dumais"], "venue": "In CHI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Involuntary autobiographical memories: An introduction to the unbidden past", "author": ["D. Berntsen"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Ranking specialization for web search: a divide-and-conquer approach by using topical ranksvm", "author": ["J. Bian", "X. Li", "F. Li", "Z. Zheng", "H. Zha"], "venue": "In WWW,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Salience-based content characterisation of text documents", "author": ["B. Boguraev", "C. Kennedy"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Penguins in sweaters, or serendipitous entity search on user-generated content", "author": ["I. Bordino", "Y. Mejova", "M. Lalmas"], "venue": "In CIKM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "N\u00f8rv\u030aag. Wikipop: Personalized event detection system based on wikipedia page view statistics", "author": ["K.M. Ciglan"], "venue": "In CIKM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Taer: time-aware entity retrieval-exploiting the past to find relevant entities in news articles", "author": ["G. Demartini", "M.M.S. Missen", "R. Blanco", "H. Zaragoza"], "venue": "In CIKM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Robust, light-weight approaches to compute lexical similarity", "author": ["Q. Do", "D. Roth", "M. Sammons", "Y. Tu", "V. Vydiswaran"], "venue": "Computer Science Research and Technical Reports, University of Illinois,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "A new entity salience task with millions of training", "author": ["J. Dunietz", "D. Gillick"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Identifying salient entities in web pages", "author": ["M. Gamon", "T. Yano", "X. Song", "J. Apacible", "P. Pantel"], "venue": "In CIKM,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Beyond accuracy: evaluating recommender systems by coverage and serendipity", "author": ["M. Ge", "C. Delgado-Battenfeld", "D. Jannach"], "venue": "In RecSys,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Query dependent ranking using k-nearest neighbor", "author": ["X. Geng", "T.-Y. Liu", "T. Qin", "A. Arnold", "H. Li", "H.-Y. Shum"], "venue": "In SIGIR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "In VLDB,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Judges scold lawyers for bad writing, 1952", "author": ["R. Gunning"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1952}, {"title": "Kore: keyphrase overlap relatedness for entity disambiguation", "author": ["J. Hoffart", "S. Seufert", "D.B. Nguyen", "M. Theobald", "G. Weikum"], "venue": "In CIKM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Improving effectiveness of query expansion using information theoretic approach", "author": ["H. Imran", "A. Sharan"], "venue": "In Trends in Applied Intelligent Systems", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "In KDD,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel", "author": ["J.P. Kincaid", "R.P. Fishburne Jr.", "R.L. Rogers", "B.S. Chissom"], "venue": "Technical report, DTIC Document,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1975}, {"title": "Boilerplate detection using shallow text features", "author": ["C. Kohlsch\u00fctter", "P. Fankhauser", "W. Nejdl"], "venue": "In WSDM. ACM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Joint entity and event coreference resolution across documents", "author": ["H. Lee", "M. Recasens", "A. Chang", "M. Surdeanu", "D. Jurafsky"], "venue": "In EMNLP,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Incremental update summarization: Adaptive sentence selection based on prevalence and novelty", "author": ["R. McCreadie", "C. Macdonald", "I. Ounis"], "venue": "In CIKM,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Entity-centric topic-oriented opinion summarization in twitter", "author": ["X. Meng", "F. Wei", "X. Liu", "M. Zhou", "S. Li", "H. Wang"], "venue": "In KDD,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Influence of timeline and named-entity components on user engagement", "author": ["Y. Moshfeghi", "M. Matthews", "R. Blanco", "J.M. Jose"], "venue": "In ECIR", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Evaluating content selection in summarization: The pyramid method", "author": ["A. Nenkova", "R. Passonneau"], "venue": "In NAACL-HLT,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Trains of thought: Generating information maps", "author": ["D. Shahaf", "C. Guestrin", "E. Horvitz"], "venue": "In WWW,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Leverage learning to rank in an optimization framework for timeline summarization", "author": ["G.B. Tran", "T. Tran", "N.-K. Tran", "M. Alrifai", "N. Kanhabua"], "venue": "In TAIA Workshop at SIGIR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "The cue is key - design for real-life remembering", "author": ["E. van den Hoven", "B. Egge"], "venue": "Zeitschrift fu\u0308r Psychologie.,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Beyond sumbasic: Task-focused summarization with sentence simplification and lexical expansion", "author": ["L. Vanderwende", "H. Suzuki", "C. Brockett", "A. Nenkova"], "venue": "Information Processing & Management,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "A sentence compression based framework to query-focused multi-document summarization", "author": ["L. Wang", "H. Raghavan", "V. Castelli", "R. Florian", "C. Cardie"], "venue": "In ACL,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Wikipedia as a time machine", "author": ["S. Whiting", "J. Jose", "O. Alonso"], "venue": "In WWW,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Measuring term informativeness in context", "author": ["Z. Wu", "C.L. Giles"], "venue": "In NAACL-HLT,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Evolutionary timeline summarization: a balanced optimization framework via iterative substitution", "author": ["R. Yan", "X. Wan", "J. Otterbacher", "L. Kong", "X. Li", "Y. Zhang"], "venue": "In SIGIR,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Timeline generation with social attention", "author": ["X.W. Zhao", "Y. Guo", "R. Yan", "Y. He", "X. Li"], "venue": "In SIGIR,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}], "referenceMentions": [{"referenceID": 33, "context": "In contrast to other work in event summarization [34, 23], our entity timelines use entities instead of sentences as main units of summarization as depicted in the case of the 2015 Germanwings plan crash (Figure 1).", "startOffset": 49, "endOffset": 57}, {"referenceID": 22, "context": "In contrast to other work in event summarization [34, 23], our entity timelines use entities instead of sentences as main units of summarization as depicted in the case of the 2015 Germanwings plan crash (Figure 1).", "startOffset": 49, "endOffset": 57}, {"referenceID": 28, "context": "In this sense, our work is related to the idea of designing or creating memory cues for real-life remembering [29].", "startOffset": 110, "endOffset": 114}, {"referenceID": 1, "context": "Entities such as persons and locations have been identified as very effective external memory cues [2].", "startOffset": 99, "endOffset": 102}, {"referenceID": 23, "context": "In addition, the importance of entities in event summarization has also been shown in recent work [24, 25].", "startOffset": 98, "endOffset": 106}, {"referenceID": 24, "context": "In addition, the importance of entities in event summarization has also been shown in recent work [24, 25].", "startOffset": 98, "endOffset": 106}, {"referenceID": 4, "context": "the one hand, considers the property of being in the focus of attention in a document has been studied in previous work [5, 12, 10].", "startOffset": 120, "endOffset": 131}, {"referenceID": 11, "context": "the one hand, considers the property of being in the focus of attention in a document has been studied in previous work [5, 12, 10].", "startOffset": 120, "endOffset": 131}, {"referenceID": 9, "context": "the one hand, considers the property of being in the focus of attention in a document has been studied in previous work [5, 12, 10].", "startOffset": 120, "endOffset": 131}, {"referenceID": 4, "context": "In [5], Boguraev and Kennedy use salient text phrases for the creation of so-called capsule overviews, whereas recently methods for the identification of salient entities, e.", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": ", in Web pages [12] and news articles [10], have been developed.", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": ", in Web pages [12] and news articles [10], have been developed.", "startOffset": 38, "endOffset": 42}, {"referenceID": 32, "context": "Informativeness, on the other hand, assesses the level of new information associated with an entity in a text and can be computationally measured using features derived from statistical and linguistic information [33].", "startOffset": 213, "endOffset": 217}, {"referenceID": 4, "context": "The task of news summarization has been already studied in various contexts, which range from focusing on multi-document summarization [5, 11] to generating a timeline summary for a specific news story [34, 23, 28, 35].", "startOffset": 135, "endOffset": 142}, {"referenceID": 10, "context": "The task of news summarization has been already studied in various contexts, which range from focusing on multi-document summarization [5, 11] to generating a timeline summary for a specific news story [34, 23, 28, 35].", "startOffset": 135, "endOffset": 142}, {"referenceID": 33, "context": "The task of news summarization has been already studied in various contexts, which range from focusing on multi-document summarization [5, 11] to generating a timeline summary for a specific news story [34, 23, 28, 35].", "startOffset": 202, "endOffset": 218}, {"referenceID": 22, "context": "The task of news summarization has been already studied in various contexts, which range from focusing on multi-document summarization [5, 11] to generating a timeline summary for a specific news story [34, 23, 28, 35].", "startOffset": 202, "endOffset": 218}, {"referenceID": 27, "context": "The task of news summarization has been already studied in various contexts, which range from focusing on multi-document summarization [5, 11] to generating a timeline summary for a specific news story [34, 23, 28, 35].", "startOffset": 202, "endOffset": 218}, {"referenceID": 34, "context": "The task of news summarization has been already studied in various contexts, which range from focusing on multi-document summarization [5, 11] to generating a timeline summary for a specific news story [34, 23, 28, 35].", "startOffset": 202, "endOffset": 218}, {"referenceID": 26, "context": "[27] propose a novel method for summarizing complex stories using metro maps that explicitly capture the relations among different aspects and display a temporal development of the stories.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": ", [24, 25].", "startOffset": 2, "endOffset": 10}, {"referenceID": 24, "context": ", [24, 25].", "startOffset": 2, "endOffset": 10}, {"referenceID": 7, "context": "[8], where the task tackled is to identify the entities that best describe the documents for a given query.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "In IR, salient entities in news documents help improving the performance of document retrieval [25].", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "There is a body of work in identifying salient entities in general Web [12] and in news domain [10], but the existing work does not take into account the time dimension.", "startOffset": 71, "endOffset": 75}, {"referenceID": 9, "context": "There is a body of work in identifying salient entities in general Web [12] and in news domain [10], but the existing work does not take into account the time dimension.", "startOffset": 95, "endOffset": 99}, {"referenceID": 27, "context": "In our work, we apply a learning to rank (L2R) technique to generate timeline summaries in a similar way as done in [28, 23].", "startOffset": 116, "endOffset": 124}, {"referenceID": 22, "context": "In our work, we apply a learning to rank (L2R) technique to generate timeline summaries in a similar way as done in [28, 23].", "startOffset": 116, "endOffset": 124}, {"referenceID": 22, "context": "Following [23], we define a longrunning event as \u201ca newsworthy happening in the world that is significant enough to be reported on over multiple days\u201d.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "Similarly to [10], we define the entity salience as the quality of \u201cbeing in the focus of attention\u201d in the corresponding documents.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "Another relevant aspect considered for selecting entities to be included in an event timeline is informativeness [12], which imposes that selected entities in an evolving event should also deliver novel information when compared to the past information.", "startOffset": 113, "endOffset": 117}, {"referenceID": 18, "context": "In our work, a ranking function is based on a learningto-rank technique [19].", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Speeding up co-reference resolution: To speed up the computation, we do not use EM clustering as [22], but employ a set of heuristics, which have proven to be effective in practice.", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "Hence, we represent entity mention labels as vectors using two-grams (for instance, \u201cObama\u201d becomes \u201cob\u201d, \u201cba\u201d, \u201cam\u201d, \u201cma\u201d) and apply LSH clustering [15] to group similar mentions.", "startOffset": 149, "endOffset": 153}, {"referenceID": 16, "context": "LSH has been proven to perform well in entity disambiguation tasks [17], and it is much faster than the standard EM-based clustering.", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "The use of soft labeling for entities\u2019 salience has already been proposed in [12], where user click behaviour in query logs is used as an indicator for entity salience scores.", "startOffset": 77, "endOffset": 81}, {"referenceID": 9, "context": "[10] proposed treating entities in news headlines as salient, and propagate those salience scores to other entities via the PageRank algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Actually, Wikipedia has gained attention in recent years as a source of temporal event information [32].", "startOffset": 99, "endOffset": 103}, {"referenceID": 6, "context": "Event-triggered bursts in page views, as they are for example used in [7] for event detection, are thus a good proxy for the interestingness of an event-related entity at a considered point in time, which is influenced both by the salience and the informativeness of the event.", "startOffset": 70, "endOffset": 73}, {"referenceID": 13, "context": "Secondly, previous work has pointed out that a \u201chard\u201d classification of query based on intent itself is a difficult problem, and can harm the ranking performance [14].", "startOffset": 162, "endOffset": 166}, {"referenceID": 2, "context": "In this work, we exploit the divide and conquer (DAC) learning framework in [3] as follows.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "For instance, if we use hinge loss as in [3], we can then adapt the RankSVM [19], an algorithm that seeks to learn the linear function g in (5) by minimizing the number of misordered", "startOffset": 41, "endOffset": 44}, {"referenceID": 18, "context": "For instance, if we use hinge loss as in [3], we can then adapt the RankSVM [19], an algorithm that seeks to learn the linear function g in (5) by minimizing the number of misordered", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "Instead, previous work on query-dependent ranking [14, 3] often exploit the \u201clocality property\u201d of query spaces, i.", "startOffset": 50, "endOffset": 57}, {"referenceID": 2, "context": "Instead, previous work on query-dependent ranking [14, 3] often exploit the \u201clocality property\u201d of query spaces, i.", "startOffset": 50, "endOffset": 57}, {"referenceID": 2, "context": "[3] constructed query features using top-retrieved documents, and clustered them via a mixture model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "In this work, we inherit and adjust the approach in [3] as follows.", "startOffset": 52, "endOffset": 55}, {"referenceID": 0, "context": "For all features, we apply quantile normalization, such that all individual features (and thus entity aggregated features) are scaled between [0, 1].", "startOffset": 142, "endOffset": 148}, {"referenceID": 11, "context": "It is wellknown that text at the beginning of document contains more salient information [12, 10].", "startOffset": 89, "endOffset": 97}, {"referenceID": 9, "context": "It is wellknown that text at the beginning of document contains more salient information [12, 10].", "startOffset": 89, "endOffset": 97}, {"referenceID": 25, "context": "We apply SumBasic [26] and LexRank [11] summarization algorithms to obtain the scores of contexts (features Sum-B and PR, respectively).", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "We apply SumBasic [26] and LexRank [11] summarization algorithms to obtain the scores of contexts (features Sum-B and PR, respectively).", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "Entity salience can be assessed by reader\u2019s intentions or interests [12], and recent study suggests that user interest in entities can be attracted via serendipity in texts [6].", "startOffset": 68, "endOffset": 72}, {"referenceID": 5, "context": "Entity salience can be assessed by reader\u2019s intentions or interests [12], and recent study suggests that user interest in entities can be attracted via serendipity in texts [6].", "startOffset": 173, "endOffset": 176}, {"referenceID": 5, "context": "We follow this direction and apply features presented in[6], namely setimentality, attitudes, and readability scores of each mention context.", "startOffset": 56, "endOffset": 59}, {"referenceID": 15, "context": "For readability, we also include two other standard metrics, Gunning Fog index [16] and Fleisch-Kincaid index [20].", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "For readability, we also include two other standard metrics, Gunning Fog index [16] and Fleisch-Kincaid index [20].", "startOffset": 110, "endOffset": 114}, {"referenceID": 30, "context": "Following [31], we use the overlap between contexts and the event query at the unigram, bigram levels, and at bigram where tokens are tolerable to have 4 words in between (Bi4 feature).", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "We also compute the query-focused SumFocus [30] score of contexts as another feature.", "startOffset": 43, "endOffset": 47}, {"referenceID": 32, "context": "offset of mention in context CoEntE (M) Number of co-occurring entities in Dq,i\u22121 SentLen (C) Context length with/without stopwords CTI(C) CTI score of mention given its context in Dq,i and Dq,i\u22121 [33] Sent-5 /-10 (C) Context length with/without stopwords > 5/10 ? 1- / 2- / 3-Sent (C) Is context among 1/3/5 first sentences ? TDiv (C,M) Topic diversity of context in Dq,i, Dq,i\u22121 TITLE (M) Is the mention in titles of any d \u2208 Dq,i ? CosSim (C) Cosine similarity of context in Dq,i, Dq,i\u22121 CoEntM (M) No.", "startOffset": 197, "endOffset": 201}, {"referenceID": 32, "context": "We adapt the state-of-the-art measure [33], which incorporates the context of mentions.", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "We normalize the metric to [0, 1], and set it to 1, when the entity first appears in Dq,i.", "startOffset": 27, "endOffset": 33}, {"referenceID": 8, "context": "We use the NESim method [9] for this strategy.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "Besides lexicon-based, we also calculate the informativeness on a higher level by representing contexts by latent topics, using latent Dirichlet Allocation model [4].", "startOffset": 162, "endOffset": 165}, {"referenceID": 20, "context": "For each index, we remove boilerplate texts from document using Boilerpipe [21], skip stop words, and lemmatize the terms.", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "We improve the results using Kullback-Leibler query expansion [18], and add top 30 expanded terms to construct the event query q used for query-related features computation.", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "[8] proposed a learning framework to retrieve the most salient entities from the news, taking into consideration information from documents previously published.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "IUS [23]: This work represents the \u201cinformativeness-pro\u201d approach, it attempts to build update summaries for events by incrementally selecting sentences, maximizing the gain and coverage with respect to summaries on previous days.", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "Since we are not interested in adaptively determining the cutoff values for the summary, we implement only the learningto-rank method reported in [23] to score the sentences.", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": "This was inspired by recent work in user experience of entity retrieval [13, 6].", "startOffset": 72, "endOffset": 79}, {"referenceID": 5, "context": "This was inspired by recent work in user experience of entity retrieval [13, 6].", "startOffset": 72, "endOffset": 79}, {"referenceID": 0, "context": "One popular metric widely adopted in existing work to measure such user-perceived quality is the serendipity, which measures degree to which results are \u201cnot highly relevant but interesting\u201d to user taste [1].", "startOffset": 205, "endOffset": 208}, {"referenceID": 5, "context": "Traditional serendipity metric was proposed to contrast the retrieval results with some obvious baseline [6].", "startOffset": 105, "endOffset": 108}], "year": 2017, "abstractText": "Long-running, high-impact events such as the Boston Marathon bombing often develop through many stages and involve a large number of entities in their unfolding. Timeline summarization of an event by key sentences eases story digestion, but does not distinguish between what a user remembers and what she might want to re-check. In this work, we present a novel approach for timeline summarization of high-impact events, which uses entities instead of sentences for summarizing the event at each individual point in time. Such entity summaries can serve as both (1) important memory cues in a retrospective event consideration and (2) pointers for personalized event exploration. In order to automatically create such summaries, it is crucial to identify the \u201cright\u201d entities for inclusion. We propose to learn a ranking function for entities, with a dynamically adapted trade-off between the in-document salience of entities and the informativeness of entities across documents, i.e., the level of new information associated with an entity for a time point under consideration. Furthermore, for capturing collective attention for an entity we use an innovative soft labeling approach based on Wikipedia. Our experiments on a real large news datasets confirm the effectiveness of the proposed methods.", "creator": "LaTeX with hyperref package"}}}