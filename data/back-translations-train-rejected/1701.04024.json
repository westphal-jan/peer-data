{"id": "1701.04024", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2017", "title": "A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue", "abstract": "Task-oriented dialogue focuses on conversational agents that participate in user-initiated dialogues on domain-specific topics. In contrast to chatbots, which simply seek to sustain open-ended meaningful discourse, existing task-oriented agents usually explicitly model user intent and belief states. This paper examines bypassing such an explicit representation by depending on a latent neural embedding of state and learning selective attention to dialogue history together with copying to incorporate relevant prior context. We complement recent work by showing the effectiveness of simple sequence-to-sequence neural architectures with a copy mechanism. Our model outperforms more complex memory-augmented models by 7% in per-response generation and is on par with the current state-of-the-art on DSTC2.", "histories": [["v1", "Sun, 15 Jan 2017 10:38:17 GMT  (221kb,D)", "http://arxiv.org/abs/1701.04024v1", "5 pages"], ["v2", "Sat, 4 Feb 2017 09:31:18 GMT  (221kb,D)", "http://arxiv.org/abs/1701.04024v2", "5 pages"], ["v3", "Mon, 14 Aug 2017 22:18:38 GMT  (223kb,D)", "http://arxiv.org/abs/1701.04024v3", "6 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["mihail eric", "christopher d manning"], "accepted": false, "id": "1701.04024"}, "pdf": {"name": "1701.04024.pdf", "metadata": {"source": "CRF", "title": "A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue", "authors": ["Mihail Eric", "Christopher D. Manning"], "emails": ["meric@cs.stanford.edu", "manning@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "To this end, research efforts have focused on the use of machine learning methods to train agents through dialogue corpora. One line of work has addressed the problem with partially observable Markov decision-making processes and reinforcement learning with carefully designed action spaces. (Young et al., 2013) However, the large, hand-designed action and state spaces make this class of models fragile, and in practice, most dialogue systems used remain handwritten rule-based. Recently, neural network models have succeeded in a variety of natural language process tasks (Bahdanau et al., 2014; Sutskever et al., 2015b), due to their ability to learn implicitly distributed representations of data in ways that are not sustainable."}, {"heading": "2 Architecture", "text": "In fact, it is a matter of a way in which people are able to unfold, and in which people are able to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to retaliate, to show that they are able to unfold, to unfold, to unfold."}, {"heading": "2.1 Training", "text": "We train with a cross-entropy loss and the Adam Optimizer (Kingma and Ba, 2015), applying dropout (Hinton et al., 2012) as regularization factor to the input and output of the LSTM. We identified hyperparameters by random search, evaluating a subset of data extending over a predetermined validation period. Dropout rates ranged from 0.75 to 0.95. We used word embedding with size 300, and the hidden layer and cell sizes were determined by our search to 353. We used gradient clipping with a clip value of 10 to avoid gradient explosions during training. Attention, output parameters, word embedding and LSTM weights are randomly determined from a uniform piecemeal distribution in the style of initialized (Sllo and Abbott, 2015)."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Data", "text": "For our experiments, we used dialogs extracted from the State Tracking Challenge 2 (DSTC2) dialog (Henderson et al., 2014), a data set for restaurant reservation systems. While the original challenge was to develop a system for returning the dialog state, for our study we used the Bordes and Weston version of the data (2016), which ignores the dialog state comments and uses only the raw text of the dialogs while adding system commands. Therefore, the raw text includes user and system statements, as well as the API calls that the system would make in response to user queries to the underlying KB. We use the turn / validation / test sections from this modified version of the data set. The data set is attractive for a number of reasons: 1) It is derived from a real system, so that it directly compares the kind of linguistic diversity and entertainment capabilities that we would like to have in a table based on effective knowledge 2)."}, {"heading": "3.2 Metrics", "text": "Assessing dialog systems is notoriously difficult (Liu et al., 2016). We use several metrics to assess certain aspects of our model that come from previous work: \u2022 Per-Response Accuracy: Bordes and Weston (2016) report a pro-turn response accuracy that verifies your model's ability to select the system response at a given time. Your system performs a multi-class classification of a pre-defined candidate created by aggregating all system responses seen in training, validation, and test sets. \u2022 Per-Dialogue Accuracy: Bordes and Weston (2016) report a pro-dialogue accuracy that evaluates the ability to classify each model. \u2022 Per-Dialogue Accuracy: Bordes and Weston (2016) report a pro-dialogue accuracy that evaluates the ability to classify each model."}, {"heading": "3.3 Results", "text": "In Table 2, we include the results of our models compared to the reported performance of the best performing model of (Bordes and Weston, 2016), which is a variant of an end-to-end storage network (Sukhbaatar et al., 2015), and their model is referred to as MemNN. We also refer to the model of (Liu and Perez, 2016), which is referred to as GMemNN, and the model of (Seo et al., 2016), which is referred to as QRN, which is currently state-of-the-art. In the table, Seq2Seq refers to our vanilla encoder decoder architecture with (1), (2) and (3) LSTM layers each achieved. + Attn refers to a 1-layer Seq2Seq with attention-based decoding. + Copy refers to + Attn with our copying mechanism to encoder layer and encoder layer encoding."}, {"heading": "4 Discussion and Conclusion", "text": "We have iteratively developed a class of neural models for task-oriented dialog that is capable of trumping other more complex neural architectures by a number of metrics, and the model easily incorporates capabilities that we believe are critical to building good task-oriented dialog agents, namely maintaining the dialog state and the ability to extract and use relevant units from the dialog context without requiring interim oversight of dialog state or belief tracker modules. We attribute the large gains in accuracy per response and unit F1 that + EntType has shown to its ability to select the relevant KB units from the dialog context that are fed into the encoder. Figure 1 shows the attention-based copy weights of the model, indicating that the model is able to learn the relevant units that it should focus on in the context."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau et al.2014] D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning end-to-end goal-oriented dialog. arXiv preprint arXiv:1605.07683", "author": ["Bordes", "Weston2016] A. Bordes", "J. Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2016}, {"title": "The second dialog state tracking challenge", "author": ["B. Thomson", "J. Williams"], "venue": "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Hinton et al.2012] G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Data recombination for neural semantic parsing. Association for Computational Linguistics", "author": ["Jia", "Liang2016] R. Jia", "P. Liang"], "venue": null, "citeRegEx": "Jia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2016}, {"title": "Adam: a method for stochastic optimization. International Conference for Learning Representations", "author": ["Kingma", "Ba2015] D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "A diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055", "author": ["J. Li", "M. Galley", "C. Brockett", "J. Gao", "W.B. Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "2016. Gates end-to-end memory networks. arXiv preprint arXiv:1610.04211", "author": ["Liu", "Perez2016] F. Liu", "J. Perez"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "How not to evaluate your dialogue system: an empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["C.-W. Liu", "R. Lowe", "I.V. Serban", "M. Noseworthy", "L. Charlin", "J. Pineau"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Luong et al.2015a] M. Luong", "H. Pham", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine", "author": ["Papineni et al.2002] K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Data-driven response generation in social media", "author": ["Ritter et al.2011] A. Ritter", "C. Cherry", "W.B. Dolan"], "venue": "Empirical Methods in Natural Language Processing", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Query-reduction networks for question answering", "author": ["Seo et al.2016] M. Seo", "S. Min", "A. Farhadi", "H. Hajishirzi"], "venue": "arXiv preprint arXiv:1606.04582", "citeRegEx": "Seo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Random walk initialization for training very deep feed forward networks. arXiv preprint arXiv:1412.6558", "author": ["Sussillo", "Abbott2015] D. Sussillo", "L.F. Abbott"], "venue": null, "citeRegEx": "Sussillo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sussillo et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "2015b. Grammar as a foreign language", "author": ["Vinyals et al.2015b] O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "2016b. A network-based end-toend trainable task-oriented dialogue system", "author": ["Wen et al.2016b] T.H. Wen", "M. Gasic", "N. Mrksic", "L.M.R.-B", "P.-H. Su", "S. Ultes", "D. Vandyke", "S. Young"], "venue": "arXiv preprint arXiv:1604.04562", "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "POMDP-based statistical spoken dialog systems: a review", "author": ["Young et al.2013] S. Young", "M. Gasic", "B. Thomson", "J.D. Williams"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Young et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "Recurrent neural network regularization", "author": ["Zaremba et al.2015] W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "One line of work has tackled the problem using partially observable Markov decision processes and reinforcement learning with carefully designed action spaces (Young et al., 2013).", "startOffset": 159, "endOffset": 179}, {"referenceID": 19, "context": "the recurrent unit (Hochreiter and Schmidhuber, 1997) with a bias of 1 added to the forget gate in the style of (Zaremba et al., 2015).", "startOffset": 112, "endOffset": 134}, {"referenceID": 3, "context": "We train using a cross-entropy loss and the Adam optimizer (Kingma and Ba, 2015), applying dropout (Hinton et al., 2012) as a regularizer to", "startOffset": 99, "endOffset": 120}, {"referenceID": 2, "context": "For our experiments, we used dialogues extracted from the Dialogue State Tracking Challenge 2 (DSTC2) (Henderson et al., 2014), a restaurant reservation system dataset.", "startOffset": 102, "endOffset": 126}, {"referenceID": 2, "context": "For our experiments, we used dialogues extracted from the Dialogue State Tracking Challenge 2 (DSTC2) (Henderson et al., 2014), a restaurant reservation system dataset. While the goal of the original challenge was building a system for inferring dialogue state, for our study, we use the version of the data from Bordes and Weston (2016), which ignores the dialogue state annotations, using only the raw text of the dialogues,", "startOffset": 103, "endOffset": 338}, {"referenceID": 8, "context": "ficult (Liu et al., 2016).", "startOffset": 7, "endOffset": 25}, {"referenceID": 11, "context": "\u2022 BLEU: We use the BLEU metric, commonly employed in evaluating machine translation systems (Papineni et al., 2002), which has also been used in past literature for evaluating dialogue systems (Ritter et al.", "startOffset": 92, "endOffset": 115}, {"referenceID": 12, "context": ", 2002), which has also been used in past literature for evaluating dialogue systems (Ritter et al., 2011; Li et al., 2015).", "startOffset": 85, "endOffset": 123}, {"referenceID": 7, "context": ", 2002), which has also been used in past literature for evaluating dialogue systems (Ritter et al., 2011; Li et al., 2015).", "startOffset": 85, "endOffset": 123}, {"referenceID": 13, "context": "We also include the model of (Liu and Perez, 2016), referred to as GMemNN, and the model of (Seo et al., 2016), referred to as QRN, which currently is state-of-the-art.", "startOffset": 92, "endOffset": 110}], "year": 2017, "abstractText": "Task-oriented dialogue focuses on conversational agents that participate in userinitiated dialogues on domain-specific topics. In contrast to chatbots, which simply seek to sustain open-ended meaningful discourse, existing task-oriented agents usually explicitly model user intent and belief states. This paper examines bypassing such an explicit representation by depending on a latent neural embedding of state and learning selective attention to dialogue history together with copying to incorporate relevant prior context. We complement recent work by showing the effectiveness of simple sequence-to-sequence neural architectures with a copy mechanism. Our model outperforms more complex memory-augmented models by 7% in per-response generation and is on par with the current state-of-the-art on DSTC2.", "creator": "LaTeX with hyperref package"}}}