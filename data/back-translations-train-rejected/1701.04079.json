{"id": "1701.04079", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2017", "title": "Agent-Agnostic Human-in-the-Loop Reinforcement Learning", "abstract": "Providing Reinforcement Learning agents with expert advice can dramatically improve various aspects of learning. Prior work has developed teaching protocols that enable agents to learn efficiently in complex environments; many of these methods tailor the teacher's guidance to agents with a particular representation or underlying learning scheme, offering effective but specialized teaching procedures. In this work, we explore protocol programs, an agent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is to incorporate the beneficial properties of a human teacher into Reinforcement Learning without making strong assumptions about the inner workings of the agent. We show how to represent existing approaches such as action pruning, reward shaping, and training in simulation as special cases of our schema and conduct preliminary experiments on simple domains.", "histories": [["v1", "Sun, 15 Jan 2017 17:14:40 GMT  (2091kb,D)", "http://arxiv.org/abs/1701.04079v1", "Presented at the NIPS Workshop on the Future of Interactive Learning Machines, 2016"]], "COMMENTS": "Presented at the NIPS Workshop on the Future of Interactive Learning Machines, 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["david abel", "john salvatier", "reas stuhlm\\\"uller", "owain evans"], "accepted": false, "id": "1701.04079"}, "pdf": {"name": "1701.04079.pdf", "metadata": {"source": "CRF", "title": "Agent-Agnostic Human-in-the-Loop Reinforcement Learning", "authors": ["David Abel", "John Salvatier"], "emails": ["david_abel@brown.edu", "jsalvatier@gmail.com", "andreas@stuhlmueller.org", "owaine@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "A key goal of Reinforcement Learning (RL) is to design agents that learn in a fully autonomous manner. An engineer designs a reward function, input / output channels, and learning algorithm, and then, apart from debugging, the engineer does not have to intervene during the actual learning process. However, fully autonomous learning is often unfeasible due to the complexity of real-world problems, the difficulty of determining reward functions, and the presence of potentially dangerous outcomes that impede exploration. Human engineers create a curriculum that moves the agent between simulation, real-world environments, and real-world home environments. Over time, they can optimize reward functions, heuristics, and government or action portrayals. They can intervene directly in real-world training to prevent the robot from damaging itself, destroying valuable goods, or harming people who interact with it."}, {"heading": "1.1 Our contribution: agent-agnostic guidance of RL algorithms", "text": "Our goal is to develop a framework for human interaction that is (a) huge and (b) has a wide range of possibilities that can help a person find an RL agent. Such an attitude is informative for the structure of a general faculty, the relationship between the teacher and the teacher, as well as for the effectiveness of new teaching methods, which we will discuss in Section 6. Approaching a person in the RL loop can also help illustrate the relationship between the teacher and the teacher."}, {"heading": "2 Framework", "text": "Any system for RL with a human in the loop must coordinate three components: 1. The environment is an MDP and is specified by a tuple M = (S, A, T, R, \u03b3), where S is the state space, A is the action space, T: S \u00b7 A \u00b7 S 7 \u2192 [0, 1], the transition function, a probability distribution to states underlying a state and action, R: S \u00b7 A 7 \u2192 R is the reward function, and \u03b3 is the discount factor. 2. The actor is a (statutic, potentially stochastic) function L: S \u00b7 R \u2192 A.3. Man can receive and send information of a flexible type, say Xin and Xout, so we treat man as a (statutic, potentially stochastic) function H: Xin \u2192 Xin could contain the history of actions, states and rewards that are so far apart, and a new proposed action could be an action as good."}, {"heading": "3 Capturing Existing Advice Schemes", "text": "Of course, protocol programs cannot cover all consulting protocols. Any protocol that depends on prior knowledge of the learning algorithm, presentation, priors, or hyperparameters of the agent can include a set of existing protocols in which a person leads an agent in the loop. (Figure 1 shows that a person can manipulate the actions (A) that are sent into the environment, the observed states of the agent (S), and observed rewards (R). This refers to the following combinatorial sets of protocol families in which a person manipulates one or more of these components to influence learning: {S, A, R), (A, R), (R), (R, R). The first three elements of the set correspond to state manipulation, action principles, and reward protocols families."}, {"heading": "3.1 Reward shaping", "text": "In Section 2, the reward function R was defined as part of the MDP M. Although humans generally do not design the environment, we design reward functions. Normally, the reward function is hand-coded prior to learning and must assign precise reward values to each state that the agent could reach. An alternative is to let a human interactively generate the rewards: Man observes the state and action and returns a scalar to the agent. This setup has been studied in the work on TAMER [23]. A similar setup (with an agent-specific protocol) was applied by Daniel et al. [6] to robotics. It is easy to represent rewards generated interactively (or online) using log programs. We will now turn to other protocols in which humans manipulate rewards, starting from a fixed reward function that is part of the MDP M."}, {"heading": "3.1.1 Reward shaping and Q-value initialization", "text": "In the Reward Formation protocols, the human engineer changes the rewards that are given through a fixe reward function to influence the learning of an agent. Ng et al. [32] introduced a potentially-based design, the rewards form without the optimal policy of an MDP. In particular, each reward that is received from the environment becomes modified by a design function: F (s), n (s), n (s), n (s), n (s), n???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "3.2 Training in Simulation", "text": "Algorithm 5 (Figure 2) shows how the training process in the simulation can be represented as a protocol program. We let M represent the real decision problem and let M * be a simulator for M included in the protocol program. First, the protocol program lets Agent L interact with M * while the human observes the interaction. If the human decides that the agent is ready, the protocol program interacts instead with M."}, {"heading": "3.3 Action Pruning", "text": "In Section 5, we apply action trunks to prevent catastrophic results during research, a problem identified by Lipton et al. [27], Garcia and Fernandez [14, 15], Hans et al. [18], Moldova and Abbeel [31]. Protocol programs allow interactive conduct of action samples. Instead of having to decide which actions should be curtailed before learning, man can wait to observe the states that actually encounter the agent, which can be valuable in cases where man has limited knowledge of the environment or the agent's ability to learn. In Section 4, we show an agnostic protocol for interactive treatment of actions that deems human activity unnecessary."}, {"heading": "3.4 Manipulating state representation", "text": "Suppose the states of MDP M consist of a number of characteristics that define a state vector. A human engineer can specify a state representation in such a way that the agent always receives \u03c6 (s) = s instead of this vector. Such mappings are used to determine high-level state characteristics that are important for learning, or to dynamically ignore confusing characteristics of the agent. This transformation of the state vector is usually fixed before learning. A protocol program can allow humans to interactively provide processed states or high-level characteristics. By the time humans stop providing traits, the agent may have learned to create them himself (as in Learning with Privileged Information [45, 35]). Other methods focus on state abstraction functions to reduce learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13]."}, {"heading": "4 Theory", "text": "Here we illustrate some simple ways in which our proposed agent-agnostic interaction scheme captures other existing agent-agnostic protocols. The following results apply to all tabular MDPs, but should also provide intuition for high-dimensional or continuous environments."}, {"heading": "4.1 Reward Shaping", "text": "Note 1: For each reward-shaping function F, including potential-based design, potential-based counseling, and dynamic potential-based counseling, there is a protocol that produces the same rewards. To construct such a protocol for a particular F, simply have the reward output of the protocol r assume the value F (s) + r at each step. That is, in algorithm 4, simply define H (s, r) = F (s) + r."}, {"heading": "4.2 Action Pruning", "text": "We now show that there is a simple class of protocol programs in which the partial (but useful) problem of approximate interest in a certain function Q = \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 Q = \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 Q = \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 (\u03b2 \u03b2 \u03b2) Q (\u03b2 \u03b2 \u03b2) Q (\u03b2 \u03b2 \u03b2) Q (\u03b2 \u03b2 \u03b2) Q (\u03b2 \u03b2) Q (\u03b2 \u03b2) Q (\u03b2 \u03b2) Q (\u03b2) Q (\u03b2) Q (\u03b2) Q (\u03b2) Q (\u03b2) Q (\u03b2) Q (\u03b2) S (\u03b2) S) S (S) S (S) S \"S) S (S) S\" S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S) S (S) S (S) S) S (S) S (S) S (S) S (S) S) S (S) S (S) S (S) S (S) S (S) S) S (S) S (S) S (S) S (S) S (S) S (S) S (S) S) S (S (S) S) S (S) S (S (S) S (S) S) S (S (S) S) S (S (S) S) S (S) S (S) S (S) S (S (S) S (S (S) S (S) S (S) S (S (S) S (S) S (S (S) S (S) S) S (S (S (S (S) S (S) S) S (S (S (S) S (S (S) S (S) S \"S (S) S) S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S (S"}, {"heading": "5 Experiments", "text": "This section applies our protocols to the action section (section 3.3 and notes 2 and 3 above) to specific RL problems. In Experiment 1, the action section is used to prevent the pathogen from attempting catastrophic actions, i.e. to achieve safe exploration. In Experiment 2, the action section is used to accelerate learning."}, {"heading": "5.1 Protocol for Preventing Catastrophes", "text": "The human being in the loop RL can help prevent catastrophic results resulting from ignorance of the dynamics of the environment or the reward function. Our goal in this experiment is to prevent the agent from taking catastrophic action, which is actions in the real world that are so costly that we want the agent never to take action. [3] This notion of catastrophic action is closely related to the ideas in Safe RL [16, 31] and to working on \"significant rare events.\" [34] Section 3.3 describes our program of protocols for preventing catastrophes in finite MDPs through action studs. There are two important elements of this program: 1. When the agent attempts a catastrophic action in states, the agent is prevented from executing the action in the real world, and the agent receives state and the reward: (s, rbad), whereby the negative reward is extreme."}, {"heading": "5.2 Experiment 1: Preventing Catastrophes in a Pong-like Game", "text": "Our disaster prevention protocol is designed for use in the real world. Here, we provide a preliminary test of our protocol in a simple video game. Our protocol treats the RL agent as a black box. To this end, we applied our protocol to an open source implementation of the state-of-the-art RL algorithm \"Trust Region Policy Optimization\" by Duan et al. [11] The environment was Catcher, a simplified version of Pong with non-visual state representation. As there are no catastrophic actions in Catcher, we modified the game to receive a large negative reward if the speed of the paddle exceeds a speed limit. We compare the performance of an agent supported by the protocol (\"Pruned\"), thus preventing the catastrophic actions in Catcher."}, {"heading": "5.3 Protocol for Accelerating Learning", "text": "We also conducted a simple experiment in the taxi area of Dietterich [9]. The taxi problem is a more complex version of the network world: each problem consists of a taxi and a certain number of passengers. The agent directs the taxi to each passenger, collects the passenger, brings him to his destination and drops him off.4 We did not use a real person in the loop. Instead, the agent was blocked by a protocol program that checked whether any action would exceed the speed limit. This is essentially the protocol outlined in Appendix A, but with the classifier trained offline to detect disasters. Future work will test similar protocols on real people. (In this experiment, a human can easily detect catastrophic actions by reading the speed of the passenger directly from the game state.) We use taxis to evaluate the effect of our action protocol to speed up learning in discrete MDPs."}, {"heading": "5.4 Experiment 2: Accelerated Learning in Taxi", "text": "We evaluated Q-Learning [47] and R-MAX [5] with and without action studs in a simple 10 x 10 instance with a passenger. The taxi starts at (1, 1), the passenger at (4, 3) with destination (2, 2). We conducted standard Q-Learning with unbridled exploration at \u03b5 = 0.2 and with R-MAX with a planning horizon of four. Results are shown in Figure 5.Our results suggest that the action studs protocol simplifies the problem for a Q-Learner and dramatically so for R-Max. From the number of episodes assigned, we see that the cut significantly improves the overall cumulative reward achieved; in the case of R-MAX, the agent is able to effectively solve the problem after a small number of episodes. Furthermore, the results suggest that the agentagnostic method of stuffing is effective without having internal access to the agent code."}, {"heading": "6 Conclusion", "text": "We presented an agnostic method to guide Reinforcement Learning Agents. Protocol programs written within this framework apply to all possible RL agents, so that sophisticated schemes for human-agent interaction can be designed in a modular manner, without the need for adaptation to different RL algorithms. We presented some simple theoretical results that draw our method on existing schemes for interactive RL and illustrate the power of the interface in two toy areas. A promising path for future work is dynamic state manipulation protocols that can guide an agent's learning process by gradually obscuring confusing features, highlighting relevant features, or simply reducing the dimensionality of representation. In addition, future work could examine whether certain types of value initialization protocols can be captured by protocol programs, such as the optimistic initialization for arbitrary, formal areas developed by Machado al Learning Space. [29]"}, {"heading": "Acknowledgments", "text": "We thank Shimon Whiteson, James MacGlashan and D. Ellis Herskowitz for their helpful conversations."}, {"heading": "A Protocol program for preventing catastrophes in high-dimensional state spaces", "text": "We provide an informal overview of the disaster prevention protocol program. We focus on the differences between the high-dimensional case and the finite case described in Section 5.1. In the finite case, truncated actions are stored in a table. If man is satisfied that all catastrophic actions are in the table, the monitoring of the agent by the protocol program can be fully automated. Man must be in the loop until the agent has attempted every catastrophic action once - after that, man can \"retire.\" In the infinite case, we replace this table with a supervised classification algorithm. All state actions visited are stored and labeled (\"catastrophic\" or \"not catastrophic\")."}], "references": [{"title": "Goal-based action priors", "author": ["David Abel", "David Ellis Hershkowitz", "Gabriel Barth-Maron", "Stephen Brawner", "Kevin O\u2019Farrell", "James MacGlashan", "Stefanie Tellex"], "venue": "In ICAPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Near optimal behavior via approximate state abstraction", "author": ["David Abel", "D Ellis Hershkowitz", "Michael L. Littman"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Interactive teaching strategies for agent training", "author": ["Ofra Amir", "Ece Kamar", "Andrey Kolobov", "Barbara Grosz"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Ronen I Brafman", "Moshe Tennenholtz"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Active reward learning", "author": ["Christian Daniel", "Malte Viering", "Jan Metz", "Oliver Kroemer", "Jan Peters"], "venue": "In Proceedings of Robotics Science & Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Model reduction techniques for computing approximately optimal solutions for markov decision processes", "author": ["Thomas Dean", "Robert Givan", "Sonia Leach"], "venue": "In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Dynamic potential-based reward shaping", "author": ["Sam Devlin", "Daniel Kudenko"], "venue": "Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["Thomas G Dietterich"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Integrating guidance into relational reinforcement learning", "author": ["Kurt Driessens", "Sa\u0161o D\u017eeroski"], "venue": "Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1604.06778,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Approximate equivalence of Markov decision processes", "author": ["Eyal Even-Dar", "Yishay Mansour"], "venue": "In Learning Theory and Kernel Machines,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Methods for computing state similarity in markov decision processes", "author": ["Norman Ferns", "Pablo Samuel Castro", "Doina Precup", "Prakash Panangaden"], "venue": "Proceedings of the 22nd conference on Uncertainty in artificial intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Safe reinforcement learning in high-risk tasks through policy improvement", "author": ["Javier Garcia", "Fernando Fernandez"], "venue": "IEEE SSCI 2011: Symposium Series on Computational Intelligence", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Safe exploration of state and action spaces in reinforcement learning", "author": ["Javier Garcia", "Fernando Fernandez"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "A Comprehensive Survey on Safe Reinforcement Learning", "author": ["Javier Garcia", "Fernando Fernandez"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Policy Shaping: Integrating Human Feedback with Reinforcement Learning", "author": ["Shane Griffith", "Kaushik Subramanian", "J Scholz"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Safe Exploration for Reinforcement Learning", "author": ["Alexander Hans", "Daniel Schneega\u00df", "Anton Maximilian Sch\u00e4fer", "Steffen Udluft"], "venue": "Proceedings of the 16th European Symposium on Artificial Neural Networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Reinforcement learning for mixed open-loop and closed-loop control", "author": ["Eric A Hansen", "Andrew G Barto", "Shlomo Zilberstein"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1996}, {"title": "State abstraction discovery from irrelevant state variables", "author": ["Nicholas K Jong", "Peter Stone"], "venue": "In IJCAI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Reinforcement Learning Via Practice and Critique Advice", "author": ["Kshitij Judah", "Saikat Roy", "Alan Fern", "Thomas G Dietterich"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Reinforcement learning: A survey", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1996}, {"title": "Interactively shaping agents via human reinforcement: The tamer framework", "author": ["W Bradley Knox", "Peter Stone"], "venue": "In Proceedings of the fifth international conference on Knowledge capture,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Augmenting reinforcement learning with human feedback", "author": ["W Bradley Knox", "Peter Stone"], "venue": "Proceedings of the ICML Workshop on New Developments in Imitation Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Beyond Rewards : Learning from Richer Supervision", "author": ["K.V.N. Pradyot"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Towards a unified theory of state abstraction for mdps", "author": ["Lihong Li", "Thomas J Walsh", "Michael L Littman"], "venue": "In ISAIM,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Combating reinforcement learning\u2019s sisyphean curse with intrinsic fear", "author": ["Zachary C Lipton", "Jianfeng Gao", "Lihong Li", "Jianshu Chen", "Li Deng"], "venue": "arXiv preprint arXiv:1611.01211,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Learning something from nothing: Leveraging implicit human feedback strategies", "author": ["Robert Loftin", "Bei Peng", "James MacGlashan", "Michael L Littman", "Matthew E Taylor", "Jie Huang", "David L Roberts"], "venue": "In Robot and Human Interactive Communication,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Domain-Independent Optimistic Initialization for Reinforcement Learning", "author": ["Marlos C. Machado", "Sriram Srinivasan", "Michael Bowling"], "venue": "AAAI Workshop on Learning for General Competency in Video Games,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Creating advice-taking reinforcement learners", "author": ["Richard Maclin", "Jude W. Shavlik"], "venue": "Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1996}, {"title": "Safe Exploration in Markov Decision Processes", "author": ["Teodor Mihai Moldovan", "Pieter Abbeel"], "venue": "Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Andrew Y Ng", "Daishi Harada", "Stuart Russell"], "venue": "In ICML,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1999}, {"title": "Adaptive aggregation for reinforcement learning in average reward Markov decision processes", "author": ["Ronald Ortner"], "venue": "Annals of Operations Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Alternating optimisation and quadrature for robust reinforcement learning", "author": ["Supratik Paul", "Kamil Ciosek", "Michael A Osborne", "Shimon Whiteson"], "venue": "arXiv preprint arXiv:1605.07496,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "On the Theory of Learnining with Privileged Information", "author": ["Dmitry Pechyony", "Vladimir Vapnik"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "A need for speed: Adapting agent action speed to improve task learning from non-expert humans", "author": ["Bei Peng", "James MacGlashan", "Robert Loftin", "Michael L Littman", "David L Roberts", "Matthew E Taylor"], "venue": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Markov decision processes: discrete stochastic dynamic programming", "author": ["Martin L Puterman"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "What good are actions? accelerating learning using learned action priors", "author": ["Benjamin Rosman", "Subramanian Ramamoorthy"], "venue": "In Development and Learning and Epigenetic Robotics (ICDL),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Improving action selection in mdp\u2019s via knowledge transfer", "author": ["A.A. Sherstov", "P. Stone"], "venue": "In Proceedings of the 20th national conference on Artificial Intelligence,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1998}, {"title": "Virtual humans as centaurs: Melding real and virtual", "author": ["William R Swartout"], "venue": "In International Conference on Virtual, Augmented and Mixed Reality,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Reinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance", "author": ["Andrea Lockerd Thomaz", "Cynthia Breazeal"], "venue": "Aaai,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2006}, {"title": "Teaching on a budget: Agents advising agents in reinforcement learning", "author": ["Lisa Torrey", "Matthew Taylor"], "venue": "In Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Help an agent out: Student/teacher learning in sequential decision tasks", "author": ["Lisa Torrey", "Matthew E. Taylor"], "venue": "Proceedings of the Adaptive and Learning Agents Workshop", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "A new learning paradigm: Learning using privileged information", "author": ["Vladimir Vapnik", "Akshay Vashist"], "venue": "Neural Networks,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2009}, {"title": "Blending Autonomous Exploration and Apprenticeship Learning", "author": ["Thomas J. Walsh", "Daniel Hewlett", "Clayton T Morrison"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2011}, {"title": "Principled methods for advising reinforcement learning agents", "author": ["Eric Wiewiora", "Garrison Cottrell", "Charles Elkan"], "venue": "In ICML,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2003}], "referenceMentions": [{"referenceID": 30, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 22, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 26, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 34, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 40, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 45, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 19, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 15, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 23, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 41, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 21, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 44, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 42, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 28, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 8, "context": "Prior literature has investigated how people can help RL agents learn more efficiently through different methods of interaction [32, 24, 28, 36, 42, 48, 21, 17, 25, 43, 23, 49, 46, 44, 30, 10].", "startOffset": 128, "endOffset": 192}, {"referenceID": 15, "context": "[17] investigate the power of policy advice for a Bayesian Q-Learner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "First, we consider environments where the state is fully observed; that is, the learning agent interacts with a Markov Decision Process (MDP) [37, 22, 40].", "startOffset": 142, "endOffset": 154}, {"referenceID": 20, "context": "First, we consider environments where the state is fully observed; that is, the learning agent interacts with a Markov Decision Process (MDP) [37, 22, 40].", "startOffset": 142, "endOffset": 154}, {"referenceID": 38, "context": "First, we consider environments where the state is fully observed; that is, the learning agent interacts with a Markov Decision Process (MDP) [37, 22, 40].", "startOffset": 142, "endOffset": 154}, {"referenceID": 2, "context": "The agent is not specialized to the protocol, so it is unable to ask the human informative questions as in [4], or will not have an observation model that faithfully represents the process the human uses to generate advice, as in [17, 21].", "startOffset": 107, "endOffset": 110}, {"referenceID": 15, "context": "The agent is not specialized to the protocol, so it is unable to ask the human informative questions as in [4], or will not have an observation model that faithfully represents the process the human uses to generate advice, as in [17, 21].", "startOffset": 230, "endOffset": 238}, {"referenceID": 19, "context": "The agent is not specialized to the protocol, so it is unable to ask the human informative questions as in [4], or will not have an observation model that faithfully represents the process the human uses to generate advice, as in [17, 21].", "startOffset": 230, "endOffset": 238}, {"referenceID": 21, "context": "This setup has been explored in work on TAMER [23].", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[32] introduced potential-based shaping, which shapes rewards without changing an MDP\u2019s optimal policy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[48] showed potential shaping to be equivalent (for Q-learners) to a subset Q-value initialization under some assumptions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Further, Devlin and Kudenko [8] propose dynamic potential shaping functions that change over time.", "startOffset": 28, "endOffset": 31}, {"referenceID": 45, "context": "[48] extend potential shaping to potential-based advice functions, which identifies a similar class of shaping functions on (s, a) pairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "Such techniques have been shown to accelerate learning and planning time [39, 19, 38, 2].", "startOffset": 73, "endOffset": 88}, {"referenceID": 17, "context": "Such techniques have been shown to accelerate learning and planning time [39, 19, 38, 2].", "startOffset": 73, "endOffset": 88}, {"referenceID": 36, "context": "Such techniques have been shown to accelerate learning and planning time [39, 19, 38, 2].", "startOffset": 73, "endOffset": 88}, {"referenceID": 0, "context": "Such techniques have been shown to accelerate learning and planning time [39, 19, 38, 2].", "startOffset": 73, "endOffset": 88}, {"referenceID": 25, "context": "[27], Garcia and Fernandez [14, 15], Hans et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[27], Garcia and Fernandez [14, 15], Hans et al.", "startOffset": 27, "endOffset": 35}, {"referenceID": 13, "context": "[27], Garcia and Fernandez [14, 15], Hans et al.", "startOffset": 27, "endOffset": 35}, {"referenceID": 16, "context": "[18], Moldovan and Abbeel [31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[18], Moldovan and Abbeel [31].", "startOffset": 26, "endOffset": 30}, {"referenceID": 43, "context": "By the time the human stops providing features, the agent might have learned to generate them on its own (as in Learning with Privileged Information [45, 35]).", "startOffset": 149, "endOffset": 157}, {"referenceID": 33, "context": "By the time the human stops providing features, the agent might have learned to generate them on its own (as in Learning with Privileged Information [45, 35]).", "startOffset": 149, "endOffset": 157}, {"referenceID": 24, "context": "Other methods have focused on state abstraction functions to decrease learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13].", "startOffset": 136, "endOffset": 162}, {"referenceID": 31, "context": "Other methods have focused on state abstraction functions to decrease learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13].", "startOffset": 136, "endOffset": 162}, {"referenceID": 10, "context": "Other methods have focused on state abstraction functions to decrease learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13].", "startOffset": 136, "endOffset": 162}, {"referenceID": 18, "context": "Other methods have focused on state abstraction functions to decrease learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13].", "startOffset": 136, "endOffset": 162}, {"referenceID": 5, "context": "Other methods have focused on state abstraction functions to decrease learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13].", "startOffset": 136, "endOffset": 162}, {"referenceID": 1, "context": "Other methods have focused on state abstraction functions to decrease learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13].", "startOffset": 136, "endOffset": 162}, {"referenceID": 11, "context": "Other methods have focused on state abstraction functions to decrease learning time and preserve the quality of learned behavior, as in [26, 33, 12, 20, 7, 3, 13].", "startOffset": 136, "endOffset": 162}, {"referenceID": 14, "context": "This notion of catastrophic action is closely related to ideas in \u201cSafe RL\u201d [16, 31] and to work on \u201csignificant rare events\u201d [34].", "startOffset": 76, "endOffset": 84}, {"referenceID": 29, "context": "This notion of catastrophic action is closely related to ideas in \u201cSafe RL\u201d [16, 31] and to work on \u201csignificant rare events\u201d [34].", "startOffset": 76, "endOffset": 84}, {"referenceID": 32, "context": "This notion of catastrophic action is closely related to ideas in \u201cSafe RL\u201d [16, 31] and to work on \u201csignificant rare events\u201d [34].", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "We also conducted a simple experiment in the Taxi domain from Dietterich [9].", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "4 Experiment 2: Accelerated Learning in Taxi We evaluated Q-learning [47] and R-MAX [5] with and without action pruning in a simple 10\u00d7 10 instance with one passenger.", "startOffset": 84, "endOffset": 87}, {"referenceID": 27, "context": "[29].", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "Lastly, an alternate perspective on the framework is that of a centaur system: a joint Human-AI decision maker [41].", "startOffset": 111, "endOffset": 115}], "year": 2017, "abstractText": "Providing Reinforcement Learning agents with expert advice can dramatically improve various aspects of learning. Prior work has developed teaching protocols that enable agents to learn efficiently in complex environments; many of these methods tailor the teacher\u2019s guidance to agents with a particular representation or underlying learning scheme, offering effective but specialized teaching procedures. In this work, we explore protocol programs, an agent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is to incorporate the beneficial properties of a human teacher into Reinforcement Learning without making strong assumptions about the inner workings of the agent. We show how to represent existing approaches such as action pruning, reward shaping, and training in simulation as special cases of our schema and conduct preliminary experiments on simple domains.", "creator": "LaTeX with hyperref package"}}}