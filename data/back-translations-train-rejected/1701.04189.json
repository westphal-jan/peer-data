{"id": "1701.04189", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2017", "title": "Deep Memory Networks for Attitude Identification", "abstract": "We consider the task of identifying attitudes towards a given set of entities from text. Conventionally, this task is decomposed into two separate subtasks: target detection that identifies whether each entity is mentioned in the text, either explicitly or implicitly, and polarity classification that classifies the exact sentiment towards an identified entity (the target) into positive, negative, or neutral.", "histories": [["v1", "Mon, 16 Jan 2017 06:49:01 GMT  (1136kb,D)", "http://arxiv.org/abs/1701.04189v1", "Accepted to WSDM'17"]], "COMMENTS": "Accepted to WSDM'17", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["cheng li", "xiaoxiao guo", "qiaozhu mei"], "accepted": false, "id": "1701.04189"}, "pdf": {"name": "1701.04189.pdf", "metadata": {"source": "CRF", "title": "Deep Memory Networks for Attitude Identification", "authors": ["Cheng Li", "Xiaoxiao Guo", "Qiaozhu Mei"], "emails": ["lichengz@umich.edu", "guoxiao@umich.edu", "qmei@umich.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "Instead, we show that the identification of attitudes can be solved with an end-to-end machine learning architecture, in which the two sub-tasks are linked by a deep memory network. In this way, signals generated during target capture provide clues to polarity classification, and vice versa, the predicted polarity provides feedback for target identification. In addition, target group treatments also influence each other - the representations learned may share the same semantics for some targets, but vary for others. The proposed deep memory network, AttNet, exceeds methods that do not take into account the interactions between sub-tasks or those between targets, including conventional machine learning methods and state-of-the-art deep learning models."}, {"heading": "1. INTRODUCTION", "text": "In fact, it is the case that most of us are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to determine for themselves what they want. (...) In fact, it is the case that they are able to determine for themselves. (...) In fact, it is the case that they do not want it. (...) In fact, it is the case that they do not want it. (...) In fact, it is the case that they do not want it. (...) In fact, it is the case that they do not want it. \"(...)"}, {"heading": "2. RELATED WORK", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "3. ATTNET FOR ATTITUDE IDENTIFICATION", "text": "We propose an end-to-end neural network model to link the target detection task and the polarity classification task. \u2022 The target detection task is to determine whether a particular target occurs in a particular context, either explicitly or implicitly. \u2022 The polarity classification task is to decide the attitude of the given context towards the specific target when the target occurs in context. Formally, a target detection classifier is a function that translates target pairs and contexts into binary labels (context, goal). A polarity classifier is a functional mapping pair of targets and contexts in three settings labels, (context, goal) \u2192 positive, negative, neutral}. Considering a context where everyone has weapons, there would be only chaos, and a goal, polarity control, the right designation for the target detection task is available and positive for the polarity classification."}, {"heading": "3.1 Background: Memory Networks", "text": "As one of the most recent developments in deep learning, memory networks [35] have been successfully applied to speech modeling, question answering and sentiment analysis at the aspect level [36], which produce better performance compared to alternative deep learning methods, such as LSTM. Faced with a context (or document, such as \"we have been waiting for food for an hour\") and a target (such as service), a memory network layer converts context into vector representation by calculating a weighted sum of context word vector representations. Weight is a score measuring the relevance between the context word and the target (such as a higher score between the words Wait and the service). Vector representation of the context is then passed on to a classifier for target recognition or polarity classification. An attractive feature is that all parameters, including target embedding, context folding and end-to-end-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-"}, {"heading": "3.2 Single Layer AttNet", "text": "We start by describing AttNet in the single-layer vector in context and showing the number of words in the simplicity of context. We point to the task of target capture as TD, and the polarity classification as PC. (1) Target capture is presented as a single-layer vector in which the number of targets is the number of targets. (2) The input representation and attention for TD. We calculate the results between the context (or document) and the target for content-based addressing. The context is first converted into a sequence of uniform vector, {xi RNvoc}, where xi is the most uniform vector for the most uniform vector."}, {"heading": "3.3 Multiple Layer AttNet", "text": "Figure 2 shows a three-layered version of our model q = q. The layers are stacked in the following way: functionality of each layer. For TD, input to the (k + 1) th layer is the sum of output otk = q and input uk from the k-th layer, followed by a sigmoid nonlinearity: uk + 1 = \u03c3 (Htuk + otk) th layer, where \u03c3 (x) = 1 + exp (x))) is the sigmoid function and Ht is a learnable linear mapping matrix divided across layers. For the PC task, input to the first layer is the transformed sum of the last layer of the TD module, z1 = HtuKt + otKt is a learnable linear mapping matrix. For the PC task, input to the first layer is the transformed sum of the last layer of the TD module."}, {"heading": "3.4 End-to-End Multi-Task Training", "text": "We use cross entropy losses to train our model end-to-end, because a set of training data {cti, qj, gtij, gpij}, where cti is the i-th context (or document), qj is the j-th target, gtij and g p ij are the ground truth labels for TD and PC tasks, respectively, where cti is to minimize the objective function: L = \u2212 \u2211 i \u2211 j (log (ytij (g t ij)) + 1gtij log (y p ij (g p ij (g p ij (g p ij (g p ij))))), where ytij is a vector of the predicted probability for each class of TD, ytij (s) chooses the s-th element of yt ij, 1gtij corresponds to 1, if g t ij is equal to the current class and 0 otherwise. Note that if a target is not mentioned in a given context, the value of polarity is not equal to 1j."}, {"heading": "4. EXPERIMENT SETUP", "text": "In the experiments, we compare AttNet with conventional approaches and alternative deep learning approaches based on three real data sets and demonstrate the superior performance of our model. We also experiment with variants of AttNet as lending for the key components of our model."}, {"heading": "4.1 Data Sets", "text": "We are investigating AttNet in three areas related to the classification of attitudes: online debates (debates), multi-aspect sentiment analysis on product reviews (reviews) and attitudes in tweets (tweets).Debating. This dataset is taken from the Internet Argument Corpus version 22. The dataset consists of political debates in three Internet forums. In these forums, a person can trigger a debate by posting a topic and positions such as favoritism. Examples of topics are gun control, capital punishment and abortion. Other users participate in these debates by posting their arguments in favor of one of the sites.Tweets. This dataset comes from a task at the SemEval2016 workshop on recognizing attitudes from tweets [24]. The goals relate largely to ideology, e.g. atheism and feminist movement4.Review. This dataset includes ratings of restaurants and laptops from SemEval 2014 [29] and 2015 [28], where tasks are given to identify and classify aspects."}, {"heading": "4.2 Metrics", "text": "For our problem, each data set has several targets, and each target can be divided into one of the results: absent (there is no such target), neutral, positive and negative. If we treat each result of each target as a category, we can apply common metrics for multi-class classification. As most targets do not occur in most cases, we have a greatly distorted class distribution, where metrics such as accuracy are not a good choice [3]. Apart from precision, recall and AUC, we also use the macro-average F metric [44]. Let us remember and precision for a certain category i, \u03c1i = TPiTPi + FNi, \u03c0i = TPi TPi + FPi, where TPi, FPi, FNi is the number of true positive, false positive and false negative values for category i. Given that the F metrics of category i are calculated as Fi = 2cepiciency equivalents, the macro-accuracy is also achieved over the final metrics F."}, {"heading": "4.3 Baselines", "text": "Spain, Spain, Spain, Spain, Spain, Italy, Spain, Spain, Spain, Spain, Spain, Italy, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Spain, Italy, Spain, Spain, Italy, Spain, Italy, Spain, Spain, Italy, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Spain, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Spain, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy,"}, {"heading": "4.4 Variants of AttNet", "text": "In contrast to our proposed model, AttNet replaces the target-specific projection matrix Vpq and Vtq with the identity matrix and fixes it during training. Thus, the AttNet model leaves target recognition and polarity classification to the task, but does not take into account the interactions between targets. We refer to our proposed model as AttNet, which makes it possible to learn the projection matrices during training, and therefore word semantics may vary for the target groups. AttNet reports two settings in our experiments: AttNetind and AttNet-all. The former results in all targets sharing the same embedding, while the latter completely separates the embedding space for each target, i.e. the targets are trained on separate models, i.e. the embedding size is set to 100 for all datasets. The sliding window size of the moving average function in Equation 1 is set to 3. # Layers (Target) is the number of memory layers for attention."}, {"heading": "4.5 Training Details", "text": "All hyperparameters are set in such a way that the best performance of the F-score is achieved during validation. The embedding size of the candidate is in the LSTM-related methods SVM and CNN {50, 100, 150}. The relaxation parameter C of the SVM model is {27, 26,..., 2 \u2212 3}. The CNN model has three revolutionary filter sizes and its filter sizes are candidates {1, 2, 3, 4}, {3, 4, 5}, {2, 4, 6}, and the number of filters of the candidate is {50, 100, 200, 300}. For ParaVec we experiment with both models with skipping programs or word bag models and select the hidden layer size from {26, 27,..., 210}."}, {"heading": "5. EXPERIMENT RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Overall Performance", "text": "The overall performance of all competing methods on datasets is evaluated in Table 38. Evaluated with F-Score and AUC, we make the following observations. Our AttNet method clearly outperforms all competing methods, empirically confirming that linking target detection and polarity classification partial tasks in combination with target-specific representations can benefit from bearing identification. The variants of our model, AttNet-all and AttNet-ind, have already achieved significant improvements over the strongest baselines in all datasets. More importantly, the two methods significantly outperform the Memnet-sep-all and Memnet-sep-all baselines, which do not overlap the sub-tasks. Such empirical results shed light on the fact that linking the sub-tasks actually improves bearing identification performance. In contrast, the separation of the two sub-tasks of bearing identification into performance degenerations.Our AttNet model also outperforms its variants Net-all and Attind-all datasets, in all."}, {"heading": "5.2 Performance on Subtasks", "text": "We have found that our models outperform competing methods in all data sets. To further assign credits to improving our methods, we evaluate our models on the basis of two subtasks: target recognition and polarity classification, with the results given in Table 4 and 5, respectively. As different configurations of the same method work similarly, we only present the results if separate models are trained for each task. Table 4 shows that the target recognition task is relatively simple as all methods can achieve quite high scores, which also means that it is difficult to make further improvements to this task. In terms of precision and retrieval, SVM performs reasonably well on precision measurement metrics, especially on evaluation data. While most deeper learning methods focus more on improving memory, most deeper learning methods are still better, as the F score shows that the second task is evaluated only on documents with a basic sense of truth about specific goals, with F scores across all target classes being positive: negative and neutral."}, {"heading": "5.3 Training Time Analysis", "text": "To measure the training speed of each model, we train all deep learning methods on a server with a single TITAN X GPU. For SVM, it is trained on the same server with a 2.40 GHz CPU and 120G RAM. All methods are trained sequentially without parallelism. SVM can finish training in less than an hour, but the training time required increases linearly as the number of targets increases. For all deep learning methods, the number of periods required for training is generally very close, which is about 20 epochs on average of all data seconds. Comparing training time per epoch, ParaVec and CNN are much faster than other methods (less than 5 seconds / epoch). Despite training efficiency, their effectiveness is a problem. If all targets share a single model, LSTM has a speed of 200 seconds / epoch, while standard storage networks have a speed of 150 seconds / epoch."}, {"heading": "5.4 Visualization of attention", "text": "To better understand the behavior of our models, we compare the attention weights of our AttNets model with the competing Memnet method."}, {"heading": "2. Highly impressed from the decor to the food to the great night ! (Truth: service+, ambience+, food+. Predict + given ambience.)", "text": "Figure 3 (a) and (b) lists some examples of word attention generated by different models for the same sentences in the test sentence. In the first sentence, both they and weapons are found by AttNets as targets, while words such as chaos and politics are found as sentiment words. Although Memnet correctly identifies the existence of attitudes toward gun control, it does not find important words to classify the polarity of mood, which suggests how important it is to leave the two tasks - the successful identification of aforementioned targets could shed light on finding sentiment words for the second task. The second sentence comes from a review of a restaurant when Ambience is used as a query target. We can see that the AttNets Target Detection Module captures the word decor that signals the presence of the target environment, and the polarity classification module then focuses on extracting the senses associated with the target."}, {"heading": "6. CONCLUSION", "text": "The identification of attitudes, a key problem of morally natural language processing, concerns the recognition of one or more target units from the text and the subsequent classification of polarity between target units. This problem is addressed conventionally by solving the two sub-tasks separately and usually treating each target separately, thereby not effectively utilizing the interaction between the two sub-tasks and the interaction between the target units. Our study shows that modelling these interactions in a carefully designed, end-to-end deep memory network significantly improves the accuracy of the two sub-tasks, target detection and polarity classification, as well as the identification of attacks as the whole. Empirical experiments show that this novel model surpasses models that do not take into account the interactions between the two sub-tasks or between the targets, including conventional methods and state-of-the-art deep learning models. This work opens the exploration of interactions between sub-tasks and between contexts (in our case) for the sensitivity analysis of end-to-end-feel aspects of such an architecture."}, {"heading": "Acknowledgment", "text": "This work is partially supported by the National Science Foundation under grant numbers IIS-1054199 and SES-1131500."}, {"heading": "7. REFERENCES", "text": "[1] D. Bespalov, B. Bai, Y. Qi, and A. Shokoufandeh. Networks dependent M. Sentimentclassification based on supervised latent n-gram analysis. In Proc. of CIKM, 2011. [2] C.-C. Chang and C.-J. Lin. Libsvm: a library for support vector machines. TIST, 2011. [3] N. V. Chawla. Data mining for imbalanced datasets. An overview. In Data mining and knowledge discovery handbook. 2005. [4] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. JMLR, 2011. De Marneffe, B. MacCartney, C. D. Manning, et al. Generating ed dependency parses. In phrase structure parses. In Proc. of LREC, 2006. [6] L. Dong, F. i Wei, Tang D. Netzwerk."}], "references": [{"title": "Sentiment classification based on supervised latent n-gram analysis", "author": ["D. Bespalov", "B. Bai", "Y. Qi", "A. Shokoufandeh"], "venue": "In Proc. of CIKM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Libsvm: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "TIST,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Data mining for imbalanced datasets: An overview. In Data mining and knowledge discovery handbook", "author": ["N.V. Chawla"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["M.-C. De Marneffe", "B. MacCartney", "C.D. Manning"], "venue": "In Proc. of LREC,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Adaptive recursive neural network for target-dependent twitter sentiment classification", "author": ["L. Dong", "F. Wei", "C. Tan", "D. Tang", "M. Zhou", "K. Xu"], "venue": "In ACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Automated classification of stance in student essays: An approach using stance target information and the wikipedia link-based measure", "author": ["A. Faulkner"], "venue": "In FLAIRS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Part-of-speech tagging for twitter: Annotation, features, and experiments", "author": ["K. Gimpel", "N. Schneider", "B. O\u2019Connor", "D. Das", "D. Mills", "J. Eisenstein", "M. Heilman", "D. Yogatama", "J. Flanigan", "N.A. Smith"], "venue": "In Proc. of ACL,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proc. of ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Stance classification of ideological debates: Data, models, features, and constraints", "author": ["K.S. Hasan", "V. Ng"], "venue": "In IJCNLP,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["K.M. Hermann", "P. Blunsom"], "venue": "In ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "In Proc. of SIGKDD,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["O. Irsoy", "C. Cardie"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Target-dependent twitter sentiment classification", "author": ["L. Jiang", "M. Yu", "M. Zhou", "X. Liu", "T. Zhao"], "venue": "In Proc. of ACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Opinion mining on the web by extracting subject-aspect-evaluation relations", "author": ["N. Kobayashi", "R. Iida", "K. Inui", "Y. Matsumoto"], "venue": "In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "In ICML,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Structure-aware review mining and summarization", "author": ["F. Li", "C. Han", "M. Huang", "X. Zhu", "Y.-J. Xia", "S. Zhang", "H. Yu"], "venue": "In Proc. of ACL,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Fine-grained opinion mining with recurrent neural networks and word embeddings", "author": ["P. Liu", "S. Joty", "H. Meng"], "venue": "In EMNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Hierarchical multi-label conditional random fields for aspect-oriented opinion mining", "author": ["D. Marcheggiani", "O. T\u00e4ckstr\u00f6m", "A. Esuli", "F. Sebastiani"], "venue": "In ECIR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Topic sentiment mixture: modeling facets and opinions in weblogs", "author": ["Q. Mei", "X. Ling", "M. Wondra", "H. Su", "C. Zhai"], "venue": "In Proc. of WWW,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Semeval-2016 task 6: Detecting stance in tweets", "author": ["S.M. Mohammad", "S. Kiritchenko", "P. Sobhani", "X. Zhu", "C. Cherry"], "venue": "In Proc. of SemEval,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Stance and sentiment in tweets", "author": ["S.M. Mohammad", "P. Sobhani", "S. Kiritchenko"], "venue": "arXiv preprint arXiv:1605.01655,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon", "author": ["S.M. Mohammad", "P.D. Turney"], "venue": "In Proc. of NAACL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Opinion mining and sentiment analysis", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and trends in information retrieval,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Semeval-2015 task 12: Aspect based sentiment analysis", "author": ["M. Pontiki", "D. Galanis", "H. Papageorgiou", "S. Manandhar", "I. Androutsopoulos"], "venue": "In Proc. of SemEval,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Semeval-2014 task 4: Aspect based sentiment analysis", "author": ["M. Pontiki", "D. Galanis", "J. Pavlopoulos", "H. Papageorgiou", "I. Androutsopoulos", "S. Manandhar"], "venue": "In Proc. of SemEval,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Dancing with the stars, nba games, politics: An exploration of twitter users\u00e2\u0102\u0179 response to events", "author": ["A. Popescu", "M. Pennacchiotti"], "venue": "In Proc. of ICWSM,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Identifying users with opposing opinions in twitter debates", "author": ["A. Rajadesingan", "H. Liu"], "venue": "In Social Computing, Behavioral-Cultural Modeling and Prediction", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Automatic aggregation by joint modeling of aspects and values", "author": ["C. Sauper", "R. Barzilay"], "venue": "JAIR,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "In Proc. of EMNLP-CoNLL,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "In Proc. of EMNLP,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "J. Weston", "R. Fergus"], "venue": "In NIPS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Aspect level sentiment classification with deep memory network", "author": ["D. Tang", "B. Qin", "T. Liu"], "venue": "In EMNLP,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Learning sentiment-specific word embedding for twitter sentiment classification", "author": ["D. Tang", "F. Wei", "N. Yang", "M. Zhou", "T. Liu", "B. Qin"], "venue": "In ACL,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Target-dependent twitter sentiment classification with rich automatic features", "author": ["D.-T. Vo", "Y. Zhang"], "venue": "In IJCAI,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Stance classification using dialogic properties of persuasion", "author": ["M.A. Walker", "P. Anand", "R. Abbott", "R. Grant"], "venue": "In Proc. of NAACL,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "That is your evidence?: Classifying stance in online political debate", "author": ["M.A. Walker", "P. Anand", "R. Abbott", "J.E.F. Tree", "C. Martell", "J. King"], "venue": "Decision Support Systems,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}, {"title": "Latent aspect rating analysis without aspect keyword supervision", "author": ["H. Wang", "Y. Lu", "C. Zhai"], "venue": "In Proc. of SIGKDD,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["S. Wang", "C.D. Manning"], "venue": "In Proc. of ACL,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "In Proc. of HLT/EMNLP,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2005}, {"title": "A re-examination of text categorization methods", "author": ["Y. Yang", "X. Liu"], "venue": "In Proc. of SIGIR,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1999}, {"title": "Gated neural networks for targeted sentiment analysis", "author": ["M. Zhang", "Y. Zhang", "D.-T. Vo"], "venue": "In Proc. of AAAI,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "Fine-grained sentiment analysis with structural features", "author": ["C. Zirn", "M. Niepert", "H. Stuckenschmidt", "M. Strube"], "venue": "In IJCNLP,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2011}], "referenceMentions": [{"referenceID": 37, "context": "Indeed, deep learning has recently been applied to target-specific sentiment analysis (mostly the second subtask of attitude identification) and achieved promising performance, where a given target is assumed to have appeared exactly once in a piece of text and the task is to determine the polarity of this text [38, 45, 36].", "startOffset": 313, "endOffset": 325}, {"referenceID": 44, "context": "Indeed, deep learning has recently been applied to target-specific sentiment analysis (mostly the second subtask of attitude identification) and achieved promising performance, where a given target is assumed to have appeared exactly once in a piece of text and the task is to determine the polarity of this text [38, 45, 36].", "startOffset": 313, "endOffset": 325}, {"referenceID": 35, "context": "Indeed, deep learning has recently been applied to target-specific sentiment analysis (mostly the second subtask of attitude identification) and achieved promising performance, where a given target is assumed to have appeared exactly once in a piece of text and the task is to determine the polarity of this text [38, 45, 36].", "startOffset": 313, "endOffset": 325}, {"referenceID": 20, "context": "Aspects are often assumed to be mentioned explicitly in text, so that the related entities can be extracted through supervised sequence labeling methods [21, 19, 46]; aspects mentioned implicitly can be extracted as fuzzy representaar X iv :1 70 1.", "startOffset": 153, "endOffset": 165}, {"referenceID": 18, "context": "Aspects are often assumed to be mentioned explicitly in text, so that the related entities can be extracted through supervised sequence labeling methods [21, 19, 46]; aspects mentioned implicitly can be extracted as fuzzy representaar X iv :1 70 1.", "startOffset": 153, "endOffset": 165}, {"referenceID": 45, "context": "Aspects are often assumed to be mentioned explicitly in text, so that the related entities can be extracted through supervised sequence labeling methods [21, 19, 46]; aspects mentioned implicitly can be extracted as fuzzy representaar X iv :1 70 1.", "startOffset": 153, "endOffset": 165}, {"referenceID": 21, "context": "tions through unsupervised methods such as topic models [22, 41, 32].", "startOffset": 56, "endOffset": 68}, {"referenceID": 40, "context": "tions through unsupervised methods such as topic models [22, 41, 32].", "startOffset": 56, "endOffset": 68}, {"referenceID": 31, "context": "tions through unsupervised methods such as topic models [22, 41, 32].", "startOffset": 56, "endOffset": 68}, {"referenceID": 16, "context": "While unsupervised methods suffer from low accuracy, it is usually difficult for supervised methods, like support vector machines (SVMs) [17], to interleave aspect extraction and sentiment classification.", "startOffset": 137, "endOffset": 141}, {"referenceID": 26, "context": "Sentiment analysis has been a very active area of research [27, 30].", "startOffset": 59, "endOffset": 67}, {"referenceID": 29, "context": "Sentiment analysis has been a very active area of research [27, 30].", "startOffset": 59, "endOffset": 67}, {"referenceID": 13, "context": "[14] developed seven rule-based target-dependent features, which are fed to an SVM classifier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] proposed an adaptive recursive neural network that propagates sentiment signals from sentiment-baring words to specific targets on a dependence tree.", "startOffset": 0, "endOffset": 3}, {"referenceID": 37, "context": "[38] split a Tweet into a left context and a right context according to a given target, and used pre-trained word embeddings and neural pooling functions to extract features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[45] extended this idea by using gated recursive neural networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36], which applied Memory Networks [35] to the task of multi-aspect sentiment analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[36], which applied Memory Networks [35] to the task of multi-aspect sentiment analysis.", "startOffset": 36, "endOffset": 40}, {"referenceID": 39, "context": "They utilized various features based on n-grams, part of speech, syntactic rules, and dialogic relations between posts [40, 10, 7, 31].", "startOffset": 119, "endOffset": 134}, {"referenceID": 9, "context": "They utilized various features based on n-grams, part of speech, syntactic rules, and dialogic relations between posts [40, 10, 7, 31].", "startOffset": 119, "endOffset": 134}, {"referenceID": 6, "context": "They utilized various features based on n-grams, part of speech, syntactic rules, and dialogic relations between posts [40, 10, 7, 31].", "startOffset": 119, "endOffset": 134}, {"referenceID": 30, "context": "They utilized various features based on n-grams, part of speech, syntactic rules, and dialogic relations between posts [40, 10, 7, 31].", "startOffset": 119, "endOffset": 134}, {"referenceID": 23, "context": "The workshop SemEval-2016 presented a task on detecting stance from tweets [24], where an additional category is added for the given target, indicating the absence of sentiment towards the target.", "startOffset": 75, "endOffset": 79}, {"referenceID": 24, "context": "[25] beat all teams by building an SVM classifier for each target.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] extracted product aspects via association mining, and expanded seed opinion terms by using synonyms and antonyms in WordNet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "When supervised learning approaches are taken, both tasks of aspect extraction and polarity classification can be cast as a binary classification problem [17], or as a sequence labeling task and solved using sequence learning models such as conditional random fields (CRFs) [21, 19] or hidden Markov models (HMMs) [46].", "startOffset": 154, "endOffset": 158}, {"referenceID": 20, "context": "When supervised learning approaches are taken, both tasks of aspect extraction and polarity classification can be cast as a binary classification problem [17], or as a sequence labeling task and solved using sequence learning models such as conditional random fields (CRFs) [21, 19] or hidden Markov models (HMMs) [46].", "startOffset": 274, "endOffset": 282}, {"referenceID": 18, "context": "When supervised learning approaches are taken, both tasks of aspect extraction and polarity classification can be cast as a binary classification problem [17], or as a sequence labeling task and solved using sequence learning models such as conditional random fields (CRFs) [21, 19] or hidden Markov models (HMMs) [46].", "startOffset": 274, "endOffset": 282}, {"referenceID": 45, "context": "When supervised learning approaches are taken, both tasks of aspect extraction and polarity classification can be cast as a binary classification problem [17], or as a sequence labeling task and solved using sequence learning models such as conditional random fields (CRFs) [21, 19] or hidden Markov models (HMMs) [46].", "startOffset": 314, "endOffset": 318}, {"referenceID": 21, "context": "Unsupervised learning approaches like topic modeling treat aspects as topics, so that topics and sentiment polarity can be jointly modeled [22, 41, 32].", "startOffset": 139, "endOffset": 151}, {"referenceID": 40, "context": "Unsupervised learning approaches like topic modeling treat aspects as topics, so that topics and sentiment polarity can be jointly modeled [22, 41, 32].", "startOffset": 139, "endOffset": 151}, {"referenceID": 31, "context": "Unsupervised learning approaches like topic modeling treat aspects as topics, so that topics and sentiment polarity can be jointly modeled [22, 41, 32].", "startOffset": 139, "endOffset": 151}, {"referenceID": 27, "context": "The workshop of SemEval-2015 announced a task of aspect based sentiment analysis [28], which separates aspect identification and polarity classification into two subtasks.", "startOffset": 81, "endOffset": 85}, {"referenceID": 0, "context": "[1] used Latent Semantic Analysis to initialize the word embedding, representing each document as the linear combination of n-gram vectors.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] applied Denoising Autoencoders for domain adaptation in sentiment classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 32, "context": "A set of models have been proposed to learn the compositionality of phrases based on the representation of children in the syntactic tree [33, 34, 11].", "startOffset": 138, "endOffset": 150}, {"referenceID": 33, "context": "A set of models have been proposed to learn the compositionality of phrases based on the representation of children in the syntactic tree [33, 34, 11].", "startOffset": 138, "endOffset": 150}, {"referenceID": 10, "context": "A set of models have been proposed to learn the compositionality of phrases based on the representation of children in the syntactic tree [33, 34, 11].", "startOffset": 138, "endOffset": 150}, {"referenceID": 7, "context": ", tweets [8].", "startOffset": 9, "endOffset": 12}, {"referenceID": 19, "context": "[20] used recurrent neural networks to extract explicit aspects in reviews.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Moreover, when a context has multiple targets, the signal words usually cluster for different targets [12, 30].", "startOffset": 102, "endOffset": 110}, {"referenceID": 29, "context": "Moreover, when a context has multiple targets, the signal words usually cluster for different targets [12, 30].", "startOffset": 102, "endOffset": 110}, {"referenceID": 34, "context": "As one of the recent developments of deep learning, memory networks [35] have been successfully applied to language modeling, question answering, and aspect-level sentiment analysis [36], which generates superior performance over alternative deep learning methods, e.", "startOffset": 68, "endOffset": 72}, {"referenceID": 35, "context": "As one of the recent developments of deep learning, memory networks [35] have been successfully applied to language modeling, question answering, and aspect-level sentiment analysis [36], which generates superior performance over alternative deep learning methods, e.", "startOffset": 182, "endOffset": 186}, {"referenceID": 11, "context": "It has been observed that sentiment-baring words are often close to the target [12, 30].", "startOffset": 79, "endOffset": 87}, {"referenceID": 29, "context": "It has been observed that sentiment-baring words are often close to the target [12, 30].", "startOffset": 79, "endOffset": 87}, {"referenceID": 34, "context": "The embedding matrices and projection matrices are constrained to ease training and reduce the number of parameters [35].", "startOffset": 116, "endOffset": 120}, {"referenceID": 23, "context": "This data set comes from a task of the workshop SemEval2016 on detecting stance from tweets [24].", "startOffset": 92, "endOffset": 96}, {"referenceID": 28, "context": "This data set includes reviews of restaurants and laptops from SemEval 2014 [29] and 2015 [28], where subtasks of identifying aspects and classifying sentiments are provided.", "startOffset": 76, "endOffset": 80}, {"referenceID": 27, "context": "This data set includes reviews of restaurants and laptops from SemEval 2014 [29] and 2015 [28], where subtasks of identifying aspects and classifying sentiments are provided.", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "Text pre-processing includes stopword removal and tokenization by the CMU Twitter NLP tool [8].", "startOffset": 91, "endOffset": 94}, {"referenceID": 2, "context": "Since most targets do not appear in most instances, we have a highly skewed class distribution, where measures like accuracy are not good choices [3].", "startOffset": 146, "endOffset": 149}, {"referenceID": 43, "context": "Apart from precision, recall and AUC, we also use the macroaverage F-measure [44].", "startOffset": 77, "endOffset": 81}, {"referenceID": 24, "context": "SVM using a set of hand-crafted features has achieved the state-of-the-art performance in stance classification of SemEval 2016 task [25], online debates [10], and aspect-based sentiment analysis [36].", "startOffset": 133, "endOffset": 137}, {"referenceID": 9, "context": "SVM using a set of hand-crafted features has achieved the state-of-the-art performance in stance classification of SemEval 2016 task [25], online debates [10], and aspect-based sentiment analysis [36].", "startOffset": 154, "endOffset": 158}, {"referenceID": 35, "context": "SVM using a set of hand-crafted features has achieved the state-of-the-art performance in stance classification of SemEval 2016 task [25], online debates [10], and aspect-based sentiment analysis [36].", "startOffset": 196, "endOffset": 200}, {"referenceID": 41, "context": "SVM has also demonstrated superior performance in document-level sentiment analysis compared with conditional random field methods [42].", "startOffset": 131, "endOffset": 135}, {"referenceID": 1, "context": "Therefore we include all features from these methods that are general across domains, and use a linear kernel SVM implemented by LIBSVM [2] for classification.", "startOffset": 136, "endOffset": 139}, {"referenceID": 39, "context": "We insert symbols that represent the start and end of a document to capture cue words [40].", "startOffset": 86, "endOffset": 90}, {"referenceID": 25, "context": "Sentiment: the number of positive and negative words counted from the NRC Emotion Lexicon [26], Hu and Liu Lexicon [12], and the MPQA Subjectivity Lexicon [43].", "startOffset": 90, "endOffset": 94}, {"referenceID": 11, "context": "Sentiment: the number of positive and negative words counted from the NRC Emotion Lexicon [26], Hu and Liu Lexicon [12], and the MPQA Subjectivity Lexicon [43].", "startOffset": 115, "endOffset": 119}, {"referenceID": 42, "context": "Sentiment: the number of positive and negative words counted from the NRC Emotion Lexicon [26], Hu and Liu Lexicon [12], and the MPQA Subjectivity Lexicon [43].", "startOffset": 155, "endOffset": 159}, {"referenceID": 13, "context": "Furthermore, if the target is present, we generate a set of target dependent features according to [14].", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "Syntactic dependency: a set of triples obtained by Stanford dependency parser [5].", "startOffset": 78, "endOffset": 81}, {"referenceID": 38, "context": "Generalized dependency: the first word of the dependency triple is \u201cbacked off\u201d to its part-of-speech tag [39].", "startOffset": 106, "endOffset": 110}, {"referenceID": 38, "context": "Additionally, words that appear in sentiment lexicons are replaced by positive or negative polarity equivalents [39].", "startOffset": 112, "endOffset": 116}, {"referenceID": 37, "context": "Two of them are from studies on target-dependent sentiment classification [38, 45], which are the skip-gram embeddings of Mikilov et al.", "startOffset": 74, "endOffset": 82}, {"referenceID": 44, "context": "Two of them are from studies on target-dependent sentiment classification [38, 45], which are the skip-gram embeddings of Mikilov et al.", "startOffset": 74, "endOffset": 82}, {"referenceID": 22, "context": "[23] and the sentiment-driven embeddings of Tang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] and trained on English Wikipedia.", "startOffset": 0, "endOffset": 3}, {"referenceID": 44, "context": "We also compare to the bidirectional LSTM (BiLSTM) model, the state-of-the-art in target-dependent sentiment classification [45].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "We include the standard multi-layered bidirectional LSTM (MultiBiLSTM) [13] as an extension.", "startOffset": 71, "endOffset": 75}, {"referenceID": 35, "context": "[36] applied memory networks (Memnet) to multi-aspect sentiment analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "We include related deep learning techniques beyond the sentiment analysis domain, such as the convolutional neural networks (CNN) [15] and ParaVec [18].", "startOffset": 130, "endOffset": 134}, {"referenceID": 17, "context": "We include related deep learning techniques beyond the sentiment analysis domain, such as the convolutional neural networks (CNN) [15] and ParaVec [18].", "startOffset": 147, "endOffset": 151}, {"referenceID": 32, "context": "Parser-dependent deep learning methods have also been applied to sentiment analysis [33, 34, 11].", "startOffset": 84, "endOffset": 96}, {"referenceID": 33, "context": "Parser-dependent deep learning methods have also been applied to sentiment analysis [33, 34, 11].", "startOffset": 84, "endOffset": 96}, {"referenceID": 10, "context": "Parser-dependent deep learning methods have also been applied to sentiment analysis [33, 34, 11].", "startOffset": 84, "endOffset": 96}, {"referenceID": 7, "context": "Second, their parsers do not extend to user generated content, such as Tweets and Debates [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 15, "context": "Deep learning models are optimized by Adam [16].", "startOffset": 43, "endOffset": 47}, {"referenceID": 43, "context": "*(**,***) indicate that one method is statistically significantly better or worse than SVM-sep-ind (which is in general the best configuration among all SVM models) according to t-test [44] at the significance level of 0.", "startOffset": 185, "endOffset": 189}], "year": 2017, "abstractText": "We consider the task of identifying attitudes towards a given set of entities from text. Conventionally, this task is decomposed into two separate subtasks: target detection that identifies whether each entity is mentioned in the text, either explicitly or implicitly, and polarity classification that classifies the exact sentiment towards an identified entity (the target) into positive, negative, or neutral. Instead, we show that attitude identification can be solved with an end-to-end machine learning architecture, in which the two subtasks are interleaved by a deep memory network. In this way, signals produced in target detection provide clues for polarity classification, and reversely, the predicted polarity provides feedback to the identification of targets. Moreover, the treatments for the set of targets also influence each other \u2013 the learned representations may share the same semantics for some targets but vary for others. The proposed deep memory network, the AttNet, outperforms methods that do not consider the interactions between the subtasks or those among the targets, including conventional machine learning methods and the state-of-the-art deep learning models.", "creator": "LaTeX with hyperref package"}}}