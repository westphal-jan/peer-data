{"id": "1701.04292", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2017", "title": "Semantic classifier approach to document classification", "abstract": "In this paper we propose a new document classification method, bridging discrepancies (so-called semantic gap) between the training set and the application sets of textual data. We demonstrate its superiority over classical text classification approaches, including traditional classifier ensembles. The method consists in combining a document categorization technique with a single classifier or a classifier ensemble (SEMCOM algorithm - Committee with Semantic Categorizer).", "histories": [["v1", "Mon, 16 Jan 2017 14:02:19 GMT  (82kb,D)", "http://arxiv.org/abs/1701.04292v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["piotr borkowski", "krzysztof ciesielski", "mieczys{\\l}aw a k{\\l}opotek"], "accepted": false, "id": "1701.04292"}, "pdf": {"name": "1701.04292.pdf", "metadata": {"source": "CRF", "title": "Semantic classifier approach to document classification", "authors": ["Piotr Borkowski", "Krzysztof Ciesielski", "Mieczys\u0142aw A. K\u0142opotek"], "emails": ["klopotek@ipipan.waw.pl"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of us are able to play by the rules we have set ourselves."}, {"heading": "1.1 Our contribution", "text": "Our contribution in this paper is: - the construction of a new supervised classifier based on an unattended semantic document categorizer; - the demonstration of the feasibility of the new classifier to bridge the semantic gap between test and training set; - the development of a heterogeneous committee combining classifiers and the semantic classifier."}, {"heading": "2 Previous work", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "3 Our taxonomy-based semantic categorization method", "text": "Our taxonomy-based categorization method SemCat has been described in detail in [1], so we will only present its brief description in the following."}, {"heading": "3.1 Outline of the algorithm", "text": "Suppose we have a taxonomy of categories (a directed acyclic graph with a root category) such as Wikipedia (W) category graph or Medical Subject Headings (MeSH) ontology2. We assume that there are a number of concepts associated with the taxonomy in the following way: Each concept is associated with one or more categories. Each category and concept is provided with a string label. Strings associated with categories are presented as the result of a user. And those associated with concepts are used to map a text of the document into the series of concepts.For the experimental design, we used W category diagram with the concept of W pages. Tags for W categories were their original string names. Set of string tags associated with a single W page: lemmated page name and name of all categories we define from the concept of W pages. Keywords for W categories are their original string names. Set of string tags associated with a single W page: we find lemmatized page names and we remove all common phrasing pages from the document pages, the process we associate the phrase pages with these common categories, in the document pages, in the process we very standardized categories."}, {"heading": "3.2 Similarity measures", "text": "The semantic measures are based on: the unary function IC (information content) and the binary function MSCA (Most Specific Common Abstraction). Your inputs are categories from a taxonomy.2 https: / / www.nlm.nih.gov / mesh / Although superficially similar, our IC definition differs substantially from the one proposed for WordNet. WordNet calculates the IC for concepts based on the number of subordinate concepts. We calculate the IC for categories that belong to subordinate categories. WordNet calculates the IC for concepts based on the number of subordinate concepts (MLP). We calculate the IC for categories based on the number of categories that belong to subordinate categories. So the IC of a category is weighted according to the frequency of their use in the language and not according to its defined complexity."}, {"heading": "4 Application to classification task", "text": "To demonstrate the value of semantic categorization, we have it as an ingredient (for a classifier ensemble) in the classification algorithms and their committees SemCom, as well as in a standalone classifier SemCla.In this section, we recall commonly known classifier algorithms that we used in our experiments.These were Naive Bayes, Balanced Winnow, Labeled LDA, and the committees of classifiers (bagging type ensembles), which are based on Naive Bayes classifiers and Balanced Winnow. We also describe our own semantic classifier SemCla and our heterogeneous SemCom committee (which includes both the proprietary SemCat method and the above monitored classification methods)."}, {"heading": "4.1 Naive Bayes", "text": "Naive Bayes Classification Method (cf. [15]), based on knowledge derived from the training data set, creates a probabilistic model that assigns one of the predefined classes (i.e. designations) to a new observation (i.e. a document). In this approach, each document is treated like a bag of words that does not take into account the order (syntax). In addition, a simplistic assumption is made that the individual words in the document are independent.The probability of assigning a specific class c to a document d is calculated as follows: P (c | d) = P (c), w (c), w (c) nwdP (d), where nwd is the total number of occurrences of the word w in the document, a P (w | c) is the probability of occurrence of a word in class c."}, {"heading": "4.2 Balanced Winnow", "text": "The main concept is based on the Perceptron algorithm (cf. [16] and [17]). For our purpose, the Balanced Winnow version of the algorithm was chosen because of its high observed effectiveness. For each word, the algorithm stores two weights: w + and w \u2212, on the basis of which the algorithm calculates the affiliation to each class (binary classification). Positive weights speak for a certain class, negative weights against it. The difference between the weights (w + \u2212 w \u2212) is the total weight associated with a given word. Suppose that the classified document is a vector of words with weights x = (x1,.., xn). Then, the classification rule is based on the inequality."}, {"heading": "4.3 Labeled LDA", "text": "The latent dirichlet allocation (LLDA) is an extension of the latent dirichlet allocation model described in [19], which is useful in the analysis of text documents. Specifically, the verification of this topic can be found in [20]. LDA is an unattended method in which each document is treated as a probabilistic mix of different topics; the resulting generative model is characterized by the discrete probability distribution of words within a given topic; the model assumes the following way of generating each document; the length N of the document is selected (the Poisson distribution is used); then the proportion of subjects that make up the document is determined (Dirichlet distribution randomizes the set of K topics); subsequent words in the document are generated by randomly selecting the topic (with a multinomial distribution of viviviviviae); and then, within this topic (determining the distribution of words) a certain word is generated."}, {"heading": "4.4 Semantic classification", "text": "Below we present a description of a new semantic classifier, which we call SemCla. It is based on a category representation of a document produced by SemCat (see Section 3.1), which is used in combination with semantic measures (see Section 3.2).Outline of the algorithm Remember that SemCat uses words and phrases from the document to create a list of categories with weights. This representation of a document can be considered as a vector of weights for all categories from the W category structure. Therefore, we call it vector of categories. We use it to calculate cosinal products. We found that the algorithm works better if we add a supercategory of it for each category from the vector of the categories documents (according to W hierarchy) multiplied by the initial weight of the categories. (We used the value \u03b1 = 0.33, we explain below how we calibrated this parameter) Thus we expand the categories."}, {"heading": "4.5 Ensemble of classifiers", "text": "The experimental setting was also based on the ensemble of classifiers. For each document, the classification process is carried out by each classifier within the ensemble (it can also be a classifier of the same type, but be trained on a different test), and then the results of all classifiers are summarized as the final classifier of the ensemble. In the existing implementation, this can be done in three ways: (a) Each classifier has a voice - category with the highest number of votes is selected; (b) the weights of the classification results are additionally taken into account in the vote count (this option requires that all classifiers are of the same type); (c) the ranks of the elements returned by the classifier are aggregated instead of raw votes or weights. In the case that two (or more) categories have received exactly the same number of votes, the result is randomly selected from the winning categories."}, {"heading": "4.6 Heterogeneous committee of classifier with categorization method", "text": "In our new approach, we have developed a heterogeneous committee of classifiers SemCom, which includes the supervised methods of Naive Bayes, Balanced Winnow, LLDA, and our proprietary, unsupervised classification method SemCat using the taxonomy of the W-categories. Therefore, the categorization method is unsupervised and therefore cannot be trained in a similar way on different samples as supervised classifiers (the categorization method uses data from the entire W-taxonomy). Therefore, the committee contained only one instance of the categorization algorithm. In order to increase the influence of SemCat on the final results of the committee as a whole, the categorization votes were counted with greater weight. Furthermore, it should be taken into account that the categorization algorithm provides a ranking of categories (and not just a single category)."}, {"heading": "4.7 Remarks on denotation of classifier and ensemble parameters and composition", "text": "For the classical classification task (Tables 1 and 2), out of all W-pages belonging to a single category [S = 50,100,200] pages were randomly drawn, and on the basis of such a sample, a single classifier was trained. For a specific group of classes into which documents are to be placed, we select W-categories that represent these classes. We will call them W-classes. If we select W-documents for education, we can either select documents that have W-categories that are identical to the W-classes or their subcategories. We say that we select Level 1 documents (L = 1) if for each document at least one of its categories is identical to the class category. If we choose L = 2 documents for education, we also select documents that have categories that are direct subcategories of the class category."}, {"heading": "5 Experimental setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Performed experiments", "text": "We conducted two types of experiments, the results of which are shown in Tables 1-4. The first experiment aimed to show that adding a semantic categorizer to a committee of traditional classifiers improves classification correctness in the classical classification task (Table 1, 2), and the second experiment was designed to show that a semantic categorizer is able to bridge a semantic gap between the training data and the test data (Table 3, 4)."}, {"heading": "5.2 Benchmark data sets", "text": "For experimental purposes, we used two different benchmark datasets. We needed different datasets because of the different nature of the problems investigated. Benchmark used for classification comparisons. The benchmark dataset was based on the Polish subdirectory of the DMOZ Taxonomy / Open Directory Project http: / / www.dmoz.org. It contains 1063 text files of Polish websites only with HTML tags removed. Selected documents belong to 15 directories that map in W categories. They are: Astronomy, Biology, Philosophy, Physics, Graphics, Education, Politics, Law, Religious Sciences, Sociology, Technology. None of these categories is a subcategory of the others in the W taxonomy. We have omitted a few cases of multi-labeled documents. For the benchmark documents, the reader is referred to the Benchmark Web-Page3."}, {"heading": "5.3 Efficiency measures", "text": "In order to assess the efficiency of the algorithms studied, we use two different measurements: the first is usually used as standard precision measurement, the second as modified precision based on the similarity measurement Lin (Equation (1) in Section 3.2). The difference is in the use of the Lin measurement instead of the indicator function. For documents d1,..., dn with real categories categ (di) and its prediction pred (di), the Lin precision is defined as follows: 1n \u2211 n i = 1 Lin (categ (di), pred (di)). The motivation for using the second measurement is that the standard precision does not take into account the dependence between categories. In the event that we make a false prediction, we would like to know how much the predicted category differs from the real one."}, {"heading": "5.4 Classical classification task", "text": "The first part of the experimental work concerned the comparison of different methods of text classification. We started with documents from the DMOZ corpus with fixed labels described in Section 5.2. Documents were divided according to their text length, measured by the number of characters (C): short (1000 \u2264 C < 2000), medium (2000 \u2264 C < 10,000), long (10,000 \u2264 C). Files shorter than 1000 characters were not edited. Results for different classification methods are presented in Tables 1, 2. They were divided by a file size and efficiency measurement. Methods based on categorization algorithms provide a list of weighted W categories. Therefore, we transformed the result categories into the target set of 15 categories and took only one category with the highest weight. Categorization was based on a selection of 10 words (nouns only) / phrases with the highest resolution of the document."}, {"heading": "5.5 Classification for data with the semantic gap", "text": "The second experiment focuses on the problem of the semantic gap observed in the classification of data from different domains. In such data, two documents often express the same concepts, but since they use different formulations (due to the presence of synonyms, hypernyms, hyponyms), the traditional classification / cluster algorithms based on the standard bag-of-words approach do not work well. Such classifiers often do not recognize different linguistic representations for test and training sets. Some work related to the problem was published in Section 2. Our approach, which was thoroughly presented above, differs from them. There are other linguistic phenomena such as ellipses, paraphrases, and others. We focus on synonyms, hypernyms, hyponyms due to the Wikipedia structure on which our algorithm is based."}, {"heading": "6 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Classical task", "text": "As can be seen in tables, the best method among the semester algorithms mentioned is the one in which the terms / phrases of the W-sides are mapped. Modifications of the basic method (variants of adaptation, shifting of the level of projection) do not lead to significant changes in performance. Although Semcla surpasses the individual non-semantic classifications, it can be seen that a classical classification ensemble is able to surpass Semcu."}, {"heading": "6.2 Semantic gap problem", "text": "As can be seen in Tables 3, 4 in the case of the semantic gap problem, semantic methods and commissions produce much better results than conventional classifiers, even if the latter work with the modified representation (category bags instead of pouches).It turns out that the use of terms alone leads to poor results in semantic gap assignments. Classical methods are most helpful when categories are provided for training purposes, but the use of concepts is only half as good. This means that our SemCla algorithm actually uses a much deeper insight into the content of the document than just a category assignment. It is also worth stressing the fact that SemCla (unlike SemCat) is monitored, but can also be used in an unattended version. In such a setting, instead of using hidden document labels as training classes (see Figure 2), one can use document clusters where the cluster is also based on this large semantic (semantic) categorization."}, {"heading": "7 Conclusions", "text": "In this paper, we have demonstrated the value of the semantic approach in the task of classifying documents. In particular, we show here that an unsupervised approach to classification is possible when a semantic approach is used, which in itself can be considered an interesting result. Admittedly, the semantic classifier we introduce does not perform as well as ensembles of traditional classifiers, but it seems that the inclusion of a semantic classifier in such an ensemble is capable of significantly improving its performance in classification tasks. Intuitively, one might imagine that a classifier incorporating semantic information should be superior to traditional classifiers who do not use such information. As we see from our experiments, it is not so obvious. Although the semantic classifier has proven to be a competitor for individual classifiers, ensembles of classifiers can beat it. Therefore, taking advantage of semantic information requires a certain degree of complexity and cannot be considered unmistakable."}], "references": [{"title": "Wikipediabased document categorization", "author": ["K. Ciesielski", "P. Borkowski", "M.A. Klopotek", "K. Trojanowski", "K. Wysocki"], "venue": "Security and Intelligent Information Systems, SIIS 2011, Warsaw, Poland, June 13-14, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Machine learning in automated text categorization", "author": ["F. Sebastiani"], "venue": "ACM Comput. Surv. 34(1)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Text categorization", "author": ["F. Sebastiani"], "venue": "Text Mining and its Applications to Intelligence, CRM and Knowledge Management, WIT Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "A simple algorithm for topic identification in 0-1 data", "author": ["J.K. Sepp\u00e4nen", "E. Bingham", "H. Mannila"], "venue": "In Lavrac, N., Gamberger, D., Blockeel, H., Todorovski, L., eds.: PKDD. Volume 2838 of Lecture Notes in Computer Science., Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Topic indexing with wikipedia", "author": ["O. Medelyan", "I.H. Witten", "D. Milne"], "venue": "Proceedings of the first AAAI Workshop on Wikipedia and Artificial Intelligence (WIKIAI\u201908).", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "An effective, low-cost measure of semantic relatedness obtained from wikipedia links", "author": ["D. Milne", "I.H. Witten"], "venue": "Proceedings of the first AAAI Workshop on Wikipedia and Artificial Intelligence (WIKIAI\u201908).", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to link with wikipedia", "author": ["D.N. Milne", "I.H. Witten"], "venue": "Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM 2008, Napa Valley, CA, USA, October 26-30, 2008, ACM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Wikify!: linking documents to encyclopedic knowledge", "author": ["R. Mihalcea", "A. Csomai"], "venue": "Proceedings of the Sixteenth ACM Conference on Information and Knowledge Management, CIKM 2007, Lisbon, Portugal, November 6-10, 2007, ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Debora: Dependency-based method for extracting entityrelationship triples from open-domain texts in polish", "author": ["A. Wroblewska", "M. Sydow"], "venue": "In Chen, L., Felfernig, A., Liu, J., Ras, Z., eds.: Foundations of Intelligent Systems. Volume 7661 of Lecture Notes in Computer Science. Springer Berlin Heidelberg", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "A survey of crossdomain text categorization techniques", "author": ["M. Ramakrishna Murty", "J. Murthy", "P. Prasad Reddy", "S. Satapathy"], "venue": "Recent Advances in Information Technology (RAIT), 2012 1st International Conference on, IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Using wikipedia for co-clustering based cross-domain text classification", "author": ["P. Wang", "C. Domeniconi", "J. Hu"], "venue": "Data Mining, 2008. ICDM\u201908. Eighth IEEE International Conference on, IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Bridging semantic gaps in information retrieval: Context-based approaches", "author": ["C.T. Nguyen"], "venue": "ACM VLDB 10", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Content-based text categorization using wikitology", "author": ["M. Rafi", "S. Hassan", "M.S. Shaikh"], "venue": "CoRR abs/1208.3623", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Design, implementation and evaluation of a new semantic similarity metric combining features and intrinsic information content", "author": ["G. Pirr\u00f2", "N. Seco"], "venue": "On the Move to Meaningful Internet Systems. Volume 5332 of LNCS., Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Text categorisation: A survey", "author": ["K. Aas", "L. Eikvil"], "venue": "Report No. 941", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}, {"title": "General convergence results for linear discriminant updates", "author": ["A.J. Grove", "N. Littlestone", "D. Schuurmans"], "venue": "Mach. Learn. 43(3)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", "author": ["N. Littlestone"], "venue": "Machine Learning 2", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1988}, {"title": "The perceptron: A perceiving and recognizing automaton", "author": ["F. Rosenblatt"], "venue": "Technical Report 85-460-1, Ithaca, New York", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1957}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res. 3", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Probabilistic topic models", "author": ["M. Steyvers", "T. Griffiths"], "venue": "In Landauer, T., Mcnamara, D., Dennis, S., Kintsch, W., eds.: Latent Semantic Analysis: A Road to Meaning. Laurence Erlbaum", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Labeled lda: a supervised topic model for credit attribution in multi-labeled corpora", "author": ["D. Ramage", "D. Hall", "R. Nallapati", "C.D. Manning"], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1. EMNLP \u201909, Stroudsburg, PA, USA, Association for Computational Linguistics", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Supervised topic models", "author": ["D.M. Blei", "J.D. McAuliffe"], "venue": "NIPS.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Partially labeled topic models for interpretable text mining", "author": ["D. Ramage", "C.D. Manning", "S. Dumais"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. KDD \u201911, New York, NY, USA, ACM", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J.H. Friedman"], "venue": "Springer", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "The notion of semantic similarity, as used in this paper, was described in [1].", "startOffset": 75, "endOffset": 78}, {"referenceID": 0, "context": "Both methods introduced in the paper are based on our SemCat (Semantic Categorizer) algorithm, that has also been introduced in [1].", "startOffset": 128, "endOffset": 131}, {"referenceID": 1, "context": "The categorization task can be viewed formally as a special case of classification [2,3], but with a couple of differences.", "startOffset": 83, "endOffset": 88}, {"referenceID": 2, "context": "The categorization task can be viewed formally as a special case of classification [2,3], but with a couple of differences.", "startOffset": 83, "endOffset": 88}, {"referenceID": 3, "context": "and Finite Mixture of Multidimensional Bernoulli Distributions, described in [4].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "This approach was exploited in WikipediaMiner Project1, developed at the University of Waikato in Hamilton, New Zeeland [5,6,7].", "startOffset": 120, "endOffset": 127}, {"referenceID": 5, "context": "This approach was exploited in WikipediaMiner Project1, developed at the University of Waikato in Hamilton, New Zeeland [5,6,7].", "startOffset": 120, "endOffset": 127}, {"referenceID": 6, "context": "This approach was exploited in WikipediaMiner Project1, developed at the University of Waikato in Hamilton, New Zeeland [5,6,7].", "startOffset": 120, "endOffset": 127}, {"referenceID": 7, "context": "For terms from W their \u201ckeyphraseness\u201d [8] that is share of occurrences in W links is computed.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "In this paper we exploit our new unsupervised categorization method, SemCat, introduced in [1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 8, "context": "A novelty here is also the usage of more challenging Polish language [9].", "startOffset": 69, "endOffset": 72}, {"referenceID": 9, "context": "The article [10] shows a review of cross-domain text categorization problem.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "5; Naive Bayes classifier, KNN, Support Vector Machines; and some novel cross-domain classification algorithms: Expectation-Maximization Algorithm, Probabilistic Latent Semantic Analysis (PLSA), Latent Dirichlet Allocation(LDA), CFC Algorithm, Co-cluster based Classification Algorithm [11].", "startOffset": 286, "endOffset": 290}, {"referenceID": 11, "context": "Paper [12] gives a general overview of the problem of semantic gap in information retrieval.", "startOffset": 6, "endOffset": 10}, {"referenceID": 12, "context": "In the paper [13] authors propose a way to improve categorization by adding semantic knowledge from Wikitology (knowledge repository based on Wikipedia).", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "Our taxonomy-based categorization method SemCat was described in detail in [1], so below we present only its brief description.", "startOffset": 75, "endOffset": 78}, {"referenceID": 13, "context": "We were inspired by the paper [14].", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "In the literature dealing with Wordnet many measures based on IC and MSCA have been proposed [14], including LIN and PIRRO-SECO similarity:", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "[15]) on the basis of knowledge derived from training data set, creates a probabilistic model assigning one of the predefined classes (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Balanced Winnow algorithm details can be found in [16] and [17].", "startOffset": 50, "endOffset": 54}, {"referenceID": 16, "context": "Balanced Winnow algorithm details can be found in [16] and [17].", "startOffset": 59, "endOffset": 63}, {"referenceID": 17, "context": "[18]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Labeled Latent Dirichlet Allocation (LLDA) is an extension of the popular \u2013 among practitioners and theorists \u2013 Latent Dirichlet Allocation model described in [19].", "startOffset": 159, "endOffset": 163}, {"referenceID": 19, "context": "In particular the review of this subject can be found in [20].", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "LLDA algorithm is very similar to its unsupervised prototype, with the exception that the document topics are selected only from among those that correspond to the observed document labels \u2013 details can be found in [21].", "startOffset": 215, "endOffset": 219}, {"referenceID": 21, "context": "There are other supervised variants of the LDA algorithm, such as Supervised LDA ([22]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": "[23]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "More information on ensemble methods can be found in [24].", "startOffset": 53, "endOffset": 57}], "year": 2017, "abstractText": "In this paper we propose a new document classification method, bridging discrepancies (so-called semantic gap) between the training set and the application sets of textual data. We demonstrate its superiority over classical text classification approaches, including traditional classifier ensembles. The method consists in combining a document categorization technique with a single classifier or a classifier ensemble (SemCom algorithm Committee with Semantic Categorizer).", "creator": "LaTeX with hyperref package"}}}