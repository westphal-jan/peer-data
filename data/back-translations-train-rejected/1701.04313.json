{"id": "1701.04313", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2017", "title": "End-to-End ASR-free Keyword Search from Speech", "abstract": "End-to-end (E2E) systems have achieved competitive results compared to conventional hybrid hidden Markov model (HMM)-deep neural network based automatic speech recognition (ASR) systems. Such E2E systems are attractive due to the lack of dependence on alignments between input acoustic and output grapheme or HMM state sequence during training. This paper explores the design of an ASR-free end-to-end system for text query-based keyword search (KWS) from speech trained with minimal supervision. Our E2E KWS system consists of three sub-systems. The first sub-system is a recurrent neural network (RNN)-based acoustic auto-encoder trained to reconstruct the audio through a finite-dimensional representation. The second sub-system is a character-level RNN language model using embeddings learned from a convolutional neural network. Since the acoustic and text query embeddings occupy different representation spaces, they are input to a third feed-forward neural network that predicts whether the query occurs in the acoustic utterance or not. This E2E ASR-free KWS system performs respectably despite lacking a conventional ASR system and trains much faster.", "histories": [["v1", "Fri, 13 Jan 2017 15:05:39 GMT  (740kb,D)", "http://arxiv.org/abs/1701.04313v1", "Published in the IEEE 2017 International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2017), scheduled for 5-9 March 2017 in New Orleans, Louisiana, USA"]], "COMMENTS": "Published in the IEEE 2017 International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2017), scheduled for 5-9 March 2017 in New Orleans, Louisiana, USA", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG cs.NE", "authors": ["kartik audhkhasi", "rew rosenberg", "abhinav sethy", "bhuvana ramabhadran", "brian kingsbury"], "accepted": false, "id": "1701.04313"}, "pdf": {"name": "1701.04313.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Kartik Audhkhasi", "Andrew Rosenberg", "Abhinav Sethy", "Bhuvana Ramabhadran", "Brian Kingsbury"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of them are able to play by the rules without having to play by the rules."}, {"heading": "1.1. Prior Work", "text": "Most of the earlier work relevant to this paper focused on query-byexample (QbyE) retrieval of language from a database. The user provides a language expression of the query that must be searched for, as opposed to a text query used in the KWS setup of this paper. Dynamic time distortion (DTW) of acoustic properties extracted from the speech query and speech expressions from the database is a classic technique in such QbyE systems [9, 10]. The cost of DTW alignment serves as a suitable result for retrieval. Chen, Parada and Sainath [11] present a system for QbyE in which the audio database contains examples of certain key phrases, such as \"hello genie.\" The last k-state vectors from the last hidden layer of an RNN acoustic system give a fixed three-dimensional representation of the system."}, {"heading": "2.1. RNN Acoustic Auto-encoder", "text": "Motivated by previous work [11, 13] on the calculation of fixed-dimensional representations from a vector sequence of variable length acoustic characteristics, we use an RNN-based auto-encoder as shown in Figure 1. The encoder processes T-acoustic vectors (x1,.., xT) through a unidirectional RNN with gated recurrent unit (GRU) [16] hidden units, which are unrolled in T-time steps. A completely connected layer with the weight matrix W and D rectified linear units (ReLUs), which are indicated by g, then processes the hidden state vector heT from time step T. A decoder GRU-RNN takes the resulting D-dimensional acoustic representation g (WheT) as input at each time step."}, {"heading": "2.2. CNN-RNN Character LM", "text": "We use the CNN-based character architecture RNN-LM of Kim et. al [17] to derive query embedding. Figure 2 shows this LM for the simple case of 2 wave masks. We map the input sequence of N characters (c1,..., cN) to a matrix of d-dimensional embedding of characters via an overview table. Next, M d x w-wave masks work on the resulting d x-N embedding matrix to create a set of M N-dimensional vectors, one per mask. We then perform a maximum pooling over time on each of these vectors to obtain a scalar per mask and an M-dimensional embedding vector. This embedding vector then feeds into a GRU-RNN that predicts one of K characters at any time. A significant difference between our M and the next section of our words consists of predicting our M [17]."}, {"heading": "2.3. Overall E2E ASR-free KWS System", "text": "The last block in the entire KWS system is a neural network that takes the embedding of speech expression and text query as input and predicts whether the query takes place in the utterance or not. In contrast to previous work on QbyE in the language, where both speech expression and speech query are located in the same acoustic representation space and the cosinal similarity is sufficient to coincide with both, Figure 3 shows the entire E2E-KWS system. We extract the encoders from both the acoustic RNN auto encoder and the CNN RNN character LM and feed them into an upstream neural network. In contrast to conventional ASR-based approaches for KWS from the language, the E2E system in Figure 3 after the utterance and the query encoder is also portable. In the next section, our data processing, experiments, results and analyses are presented, REPENSIMENTS and EXPENSIMENTS."}, {"heading": "3.1. Data Description and Preparation", "text": "The training data includes 40 hours of transcribed audio equivalent to 45k utterances, and a graphical pronunciation dictionary containing 35k words. We use 15 hours of development audio, 2k-in vocabulary (IV) keywords, and 400 out-of-vocabulary keywords (OOV) keywords for testing. Keywords include both single-word and multi-word queries. Multilingual acoustic features have been successful in the Babel program compared to traditional features, such as Mel frequency receiver coefficients. We use an 80-dimensional multilingual acoustic front-end [18] that has been trained on all 24 Babel languages from the base period to election period 3, with positive acoustic features having been successful compared to conventional features. Formation of the KNN acoustic Auto-Encoder and the CNN-RNN character LM does not require a specific selection of networks, but does not require a final training sample from this network."}, {"heading": "3.2. Acoustic Auto-encoder", "text": "We used 300 hidden GRU neurons in the acoustic encoder and decoder RNNs. We sorted the acoustic feature sequences in the training dataset in increasing order as they improved training convergence. We unrolled both the RNNs for 15 seconds and 1500 time steps, corresponding to the length of the maximum acoustic expression in the training dataset. We added all the acoustic feature sequences to make their length equal to 1500 time steps, and excluded these additional frames from the loss function and gradient calculation. We used a linear dense layer of size 300 to calculate the embedding from the hidden state vector of the encoder RNN. We trained the acoustic auto encoder by minimizing the reconstruction error in the mean square of the input sequence of acoustic feature vectors by using the Adam optimization algorithm [21] by reducing the validation loss from 3 to 4 \u2212 1."}, {"heading": "3.3. CNN-RNN Character LM", "text": "We used all the acoustic transcripts and converted them into sentences of 39 unique graphs. We divided sentences longer than 50 graphs into smaller pieces to prepare mini-stacks, as the maximum length of a query is 23 graphs. We used 50-dimensional embedding for each graph and 300 50 x 3 convolutionary masks, resulting in a 300-dimensional embedding for each input sequence. The RNN decoder used 256 GRUs and a softmax layer of 39 neurons in each time step. We minimized the cross-entropy of the output graph sequence and used Adam with a mini-stack size of 256 sequences, a learning rate of 1 x 10 \u2212 3 and the newbob annealing schedule. The bottom chart in Figure 4 shows the progress in cross-entropy training. In contrast to the acoustic RNN network we used this pair of encoding for this."}, {"heading": "3.4. KWS Neural Network", "text": "This year it is so far that it will only take one year to move on to the next round."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "The IBM 2016 English Conversational Telephone Speech Recognition System", "author": ["G. Saon", "T. Sercu", "S. Rennie", "H.J. Kuo"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "The Microsoft 2016 Conversational Speech Recognition System", "author": ["W. Xiong", "J. Droppo", "X. Huang", "F. Seide", "M. Seltzer", "A. Stolcke", "D. Yu", "G. Zweig"], "venue": "arXiv:1609.03528, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proc. ICML. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding", "author": ["Y. Miao", "M. Gowayyed", "F. Metze"], "venue": "Proc. ASRU. IEEE, 2015, pp. 167\u2013174.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Endto-end attention-based large vocabulary speech recognition", "author": ["D Bahdanau", "J. Chorowski", "D. Serdyuk", "Y. Bengio"], "venue": "Proc. ICASSP. IEEE, 2016, pp. 4945\u20134949.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Lattice indexing for spoken term detection", "author": ["D. Can", "M. Saraclar"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 8, pp. 2338\u20132347, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Query-by-example spoken term detection using phonetic posteriorgram templates", "author": ["T.J. Hazen", "W. Shen", "C. White"], "venue": "Proc. ASRU. IEEE, 2009, pp. 421\u2013426.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams", "author": ["Y. Zhang", "J.R. Glass"], "venue": "Proc. ASRU. IEEE, 2009, pp. 398\u2013403.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Query-by-example keyword spotting using long short-term memory networks", "author": ["G. Chen", "C. Parada", "T.N. Sainath"], "venue": "Proc. ICASSP. IEEE, 2015, pp. 5236\u20135240.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Fixeddimensional acoustic embeddings of variable-length segments in low-resource settings", "author": ["K. Levin", "K. Henry", "A. Jansen", "K. Livescu"], "venue": "Proc. ASRU. IEEE, 2013, pp. 410\u2013 415.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Audio Word2Vec: Unsupervised Learning of Audio Segment Representations using Sequence-to-sequence Autoencoder", "author": ["Y. Chung", "C. Wu", "C. Shen", "H. Lee", "L. Lee"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["H. Kamper", "W. Wang", "K. Livescu"], "venue": "Proc. ICASSP. IEEE, 2016, pp. 4950\u20134954.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Jointly learning to locate and classify words using convolutional networks", "author": ["D. Palaz", "G. Synnaeve", "R. Collobert"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "arXiv preprint arXiv:1508.06615, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Multilingual representations for low resource speech recognition and keyword search", "author": ["J. Cui", "B. Kingsbury", "B. Ramabhadran", "A. Sethy", "K. Audhkhasi", "X. Cui", "E. Kislal", "L. Mangu", "M. Nussbaum-Thom", "M. Picheny"], "venue": "Proc. ASRU. IEEE, 2015, pp. 259\u2013266.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/ keras, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv eprints, vol. abs/1605.02688, May 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "Proc. ICML, 2010, pp. 735\u2013742.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNNs) have pushed the state-of-the-art for automatic speech recognition (ASR) systems [1].", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "This has led to significant performance improvements on several well-known ASR benchmarks such as Switchboard [2, 3].", "startOffset": 110, "endOffset": 116}, {"referenceID": 2, "context": "This has led to significant performance improvements on several well-known ASR benchmarks such as Switchboard [2, 3].", "startOffset": 110, "endOffset": 116}, {"referenceID": 3, "context": "These include the connectionist temporal classification [4, 5] loss-based recurrent neural network (RNN) and attention-based RNNs [6, 7].", "startOffset": 56, "endOffset": 62}, {"referenceID": 4, "context": "These include the connectionist temporal classification [4, 5] loss-based recurrent neural network (RNN) and attention-based RNNs [6, 7].", "startOffset": 56, "endOffset": 62}, {"referenceID": 5, "context": "These include the connectionist temporal classification [4, 5] loss-based recurrent neural network (RNN) and attention-based RNNs [6, 7].", "startOffset": 130, "endOffset": 136}, {"referenceID": 6, "context": "These include the connectionist temporal classification [4, 5] loss-based recurrent neural network (RNN) and attention-based RNNs [6, 7].", "startOffset": 130, "endOffset": 136}, {"referenceID": 7, "context": "Conventional KWS from speech uses an ASR system as a front-end that converts the speech database into a finite-state transducer (FST) index containing all possible hypotheses word sequences with their associated confidence scores and time stamps [8].", "startOffset": 246, "endOffset": 249}, {"referenceID": 8, "context": "Dynamic time warping (DTW) of acoustic features extracted from the speech query and speech utterances from the database is a classic technique in such QbyE systems [9, 10].", "startOffset": 164, "endOffset": 171}, {"referenceID": 9, "context": "Dynamic time warping (DTW) of acoustic features extracted from the speech query and speech utterances from the database is a classic technique in such QbyE systems [9, 10].", "startOffset": 164, "endOffset": 171}, {"referenceID": 10, "context": "Chen, Parada, and Sainath [11] present a system for QbyE where the audio database contains examples of certain key-phrases, such as \u201chello genie\u201d.", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "al [12] compares several non-neural network-based techniques for computing fixeddimensional representations of speech segments for QbyE, including principal component analysis and Laplacian eigenmaps.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "al [13] also present a similar QbyE system using an acoustic RNN encoder-decoder network.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Kamper, Wang, and Livescu [14] use a Siamese convolutional neural network (CNN) for obtaining acoustic embeddings of the audio query and utterances from the database, and train this network by minimizing a triplet hinge loss.", "startOffset": 26, "endOffset": 30}, {"referenceID": 14, "context": "Our paper is closely related to the recent work of Palaz, Synnaeve, and Collobert [15], where the authors propose a CNN-based ASR system trained on a bag of words in the speech utterance.", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "Motivated by prior work [11, 13] on computing fixed-dimensional representations from variable length acoustic feature vector sequence, we use an RNN-based auto-encoder as shown in Figure 1.", "startOffset": 24, "endOffset": 32}, {"referenceID": 12, "context": "Motivated by prior work [11, 13] on computing fixed-dimensional representations from variable length acoustic feature vector sequence, we use an RNN-based auto-encoder as shown in Figure 1.", "startOffset": 24, "endOffset": 32}, {"referenceID": 15, "context": ",xT ) by a uni-directional RNN with gated recurrent unit (GRU) [16] hidden units unrolled over T time steps.", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "al [17] for deriving query embeddings.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "al [17] is that we train our LM to predict a sequence of characters instead of words.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "We use an 80dimensional multilingual acoustic front-end [18] trained on all 24 Babel languages from the Base Period to Option Period 3, excluding Georgian.", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "We implemented the system in Keras [19] and Theano [20].", "startOffset": 35, "endOffset": 39}, {"referenceID": 19, "context": "We implemented the system in Keras [19] and Theano [20].", "startOffset": 51, "endOffset": 55}, {"referenceID": 20, "context": "We trained the acoustic auto-encoder by minimizing the mean-squared reconstruction error of the input sequence of acoustic feature vectors using the Adam optimization algorithm [21] with a mini-batch size of 40 utterances and learning rate of 1 \u00d7 10\u22123.", "startOffset": 177, "endOffset": 181}, {"referenceID": 21, "context": "We trained this network first using the frame-wise cross-entropy criterion and then using Hessian-free sequence minimum Bayes risk (sMBR) training [22, 23].", "startOffset": 147, "endOffset": 155}], "year": 2017, "abstractText": "End-to-end (E2E) systems have achieved competitive results compared to conventional hybrid hidden Markov model (HMM)-deep neural network based automatic speech recognition (ASR) systems. Such E2E systems are attractive due to the lack of dependence on alignments between input acoustic and output grapheme or HMM state sequence during training. This paper explores the design of an ASR-free end-to-end system for text query-based keyword search (KWS) from speech trained with minimal supervision. Our E2E KWS system consists of three sub-systems. The first sub-system is a recurrent neural network (RNN)-based acoustic auto-encoder trained to reconstruct the audio through a finite-dimensional representation. The second sub-system is a character-level RNN language model using embeddings learned from a convolutional neural network. Since the acoustic and text query embeddings occupy different representation spaces, they are input to a third feed-forward neural network that predicts whether the query occurs in the acoustic utterance or not. This E2E ASR-free KWS system performs respectably despite lacking a conventional ASR system and trains much faster.", "creator": "LaTeX with hyperref package"}}}