{"id": "1701.04600", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jan-2017", "title": "Faster K-Means Cluster Estimation", "abstract": "There has been considerable work on improving popular clustering algorithm `K-means' in terms of mean squared error (MSE) and speed, both. However, most of the k-means variants tend to compute distance of each data point to each cluster centroid for every iteration. We propose a fast heuristic to overcome this bottleneck with only marginal increase in MSE. We observe that across all iterations of K-means, a data point changes its membership only among a small subset of clusters. Our heuristic predicts such clusters for each data point by looking at nearby clusters after the first iteration of k-means. We augment well known variants of k-means with our heuristic to demonstrate effectiveness of our heuristic. For various synthetic and real-world datasets, our heuristic achieves speed-up of up-to 3 times when compared to efficient variants of k-means.", "histories": [["v1", "Tue, 17 Jan 2017 10:00:51 GMT  (20kb)", "http://arxiv.org/abs/1701.04600v1", "6 pages, Accepted at ECIR 2017"]], "COMMENTS": "6 pages, Accepted at ECIR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["siddhesh khandelwal", "amit awekar"], "accepted": false, "id": "1701.04600"}, "pdf": {"name": "1701.04600.pdf", "metadata": {"source": "CRF", "title": "Faster K-Means Cluster Estimation", "authors": ["Siddhesh Khandelwal", "Amit Awekar"], "emails": ["siddhesh166@gmail.com,", "awekar@iitg.ernet.in"], "sections": [{"heading": null, "text": "ar Xiv: 170 1.04 600v 1 [cs.L G] 17 Jan 2017Keywords: K-Means, Clustering, Heuristics"}, {"heading": "1 Introduction", "text": "K-mean is a popular cluster technique used in various fields such as humanities, bioinformatics and astronomy. Faced with a dataset D with n data points in Rd space, K-mean D divides into k-clusters with the aim of minimizing the average square error (MSE). MSE is defined as the sum of the square distance of each point from its corresponding center. K-mean is NP-hard. To achieve a local minimum, polynomial time euristics are commonly used. One such popular heuristics is the Lloyd's algorithm [6], which randomly selects certain initial centers (also known as seeds) from the dataset. Each data point is assigned to the cluster corresponding to the nearest center. Each center is then recalculated as the mean value of the points assigned to that cluster. This procedure repeats until convergence."}, {"heading": "2 Our Work: Candidate cluster list for each data point", "text": "Our main contribution is the definition of a heuristic that can be used as an extension to current variants of k means for faster cluster estimation. Let algorithm V be a variant of k means and algorithm V \u2032 be the same variant that can be extended with our heuristic selection. Let T be the time required for V to approach the MSE value of E. Similarly, the time required for V \u00b2 to approach the MSE value of E \u00b2 is similar. We should meet the following two conditions when comparing V \u2032: - Condition 1: T \u2032 is lower than T, and - Condition 2: E \u2032 is either lower or only marginally higher than E.In short, these conditions state that a variant of the K mean should be augmented variant faster without a significant increase in the final MSE.Major bottleneck of K means clustering is the calculation of data point to cluster."}, {"heading": "3 Related Work", "text": "Over the past three decades, there has been significant work on improving Lloyd's algorithm [6], both in terms of reducing MSE and maturity. Follow-up work on Lloyd's algorithm can be roughly divided into three categories: Better seed selection [2,5], selection of the ideal value for the number of clusters [8], and limitations of the data point to cluster centricity [3,7,4]. Arthur et. al. [2] provided a better method for seed selection based on a probability distribution over the narrowest cluster centricities for each data point. Likas et. al. [5] proposed the global k-mean method for selecting a seed at once to reduce the final mean square meter error. Pham et. al. [8] designed a novel function to evaluate the quality of cluster centricities for various potential cluster number values. Elkan [3] used triangular means algorithms [4] to avoid this data imbalance between cluster redundancy and cluster redundancy]."}, {"heading": "4 Experimental Results", "text": "s algorithm, our heuristic analysis represents an acceleration of about 30 times with an error within 0.2% of the mentioned distance. However, in order to show the effectiveness of our heuristic method, we must apply the results of the augmentation to faster variants of K means such as K means with triangular information (KMT). Due to space constraints, we present only this variant. Augmentation of KMT with our heuristic methods is reference redas. Code and datasets used for our experiments are available for download [1]. During each iteration of KMT, one data point calculates the distance to the current cluster. KMT uses triangle inequality to calculate more efficient distances to all other centroids."}, {"heading": "5 Conclusion", "text": "We presented heuristics to address the bottleneck of redundant distance calculations in K averages. Our heuristic limits for distance calculations for each data point suggest CCL. Our heuristics can be expanded by different variants of K averages to allow for faster convergence without a significant increase in MSE. In extensive experiments with real and synthetic datasets, we have shown that our heuristics perform well with variations in the dimensionality of the dataset, the CCL size, the number of clusters and the degree of separation between clusters. This work can be further improved by making CCL dynamic to achieve a better speed while reducing the PIM value."}], "references": [{"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "In ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Using the triangle inequality to accelerate k-means", "author": ["C. Elkan"], "venue": "In International Conference om Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "An efficient k-means clustering algorithm: Analysis and implementation", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Trans. on,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "The global k-means clustering algorithm", "author": ["A. Likas", "N. Vlassis", "J.J. Verbeek"], "venue": "Pattern recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Least squares quantization in pcm", "author": ["S.P. Lloyd"], "venue": "Information Theory, IEEE Trans. on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "Accelerating exact k-means algorithms with geometric reasoning", "author": ["D. Pelleg", "A. Moore"], "venue": "In ACM SIGKDD,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Selection of k in k-means clustering", "author": ["D.T. Pham", "S.S. Dimov", "C. Nguyen"], "venue": "Journal of Mechanical Engineering Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "One such popular heuristic is the Lloyd\u2019s algorithm[6] that selects certain initial centroids (also referred as seeds) at random from the dataset.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "In last three decades, there has been significant work on improving Lloyd\u2019s algorithm [6] both in terms of reducing MSE and running time.", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "The follow up work on Lloyd\u2019s algorithm can be broadly divided into three categories: Better seed selection[2,5], Selecting ideal value for number of clusters[8], and Bounds on data point to cluster centroid distance[3,7,4].", "startOffset": 107, "endOffset": 112}, {"referenceID": 3, "context": "The follow up work on Lloyd\u2019s algorithm can be broadly divided into three categories: Better seed selection[2,5], Selecting ideal value for number of clusters[8], and Bounds on data point to cluster centroid distance[3,7,4].", "startOffset": 107, "endOffset": 112}, {"referenceID": 6, "context": "The follow up work on Lloyd\u2019s algorithm can be broadly divided into three categories: Better seed selection[2,5], Selecting ideal value for number of clusters[8], and Bounds on data point to cluster centroid distance[3,7,4].", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "The follow up work on Lloyd\u2019s algorithm can be broadly divided into three categories: Better seed selection[2,5], Selecting ideal value for number of clusters[8], and Bounds on data point to cluster centroid distance[3,7,4].", "startOffset": 216, "endOffset": 223}, {"referenceID": 5, "context": "The follow up work on Lloyd\u2019s algorithm can be broadly divided into three categories: Better seed selection[2,5], Selecting ideal value for number of clusters[8], and Bounds on data point to cluster centroid distance[3,7,4].", "startOffset": 216, "endOffset": 223}, {"referenceID": 2, "context": "The follow up work on Lloyd\u2019s algorithm can be broadly divided into three categories: Better seed selection[2,5], Selecting ideal value for number of clusters[8], and Bounds on data point to cluster centroid distance[3,7,4].", "startOffset": 216, "endOffset": 223}, {"referenceID": 0, "context": "[2] provided a better method for seed selection based on a probability distribution over closest cluster centroid distances for each data point.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] proposed the Global k-means method for selecting one seed at a time to reduce final mean squared error.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] designed a novel function to evaluate goodness of clustering for various potential values of number of clusters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Elkan[3] use triangle inequality to avoid redundant computations of distance between data points and cluster centroids.", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "Pelleg and Moore[7] and Kanungo et al.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "[4] proposed similar algorithms that use k-d trees.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "However to show the effectiveness of our heuristic, we present results of augmenting it to faster variants of K-means such as K-means with triangle inequality (KMT)[3].", "startOffset": 164, "endOffset": 167}, {"referenceID": 0, "context": "RND = Random initialization ; KPP = Initialization using Kmeans++[2]", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "[3] to demonstrate the effectiveness of KMT and one is a synthetically generated dataset by us.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "We tried two different methods for initial seed selection: random [6]and K-means++ [2].", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "We tried two different methods for initial seed selection: random [6]and K-means++ [2].", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "RND = Random initialization ; KPP = Initialization using Kmeans++[2]", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "For each value of k in Table 2 and k in Table 3, we used two different initial seedings - random (RND) and Kmeans++ [2].", "startOffset": 116, "endOffset": 119}], "year": 2017, "abstractText": "There has been considerable work on improving popular clustering algorithm \u2018K-means\u2019 in terms of mean squared error (MSE) and speed, both. However, most of the k-means variants tend to compute distance of each data point to each cluster centroid for every iteration. We propose a fast heuristic to overcome this bottleneck with only marginal increase in MSE. We observe that across all iterations of K-means, a data point changes its membership only among a small subset of clusters. Our heuristic predicts such clusters for each data point by looking at nearby clusters after the first iteration of k-means. We augment well known variants of k-means with our heuristic to demonstrate effectiveness of our heuristic. For various synthetic and real-world datasets, our heuristic achieves speed-up of up-to 3 times when compared to efficient variants of k-means.", "creator": "LaTeX with hyperref package"}}}