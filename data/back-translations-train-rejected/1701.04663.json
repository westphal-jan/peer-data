{"id": "1701.04663", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jan-2017", "title": "Intrinsically Motivated Acquisition of Modular Slow Features for Humanoids in Continuous and Non-Stationary Environments", "abstract": "A compact information-rich representation of the environment, also called a feature abstraction, can simplify a robot's task of mapping its raw sensory inputs to useful action sequences. However, in environments that are non-stationary and only partially observable, a single abstraction is probably not sufficient to encode most variations. Therefore, learning multiple sets of spatially or temporally local, modular abstractions of the inputs would be beneficial. How can a robot learn these local abstractions without a teacher? More specifically, how can it decide from where and when to start learning a new abstraction? A recently proposed algorithm called Curious Dr. MISFA addresses this problem. The algorithm is based on two underlying learning principles called artificial curiosity and slowness. The former is used to make the robot self-motivated to explore by rewarding itself whenever it makes progress learning an abstraction; the later is used to update the abstraction by extracting slowly varying components from raw sensory inputs. Curious Dr. MISFA's application is, however, limited to discrete domains constrained by a pre-defined state space and has design limitations that make it unstable in certain situations. This paper presents a significant improvement that is applicable to continuous environments, is computationally less expensive, simpler to use with fewer hyper parameters, and stable in certain non-stationary environments. We demonstrate the efficacy and stability of our method in a vision-based robot simulator.", "histories": [["v1", "Tue, 17 Jan 2017 13:24:37 GMT  (1304kb,D)", "http://arxiv.org/abs/1701.04663v1", "8 pages, 5 figures"]], "COMMENTS": "8 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["varun raj kompella", "laurenz wiskott"], "accepted": false, "id": "1701.04663"}, "pdf": {"name": "1701.04663.pdf", "metadata": {"source": "CRF", "title": "Intrinsically Motivated Acquisition of Modular Slow Features for Humanoids in Continuous and Non-Stationary Environments", "authors": ["Varun Raj Kompella", "Laurenz Wiskott"], "emails": ["laurenz.wiskott}@ini.rub.de"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 CD-MISFA 2.0", "text": "We will discuss the details of our new method here. To keep it short, we will refer to the original behavior of Dr. MISFA as CDMISFA 1.0 and our new method as CD-MISFA 2.0 (see Section 3.1 for a detailed comparison between the two methods). Next, we will provide an intuitive analog example to explain the underlying problem that is being solved. Consider a specific channel that watches different channels on television. Each channel generates a continuous stream of images (which in most cases may or may not be predictable), and the agent can only access information from a single channel. He can explore the channels by selecting a specific channel for a certain period of time and then switching. Distribution of the images that the agent receives is in most cases not stationary, making it impracticable to encode all channels."}, {"heading": "3 Experimental Results", "text": "Here we evaluate the performance of our algorithm. The desired result is a sequence of SF abstractions acquired in the order of increasing learning difficulties. To quantify the learning difficulties of an observation stream with IncSFA, we use values of the Curiosity function [19]."}, {"heading": "3.1 CD-MISFA 1.0 vs CD-MISFA 2.0", "text": "We compare our method with the previous CD-MISFA 1.0 algorithm: (a) CD-MISFA 1.0 is not complex, but uses a clustering algorithm called Robust Online Clustering (ROC) [43] coupled to the IncSFA. The ROC error is used to decide when to stop updating the results of IncSFA and whether to correlate the results with some pre-defined discrete meta class labels. Disadvantages of using ROC are: (a) It requires discrete meta class labels, which are difficult to provide in general environments (see Section 3.4). (b) It limits the abstractions that are correlated to the labels. (c) It limits the application of the algorithms to discrete environments. (d) It adds to the general utational complexity."}, {"heading": "3.2 Oscillatory Streams Environment", "text": "We performed the algorithm for 20 experiments with different random initializations (seeds), obtaining optimal results for all experiments. An optimal result is the abstraction set set. [1] Abstraction set. [2] Abstraction set. [3] Abstraction set. [4] Abstraction set. [5] Abstraction set. [5] Abstraction set.] Abstraction set. [5] Abstraction set. [6] Abstraction set. [6] Abstraction set. [6] Abstraction set. [7] Abstraction set. [7] Abstraction set. [8] Abstraction set. [8] Abstraction set. [8] Abstraction measure. [8] Abstraction measure. [8] Abstraction set. [8]. Abstraction set. [8]. Abstraction measure. [8]."}, {"heading": "3.3 Non-Stationary Dynamic Environments", "text": "Here we discuss the results of experiments carried out in non-stationary environments where statistics change abruptly in time. < 1 > 76 previous experiments. Consider an environment with 3 currents; the first current generates zeros, the second current is x2 (Eq. (8)) and the third is x3 (Eq. (9)). Since x2 is easier to learn than x3, the optimal partial policy is [1, 0, 1]. We let the algorithm policy stabilize, and when the decreasing -greedy strategy falls under a constant, we replace the zero current with x1 (Eq. (7). The new optimal partial policy after this signal swap is [0, 1] because x1 is the easiest strategy to learn. For the rest of this section, we designate [1, 0, 1] as the old optimal partial policy and [0, 1, 1] as the new optimal partial policy."}, {"heading": "3.4 Curiosity-Driven Vision-Enabled iCub", "text": "An important open problem in vision-based development robotics is how can an online vision-enabled humanoid robot akin to a human baby focus / shift its attention towards events that it finds interesting? Can its cub is placed next to a table with three objects of different size (Figure 5 (a))). The environment is dynamic and continuous; all the positions of the three objects (unknown to the iCub) change at any given time. The x-position of the objects changes universally within the range (-0.4, -0.6) and their position is either 0.4 or 0.6 and toggles at a fixed unknown frequency. Both x and y-position of the object-2 change universally."}, {"heading": "4 Conclusion", "text": "This paper introduces an online learning system that allows an agent to search in regions where he can find the next simple but unknown regularity in his high-dimensional sensory inputs. We have demonstrated through experiments that the method is stable in certain non-stationary environments. the iCub experiment shows that the reliable performance of the algorithm extends to high-dimensional image inputs, making it valuable for vision-based developmental learning. our future work includes implementing the algorithm in environments where the input observation streams are generated as a result of executing various temporally varying behaviors (e.g. options [37]), and also in environments where he can learn to reuse the learned modular abstractions to solve an external task."}], "references": [{"title": "Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization", "author": ["B. Bakker", "J. Schmidhuber"], "venue": "In F. Groen et al., editor, Proc. 8th Conference on Intelligent Autonomous Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Intrinsically motivated learning systems: an overview", "author": ["G. Baldassarre", "M. Mirolli"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Active learning of inverse models with intrinsically motivated goal exploration in robots", "author": ["A. Baranes", "P. Oudeyer"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Intrinsically motivated learning of hierarchical collections of skills", "author": ["A.G. Barto", "S. Singh", "N. Chentanez"], "venue": "Proceedings of International Conference on Developmental Learning (ICDL)", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Construction of approximation spaces for reinforcement learning", "author": ["W. B\u00f6hmer", "S. Gr\u00fcnew\u00e4lder", "Y. Shen", "M. Musial", "K. Obermayer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Sparse coding in the primate cortex", "author": ["P. F\u00f6ldi\u00e1k", "M.P. Young"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Bayesian surprise attracts human attention", "author": ["L. Itti", "P.F. Baldi"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Reinforcement learning: a survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "Journal of AI research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "Slowness Learning for Curiosity- Driven Agents", "author": ["V.R. Kompella"], "venue": "PhD thesis, Informatics Department, Universita\u0300 della Svizzera Italiana,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Incremental slow feature analysis: Adaptive low-complexity slow feature updating from high-dimensional input streams", "author": ["V.R. Kompella", "M. Luciw", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Autoincsfa and vision-based developmental learning for humanoid robots", "author": ["V.R. Kompella", "L. Pape", "J. Masci", "M. Frank", "J. Schmidhuber"], "venue": "In IEEE-RAS International Conference on Humanoid Robots,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Explore to see, learn to perceive, get the actions for free: Skillability", "author": ["V.R. Kompella", "M. Stollenga", "M. Luciw", "J. Schmidhuber"], "venue": "In International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Continual curiosity-driven skill acquisition from high-dimensional video inputs for humanoid robots", "author": ["V.R. Kompella", "M. Stollenga", "M. Luciw", "J. Schmidhuber"], "venue": "Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Autonomous skill acquisition on a mobile manipulator", "author": ["G. Konidaris", "S. Kuindersma", "R. Grupen", "A.G. Barto"], "venue": "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Evolving deep unsupervised convolutional networks for visionbased reinforcement learning", "author": ["J. Koutn\u0131\u0301k", "J. Schmidhuber", "F. Gomez"], "venue": "In Proceedings of the 2014 conference on Genetic and evolutionary computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Reinforcement learning on slow features of high-dimensional input streams", "author": ["R. Legenstein", "N. Wilbert", "L. Wiskott"], "venue": "PLoS Computational Biology,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "An intrinsic value system for developing multiple invariant representations with incremental slowness learning", "author": ["M. Luciw", "V.R. Kompella", "S. Kazerounian", "J. Schmidhuber"], "venue": "Frontiers in Neurorobotics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Low complexity protovalue function learning from sensory observations with incremental slow feature analysis", "author": ["M. Luciw", "J. Schmidhuber"], "venue": "In Proc. 22nd International Conference on Artificial Neural Networks (ICANN),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Removing time variation with the antihebbian differential synapse", "author": ["G. Mitchison"], "venue": "Neural Computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1991}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "The initial development of object knowledge by a learning robot", "author": ["J. Modayil", "B. Kuipers"], "venue": "Robotics and autonomous systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Autonomous learning of highlevel states and actions in continuous environments", "author": ["J. Mugan", "B. Kuipers"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Confidence-based progress-driven self-generated goals for skill acquisition in developmental robots", "author": ["H. Ngo", "M. Luciw", "A. F\u00f6rster", "J. Schmidhuber"], "venue": "Frontiers in Psychology,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Learning tactile skills through curious exploration", "author": ["L. Pape", "C.M. Oddo", "M. Controzzi", "C. Cipriani", "A. F\u00f6rster", "M.C. Carrozza", "J. Schmidhuber"], "venue": "Frontiers in neurorobotics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Which is the best intrinsic motivation signal for learning multiple skills? Intrinsic motivations and open-ended development", "author": ["V.G. Santucci", "G. Baldassarre", "M. Mirolli"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Evolution and learning in an intrinsically motivated reinforcement learning robot", "author": ["M. Schembri", "M. Mirolli", "G. Baldassarre"], "venue": "Proceedings of the 9th European Conference on Artificial Life (ECAL2007),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Curious model-building control systems", "author": ["J. Schmidhuber"], "venue": "In Proceedings of the International Joint Conference on Neural Networks, Singapore,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1991}, {"title": "A possibility for implementing curiosity and boredom in model-building neural controllers", "author": ["J. Schmidhuber"], "venue": "Proc. of the  International Conference on Simulation of Adaptive Behavior: From Animals to Animats,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1991}, {"title": "Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts", "author": ["J. Schmidhuber"], "venue": "Connection Science,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)", "author": ["J. Schmidhuber"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Maximizing fun by creating data with easily reducible subjective complexity", "author": ["J. Schmidhuber"], "venue": "In Intrinsically Motivated Learning in Natural and Artificial Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "On the relation of slow feature analysis and laplacian eigenmaps", "author": ["H. Sprekeler"], "venue": "Neural Computation,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Competence progress intrinsic motivation", "author": ["A. Stout", "A. G Barto"], "venue": "In Development and Learning (ICDL),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1998}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial intelligence,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1999}, {"title": "Many-layered learning", "author": ["P.E. Utgoff", "D.J. Stracuzzi"], "venue": "Neural Computation,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2002}, {"title": "An open-source simulator for cognitive robotics research: The prototype of the icub humanoid robot", "author": ["A. Cangelosi", "F. Nori"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2008}, {"title": "Estimating driving forces of nonstationary time series with slow feature analysis", "author": ["L. Wiskott"], "venue": "arXiv preprint cond-mat/0312317,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2003}, {"title": "Slow feature analysis: Unsupervised learning of invariances", "author": ["L. Wiskott", "T. Sejnowski"], "venue": "Neural Computation,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2002}, {"title": "Steps Towards the Object Semantic Hierarchy", "author": ["C. Xu"], "venue": "PhD thesis,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "Improving the robustness of online agglomerative clustering method based on kernel-induce distance measures", "author": ["D. Zhang", "S. Chen", "K. Tan"], "venue": "Neural processing letters,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2005}], "referenceMentions": [{"referenceID": 6, "context": "There exists several computational approaches that model different IM signals for RL agents, for example, IM signals that are based on novelty [7], prediction error [30; 4], knowledge/prediction improvements [29] and those that are based on the competence to reach a certain goal [28].", "startOffset": 143, "endOffset": 146}, {"referenceID": 27, "context": "There exists several computational approaches that model different IM signals for RL agents, for example, IM signals that are based on novelty [7], prediction error [30; 4], knowledge/prediction improvements [29] and those that are based on the competence to reach a certain goal [28].", "startOffset": 208, "endOffset": 212}, {"referenceID": 26, "context": "There exists several computational approaches that model different IM signals for RL agents, for example, IM signals that are based on novelty [7], prediction error [30; 4], knowledge/prediction improvements [29] and those that are based on the competence to reach a certain goal [28].", "startOffset": 280, "endOffset": 284}, {"referenceID": 2, "context": "Most of the intrinsically motivated RL techniques have been applied to exploring agents in simple domains [1; 35; 26; 27], agents that use hand-designed or pre-trained state abstractions of high-dimensional environments [14; 25], or agents that are provided with a low-dimensional taskspace [3].", "startOffset": 291, "endOffset": 294}, {"referenceID": 22, "context": "Mugan and Kuipers QLAP [24], Xu and Kuipers OSH [42] and Kompella et al.", "startOffset": 23, "endOffset": 27}, {"referenceID": 40, "context": "Mugan and Kuipers QLAP [24], Xu and Kuipers OSH [42] and Kompella et al.", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "learning through tracking\u201d [23] strategy to model the static background and the individual foreground objects assuming that the image background is static.", "startOffset": 27, "endOffset": 31}, {"referenceID": 39, "context": "The agent actively explores within a set of high-dimensional video streams1 and learns to select the stream where it can find the next easiest (quickest) to learn a slow feature (SF; [41]) abstraction.", "startOffset": 183, "endOffset": 187}, {"referenceID": 9, "context": "It does this optimally while simultaneously updating the SF abstractions using Incremental Slow Feature Analysis (IncSFA; [10]).", "startOffset": 122, "endOffset": 126}, {"referenceID": 36, "context": "MISFA is an optimal sequence of SF abstractions acquired in the order from easy to difficult-to-learn ones, principally similar to the learning process of Utgoff and Stracuzzi\u2019s manylayered learning [38].", "startOffset": 199, "endOffset": 203}, {"referenceID": 9, "context": "Details on the learning rules of IncSFA can be found in Kompella\u2019s previous work [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 39, "context": "For the first task, we estimate and use the time derivative of the slowness measure [41].", "startOffset": 84, "endOffset": 88}, {"referenceID": 0, "context": ", u1) is a vector [1, 1, 0, 1, 1], and the second abstraction \u03c0\u2217(.", "startOffset": 18, "endOffset": 33}, {"referenceID": 0, "context": ", u1) is a vector [1, 1, 0, 1, 1], and the second abstraction \u03c0\u2217(.", "startOffset": 18, "endOffset": 33}, {"referenceID": 0, "context": ", u1) is a vector [1, 1, 0, 1, 1], and the second abstraction \u03c0\u2217(.", "startOffset": 18, "endOffset": 33}, {"referenceID": 0, "context": ", u1) is a vector [1, 1, 0, 1, 1], and the second abstraction \u03c0\u2217(.", "startOffset": 18, "endOffset": 33}, {"referenceID": 0, "context": ", u2) is [0, 1, 1, 1, 1].", "startOffset": 9, "endOffset": 24}, {"referenceID": 0, "context": ", u2) is [0, 1, 1, 1, 1].", "startOffset": 9, "endOffset": 24}, {"referenceID": 0, "context": ", u2) is [0, 1, 1, 1, 1].", "startOffset": 9, "endOffset": 24}, {"referenceID": 0, "context": ", u2) is [0, 1, 1, 1, 1].", "startOffset": 9, "endOffset": 24}, {"referenceID": 15, "context": "After every \u03c4 observations, a value function Q and the sub-policy \u03c0ui are updated using the estimatedR via Least Squares Policy Iteration (LSPI; [16]).", "startOffset": 145, "endOffset": 149}, {"referenceID": 34, "context": "The agent uses a decaying -greedy strategy [36] on \u03c0u1 to take a new action and the process repeats.", "startOffset": 43, "endOffset": 47}, {"referenceID": 9, "context": "\u03bd is quite intuitive to set [10].", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "We use curiosity function values [19] as a metric to quantify the learning difficulty of an observation stream w.", "startOffset": 33, "endOffset": 37}, {"referenceID": 41, "context": "0 uses a clustering algorithm called the Robust Online Clustering (ROC) [43] coupled to the IncSFA.", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "0 uses a tabular reward function update rule [9]: R\u0303 \u2032 a \u2190 \u03b1 r \u2032 a + (1 \u2212 \u03b1)R\u0303 \u2032 a ; R \u2190 R\u0303/\u2016R\u0303\u2016, where \u03b1 is a constant.", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "To demonstrate this, we select an environment consisting of 3 nonlinear oscillatory streams [9] each learnable by IncSFA:", "startOffset": 92, "endOffset": 95}, {"referenceID": 8, "context": "It can be found based on the learning difficulty values [9] that the slowest feature of the stream x1 is the easiest to learn followed by x2 and then x3.", "startOffset": 56, "endOffset": 59}, {"referenceID": 0, "context": "The optimal result also includes the policy to learn these abstractions for the given environment; \u03c0\u2217 = {[0, 1, 1], [1, 0, 1], [1, 1, 0]}.", "startOffset": 105, "endOffset": 114}, {"referenceID": 0, "context": "The optimal result also includes the policy to learn these abstractions for the given environment; \u03c0\u2217 = {[0, 1, 1], [1, 0, 1], [1, 1, 0]}.", "startOffset": 105, "endOffset": 114}, {"referenceID": 0, "context": "The optimal result also includes the policy to learn these abstractions for the given environment; \u03c0\u2217 = {[0, 1, 1], [1, 0, 1], [1, 1, 0]}.", "startOffset": 116, "endOffset": 125}, {"referenceID": 0, "context": "The optimal result also includes the policy to learn these abstractions for the given environment; \u03c0\u2217 = {[0, 1, 1], [1, 0, 1], [1, 1, 0]}.", "startOffset": 116, "endOffset": 125}, {"referenceID": 0, "context": "The optimal result also includes the policy to learn these abstractions for the given environment; \u03c0\u2217 = {[0, 1, 1], [1, 0, 1], [1, 1, 0]}.", "startOffset": 127, "endOffset": 136}, {"referenceID": 0, "context": "The optimal result also includes the policy to learn these abstractions for the given environment; \u03c0\u2217 = {[0, 1, 1], [1, 0, 1], [1, 1, 0]}.", "startOffset": 127, "endOffset": 136}, {"referenceID": 0, "context": "Since x2 is easier to learn than x3, the optimal sub-policy is [1, 0, 1].", "startOffset": 63, "endOffset": 72}, {"referenceID": 0, "context": "Since x2 is easier to learn than x3, the optimal sub-policy is [1, 0, 1].", "startOffset": 63, "endOffset": 72}, {"referenceID": 0, "context": "The new optimal sub-policy after that signal swap is [0, 1, 1], since x1 is now the easiest to learn.", "startOffset": 53, "endOffset": 62}, {"referenceID": 0, "context": "The new optimal sub-policy after that signal swap is [0, 1, 1], since x1 is now the easiest to learn.", "startOffset": 53, "endOffset": 62}, {"referenceID": 0, "context": "For the rest of this section, we denote [1, 0, 1] as the old optimal sub-policy and [0, 1, 1] as the new optimal subpolicy.", "startOffset": 40, "endOffset": 49}, {"referenceID": 0, "context": "For the rest of this section, we denote [1, 0, 1] as the old optimal sub-policy and [0, 1, 1] as the new optimal subpolicy.", "startOffset": 40, "endOffset": 49}, {"referenceID": 0, "context": "For the rest of this section, we denote [1, 0, 1] as the old optimal sub-policy and [0, 1, 1] as the new optimal subpolicy.", "startOffset": 84, "endOffset": 93}, {"referenceID": 0, "context": "For the rest of this section, we denote [1, 0, 1] as the old optimal sub-policy and [0, 1, 1] as the new optimal subpolicy.", "startOffset": 84, "endOffset": 93}, {"referenceID": 37, "context": "To this end, we use the iCub Simulation software [39].", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "Furthermore, we calculated the learning difficulty values [19] and found that x1 is easier to encode by IncSFA than x3.", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "\u2019s work [10] for details on why IncSFA learns the positions).", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "deviation (shaded region) of the stream selection policy: {[0, 1, 1], [1, 1, 0]}.", "startOffset": 59, "endOffset": 68}, {"referenceID": 0, "context": "deviation (shaded region) of the stream selection policy: {[0, 1, 1], [1, 1, 0]}.", "startOffset": 59, "endOffset": 68}, {"referenceID": 0, "context": "deviation (shaded region) of the stream selection policy: {[0, 1, 1], [1, 1, 0]}.", "startOffset": 70, "endOffset": 79}, {"referenceID": 0, "context": "deviation (shaded region) of the stream selection policy: {[0, 1, 1], [1, 1, 0]}.", "startOffset": 70, "endOffset": 79}, {"referenceID": 0, "context": "Therefore, as the decays, it finds the stay action in state s1 most valuable (Figure 5(c)) and the sub-policy converges to \u03c0u1 = [0, 1, 1] (Figure 5(d)).", "startOffset": 129, "endOffset": 138}, {"referenceID": 0, "context": "Therefore, as the decays, it finds the stay action in state s1 most valuable (Figure 5(c)) and the sub-policy converges to \u03c0u1 = [0, 1, 1] (Figure 5(d)).", "startOffset": 129, "endOffset": 138}, {"referenceID": 0, "context": "The process repeats, but the gating system prevents re-learning x1 and the agent now learns \u03c0u2 = [1, 1, 0] and an abstraction \u03c62 corresponding to x3.", "startOffset": 98, "endOffset": 107}, {"referenceID": 0, "context": "The process repeats, but the gating system prevents re-learning x1 and the agent now learns \u03c0u2 = [1, 1, 0] and an abstraction \u03c62 corresponding to x3.", "startOffset": 98, "endOffset": 107}, {"referenceID": 35, "context": "options [37]) and also in environments where it can learn to reuse the learned modular abstractions to solve an external task.", "startOffset": 8, "endOffset": 12}], "year": 2017, "abstractText": "A compact information-rich representation of the environment, also called a feature abstraction, can simplify a robot\u2019s task of mapping its raw sensory inputs to useful action sequences. However, in environments that are non-stationary and only partially observable, a single abstraction is probably not sufficient to encode most variations. Therefore, learning multiple sets of spatially or temporally local, modular abstractions of the inputs would be beneficial. How can a robot learn these local abstractions without a teacher? More specifically, how can it decide from where and when to start learning a new abstraction? A recently proposed algorithm called Curious Dr. MISFA addresses this problem. The algorithm is based on two underlying learning principles called artificial curiosity and slowness. The former is used to make the robot self-motivated to explore by rewarding itself whenever it makes progress learning an abstraction; the later is used to update the abstraction by extracting slowly varying components from raw sensory inputs. Curious Dr. MISFA\u2019s application is, however, limited to discrete domains constrained by a predefined state space and has design limitations that make it unstable in certain situations. This paper presents a significant improvement that is applicable to continuous environments, is computationally less expensive, simpler to use with fewer hyper parameters, and stable in certain non-stationary environments. We demonstrate the efficacy and stability of our method in a vision-based robot simulator.", "creator": "LaTeX with hyperref package"}}}