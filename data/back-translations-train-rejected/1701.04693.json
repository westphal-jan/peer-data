{"id": "1701.04693", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jan-2017", "title": "Incremental Learning for Robot Perception through HRI", "abstract": "Scene understanding and object recognition is a difficult to achieve yet crucial skill for robots. Recently, Convolutional Neural Networks (CNN), have shown success in this task. However, there is still a gap between their performance on image datasets and real-world robotics scenarios. We present a novel paradigm for incrementally improving a robot's visual perception through active human interaction. In this paradigm, the user introduces novel objects to the robot by means of pointing and voice commands. Given this information, the robot visually explores the object and adds images from it to re-train the perception module. Our base perception module is based on recent development in object detection and recognition using deep learning. Our method leverages state of the art CNNs from off-line batch learning, human guidance, robot exploration and incremental on-line learning.", "histories": [["v1", "Tue, 17 Jan 2017 14:29:05 GMT  (4203kb,D)", "http://arxiv.org/abs/1701.04693v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.HC cs.LG", "authors": ["sepehr valipour", "camilo perez", "martin jagersand"], "accepted": false, "id": "1701.04693"}, "pdf": {"name": "1701.04693.pdf", "metadata": {"source": "CRF", "title": "Incremental Learning for Robot Perception through HRI", "authors": ["Sepehr Valipour", "Camilo Perez", "Martin Jagersand"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is so that most people are able to determine for themselves what they want and what they do not want. (...) In fact, it is so that most people are able to determine for themselves what they want. (...) It is so that they are not ready to decide. (...) It is so that they are not ready to decide. (...) It is so. (...) It is so that they want to do. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (.). (...). (.). (...). (). (). (). (.). (). (.). (...). (). (.). (). (). (). (). (). (.). ().). (...). (...). (.). (.). (). (). ().). (). (). (). ().). ().). (.). (). (). ().). (.). ().). (). (...). (.). (). (.). (). (.). ().). (. ().). (). (). (). (). (). ().). (). (). (). (). (). (). (). (). (). (). (). (). ().). (). (). (). (). (). (). (). (). (). ().). (). (). (). ().). (). (). (). (). (). ().). (). (). ().). (). ().). (). ().)."}, {"heading": "II. METHOD", "text": "A. InteractionAn example of interaction using our proposed interface is in Fig. 1. Our example shows a person soldering a board. During this work, they require the use of a multimeter that is out of their reach. He asks his robot wizard to bring the multimeter. (Fig. 1A) The robot is equipped with a recognition module. Unfortunately, the multimeter class has not been found as a recognizable object. The person then asks the robot what he sees (in view of its current world view) (Fig. 1A) to find out whether the robot recognizes the object under another name / category or does not see it at all. By speech, the robot communicates the detected objects in the scene and by pointing gestures, the robot provides object locations (Fig. 1B). Pointing allows the robot to skip complex spatial sentences (e.g. \"in my view, the laptop brings the detected objects in the upper left corner of the table.\" After communication, which objects can be detected by the robot and the object can be corrected by the human interface line or the object C."}, {"heading": "B. Localization and Recognition Network", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "C. Open-Set Recognition Facilitated by Human Guidance", "text": "The main cause of this problem is the fact that a careful semantic understanding is required to distinguish between new and known classes. However, human leadership can circumvent this problem. However, the user can reliably introduce novel objects into the system and therefore use the incremental learning approach as a basic truth. Accordingly, in this section we assume that reliable positive samples of a novel object are given to the incremental learning module through the user's interaction with the robot. In order to enable the network to detect a new class, we modify the last fully connected layer that performs the classification by adding a new set of weights corresponding to the new class. Specifically, if we designate the weights of this layer as such, we will not recognize them as such."}, {"heading": "III. SYSTEM DESCRIPTION", "text": "It consists of 7 modules, as shown in Fig. 4, all modules are fully integrated with ROS [16]. The speech recognition module integrates the CMU Sphinx Toolkit [1]. It provides basic word and sentence recognition, which is used by the robot to shift the states during interaction. \u2022 The speech synthesis module is based on the speech synthesis system of the festival [5] and provides feedback to humans in a verbal channel. \u2022 The object localization and recognition module provides labels and 2D locations of objects in the scene. \u2022 The incremental learning module uses HRI to enable changes in the robotic world."}, {"heading": "IV. EXPERIMENTS", "text": "To validate our approach, we have tested three main components of our methods: object detection and detection baseline, incremental learning algorithm, and finally incremental learning by human guidance. Incremental learning by HRIIn this experiment, we aim to mimic a real-world scenario with an assistant robot in an electronics workshop. The robot starts with object detection and detection and we try to teach it to detect new objects in the workshop. Accordingly, with our System III, we introduced new objects to the robot and the robot collected images of this new object with its eye-in-hand camera. Samples of these images can be seen in Fig. 6. After each collection, the new object is attached to the robot detection module in real time and then the other object is added. After adding each new class, we re-evaluate the detection accuracy of this new object by adding the accuracy of the test results on the MS-COCO test set to the newly added test increment class."}, {"heading": "B. Object Detection and Recognition Baseline", "text": "With the simplifications that have been made in the network II-B, we can achieve near real-time performance with our detection and detection system. Our model's inference time is 150ms for both detection and detection on the GeForce 960 GPU. It is more than twice as fast as R-CNN's 350ms on the Titan X GPU [8]. For the object detection task, we achieved an average precision (AP) of 0.2026, which is comparable to the state of the art at 0.224 [20]. Based on AP metrics, detection is counted as correct if it has an IoU of more than 0.5 with ground truth. We measured the top-1 accuracy of the base model for detection. The prediction is correct if the class with the highest probability of basic truth resembles. We achieved a detection accuracy of 0.45. We are not aware of any detection performance, but COMS comparing the COMS to COCO accuracy is expected."}, {"heading": "C. Our Incremental Learning Approach", "text": "In order to make our approach verifiable by researchers, we tested our incremental learning system using publicly available data sets. In this experiment, we started with our basic model trained on MS-COCO and then gradually added new classes to the model. These new classes are randomly selected from the Imagenet [11]. We used the same evaluation procedure as the first IVA experiment except that the labeled data for new objects now comes from the Imagenet and not from the HRI. The results are summarized in Fig. 8 in the same way as in Fig. 7 and Fig. 8. We can also observe a slight drop in performance from the robot's collected data and Imagenet data, which can contribute to the diversity of images in the Imagenet and thus reduce the likelihood of overfitting. However, this small decrease proves that the images collected by the robot work almost as well as a generic data set, at least for the local use that is our intended purpose."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "In this thesis, we introduced a novel paradigm for gradually improving the visual perception of a robot through active interactions with humans. To demonstrate the feasibility of the proposed system, a complete human-robot interface was developed that enables natural interaction with humans. Use of the system in the real world (an assistant robot in an electronics workshop) is demonstrated and its performance measured after successive additions to the perception module. Although we closely monitor the state of the art in object recognition and recognition, its performance still needs to be improved. One factor that is not particularly taken into account by the visual community is the continuity of the image stream. We expect that the use of temporal information about surrounding box positions and their labels will improve recognition and recognition accuracy."}], "references": [{"title": "Towards open world recognition", "author": ["Abhijit Bendale", "Terrance Boult"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "The Festival Speech Synthesis System: System documentation", "author": ["Alan W. Black", "Paul A. Taylor"], "venue": "Technical Report HCRC/TR-83, Human Communciation Research Centre,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Online passive-aggressive algorithms", "author": ["Koby Crammer", "Ofer Dekel", "Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Visual task specification interface for manipulation with uncalibrated visual servoing", "author": ["Mona Gridseth", "Oscar Ramirez", "Camilo Perez Quintero", "Martin Jagersand. Vita"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["Justin Johnson", "Andrej Karpathy", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1511.07571,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Pn learning: Bootstrapping binary classifiers by structural constraints", "author": ["Zdenek Kalal", "Jiri Matas", "Krystian Mikolajczyk"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Strategies for human-in-the-loop robotic grasping", "author": ["Adam Eric Leeper", "Kaijen Hsiao", "Matei Ciocarlie", "Leila Takayama", "David Gossow"], "venue": "In Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Incremental and decremental support vector machine learning", "author": ["T Poggio", "G Cauwenberghs"], "venue": "Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "The darpa robotics challenge [competitions", "author": ["Gill Pratt", "Justin Manzo"], "venue": "Robotics & Automation Magazine, IEEE,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Ros: an open-source robot operating system", "author": ["Morgan Quigley", "Ken Conley", "Brian Gerkey", "Josh Faust", "Tully Foote", "Jeremy Leibs", "Rob Wheeler", "Andrew Y Ng"], "venue": "In ICRA workshop on open source software,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Sepo: Selecting by pointing as an intuitive human-robot command interface", "author": ["Camilo Perez Quintero", "Romeo Tatsambon Fomena", "Azad Shademan", "Nina Wolleb", "Travis Dick", "Martin Jagersand"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Vibi: Assistive vision-based interface for robot manipulation", "author": ["Camilo Perez Quintero", "Oscar Ramirez", "Martin J\u00e4gersand"], "venue": "In 2015 IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Visual pointing gestures for bi-directional human robot interaction in a pick-and-place task", "author": ["Camilo Perez Quintero", "Romeo Tatsambon", "Mona Gridseth", "Martin J\u00e4gersand"], "venue": "In Robot and Human Interactive Communication (RO-MAN),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Faster rcnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing", "author": ["Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Toward open set recognition", "author": ["Walter J Scheirer", "Anderson de Rezende Rocha", "Archana Sapkota", "Terrance E Boult"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Probability models for open set recognition", "author": ["Walter J Scheirer", "Lalit P Jain", "Terrance E Boult"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Analysis of human-robot interaction at the darpa robotics challenge trials", "author": ["Holly A Yanco", "Adam Norton", "Willard Ober", "David Shane", "Anna Skinner", "Jack Vice"], "venue": "Journal of Field Robotics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}], "referenceMentions": [{"referenceID": 21, "context": "[25] conducted a study during the recent DARPA robotics challenge [15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[25] conducted a study during the recent DARPA robotics challenge [15].", "startOffset": 66, "endOffset": 70}, {"referenceID": 3, "context": "guidance is used whenever the robot cannot make a decision on its own [7], [12], [18].", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "guidance is used whenever the robot cannot make a decision on its own [7], [12], [18].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "guidance is used whenever the robot cannot make a decision on its own [7], [12], [18].", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "[23] was one of the earliest attempts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Therefore, more recent attempts [20] [8] utilized Region Proposal Networks (RPN) to", "startOffset": 32, "endOffset": 36}, {"referenceID": 4, "context": "Therefore, more recent attempts [20] [8] utilized Region Proposal Networks (RPN) to", "startOffset": 37, "endOffset": 40}, {"referenceID": 2, "context": "In the literature these problems are addressed by Incremental Machine Learning (IML) [6], [14] and Open", "startOffset": 85, "endOffset": 88}, {"referenceID": 10, "context": "In the literature these problems are addressed by Incremental Machine Learning (IML) [6], [14] and Open", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "Set Recognition(OSR) [4], [21], [22].", "startOffset": 21, "endOffset": 24}, {"referenceID": 17, "context": "Set Recognition(OSR) [4], [21], [22].", "startOffset": 26, "endOffset": 30}, {"referenceID": 18, "context": "Set Recognition(OSR) [4], [21], [22].", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "We used first 30 layers of vgg-16 [24] (counting pooling and activation layers), trained on image-net [11] for the first part of the network.", "startOffset": 34, "endOffset": 38}, {"referenceID": 7, "context": "We used first 30 layers of vgg-16 [24] (counting pooling and activation layers), trained on image-net [11] for the first part of the network.", "startOffset": 102, "endOffset": 106}, {"referenceID": 4, "context": "The structure of our localization network is based on the Densecap [8] which in turn is a modified version of the Faster RCNN [20].", "startOffset": 67, "endOffset": 70}, {"referenceID": 16, "context": "The structure of our localization network is based on the Densecap [8] which in turn is a modified version of the Faster RCNN [20].", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "For the base training of our model, we used Microsoft COCO dataset [13].", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "Components from left to right, Conv Net: first 30 layers of VGG16 [24]", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "Localization Net: Object proposal and detection network based on Densecap [8].", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "01 deviation [11].", "startOffset": 13, "endOffset": 17}, {"referenceID": 6, "context": "As for the optimizer, we use ADAM [10] due to its easy tuning.", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "4, all modules are fully integrated with ROS [16].", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": "\u2022 The speech synthesis module relies on the Festival speech synthesis system [5] and provides feedback to the human in a verbal channel.", "startOffset": 77, "endOffset": 80}, {"referenceID": 13, "context": "\u2022 The gesturing module is based on our previous work [17], [19], where we proposed a non-verbal robot-vision system capable of inferring human pointing and perform simple pick and place tasks based on human gesture", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": "\u2022 The gesturing module is based on our previous work [17], [19], where we proposed a non-verbal robot-vision system capable of inferring human pointing and perform simple pick and place tasks based on human gesture", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "lection and pick-up object actions [19].", "startOffset": 35, "endOffset": 39}, {"referenceID": 5, "context": "the collection state, a TLD tracker [9] is used to guarantee the cropping of the object during the data collection (See Fig.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "with 350ms on Titan X GPU [8].", "startOffset": 26, "endOffset": 29}, {"referenceID": 16, "context": "224 [20].", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "randomly chosen from imagenet [11].", "startOffset": 30, "endOffset": 34}], "year": 2017, "abstractText": "Scene understanding and object recognition is a difficult to achieve yet crucial skill for robots. Recently, Convolutional Neural Networks (CNN), have shown success in this task. However, there is still a gap between their performance on image datasets and real-world robotics scenarios. We present a novel paradigm for incrementally improving a robot\u2019s visual perception through active human interaction. In this paradigm, the user introduces novel objects to the robot by means of pointing and voice commands. Given this information, the robot visually explores the object and adds images from it to re-train the perception module. Our base perception module is based on recent development in object detection and recognition using deep learning. Our method leverages state of the art CNNs from off-line batch learning, human guidance, robot exploration and incremental on-line learning.", "creator": "LaTeX with hyperref package"}}}