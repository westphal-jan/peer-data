{"id": "1701.04739", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jan-2017", "title": "Summoning Demons: The Pursuit of Exploitable Bugs in Machine Learning", "abstract": "Governments and businesses increasingly rely on data analytics and machine learning (ML) for improving their competitive edge in areas such as consumer satisfaction, threat intelligence, decision making, and product efficiency. However, by cleverly corrupting a subset of data used as input to a target's ML algorithms, an adversary can perturb outcomes and compromise the effectiveness of ML technology. While prior work in the field of adversarial machine learning has studied the impact of input manipulation on correct ML algorithms, we consider the exploitation of bugs in ML implementations. In this paper, we characterize the attack surface of ML programs, and we show that malicious inputs exploiting implementation bugs enable strictly more powerful attacks than the classic adversarial machine learning techniques. We propose a semi-automated technique, called steered fuzzing, for exploring this attack surface and for discovering exploitable bugs in machine learning programs, in order to demonstrate the magnitude of this threat. As a result of our work, we responsibly disclosed five vulnerabilities, established three new CVE-IDs, and illuminated a common insecure practice across many machine learning systems. Finally, we outline several research directions for further understanding and mitigating this threat.", "histories": [["v1", "Tue, 17 Jan 2017 15:59:17 GMT  (695kb,D)", "http://arxiv.org/abs/1701.04739v1", null]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["rock stevens", "octavian suciu", "rew ruef", "sanghyun hong", "michael hicks", "tudor dumitra\\c{s}"], "accepted": false, "id": "1701.04739"}, "pdf": {"name": "1701.04739.pdf", "metadata": {"source": "CRF", "title": "The Pursuit of Exploitable Bugs in Machine Learning", "authors": ["Summoning Demons", "Rock Stevens", "Octavian Suciu", "Andrew Ruef", "Sanghyun Hong", "Michael Hicks", "Tudor Dumitras"], "emails": [], "sections": [{"heading": null, "text": "Keywords Machine Learning, Vulnerability Research, Application Security, Vulnerability Exploitation, Fuzzing"}, {"heading": "1. INTRODUCTION", "text": "This year, it will only take one year for an agreement to be reached."}, {"heading": "2. PROBLEM STATEMENT", "text": "We consider an exploit to be a piece of code designed to undermine the intended functionality of the software. If we limit our scope to machine learning, an exploit would be designed to corrupt the results of programs or impede their operation. Such exploitable bugs can either be present in the core implementation of the ML algorithm or in libraries used to extract features or model representation. In terms of impact, we distinguish between three possible outcomes of successful exploits. First, an exploit that causes specific instances to be assigned to a wrong label leads to miscalculations. Specifically, an exploit that targets the training phase leads to the poisoning of the classifier, while an explosion that targets a cluster algorithm leads to inputs being placed in different clusters, leading to misbehavior."}, {"heading": "2.1 Threat Model", "text": "We assume that the adversary has access to the source code of the program. We also assume that the adversary controls some of the input factors of the program, but is unable to prevent bona fide users from providing additional input factors. In addition, much ML software is open source in many areas, such as OpenCV [4] and Scikit Learn [27]. When looking for exploitable bugs, the adversary does not work on input factors coming from many sources, including potential adversaries. In addition, much ML software is open source, such as OpenCV [4] and Scikit Learn [27]."}, {"heading": "3.1 Machine Learning Architecture", "text": "The two main categories of learning algorithms are monitored, not supervised. In supervised learning, the algorithm receives a set of labelled examples and calculates a predictive model that matches the training examples. The predictive model is then used to classify new, unlabelled samples. In contrast, unlabelled learning relies solely on unlabelled samples to find clusters of similar samples. Figure 1 does not represent the general flow of a learning algorithm that fits all algorithms, but some of the most popular supervised techniques are variations of iterative minimisation algorithms. In unsupervised learning, clustering is one of the most common classes of algorithms. Figure 1 represents the general flow of a learning algorithm, highlighting the key features of each phase. In this setting, the input samples are transformed into a feature model that serves as a predictive model."}, {"heading": "3.2 From Architecture to Attack Surface", "text": "In fact, it is a reactionary act that is able to govern the country, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "3.3 Discovery Methods", "text": "Fuzzing [20] is a popular method of error detection. A fuzzing tool tests a program with randomly generated input, which is often invalid or unexpected due to implementation, and records program exceptions or failures. In security, fuzzing has been used to identify crashes that indicate memory security errors in the application. This technique has obvious applications for detecting a class of errors in machine learning systems - crash tests - but can we use fuzzing to find errors that silently corrupt the results of the system? In this section, we are using OpenCV as a running example as we describe our error detection method. Our use case is in a sense a natural fit for general fuzzing, because we can have a single program running on some input (i.e. an image) to produce some results (i.e. a text classification of this image). We need to make sure that we separate the two types of interest from one another and identify them."}, {"heading": "4. RESULTS", "text": "We look for exploitable ML errors in the OpenCV [4] computer vision library and the Malheur [31] library for analyzing and clustering malware. We select these libraries because they are open source and widely used. OpenCV provides its users with a common framework for computer vision applications and can process still images, live streaming videos and pre-recorded video clips. Companies can use, for example, computer vision and machine learning to strengthen physical security systems [32]. In such a scenario, an adversary might want to thwart physical security by attacking the machine learning application itself. Malheur is a security tool that analyzes malware reports recorded in sandbox environments. Malheur can cluster the reports to determine which samples are likely to belong to the same malware family; these malware reports can be raw text files or compressed file archives. Malheur relies on bare chive to extract the malware reports from the file archives."}, {"heading": "4.1 Discovery Results", "text": "In fact, most of them will be able to abide by the rules that they have applied in practice."}, {"heading": "4.2 Steered Fuzzing Results", "text": "(http: / / www.flickr.com / gp / 138669175 @ N07 / L53K8eOpenCV). (http: / / www.flickr.com / gp / 138669175 @ N07 / L53K8eOpenCV) (http: / / www.flickr.com / gp / 138669175 @ N07 / L53K8eOpenCV). (http: / / www.flickr.com / gp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ bsp _ bsp _ nbsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ nbsp _ bsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ bsp _ nbsp _ bsp _ nbsp _ nbsp _ bsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbsp _ nbs"}, {"heading": "5. RELATED WORK", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "6. DISCUSSION AND FUTURE WORK", "text": "In this paper, we focus on attacks against machine learning systems. However, our threat model has broader applicability, so nation-state opponents may be interested in attacking long-running simulations on supercomputers with the aim of subtly distorting their results. In supercomputing, the results are generally difficult to validate and expensive to recalculate, making it difficult to defend against such attacks. Other data analysis systems may also be vulnerable to such attacks. It is unclear who is responsible for some of the bugs we have discovered. Should malheur maintainers have to worry about bugs in libaric archives to maintain the integrity of their application? Should the architects of OpenCV make sacrifices to deal with invalid input that developers do not filter, and start integrating the everyday devices."}, {"heading": "7. CONCLUSIONS", "text": "Entities that rely on data from unverified sources are subject to a plethora of potential attacks where a rogue needs minimal control over the entire data set. For an attacker who wants to control the decision-making process of his competitors or opponents, this represents a powerful paradigm shift in attack vectors. We discovered several vulnerabilities within OpenCV and Malheur that allow an attacker to exploit flaws in the underlying dependencies and the applications themselves to gain a significant advantage in influencing or illegally controlling the output of ML applications."}, {"heading": "8. REFERENCES", "text": "[1] M. Barreno, B. Nelson, A. D. Joseph, and J. Tygar.The security of machine learning, A. 7, 2015. [2] B. Biggio, B. Nelson, and P. Laskov. Support of vector machines under adverse label noise. In ACML, pp. 97-112, 2011. [3] B. Biggio, I. Pillai, S. Rota Bul'o, D. Ariu, M. Pelillo, and F. Roli. Is data clustering in adversarial settings secure? In Proceedings of the 2013 ACM workshop on Artificial intelligence and security, pages 87-98. ACM, 2013. [4] G. Bradski. OpenCV. Dr. Dobb's Journal of Software Tools, 2000. [5] S. K. Cha, M. Woo, and D. Brumley. Program-adaptive mutational fuzzing."}], "references": [{"title": "The security of machine learning", "author": ["M. Barreno", "B. Nelson", "A.D. Joseph", "J. Tygar"], "venue": "Machine Learning, 81(2):121\u2013148", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Support vector machines under adversarial label noise", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "ACML, pages 97\u2013112", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "S", "author": ["B. Biggio", "I. Pillai"], "venue": "Rota Bul\u2018o, D. Ariu, M. Pelillo, and F. Roli. Is data clustering in adversarial settings secure? In Proceedings of the 2013 ACM workshop on Artificial intelligence and security, pages 87\u201398. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "OpenCV", "author": ["G. Bradski"], "venue": "Dr. Dobb\u2019s Journal of Software Tools", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Program-adaptive mutational fuzzing", "author": ["S.K. Cha", "M. Woo", "D. Brumley"], "venue": "2015 IEEE Symposium on Security and Privacy, pages 725\u2013741. IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Casting out demons: Sanitizing training data for anomaly sensors", "author": ["G.F. Cretu", "A. Stavrou", "M.E. Locasto", "S.J. Stolfo", "A.D. Keromytis"], "venue": "Security and Privacy, 2008. SP 2008. IEEE Symposium on, pages 81\u201395. IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Large-scale malware classification using random projections and neural networks", "author": ["G.E. Dahl", "J.W. Stokes", "L. Deng", "D. Yu"], "venue": "IEEE 8  International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013, Vancouver, BC, Canada, May 26-31, 2013, pages 3422\u20133426", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Bypassing PAX ASLR protection", "author": ["T. Durden"], "venue": "Phrack Magazine, 59(9):9\u20139", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "arXiv preprint arXiv:1412.6572", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Adversarial learning", "author": ["D. Lowd", "C. Meek"], "venue": "Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 641\u2013647. ACM", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Software security: building security in", "author": ["G. McGraw"], "venue": "volume 1. Addison-Wesley Professional", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Adversarial active learning", "author": ["B. Miller", "A. Kantchelian", "S. Afroz", "R. Bachwani", "E. Dauber", "L. Huang", "M.C. Tschantz", "A.D. Joseph", "J. Tygar"], "venue": "Proceedings of the 2014 Workshop on Artificial Intelligent and Security Workshop, pages 3\u201314. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical study of the reliability of UNIX utilities", "author": ["B.P. Miller", "L. Fredriksen", "B. So"], "venue": "Commun. ACM, 33(12):32\u201344", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1990}, {"title": "Paragraph: Thwarting signature learning by training maliciously", "author": ["J. Newsome", "B. Karp", "D. Song"], "venue": "Recent advances in intrusion detection, pages 81\u2013105. Springer", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Violating assumptions with fuzzing", "author": ["P. Oehlert"], "venue": "Security & Privacy, IEEE, 3(2):58\u201362", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["N. Papernot", "P.D. McDaniel", "I.J. Goodfellow", "S. Jha", "Z.B. Celik", "A. Swami"], "venue": "CoRR, abs/1602.02697", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, 12:2825\u20132830", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Behavioral clustering of http-based malware and signature generation using malicious network traces", "author": ["R. Perdisci", "W. Lee", "N. Feamster"], "venue": "Proceedings of the 7th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2010, April 28-30, 2010, San Jose, CA, USA, pages 391\u2013404", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": " why  should i trust you?\u201d: Explaining the predictions of any classifier", "author": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "venue": "arXiv preprint arXiv:1602.04938", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Fix for problem with corrupt archives., 2016", "author": ["K. Rieck"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Automatic analysis of malware behavior using machine learning", "author": ["K. Rieck", "P. Trinius", "C. Willems", "T. Holz"], "venue": "Journal of Computer Security, 19(3)", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Machine Learning for Smart Home Security Systems, 2016", "author": ["J. Sandhu"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Data mining methods for detection of new malicious executables", "author": ["M.G. Schultz", "E. Eskin", "E. Zadok", "S.J. Stolfo"], "venue": "2001 IEEE Symposium on Security and Privacy, Oakland, California, USA May 14-16, 2001, pages 38\u201349", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2001}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "arXiv preprint arXiv:1312.6199", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "The problem of concept drift: definitions and related work", "author": ["A. Tsymbal"], "venue": "Computer Science Department, Trinity College Dublin, 106", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 5, "context": "[7] discussed the importance of \u201ccasting out demons,\u201d or sanitizing the training datasets for safe machine learning ingestion.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Research on adversarial machine learning [16, 19, 1, 3, 13] has explored various attacks against ML algorithms, with a focus on skewing their outputs through malicious perturbations to the input data.", "startOffset": 41, "endOffset": 59}, {"referenceID": 11, "context": "Research on adversarial machine learning [16, 19, 1, 3, 13] has explored various attacks against ML algorithms, with a focus on skewing their outputs through malicious perturbations to the input data.", "startOffset": 41, "endOffset": 59}, {"referenceID": 0, "context": "Research on adversarial machine learning [16, 19, 1, 3, 13] has explored various attacks against ML algorithms, with a focus on skewing their outputs through malicious perturbations to the input data.", "startOffset": 41, "endOffset": 59}, {"referenceID": 2, "context": "Research on adversarial machine learning [16, 19, 1, 3, 13] has explored various attacks against ML algorithms, with a focus on skewing their outputs through malicious perturbations to the input data.", "startOffset": 41, "endOffset": 59}, {"referenceID": 8, "context": "Research on adversarial machine learning [16, 19, 1, 3, 13] has explored various attacks against ML algorithms, with a focus on skewing their outputs through malicious perturbations to the input data.", "startOffset": 41, "endOffset": 59}, {"referenceID": 12, "context": "misprediction) into a crash that can be detected by a fuzzing tool [20], which generates test cases and records program exceptions on these inputs.", "startOffset": 67, "endOffset": 71}, {"referenceID": 3, "context": "We utilize this technique to discover attacks against OpenCV [4] and Malheur [31], two open source ML implementations.", "startOffset": 61, "endOffset": 64}, {"referenceID": 20, "context": "We utilize this technique to discover attacks against OpenCV [4] and Malheur [31], two open source ML implementations.", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "These assumptions are realistic in many settings; for example, the machine learning techniques proposed for malware classification or clustering [33, 28, 10, 31] operate on inputs that come from many sources, including possible adversaries.", "startOffset": 145, "endOffset": 161}, {"referenceID": 17, "context": "These assumptions are realistic in many settings; for example, the machine learning techniques proposed for malware classification or clustering [33, 28, 10, 31] operate on inputs that come from many sources, including possible adversaries.", "startOffset": 145, "endOffset": 161}, {"referenceID": 6, "context": "These assumptions are realistic in many settings; for example, the machine learning techniques proposed for malware classification or clustering [33, 28, 10, 31] operate on inputs that come from many sources, including possible adversaries.", "startOffset": 145, "endOffset": 161}, {"referenceID": 20, "context": "These assumptions are realistic in many settings; for example, the machine learning techniques proposed for malware classification or clustering [33, 28, 10, 31] operate on inputs that come from many sources, including possible adversaries.", "startOffset": 145, "endOffset": 161}, {"referenceID": 3, "context": ", OpenCV [4] and Scikit-Learn [27].", "startOffset": 9, "endOffset": 12}, {"referenceID": 16, "context": ", OpenCV [4] and Scikit-Learn [27].", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "Fuzzing [20] is a popular method for bug discovery.", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "We search for exploitable ML bugs in the OpenCV [4] computer vision library and in the Malheur [31] library for analyzing and clustering malware behavior.", "startOffset": 48, "endOffset": 51}, {"referenceID": 20, "context": "We search for exploitable ML bugs in the OpenCV [4] computer vision library and in the Malheur [31] library for analyzing and clustering malware behavior.", "startOffset": 95, "endOffset": 99}, {"referenceID": 21, "context": "For example, businesses can use computer vision and machine learning to reinforce physical security systems [32].", "startOffset": 108, "endOffset": 112}, {"referenceID": 19, "context": "This bug could trigger another bug in Malheur\u2019s feature matrix extraction/selection and was patched on March 6, 2016 [30].", "startOffset": 117, "endOffset": 121}, {"referenceID": 23, "context": "In contrast to existing techniques for crafting adversarial samples that evade detection [34, 3, 2, 19, 1], our attack does not depend on the learned model and succeeds from the first attempt.", "startOffset": 89, "endOffset": 106}, {"referenceID": 2, "context": "In contrast to existing techniques for crafting adversarial samples that evade detection [34, 3, 2, 19, 1], our attack does not depend on the learned model and succeeds from the first attempt.", "startOffset": 89, "endOffset": 106}, {"referenceID": 1, "context": "In contrast to existing techniques for crafting adversarial samples that evade detection [34, 3, 2, 19, 1], our attack does not depend on the learned model and succeeds from the first attempt.", "startOffset": 89, "endOffset": 106}, {"referenceID": 11, "context": "In contrast to existing techniques for crafting adversarial samples that evade detection [34, 3, 2, 19, 1], our attack does not depend on the learned model and succeeds from the first attempt.", "startOffset": 89, "endOffset": 106}, {"referenceID": 0, "context": "In contrast to existing techniques for crafting adversarial samples that evade detection [34, 3, 2, 19, 1], our attack does not depend on the learned model and succeeds from the first attempt.", "startOffset": 89, "endOffset": 106}, {"referenceID": 7, "context": "An attacker can couple our PoC exploit with ASLR bypass [11] techniques using another information disclosure exploit to find the desired offset.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "Insufficient input sanitization is a common cause of exploitable bugs [17].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "Fuzzing is an automated technique that allows developers to test how gracefully their application handle various valid and invalid input [20, 23].", "startOffset": 137, "endOffset": 145}, {"referenceID": 14, "context": "Fuzzing is an automated technique that allows developers to test how gracefully their application handle various valid and invalid input [20, 23].", "startOffset": 137, "endOffset": 145}, {"referenceID": 0, "context": "[1] proposed a general classification system for these attacks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "Concept drift [35] is a phenomenon that occurs within machine learning systems as the prediction becomes less accurate over time due to unforeseen changes.", "startOffset": 14, "endOffset": 18}, {"referenceID": 11, "context": "Adversarial drift [19] describes intentionally induced concept drift in an effort to decrease the classification accuracy.", "startOffset": 18, "endOffset": 22}, {"referenceID": 2, "context": "[3] described a threat model in which an attacker desires to conceal malicious input in an effort to evade detection without negatively impacting the classification of legitimate samples.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "The adversarial classifier reverse engineering [16] describes techniques for learning sufficient information about a classifier to instrument adversarial attacks.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "[22] introduce a delusive adversary that provides malicious input in an attempt to obstruct the ML training phase; the attacker assumes full control over the input and its order.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[34] identified that an adversary can apply a perturbation to an image that is imperceptible to humans yet it changes the network\u2019s prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[13] present a fast method for generating adversarial perturbations to fool an image classifier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] further explores automated generation of such perturbations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] discuss the process and importance of \u201ccasting out demons,\u201d sanitizing ML training datasets for anomaly detection (AD) sensors.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[26] proposed model extraction attacks, by building surrogate classifiers that approximate black-box ML models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[29] proposed a technique for model explanation by building locally optimal classifiers around points of interest.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Governments and businesses increasingly rely on data analytics and machine learning (ML) for improving their competitive edge in areas such as consumer satisfaction, threat intelligence, decision making, and product efficiency. However, by cleverly corrupting a subset of data used as input to a target\u2019s ML algorithms, an adversary can perturb outcomes and compromise the effectiveness of ML technology. While prior work in the field of adversarial machine learning has studied the impact of input manipulation on correct ML algorithms, we consider the exploitation of bugs in ML implementations. In this paper, we characterize the attack surface of ML programs, and we show that malicious inputs exploiting implementation bugs enable strictly more powerful attacks than the classic adversarial machine learning techniques. We propose a semi-automated technique, called steered fuzzing, for exploring this attack surface and for discovering exploitable bugs in machine learning programs, in order to demonstrate the magnitude of this threat. As a result of our work, we responsibly disclosed five vulnerabilities, established three new CVE-IDs, and illuminated a common insecure practice across many machine learning systems. Finally, we outline several research directions for further understanding and mitigating this threat.", "creator": "LaTeX with hyperref package"}}}