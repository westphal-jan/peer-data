{"id": "1701.04783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jan-2017", "title": "Joint Deep Modeling of Users and Items Using Reviews for Recommendation", "abstract": "A large amount of information exists in reviews written by users. This source of information has been ignored by most of the current recommender systems while it can potentially alleviate the sparsity problem and improve the quality of recommendations. In this paper, we present a deep model to learn item properties and user behaviors jointly from review text. The proposed model, named Deep Cooperative Neural Networks (DeepCoNN), consists of two parallel neural networks coupled in the last layers. One of the networks focuses on learning user behaviors exploiting reviews written by the user, and the other one learns item properties from the reviews written for the item. A shared layer is introduced on the top to couple these two networks together. The shared layer enables latent factors learned for users and items to interact with each other in a manner similar to factorization machine techniques. Experimental results demonstrate that DeepCoNN significantly outperforms all baseline recommender systems on a variety of datasets.", "histories": [["v1", "Tue, 17 Jan 2017 17:46:04 GMT  (2780kb)", "http://arxiv.org/abs/1701.04783v1", "WSDM 2017"]], "COMMENTS": "WSDM 2017", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["lei zheng", "vahid noroozi", "philip s yu"], "accepted": false, "id": "1701.04783"}, "pdf": {"name": "1701.04783.pdf", "metadata": {"source": "CRF", "title": "Joint Deep Modeling of Users and Items Using Reviews for Recommendation", "authors": ["Lei Zheng", "Vahid Noroozi", "Philip S. Yu"], "emails": ["lzheng21@uic.edu", "vnoroo2@uic.edu", "psyu@uic.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "ar Xiv: 170 1.04 783v 1 [cs.L G] 17 Jan 20CCS Concepts \u2022 Information Systems \u2022 Collaborative Filtering; Recommendation Systems; \u2022 Computer Methods \u2192 Neural Networks; Keywords Recommendation Systems, Deep Learning, Convolutionary Neural Networks, Rating Prediction"}, {"heading": "1. INTRODUCTION", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "2. METHODOLOGY", "text": "The proposed model, DeepCoNN, is described in detail in this section. DeepCoNN models user behavior and item characteristics based on ratings. It learns hidden latent factors for users and items by evaluating rating texts in such a way that the learned factors can estimate the ratings given by users. It is performed using a CNN-based model consisting of two parallel neural networks coupled with a common layer at the top. Networks are trained together to predict ratings with minimal predictive error. We first describe the notations used in this essay and formulate the definition of our problem. Then, the architecture of DeepCoNN and the objective function are explained in order to optimize it. Finally, we describe how to train this model."}, {"heading": "2.1 Definition and Notation", "text": "Each tuple (u, i, rui, wui) denotes a review written by the user u for item i, with rating rui and text review wui. The mathematical notations used in this thesis are summarized in Table 1."}, {"heading": "2.2 Architecture", "text": "The model consists of two parallel neural networks that are coupled in the last layer, a network for users (Netu) and a network for items (Neti). User ratings and items are given to Netu or Neti as inputs, and corresponding ratings are produced as output. In the first layer, called the lookup layer, review texts for users or items are presented as matrices of word embeddings to capture the semantic information in the review text. Next layers are the common layers used in CNN-based models to detect multiple feature layers for users and items, including folding layer, maximum pooling layer and fully connected layer. In addition, an upper layer is added to the two networks to interact with the hidden latent factors of user and item. This layer calculates a network objective function, as the netu function is used for the evaluation error and the following factors in the evaluation process."}, {"heading": "2.3 Word Representation", "text": "A word embedding f: M \u2192 n, where M is the dictionary of words, is a parameterized function that maps words to n-dimensionally distributed vectors. Recently, this approach has increased performance in many NLP applications [12, 7]. DeepCoNN uses this representation technique to take advantage of the semantics of reviews. At the look-up level, reviews are presented as a matrix of word embeddings to extract their semantic information. To achieve this, all reviews written by user u, called user reviews, are merged into a single document consisting of n words altogether. Subsequently, a matrix of word vectors, called V u1: n, is constructed for user u as follows: V u 1: n = digit (d u 1)."}, {"heading": "2.4 CNN Layers", "text": "The next layers including folding layer, max. pooling and fully connected layer follow the CNN model introduced in [7]. Folding layer consists of m neurons that produce new features by applying folding operators to user's word vectors V u1: n. Each neuron j in the folding layer filters Kj c \u00b7 t on a window of words with the size. For V u1: n we perform a folding operation in relation to each kernel Kj in the folding layer. zj = f (V u 1: n Kj + bj) (2) Here the symbol is a folding operator, bj is a bias term and f is an activation function. In the proposed model we use Reflected linear units (ReLUs) [22]. It is defined as an equation. 3. Depth folding-related neural networks with ReLUs train many times faster than their equivalents with tanh units [14] [max.f]."}, {"heading": "2.5 The Shared Layer", "text": "Although these outputs can be viewed as attributes of users and objects, they can be in different attribute spaces and not be comparable, so to map them into the same attribute space, we introduce a common layer at the top to pair Netu and Neti. First, let's merge xu and yi into a single vector: z = (xu, yi). To model all the nested variable interactions in z, we introduce the Factorization Machine (FM) [24] as an estimate of the corresponding evaluation, so we can give their cost as an equation: 7.J = w, 0 + | z, i = 1w, iz, i + | z."}, {"heading": "2.6 Network Training", "text": "Our network is trained by minimizing Equation 7. We take derivatives of J in relation to z, as shown in Equation 8. Derivatives of other parameters in different layers can be calculated by applying a differentiation chain rule. In view of a training set consisting of N tuples T, we optimize the model by RMSprop [30] using mixed minibatches. RMSprop is an adaptive version of the gradient descend, which adaptively controls the step size in relation to the absolute value of the gradient by scaling the update value of each weight by a running average of its gradient norm. The updating rules for the network parameter set are as follows: rt \u2190 0.9 (formerly J tuples) 2 + 0.1 \u2212 1 (9)."}, {"heading": "2.7 Some Analysis on DeepCoNN", "text": "2.7.1 Word Order Preservation Most recommendation systems that use ratings within the modeling process use thematic modeling techniques to model users or items [6]. Thematic modeling techniques derive latent theme variables using the sack-of-words assumption that ignores word order. However, in many text modeling applications, word order is critical [32]. DeepCoNN is not based on thematic modeling and uses word embedding to create a matrix of word vectors that preserves the word order. In this way, folding operations use the internal structure of data and provide a mechanism for efficiently using the sequence of words in text modeling [11]. 2.7.2 Scalability of online learning and handling dynamic pools of items and users are considered critical needs of many recommendation systems, and the time sensitivity of recommendation systems can also be updated on the basis of these factors and the scope of recommendation systems."}, {"heading": "3. EXPERIMENTS", "text": "We have conducted extensive experiments on a variety of data sets to demonstrate the effectiveness of DeepCoNN compared to other state-of-the-art recommendation systems. First, we present the data sets and evaluation metrics used in our experiments in Section 3.1. Basic algorithms selected for comparisons are explained in Section 3.2. Experimental settings are explained in Section 3.3. Performance evaluation and some analyses of the model are discussed in Section 3.4 and 3.5 respectively."}, {"heading": "3.1 Datasets and Evaluation Metric", "text": "In our experiments, we selected the following three sets of data to evaluate our model. \u2022 Yelp: This is a large-scale data set consisting of restaurant ratings introduced in the 6th round of Yelp Challenge 1 in 2015. It contains more than 1M reviews and ratings. \u2022 Amazon: Amazon Review Data Set [19] contains product ratings and metadata from Amazon's website2. It includes more than 143.7 million reviews from May 1996 to July 2014. It includes 21 categories of items, and as far as we know, it is the largest publicly available data set of text ratings. \u2022 Beer: It is one from ratebeer.com. The data covers a period of more than 10 years, including nearly 3 million reviews through November 2011 [18]. As we can see in Table 2, all records contain more than half a million reviews."}, {"heading": "3.2 Baselines", "text": "To confirm the effectiveness of DeepCoNN, we have selected three categories of rating algorithms: (i) purely valuation-based models. We chose Matrix Factorization (MF) and Probabilistic Matrix Factorization (PMF) to validate this rating information for recommendation systems, (ii) topic models that use rating information. Most recommendation systems that take ratings into account are based on topic modeling techniques. To compare our model with topic-based recommendation systems, we choose three representative models: Latent Dirichlet Allocation (LDA) [5], Collaborative Topic Regression (CTR), and Hidden Factor as Topic (HFT)."}, {"heading": "3.3 Experimental Settings", "text": "We divided each data set shown in Table 2 into three sets of training sets, validation sets and test sets. We use 80% of each data set as a training set, 10% are treated as a validation set to set the hyperparameters, and the rest is used as a test set. All baseline and DeepCoNN hyperparameters are selected based on performance on the validation set. For MF and PMF, we used the grid search to find the best values for the number of latent factors from {25, 50, 100, 150, 200} and regulation parameters from {0.001, 0.01, 0.1, 1.0}.For LDA, CTR and HFT, the number of topics K is selected to match the number of latent factors from {25, 100, 150, 200} and regulation parameters from {0.001, 0.1, 1.0}."}, {"heading": "3.4 Performance Evaluation", "text": "The performance of DeepCoNN and baselines (see Section 3.2) is presented with respect to the MSE in Tables 3. Table 3 shows the results on the three data sets, including the averaged performance on all 21 categories of Amazon. The experiments are repeated 3 times, and the averages are presented in bold with the best performance. The last column indicates the percentage of improvements that DeepCoNN has made compared to the best baseline in the corresponding category. In Table 3, all the models show better performance on Beer data set than on Yelp and Amazon. It mainly depends on the scarcity of Yelp and Amazon. Although PMF performs better on Yelp, Beer and most categories of Amazon than MF, both techniques do not perform well compared to those that use ratings. It confirms our hypothesis that review text provides additional information, and the consideration of ratings in models can improve the rating of LDA. Although simply the use of LDA pre-scores can contribute to achieving improvements from each other, the results of LDA can be evaluated independently."}, {"heading": "3.5 Model Analysis", "text": "The two parallel networks really cooperate to learn effective functions from reviews? Has the proposed model benefited from the use of word embedding to exploit the semantic information in the review text? How much does the common level help improve predictor accuracy compared to 3https: / / code.google.com / archive / p / word2vec / a simpler coupling method? To answer these questions, we compare the DeepCoNN with its five variants: \u2022 DeepCoNNNUser, DeepCoNN-Item, DeepCoNN-TFIDF, DeepCoNN-Rndom and DeepCoNN-DP. These five variants are summarized in the following: DeepCoNN-User: The Neti of DeepCoNN is substituted with a matrix. Each set of the matrix is the latent factor of an article."}, {"heading": "3.6 The Impact of the Number of Reviews", "text": "The cold start problem [27] is widespread in recommendation systems, especially when a new user is added or a new element is added to the system, the available ratings are limited. It would not be easy for the system to determine the preferences of such users based only on their ratings. In some of the previous work, it was shown that the use of review texts can help alleviate this problem, especially for users or articles with few ratings [17]. In this section, we are conducting a series of experiments to answer the following questions. Can DeepCoNN help solve the cold start problem? How does the number of reviews affect the effectiveness of the proposed algorithm? In Fig. 3, we have the reductions in MSE by DeepCoNN compared to the MF technique on three sets of data from Yelp, Beer and a group of Amazon (musical instruments) on the effectiveness of the proposed algorithm? By reducing the MSE from MF to MSE, we mean the difference between MSE and MSE."}, {"heading": "4. RELATED WORKS", "text": "In fact, most of them are able to survive on their own."}, {"heading": "5. CONCLUSION", "text": "In this paper, we introduced Deep Cooperative Neural Networks (DeepCoNN), which use the information available in the ratings for recommendation systems. DeepCoNN consists of two deep neural networks that are linked by a common layer of model users and articles from the reviews. DeepCoNN maps the user and article representations in a common feature space. Similar to MF techniques, users and the latent factors of the article can interact effectively to predict the appropriate rating. Compared to state-of-the-art baselines, DeepCoNN achieved 8.5% and 7.6% improvements in data sets from Yelp and Beer. On Amazon, it outperformed all baselines and achieved 8.7% improvements in the DeepCoNN rating. Overall, the proposed model achieves an 8.3% improvement on all three datasets."}, {"heading": "6. ACKNOWLEDGEMENTS", "text": "This work is supported in part by NSF grants IIS-1526499 and CNS-1626432. We thank NVIDIA Corporation for their support by donating the Titan X GPU used for this research."}, {"heading": "7. REFERENCES", "text": "[1] A. Almahairi, K. Cho, and A. Courville.Learning distributed representations from reviews for collaborative filtering. In Proceedings of the 9th ACM Conference on Recommender Systems, pages 147-154. ACM, 2015. [2] S. Baccianella, A. Esuli, and F. Sebastiani. Multi-facet rating of product reviews. In Advances in Information Retrieval, pages 461-472. S. Baccianella, A. Zhang. Topicmf: Simultaneously exploiting ratings and reviews of product reviews. In AAAI, pages 2-8. AAI Press, 2014."}], "references": [{"title": "Learning distributed representations from reviews for collaborative filtering", "author": ["A. Almahairi", "K. Kastner", "K. Cho", "A. Courville"], "venue": "Proceedings of the 9th ACM Conference on Recommender Systems, pages 147\u2013154. ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-facet rating of product reviews", "author": ["S. Baccianella", "A. Esuli", "F. Sebastiani"], "venue": "Advances in Information Retrieval, pages 461\u2013472. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Topicmf: Simultaneously exploiting ratings and reviews for recommendation", "author": ["Y. Bao", "H. Fang", "J. Zhang"], "venue": "AAAI, pages 2\u20138. AAAI Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, 3:993\u20131022", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Recommender systems based on user reviews: the state of the art", "author": ["L. Chen", "G. Chen", "F. Wang"], "venue": "User Modeling and User-Adapted Interaction, 25(2):99\u2013154", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Jointly modeling aspects", "author": ["Q. Diao", "M. Qiu", "C. Wu", "A.J. Smola", "J. Jiang", "C. Wang"], "venue": "ratings and sentiments for movie recommendation (JMARS). In KDD, pages 193\u2013202. ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "A multi-view deep learning approach for cross domain user modeling in recommendation systems", "author": ["A.M. Elkahky", "Y. Song", "X. He"], "venue": "Proceedings of the 24th International Conference on World Wide Web, pages 278\u2013288. International World Wide Web Conferences Steering Committee", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Beyond the stars: exploiting free-text user reviews to improve the accuracy of movie recommendations", "author": ["N. Jakob", "S.H. Weber", "M.C. M\u00fcller", "I. Gurevych"], "venue": "Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion, pages 57\u201364. ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["R. Johnson", "T. Zhang"], "venue": "HLT-NAACL, pages 103\u2013112. The Association for Computational Linguistics", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "arXiv preprint arXiv:1408.5882", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer, (8):30\u201337", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep collaborative filtering via marginalized denoising auto-encoder", "author": ["S. Li", "J. Kawale", "Y. Fu"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 811\u2013820. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Ratings meet reviews", "author": ["G. Ling", "M.R. Lyu", "I. King"], "venue": "a combined approach to recommend. In Proceedings of the 8th ACM Conference on Recommender systems, pages 105\u2013112. ACM", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Hidden factors and hidden topics: understanding rating dimensions with review text", "author": ["J. McAuley", "J. Leskovec"], "venue": "Proceedings of the 7th ACM conference on Recommender systems, pages 165\u2013172. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning attitudes and attributes from multi-aspect reviews", "author": ["J. McAuley", "J. Leskovec", "D. Jurafsky"], "venue": "Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages 1020\u20131025. IEEE", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Inferring networks of substitutable and complementary products", "author": ["J. McAuley", "R. Pandey", "J. Leskovec"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 785\u2013794. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH, pages 1045\u20131048", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": " Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "One-class collaborative filtering", "author": ["R. Pan", "Y. Zhou", "B. Cao", "N.N. Liu", "R. Lukose", "M. Scholz", "Q. Yang"], "venue": "Data Mining, 2008. ICDM\u201908. Eighth IEEE International Conference on, pages 502\u2013511. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Factorization machines with libfm", "author": ["S. Rendle"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 3(3):57", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic matrix factorization", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "NIPS, pages 1257\u20131264. Curran Associates, Inc.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th international conference on Machine learning, pages 791\u2013798. ACM", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Methods and metrics for cold-start recommendations", "author": ["A.I. Schein", "A. Popescul", "L.H. Ungar", "D.M. Pennock"], "venue": "Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 253\u2013260. ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Deep content-based music recommendation", "author": ["A. Van den Oord", "S. Dieleman", "B. Schrauwen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Topic modeling: beyond bag-of-words", "author": ["H.M. Wallach"], "venue": "Proceedings of the 23rd international conference on Machine learning, pages 977\u2013984. ACM", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Collaborative topic modeling for recommending scientific articles", "author": ["C. Wang", "D.M. Blei"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 448\u2013456. ACM", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Latent aspect rating analysis on review text data: a rating regression approach", "author": ["H. Wang", "Y. Lu", "C. Zhai"], "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 783\u2013792. ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Collaborative deep learning for recommender systems", "author": ["H. Wang", "N. Wang", "D.-Y. Yeung"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1235\u20131244. ACM", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving content-based and hybrid music recommendation using deep learning", "author": ["X. Wang", "Y. Wang"], "venue": "Proceedings of the ACM International Conference on Multimedia, pages 627\u2013636. ACM", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "FLAME: A probabilistic model combining aspect based opinion mining and collaborative filtering", "author": ["Y. Wu", "M. Ester"], "venue": "WSDM, pages 199\u2013208. ACM", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Many of the prominent approaches employed in recommender systems [13] are based on Collaborative Filtering (CF) techniques.", "startOffset": 65, "endOffset": 69}, {"referenceID": 12, "context": "Many of the most successful CF techniques are based on matrix factorization [13].", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "Although CF techniques have shown good performance for many applications, the sparsity problem is considered as one of their significant challenges [13].", "startOffset": 148, "endOffset": 152}, {"referenceID": 15, "context": "One of the approaches employed to address this lack of data is using the information in review text [16, 17].", "startOffset": 100, "endOffset": 108}, {"referenceID": 16, "context": "One of the approaches employed to address this lack of data is using the information in review text [16, 17].", "startOffset": 100, "endOffset": 108}, {"referenceID": 16, "context": "Recently, some studies [17] [16] have shown that using review text can improve the prediction accuracy of recommender systems, in particular for the items and users with few ratings [34].", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "Recently, some studies [17] [16] have shown that using review text can improve the prediction accuracy of recommender systems, in particular for the items and users with few ratings [34].", "startOffset": 28, "endOffset": 32}, {"referenceID": 32, "context": "Recently, some studies [17] [16] have shown that using review text can improve the prediction accuracy of recommender systems, in particular for the items and users with few ratings [34].", "startOffset": 182, "endOffset": 186}, {"referenceID": 12, "context": "This interaction layer is motivated by matrix factorization techniques [13] to let latent factors of users and items interact with each other.", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "Another key contribution is that DeepCoNN represents review text using pre-trained word-embedding technique [21, 20] to extract semantic information from the reviews.", "startOffset": 108, "endOffset": 116}, {"referenceID": 19, "context": "Another key contribution is that DeepCoNN represents review text using pre-trained word-embedding technique [21, 20] to extract semantic information from the reviews.", "startOffset": 108, "endOffset": 116}, {"referenceID": 6, "context": "Recently, this representation has shown excellent results in many Natural Language Processing (NLP) tasks [7, 4, 21].", "startOffset": 106, "endOffset": 116}, {"referenceID": 3, "context": "Recently, this representation has shown excellent results in many Natural Language Processing (NLP) tasks [7, 4, 21].", "startOffset": 106, "endOffset": 116}, {"referenceID": 20, "context": "Recently, this representation has shown excellent results in many Natural Language Processing (NLP) tasks [7, 4, 21].", "startOffset": 106, "endOffset": 116}, {"referenceID": 16, "context": "Moreover, a significant advantage of DeepCoNN compared to most other approaches [17, 16] which benefit from reviews is that it models users and items in a joint manner with respect to prediction accuracy.", "startOffset": 80, "endOffset": 88}, {"referenceID": 15, "context": "Moreover, a significant advantage of DeepCoNN compared to most other approaches [17, 16] which benefit from reviews is that it models users and items in a joint manner with respect to prediction accuracy.", "startOffset": 80, "endOffset": 88}, {"referenceID": 18, "context": "The experiments on real-world datasets including Yelp, Amazon [19], and Beer [18] show that DeepCoNN outperforms all the compared baselines in prediction accuracy.", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "The experiments on real-world datasets including Yelp, Amazon [19], and Beer [18] show that DeepCoNN outperforms all the compared baselines in prediction accuracy.", "startOffset": 77, "endOffset": 81}, {"referenceID": 35, "context": "All competing techniques which are based on topic modeling [38, 3, 8] use the traditional bag of words techniques.", "startOffset": 59, "endOffset": 69}, {"referenceID": 2, "context": "All competing techniques which are based on topic modeling [38, 3, 8] use the traditional bag of words techniques.", "startOffset": 59, "endOffset": 69}, {"referenceID": 7, "context": "All competing techniques which are based on topic modeling [38, 3, 8] use the traditional bag of words techniques.", "startOffset": 59, "endOffset": 69}, {"referenceID": 16, "context": "It outperforms state-of-the-art techniques [17, 33, 25, 35] in terms of prediction accuracy on all of the evaluated datasets including Yelp, 21 categories of Amazon, and Beer (see Section 3).", "startOffset": 43, "endOffset": 59}, {"referenceID": 31, "context": "It outperforms state-of-the-art techniques [17, 33, 25, 35] in terms of prediction accuracy on all of the evaluated datasets including Yelp, 21 categories of Amazon, and Beer (see Section 3).", "startOffset": 43, "endOffset": 59}, {"referenceID": 24, "context": "It outperforms state-of-the-art techniques [17, 33, 25, 35] in terms of prediction accuracy on all of the evaluated datasets including Yelp, 21 categories of Amazon, and Beer (see Section 3).", "startOffset": 43, "endOffset": 59}, {"referenceID": 33, "context": "It outperforms state-of-the-art techniques [17, 33, 25, 35] in terms of prediction accuracy on all of the evaluated datasets including Yelp, 21 categories of Amazon, and Beer (see Section 3).", "startOffset": 43, "endOffset": 59}, {"referenceID": 11, "context": "Recently, this approach has boosted the performance in many NLP applications [12, 7].", "startOffset": 77, "endOffset": 84}, {"referenceID": 6, "context": "Recently, this approach has boosted the performance in many NLP applications [12, 7].", "startOffset": 77, "endOffset": 84}, {"referenceID": 6, "context": "Next layers including convolution layer, max pooling, and fully connected layer follow the CNN model introduced in [7].", "startOffset": 115, "endOffset": 118}, {"referenceID": 21, "context": "In the proposed model, we use Rectified Linear Units (ReLUs) [22].", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "Deep convolutional neural networks with ReLUs train several times faster than their equivalents with tanh units [14].", "startOffset": 112, "endOffset": 116}, {"referenceID": 6, "context": "Following the work of [7], we then apply Eq.", "startOffset": 22, "endOffset": 25}, {"referenceID": 23, "context": "To model all nested variable interactions in \u1e91, we introduce Factorization Machine (FM) [24] as the estimator of the corresponding rating.", "startOffset": 88, "endOffset": 92}, {"referenceID": 28, "context": "Given a set of training set T consisting of N tuples, we optimize the model through RMSprop [30] over shuffled minibatches.", "startOffset": 92, "endOffset": 96}, {"referenceID": 27, "context": "Additionally, to prevent overfitting, the dropout [28] strategy has also been applied to the fully connected layers of the two networks.", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "Most of the recommender systems which use reviews in the modeling process employ topic modeling techniques to model users or items [6].", "startOffset": 131, "endOffset": 134}, {"referenceID": 30, "context": "However, in many text modeling applications, word order is crucial [32].", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "In this way, convolution operations make use of the internal structure of data and provide a mechanism for efficient use of words\u2019 order in text modeling [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 18, "context": "\u2022 Amazon: Amazon Review dataset [19] contains product reviews and metadata from Amazon website.", "startOffset": 32, "endOffset": 36}, {"referenceID": 17, "context": "The data span a period of more than 10 years, including almost 3 million reviews up to November 2011 [18].", "startOffset": 101, "endOffset": 105}, {"referenceID": 16, "context": "It is selected because most of the related works have used the same evaluation metric[17, 16, 1].", "startOffset": 85, "endOffset": 96}, {"referenceID": 15, "context": "It is selected because most of the related works have used the same evaluation metric[17, 16, 1].", "startOffset": 85, "endOffset": 96}, {"referenceID": 0, "context": "It is selected because most of the related works have used the same evaluation metric[17, 16, 1].", "startOffset": 85, "endOffset": 96}, {"referenceID": 4, "context": "To compare our model with topic modeling based recommender systems, we select three representative models: Latent Dirichlet Allocation (LDA) [5], Collaborative Topic Regression (CTR) [33] and Hidden Factor as Topic (HFT) [17], and (iii) deep recommender systems.", "startOffset": 141, "endOffset": 144}, {"referenceID": 31, "context": "To compare our model with topic modeling based recommender systems, we select three representative models: Latent Dirichlet Allocation (LDA) [5], Collaborative Topic Regression (CTR) [33] and Hidden Factor as Topic (HFT) [17], and (iii) deep recommender systems.", "startOffset": 183, "endOffset": 187}, {"referenceID": 16, "context": "To compare our model with topic modeling based recommender systems, we select three representative models: Latent Dirichlet Allocation (LDA) [5], Collaborative Topic Regression (CTR) [33] and Hidden Factor as Topic (HFT) [17], and (iii) deep recommender systems.", "startOffset": 221, "endOffset": 225}, {"referenceID": 33, "context": "In [35], authors have proposed a state-of-the-art deep recommender system named Collaborative Deep Learning (CDL).", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "\u2022 MF: Matrix Factorization [13] is the most popular CF-based recommendation method.", "startOffset": 27, "endOffset": 31}, {"referenceID": 24, "context": "\u2022 PMF:Probabilistic MatrixFactorization is introduced in [25].", "startOffset": 57, "endOffset": 61}, {"referenceID": 4, "context": "\u2022 LDA:LatentDirichletAllocation is a well-known topic modeling algorithm presented in [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 16, "context": "In [17], it is proposed to employ LDA to learn a topic distribution from a set of reviews for each item.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "\u2022 CTR: Collaborative Topic Regression has been proposed by [33].", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "\u2022 HFT: Hidden Factor as Topic proposed in [17] employs topic distributions to learn latent factors from user or item reviews.", "startOffset": 42, "endOffset": 46}, {"referenceID": 22, "context": "The CTR model solves the one-class collaborative filtering problem [23] by using two different values for the precision parameter c of a Gaussian distribution.", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "Following the work of [16], in our experiments, we set precision c as the same for all the observed ratings for rating prediction.", "startOffset": 22, "endOffset": 26}, {"referenceID": 20, "context": "on more than 100 billion words from Google News [21] .", "startOffset": 48, "endOffset": 52}, {"referenceID": 26, "context": "The cold start problem [27] is prevalent in recommender systems.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "It has been shown in some of the previous works that exploiting review text can help to alleviate this problem especially for users or items with few ratings [17].", "startOffset": 158, "endOffset": 162}, {"referenceID": 1, "context": "The first studies that used online review text in rating prediction tasks were mostly focused on predicting ratings for an existing review [2, 38], while in our paper, we predict the ratings from the history of review text written by a user to recommend desirable products to that user.", "startOffset": 139, "endOffset": 146}, {"referenceID": 35, "context": "The first studies that used online review text in rating prediction tasks were mostly focused on predicting ratings for an existing review [2, 38], while in our paper, we predict the ratings from the history of review text written by a user to recommend desirable products to that user.", "startOffset": 139, "endOffset": 146}, {"referenceID": 9, "context": "One of the pioneer works that explored using reviews to improve the rating prediction is presented in [10].", "startOffset": 102, "endOffset": 106}, {"referenceID": 16, "context": "In [17], the authors proposed Hidden Factors as Topics (HFT) to employ topic modeling techniques to discover latent aspects from either item or user reviews.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "A similar approach is followed in [3] with the main difference that it models user\u2019s and items\u2019 reviews simultaneously.", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "In [8], a probabilistic model is proposed based on collaborative filtering and topic modeling.", "startOffset": 3, "endOffset": 6}, {"referenceID": 15, "context": "Ratings Meet Reviews (RMR) [16] also tries to harness the information of both ratings and reviews.", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "Several works [26, 37, 15] model users and/or items from the rating matrix using neural networks like denoising auto-encoders or Restricted Boltzmann Machines (RBM).", "startOffset": 14, "endOffset": 26}, {"referenceID": 14, "context": "Several works [26, 37, 15] model users and/or items from the rating matrix using neural networks like denoising auto-encoders or Restricted Boltzmann Machines (RBM).", "startOffset": 14, "endOffset": 26}, {"referenceID": 29, "context": "In [31] and [36], deep models of CNN and Deep Belief Network (DBN) are introduced to learn latent factors from music data for music recommendation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "In [31] and [36], deep models of CNN and Deep Belief Network (DBN) are introduced to learn latent factors from music data for music recommendation.", "startOffset": 12, "endOffset": 16}, {"referenceID": 33, "context": "A similar approach is followed in [35] for movie recommendation by using a generalized Stacked Auto Encoder (SAE) model.", "startOffset": 34, "endOffset": 38}, {"referenceID": 29, "context": "In all these works [31, 36, 35], an item\u2019s latent factors are learned from item\u2019s content and review text is ignored.", "startOffset": 19, "endOffset": 31}, {"referenceID": 34, "context": "In all these works [31, 36, 35], an item\u2019s latent factors are learned from item\u2019s content and review text is ignored.", "startOffset": 19, "endOffset": 31}, {"referenceID": 33, "context": "In all these works [31, 36, 35], an item\u2019s latent factors are learned from item\u2019s content and review text is ignored.", "startOffset": 19, "endOffset": 31}, {"referenceID": 8, "context": "In [9], a multi-view deep model is built to learn the user and item latent factors in a joint manner and map them to a common space.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "To the best of our knowledge, the only work which has utilized deep learning techniques to use review text to improve recommendation is presented in [1].", "startOffset": 149, "endOffset": 152}], "year": 2017, "abstractText": "A large amount of information exists in reviews written by users. This source of information has been ignored by most of the current recommender systems while it can potentially alleviate the sparsity problem and improve the quality of recommendations. In this paper, we present a deep model to learn item properties and user behaviors jointly from review text. The proposed model, named Deep Cooperative Neural Networks (DeepCoNN), consists of two parallel neural networks coupled in the last layers. One of the networks focuses on learning user behaviors exploiting reviews written by the user, and the other one learns item properties from the reviews written for the item. A shared layer is introduced on the top to couple these two networks together. The shared layer enables latent factors learned for users and items to interact with each other in a manner similar to factorization machine techniques. Experimental results demonstrate that DeepCoNN significantly outperforms all baseline recommender systems on a variety of datasets. CCS Concepts \u2022Information systems\u2192Collaborative filtering; Recommender systems; \u2022Computing methodologies \u2192 Neural networks;", "creator": "LaTeX with hyperref package"}}}