{"id": "1701.04949", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2017", "title": "A Deep Convolutional Auto-Encoder with Pooling - Unpooling Layers in Caffe", "abstract": "This paper presents the development of several models of a deep convolutional auto-encoder in the Caffe deep learning framework and their experimental evaluation on the example of MNIST dataset. We have created five models of a convolutional auto-encoder which differ architecturally by the presence or absence of pooling and unpooling layers in the auto-encoder's encoder and decoder parts. Our results show that the developed models provide very good results in dimensionality reduction and unsupervised clustering tasks, and small classification errors when we used the learned internal code as an input of a supervised linear classifier and multi-layer perceptron. The best results were provided by a model where the encoder part contains convolutional and pooling layers, followed by an analogous decoder part with deconvolution and unpooling layers without the use of switch variables in the decoder part. The paper also discusses practical details of the creation of a deep convolutional auto-encoder in the very popular Caffe deep learning framework. We believe that our approach and results presented in this paper could help other researchers to build efficient deep neural network architectures in the future.", "histories": [["v1", "Wed, 18 Jan 2017 05:24:24 GMT  (3514kb)", "http://arxiv.org/abs/1701.04949v1", "21 pages, 11 figures, 5 tables, 62 references"]], "COMMENTS": "21 pages, 11 figures, 5 tables, 62 references", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["volodymyr turchenko", "eric chalmers", "artur luczak"], "accepted": false, "id": "1701.04949"}, "pdf": {"name": "1701.04949.pdf", "metadata": {"source": "CRF", "title": "A Deep Convolutional Auto-Encoder with Pooling - Unpooling Layers in Caffe", "authors": ["Volodymyr Turchenko", "Eric Chalmers", "Artur Luczak"], "emails": ["luczak}@uleth.ca"], "sections": [{"heading": null, "text": "In fact, the fact is that most of them are able to move, to move and to move."}, {"heading": "2. Related Work", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "3. Model description", "text": "A typical network begins with a data layer that loads data from a hard disk and ends with one or more loss layers indicating a learning objective (also known as an error, cost or loss function) Our CAE models include a convolutional, pooling, full-composite, deconversion and loss layer described by the expression [26]. (For this reason, the conversion and deconversion layer is followed by an activation function described by the expression [26]. (For this reason]) Llkklk produces the latent representation of the current layer, f is an activation function (normally not linear), lx is l -th feature card of the group of function boards L of the previous layer or l -th channel of input of total L channels in a case of the first layer."}, {"heading": "4. Experimental results", "text": "This year, it has reached the stage where it will be able to take the lead in order to achieve the objectives I have mentioned."}, {"heading": "5. Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Why does Model 3 WITH switches provide the worst dimensionality reduction and unsupervised clustering?", "text": "It's one of the biggest nodes in the world that's ever existed, that's what it's about, that's what it's about. \"It's one of the biggest nodes that's ever existed,\" he says. \"It's the way it is,\" he says. \"It's one of the biggest nodes that's ever existed,\" he says."}, {"heading": "6. Conclusions", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "Acknowledgements", "text": "We thank the Caffe developers (the Berkeley Vision and Learning Center, UC Berkeley) for creating such a powerful framework for deep machine learning research. We thank Karim Ali (CCBN) for helping install Caffe on Hodgkin and Polaris1, Hyeonwoo Noh (POSTECH, Korea) for using his Caffe implementation of the unwinding layer and discussing some of the results presented in this paper, and Dr. Robert Sutherland (CCBN) for helping to provide financial support."}], "references": [{"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature. 323 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "Modeles connexionistes de l\u2019apprentissage", "author": ["Y. LeCun"], "venue": "Ph.D. thesis, Universite de Paris VI", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1987}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourland", "Y. Kamp"], "venue": "Biological Cybernetics. 59 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1988}, {"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["P. Baldi", "K. Hornik"], "venue": "Neural Networks. 2 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1989}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science. 313 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. of the IEEE. 86 (11) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Torch7: A Matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "in: J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, K.Q. Weinberger (Eds.), Advances in Neural Information Processing Systems 24, NIPS Foundation Inc., Granada", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv:1408.5093", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Creation of a deep convolutional auto-encoder in Caffe", "author": ["V. Turchenko", "A. Luczak"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M. Ranzato", "F.J. Huang", "Y.-L. Boureau", "Y. LeCun"], "venue": "in: 2007 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Minneapolis, MN", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "in: 2008 25th International Conference on Machine Learning (ICML), International Machine Learning Society, Helsinki", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "Book in preparation for MIT Press, http://www.deeplearningbook.org, 2016 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Transforming auto-encoders", "author": ["G.E. Hinton", "A. Krizhevsky", "S.D. Wang"], "venue": "Lecture Notes in Computer Sci. 6791 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "in: 28th International Conference on Machine Learning (ICML), International Machine Learning Society, Bellevue, WA", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "K-Sparse autoencoders", "author": ["A. Makhzani", "B. Frey"], "venue": "in: International Conference on Learning Representations (ICLR), arXiv:1312.5663, Banff", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "in: International Conference on Learning Representations (ICLR), arXiv:1312.6114, Banff", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Importance weighted autoencoders", "author": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"], "venue": "arXiv:1509.00519", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial autoencoders", "author": ["A. Makhzani", "J. Shlens", "N. Jaitly", "I. Goodfellow", "B. Frey"], "venue": "in: International Conference on Learning Representations (ICLR), arXiv:1511.05644, San Juan", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "in: 26th Annual International Conference on Machine Learning (ICML), ACM, New York, NY", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Stacks of convolutional restricted boltzmann machines for shift-invariant feature learning", "author": ["M. Norouzi", "M. Ranjbar", "G. Mori"], "venue": "in: 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, Miami, FL", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["J. Masci", "U. Meier", "D. Ciresan", "J. Schmidhuber"], "venue": "Lecture Notes in Computer Sci. 6791 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Stacked what-where auto-encoders", "author": ["J. Zhao", "M. Mathieu", "R. Goroshin", "Y. LeCun"], "venue": "in: International Conference on Learning Representations (ICLR), town, arXiv:1506.02351, San Juan", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "in: 2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, San Francisco, CA", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M.D. Zeiler", "G.W. Taylor", "R. Fergus"], "venue": "in: 2011 IEEE International Conference on Computer Vision (ICCV), IEEE, Barcelona", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "J. Machine Learning Res. 15 ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluation of pooling operations in convolutional architectures for object recognition", "author": ["D. Scherer", "A. Muller", "S. Behnke"], "venue": "Lecture Notes in Computer Sci. 6354 ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Striving for simplicity: the all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "in: International Conference on Learning Representations (ICLR), arXiv:1412.6806, San Diego", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Discriminative unsupervised feature learning with exemplar convolutional neural networks", "author": ["A. Dosovitskiy", "P. Fischer", "J. Springenberg", "M. Riedmiller", "T. Brox"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence. 38 (9) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional autoencoders in python/theano/lasagne", "author": ["M. Swarbrick Jones"], "venue": "https://swarbrickjones.wordpress.com/2015/04/29/convolutional-autoencoders-in-pythontheanolasagne/, 2015 ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Training autoencoders on ImageNet using Torch 7", "author": ["S. Khallaghi"], "venue": "http://siavashk.github.io/2016/02/22/autoencoder-imagenet/, 2016 ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Modified version of Caffe which support DeconvNet and DecoupledNet", "author": ["H. Noh"], "venue": "https://github.com/HyeonwooNoh/caffe, 2015 ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "in: 2015 IEEE International Conference on Computer Vision (ICCV), IEEE, Santiago", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "A tutorial on the cross-entropy method", "author": ["P.T. De Boer", "D.P. Kroese", "S. Mannor", "R.Y. Rubinstein"], "venue": "Annals of Operations Res. 134 (1) ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2005}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Lecture Notes in Computer Sci. 8689 ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning with hierarchical convolutional factor analysis", "author": ["B. Chen", "G. Polatkan", "G. Sapiro", "D. Blei", "D. Dunson", "L. Carin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence. 35 (8) ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J.C. Burges"], "venue": "http://yann.lecun.com/exdb/mnist/, 1998 ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "in: 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, New York", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2006}, {"title": "A systematic analysis of performance measures for classification tasks", "author": ["M. Sokolova", "G. Lapalme"], "venue": "Inf. Processing and Management. 45 (4) ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2009}, {"title": "Visualizing data using t-SNE", "author": ["L.J.P. van der Maaten", "G. Hinton"], "venue": "J. Machine Learning Res", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2008}, {"title": "Stochastic neighbor embedding", "author": ["G.E. Hinton", "S.T. Roweis"], "venue": "in: Advances in Neural Information Processing Systems (NIPS), the MIT Press, Cambridge, MA", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2002}, {"title": "D", "author": ["V. Mnih", "K. Kavukcuoglu"], "venue": "Silver et al, Human-level control through deep reinforcement learning, Nature. 518 ", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional Auto-Encoder in Caffe", "author": ["V. Turchenko"], "venue": "but still without pooling-unpooling layers. https://groups.google.com/forum/#!topic/caffe-users/GhrCtONcRxY, 2015 ", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "Lethbridge Brain Dynamics", "author": ["A. Luczak"], "venue": "http://lethbridgebraindynamics.com/artur-luczak/, 2009 ", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2016}, {"title": "Comparing measures of sparsity", "author": ["N. Hurley", "S. Rickard"], "venue": "IEEE Transactions on Inf. Theory. 55 (10) ", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2009}, {"title": "Neural networks and the bias/variance dilemma", "author": ["S. Geman", "E. Bienenstock", "R. Doursat"], "venue": "Neural Computation. 4 ", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1992}, {"title": "Rectified linear units in autoencoder", "author": ["Bios Volodymyr"], "venue": "Artur Luczak received a M.Sc. in Biomedical Engineering", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "An auto-encoder (AE) model is based on an encoder-decoder paradigm, where an encoder first transforms an input into a typically lower-dimensional representation, and a decoder is tuned to reconstruct the initial input from this representation through the minimization of a cost function [1-4].", "startOffset": 287, "endOffset": 292}, {"referenceID": 1, "context": "An auto-encoder (AE) model is based on an encoder-decoder paradigm, where an encoder first transforms an input into a typically lower-dimensional representation, and a decoder is tuned to reconstruct the initial input from this representation through the minimization of a cost function [1-4].", "startOffset": 287, "endOffset": 292}, {"referenceID": 2, "context": "An auto-encoder (AE) model is based on an encoder-decoder paradigm, where an encoder first transforms an input into a typically lower-dimensional representation, and a decoder is tuned to reconstruct the initial input from this representation through the minimization of a cost function [1-4].", "startOffset": 287, "endOffset": 292}, {"referenceID": 3, "context": "An auto-encoder (AE) model is based on an encoder-decoder paradigm, where an encoder first transforms an input into a typically lower-dimensional representation, and a decoder is tuned to reconstruct the initial input from this representation through the minimization of a cost function [1-4].", "startOffset": 287, "endOffset": 292}, {"referenceID": 4, "context": "In comparison with a shallow AE, when the number of trainable parameters is the same, a deep AE can reproduce the input with lower reconstruction error [5].", "startOffset": 152, "endOffset": 155}, {"referenceID": 4, "context": "One of the variations of a deep AE [5] is a deep convolutional auto-encoder (CAE) which, instead of fully-connected layers, contains convolutional layers in the encoder part and deconvolution layers in the decoder part.", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "Deep CAEs may be better suited to image processing tasks because they fully utilize the properties of convolutional neural networks (CNNs), which have been proven to provide better results on noisy, shifted (translated) and corrupted image data [6].", "startOffset": 245, "endOffset": 248}, {"referenceID": 6, "context": "ConvNet2 [7], Theano with lightweight extensions Lasagne and Keras [810], Torch7 [11], Caffe [12], TensorFlow [13] and others, have become very popular tools in deep learning research since they provide fast deployment of state-of-the-art deep learning models along with state-of-the-art training algorithms (Stochastic Gradient Descent, AdaDelta, etc.", "startOffset": 81, "endOffset": 85}, {"referenceID": 7, "context": "ConvNet2 [7], Theano with lightweight extensions Lasagne and Keras [810], Torch7 [11], Caffe [12], TensorFlow [13] and others, have become very popular tools in deep learning research since they provide fast deployment of state-of-the-art deep learning models along with state-of-the-art training algorithms (Stochastic Gradient Descent, AdaDelta, etc.", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "Besides many outstanding features, we have chosen the Caffe deep learning framework [12] mainly for two reasons: (i) a description of a deep NN is pretty straightforward, it is just a text file describing the layers and (ii) Caffe has a Matlab wrapper, which is very convenient and allows getting Caffe results directly into a Matlab workspace for their further processing (visualization, etc.", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": ") [12].", "startOffset": 2, "endOffset": 6}, {"referenceID": 8, "context": "This study is an extended version of our paper published in arXiv [14].", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "Research on AE models was accelerated just after a breakthrough in artificial NNs connected with the success of the back propagation training algorithm in 1986 [1-4] and successfully continued a decade ago [5, 15].", "startOffset": 160, "endOffset": 165}, {"referenceID": 1, "context": "Research on AE models was accelerated just after a breakthrough in artificial NNs connected with the success of the back propagation training algorithm in 1986 [1-4] and successfully continued a decade ago [5, 15].", "startOffset": 160, "endOffset": 165}, {"referenceID": 2, "context": "Research on AE models was accelerated just after a breakthrough in artificial NNs connected with the success of the back propagation training algorithm in 1986 [1-4] and successfully continued a decade ago [5, 15].", "startOffset": 160, "endOffset": 165}, {"referenceID": 3, "context": "Research on AE models was accelerated just after a breakthrough in artificial NNs connected with the success of the back propagation training algorithm in 1986 [1-4] and successfully continued a decade ago [5, 15].", "startOffset": 160, "endOffset": 165}, {"referenceID": 4, "context": "Research on AE models was accelerated just after a breakthrough in artificial NNs connected with the success of the back propagation training algorithm in 1986 [1-4] and successfully continued a decade ago [5, 15].", "startOffset": 206, "endOffset": 213}, {"referenceID": 9, "context": "Research on AE models was accelerated just after a breakthrough in artificial NNs connected with the success of the back propagation training algorithm in 1986 [1-4] and successfully continued a decade ago [5, 15].", "startOffset": 206, "endOffset": 213}, {"referenceID": 4, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 117, "endOffset": 120}, {"referenceID": 10, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 132, "endOffset": 140}, {"referenceID": 11, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 132, "endOffset": 140}, {"referenceID": 12, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 155, "endOffset": 159}, {"referenceID": 11, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 173, "endOffset": 181}, {"referenceID": 13, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 173, "endOffset": 181}, {"referenceID": 14, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 192, "endOffset": 196}, {"referenceID": 15, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 210, "endOffset": 214}, {"referenceID": 16, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 236, "endOffset": 240}, {"referenceID": 17, "context": "2 studies presenting advanced regularization and parametrization techniques for non-convolutional AEs including deep [5], denoising [16, 17], transforming [18], contractive [17, 19], k-sparse [20], variational [21], importance-weighted [22] and adversarial [23] AEs.", "startOffset": 257, "endOffset": 261}, {"referenceID": 9, "context": "[15] is one of the first studies which uses convolutional layers for unsupervised learning of sparse hierarchical features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[24] and Norouzi et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[25] have researched unsupervised learning of hierarchical features using a stack of convolutional Restricted Boltzmann Machines (RBM) and a greedy layer-wise training approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The fully-connected operations were substituted by convolutional operations, and the probabilistic max-pooling was introduced in [24].", "startOffset": 129, "endOffset": 133}, {"referenceID": 19, "context": "Deterministic max-pooling was used in [25].", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "[26] have investigated shallow and deep (stacked) CAEs for hierarchical feature extraction, trained by a greedy layer-wise approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[27] entitled \u201cStacked What-Where Auto-Encoders\u201d (SWWAE).", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "The terms \u201cwhat\u201d and \u201cwhere\u201d correspond to pooling and appropriate unpooling operations which were proposed in [28-29].", "startOffset": 111, "endOffset": 118}, {"referenceID": 23, "context": "The terms \u201cwhat\u201d and \u201cwhere\u201d correspond to pooling and appropriate unpooling operations which were proposed in [28-29].", "startOffset": 111, "endOffset": 118}, {"referenceID": 24, "context": "Similarly to some other solutions, the authors have used a dropout layer [30] added to the fully-connected layers and an L1 sparsity penalty on hidden layers as a regularization technique.", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "However, [23] was the only paper to provide a visualization of the extracted features in a two-dimensional space.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "Encoding the result of convolution operation with max-pooling allows higher-layer representations to be invariant to small translations of the input and reduces computational cost [24].", "startOffset": 180, "endOffset": 184}, {"referenceID": 25, "context": "[32] have shown that a max-pooling operation is considerably better at capturing invariances in image data, compared to subsampling operation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[26] have shown that a CAE without max-pooling layers learns trivial solutions and interesting and biology plausible filters only emerge once a CAE is trained with a max-pooling layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[27] have proven that their SWWAE with max-pooling \u2013 unpooling layers provides much better quality of image reconstruction than maxpooling \u2013 unsampling layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[33] have proven that a maxpooling operation can simply be replaced by a convolutional operation with increased stride without decreasing accuracy on several image recognition benchmarks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In our previous paper we have presented a CAE without pooling \u2013 unpooling layers which provided acceptable quality of the dimensionality reduction and unsupervised clustering tasks [14].", "startOffset": 181, "endOffset": 185}, {"referenceID": 23, "context": "We have chosen Caffe to use in our research, which implements the stateof-the-art training approach called \u201ctop-down\u201d [29]; other terms are \u201cjointly trained multiple layers\u201d [34] or \u201cjointly trained models\u201d [27].", "startOffset": 118, "endOffset": 122}, {"referenceID": 27, "context": "We have chosen Caffe to use in our research, which implements the stateof-the-art training approach called \u201ctop-down\u201d [29]; other terms are \u201cjointly trained multiple layers\u201d [34] or \u201cjointly trained models\u201d [27].", "startOffset": 174, "endOffset": 178}, {"referenceID": 21, "context": "We have chosen Caffe to use in our research, which implements the stateof-the-art training approach called \u201ctop-down\u201d [29]; other terms are \u201cjointly trained multiple layers\u201d [34] or \u201cjointly trained models\u201d [27].", "startOffset": 207, "endOffset": 211}, {"referenceID": 9, "context": "A top-down approach implies efficient training of all hidden layers of a model with respect to the input, while a greedy layer-wise training approach [15, 26] specifies that each layer receives its input from the latent representation of the layer below and trains independently.", "startOffset": 150, "endOffset": 158}, {"referenceID": 20, "context": "A top-down approach implies efficient training of all hidden layers of a model with respect to the input, while a greedy layer-wise training approach [15, 26] specifies that each layer receives its input from the latent representation of the layer below and trains independently.", "startOffset": 150, "endOffset": 158}, {"referenceID": 23, "context": "This makes learning fragile and impractical for models beyond a few layers\u201d [29].", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "The advantage of a top-down training approach over a greedy layer-wise has been proven in the SWWAE study too [27].", "startOffset": 110, "endOffset": 114}, {"referenceID": 28, "context": "Also there are several practical solutions/attempts to develop a CAE model on different platforms: shallow CAE [35] and convolutional RBM [36] in Matlab, deep CAE in Theano/Lasagne [37], Theano/Keras [38], Torch7 [39-40] and Neon [41].", "startOffset": 181, "endOffset": 185}, {"referenceID": 29, "context": "Also there are several practical solutions/attempts to develop a CAE model on different platforms: shallow CAE [35] and convolutional RBM [36] in Matlab, deep CAE in Theano/Lasagne [37], Theano/Keras [38], Torch7 [39-40] and Neon [41].", "startOffset": 213, "endOffset": 220}, {"referenceID": 20, "context": "The convolutional/deconvolution layer followed by an activation function is described by the expression [26]", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "convolution\u2019 and the size of the output feature map ) 1 ( ) 1 ( \u2212 + \u00d7 \u2212 + n m n m is increasing [26, 28].", "startOffset": 96, "endOffset": 104}, {"referenceID": 22, "context": "convolution\u2019 and the size of the output feature map ) 1 ( ) 1 ( \u2212 + \u00d7 \u2212 + n m n m is increasing [26, 28].", "startOffset": 96, "endOffset": 104}, {"referenceID": 30, "context": "[42], which was successfully applied for semantic segmentation task [43].", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[42], which was successfully applied for semantic segmentation task [43].", "startOffset": 68, "endOffset": 72}, {"referenceID": 31, "context": "Illustration of deconvolution and unpooling layers [43]", "startOffset": 51, "endOffset": 55}, {"referenceID": 21, "context": "The main issue with a deep AE is asymmetry [27].", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "contractive AE [19].", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "As a cost function, it is better to use two loss layers, <Sigmoid_Cross_Entropy_Loss> and <Euclidean_Loss> [5, 19].", "startOffset": 107, "endOffset": 114}, {"referenceID": 13, "context": "As a cost function, it is better to use two loss layers, <Sigmoid_Cross_Entropy_Loss> and <Euclidean_Loss> [5, 19].", "startOffset": 107, "endOffset": 114}, {"referenceID": 32, "context": "function, w is the vector of weights optimized through gradient descent and x is the input vector [44], and,", "startOffset": 98, "endOffset": 102}, {"referenceID": 33, "context": "It allows us to inspect the function of intermediate layers and, therefore, better understand how data are converted/processed inside a deep model [45]; 4.", "startOffset": 147, "endOffset": 151}, {"referenceID": 9, "context": "The main purpose of an activation function after each layer is non-linear data processing [15].", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "Thus, the use of sigmoid or hyperbolic tangent activation functions, which constrain the resulting values of feature maps to the interval [0,1] or [-1,1] respectively, sets appropriate limits on the values of feature maps at the end of the decoder part, and provides good convergence of the whole model; 5.", "startOffset": 138, "endOffset": 143}, {"referenceID": 0, "context": "Thus, the use of sigmoid or hyperbolic tangent activation functions, which constrain the resulting values of feature maps to the interval [0,1] or [-1,1] respectively, sets appropriate limits on the values of feature maps at the end of the decoder part, and provides good convergence of the whole model; 5.", "startOffset": 147, "endOffset": 153}, {"referenceID": 4, "context": "Therefore experimentation is required [5, 46] to find the AE architecture, (i.", "startOffset": 38, "endOffset": 45}, {"referenceID": 5, "context": "Direct comparison of AE and CAE models is inappropriate, because a CNN of the same size as a given fully-connected network would have fewer trainable parameters [6]; 6.", "startOffset": 161, "endOffset": 164}, {"referenceID": 8, "context": "This is the model which we have investigated in our arXiv paper [14]; Model 2 (Fig.", "startOffset": 64, "endOffset": 68}, {"referenceID": 21, "context": "Model 3 is similar to the networks presented in [27, 29]; Model 4 (Fig.", "startOffset": 48, "endOffset": 56}, {"referenceID": 23, "context": "Model 3 is similar to the networks presented in [27, 29]; Model 4 (Fig.", "startOffset": 48, "endOffset": 56}, {"referenceID": 23, "context": "This approach could be called \u201ccentralized\u201d unpooling, when the max-pooled feature is placed in the center of each pool [29, 47].", "startOffset": 120, "endOffset": 128}, {"referenceID": 34, "context": "This approach could be called \u201ccentralized\u201d unpooling, when the max-pooled feature is placed in the center of each pool [29, 47].", "startOffset": 120, "endOffset": 128}, {"referenceID": 30, "context": "In the Noh implementation [42] we used, this specific position is a left and a top corner [0,0] of each pool.", "startOffset": 26, "endOffset": 30}, {"referenceID": 23, "context": "Some research results for this unpooling mode are presented in [29]; Model 5 (does not have a Figure), notation (conv, pool (tanh) <-> deconv, unpool (tanh)), the same model as the Model 4 except of using a hyperbolic tangent activation function in all layers.", "startOffset": 63, "endOffset": 67}, {"referenceID": 35, "context": "Size of the models We have used the MNIST dataset [48] for the experimental research.", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "We have created an HDF5 version of the MNIST dataset and have not applied any modification to input images except for normalization into the range [0,1].", "startOffset": 147, "endOffset": 152}, {"referenceID": 36, "context": "[50] and a deep AE [46], proposed by Hinton et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "A Siamese network consists of two coupled LeNet [6] architectures followed by a contrastive loss function.", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "According to our rule 5 in Section 3 above, we have researched a deep AE [5] in our previous paper [14].", "startOffset": 73, "endOffset": 76}, {"referenceID": 8, "context": "According to our rule 5 in Section 3 above, we have researched a deep AE [5] in our previous paper [14].", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "We have shown that the exact deep AE architecture (1000-500-250-30) presented in [5] provides the best generalization properties out of 5 different models.", "startOffset": 81, "endOffset": 84}, {"referenceID": 8, "context": "However, the research results of CAE models without pooling \u2013 unpooling layers in [14] have actually shown that a CAE model which is twice as big (in terms of feature maps) gives better results.", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "Note that our Models 1-5 are half the size (in terms of trainable parameters) of the SWWAE with 679K trainable parameters [27].", "startOffset": 122, "endOffset": 126}, {"referenceID": 23, "context": "Since the deconvolution operation has the same nature as convolution [29], we have used the same approach to calculate the number of trainable parameters both in the encoder and decoder parts (Table 3).", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "In order to estimate the quality of feature extraction in general, and the quality of the encoded 2D, 10D and 30D internal code in particular, we have used this internal code as an input to a classifier as in literature [23-29].", "startOffset": 220, "endOffset": 227}, {"referenceID": 18, "context": "In order to estimate the quality of feature extraction in general, and the quality of the encoded 2D, 10D and 30D internal code in particular, we have used this internal code as an input to a classifier as in literature [23-29].", "startOffset": 220, "endOffset": 227}, {"referenceID": 19, "context": "In order to estimate the quality of feature extraction in general, and the quality of the encoded 2D, 10D and 30D internal code in particular, we have used this internal code as an input to a classifier as in literature [23-29].", "startOffset": 220, "endOffset": 227}, {"referenceID": 20, "context": "In order to estimate the quality of feature extraction in general, and the quality of the encoded 2D, 10D and 30D internal code in particular, we have used this internal code as an input to a classifier as in literature [23-29].", "startOffset": 220, "endOffset": 227}, {"referenceID": 21, "context": "In order to estimate the quality of feature extraction in general, and the quality of the encoded 2D, 10D and 30D internal code in particular, we have used this internal code as an input to a classifier as in literature [23-29].", "startOffset": 220, "endOffset": 227}, {"referenceID": 22, "context": "In order to estimate the quality of feature extraction in general, and the quality of the encoded 2D, 10D and 30D internal code in particular, we have used this internal code as an input to a classifier as in literature [23-29].", "startOffset": 220, "endOffset": 227}, {"referenceID": 23, "context": "In order to estimate the quality of feature extraction in general, and the quality of the encoded 2D, 10D and 30D internal code in particular, we have used this internal code as an input to a classifier as in literature [23-29].", "startOffset": 220, "endOffset": 227}, {"referenceID": 37, "context": "We used 10-fold cross-validation and calculated an average per-class classification error [52].", "startOffset": 90, "endOffset": 94}, {"referenceID": 38, "context": "Clustering and visualization results We have used the t-SNE technique [55] to visualize 10D and 30D internal code, produced by all models.", "startOffset": 70, "endOffset": 74}, {"referenceID": 39, "context": "It is based on Stochastic Neighbor Embedding (SNE) [56] which converts the high-dimensional Euclidean distances between datapoints into conditional probabilities that represent similarities.", "startOffset": 51, "endOffset": 55}, {"referenceID": 38, "context": "This allows t-SNE to have a cost function that is easier to optimize and produces significantly better visualizations [55].", "startOffset": 118, "endOffset": 122}, {"referenceID": 40, "context": "This advanced technology has been used for visualization in many state-of-the-art problems, for example, to visualize the last hidden-layer representations of a deep Q-network playing Atari 2600 games [57].", "startOffset": 201, "endOffset": 205}, {"referenceID": 8, "context": "The visualization results from our previous paper [14] have shown that there is no big difference between visualized 10D and 30D internal codes.", "startOffset": 50, "endOffset": 54}, {"referenceID": 33, "context": "Similarly to Zeiler and Fergus [45], who stated that visualization allowed them to find model architectures that outperform Krizhevsky et al.", "startOffset": 31, "endOffset": 35}, {"referenceID": 41, "context": "prototxt files along with Matlab-based visualization scripts are included in supplementary materials and will be made also available in the Caffe user group [58] and on our lab web-page [59].", "startOffset": 157, "endOffset": 161}, {"referenceID": 42, "context": "prototxt files along with Matlab-based visualization scripts are included in supplementary materials and will be made also available in the Caffe user group [58] and on our lab web-page [59].", "startOffset": 186, "endOffset": 190}, {"referenceID": 30, "context": "[42] (the date of the files in this version is Jun 16, 2015).", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "in their paper [27]: the MNIST dataset is too simple a problem for such a model as Model 3.", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "Therefore, we believe that reconstructing MNIST dataset renders insufficient regularization on the encoding pathway\u201d [27].", "startOffset": 117, "endOffset": 121}, {"referenceID": 23, "context": "Model 4 restores the max-pooled features in the predefined positions [0, 0] (so-called \u201ccentralized\u201d unpooling [29, 47]) of the unpooled feature maps and, therefore, prevents too much self-adapting.", "startOffset": 111, "endOffset": 119}, {"referenceID": 34, "context": "Model 4 restores the max-pooled features in the predefined positions [0, 0] (so-called \u201ccentralized\u201d unpooling [29, 47]) of the unpooled feature maps and, therefore, prevents too much self-adapting.", "startOffset": 111, "endOffset": 119}, {"referenceID": 13, "context": "Such \u201ccentralized\u201d unpooling plays a role of a peculiar penalty which penalizes sensitivity of a model to the input and encourages a robustness of the learned representation [19].", "startOffset": 174, "endOffset": 178}, {"referenceID": 9, "context": "Model 3 has the low-dimensional internal code and the switch variables used for reconstruction [15], but all other unsupervised models have the low-dimensional internal code only.", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "Model 3 simply learns the identity function in a trivial way and produces useless, uninteresting features; its internal code is overcomplete [15].", "startOffset": 141, "endOffset": 145}, {"referenceID": 19, "context": "[25] as the following: \u201cWhat happens is that after a few iterations of parameter update, sampled images become very close to the original ones [which means perfect reconstruction], and the learning signal disappears\u201d.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "The last hidden 10D internal code of our Model 3 is so overcomplete, that even the state-of-the-art t-SNE method [55] cannot find any appropriate similarity between the datapoints; it can barely distinguish only two clusters: digit \u201c1\u201d (label \u201c2\u201d, orange) and digit \u201c6\u201d (label \u201c7\u201d, blue).", "startOffset": 113, "endOffset": 117}, {"referenceID": 9, "context": "As stated in [15, 24-25, 27] the sparsity of an overcomplete code is low.", "startOffset": 13, "endOffset": 28}, {"referenceID": 18, "context": "As stated in [15, 24-25, 27] the sparsity of an overcomplete code is low.", "startOffset": 13, "endOffset": 28}, {"referenceID": 19, "context": "As stated in [15, 24-25, 27] the sparsity of an overcomplete code is low.", "startOffset": 13, "endOffset": 28}, {"referenceID": 21, "context": "As stated in [15, 24-25, 27] the sparsity of an overcomplete code is low.", "startOffset": 13, "endOffset": 28}, {"referenceID": 21, "context": "[27] stated in the quotation above that the regularization of their model is insufficient, which means the sparsity is low.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[28], who calculated relative sparsity of feature maps, we have calculated sparsity of the hidden internal code for all our Models (see the last column of Table 4).", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "We have used the sparsity measure [60]", "startOffset": 34, "endOffset": 38}, {"referenceID": 24, "context": "The well-known regularization techniques, for example dropout [30] and weight decay [61], aim to improve the sparsity of deep models and prevent trainable parameters from growing too large.", "startOffset": 62, "endOffset": 66}, {"referenceID": 44, "context": "The well-known regularization techniques, for example dropout [30] and weight decay [61], aim to improve the sparsity of deep models and prevent trainable parameters from growing too large.", "startOffset": 84, "endOffset": 88}, {"referenceID": 21, "context": "The overcompleteness of Model 3\u2019s internal representation makes reconstruction \u201coverly easy\u201d [27], explaining its good reconstruction but poor clustering performance.", "startOffset": 93, "endOffset": 97}, {"referenceID": 4, "context": "A complementary explanation arises when we realize that the essential function of an AE is compression: finding a low-dimensional code to represent a high-dimensional input [5].", "startOffset": 173, "endOffset": 176}, {"referenceID": 15, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 109, "endOffset": 124}, {"referenceID": 18, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 109, "endOffset": 124}, {"referenceID": 19, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 109, "endOffset": 124}, {"referenceID": 9, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 155, "endOffset": 159}, {"referenceID": 20, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 191, "endOffset": 195}, {"referenceID": 9, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 232, "endOffset": 240}, {"referenceID": 14, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 274, "endOffset": 293}, {"referenceID": 17, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 274, "endOffset": 293}, {"referenceID": 21, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 274, "endOffset": 293}, {"referenceID": 29, "context": "The summary on the use of different activation functions in the existing solutions of CAE are the following: [21, 24-25, 35] have used sigmoid (logistic), [15] has used sparsifying logistic, [26] has used scaled hyperbolic tangent, [15, 39] have used hyperbolic tangent and [20, 23, 27, 40-41] have used ReLU.", "startOffset": 274, "endOffset": 293}, {"referenceID": 45, "context": "In our opinion, it may be necessary to find some correct initialization of a CAE model with a ReLU activation function or use some ideas described in the PyLearn user group [62].", "startOffset": 173, "endOffset": 177}, {"referenceID": 4, "context": "[5], convolutional auto-encoders allow using the desirable properties of convolutional neural networks for image processing tasks while working within an unsupervised learning paradigm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 41, "context": "prototxt files along with Matlab-based visualization scripts are included in supplementary materials and will be made also available in the Caffe user group [58] and on our lab web-page [59].", "startOffset": 157, "endOffset": 161}, {"referenceID": 42, "context": "prototxt files along with Matlab-based visualization scripts are included in supplementary materials and will be made also available in the Caffe user group [58] and on our lab web-page [59].", "startOffset": 186, "endOffset": 190}], "year": 2016, "abstractText": "This paper presents the development of several models of a deep convolutional auto-encoder in the Caffe deep learning framework and their experimental evaluation on the example of MNIST dataset. We have created five models of a convolutional auto-encoder which differ architecturally by the presence or absence of pooling and unpooling layers in the auto-encoder\u2019s encoder and decoder parts. Our results show that the developed models provide very good results in dimensionality reduction and unsupervised clustering tasks, and small classification errors when we used the learned internal code as an input of a supervised linear classifier and multi-layer perceptron. The best results were provided by a model where the encoder part contains convolutional and pooling layers, followed by an analogous decoder part with deconvolution and unpooling layers without the use of switch variables in the decoder part. The paper also discusses practical details of the creation of a deep convolutional auto-encoder in the very popular Caffe deep learning framework. We believe that our approach and results presented in this paper could help other researchers to build efficient deep neural network architectures in the future.", "creator": "Acrobat PDFMaker 11 \u0434\u043b\u044f Word"}}}