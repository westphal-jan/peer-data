{"id": "1701.05053", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2017", "title": "Highly Efficient Hierarchical Online Nonlinear Regression Using Second Order Methods", "abstract": "We introduce highly efficient online nonlinear regression algorithms that are suitable for real life applications. We process the data in a truly online manner such that no storage is needed, i.e., the data is discarded after being used. For nonlinear modeling we use a hierarchical piecewise linear approach based on the notion of decision trees where the space of the regressor vectors is adaptively partitioned based on the performance. As the first time in the literature, we learn both the piecewise linear partitioning of the regressor space as well as the linear models in each region using highly effective second order methods, i.e., Newton-Raphson Methods. Hence, we avoid the well known over fitting issues by using piecewise linear models, however, since both the region boundaries as well as the linear models in each region are trained using the second order methods, we achieve substantial performance compared to the state of the art. We demonstrate our gains over the well known benchmark data sets and provide performance results in an individual sequence manner guaranteed to hold without any statistical assumptions. Hence, the introduced algorithms address computational complexity issues widely encountered in real life applications while providing superior guaranteed performance in a strong deterministic sense.", "histories": [["v1", "Wed, 18 Jan 2017 13:23:21 GMT  (2163kb,D)", "http://arxiv.org/abs/1701.05053v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["burak c civek", "ibrahim delibalta", "suleyman s kozat"], "accepted": false, "id": "1701.05053"}, "pdf": {"name": "1701.05053.pdf", "metadata": {"source": "CRF", "title": "Highly Efficient Hierarchical Online Nonlinear Regression Using Second Order Methods", "authors": ["Burak C. Civek", "Ibrahim Delibalta", "Suleyman S. Kozat"], "emails": [], "sections": [{"heading": null, "text": "We implement highly efficient non-linear online regression algorithms that are suitable for real-life applications. We process the data in a real online way, so that no storage is required, i.e. the data is discarded after use. For non-linear modelling, we use a hierarchical, piecemeal linear approach based on the idea of decision trees in which the space of regressor vectors is adaptively partitioned based on performance. First, in the literature, we learn both the gradual linear partitioning of regressor space and the linear models in each region using highly effective second-order methods, i.e. Newton-Raphson methods. Therefore, we avoid the known problems of adaptation by using piecemeal linear models, as both regional boundaries and linear models in each region are trained using second-order methods."}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move into another world, in which they are able to move into another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they are able to live, in which they, in which they, are able to"}, {"heading": "2 Problem Description", "text": "In this thesis we use non-linear functions to model yt, since in most real-world applications linear regressor is insufficient to model the intrinsic relationship between the attribute xt and the desired data."}, {"heading": "3 Highly Efficient Tree Based Sequential Piecewise Linear Predic-", "text": "torsIn this section, we present three highly effective algorithms constructed by linear models; the presented algorithms provide efficient learning even for highly non-linear data models; in addition, continuous updates based on the coming data ensure that our algorithms achieve excellence for online frameworks; in addition, we also provide a repentance analysis for the algorithms introduced, which have strong guaranteed performance; there are two major problems with piecemeal linear modeling; the first significant question is how to partition the regressor space; we perform the partition process using linear separation functions; we specify the separation functions as hyperplanes, which are (m \u2212 1) dimensional subspaces of m-dimensional regression space and identified by their normal vectors, as illustrated in Fig. 2. To maintain a highly versatile and data-adaptive partition process, we train the regional boundaries by updating the corresponding vectors."}, {"heading": "3.1 Partitioning Methods", "text": "We present two different partitioning methods: type 1 is a simple partitioning and type 2 is an efficient, tree-structured partitioning."}, {"heading": "3.1.1 Type 1 Partitioning", "text": "In this method, we allow each hyperplane to divide the entire space into two subspaces, as shown in Fig. 2. To illustrate the technique, we work on the 2-dimensional space, i.e. the coordinate plane. Suppose the observed attribute vectors xt = [xt, 1, xt, 2] T come from a limited set, so that \u2212 A \u2264 xt, 1, xt, 2 \u2264 A for some A > 0, as shown in Fig. 2. We define 1-dimensional hyperplanes whose normal vector representation is given by nt, k, R2, where k denotes the corresponding regional identity. First, we have the entire space as a single set. Then, we use a single separation function, which in this case is a line, to divide this space into subspaces {0} and {1} so that {0}, {1} and {1} can form another hyperplane that separates the group from each other."}, {"heading": "3.1.2 Type 2 Partitioning", "text": "In the second method, we use the tree term to partition the regression space, which is a more systematic method for determining the regions [13,22]. We illustrate this method in Fig. 4 for 2-dimensional cases. The first step is the same as the above-mentioned approach, i.e. we partition the entire regression space into two different regions with a separation function. In the following steps, the partitioning technique is very different. Since we have two different sub-spaces after the first step, we work on them separately, i.e. the partitioning process continues recursively in each sub-space independently of the others. Therefore, adding another hyperplane has an effect on only one region, not on the entire space. The total number of different regions increases by 1 if we use another separation function. So, to represent p + 1 different regions, we specify p-separation functions. For the tree case, we use a more anode-identifier called depth, which is determined by the number of regions, i.e., the number of which is generated by the two parts."}, {"heading": "3.2 Algorithm for Type 1 Partitioning", "text": "In this part we present our first algorithms based on type 1."}, {"heading": "3.3 Algorithm for Type 2 Partitioning", "text": "The division of the regressor space will be based on the finest model of a tree structure (13, 23). We follow this in Fig. 4. \u2212 Here we have three separation functions, pt, \u03b5, pt, 0 and pt, 1, division of the entire space into four subspaces. (The corresponding direction vectors are given by nt, \u03b5, 0 and nt, 1 respectively. Using the individual estimates of all four regions, we find the final estimate of byy, t = pt, \u03b5pt, 0y-t, 00 + pt, \u03b5 (1 \u2212 pt, 0) y vectors of p, 01 + (1 \u2212 pt, \u03b5) pt, 1y vectors of all four regions, with the final estimate of byy, t = pt, \u03b5pt, 0y vector, 0y vectors of the formula (1 \u2212 pt, 0) y vectors of d."}, {"heading": "3.4 Algorithm for Combining All Possible Models of Tree", "text": "In this algorithm, we combine the estimates generated by all possible models of a tree-based division \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "3.5 Computational Complexities", "text": "In this section, we determine the computational complexity of the proposed algorithms. In the type 1 partitioning algorithm, the regressor space is divided into a maximum of k 2 + k + 22 regions using the k-separated separation function. Therefore, this algorithm requires an update of the O (k2) weight with each iteration. In the type 2 partitioning algorithm, the regressor space for the depth d tree model is divided into 2d regions. Therefore, we perform an update of the O (k2) weight for each iteration. The latter algorithm combines all possible models of the deep tree and efficiently calculates the final estimate that requires O (4d) weight updates [30]. Let us assume that the regressor space is m-dimensional, i.e., xt (Rm) Rm. For each update, all three algorithms require the O (m2) -multiplication and we apply a second order of the matrix. \""}, {"heading": "3.6 Logarithmic Regret Bound", "text": "In this subsection we provide the results for the introduced algorithms. All three algorithms use the second order rule (Online Newton Step [34]), and achieve a logarithmic regret if the normal vectors of the region are fixed and the cost function is convex in the sense of the individual region weights. To construct the upper limits, we first let w \u00b2 n be the best predictive value, i.e., w \u00b2 n = arg min w n) t = 1 e2t (21) and the following inequalitye2t (wt) \u2212 e2t (w \u00b2 n) n be the best predictive value in hindsight, i.e. n (wt \u2212 w \u00b2 n) n = 1 e2t (w) t (21) and express the following inequalitye2t (wt) \u2212 e2t (w \u00b2 n) n of the following inequalitye2t (wt) \u2212 t (wt \u00b2 n)."}, {"heading": "4 Simulations", "text": "In this section, we evaluate the performance of the proposed algorithms under various scenarios. In the first set of simulations, we aim to gain a better understanding of our algorithms. To this end, we first consider the regression of a signal generated by a piecemeal linear model whose partitions correspond to the original partitioning of our algorithms. Then, we examine the case of mismatched initial partitions to illustrate the learning process of the presented algorithms. In the second set of simulations, we mainly evaluate the advantages of our algorithms by using the well-known real and synthetic benchmark data sets, which are widely used in signal processing and machine learning literature, e.g. California Housing [38], Kinematics [38] and Elevators [38]. We then conduct two further experiments with two chaotic processes, e.g. the Gaussian map and the Lorenz Attractor, to demonstrate the merits of our algorithms."}, {"heading": "4.1 Matched Partition", "text": "In this subsection, we look at the regression of a signal generated using a piecemeal linear model whose partitions correspond to the original partitioning of the proposed algorithms, and the main objective of this experiment is to provide an insight into the working principles of the proposed algorithms. Therefore, this experiment is not intended to evaluate the performance of our algorithms with respect to those that are not based on piecemeal linear modeling, which is just an illustration of how it is possible to achieve a performance gain when the data sequence is generated by a nonlinear system. We use the following piecemeal linear model to generate the data sequence, y t = wT1 xt + joke, x T n0 \u2265 0 and xTt n1 \u2265 0 wT2 xt xT2 xt, x T n0 < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "4.2 Mismatched Partition", "text": "In this case, the question is how the proposed algorithms capture the underlying data structure."}, {"heading": "4.3 Real and Synthetic Data Sets", "text": "In this area, we rely primarily on the evaluation of the performance of our algorithms. First, we look at the regression of a benchmark problem that can be realized in many datasets, such as: California Housing, which is an m = 8-dimensional database consisting of estimates of median house prices in the U.S. There are more than 20,000 datasets for this dataset. For this experiment, we set the learning rates to 0.004 forFMP and the DAT, 0.02 for the CTW, 0.05 for the FNF, 0.05 for the FNF and the FNF."}, {"heading": "4.4 Chaotic Signals", "text": "Finally, we examine the error performance of our algorithms when the desired data sequence is generated using chaotic processes, such as the Gaussian map and the Lorenz attractor. First, we consider the case in which the data is generated using the Gaussian map, i.e. yt = exp (\u2212 \u03b1x2t) + \u03b2 (34), which exhibits chaotic behavior for \u03b1 = 4 and \u03b2 = 0.5. The desired data sequence is represented by yt \u2212 and xt \u2212 R, corresponding to yt \u2212 1. x0 is a sample of a Gaussian process with zero and unit variance. Learning rates are set to 0.004 for the FMP, 0.05 for the SP, 0.05 for the S \u2212 DAT \u2212 and the DAT \u2212 0.025 for the VF \u2212, the FNF, the EMFNF and the CTW. As a second experiment, we consider a scenario in which we use a chaotic signal generated from the Lorenz \u2212 Lorenz attractor, which represents a series of chaotic solutions to the Lorenz system \u2212."}, {"heading": "5 Concluding Remarks", "text": "In this paper, we present three different highly efficient and effective non-linear regression algorithms for online learning problems that are suitable for real-life applications: We process only the currently available data for regression and then discard them, i.e. there is no memory requirement. For non-linear modelling, we use piecemeal linear models in which we divide the regressor space using linear separators and adapt the linear regressors to each partition. We construct our algorithms on the basis of two different approaches for dividing the space of the regressors. First, in the literature, we adaptively update both the regional boundaries and the linear regressors in each region using second-order methods, i.e. the Newton-Raphson method. We illustrate that the proposed algorithms achieve outstanding performance compared to the state of the art even for highly non-linear data models."}, {"heading": "Acknowledgment", "text": "This work is partially supported by the Outstanding Researchers Programme of the Turkish Academy of Sciences, TUBITAK Contract No. 113E517 and Turk Telekom Communications Services Incorporated."}], "references": [{"title": "Slope estimation in noisy piecewise linear functions", "author": ["A. Ingle", "J. Bucklew", "W. Sethares", "T. Varghese"], "venue": "Signal Processing 108 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonlinear spline adaptive filtering", "author": ["M. Scarpiniti", "D. Comminiello", "R. Parisi", "A. Uncini"], "venue": "Signal Processing 93 (4) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "A tree-weighting approach to sequential decision problems with multiplicative loss", "author": ["S.S. Kozat", "A.C. Singer", "A.J. Bean"], "venue": "Signal Processing 91 (4) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequential distributed detection in energy-constrained wireless sensor networks", "author": ["Y. Yilmaz", "X. Wang"], "venue": "IEEE Transactions on Signal Processing 17 (4) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "A", "author": ["N. Asadi", "J. Lin"], "venue": "de Vries, Runtime optimizations for tree-based machine learning models, IEEE Transactions on Knowledge and Data Engineering 26 (9) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Signal processing in large systems", "author": ["R. Couillet", "M. Debbah"], "venue": "IEEE Signal Processing Magazine 24 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Online learning for very large data sets", "author": ["L. Bottou", "Y.L. Cun"], "venue": "Applied Stochastic Models in Business and Industry 21 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "in: Advances in Neural Information Processing (NISP)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Fundamentals of Adaptive Filtering", "author": ["A.H. Sayed"], "venue": "John Wiley & Sons, NJ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Data mining with big data", "author": ["X. Wu", "X. Zhu", "G.-Q. Wu", "W. Ding"], "venue": "IEEE Transactions on Knowledge and Data Engineering 26 (1) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Universal FIR MMSE filtering", "author": ["T. Moon", "T. Weissman"], "venue": "IEEE Transactions on Signal Processing 57 (3) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonlinear autoregressive modeling and estimation in the presence of noise", "author": ["A.C. Singer", "G.W. Wornell", "A.V. Oppenheim"], "venue": "Digital Signal Processing 4 (4) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "Tree-structured nonlinear signal modeling and prediction", "author": ["O.J.J. Michel", "A.O. Hero", "A.-E. Badel"], "venue": "IEEE Transactions on Signal Processing 47 (11) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Coupled market behavior based financial crisis detection", "author": ["W. Cao", "L. Cao", "Y. Song"], "venue": "in: The 2013 International Joint Conference on Neural Networks (IJCNN)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Long-term trend in non-stationary time series with nonlinear analysis techniques", "author": ["L. Deng"], "venue": "in: 2013 6th International Congress on Image and Signal Processing (CISP), Vol. 2", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Supervised non-linear dimensionality reduction techniques for classification in intrusion detection", "author": ["K. mei Zheng", "X. Qian", "N. An"], "venue": "in: 2010 International Conference on Artificial Intelligence and Computational Intelligence (AICI),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Nlmf: Nonlinear matrix factorization methods for top-n recommender systems", "author": ["S. Kabbur", "G. Karypis"], "venue": "in: 2014 IEEE International Conference on Data Mining Workshop (ICDMW)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Prediction", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Learning, and Games, Cambridge University Press, Cambridge", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Universal linear least squares prediction: upper and lower bounds", "author": ["A.C. Singer", "S.S. Kozat", "M. Feder"], "venue": "IEEE Transactions on Information Theory 48 (8) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Steady state MSE performance analysis of mixture approaches to adaptive filtering", "author": ["S.S. Kozat", "A.T. Erdogan", "A.C. Singer", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing 58 (8) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Competitive randomized nonlinear prediction under additive noise", "author": ["Y. Yilmaz", "S. Kozat"], "venue": "Signal Processing Letters, IEEE 17 (4) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Random projection trees for vector quantization", "author": ["S. Dasgupta", "Y. Freund"], "venue": "IEEE Transactions on Information Theory 55 (7) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Predicting nearly as well as the best pruning of a decision tree", "author": ["D.P. Helmbold", "R.E. Schapire"], "venue": "Machine Learning 27 (1) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "Universal piecewise linear prediction via context trees", "author": ["S.S. Kozat", "A.C. Singer", "G.C. Zeitler"], "venue": "IEEE Transactions on Signal Processing 55 (7) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Introduction to linear optimization", "author": ["D. Bertsimas", "J.N. Tsitsiklis"], "venue": "Athena scientific series in optimization and neural computation, Athena Scientific, Belmont (Mass.)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Multiscale generalised linear models for nonparametric function estimation", "author": ["E.D. Kolaczyk", "R.D. Nowak"], "venue": "Biometrika 92 (1) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "The context-tree weighting method: basic properties", "author": ["F.M.J. Willems", "Y.M. Shtarkov", "T.J. Tjalkens"], "venue": "IEEE Transactions on Information Theory 41 (3) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1995}, {"title": "Universal linear prediction by model order weighting", "author": ["A.C. Singer", "M. Feder"], "venue": "IEEE Transactions on Signal Processing 47 (10) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1999}, {"title": "Efficient adaptive algorithms and minimax bounds for zero-delay lossy source coding", "author": ["A. Gyorgy", "T. Linder", "G. Lugosi"], "venue": "IEEE Transactions on Signal Processing 52 (8) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "A comprehensive approach to universal piecewise nonlinear regression based on trees", "author": ["N. Vanli", "S. Kozat"], "venue": "IEEE Transactions on Signal Processing 62 (20) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Advances in Nonlinear Modeling for Speech Processing, Adaptive computation and machine learning", "author": ["M.S.D. Raghunath S. Holambe"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Machine learning : A probabilistic perspective", "author": ["K.P. Murphy"], "venue": "Adaptive computation and machine learning series, MIT Press, Cambridge (Mass.)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "A new approach to piecewise linear modeling of time series", "author": ["M. Mattavelli", "J. Vesin", "E. Amaldi", "R. Gruter"], "venue": "in: IEEE Digital Signal Processing Workshop Proceedings", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1996}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning 69 (2-3) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernel partial least squares regression in reproducing kernel Hilbert space", "author": ["R. Rosipal", "L.J. Trejo"], "venue": "J. Mach. Learn. Res. 2 ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2002}, {"title": "The Volterra and Wiener Theories of Nonlinear Systems", "author": ["M. Schetzen"], "venue": "John Wiley & Sons, NJ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1980}, {"title": "Fourier nonlinear filters", "author": ["A. Carini", "G.L. Sicuranza"], "venue": "Signal Processing 94 (0) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 1, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 2, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 3, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 4, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 5, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 6, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 7, "context": "Recent developments in information technologies, intelligent use of mobile devices and Internet have bolstered the capacity and capabilities of data acquisition systems beyond expectation [1\u20138].", "startOffset": 188, "endOffset": 193}, {"referenceID": 8, "context": "Efficient and effective processing of this data can significantly improve the performance of many signal processing and machine learning algorithms [9\u201311].", "startOffset": 148, "endOffset": 154}, {"referenceID": 9, "context": "Efficient and effective processing of this data can significantly improve the performance of many signal processing and machine learning algorithms [9\u201311].", "startOffset": 148, "endOffset": 154}, {"referenceID": 10, "context": "Efficient and effective processing of this data can significantly improve the performance of many signal processing and machine learning algorithms [9\u201311].", "startOffset": 148, "endOffset": 154}, {"referenceID": 11, "context": "This problem arises in several different applications such as signal modeling [12,13], financial market [14] and trend analyses [15], intrusion detection [16] and recommendation [17].", "startOffset": 78, "endOffset": 85}, {"referenceID": 12, "context": "This problem arises in several different applications such as signal modeling [12,13], financial market [14] and trend analyses [15], intrusion detection [16] and recommendation [17].", "startOffset": 78, "endOffset": 85}, {"referenceID": 13, "context": "This problem arises in several different applications such as signal modeling [12,13], financial market [14] and trend analyses [15], intrusion detection [16] and recommendation [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "This problem arises in several different applications such as signal modeling [12,13], financial market [14] and trend analyses [15], intrusion detection [16] and recommendation [17].", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "This problem arises in several different applications such as signal modeling [12,13], financial market [14] and trend analyses [15], intrusion detection [16] and recommendation [17].", "startOffset": 154, "endOffset": 158}, {"referenceID": 16, "context": "This problem arises in several different applications such as signal modeling [12,13], financial market [14] and trend analyses [15], intrusion detection [16] and recommendation [17].", "startOffset": 178, "endOffset": 182}, {"referenceID": 6, "context": "However, traditional regression techniques show less than adequate performance in real-life applications having big data since (1) data acquired from diverse sources are too large in size to be efficiently processed or stored by conventional signal processing and machine learning methods [7,8,18]; (2) the performance of the conventional methods is further impaired by the highly variable properties, structure and quality of data acquired at high speeds [7,8].", "startOffset": 289, "endOffset": 297}, {"referenceID": 7, "context": "However, traditional regression techniques show less than adequate performance in real-life applications having big data since (1) data acquired from diverse sources are too large in size to be efficiently processed or stored by conventional signal processing and machine learning methods [7,8,18]; (2) the performance of the conventional methods is further impaired by the highly variable properties, structure and quality of data acquired at high speeds [7,8].", "startOffset": 289, "endOffset": 297}, {"referenceID": 17, "context": "However, traditional regression techniques show less than adequate performance in real-life applications having big data since (1) data acquired from diverse sources are too large in size to be efficiently processed or stored by conventional signal processing and machine learning methods [7,8,18]; (2) the performance of the conventional methods is further impaired by the highly variable properties, structure and quality of data acquired at high speeds [7,8].", "startOffset": 289, "endOffset": 297}, {"referenceID": 6, "context": "However, traditional regression techniques show less than adequate performance in real-life applications having big data since (1) data acquired from diverse sources are too large in size to be efficiently processed or stored by conventional signal processing and machine learning methods [7,8,18]; (2) the performance of the conventional methods is further impaired by the highly variable properties, structure and quality of data acquired at high speeds [7,8].", "startOffset": 456, "endOffset": 461}, {"referenceID": 7, "context": "However, traditional regression techniques show less than adequate performance in real-life applications having big data since (1) data acquired from diverse sources are too large in size to be efficiently processed or stored by conventional signal processing and machine learning methods [7,8,18]; (2) the performance of the conventional methods is further impaired by the highly variable properties, structure and quality of data acquired at high speeds [7,8].", "startOffset": 456, "endOffset": 461}, {"referenceID": 17, "context": ", instantly, without any storage, and then discard the data after using and learning [18, 19].", "startOffset": 85, "endOffset": 93}, {"referenceID": 18, "context": ", instantly, without any storage, and then discard the data after using and learning [18, 19].", "startOffset": 85, "endOffset": 93}, {"referenceID": 18, "context": "Hence our methods can constantly adapt to the changing statistics or quality of the data so that they can be robust and prone to variations and uncertainties [19\u201321].", "startOffset": 158, "endOffset": 165}, {"referenceID": 19, "context": "Hence our methods can constantly adapt to the changing statistics or quality of the data so that they can be robust and prone to variations and uncertainties [19\u201321].", "startOffset": 158, "endOffset": 165}, {"referenceID": 20, "context": "Hence our methods can constantly adapt to the changing statistics or quality of the data so that they can be robust and prone to variations and uncertainties [19\u201321].", "startOffset": 158, "endOffset": 165}, {"referenceID": 12, "context": ", is adaptively partitioned and continuously optimized in order to enhance the performance [13,22,23].", "startOffset": 91, "endOffset": 101}, {"referenceID": 21, "context": ", is adaptively partitioned and continuously optimized in order to enhance the performance [13,22,23].", "startOffset": 91, "endOffset": 101}, {"referenceID": 22, "context": ", is adaptively partitioned and continuously optimized in order to enhance the performance [13,22,23].", "startOffset": 91, "endOffset": 101}, {"referenceID": 12, "context": "We note that the piecewise linear models are extensively used in the signal processing literature to mitigate the overtraining issues that arise because of using nonlinear models [13].", "startOffset": 179, "endOffset": 183}, {"referenceID": 23, "context": "However their performance in real life applications are less than adequate since their successful application highly depends on the accurate selection of the piecewise regions that correctly model the underlying data [24].", "startOffset": 217, "endOffset": 221}, {"referenceID": 24, "context": ", Newton-Raphson Methods [25].", "startOffset": 25, "endOffset": 29}, {"referenceID": 24, "context": "Hence, we avoid the well known over fitting issues by using piecewise linear models, moreover, since both the region boundaries as well as the linear models in each region are trained using the second order methods we achieve substantial performance compared to the state of the art [25].", "startOffset": 283, "endOffset": 287}, {"referenceID": 17, "context": "We also provide theoretical performance results in an individual sequence manner that are guaranteed to hold without any statistical assumptions [18].", "startOffset": 145, "endOffset": 149}, {"referenceID": 22, "context": "In adaptive signal processing literature, there exist methods which develop an approach based on weighted averaging of all possible models of a tree based partitioning instead of solely relying on a particular piecewise linear model [23, 24].", "startOffset": 233, "endOffset": 241}, {"referenceID": 23, "context": "In adaptive signal processing literature, there exist methods which develop an approach based on weighted averaging of all possible models of a tree based partitioning instead of solely relying on a particular piecewise linear model [23, 24].", "startOffset": 233, "endOffset": 241}, {"referenceID": 22, "context": "Such approaches are confirmed to lessen the bias variance trade off in a deterministic framework [23, 24].", "startOffset": 97, "endOffset": 105}, {"referenceID": 23, "context": "Such approaches are confirmed to lessen the bias variance trade off in a deterministic framework [23, 24].", "startOffset": 97, "endOffset": 105}, {"referenceID": 25, "context": "One such example is that the recursive dyadic partitioning, which partitions the regressor space using separation functions that are required to be parallel to the axes [26].", "startOffset": 169, "endOffset": 173}, {"referenceID": 26, "context": "Moreover, these methods usually do not provide a theoretical justification for the weighting of the models, even if there exist inspirations from information theoretic deliberations [27].", "startOffset": 182, "endOffset": 186}, {"referenceID": 18, "context": "For instance, there is an algorithmic concern on the definitions of both the exponentially weighted performance measure and the \u201duniversal weighting\u201d coefficients [19,24,28,29] instead of a complete theoretical justifications (except the universal bounds).", "startOffset": 163, "endOffset": 176}, {"referenceID": 23, "context": "For instance, there is an algorithmic concern on the definitions of both the exponentially weighted performance measure and the \u201duniversal weighting\u201d coefficients [19,24,28,29] instead of a complete theoretical justifications (except the universal bounds).", "startOffset": 163, "endOffset": 176}, {"referenceID": 27, "context": "For instance, there is an algorithmic concern on the definitions of both the exponentially weighted performance measure and the \u201duniversal weighting\u201d coefficients [19,24,28,29] instead of a complete theoretical justifications (except the universal bounds).", "startOffset": 163, "endOffset": 176}, {"referenceID": 28, "context": "For instance, there is an algorithmic concern on the definitions of both the exponentially weighted performance measure and the \u201duniversal weighting\u201d coefficients [19,24,28,29] instead of a complete theoretical justifications (except the universal bounds).", "startOffset": 163, "endOffset": 176}, {"referenceID": 23, "context": ", one should adjust these parameters to the specific application for successful process [24].", "startOffset": 88, "endOffset": 92}, {"referenceID": 29, "context": ", the Decision Adaptive Tree (DAT) [30].", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "In this work, we use nonlinear functions to model yt, since in most real life applications, linear regressors are inadequate to successively model the intrinsic relation between the feature vector xt and the desired data yt [31].", "startOffset": 224, "endOffset": 228}, {"referenceID": 31, "context": "Different from linear regressors, nonlinear functions are quite powerful and usually overfit in most real life cases [32].", "startOffset": 117, "endOffset": 121}, {"referenceID": 32, "context": "To this end, we choose piecewise linear functions due to their capability of approximating most nonlinear models [33].", "startOffset": 113, "endOffset": 117}, {"referenceID": 21, "context": "1 [22].", "startOffset": 2, "endOffset": 6}, {"referenceID": 23, "context": "The second problem is to find out the linear model that best fits the data in each distinct region in a sequential manner [24].", "startOffset": 122, "endOffset": 126}, {"referenceID": 17, "context": "However, we do not process batch data sets, since the framework is online, and thus, cannot know the optimal weight beforehand [18].", "startOffset": 127, "endOffset": 131}, {"referenceID": 17, "context": "Eventually, we consider the regret criterion to measure the modeling performance of the designated piecewise linear model and aim to attain a low regret [18].", "startOffset": 153, "endOffset": 157}, {"referenceID": 33, "context": "Online Newton Step [34], for training of the region boundaries and the linear models.", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "In the second method, we use the tree notion to partition the regression space, which is a more systematic way to determine the regions [13,22].", "startOffset": 136, "endOffset": 143}, {"referenceID": 21, "context": "In the second method, we use the tree notion to partition the regression space, which is a more systematic way to determine the regions [13,22].", "startOffset": 136, "endOffset": 143}, {"referenceID": 33, "context": "Online Newton Step [34], to update both separator functions and region weights.", "startOffset": 19, "endOffset": 23}, {"referenceID": 33, "context": "Here, the matrix At is related to the Hessian of the error function, implying that the update rule uses the second order information [34].", "startOffset": 133, "endOffset": 137}, {"referenceID": 8, "context": "In order to avoid taking the inverse of an m\u00d7m matrix, At, at each iteration in (11) and (12), we generate a recursive formula using matrix inversion lemma for A\u22121 t given as [9] A\u22121 t = A \u22121 t\u22121 \u2212 A\u22121 t\u22121\u2207t\u2207t A\u22121 t\u22121 1 +\u2207t A\u22121 t\u22121\u2207t , (13)", "startOffset": 175, "endOffset": 178}, {"referenceID": 12, "context": "The partition of the regressor space will be based on the finest model of a tree structure [13, 23].", "startOffset": 91, "endOffset": 99}, {"referenceID": 22, "context": "The partition of the regressor space will be based on the finest model of a tree structure [13, 23].", "startOffset": 91, "endOffset": 99}, {"referenceID": 29, "context": "The last algorithm combines all possible models of depthd tree and calculates the final estimate in an efficient way requiring O(4) weight updates [30].", "startOffset": 147, "endOffset": 151}, {"referenceID": 29, "context": "\u201dDFT\u201d stands for Decision Fixed Tree and \u201dDAT\u201d represents Decision Adaptive Tree [30].", "startOffset": 81, "endOffset": 85}, {"referenceID": 23, "context": "\u201dCTW\u201d is used for Context Tree Weighting [24], \u201dGKR\u201d represents Gaussian-Kernel re-", "startOffset": 41, "endOffset": 45}, {"referenceID": 34, "context": "gressor [35], \u201dVF\u201d represents Volterra Filter [36], \u201dFNF\u201d and \u201dEMFNF\u201d stand for the Fourier and Even Mirror Fourier Nonlinear Filter [37] respectively.", "startOffset": 8, "endOffset": 12}, {"referenceID": 35, "context": "gressor [35], \u201dVF\u201d represents Volterra Filter [36], \u201dFNF\u201d and \u201dEMFNF\u201d stand for the Fourier and Even Mirror Fourier Nonlinear Filter [37] respectively.", "startOffset": 46, "endOffset": 50}, {"referenceID": 36, "context": "gressor [35], \u201dVF\u201d represents Volterra Filter [36], \u201dFNF\u201d and \u201dEMFNF\u201d stand for the Fourier and Even Mirror Fourier Nonlinear Filter [37] respectively.", "startOffset": 133, "endOffset": 137}, {"referenceID": 33, "context": "All three algorithms uses the second order update rule, Online Newton Step [34], and achieves a logarithmic regret when the normal vectors of the region boundaries are fixed and the cost function is convex in the sense of individual region weights.", "startOffset": 75, "endOffset": 79}, {"referenceID": 33, "context": "using the Lemma 3 of [34], since our cost function is \u03b1-exp-concave, i.", "startOffset": 21, "endOffset": 25}, {"referenceID": 33, "context": "By following a similar discussion [34], except that we have equality in (26) and in the proceeding parts, we achieve the inequality", "startOffset": 34, "endOffset": 38}, {"referenceID": 33, "context": "We make use of Lemma 11 given in [34], to get the following bound 1 2\u03b2 n \u2211", "startOffset": 33, "endOffset": 37}, {"referenceID": 0, "context": "where w1 = [1, 1] T , w2 = [\u22121,\u22121] , n0 = [1, 0] and n1 = [0, 1] .", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "where w1 = [1, 1] T , w2 = [\u22121,\u22121] , n0 = [1, 0] and n1 = [0, 1] .", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "where w1 = [1, 1] T , w2 = [\u22121,\u22121] , n0 = [1, 0] and n1 = [0, 1] .", "startOffset": 42, "endOffset": 48}, {"referenceID": 0, "context": "where w1 = [1, 1] T , w2 = [\u22121,\u22121] , n0 = [1, 0] and n1 = [0, 1] .", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "where w1 = [1, 1] T , w2 = [1,\u22121] , n0 = [2,\u22121] , n1 = [\u22121, 1] and n2 = [2, 1] .", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "where w1 = [1, 1] T , w2 = [1,\u22121] , n0 = [2,\u22121] , n1 = [\u22121, 1] and n2 = [2, 1] .", "startOffset": 11, "endOffset": 17}, {"referenceID": 1, "context": "where w1 = [1, 1] T , w2 = [1,\u22121] , n0 = [2,\u22121] , n1 = [\u22121, 1] and n2 = [2, 1] .", "startOffset": 72, "endOffset": 78}, {"referenceID": 0, "context": "where w1 = [1, 1] T , w2 = [1,\u22121] , n0 = [2,\u22121] , n1 = [\u22121, 1] and n2 = [2, 1] .", "startOffset": 72, "endOffset": 78}], "year": 2017, "abstractText": "We introduce highly efficient online nonlinear regression algorithms that are suitable for real life applications. We process the data in a truly online manner such that no storage is needed, i.e., the data is discarded after being used. For nonlinear modeling we use a hierarchical piecewise linear approach based on the notion of decision trees where the space of the regressor vectors is adaptively partitioned based on the performance. As the first time in the literature, we learn both the piecewise linear partitioning of the regressor space as well as the linear models in each region using highly effective second order methods, i.e., Newton-Raphson Methods. Hence, we avoid the well known over fitting issues by using piecewise linear models, however, since both the region boundaries as well as the linear models in each region are trained using the second order methods, we achieve substantial performance compared to the state of the art. We demonstrate our gains over the well known benchmark data sets and provide performance results in an individual sequence manner guaranteed to hold without any statistical assumptions. Hence, the introduced algorithms address computational complexity issues widely encountered in real life applications while providing superior guaranteed performance in a strong deterministic sense.", "creator": "LaTeX with hyperref package"}}}