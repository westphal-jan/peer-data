{"id": "1701.05221", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2017", "title": "Parsimonious Inference on Convolutional Neural Networks: Learning and applying on-line kernel activation rules", "abstract": "A new, radical CNN design approach is presented in this paper, considering the reduction of the total computational load during inference. This is achieved by a new holistic intervention on both the CNN architecture and the training procedure, which targets to the parsimonious inference by learning to exploit or remove the redundant capacity of a CNN architecture. This is accomplished, by the introduction of a new structural element that can be inserted as an add-on to any contemporary CNN architecture, whilst preserving or even improving its recognition accuracy. Our approach formulates a systematic and data-driven method for developing CNNs that are trained to eventually change size and form in real-time during inference, targeting to the smaller possible computational footprint. Results are provided for the optimal implementation on a few modern, high-end mobile computing platforms indicating a significant speed-up of up to x3 times.", "histories": [["v1", "Wed, 18 Jan 2017 20:03:12 GMT  (625kb)", "http://arxiv.org/abs/1701.05221v1", "17 pages, 10 figures, 5 tables"], ["v2", "Tue, 24 Jan 2017 06:43:02 GMT  (631kb)", "http://arxiv.org/abs/1701.05221v2", "17 pages, 10 figures, 5 tables"], ["v3", "Wed, 25 Jan 2017 08:57:29 GMT  (631kb)", "http://arxiv.org/abs/1701.05221v3", "17 pages, 10 figures, 5 tables"], ["v4", "Thu, 26 Jan 2017 08:58:52 GMT  (632kb)", "http://arxiv.org/abs/1701.05221v4", "17 pages, 10 figures, 5 tables"], ["v5", "Tue, 31 Jan 2017 12:15:43 GMT  (602kb)", "http://arxiv.org/abs/1701.05221v5", "17 pages, 10 figures, 5 tables"]], "COMMENTS": "17 pages, 10 figures, 5 tables", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG cs.NE", "authors": ["i theodorakopoulos", "v pothos", "d kastaniotis", "n fragoulis"], "accepted": false, "id": "1701.05221"}, "pdf": {"name": "1701.05221.pdf", "metadata": {"source": "CRF", "title": "Parsimonious Inference on Convolutional Neural Networks: Learning and applying on-line kernel activation rules", "authors": ["I. Theodorakopoulos", "V. Pothos", "D. Kastaniotis"], "emails": [], "sections": [{"heading": null, "text": "Frugal Conclusion on Convolutionary Neural Networks: Learning and Applying Online Nuclear Activation Rules"}, {"heading": "I. Theodorakopoulos, V. Pothos, D. Kastaniotis and N. Fragoulis, January 2017", "text": "This is achieved through a new holistic intervention on both the CNN architecture and the training process aimed at a contemporary CNN architecture, learning to exploit or remove the redundant capacity of a CNN architecture. However, this is achieved through the introduction of a new structural element that can be used as an add-on to any contemporary CNN architecture while maintaining or even improving its detection accuracy. Our approach formulates a systematic and data-driven method for developing CNNs that are ultimately trained in shape and size, while inference is worked towards targeting the smaller possible computational tasks. Results are provided for optimal implementation on a few modern, high-quality mobile computing platforms that display a significant speed of up to x3 times.Introduction Deep Learning Deep Learning [BLH] has emerged as the dominant approach to performing various classification tasks, ranging from computer vision to language processing."}, {"heading": "3.1. The learning kernel-activation module.", "text": "The idea is presented in Figure 3. In this figure, a part of a typical Convolutionary Network is shown, which represents only the i-th and (i + 1) th Convolutionary Layer. In this figure, a Learning Kernel-Activation Module (LKAM) is introduced, which links two successive Convolutionary Layers. This learning switch module is capable of turning individual cores of each layer on and off depending on their input, which is the output of the previous Convolutionary Layer. The module learns which kernel to disable during the CNN training process, which is why it was specifically designed to facilitate such an operation, taking advantage of the data sparseness normally used in the images. First, the goal of these modules is to induce the desired channel-by-channel economy into the functional sketches."}, {"heading": "In this way a vector \ud835\udc46\ud835\udc4a = \ud835\udc60\ud835\udc640, \ud835\udc60\ud835\udc64<, \u2026 , \ud835\udc60\ud835\udc64>?@AB \u2208 \u211d", "text": "F? @ 10 with values between 0 and 1. During the training phase, the elements of this vector are used by the switch S3 to multiply the values of the corresponding characteristic card in the (i + 1) -th coil layer, thus imposing the desired scarcity. In this phase, the switches S2 in Figure 3 and S3 in Figure 4 are activated, while the switches S1 in Figure 3 and S4 in Figure 4 are deactivated. In this way, the flow of information is optimized by enforcing certain characteristic cards in order to gradually have a smaller influence on the overall network under the corresponding rules, which in turn adapt to each other. The aim of the training process is to obtain the combination of cores and activation rules that produce the most economical vectors."}, {"heading": "3.2 Training procedure", "text": "The training of the LKAM modules takes place at the same time as the training of the rest of the network by using backpropagation and aims at calculating the coefficients for the kfi, 1x1 convolution masks in the LKA modules. In order to enforce the desired channel-by-channel sparseness, the primary loss function used during backpropagation must be extended by a new term punishing the use of Convolutionary Cores. The easiest way to achieve this is to add a term proportional to the L1 standard of SW vectors called HIJ and given by the following equation: HIJ = K @ < L MM (5), where M is an amplification factor and the length of the vector. The total loss will now be:,,, = Q, HIJ (6) Where is the main loss dictated by the task of the primary layer, which we can activate e.g. by the loss of the layer for the deactivation of the layer."}, {"heading": "3.3. Pruning kernels", "text": "At the end of the training phase, a simple statistical analysis of the core activations in the entire dataset can show very low-contribution cores characterized by zero or zero utilization. Such cores can safely be considered redundant and removed from the model together with the corresponding filters in the LKA modules."}, {"heading": "3.4 Real-time deactivation of kernels during Inference (recognition) phase", "text": "In the conclusion, the elements of the SW vector are used as a series of switches that control the corresponding cores in the (i + 1) -th shaft layer, depending on the input from the i-th layer (Figure 5). Since the value of each M can be any real number between 0 and 1, a simple threshold is used as an activation criterion where the elements of the SW vector are binarized (i.e. forced to take values 1 or 0), with a threshold value as follows: M = 0, M < M < 1, M \u2265 (4) The resulting binary activation values are the indicators of whether the corresponding filter cores are applied to the input data or skipped the respective calculations. Note that in the case of Inference, the switches S2 in Figure 3 and S3 in Figure 4 are considered active, while the switches S1 Figure 3 and S4 are considered active."}, {"heading": "3.5 Pruning/Deactivating Layers", "text": "In fact, it is in such a way that most of them will be able to move into another world, in which they are able to change the world, than in another world, in which they are able to live and live. In the second world, in which they live, it is as if they were able to change the world. In the third world, in which they live, it is as if they were able to live in the third world. In the third world, in which they live in the third world, in which they live, in which they live, in which they live, in which they live, in which they live, in the world, in which they live, in which they live, in which they live, in the world, in which they live, in which they live, in the world, in which they live, in the world, in which they live, in the world, in which they live, in the world, in which they live, in the world, in which they live, in which they live, in the world, in which they live, in which they live, in the world, in which they live, in which they live, in which they live in the world, in which they live in which they live, in which they live in the world, in which they live, in which they live in which they live, in which they live in the world, in which they live in which they live, in which they live in which they live in the world, in which they live, in which they live in which they live, in which they live in which they live in the world, in which they live, in which they live in which they live in the world, in which they live in which they live, in which they live in which they live in the world, in which they live, in which they live in the world, in which they live in which they live, in which they live in the world, in which they live in which they live, in which they live in the world, in which they live, in which they live in which they live in the world, in which they live in which they live, in which they live in which they live in the world, in which they live in the world, in which they live, in which they live in which they live in the world, in which they live in the world, in which they live, in which they live in which they live in which they live"}, {"heading": "4.1. Recognition Accuracy", "text": "\"For the first time in my life, I have been able to do this for the first time. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the last few years. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the last few years. I have been able to do it for the first time. I have been able to do it for the last few years. I have been able to do it for the first time. I have been able to do it for the first time. I have been able to do it for the first time."}, {"heading": "1105, 2012. 8", "text": "[PeforatedCNNs] Michael Figurnov, Dmitry Vetrov, and Pushmeet Kohl, PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions, arXiv, 2015, https: / / arxiv.org / pdf / 1504.08362.pdf [Squeeze] Iandola, Forrest N., et al. \"SqueezeNet: AlexNet-level accuracy with 50x less parameters and < 1MB model size.\" arXiv preprint arXiv: 1602.07360 (2016 [Yang] Tien-Ju Yang, Yu-Hsin Chen, Vivienne Sze: Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning. CoRR abs / 1611.05128 (2016) [Wen16] Wen, Wei, Wu Chunpeng, Wang Yandan, Chen Yiran, Li Processan, Haci Neural, Neural, Networks 20In Information"}], "references": [{"title": "Do deep nets need to be deep", "author": ["Ba] Jimmy Ba", "Rich Caruana"], "venue": "In NIPS,", "citeRegEx": "Ba and Caruana.,? \\Q2014\\E", "shortCiteRegEx": "Ba and Caruana.", "year": 2014}, {"title": "contribution), An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Yu Cheng", "Felix X. Yu", "Rogerio Feris", "Sanjiv Kumar", "Alok Choudhary", "Shih-Fu Chang (*equal"], "venue": null, "citeRegEx": "Cheng. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng. et al\\.", "year": 2015}, {"title": "Learning The Structure of Deep Convolutional Networks", "author": ["Jiashi Feng", "Trevor Darrell"], "venue": "The IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Feng and Darrell,? \\Q2015\\E", "shortCiteRegEx": "Feng and Darrell", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other \u0308 neural network architectures", "author": ["Graves] Alex Graves", "Jurgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves and Schmidhuber.,? \\Q2005\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Learning both Weights and Connections for Efficient Neural Networks,", "author": ["S. Han", "J. Pool", "J. Tran", "W.J. Dally"], "venue": "Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures", "author": ["H. Hu", "R. Peng", "Y.-W. Tai", "C.-K. Tang"], "venue": "arXiv preprint arXiv:1607.03250,", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions, arXiv", "author": ["PeforatedCNNs] Michael Figurnov", "Dmitry Vetrov", "Pushmeet Kohl"], "venue": null, "citeRegEx": "Figurnov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Figurnov et al\\.", "year": 2015}, {"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 1MB model size.", "author": ["Squeeze] Iandola", "Forrest N"], "venue": null, "citeRegEx": "Iandola and N,? \\Q2016\\E", "shortCiteRegEx": "Iandola and N", "year": 2016}, {"title": "Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning", "author": ["Yang] Tien-Ju Yang", "Yu-Hsin Chen", "Vivienne Sze"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Learning structured sparsity in deep neural networks", "author": ["Wen", "Wei", "Wu Chunpeng", "Wang Yandan", "Chen Yiran", "Li Hai"], "venue": "Advances In Neural Information Processing Systems,", "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}], "referenceMentions": [], "year": 2017, "abstractText": "A new, radical CNN design approach is presented in this paper, considering the reduction of the total computational load during inference. This is achieved by a new holistic intervention on both the CNN architecture and the training procedure, which targets to the parsimonious inference by learning to exploit or remove the redundant capacity of a CNN architecture. This is accomplished, by the introduction of a new structural element that can be inserted as an add-on to any contemporary CNN architecture, whilst preserving or even improving its recognition accuracy. Our approach formulates a systematic and data-driven method for developing CNNs that are trained to eventually change size and form in real-time during inference, targeting to the smaller possible computational footprint. Results are provided for the optimal implementation on a few modern, high-end mobile computing platforms indicating a significant speed-up of up to x3 times.", "creator": "Word"}}}