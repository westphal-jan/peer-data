{"id": "1701.05291", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jan-2017", "title": "Heterogeneous Information Network Embedding for Meta Path based Proximity", "abstract": "A network embedding is a representation of a large graph in a low-dimensional space, where vertices are modeled as vectors. The objective of a good embedding is to preserve the proximity between vertices in the original graph. This way, typical search and mining methods can be applied in the embedded space with the help of off-the-shelf multidimensional indexing approaches. Existing network embedding techniques focus on homogeneous networks, where all vertices are considered to belong to a single class.", "histories": [["v1", "Thu, 19 Jan 2017 04:00:46 GMT  (4375kb,D)", "http://arxiv.org/abs/1701.05291v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["zhipeng huang", "nikos mamoulis"], "accepted": false, "id": "1701.05291"}, "pdf": {"name": "1701.05291.pdf", "metadata": {"source": "CRF", "title": "Heterogeneous Information Network Embedding for Meta Path based Proximity", "authors": ["Zhipeng Huang", "Nikos Mamoulis"], "emails": ["nikos}@cs.hku.hk"], "sections": [{"heading": null, "text": "Keywords: heterogeneous information network; metapath; network embedding"}, {"heading": "1. INTRODUCTION", "text": "In fact, it is such that people live in the countries in which they live, live in another world in which they live, live in another world in which they live, live in which they live, live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, live."}, {"heading": "2. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Heterogeneous Information Networks", "text": "The heterogeneity of nodes and edges in HINs brings challenges, but also opportunities to support important applications. Recently, there has been an increasing interest in the effective search and analysis of information from HINs in both science and industry. The problem of classification of objects in an HIN by authority multiplication is explored in [12]. Subsequent work [13] investigates a collective classification problem in HINs using metapath-based dependencies. PathSelClus [30] is a link-based cluster algorithm for HINs in which a user can specify his cluster preference as seed based on some examples. The problem of link prediction on HINs has been extensively investigated for its important applications (e.g. in recommendation systems) [27, 28, 35]. A related problem is the entity recommendation in HINs [34], which takes advantage of the different types of relationships in HINs to make better recommendations."}, {"heading": "2.2 Meta Path and Proximity Measures", "text": "Metapath [29] is a general model of proximity between objects in a HIN. Several measures of proximity between objects and a given metapath P have been proposed. PathCount measures the number of metapath instances linking the two objects, and PathSim is a normalized version of it [29]. Path Confined Random Walk (PCRW) was first proposed [14] for the task of queriing relationships over bibliographic networks. Later [17] an automatic approach was proposed to learn the best combination of metapaths and their corresponding weights based on PCRW. Finally, HeteSim [25] is recently proposed as an extension of the metapath based on SimRank. In this paper, we focus on the two most popular proxional measures, namely PathCount and PCRW."}, {"heading": "2.3 Network Embedding", "text": "In fact, it is such that most people who are able to be able to be able to move, are able to be able to move, are able to be able to move, are able to feel able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able"}, {"heading": "3. PROBLEM DEFINITION", "text": "It is true that this is a very strange story, but it is not a very strange story, but rather a story in which it is a story, a story, a story, a story, a story, a story, a story, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a history, a, a history, a history, a history, a history, a history, a history, a history, a, a history, a history, a history, a history, a, a history, a history, a history, a history, a, a history, a history, a, a history, a history, a history, a history"}, {"heading": "4. HINE", "text": "In this section, we will present our methodology for embedding HINs. In Section 4.1, we will first discuss how to calculate the shortened estimate of metapath-based proximity. In Section 4.2, we will then present our model and define the objective function that we want to optimize in Section 4.2. Finally, we will present a negative sampling approach that accelerates the optimization of the objective function in Section 4.3."}, {"heading": "4.1 Truncated Proximity Calculation", "text": "To calculate the spatial proximity of two objects, all we need is a network in which we accumulate the corresponding meta paths. (...) We must accumulate the corresponding meta paths (...). (...) We can see that there is a length of two meta paths and that the number of possible meta paths is growing exponentially. (...) We can calculate the length of the meta paths for certain HIN paths (in this case, calculating the s-4 meta paths is impossible). (...) As we have pointed out in [29], shorter meta paths are more informative than longer, because longer meta paths (which are less related)."}, {"heading": "4.2 Model", "text": "For each object pair, we use the sigmoid function to define the common probability, i.e. p (oi, oj) = 11 + e \u2212 vi \u00b7 vj (5), where vi (or vj) Rd is the low-dimensional representation of the object oi (or oj). p: V 2 \u2192 R is a probability distribution over an object pair in the original back. In the original HIN G, the empirical common probability of oi and oj can be defined as follows: p \u0441 (oi, oj) = s (oi, oj)."}, {"heading": "4.3 Negative Sampling", "text": "The direct optimization of the objective function in Equation 8 is problematic. Firstly, there is a trivial solution: vi, d = \u221e for all i and all d. secondly, it is mathematically expensive to calculate the gradient, since we have to sum up all approximate values without zero for a given object oi (oi, oj). To solve these problems, we use the negative sample proposed in [19], which essentially scans a small number of negative objects to amplify the influence of positive objects. Formally, we define an objective function for each pair of objects with a non-zero meta path based on the proximity s (oi, oj): T (oi, oj) = \u2212 log (1 + e \u2212 vivj) \u2212 K \u00b2 1 Ev \u00b2 Pn (oi) [log (1 + e viv \u00b2)]]] (9), where K \u2212 gradients are the times of sampling, and Pn \u2212 vize (v) has a certain sound distribution."}, {"heading": "5. EXPERIMENTS", "text": "In this section, we conduct extensive experiments to test the effectiveness of our proposed HIN embedding approach. First, we present our data sets and the methods to compare in Section 5.1. Then, we evaluate the effectiveness of all approaches to five important data mining tasks: network recovery (Section 5.2), classification (Section 5.3), clustering (Section 5.4), k-NN search (Section 5.5), and visualization (Section 5.6). In addition, we perform a case study (Section 5.7) to compare the quality of top-k lists. We evaluate the impact of parameter l in Section 5.8. Finally, we evaluate the runtime costs of applying our proposed transformation in Section 5.9."}, {"heading": "5.1 Dataset and Configurations", "text": "We use four real data sets in our evaluation. Table 2 shows some statistics about them. \u2022 DBLP. The scheme of the DBLP network is shown in Figure 2 (a). We use a subset of DBLP, i.e., DBLP-4 area taken of [29], which contains 5,237 essays (P), 5,915 authors (A), 18 places (V), 4,479 topics (T). The authors come from 4 areas: database, data mining, machine learning and information retrieval. \u2022 MOVIE. We extracted a subset from YAGO [10], which contains knowledge about movies. The scheme of the MOVIE network is shown in Figure 2 (b). It contains 7,332 films (M), 10,789 actors (A), 1,741 directors (D), 3,392 producers (P) and 1,483 composers (C). The films are divided into five genres: action, horror, adventure, the network we compare."}, {"heading": "5.2 Network Recovery", "text": "We first compare the effectiveness of different network embedding methods in a link recovery task. For each type of link (i.e. edges) in the HIN scheme, we list all pairs of objects (os, ot) that can be connected by such a connection, and calculate their proximity in low-dimensional space after embedding os to vs and ot to vt. Finally, we use the area under the ROC curve (AUC) to evaluate the performance of each embedding method. For example, for edge writing, we list all pairs of authors ai and papers pj in DBLP and calculate the proximity for each pair. Finally, using the real DBLP network as the ground truth, we calculate the AUC value for each embedding method. The results for d = 10 are shown in Table 3. Note that generally HINE _ PC and HINE _ PCRW perform better than Line and DeepWalk."}, {"heading": "5.3 Classification", "text": "We perform a task of multi-label classification. For DBLP, we use authors \"ranges as labels. For MOVIE, we use the genres of films as labels. For HINP, we use restaurant types as labels. For GAME, we use the type of games as labels. We first use various methods to embed the HINs. Then, we randomly divide the samples into a training and a test set with a ratio of 4: 1. Finally, we use k nearest (k \u2212 NN) classifiers with k = 5 to predict the labels of the test samples. We repeat the process 10 times and calculate the average Macro 1 and Micro F1 values to evaluate performance. Table 5 shows the results for d = 10. We can see that all methods have better results on DBLP and YELP than on MOVIE and GAME. This is because the average degree of performance on MOVIE and GAME is smaller, and the HINE values are lower (DIVE and BLE)."}, {"heading": "5.4 Clustering", "text": "In DBLP we cluster the authors, in MOVIE we cluster the films, in YELP we cluster the restaurants, and in GAME we cluster the games. We use the same basic truth as in Section 5.3. We use normalized mutual information (NMI) to evaluate performance. Table 6 shows the results for d = 10. We can see that the performance of all methods for cluster tasks is not as stable as that in the classification. All embedding methods perform well for DBLP, but they perform relatively poorly for the other three sets of data. In DBLP, DeepWalk and HINE perform better than Line. In MOVIE, HINE _ PC and HINE _ PCRW outperform all other approaches. In YELP, LINE performs better than HINE _ PCRW, and DeepWalk performs very poorly. In GAME, DeepWalk performs slightly better than the others. Overall, in the approaches of BLE _ 5, we can beat the overall results of RP (RP) PCE."}, {"heading": "5.5 k-NN Search", "text": "We compare the performance of three methods, namely LINE, DeepWalk and HINE _ PCRW, for k-NN search tasks. Specifically, we perform a case study on DBLP to compare the quality of the k closest authors for the venue WWW in the embedded space. We first evaluate the quality of the k-NN search by counting the average number of papers published by the authors in the k-NN result on the WWW if they vary from k 1 to 100 (Figure 4 (a)). We can see that the closest authors found in the embedding of HINE _ PCRW have more papers published in the WWW than those found in the areas Line and DeepWalk. We then use the top k author list for the venue WWW in the original DBLP network as the basic truth. We use two different metrics to evaluate the quality of the top k lists in the embedded space, i.e. closer to each other's footprint rule that BLR'F and BLR'P are in each other."}, {"heading": "5.6 Visualization", "text": "We compare the performance of all approaches in the task of visualization, which aims to place an HIN on a two-dimensional space. Specifically, we first use an embedding method to map DBLP into a vector space, then map the authors \"vectors into a two-dimensional space using the t-SNE [16] package. The results are in Figure 6. Line can basically separate authors from different groups (represented by the same color), but some blue dots mix with other groups. The result of DeepWalk is not particularly good, as many authors from different groups are mixed together. HINE _ PC clearly has better performance than DeepWalk. Compared to Line, HINE _ PC can better separate different groups. Finally, HINE _ PCRW is the best result among these methods, as it clearly separates authors from different groups and leaves a lot of empty space between clusters. This is consistent with the fact that HINE _ PCRW performs best in classifying authors from DBLP."}, {"heading": "5.7 Case Study", "text": "We are conducting a case study showing that the k-NN objects to a0.500.600.700.800.901.001 2 3 4 5M acro -F1lHINE _ PC HINE _ PCRW (a) Macro-F1 w.r.t. l0.500.600.700.800.901.001 2 3 4 5M icro -F1lHINE _ PC HINE _ PCRW (b) Micro-F1 w.r.t. lgiven object in DBLP. Specifically, in Table 4 we are showing the top 5 places, authors and topics closest to author Jiawei Han in DBLP. Looking at the results for the top 5 places, it can be seen that the line does not make a good top 5 list as it cannot rank KDD in the top publishing places of the author. DeepWalk is slightly better, but it ranks ICDM in the 5th place, while PCE _ PCRW DKW and I1. respectively, the researchers are looking at the RoK and similar results."}, {"heading": "5.8 Effect of the l Threshold", "text": "Figure 7 (a) and (b) shows the results of the classification at DBLP. We can see that 1) HINE _ PCRW performs fairly well at DBLP and its best performance at l = 2. 2) The performance of HINE _ PC is best at l = 1. This is because PathCount considers metapath instances of varying lengths equally important, while PCRW assigns lower weights to the longer instances when performing the random walk. This explains why HINE _ PCRW outperforms HINE _ PC in these tasks. Figure 7 (c) shows the results of clustering at DBLP. The results are similar to those of classification except that HINE _ PC performs much better than HINE _ PCRW at l = 1. This is natural, because at l = 1 HINE _ PCRW can only detect very local proximity."}, {"heading": "5.9 Efficiency", "text": "We show in Figure 7 (d) the runtime for calculating truncated whole pairs on DBLP using the method described in Section 4.1. We can see that our proposed algorithm can run in a reasonable time and can be scaled well with l. Especially in our experiments with l = 2, the time for calculating whole pair proximities is less than 1000 s. Also note that using ASGD to solve our objective function in our experiments is very efficient and takes an average of 27.48 seconds on DBLP with d = 10."}, {"heading": "6. CONCLUSION", "text": "In this paper, we examine the problem of heterogeneous network embedding for metapath-based proximity, which can fully exploit the heterogeneity of the network. We also define an objective function that aims to minimize the removal of two distributions, one modelling metapath-based proximities, the other the proximities in embedded low-dimensional space. We also examine the use of negative samples to speed up the optimization process. As demonstrated in our extensive experiments, our embedding methods can better restore the original network and perform better on multiple data mining tasks, such as classification, clustering and visualization, using modern methods of network embedding."}, {"heading": "7. REFERENCES", "text": "[1] A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski 91 Information-Based Networks, andA. J. Smola. Distributed large-scale natural graph factorization. In WWW, pp. 37-48. ACM, 2013. [2] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives. Dbpedia: A nucleus for a web of open data. In The Semantic Web, pp. 722-735. Springer, 2007. [3] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, Volume 14, pp. 585-591. K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. Freebase."}], "references": [{"title": "Distributed large-scale natural graph factorization", "author": ["A. Ahmed", "N. Shervashidze", "S. Narayanamurthy", "V. Josifovski", "A.J. Smola"], "venue": "WWW, pages 37\u201348. ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Dbpedia: A nucleus for a web of open data", "author": ["S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z. Ives"], "venue": "The Semantic Web, pages 722\u2013735. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["M. Belkin", "P. Niyogi"], "venue": "NIPS, volume 14, pages 585\u2013591", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "SIGMOD, pages 1247\u20131250", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio"], "venue": "AAAI", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Grarep: Learning graph representations with global structural information", "author": ["S. Cao", "W. Lu", "Q. Xu"], "venue": "CIKM, pages 891\u2013900. ACM", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Heterogeneous network embedding via deep architectures", "author": ["S. Chang", "W. Han", "J. Tang", "G.-J. Qi", "C.C. Aggarwal", "T.S. Huang"], "venue": "SIGKDD, pages 119\u2013128. ACM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Multidimensional scaling", "author": ["T.F. Cox", "M.A. Cox"], "venue": "CRC press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Comparing top k lists", "author": ["R. Fagin", "R. Kumar", "D. Sivakumar"], "venue": "SIAM Journal on Discrete Mathematics, 17(1):134\u2013160", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Meta structure: Computing relevance in large heterogeneous information networks", "author": ["Z. Huang", "Y. Zheng", "R. Cheng", "Y. Sun", "N. Mamoulis", "X. Li"], "venue": "SIGKDD, pages 1595\u20131604", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Querying knowledge graphs by example entity tuples", "author": ["N. Jayaram", "A. Khan", "C. Li", "X. Yan", "R. Elmasri"], "venue": "TKDE, 27(10):2797\u20132811", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Ranking-based classification of heterogeneous information networks", "author": ["M. Ji", "J. Han", "M. Danilevsky"], "venue": "SIGKDD, pages 1298\u20131306. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Meta path-based collective classification in heterogeneous information networks", "author": ["X. Kong", "P.S. Yu", "Y. Ding", "D.J. Wild"], "venue": "CIKM, pages 1567\u20131571. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Relational retrieval using a combination of path-constrained random walks", "author": ["N. Lao", "W.W. Cohen"], "venue": "Machine learning, 81(1):53\u201367", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "The dblp computer science bibliography: Evolution", "author": ["M. Ley"], "venue": "research issues, perspectives. In SPIRE, pages 1\u201310. Springer", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Visualizing data using t-sne", "author": ["L. v. d. Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Discovering meta-paths in large heterogeneous information networks", "author": ["C. Meng", "R. Cheng", "S. Maniu", "P. Senellart", "W. Zhang"], "venue": "WWW, pages 754\u2013764. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "J. Dean"], "venue": "pages 3111\u20133119", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Exemplar queries: Give me an example of what you need", "author": ["D. Mottin", "M. Lissandrini", "Y. Velegrakis", "T. Palpanas"], "venue": "PVLDB, 7(5):365\u2013376", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Tri-party deep network representation", "author": ["S. Pan", "J. Wu", "X. Zhu", "C. Zhang", "Y. Wang"], "venue": "IJCAI, pages 1895\u20131901", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "SIGKDD, pages 701\u2013710. ACM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "NIPS, pages 693\u2013701", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, 290(5500):2323\u20132326", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "Hetesim: A general framework for relevance measure in heterogeneous networks", "author": ["C. Shi", "X. Kong", "Y. Huang", "S.Y. Philip", "B. Wu"], "venue": "TKDE, 26(10):2479\u20132492", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Yago: a core of semantic knowledge", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "WWW, pages 697\u2013706", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Co-author relationship prediction in heterogeneous bibliographic networks", "author": ["Y. Sun", "R. Barber", "M. Gupta", "C.C. Aggarwal", "J. Han"], "venue": "ASONAM, pages 121\u2013128. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "When will it happen?: relationship prediction in heterogeneous information networks", "author": ["Y. Sun", "J. Han", "C.C. Aggarwal", "N.V. Chawla"], "venue": "WSDM, pages 663\u2013672. ACM", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Pathsim: Meta path-based top-k similarity search in heterogeneous information networks", "author": ["Y. Sun", "J. Han", "X. Yan", "P.S. Yu", "T. Wu"], "venue": "PVLDB, 4(11):992\u20131003", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Pathselclus: Integrating meta-path selection with user-guided object clustering in heterogeneous information networks", "author": ["Y. Sun", "B. Norick", "J. Han", "X. Yan", "P.S. Yu", "X. Yu"], "venue": "TKDD, 7(3):11", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Line: Large-scale information network embedding", "author": ["J. Tang", "M. Qu", "M. Wang", "M. Zhang", "J. Yan", "Q. Mei"], "venue": "WWW, pages 1067\u20131077. ACM", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. De Silva", "J.C. Langford"], "venue": "Science, 290(5500):2319\u20132323", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2000}, {"title": "Structural deep network embedding", "author": ["D. Wang", "P. Cui", "W. Zhu"], "venue": "SIGKDD, pages 1225\u20131234. ACM", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Personalized entity recommendation: A heterogeneous information network approach", "author": ["X. Yu", "X. Ren", "Y. Sun", "Q. Gu", "B. Sturt", "U. Khandelwal", "B. Norick", "J. Han"], "venue": "WSDM, pages 283\u2013292. ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Meta-path based multi-network collective link prediction", "author": ["J. Zhang", "P.S. Yu", "Z.-H. Zhou"], "venue": "SIGKDD, pages 1286\u20131295. ACM", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "This motivated a lot of interest [6, 22, 31] in approaches that embed the network into a low-dimensional space, such that the original vertices of the graph are represented as vectors.", "startOffset": 33, "endOffset": 44}, {"referenceID": 21, "context": "This motivated a lot of interest [6, 22, 31] in approaches that embed the network into a low-dimensional space, such that the original vertices of the graph are represented as vectors.", "startOffset": 33, "endOffset": 44}, {"referenceID": 30, "context": "This motivated a lot of interest [6, 22, 31] in approaches that embed the network into a low-dimensional space, such that the original vertices of the graph are represented as vectors.", "startOffset": 33, "endOffset": 44}, {"referenceID": 14, "context": "Heterogeneous information networks (HINs), such as DBLP [15], YAGO [26], DBpedia [2] and Freebase [4], are networks with nodes and edges that may belong to multiple types.", "startOffset": 56, "endOffset": 60}, {"referenceID": 25, "context": "Heterogeneous information networks (HINs), such as DBLP [15], YAGO [26], DBpedia [2] and Freebase [4], are networks with nodes and edges that may belong to multiple types.", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "Heterogeneous information networks (HINs), such as DBLP [15], YAGO [26], DBpedia [2] and Freebase [4], are networks with nodes and edges that may belong to multiple types.", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "Heterogeneous information networks (HINs), such as DBLP [15], YAGO [26], DBpedia [2] and Freebase [4], are networks with nodes and edges that may belong to multiple types.", "startOffset": 98, "endOffset": 101}, {"referenceID": 10, "context": "These graph data sources contain a vast number of interrelated facts, and they can facilitate the discovery of interesting knowledge [11,14,17,20].", "startOffset": 133, "endOffset": 146}, {"referenceID": 13, "context": "These graph data sources contain a vast number of interrelated facts, and they can facilitate the discovery of interesting knowledge [11,14,17,20].", "startOffset": 133, "endOffset": 146}, {"referenceID": 16, "context": "These graph data sources contain a vast number of interrelated facts, and they can facilitate the discovery of interesting knowledge [11,14,17,20].", "startOffset": 133, "endOffset": 146}, {"referenceID": 19, "context": "These graph data sources contain a vast number of interrelated facts, and they can facilitate the discovery of interesting knowledge [11,14,17,20].", "startOffset": 133, "endOffset": 146}, {"referenceID": 28, "context": "Meta path [29] is a recently proposed proximity model in HINs.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "For example, PathCount [29] counts the number of meta path instances connecting the two objects, while Path Constrained Random Walk (PCRW) [14] measures the probability that a random walk starting from one object would reach the other via a meta path instance.", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "For example, PathCount [29] counts the number of meta path instances connecting the two objects, while Path Constrained Random Walk (PCRW) [14] measures the probability that a random walk starting from one object would reach the other via a meta path instance.", "startOffset": 139, "endOffset": 143}, {"referenceID": 28, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 110, "endOffset": 114}, {"referenceID": 26, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 132, "endOffset": 144}, {"referenceID": 27, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 132, "endOffset": 144}, {"referenceID": 34, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 132, "endOffset": 144}, {"referenceID": 33, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 161, "endOffset": 165}, {"referenceID": 11, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 182, "endOffset": 190}, {"referenceID": 12, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 182, "endOffset": 190}, {"referenceID": 29, "context": "mance compared to proximity measures not based on meta paths, in various important tasks, such as k-NN search [29], link prediction [27, 28, 35], recommendation [34], classification [12, 13] and clustering [30].", "startOffset": 206, "endOffset": 210}, {"referenceID": 6, "context": "Although there are a few works on embedding HINs [7, 21], none of them is designed for meta path based proximity in general HINs.", "startOffset": 49, "endOffset": 56}, {"referenceID": 20, "context": "Although there are a few works on embedding HINs [7, 21], none of them is designed for meta path based proximity in general HINs.", "startOffset": 49, "endOffset": 56}, {"referenceID": 28, "context": "As shown in [29], meta paths with too large lengths are not very informative; therefore, we only consider meta paths up to a given length threshold l.", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "We also investigate the use of negative sampling [19] in order to accelerate the optimization process.", "startOffset": 49, "endOffset": 53}, {"referenceID": 30, "context": ", LINE [31] and DeepWalk [22]), which do not consider meta path based proximity.", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": ", LINE [31] and DeepWalk [22]), which do not consider meta path based proximity.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "The problem of classifying objects in a HIN by authority propagation is studied in [12].", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "Follow-up work [13] investigates a collective classification problem in HINs using meta path based dependencies.", "startOffset": 15, "endOffset": 19}, {"referenceID": 29, "context": "PathSelClus [30] is a link based clustering algorithm for HINs, in which a user can specify her clustering preference by providing some examples as seeds.", "startOffset": 12, "endOffset": 16}, {"referenceID": 26, "context": "The problem of link prediction on HINs has been extensively studied [27, 28, 35], due to its important applications (e.", "startOffset": 68, "endOffset": 80}, {"referenceID": 27, "context": "The problem of link prediction on HINs has been extensively studied [27, 28, 35], due to its important applications (e.", "startOffset": 68, "endOffset": 80}, {"referenceID": 34, "context": "The problem of link prediction on HINs has been extensively studied [27, 28, 35], due to its important applications (e.", "startOffset": 68, "endOffset": 80}, {"referenceID": 33, "context": "A related problem is entity recommendation in HINs [34], which takes advantage of the different types of relationships in HINs to provide better recommendations.", "startOffset": 51, "endOffset": 55}, {"referenceID": 28, "context": "Meta path [29] is a general model for the proximity between objects in a HIN.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "PathCount measures the number of meta path instances connecting the two objects, and PathSim is a normalized version of it [29].", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "Path constrained random walk (PCRW) was firstly proposed [14] for the task of relationship retrieval over bibliographic networks.", "startOffset": 57, "endOffset": 61}, {"referenceID": 16, "context": "Later, [17] proposed an automatic approach to learn the best combination of meta paths and their corresponding weights based on PCRW.", "startOffset": 7, "endOffset": 11}, {"referenceID": 24, "context": "Finally, HeteSim [25] is recently proposed as an extension of meta path based SimRank.", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "Traditional dimensionality reduction techniques [3,8,24,32] typically construct the affinity graph using the feature vectors of the vertexes and then compute the eigenvectors of the affinity graph.", "startOffset": 48, "endOffset": 59}, {"referenceID": 7, "context": "Traditional dimensionality reduction techniques [3,8,24,32] typically construct the affinity graph using the feature vectors of the vertexes and then compute the eigenvectors of the affinity graph.", "startOffset": 48, "endOffset": 59}, {"referenceID": 23, "context": "Traditional dimensionality reduction techniques [3,8,24,32] typically construct the affinity graph using the feature vectors of the vertexes and then compute the eigenvectors of the affinity graph.", "startOffset": 48, "endOffset": 59}, {"referenceID": 31, "context": "Traditional dimensionality reduction techniques [3,8,24,32] typically construct the affinity graph using the feature vectors of the vertexes and then compute the eigenvectors of the affinity graph.", "startOffset": 48, "endOffset": 59}, {"referenceID": 0, "context": "Graph factorization [1] finds a low-dimensional representation of a graph through matrix factorization, after representing the graph as an adjacency matrix.", "startOffset": 20, "endOffset": 23}, {"referenceID": 30, "context": "However, since these general techniques are not designed for networks, they do not necessarily preserve the global network structure, as pointed out in [31].", "startOffset": 152, "endOffset": 156}, {"referenceID": 21, "context": "Recently, DeepWalk [22] is proposed as a method for learning the latent representations of the nodes of a social network, from truncated random walks in the network.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "DeepWalk combines random walk proximity with the SkipGram model [18], a language model that maximizes the co-occurrence probability among the words that appear within a window in a sentence.", "startOffset": 64, "endOffset": 68}, {"referenceID": 30, "context": "Second, as pointed out in [31], DeepWalk can only preserve secondorder proximity, leading to poor performance in some tasks, such as link recover and classification, which require first-order proximity to be well-preserved.", "startOffset": 26, "endOffset": 30}, {"referenceID": 30, "context": "LINE [31] is a recently proposed embedding approach for largescale networks.", "startOffset": 5, "endOffset": 9}, {"referenceID": 5, "context": "GraRep [6] further extends DeepWalk to utilize high-order proximities.", "startOffset": 7, "endOffset": 10}, {"referenceID": 32, "context": "SDNE [33] is a semi-supervised deep model that captures the non-linear structural information over the network.", "startOffset": 5, "endOffset": 9}, {"referenceID": 4, "context": "Similarly, [5] embeds entities in knowledge bases using an innovative neural network architecture and TriDNR [21] extends this embedding model to consider features from three aspects of the network: 1) network structure, 2) node content, and 3) label information.", "startOffset": 11, "endOffset": 14}, {"referenceID": 20, "context": "Similarly, [5] embeds entities in knowledge bases using an innovative neural network architecture and TriDNR [21] extends this embedding model to consider features from three aspects of the network: 1) network structure, 2) node content, and 3) label information.", "startOffset": 109, "endOffset": 113}, {"referenceID": 28, "context": "Each different type of a relationship is modeled by a meta path [29].", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "\u2022 Path Constrained Random Walk (PCRW) [14] is a more sophisticated way to define the proximity s(os, ot | pos\u2192ot) based on an instance pos\u2192ot .", "startOffset": 38, "endOffset": 42}, {"referenceID": 28, "context": ", PathSim [29], HeteSim [25] and BPCRW [17].", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": ", PathSim [29], HeteSim [25] and BPCRW [17].", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": ", PathSim [29], HeteSim [25] and BPCRW [17].", "startOffset": 39, "endOffset": 43}, {"referenceID": 28, "context": "As pointed out in [29], shorter meta paths are more informative than longer ones, because longer meta paths link more remote objects (which are less related semantically).", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": ", [22] reserves only second-order proximity, and [31] first-order and second-order proximities).", "startOffset": 2, "endOffset": 6}, {"referenceID": 30, "context": ", [22] reserves only second-order proximity, and [31] first-order and second-order proximities).", "startOffset": 49, "endOffset": 53}, {"referenceID": 18, "context": "To address these problems, we adopt negative sampling proposed in [19], which basically samples a small number of negative objects to enhance the influence of positive objects.", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "As suggested by [19], we set Pn(v) \u221d dout(v) where dout(v) is the out-degree of v.", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "We adopt the asynchronous stochastic gradient descent (ASGD) algorithm [23] to optimize the objective function in Equation 9.", "startOffset": 71, "endOffset": 75}, {"referenceID": 28, "context": ", DBLP-4-Area taken of [29], which contains 5,237 papers (P), 5,915 authors (A), 18 venues (V), 4,479 topics (T).", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "We extracted a subset from YAGO [10], which contains knowledge about movies.", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "We extracted from Freebase [4] a HIN, which is related to video games.", "startOffset": 27, "endOffset": 30}, {"referenceID": 21, "context": "\u2022 DeepWalk [22] is a recently proposed social network embedding method (see Section 2.", "startOffset": 11, "endOffset": 15}, {"referenceID": 30, "context": "\u2022 LINE [31] is a method that preserves first-order and secondorder proximities between vertices (see Section 2.", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": ", [10]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 8, "context": ", Spearman\u2019s footrule F and Kendall\u2019s tau K [9].", "startOffset": 44, "endOffset": 47}, {"referenceID": 15, "context": "Specifically, we first use an embedding method to map DBLP into a vector space, then, we map the vectors of authors to a 2-D space using the t-SNE [16] package.", "startOffset": 147, "endOffset": 151}], "year": 2017, "abstractText": "A network embedding is a representation of a large graph in a lowdimensional space, where vertices are modeled as vectors. The objective of a good embedding is to preserve the proximity (i.e., similarity) between vertices in the original graph. This way, typical search and mining methods (e.g., similarity search, kNN retrieval, classification, clustering) can be applied in the embedded space with the help of off-the-shelf multidimensional indexing approaches. Existing network embedding techniques focus on homogeneous networks, where all vertices are considered to belong to a single class. Therefore, they are weak in supporting similarity measures for heterogeneous networks. In this paper, we present an effective heterogeneous network embedding approach for meta path based proximity measures. We define an objective function, which aims at minimizing the distance between two distributions, one modeling the meta path based proximities, the other modeling the proximities in the embedded vector space. We also investigate the use of negative sampling to accelerate the optimization process. As shown in our extensive experimental evaluation, our method creates embeddings of high quality and has superior performance in several data mining tasks compared to state-of-the-art network embedding methods.", "creator": "LaTeX with hyperref package"}}}