{"id": "1701.05498", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jan-2017", "title": "T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects", "abstract": "We introduce T-LESS, a new public dataset for estimating the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit symmetries and mutual similarities in shape and/or size. Compared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are provided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training images depict individual objects against a black background. Test images originate from twenty test scenes having varying complexity, which increases from simple scenes with several isolated objects to very challenging ones with multiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a systematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects. Initial evaluation results indicate that the state of the art in 6D object pose estimation has ample room for improvement, especially in difficult cases with significant occlusion. The T-LESS dataset is available online at cmp.felk.cvut.cz/t-less.", "histories": [["v1", "Thu, 19 Jan 2017 16:16:36 GMT  (4094kb,D)", "http://arxiv.org/abs/1701.05498v1", "WACV 2017"]], "COMMENTS": "WACV 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.RO", "authors": ["tomas hodan", "pavel haluza", "stepan obdrzalek", "jiri matas", "manolis lourakis", "xenophon zabulis"], "accepted": false, "id": "1701.05498"}, "pdf": {"name": "1701.05498.pdf", "metadata": {"source": "CRF", "title": "T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects", "authors": ["Tom\u00e1\u0161 Hoda\u0148", "Pavel Haluza", "\u0160t\u011bp\u00e1n Obdr\u017e\u00e1lek", "Ji\u0159\u0131\u0301 Matas", "Manolis Lourakis", "Xenophon Zabulis"], "emails": [], "sections": [{"heading": null, "text": "The dataset contains thirty industry-relevant objects with no significant texture and no discriminatory color or reflection properties; the objects have symmetries and similarities in shape and / or size; and, compared to other datasets, a unique feature is that some of the objects are parts of others; the dataset includes training and test images taken with three synchronized sensors, i.e. a manually generated CAD model and a semi-automatically reconstructed RGB-D sensor and a high-resolution RGB camera; there are approximately 39K training images and 10K test images from each sensor; in addition, two types of 3D models are provided for each object, i.e. a manually generated CAD model and a semi-automatically reconstructed RGB-D sensor. Training images show individual objects against a black background. Test images are taken from twenty test scenes of varying complexity, which differ from simple scenes with multiple isolated objects to highly systematically challenging objects with multiple highly intuitive 6D images."}, {"heading": "1. Introduction", "text": "This year, it is only a matter of time before the country returns to the EU."}, {"heading": "2. Related Datasets", "text": "First, we review data sets to estimate the 6D pose of certain rigid objects, grouped by the type of images provided, then we mention a few data sets designed for similar problems. Unless otherwise stated, these data sets provide basic truth annotations in the form of 6D object positions."}, {"heading": "2.1. RGB-D Datasets", "text": "In fact, most of them are able to go in search of a solution that they can take into their own hands."}, {"heading": "2.2. Depth-only and RGB-only Datasets", "text": "The pure depth dataset from Mian et al. [34] contains 3D mesh models of 5 objects and 50 test depth images taken with an industrial scanner; the test scenes include only the modeled objects that obscure each other; a similar dataset is provided by Taati et al. [46]; the Desk3D dataset [3] includes 3D mesh models of 6 objects taken in over 850 test depth images with occlusion, disorder and similar-looking deflection objects; the dataset was obtained with an RGB-D sensor, but only the depth images are publicly available.The IKEA dataset from Lim et al. [30] provides RGB images of objects aligned with their exactly matching 3D models. Crivellaro et al. [12] provides 3D CAD models and commented RGB sequences with 3 highly enclosed and textured objects."}, {"heading": "2.3. Datasets for Similar Problems", "text": "The RGB-D dataset by Michel et al. [35] focuses on articulated objects, with the goal of estimating the 6D pose of each part of the object, depending on the constraints imposed by its joints. There are also datasets for categorical pose estimation. For example, the 3DNet [55] and the UoBHOOC [53] contain generic 3D models and RGB-D images commented with 6D object positions. UBC VRS [32], the RMRC (a subset of NYU Depth v2 [41] with comments from [18]), the B3DO [26] and the SUN RGBD [43] do not provide 3D models and the truth only in the form of Bounding Boxes. The PASCAL3D + [57] and the ObjectNet3D [56] provide generic 3D models and grounded 6D poses, but only RGB images."}, {"heading": "3. The T-LESS Dataset", "text": "Compared to the verified data sets, T-LESS is unique in its combination of the following features: It contains 1) a larger number of industry-relevant objects, 2) training images taken under controlled conditions, 3) test images with large changes in aspect, objects in multiple cases affected by disorder and occlusion; including test cases that are challenging even by the most advanced methods, 4) images taken with a synchronized and calibrated triad of sensors, 5) accurate 6D poses for all modelled objects, and 6) two types of 3D models for each object. The rest of the section describes the process of data set preparation, which includes image acquisition, camera calibration, depth correction, 3D object generation, and soil truth commenting."}, {"heading": "3.1. Acquisition Setup", "text": "The training and test images were taken using the setup shown in Fig. 3. It consists of a turntable on which the objects depicted were placed and a device with adjustable inclination to which the sensors were attached. A marking field was attached to the turntable, which was used for the camera position. The field was extended vertically to the sides of the turntable to facilitate positioning at lower heights. To capture the training images, the objects were placed in the center of the turntable and in front of a black screen, which provided a uniform background at all heights. To introduce an uneven background in the test images, a plywood panel was placed with markers at the edges on the turntable. In some scenes, the objects were placed on the top of other objects (e.g. books) to give them different heights and thus relieve a ground assumption that could be made by an evaluated method. The depth of the object areas in the test images is in the training range from 2 - 4GB to 0.5m for the measurement images."}, {"heading": "3.2. Calibration of Sensors", "text": "Intrinsic and distorting parameters of the sensors were estimated using the standard checkerboard-based method using OpenCV [6]. The root mean square re-projection error calculated at the corners of the calibration checkerboard squares is 0.51 px for Carmine, 0.35 px for Kinect and 0.43 px for Canon. For the RGB-D sensors, calibration was performed using the RGB images. Depth images were already provided with the RGB images using the factory Depthto color registration SDKs (OpenNI 2.2 and Kinect for Windows SDK 2.0). The color and aligned depth images that are included in the datasets are processed to remove radial distortions. The intrinsic parameters can be found at the database."}, {"heading": "3.3. Training and Test Images", "text": "A common strategy for dealing with poorly structured objects is to adopt a template-based approach that is trained on objects captured with a dense scanning of viewing angles, e.g. [13, 20, 38, 24]. To support such approaches, T-LESS offers educational images of each object isolated from a transparent sphere. These images were obtained using a systematic capture process that scans the height from 85 to \u2212 85 percent with a 10x step and the entire azimuth range with a 5-step uniform scan. Views from the upper and lower hemispheres were captured separately, with the object in between turned upside down. In total, there are 18 x 72 = 1296 training images per object from each sensor. Exceptions are objects 19 and 20 for which only views from the upper hemisphere were captured, specifically 648 images from the height of 85 x to 5 inches. These objects are horizontally symmetrical in the pose in which they were placed on the revolving table, i.e. the top hemisphere of the hemisphere, the hemisphere where the images were taken."}, {"heading": "3.4. Depth Correction", "text": "Similar to [16, 45], we observed that the depths measured by the RGB-D sensors have a systematic error. To eliminate it, we recorded depth measurements d with marker corner projections and calculated their expected depth values de from the known marker coordinates. Measurements were taken from the depth range 0.53 - 0.92 m, in which the objects appear in the training and test images. For depth correction, we found the following linear correction models according to the smallest square mapping: dc = 1.0247 \u00b7 d \u2212 5.19 for Carmine and dc = 1.0266 \u00b7 d \u2212 26.88 for Kinect (depth measured in mm). In [45], only scaling is used. According to Foix et al. [16], a 3-degree polynomial function is sufficient to correct the depth in the range of 1 - 2 m. In our case, a narrower range is used, and we found a simple linear polynomial to adequately reflect the distance from the 3.0 to the 3.8 mm of the average depth of the correction."}, {"heading": "3.5. 3D Object Models", "text": "For each object, a manually generated CAD model and a semi-automatically reconstructed model were provided (Fig. 5). Both models are provided in the form of 3D meshes with surface norms on model nodes. Surface color is recorded only for the reconstructed models, the standards were calculated using MeshLab [7], since the weighted sum of the facial norms meets a vertex [48]. The reconstructed models were created using quick fusion, a volumetric 3D mapping system by Steinbru \ufffd cker et al. [44] The entrance to Carmine's RGB-D training images and the associated camera are displayed with the fictional markings (see Sec. 3.2). For each object, two partial models were initially reconstructed, one for the upper and one for the lower level."}, {"heading": "3.6. Ground Truth Poses", "text": "In order to obtain Ground Truth 6D object positions for images of a test scene, a dense 3D model of the scene was first reconstructed using the Steinbru \ufffd cker et al. [44] system, using all 504 RGB-D images of the scene together with the sensor positions estimated with the rotary marker markers. Subsequently, the CAD object models were manually aligned with the scene model. To increase accuracy, the object models were rendered into several selected Canon high-resolution scene images, malpositions were identified and the poses were refined accordingly. This process was repeated until a satisfactory alignment of the renderings with the scene images was achieved. Final poses were distributed to all test images using the familiar camera-to-rotary coordinate transformations."}, {"heading": "4. Design Validation and Experiments", "text": "This section presents an accurate assessment of the actual location of the soil and examines the difficulty of T-LESS with a newer 6D localization method."}, {"heading": "4.1. Accuracy of the Ground Truth Poses", "text": "In an effort to evaluate the accuracy of the soil truths, we compared the recorded depth images after the correction described in paragraph 3.4 with depth images obtained by graphically rendering the 3D object models on the ground. For each pixel with a valid depth value in both images, we calculated the difference \u03b4 = dc \u2212 dr, where dc is the depth recorded and dr is the depth rendered. Table 1 presents statistics of these differences, aggregated via all-training and test depth images. Differences exceeding 5 cm and amounting to approximately 2.5% of the measurements were considered outliers and truncated before calculating the statistics. The present differences may be caused by faulty depth measurements or occlusion caused by distractor objects in the case of test images. The rendered depths are well aligned with the depths recorded by Carmine, as represented by the difference of micrometers or by occlusion caused by distance objects in the case of test images."}, {"heading": "4.2. 6D Localization", "text": "The latest template-based method used by Hodan et al. [24] was evaluated on the 6D localization problem. [23] The method consisted of a test image together with the identities of the object instances present in the image, and the goal is to estimate the 6D poses of these instances [23]. The method was evaluated on all test RGB-D images from the Carmine sensor. Parameters were set as described in [24], the templates were generated from the training images of Carmine, and the CAD models were used as detailed in the Pose Refinement Stage. Pose estimates were evaluated as in [20] using the average distance error for objects with indistinguishable views. This error measures the misalignment between the surface of the Model M at ground truth (R, t) and at the estimated Pose Pose Pose (R, t)."}, {"heading": "5. Conclusion", "text": "This paper presented T-LESS, a new dataset for the evaluation of 6D-pose estimates of texture-free objects, which can facilitate the systematic comparison of relevant methods.The dataset includes industry-relevant objects and is characterized by a large number of training and test images, precise 6D poses, multiple sampling modalities, test scenes with multiple object instances, and increasing difficulties due to occlusion and disorder. Initial evaluation results from the dataset indicate that the state of the art in estimating 6D object positions.The T-LESS dataset is available online at: cmp.felk.cvut.cz / t-less"}, {"heading": "Acknowledgements", "text": "This work was supported by the Technology Agency of the Czech Republic in the framework of the research programme TE01020415 (V3C - Visual Computing Competence Center), the CTU scholarship SGS15 / 155 / OHK3 / 2T / 13 and the DARWIN project of the European Commission, grant number 270138."}], "references": [{"title": "Automation of \u201cground truth\u201d annotation for multi-view RGB-D object instance recognition datasets", "author": ["A. Aldoma", "T. F\u00e4ulhammer", "M. Vincze"], "venue": "In IROS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A global hypotheses verification method for 3D object recognition", "author": ["A. Aldoma", "F. Tombari", "L. Di Stefano", "M. Vincze"], "venue": "In ECCV,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Robust instance recognition in presence of occlusion and clutter", "author": ["U. Bonde", "V. Badrinarayanan", "R. Cipolla"], "venue": "In ECCV,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Learning 6D object pose estimation using 3D object coordinates", "author": ["E. Brachmann", "A. Krull", "F. Michel", "S. Gumhold", "J. Shotton", "C. Rother"], "venue": "In ECCV,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Uncertainty-driven 6D pose estimation of objects and scenes from a single RGB", "author": ["E. Brachmann", "F. Michel", "A. Krull", "M.Y. Yang", "S. Gumhold", "C. Rother"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Learning OpenCV: Computer vision with the OpenCV library", "author": ["G. Bradski", "A. Kaehler"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Ranzuglia. MeshLab: an open-source mesh processing tool", "author": ["P. Cignoni", "M. Callieri", "M. Corsini", "M. Dellepiane", "F. Ganovelli"], "venue": "In Eurographics Italian Chapter Conf.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Metro: measuring error on simplified surfaces", "author": ["P. Cignoni", "C. Rocchini", "R. Scopigno"], "venue": "In Computer Graphics Forum,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "The MOPED framework: Object recognition and pose estimation for manipulation", "author": ["A. Collet", "M. Martinez", "S.S. Srinivasa"], "venue": "IJRR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "The development of a European benchmark for the comparison of assembly robot programming systems", "author": ["K. Collins", "A. Palmer", "K. Rathmill"], "venue": "In Robot technology and applications", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1985}, {"title": "Lessons from the Amazon picking challenge", "author": ["N. Correll", "K.E. Bekris", "D. Berenson", "O. Brock", "A. Causo", "K. Hauser", "K. Okada", "A. Rodriguez", "J.M. Romano", "P.R. Wurman"], "venue": "arXiv preprint arXiv:1601.05484,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "A novel representation of parts for accurate 3D object detection and tracking in monocular images", "author": ["A. Crivellaro", "M. Rad", "Y. Verdie", "K.M. Yi", "P. Fua", "V. Lepetit"], "venue": "In ICCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Real-time learning and detection of 3D texture-less objects: A scalable approach", "author": ["D. Damen", "P. Bunnun", "A. Calway", "W. Mayol-Cuevas"], "venue": "In BMVC,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Recovering 6D object pose and predicting next-best-view in the crowd", "author": ["A. Doumanoglou", "R. Kouskouridas", "S. Malassiotis", "T.-K. Kim"], "venue": "In CVPR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "RGBD datasets: Past, present and future", "author": ["M. Firman"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Lock-in time-of-flight (ToF) cameras: a survey", "author": ["S. Foix", "G. Alenya", "C. Torras"], "venue": "Sensors Journal,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Multiview RGB-D dataset for object instance detection", "author": ["G. Georgakis", "M.A. Reza", "A. Mousavian", "P.-H. Le", "J. Kosecka"], "venue": "arXiv preprint arXiv:1609.07826,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Support surface prediction in indoor scenes", "author": ["R. Guo", "D. Hoiem"], "venue": "In ICCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "A comprehensive performance evaluation of 3D local feature descriptors", "author": ["Y. Guo", "M. Bennamoun", "F. Sohel", "M. Lu", "J. Wan", "N.M. Kwok"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Model based training, detection and pose estimation of texture-less 3D objects in heavily cluttered scenes", "author": ["S. Hinterstoisser", "V. Lepetit", "S. Ilic", "S. Holzer", "G. Bradski", "K. Konolige", "N. Navab"], "venue": "In ACCV,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Going further with point pair features", "author": ["S. Hinterstoisser", "V. Lepetit", "N. Rajkumar", "K. Konolige"], "venue": "In ECCV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Efficient texture-less object detection for augmented reality guidance", "author": ["T. Hoda\u0148", "D. Damen", "W. Mayol-Cuevas", "J. Matas"], "venue": "In ISMARW,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Obdr\u017e\u00e1lek. On evaluation of 6D object pose estimation", "author": ["T. Hoda\u0148", "J. Matas"], "venue": "In ECCV Workshop on Recovering 6D Object Pose,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Detection and fine 3D pose estimation of textureless objects in RGB-D images", "author": ["T. Hoda\u0148", "X. Zabulis", "M. Lourakis", "\u0160. Obdr\u017e\u00e1lek", "J. Matas"], "venue": "In IROS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Occlusion reasoning for object detection under arbitrary viewpoint", "author": ["E. Hsiao", "M. Hebert"], "venue": "TPAMI,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "A category-level 3D object dataset: Putting the Kinect to work", "author": ["A. Janoch", "S. Karayev", "Y. Jia", "J.T. Barron", "M. Fritz", "K. Saenko", "T. Darrell"], "venue": "In Consumer Depth Cameras for Computer Vision", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Deep learning of local RGB-D patches for 3D object detection and 6D pose estimation", "author": ["W. Kehl", "F. Milletari", "F. Tombari", "S. Ilic", "N. Navab"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Accuracy and resolution of Kinect depth data for indoor mapping applications", "author": ["K. Khoshelham", "S. Elberink"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "A large-scale hierarchical multi-view RGB-D object dataset", "author": ["K. Lai", "L. Bo", "X. Ren", "D. Fox"], "venue": "In ICRA,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Parsing IKEA Objects: Fine Pose Estimation", "author": ["J.J. Lim", "H. Pirsiavash", "A. Torralba"], "venue": "In ICCV,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Model-based pose estimation for rigid objects", "author": ["M. Lourakis", "X. Zabulis"], "venue": "In Computer Vision Systems", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Mobile 3D object detection in clutter", "author": ["D. Meger", "J.J. Little"], "venue": "In IROS,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "On the repeatability and quality of keypoints for local feature-based 3D object retrieval from cluttered scenes", "author": ["A. Mian", "M. Bennamoun", "R. Owens"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Threedimensional model-based object recognition and segmentation in cluttered scenes", "author": ["A.S. Mian", "M. Bennamoun", "R. Owens"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Pose estimation of kinematic chain instances via object coordinate regression", "author": ["F. Michel", "A. Krull", "E. Brachmann", "M.Y. Yang", "S. Gumhold", "C. Rother"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Fast 6D pose estimation for texture-less objects from a single RGB image", "author": ["E. Mu\u00f1oz", "Y. Konishi", "V. Murino", "A.D. Bue"], "venue": "In ICRA,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "A dataset for improved RGBD-based object detection and pose estimation for warehouse pickand-place", "author": ["C. Rennie", "R. Shome", "K.E. Bekris", "A.F.D. Souza"], "venue": "RA-L,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Discriminatively trained templates for 3D object detection: A real time scalable approach", "author": ["R. Rios-Cabrera", "T. Tuytelaars"], "venue": "In ICCV,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "A new benchmark for pose estimation with ground truth from virtual reality", "author": ["C. Schlette"], "venue": "Production Engineering,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Indoor segmentation and support inference from RGBD images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "In ECCV,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "BigBIRD: A large-scale 3D database of object instances", "author": ["A. Singh", "J. Sha", "K.S. Narayan", "T. Achim", "P. Abbeel"], "venue": "In ICRA,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Sun RGB-D: A RGB-D scene understanding benchmark suite", "author": ["S. Song", "S.P. Lichtenberg", "J. Xiao"], "venue": "In CVPR,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Volumetric 3D mapping in real-time on a CPU", "author": ["F. Steinbr\u00fccker", "J. Sturm", "D. Cremers"], "venue": "In ICRA,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "A benchmark for the evaluation of RGB-D SLAM systems", "author": ["J. Sturm", "N. Engelhard", "F. Endres", "W. Burgard", "D. Cremers"], "venue": "In IROS,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Variable dimensional local shape descriptors for object recognition in range data", "author": ["B. Taati", "M. Bondy", "P. Jasiobedzki", "M. Greenspan"], "venue": "In ICCV,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2007}, {"title": "Latent-class hough forests for 3D object detection and pose estimation", "author": ["A. Tejani", "D. Tang", "R. Kouskouridas", "T.-K. Kim"], "venue": "In ECCV,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "Computing vertex normals from polygonal facets", "author": ["G. Th\u00fcrrner", "C.A. W\u00fcthrich"], "venue": "Journal of Graphics Tools,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1998}, {"title": "Online learning for automatic segmentation of 3D data", "author": ["F. Tombari", "L. Di Stefano", "S. Giardino"], "venue": "In 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2011}, {"title": "BOLD features to detect texture-less objects", "author": ["F. Tombari", "A. Franchi", "L. Di Stefano"], "venue": "In ICCV,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2013}, {"title": "Unique signatures of histograms for local surface description", "author": ["F. Tombari", "S. Salti", "L. Di Stefano"], "venue": "In ECCV,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "ARToolKitPlus for pose tracking on mobile devices", "author": ["D. Wagner", "D. Schmalstieg"], "venue": "CVWW,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2007}, {"title": "UoB highly occluded object challenge II, 2016", "author": ["K. Walas", "A. Leonardis"], "venue": "www.cs.bham.ac.uk/research/ projects/uob-hooc", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2016}, {"title": "Learning descriptors for object recognition and 3D pose estimation", "author": ["P. Wohlhart", "V. Lepetit"], "venue": "In CVPR,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2015}, {"title": "3DNet: Large-scale object class recognition from CAD models", "author": ["W. Wohlkinger", "A. Aldoma", "R.B. Rusu", "M. Vincze"], "venue": "In ICRA,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2012}, {"title": "ObjectNet3D: A large scale database for 3D object recognition", "author": ["Y. Xiang"], "venue": "In ECCV,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2016}, {"title": "Beyond PASCAL: A benchmark for 3D object detection in the wild", "author": ["Y. Xiang", "R. Mottaghi", "S. Savarese"], "venue": "In Winter Conference on Applications of Computer Vision,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2014}, {"title": "Multimodal blending for high-accuracy instance recognition", "author": ["Z. Xie", "A. Singh", "J. Uang", "K.S. Narayan", "P. Abbeel"], "venue": "In IROS,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2013}, {"title": "3D object pose refinement in range images", "author": ["X. Zabulis", "M. Lourakis", "P. Koutlemanis"], "venue": "In ICVS,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "The lack of texture implies that the object cannot be reliably recognized with traditional techniques relying on photometric local patch detectors and descriptors [9, 31].", "startOffset": 163, "endOffset": 170}, {"referenceID": 30, "context": "The lack of texture implies that the object cannot be reliably recognized with traditional techniques relying on photometric local patch detectors and descriptors [9, 31].", "startOffset": 163, "endOffset": 170}, {"referenceID": 32, "context": "Instead, recent approaches that can deal with texture-less objects have focused on local 3D feature description [33, 51, 19], and semi-global or ar X iv :1 70 1.", "startOffset": 112, "endOffset": 124}, {"referenceID": 49, "context": "Instead, recent approaches that can deal with texture-less objects have focused on local 3D feature description [33, 51, 19], and semi-global or ar X iv :1 70 1.", "startOffset": 112, "endOffset": 124}, {"referenceID": 18, "context": "Instead, recent approaches that can deal with texture-less objects have focused on local 3D feature description [33, 51, 19], and semi-global or ar X iv :1 70 1.", "startOffset": 112, "endOffset": 124}, {"referenceID": 19, "context": "global description relying primarily on intensity edges and depth cues [20, 24, 54, 5, 14, 21, 27].", "startOffset": 71, "endOffset": 98}, {"referenceID": 23, "context": "global description relying primarily on intensity edges and depth cues [20, 24, 54, 5, 14, 21, 27].", "startOffset": 71, "endOffset": 98}, {"referenceID": 52, "context": "global description relying primarily on intensity edges and depth cues [20, 24, 54, 5, 14, 21, 27].", "startOffset": 71, "endOffset": 98}, {"referenceID": 4, "context": "global description relying primarily on intensity edges and depth cues [20, 24, 54, 5, 14, 21, 27].", "startOffset": 71, "endOffset": 98}, {"referenceID": 13, "context": "global description relying primarily on intensity edges and depth cues [20, 24, 54, 5, 14, 21, 27].", "startOffset": 71, "endOffset": 98}, {"referenceID": 20, "context": "global description relying primarily on intensity edges and depth cues [20, 24, 54, 5, 14, 21, 27].", "startOffset": 71, "endOffset": 98}, {"referenceID": 26, "context": "global description relying primarily on intensity edges and depth cues [20, 24, 54, 5, 14, 21, 27].", "startOffset": 71, "endOffset": 98}, {"referenceID": 22, "context": "The dataset is intended for evaluating various flavors of the 6D object pose estimation problem [23] and other related problems, such as 2D object detection [50, 22] and object segmentation [49, 17].", "startOffset": 96, "endOffset": 100}, {"referenceID": 48, "context": "The dataset is intended for evaluating various flavors of the 6D object pose estimation problem [23] and other related problems, such as 2D object detection [50, 22] and object segmentation [49, 17].", "startOffset": 157, "endOffset": 165}, {"referenceID": 21, "context": "The dataset is intended for evaluating various flavors of the 6D object pose estimation problem [23] and other related problems, such as 2D object detection [50, 22] and object segmentation [49, 17].", "startOffset": 157, "endOffset": 165}, {"referenceID": 47, "context": "The dataset is intended for evaluating various flavors of the 6D object pose estimation problem [23] and other related problems, such as 2D object detection [50, 22] and object segmentation [49, 17].", "startOffset": 190, "endOffset": 198}, {"referenceID": 16, "context": "The dataset is intended for evaluating various flavors of the 6D object pose estimation problem [23] and other related problems, such as 2D object detection [50, 22] and object segmentation [49, 17].", "startOffset": 190, "endOffset": 198}, {"referenceID": 42, "context": "Another option is to use the training images for evaluating 3D object reconstruction methods [44], where the provided CAD models can serve as the ground truth.", "startOffset": 93, "endOffset": 97}, {"referenceID": 23, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Only a few public RGB-D datasets, from over one hundred reported by Firman in [15], enable the evaluation of 6D object pose estimation methods.", "startOffset": 78, "endOffset": 82}, {"referenceID": 16, "context": "The dataset introduced in [17] was captured with Microsoft Kinect v2, which is based on the time-of-flight principle.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "[20] has become a standard benchmark used in most of the recent work, e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38, 4, 47, 24, 54].", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "[38, 4, 47, 24, 54].", "startOffset": 0, "endOffset": 19}, {"referenceID": 45, "context": "[38, 4, 47, 24, 54].", "startOffset": 0, "endOffset": 19}, {"referenceID": 23, "context": "[38, 4, 47, 24, 54].", "startOffset": 0, "endOffset": 19}, {"referenceID": 52, "context": "[38, 4, 47, 24, 54].", "startOffset": 0, "endOffset": 19}, {"referenceID": 22, "context": "In the 6D localization problem (where information about the number and identity of objects present in the images is provided beforehand [23]), state-of-the-art methods achieve recognition rates that exceed 95% for most of the objects.", "startOffset": 136, "endOffset": 140}, {"referenceID": 3, "context": "[4] provided additional ground truth poses for all modeled objects in one of the test sequences from [20].", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[4] provided additional ground truth poses for all modeled objects in one of the test sequences from [20].", "startOffset": 101, "endOffset": 105}, {"referenceID": 45, "context": "[47] presented a dataset with 2 texture-less and 4 textured objects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] provide a dataset with 183 test images of 2 textured objects from [47] that appear in multiple instances in a challenging bin-picking scenario with heavy occlusion.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[14] provide a dataset with 183 test images of 2 textured objects from [47] that appear in multiple instances in a challenging bin-picking scenario with heavy occlusion.", "startOffset": 71, "endOffset": 75}, {"referenceID": 56, "context": "The Challenge and Willow datasets [58], which were collected for the 2011 ICRA Solutions in Perception Challenge, share a set of 35 textured household objects.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "Similar is the TUW dataset [1] that presents 17 textured and texture-less objects appearing in 224 test RGB-D images.", "startOffset": 27, "endOffset": 30}, {"referenceID": 36, "context": "The Rutgers dataset [37] is focused on perception for robotic manipulation during pick-and-place tasks and comprises of images from a cluttered warehouse environment.", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "It includes color 3D mesh models for 24 mostly textured objects from the Amazon Picking Challenge 2015 [11], that were captured in more than 10K test RGB-D images with various amounts of occlusion.", "startOffset": 103, "endOffset": 107}, {"referenceID": 1, "context": "[2] provide 3D mesh models without color information of 35 household objects that are both textured and texture-less and are often symmetric and mutually similar in shape and size.", "startOffset": 0, "endOffset": 3}, {"referenceID": 40, "context": "The BigBIRD dataset [42] includes images of 125 mostly textured objects that were captured in isolation on a turntable with multiple calibrated RGB-D and DSLR sensors.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "[17] provide 6735 test RGB-D images from kitchen scenes including a subset of the BigBIRD objects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] created an extensive dataset with 300 common household objects captured on a turntable from three elevations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[40] synthesized RGB-D images from simulated object manipulation scenarios involving 4 textureless objects from the Cranfield assembly benchmark [10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[40] synthesized RGB-D images from simulated object manipulation scenarios involving 4 textureless objects from the Cranfield assembly benchmark [10].", "startOffset": 145, "endOffset": 149}, {"referenceID": 33, "context": "[34] includes 3D mesh models of 5 objects and 50 test depth images acquired with an industrial range scanner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[46].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The Desk3D dataset [3] comprises of 3D mesh models for 6 objects which are captured in over 850 test depth images with occlusion, clutter and similarly looking distractor objects.", "startOffset": 19, "endOffset": 22}, {"referenceID": 29, "context": "[30] provides RGB images with objects being aligned with their exactly matched 3D models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] supply 3D CAD models and annotated RGB sequences with 3 highly occluded and texture-less objects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] provide RGB sequences of 6 texture-less objects that are each imaged in isolation against a clean background and without occlusion.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Further to the above, there exist RGB datasets such as [13, 50, 38, 25], for which the ground truth is provided only in the form of 2D bounding boxes.", "startOffset": 55, "endOffset": 71}, {"referenceID": 48, "context": "Further to the above, there exist RGB datasets such as [13, 50, 38, 25], for which the ground truth is provided only in the form of 2D bounding boxes.", "startOffset": 55, "endOffset": 71}, {"referenceID": 37, "context": "Further to the above, there exist RGB datasets such as [13, 50, 38, 25], for which the ground truth is provided only in the form of 2D bounding boxes.", "startOffset": 55, "endOffset": 71}, {"referenceID": 24, "context": "Further to the above, there exist RGB datasets such as [13, 50, 38, 25], for which the ground truth is provided only in the form of 2D bounding boxes.", "startOffset": 55, "endOffset": 71}, {"referenceID": 34, "context": "[35] is focused on articulated objects, where the goal is to estimate the 6D pose of each object part, subject to the constraints introduced by their joints.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "For example, the 3DNet [55] and the UoBHOOC [53] contain generic 3D models and RGB-D images annotated with 6D object poses.", "startOffset": 23, "endOffset": 27}, {"referenceID": 51, "context": "For example, the 3DNet [55] and the UoBHOOC [53] contain generic 3D models and RGB-D images annotated with 6D object poses.", "startOffset": 44, "endOffset": 48}, {"referenceID": 31, "context": "The UBC VRS [32], the RMRC (a subset of NYU Depth v2 [41] with annotations derived from [18]), the B3DO [26], and the SUN RGBD [43] provide no 3D models and ground truth only in the form of bounding boxes.", "startOffset": 12, "endOffset": 16}, {"referenceID": 39, "context": "The UBC VRS [32], the RMRC (a subset of NYU Depth v2 [41] with annotations derived from [18]), the B3DO [26], and the SUN RGBD [43] provide no 3D models and ground truth only in the form of bounding boxes.", "startOffset": 53, "endOffset": 57}, {"referenceID": 17, "context": "The UBC VRS [32], the RMRC (a subset of NYU Depth v2 [41] with annotations derived from [18]), the B3DO [26], and the SUN RGBD [43] provide no 3D models and ground truth only in the form of bounding boxes.", "startOffset": 88, "endOffset": 92}, {"referenceID": 25, "context": "The UBC VRS [32], the RMRC (a subset of NYU Depth v2 [41] with annotations derived from [18]), the B3DO [26], and the SUN RGBD [43] provide no 3D models and ground truth only in the form of bounding boxes.", "startOffset": 104, "endOffset": 108}, {"referenceID": 41, "context": "The UBC VRS [32], the RMRC (a subset of NYU Depth v2 [41] with annotations derived from [18]), the B3DO [26], and the SUN RGBD [43] provide no 3D models and ground truth only in the form of bounding boxes.", "startOffset": 127, "endOffset": 131}, {"referenceID": 55, "context": "The PASCAL3D+ [57] and the ObjectNet3D [56] provide generic 3D models and ground truth 6D poses, but only RGB images.", "startOffset": 14, "endOffset": 18}, {"referenceID": 54, "context": "The PASCAL3D+ [57] and the ObjectNet3D [56] provide generic 3D models and ground truth 6D poses, but only RGB images.", "startOffset": 39, "endOffset": 43}, {"referenceID": 5, "context": "Intrinsic and distortion parameters of the sensors were estimated with the standard checkerboard-based procedure using OpenCV [6].", "startOffset": 126, "endOffset": 129}, {"referenceID": 50, "context": "The extrinsic calibration was achieved using fiducial BCH-code markers from ARToolKitPlus [52].", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "These were used to estimate the camera pose in the turntable coordinate system by robustly solving the PnP problem and then refining the estimated 6D pose by non-linearly minimizing the cumulative re-projection error with the posest library from [31].", "startOffset": 246, "endOffset": 250}, {"referenceID": 12, "context": "[13, 20, 38, 24].", "startOffset": 0, "endOffset": 16}, {"referenceID": 19, "context": "[13, 20, 38, 24].", "startOffset": 0, "endOffset": 16}, {"referenceID": 37, "context": "[13, 20, 38, 24].", "startOffset": 0, "endOffset": 16}, {"referenceID": 23, "context": "[13, 20, 38, 24].", "startOffset": 0, "endOffset": 16}, {"referenceID": 15, "context": "Similarly to [16, 45], we observed that the depths measured by the RGB-D sensors exhibit a systematic error.", "startOffset": 13, "endOffset": 21}, {"referenceID": 43, "context": "Similarly to [16, 45], we observed that the depths measured by the RGB-D sensors exhibit a systematic error.", "startOffset": 13, "endOffset": 21}, {"referenceID": 43, "context": "In [45], only scaling is used for the depth correction.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "[16], a 3-degree polynomial function suffices to correct depth in the 1 \u2013 2 m range.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "The normals were calculated using MeshLab [7] as the angleweighted sum of face normals incident to a vertex [48].", "startOffset": 42, "endOffset": 45}, {"referenceID": 46, "context": "The normals were calculated using MeshLab [7] as the angleweighted sum of face normals incident to a vertex [48].", "startOffset": 108, "endOffset": 112}, {"referenceID": 42, "context": "[44].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] was used to measure the model differences.", "startOffset": 0, "endOffset": 3}, {"referenceID": 42, "context": "[44].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The average absolute difference \u03bc|\u03b4| is less than 5 mm for Carmine and 9 mm for Kinect, which is near the accuracy of the sensors [28] and is relatively small compared to the size of objects.", "startOffset": 130, "endOffset": 134}, {"referenceID": 23, "context": "[24] was evaluated on the 6D localization problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "The input is comprised of a test image together with the identities of object instances that are present in the image, and the goal is to estimate the 6D poses of these instances [23].", "startOffset": 179, "endOffset": 183}, {"referenceID": 23, "context": "The parameters were set as described in [24], the templates were generated from the training images from Carmine, and the CAD models were employed in the pose refinement stage as detailed in [59].", "startOffset": 40, "endOffset": 44}, {"referenceID": 57, "context": "The parameters were set as described in [24], the templates were generated from the training images from Carmine, and the CAD models were employed in the pose refinement stage as detailed in [59].", "startOffset": 191, "endOffset": 195}, {"referenceID": 19, "context": "Pose estimates were evaluated as in [20], using the average distance error for objects with indistinguishable views.", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "The visibility was estimated as in [23].", "startOffset": 35, "endOffset": 39}, {"referenceID": 23, "context": "[24] on the 6D localization problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20], which is close to the state of the art: [20] reports 96.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20], which is close to the state of the art: [20] reports 96.", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "6% and [5] reports 99.", "startOffset": 7, "endOffset": 10}], "year": 2017, "abstractText": "We introduce T-LESS, a new public dataset for estimating the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit symmetries and mutual similarities in shape and/or size. Compared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are provided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training images depict individual objects against a black background. Test images originate from twenty test scenes having varying complexity, which increases from simple scenes with several isolated objects to very challenging ones with multiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a systematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects. Initial evaluation results indicate that the state of the art in 6D object pose estimation has ample room for improvement, especially in difficult cases with significant occlusion. The T-LESS dataset is available online at cmp.felk.cvut.cz/t-less.", "creator": "TeX"}}}