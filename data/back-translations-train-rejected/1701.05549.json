{"id": "1701.05549", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jan-2017", "title": "Deep Neural Networks - A Brief History", "abstract": "Introduction to deep neural networks and their history.", "histories": [["v1", "Thu, 19 Jan 2017 18:43:56 GMT  (818kb)", "http://arxiv.org/abs/1701.05549v1", "14 pages, 14 figures"]], "COMMENTS": "14 pages, 14 figures", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["krzysztof j cios"], "accepted": false, "id": "1701.05549"}, "pdf": {"name": "1701.05549.pdf", "metadata": {"source": "CRF", "title": "Deep Neural Networks \u2013 A Brief History", "authors": ["Krzysztof J. Cios"], "emails": [], "sections": [{"heading": null, "text": "Deep Neural Networks - A Brief History Krzysztof J. Cios Virginia Commonwealth University and IITiS Polish Academy of Sciences"}, {"heading": "Introduction", "text": "In fact, it is so that most of them are able to survive themselves, without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process and in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which"}, {"heading": "Neuron Models", "text": "A wide range of neuron models from very easy to spiking ones is described next. Note that increasing biological detail of an artificial neuron model = = 2 cm = = S = 1 = 1 = 1 = 1 = 1 - It also increases its computational complexity. The first simple model of a neuron, called the threshold neuron, was developed by McCulloch and Pitts (1943). It calculates a dot product between the input vector and the weight vector of a neuron, and if it is higher than its transmission function (such as step function), it becomes an output threshold of 1 (otherwise 0).The first spiking neuron model was developed by Hodgkin and Huxley (1952), for which they later received a Nobel Prize. They model the giant neurons and they treated every component of the neuron, including its membrane, as an electrical component. The model is described by: where: Iulus = stimulated / injected current = 120 V = 1"}, {"heading": "Learning Rules", "text": "In fact, it is such that most of them will be able to put themselves in a different world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they are able to move, in which they are able to move, in which they live, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they"}, {"heading": "Network Architecture", "text": "In fact, it is not that we are able to find a solution that we can deal with, but we are able to find a solution that solves the problems that we want to solve, \"he told the Deutsche Presse-Agentur."}, {"heading": "Problems with DNN learning", "text": "Popular literature presents the advent of DNN as a panacea for solving difficult problems, such as image recognition, handwritten character recognition, etc. Moreover, it is carried out with high confidence / precision and without the need for human involvement. Unfortunately, the history of science teaches us that new technologies often come with a high dose of hype, and DNN is no exception. As described below, two groups of researchers have demonstrated spectacular failure of DNN in image recognition tasks that are trivial to humans. In one experiment, the researchers used a trained DNN and used it on slightly modified images, the so-called contrarian examples. The network saw the original images (before the modification) in training; the modification was such that no difference was perceptible to the human eye between the original and the contrary image; the latter had only slightly different statistical properties."}, {"heading": "Conclusions", "text": "The shortcomings of the DNN described above do not outweigh its many advantages, but to answer the question of why they failed in these experiments requires a lot of research. I think it is becoming increasingly important for computer scientists to work with neuroscientists to develop better image recognition algorithms so that the algorithms are not so easily deceived (Lim et al., 2011). This primarily requires more work by neuroscientists to better understand processes used by the brain in recognition tasks. Simply deceiving the DNN in some recognition tasks that are easily detectable by humans poses a very serious cyber security risk. Modern society relies heavily on machine learning techniques like DNN to perform many everyday tasks such as medical diagnosis, self-driving cars, investment in financial facilities, and even in a legal system. As researchers have shown that it is relatively easy to start with adverse examples, the automated systems on which we depend so much today may produce catastrophic results."}], "references": [{"title": "Spiking Deep Convolutional Neural Networks for Energy-Efficient Object Recognition", "author": ["Y Cao", "Y Chen", "D. Khosla"], "venue": "Intern. Journal of Computer Vision", "citeRegEx": "Cao et al\\.,? 2014", "shortCiteRegEx": "Cao et al\\.", "year": 2014}, {"title": "Image recognition neural network: IRNN", "author": ["KJ Cios", "I. Shin"], "venue": "Neurocomputing, 7(2):159-185", "citeRegEx": "Cios and Shin,? 1995", "shortCiteRegEx": "Cios and Shin", "year": 1995}, {"title": "Neocognitron: a self organizing neural network model for a mechanism for pattern recognition unaffected by shift in position. Biological Cybernetics 36:193-202", "author": ["K. Fukushima"], "venue": "Hebb DO", "citeRegEx": "Fukushima,? \\Q1980\\E", "shortCiteRegEx": "Fukushima", "year": 1980}, {"title": "A fast learning algorithm for deep belief nets,", "author": ["GE Hinton", "S Osindero", "Y. Teh"], "venue": "Neural Comput.,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": "Science, 313 (5786), 504-507.", "citeRegEx": "Hinton and Salakhutdinov,? 2006b", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "A quantitative description of membrane current and its application to conduction and excitation in nerve", "author": ["A.L. Hodgkin", "A.F. Huxley"], "venue": "J. Physiol. 177:500-544", "citeRegEx": "Hodgkin and Huxley,? 1952", "shortCiteRegEx": "Hodgkin and Huxley", "year": 1952}, {"title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex", "author": ["Hubel DH", "Wiesel TN."], "venue": "J. Physiol., 160:106\u2013 154", "citeRegEx": "DH and TN.,? 1962", "shortCiteRegEx": "DH and TN.", "year": 1962}, {"title": "Simple Model of Spiking Neurons", "author": ["E.M. Izhikevich"], "venue": "IEEE Transactions on Neural Networks, 14:1569-1572", "citeRegEx": "Izhikevich,? 2003", "shortCiteRegEx": "Izhikevich", "year": 2003}, {"title": "Polychronization: Computation with spikes,", "author": ["Izhikevich EM"], "venue": "Neural Comput.,", "citeRegEx": "EM.,? \\Q2006\\E", "shortCiteRegEx": "EM.", "year": 2006}, {"title": "Self-Organized Formation of Topologically Correct Feature Maps", "author": ["T. Kohonen"], "venue": "Biological Cybernetics, (43) 59-69", "citeRegEx": "Kohonen,? 1982", "shortCiteRegEx": "Kohonen", "year": 1982}, {"title": "Conditioned reflexes and Neuron Organization", "author": ["J. Konorski"], "venue": "Cambridge University Press, Cambridge (267p); Reprinted with a supplementary chapter in 1968 by Hafner Publ. Co., New York", "citeRegEx": "Konorski,? 1948", "shortCiteRegEx": "Konorski", "year": 1948}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["I. Krizhevsky. A. Sutskever", "G. Hinton"], "venue": "NIPS' 2012.", "citeRegEx": "Sutskever and Hinton,? 2012", "shortCiteRegEx": "Sutskever and Hinton", "year": 2012}, {"title": "Modeling of Multisensory Convergence with a Network of Spiking Neurons: A Reverse Engineering Approach", "author": ["HK Lim", "LP Keniston", "KJ. Cios"], "venue": "IEEE Transactions on Biomedical Engineering, 58(7):1940-1949", "citeRegEx": "Lim et al\\.,? 2011", "shortCiteRegEx": "Lim et al\\.", "year": 2011}, {"title": "Gradient-based learning applied to document recognition,\u201d Proc", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P. Haffner"], "venue": "IEEE, vol. 86, no. 11, pp. 2278\u2013 2324, 1998.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Convolutional networks and applications in vision", "author": ["Y. LeCun", "K. Kavukcuoglu", "C. Farabet"], "venue": "Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on, pages 253-256. IEEE", "citeRegEx": "LeCun et al\\.,? 2010", "shortCiteRegEx": "LeCun et al\\.", "year": 2010}, {"title": "Deep learning", "author": ["Y LeCun", "Y Bengio", "G. Hinton"], "venue": "Nature 521, 436\u2013444 (28 May 2015)", "citeRegEx": "LeCun et al\\.,? 2015", "shortCiteRegEx": "LeCun et al\\.", "year": 2015}, {"title": "A logical calculus of the ideas immanent in nervous activity", "author": ["McCulloch WS", "Pitts WH."], "venue": "Bulletin of Mathematics and Biophysics, 5:115-133", "citeRegEx": "WS and WH.,? 1943", "shortCiteRegEx": "WS and WH.", "year": 1943}, {"title": "Neural and Brain Modeling", "author": ["MacGregor RJ"], "venue": null, "citeRegEx": "RJ.,? \\Q1987\\E", "shortCiteRegEx": "RJ.", "year": 1987}, {"title": "An Introduction to Computational Geometry", "author": ["M Minsky", "S. Papert"], "venue": "MIT Press.", "citeRegEx": "Minsky and Papert,? 1969", "shortCiteRegEx": "Minsky and Papert", "year": 1969}, {"title": "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images", "author": ["A Nguyen", "J Yosinski", "J. Clune"], "venue": "arXiv:1412.1897v2 [cs.CV] 18 Dec 2014", "citeRegEx": "Nguyen et al\\.,? 2014", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "A Stochastic Approximation Method", "author": ["H Robbins", "S. Monro"], "venue": "The Annals of Mathematical Statistics , Vol. 22, No. 3. (Sep.), pp. 400-407", "citeRegEx": "Robbins and Monro,? 1951", "shortCiteRegEx": "Robbins and Monro", "year": 1951}, {"title": "Learning representations by back-propagating errors", "author": ["DE Rumelhart", "GE Hinton", "RJ. Williams"], "venue": "Nature 323, 533 - 536 (09 October)", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "The Perceptron - a perceiving and recognizing automaton", "author": ["F. Rosenblatt"], "venue": "Report 85-460-1, Cornell Aeronautical Laboratory", "citeRegEx": "Rosenblatt,? 1957", "shortCiteRegEx": "Rosenblatt", "year": 1957}, {"title": "Solving graph algorithms with networks of spiking neurons", "author": ["Sala DM", "Cios KJ."], "venue": "IEEE Transactions on Neural Networks, 10(4):953-957", "citeRegEx": "DM and KJ.,? 1999", "shortCiteRegEx": "DM and KJ.", "year": 1999}, {"title": "Recognition of Partially Occluded and Rotated Images with a Network of Spiking Neurons", "author": ["JH Shin", "D Smith", "W Swiercz", "K Staley", "T Rickard", "J Montero", "L Kurgan", "KJ. Cios"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Shin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shin et al\\.", "year": 2010}, {"title": "Competitive Hebbian Learning Through Spike Timing-Dependent Synaptic Plasticity", "author": ["S. Song", "K.D. Miller", "Abbot L.F"], "venue": "Nature Neuroscience,", "citeRegEx": "Song et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Song et al\\.", "year": 2000}, {"title": "Simulating vertical and horizontal inhibition with short term dynamics in a multi-column multi-layer model of Neocortex", "author": ["B Strack", "K Jacobs", "KJ. Cios"], "venue": "International Journal of Neural Systems, 24(5): 1440002 (2014) [19 pages]", "citeRegEx": "Strack et al\\.,? 2014", "shortCiteRegEx": "Strack et al\\.", "year": 2014}, {"title": "New synaptic plasticity rule for networks of spiking neurons", "author": ["W. Swiercz", "K.J. Cios", "K Staley"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Swiercz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Swiercz et al\\.", "year": 2006}, {"title": "Intriguing properties of neural networks\u201d, Int", "author": ["C Szeged", "W Zaremba", "I Sutskever", "J Bruna", "D Erhan", "D Goodfellow", "R. Fergus"], "venue": "Conf. on Learning Representations.", "citeRegEx": "Szeged et al\\.,? 2014", "shortCiteRegEx": "Szeged et al\\.", "year": 2014}, {"title": "Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences", "author": ["P. Werbos"], "venue": "PhD thesis, Harvard University", "citeRegEx": "Werbos,? 1974", "shortCiteRegEx": "Werbos", "year": 1974}], "referenceMentions": [{"referenceID": 5, "context": "The first spiking neuron model was developed by Hodgkin and Huxley (1952), for which they later received a Nobel Prize.", "startOffset": 48, "endOffset": 74}, {"referenceID": 7, "context": "The simplest spiking neuron model was developed by Izhikevich (2003). It does not model any of the biological neuron functions except that it accurately mimics several types /shapes of the postsynaptic potentials generated by human brain neurons.", "startOffset": 51, "endOffset": 69}, {"referenceID": 7, "context": "A) unbounded firing of the original Izhikevich model neurons; B) firing of the neurons after Strack at el. (2014) modification accounting for absolute refractory periods.", "startOffset": 36, "endOffset": 114}, {"referenceID": 10, "context": "The credit for the above observation most often is given to Hebb (Hebb, 1949) although Konorski published it a year earlier (Konorski, 1948).", "startOffset": 124, "endOffset": 140}, {"referenceID": 25, "context": ", equations corresponding to the above observation were specified much later by computational scientists (Song et al., 2000; Swiercz et al., 2006).", "startOffset": 105, "endOffset": 146}, {"referenceID": 27, "context": ", equations corresponding to the above observation were specified much later by computational scientists (Song et al., 2000; Swiercz et al., 2006).", "startOffset": 105, "endOffset": 146}, {"referenceID": 10, "context": "The credit for the above observation most often is given to Hebb (Hebb, 1949) although Konorski published it a year earlier (Konorski, 1948). The practical learning rules, i.e., equations corresponding to the above observation were specified much later by computational scientists (Song et al., 2000; Swiercz et al., 2006). Similar case, of not giving credit to the original inventor, involves a popular backpropagation learning rule that was first specified by statisticians Robbins and Monroe (1951): they called it a stochastic approximation method.", "startOffset": 87, "endOffset": 502}, {"referenceID": 10, "context": "The credit for the above observation most often is given to Hebb (Hebb, 1949) although Konorski published it a year earlier (Konorski, 1948). The practical learning rules, i.e., equations corresponding to the above observation were specified much later by computational scientists (Song et al., 2000; Swiercz et al., 2006). Similar case, of not giving credit to the original inventor, involves a popular backpropagation learning rule that was first specified by statisticians Robbins and Monroe (1951): they called it a stochastic approximation method. However, the credit for the rule in neural networks literature was given to Rumelhart, Hinton and Williams (1986) before it was found that Werbos (1974) specified the rule, also for neural networks, a dozen years before them.", "startOffset": 87, "endOffset": 667}, {"referenceID": 10, "context": "The credit for the above observation most often is given to Hebb (Hebb, 1949) although Konorski published it a year earlier (Konorski, 1948). The practical learning rules, i.e., equations corresponding to the above observation were specified much later by computational scientists (Song et al., 2000; Swiercz et al., 2006). Similar case, of not giving credit to the original inventor, involves a popular backpropagation learning rule that was first specified by statisticians Robbins and Monroe (1951): they called it a stochastic approximation method. However, the credit for the rule in neural networks literature was given to Rumelhart, Hinton and Williams (1986) before it was found that Werbos (1974) specified the rule, also for neural networks, a dozen years before them.", "startOffset": 87, "endOffset": 706}, {"referenceID": 21, "context": "The simplest learning rule, called Perceptron, for one-layer feed-forward neural networks, was defined by Rosenblatt (1957). Backpropagation rule is in fact the Perceptron\u2019s rule extension to many-layer networks.", "startOffset": 106, "endOffset": 124}, {"referenceID": 18, "context": "This seemingly small change led to an explosion in neural networks research that stagnated for almost 20 years after Minsky and Papert (1969) stated that neural networks were useless for solving complex problems.", "startOffset": 117, "endOffset": 142}, {"referenceID": 2, "context": "The first researcher to design a direct precursor of DNN, using Hubel and Wiesel\u2019s discoveries, was Fukushima (1980) who called his network Neocognitron.", "startOffset": 100, "endOffset": 117}, {"referenceID": 1, "context": "Instead of describing Neocognitron or convolutional neural network of LeCun for which many excellent online resources exist, we describe below a network called IRNN (Image Recognition Neural Network), which was inspired by the works of Hubel and Wiesel and Fukushima (Cios and Shin, 1995).", "startOffset": 267, "endOffset": 288}, {"referenceID": 1, "context": "We see there three (hashed) subimages/windows of the three input images, which are clustered using a novel image similarity measure (Cios and Shin, 1995).", "startOffset": 132, "endOffset": 153}, {"referenceID": 24, "context": "But is it possible to perform deep learning using networks of spiking neurons? Shin et al. (2010) used such a network for face recognition, without any preprocessing of the images.", "startOffset": 79, "endOffset": 98}, {"referenceID": 24, "context": "But is it possible to perform deep learning using networks of spiking neurons? Shin et al. (2010) used such a network for face recognition, without any preprocessing of the images. The network self-organizes at each level of its hierarchical processing. Even at the output layer spiking neurons are used for labeling faces, in contrast to more popular use of supervised methods such as backpropagation; similar approach was later used in Cao, et.al. (2014). Specifically, the spiking neuron model used was McGregor\u2019s with SAPR and STDP learning rules for self-organization of the neurons; self-organization in essence being a clustering operation.", "startOffset": 79, "endOffset": 457}, {"referenceID": 28, "context": "However, when the latter was input to the AlexNet (open source implementation of a convolutional neural network), which was trained on the original image of the dog, it failed to recognize it (Szeged et al., 2014).", "startOffset": 192, "endOffset": 213}, {"referenceID": 12, "context": "I think it is increasingly more important for the computational researchers to team up with neuroscientists to come up with better algorithms for image recognition so that the algorithms cannot be so easily fooled (Lim et al., 2011).", "startOffset": 214, "endOffset": 232}], "year": 2017, "abstractText": "We shall illustrate how DNN work by the use of an example from the area of face recognition. There, the inputs are images from which at the first level (first hidden layer) of processing simple image characteristics such as edges are extracted. At the second and subsequent levels, more complex parts of an image are formed to finally, at the output layer, recognize human faces. This is in contrast to using a traditional approach where in the first step, known as preprocessing, an expert guides the process of extracting key features, and then they are used for recognizing faces. The common part of these two, very different, approaches is that at the output layer the labeled data are needed to perform supervised learning, i.e., assigning names/labels to faces.", "creator": "Acrobat PDFMaker 11 for Word"}}}