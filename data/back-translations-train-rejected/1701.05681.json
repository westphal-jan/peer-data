{"id": "1701.05681", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jan-2017", "title": "Git Blame Who?: Stylistic Authorship Attribution of Small, Incomplete Source Code Fragments", "abstract": "Program authorship attribution has implications for the privacy of programmers who wish to contribute code anonymously. While previous work has shown that complete files that are individually authored can be attributed, we show here for the first time that accounts belonging to open source contributors containing short, incomplete, and typically uncompilable fragments can also be effectively attributed.", "histories": [["v1", "Fri, 20 Jan 2017 04:17:30 GMT  (589kb,D)", "https://arxiv.org/abs/1701.05681v1", null], ["v2", "Thu, 16 Mar 2017 22:18:24 GMT  (589kb,D)", "http://arxiv.org/abs/1701.05681v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["edwin dauber", "aylin caliskan", "richard harang", "rachel greenstadt"], "accepted": false, "id": "1701.05681"}, "pdf": {"name": "1701.05681.pdf", "metadata": {"source": "CRF", "title": "Git Blame Who?: Stylistic Authorship Attribution of Small, Incomplete Source Code Fragments", "authors": ["Edwin Dauber", "Aylin Caliskan", "Richard Harang"], "emails": [], "sections": [{"heading": null, "text": "We propose a technique for assigning the authorship of contribution accounts that contain small samples of source code, such as can be obtained from version control systems or other direct comparisons of sequential versions. We show that applying previous methods to individual small samples of source code yields an accuracy of about 73% for 106 programmers as a baseline by recreating and averaging the classification probabilities of a sufficiently large set of samples belonging to the same author. In addition, we propose the use of calibration curves to identify samples from unknown and previously unencountered authors in the open world. We show that we can also use these calibration curves if we do not have linkage information and are therefore forced to classify individual samples directly because the calibration curves allow us to identify the samples that are most likely to avoid a correction of costs."}, {"heading": "1. Introduction", "text": "This year, it is time for us to set out to find a solution that paves the way to the future."}, {"heading": "2. Related Work", "text": "We observe two main categories of related work: we refer back to previous work in the field of assignment of text authors, and then we look at some critical work in the field of assignment of text authors. Although the two areas have developed different characteristics and classification techniques, recent work in the field of assignment of text authors is closely related to our work."}, {"heading": "2.1. Source Code Authorship Attribution", "text": "In fact, we are able to go in search of a solution that meets the needs of the individual."}, {"heading": "2.2. Text Authorship Attribution", "text": "The primary piece of related research in the domain of text authoring assignment is the work of Overdorf and Greenstadt on cross-domain authorship assignment [22]. This work links authorship between blogs, tweets and Reddit comments. This work is related to us in two ways. First, and most obviously, they work with short texts in the forms of tweets and Reddit comments. For these areas, they use a technique of merging text before extracting functions. We propose a similar merge technology, although we come together after extracting functions. More importantly, they also use a method of averaging probabilities for multiple samples to classify collections of samples. We show that this technique is similarly applicable in the source code domain, and that we achieve excellent results even though they are after extracting features earlier than before."}, {"heading": "3. Methodology", "text": "Our method is summarized in Figure 1. We start by collecting C + + repositories on GitHub, and then split collaborative files from those repositories into smaller pieces using git blame, as described in Section 3.2. For each piece of code, we extract the abstract syntax tree and use it to extract a feature vector, as described in Section 3.3. Then we proceed to perform the assignment of each sample to a random forest, as described in Section 3.4. Then we intersect the classification probability of linked samples as described in Section 3.5, and construct a calibration curve as described in Section 3.6."}, {"heading": "3.1. Problem Statement", "text": "In this work, we consider ourselves to be in the role of the analyst trying to map source code and break anonymity. We assume that the jointly written code we are investigating has been pre-segmented by the author. This segmentation may come from version control obligations, git blame, or some other method of decomposition; we only assume that we have small samples that can reasonably be attributed to an individual person. We also assume that we have training data that consists of similarly segmented code samples from our suspect programmers, rather than complete files. Note that this later assumption does not particularly restrict us in practice, because we can artificially segment a single authorized file when necessary. In our primary case, we assume that our segmented samples are linked to each other by the segmentation method. By version control methods, this would correspond to accounts, while by other methods it may coincide with clusters that we are generating a cluster algorithm rather than a cluster algorithm."}, {"heading": "3.2. Data Preparation", "text": "We collected data from public C + + repositories on GitHub. We collected data from 1,649 repositories and 1,178 programmers that listed C + + as the primary language, starting with 14 seed contributors and spiders through their collaborators. In the process, we collected data from 1,649 repositories and 1,178 programmers, although we found in future processing steps that many of these programmers and repositories had insufficient data. In addition, some of these programmers were renamed or group accounts, while some repositories contained non-C + codes that needed to be discounted. After eliminating this and setting the threshold to at least 150 samples per author with at least one line of actual code (not whitespace or comments), we were left with 106 programmers that selected text other than C + codes that needed to be discounted."}, {"heading": "3.3. Features", "text": "In this thesis, we use a set of functions derived from the work of Caliskan-Islam et al. [9]. Our primary features come from the abstract syntax tree and include nodes and node bigrams [3]. The abstract syntax tree, or AST, is a tree representation of the source code with nodes representing syntactic constructs in the code, and we used the blurred parser Joern to extract them [28]. This parsing allows us to generate the AST without a full build and without the full build environment. Therefore, we can even extract the AST for our partial code samples. We also include word unigrams, API symbols and keywords. Our set of functions includes both raw and TFIDF versions of many of our features. TFIDF is a measure that combines the raw frequency with a measure of how many authors use the feature. Due to the frugality of the feature set, we do not use the Sample 61 as a whole, because we do not use the Sample 471 for all of the information we have."}, {"heading": "3.4. Single Sample Attribution", "text": "In this case, we assume that we do not have information about the authorship of the samples we are trying to classify. As a use case, we can only imagine the case where the sample is attributed to an anonymous user, not to an account, or when a cautious person creates a new account for each commit. Therefore, we can only classify at the level of each sample. To do this, we perform cross-validations with random forests, as in [9]. Random forests are multi-group multi-class classifiers that combine multiple decision trees to vote on which class to assign to an instance [5]. This serves as a starting point for our work. Ideally, we would then continue as in Section 3.5 for multiple sample assignment. If we cannot, however, we use a technique to help analysts better interpret the results. As we suspect that some samples will remain difficult, if not impossible to classify, we want to know at what level of confidentiality we can select the class to say."}, {"heading": "3.5. Multiple Sample Attribution", "text": "In this case, we assume that we were able to group the samples as in a billing and identify the owner. Therefore, we can use the group of samples to identify the author of all the samples. To do this, we have essentially found two ways: These experiments have as parameters the number of combined samples and the number of cross-validation folds, and we have ensured that the product of the two is always either the total number of samples per author or a divider of the same. Our first method is the merging of the samples. To do this, we added the characteristic vectors of the samples together. We have tried to merge both in the extracted (alphabetical by sample name) order and in random order. By merging in the extracted order we retain the code locality, while merging in the random order we distribute the distribution code across combined groups. We refer to the merging, the rectification of the locality as we arrange and the second locality of the samples, and the merger is the only way of the merging of the combined group."}, {"heading": "3.6. The Open World", "text": "For the open world, we propose a variation of the calibration curves described in Section 3.4. We perform the mapping as normal according to either individual sample assignments or account assignments, and our goal is to use a threshold to separate samples from unknown authors attributed to a suspect due to the mechanics of the classifier from samples correctly assigned to one of our suspicions.For the purposes of our experiments, we used our 15 programmer data sets. We conducted an initial proof-of-concept experiment with a small set of unknown authorized samples and 250 samples of training data per known programmer. We then conducted experiments in the following manner. We divided our 15 programmer data sets into three disjointed subsets of five programmers. We conducted three rounds of experiments, and for each round, we took one of the subsets as a set of unknown authors U. Then we performed an 11-fold cross-validation on the remaining data by adding all the evaluation documents."}, {"heading": "3.7. Dataset Size Factors", "text": "It is well known that the amount of available training data and the number of suspects have an important influence on classification accuracy. In our problem, we have two ways to vary the amount of available training data: one is to vary the minimum number of code lines per sample, and the other is to vary the number of samples used per programmer. To observe these effects, we performed a single sample assignment to numerous different subsets of our larger data set, obtained by layered random strains of the author, and experimented with different levels of merging for specific subsets. To investigate the effects of the data set size more closely, we used the 15-author data set and limited ourselves to samples that are one line long. We then varied the number of samples per author from 10 to 130 in decimal steps and used 10-fold cross-validations."}, {"heading": "3.8. Special Case Attribution Tasks", "text": "The first task is a two-tier assignment scenario, where we have two people claiming authorship and we want to determine who wrote the code. The second task is verification, where we have a suspect and want to know if that person wrote the code or not. We use our 15-class record for both. For the two-class scenario, we check each pair of programmers for individual example assignments. For verification, we create an additional \"author\" for each programmer from samples of the remaining 14. Then we perform a two-class assignment task for individual sample authors and the collected 14. For both experiments, we have used an 11-fold cross-validation."}, {"heading": "3.9. Ground Truth Considerations", "text": "We recognize that the basic truth in the GitHub environment is not perfect. First, we know that it is possible that the code is copied from other sources and therefore not the original contribution of the credited author. Furthermore, the use of Git blame at the line level shows that the last author touched the line, not the original author of all parts of the line. We note that despite the inherent \"clutter\" of the data, it is important to evaluate the data collected from the wilderness, which reflects a realistic application of our techniques well. Therefore, we developed an experiment to try to quantify the contribution of the inaccurate basic truth. For this experiment, we perform cross-validation on Google Code Jam data, as used by [9] with 250 authors and 9 files each. We then set a parameter for the number of basic truth corruptions to be executed. Any corruption is an author exchange between two files. We use Google Code Jam data as we would know the basic truth, because we can already verify the amount of the data used in the Hub, and we can verify it."}, {"heading": "4. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Single Sample Attribution", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "4.2. Multiple Sample Attribution", "text": "This year, the time has come for us to be able to go in search of a solution that is capable, that we are able to find a solution, that we are able to find, that we are able to find, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution. \""}, {"heading": "4.3. The Open World", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "4.4. Dataset Size Factors", "text": "To test the effect of the amount of training data, we experiment with 90 samples of at least five lines of code per author for 50 authors, cross-validating with both 10 folds and 30 folds, with 500 trees for each experiment. In the 10-fold case, each experiment is trained on six fewer samples than in the 30-fold case. We get 74% accuracy for 10-fold cross-validation and 81% accuracy for 30-fold cross-validation. We conclude that the six training files difference leads to an increase in classification accuracy of 7%. We also use three stages of matching to test the effect of the merged samples and the number of training samples available. Our best accuracy with the merge technique, 83%, comes from merging 10 samples and performing 9-fold cross-validation. If we merge more samples, we have insufficient training data and thus have worse results than performing 6-fold cross-validation gives us 81% accuracy."}, {"heading": "4.5. Special Case Attribution Tasks", "text": "Table 5 shows the accuracy results of our special case mapping tasks, represented by average accuracy, min-TABLE 5. SPECIAL CASE ATTRIBUTION TASKSAttribution Task Average Minimum Maximum Two-Class Attribution 91.8% 77.1% 96.6% Verification 88.1% 79.9% 96.5% Figure 15. These are the results for error corruption in the Google Code Jam records. Minimum accuracy and maximum accuracy across all 15 programmers for verification and all pairs for two-class mapping. We see that these tasks are easier than the mapping task in a sample. The variability also gives an insight into our previous accuracy results. By spreading the accuracy, we can see that some programmers are harder to map than others."}, {"heading": "4.6. Ground Truth Corruption", "text": "Figure 15 shows the accuracy for different levels of soil truth corruption for different percentages of corrupt labels in the Google Code Jam dataset. We observe that the level of decrease in accuracy is close to the level of false soil truth labels for relatively small amounts of corruption. Therefore, we conclude that individual faulty labels have minimal impact on the overall quality of the classifier, and that it would require serious systemic soil truth problems to cause extreme classification problems. These results are not surprising since random forests are known to be resilient to mislabeled data due to the use of bootstrap samples, resulting in each sample affecting only part of the trees in the overall classifier."}, {"heading": "5. Discussion", "text": "From these results, we conclude that a major difficulty in assigning source code samples to Git blame level is the high variance within the author of such samples: Short code segments, which can be distributed across a number of tasks, contain limited and in many cases ambiguous stylistic information. Variance reduction techniques are required to make strong predictions from such data. All of our techniques for simultaneously classifying multiple samples reduce the variance in our classification set, and thus increase our accuracy. We also point out that it is possible that our techniques reduce the impact of basic truth problems. By aggregating samples, either at trait vector level or at the time of classification, we can reduce the impact of individual samples with poor basic truthfulness, provided that such samples are relatively rare."}, {"heading": "5.1. Poorly correlated test examples improve ensemble accuracy", "text": "It has long been known that a simple averaging of probable predictors yields an ensemble classifier that has significantly improved generalization performance and error resistance over individual components [18]. This improvement has also long been known to be inversely related to the degree of correlation between the predictions of classifiers using a single example [27]. The standard approach to averaging considers an ensemble of learners h1,.. hT, and takes the total classification of a single sample x as: H (x) = 1T T \u2211 i = 1 hi (x). We are investigating an interesting variation on this problem in which, instead of subjecting a single test sample to a diverse set of classifiers, we present a diverse collection of test samples known to share the same unknown designation x (i) 1, x (i) 2."}, {"heading": "5.2. Merging test samples reduces feature variance", "text": "In fact, most of us are able to go in search of a solution that originates in the real world."}, {"heading": "6. Future Work", "text": "We assume that we know that the correct programmer is one of our suspects, and that we have segmentation in the form of either commits or Git-blame. We've made progress in eliminating these assumptions, and we want to continue to do so. In this work, we've already presented a way to remove the closed world, but hard to trust, attributions in the case of each example assignment, and some of them are responsible for the account mapping. While this technique has a simple approach to removing most samples obtained from the world, it is difficult to find attributions in relation to each example mapping, and some are responsible for the account mapping."}, {"heading": "7. Conclusion", "text": "This year it has come to the point that we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in a time in which we are in which we are in a time."}], "references": [{"title": "Compilers, Principles, Techniques", "author": ["A.V. Aho", "R. Sethi", "J.D. Ullman"], "venue": "Addison wesley,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1986}, {"title": "A generic unsupervised method for decomposing multi-author documents", "author": ["N. Akiva", "M. Koppel"], "venue": "Journal of the American Society for Information Science and Technology, 64(11):2256\u20132264,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Source code authorship attribution", "author": ["S. Burrows"], "venue": "PhD thesis, RMIT University, nov", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Source code authorship attribution using n-grams", "author": ["S. Burrows", "S.M. Tahaghoghi"], "venue": "Proceedings of the Twelth Australasian Document Computing Symposium, Melbourne, Australia, RMIT University, pages 32\u201339. Citeseer,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Application of information retrieval techniques for source code authorship attribution", "author": ["S. Burrows", "A.L. Uitdenbogerd", "A. Turpin"], "venue": "Database Systems for Advanced Applications, pages 699\u2013713. Springer,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "De-anonymizing programmers via code stylometry", "author": ["A. Caliskan-Islam", "R. Harang", "A. Liu", "A. Narayanan", "C. Voss", "F. Yamaguchi", "R. Greenstadt"], "venue": "24th USENIX Security Symposium (USENIX Security 15), pages 255\u2013270,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Extraction of java program fingerprints for software authorship identification", "author": ["H. Ding", "M.H. Samadzadeh"], "venue": "Journal of Systems and Software, 72(1):49\u201357,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Unsupervised authorship attribution", "author": ["D. Fifield", "T. Follan", "E. Lunde"], "venue": "arXiv preprint arXiv:1503.07613,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Examining the significance of high-level programming features in source code author classification", "author": ["G. Frantzeskou", "S. MacDonell", "E. Stamatatos", "S. Gritzalis"], "venue": "Journal of Systems and Software, 81(3):447\u2013460,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Identifying authorship by byte-level n-grams: The source code author profile (scap) method", "author": ["G. Frantzeskou", "E. Stamatatos", "S. Gritzalis", "C.E. Chaski", "B.S. Howald"], "venue": "International Journal of Digital Evidence, 6(1):1\u201318,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Effective identification of source code authors using byte-level information", "author": ["G. Frantzeskou", "E. Stamatatos", "S. Gritzalis", "S. Katsikas"], "venue": "Proceedings of the 28th international conference on Software engineering, pages 893\u2013896. ACM,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Recognizing authors: an examination of the consistent programmer hypothesis", "author": ["J.H. Hayes", "J. Offutt"], "venue": "Software Testing, Verification and Reliability, 20(4):329\u2013356,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Unsupervised decomposition of a document into authorial components", "author": ["M. Koppel", "N. Akiva", "I. Dershowitz", "N. Dershowitz"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 1356\u20131364. Association for Computational Linguistics,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Synergy of clustering multiple back propagation networks", "author": ["W.P. Lincoln", "J. Skrzypek"], "venue": "Advances in neural information processing systems, pages 650\u2013657,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1990}, {"title": "Software forensics for discriminating between program authors using case-based reasoning, feedforward neural networks and multiple discriminant analysis", "author": ["S.G. MacDonell", "A.R. Gray", "G. MacLennan", "P.J. Sallis"], "venue": "Neural Information Processing, 1999. Proceedings. ICONIP\u201999. 6th International Conference on, volume 1, pages 66\u201371. IEEE,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Mining software repositories for accurate authorship", "author": ["X. Meng", "B.P. Miller", "W.R. Williams", "A.R. Bernat"], "venue": "2013 IEEE International Conference on Software Maintenance, pages 250\u2013259. IEEE,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Predicting good probabilities with supervised learning", "author": ["A. Niculescu-Mizil", "R. Caruana"], "venue": "Proceedings of the 22nd international conference on Machine learning, pages 625\u2013632. ACM,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Blogs, twitter feeds, and reddit comments: Cross-domain authorship attribution", "author": ["R. Overdorf", "R. Greenstadt"], "venue": "PoPETs, 2016(3):155\u2013 171,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Using classification techniques to determine source code authorship", "author": ["B.N. Pellin"], "venue": "White Paper: Department of Computer Science, University of Wisconsin,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine learning, 1(1):81\u2013 106,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1986}, {"title": "Software forensics: Can we track code to its authors", "author": ["E.H. Spafford", "S.A. Weeber"], "venue": "Computers & Security,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1993}, {"title": "Classify, but verify: Breaking the closed-world assumption in stylometric authorship attribution", "author": ["A. Stolerman", "R. Overdorf", "S. Afroz", "R. Greenstadt"], "venue": "IFIP Working Group, volume 11,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Error correlation and error reduction in ensemble classifiers", "author": ["K. Tumer", "J. Ghosh"], "venue": "Connection science, 8(3-4):385\u2013404,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Modeling and discovering vulnerabilities with code property graphs", "author": ["F. Yamaguchi", "N. Golde", "D. Arp", "K. Rieck"], "venue": "Proc. of IEEE Symposium on Security and Privacy (S&P),", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Twitter feed: \u201cI just heard from an intern at Apple that they disallow her from contributing to open source on her own time. That\u2019s illegal, right?", "author": ["zooko (@zooko"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "Recently, the legality and reality of this situation has been a topic of discussion and debate on Twitter [29].", "startOffset": 106, "endOffset": 110}, {"referenceID": 17, "context": "We construct calibration curves to indicate the accuracy for collections which were attributed with given confidence, and analysts can use these curves to set a threshold below which to more carefully examine authorship due to higher probability of being outside of the suspect set [21].", "startOffset": 282, "endOffset": 286}, {"referenceID": 3, "context": "Previous work has attributed authorship to whole code files collected either from small suspect sets or from datasets which are near to laboratory condition, such as single authored code submitted to the Google Code Jam [2], [6].", "startOffset": 225, "endOffset": 228}, {"referenceID": 6, "context": "using random forests to perform authorship attribution of Google Code Jam submissions, with features extracted from the abstract syntax tree [9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 9, "context": "used byte level n-grams to achieve high accuracy with small suspect sets [13], [14], [15].", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "used byte level n-grams to achieve high accuracy with small suspect sets [13], [14], [15].", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "used byte level n-grams to achieve high accuracy with small suspect sets [13], [14], [15].", "startOffset": 85, "endOffset": 89}, {"referenceID": 19, "context": "The use of abstract syntax trees for authorship attribution was pioneered by Pellin and used on pairs of Java programs in order to ensure that the studied programs had the same functionality [23].", "startOffset": 191, "endOffset": 195}, {"referenceID": 7, "context": "Ding and Samadzadeh studied a set of 46 programmers and Java using statistical methods [10].", "startOffset": 87, "endOffset": 91}, {"referenceID": 15, "context": "analyzed C++ code from a set of 7 professional programmers using neural networks, multiple discriminant analysis, and case-based reasoning [19].", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": "proposed techniques that achieved high accuracy for small suspect sets, but had poor scalability [6], [7], [8] Spafford and Weeber were among the first to suggest performing authorship attribution on source code [25].", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "proposed techniques that achieved high accuracy for small suspect sets, but had poor scalability [6], [7], [8] Spafford and Weeber were among the first to suggest performing authorship attribution on source code [25].", "startOffset": 102, "endOffset": 105}, {"referenceID": 5, "context": "proposed techniques that achieved high accuracy for small suspect sets, but had poor scalability [6], [7], [8] Spafford and Weeber were among the first to suggest performing authorship attribution on source code [25].", "startOffset": 107, "endOffset": 110}, {"referenceID": 21, "context": "proposed techniques that achieved high accuracy for small suspect sets, but had poor scalability [6], [7], [8] Spafford and Weeber were among the first to suggest performing authorship attribution on source code [25].", "startOffset": 212, "endOffset": 216}, {"referenceID": 12, "context": "Hayes and Offutt performed a manual statistical analysis of 5 professional programmers and 15 graduate students, and found that programmers do have distinguishable styles which they use consistently [16].", "startOffset": 199, "endOffset": 203}, {"referenceID": 16, "context": "proposed a tool called git-author which assigns weighted values to contributions in order to better represent the evolution of a line of code [20].", "startOffset": 142, "endOffset": 146}, {"referenceID": 18, "context": "The primary piece of related research in the domain of text authorship attribution is the work by Overdorf and Greenstadt in cross-domain authorship attribution [22].", "startOffset": 161, "endOffset": 165}, {"referenceID": 1, "context": "on decomposition of artificially combined biblical books [4], [17].", "startOffset": 57, "endOffset": 60}, {"referenceID": 13, "context": "on decomposition of artificially combined biblical books [4], [17].", "startOffset": 62, "endOffset": 66}, {"referenceID": 8, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "We then used git blame on each line of code, and for each set of consecutive lines blamed to the same programmer we encapsulated those lines in a dummy main function and extracted features from the abstract syntax tree as in [9].", "startOffset": 225, "endOffset": 228}, {"referenceID": 6, "context": "However, unlike in [9] we cannot use information gain to prune the feature set due to having extremely sparse feature vectors and therefore very few features with individual information gain.", "startOffset": 19, "endOffset": 22}, {"referenceID": 6, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Our primary features come from the abstract syntax tree and include nodes and node bigrams [3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 24, "context": "The abstract syntax tree, or AST, is a tree representation of source code with nodes representing syntactic constructs in the code, and we used the fuzzy parser joern to extract them [28].", "startOffset": 183, "endOffset": 187}, {"referenceID": 20, "context": "Due to the sparsity of the feature set, we do not use information gain to prune the set, and instead keep the entire feature set, minus any features which are constant across all samples [24].", "startOffset": 187, "endOffset": 191}, {"referenceID": 6, "context": "For this, we perform cross-validation with random forests, as in [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "Random forests are ensemble multi-class classifiers which combine multiple decision trees which vote on which class to assign to an instance [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "For this experiment, we perform cross validation on Google Code Jam data as is used by [9] with 250 authors and 9 files each.", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "We note that while this is much lower than the accuracies reported by [9], the data itself is very different.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "The work of [9] attributes whole source code files written privately with an average of 70 lines of code per file, while our work attributes pieces of files written publicly and collaboratively with an average of 4.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "As we know from [9], word unigrams provide less information than abstract syntax tree nodes.", "startOffset": 16, "endOffset": 19}, {"referenceID": 14, "context": "Simple averaging of probabilistic predictors has long been known to yield an ensemble classifier that has significantly improved generalization performance and robustness to error over any individual component [18].", "startOffset": 210, "endOffset": 214}, {"referenceID": 23, "context": "This improvement has also long been known to be inversely related to the degree of correlation between the predictions of the classifiers on a single example [27].", "startOffset": 158, "endOffset": 162}], "year": 2017, "abstractText": "Program authorship attribution has implications for the privacy of programmers who wish to contribute code anonymously. While previous work has shown that complete files that are individually authored can be attributed, we show here for the first time that accounts belonging to open source contributors containing short, incomplete, and typically uncompilable fragments can also be effectively attributed. We propose a technique for authorship attribution of contributor accounts containing small source code samples, such as those that can be obtained from version control systems or other direct comparison of sequential versions. We show that while application of previous methods to individual small source code samples yields an accuracy of about 73% for 106 programmers as a baseline, by ensembling and averaging the classification probabilities of a sufficiently large set of samples belonging to the same author we achieve 99% accuracy for assigning the set of samples to the correct author. Through these results, we demonstrate that attribution is an important threat to privacy for programmers even in real-world collaborative environments such as GitHub. Additionally, we propose the use of calibration curves to identify samples by unknown and previously unencountered authors in the open world setting. We show that we can also use these calibration curves in the case that we do not have linking information and thus are forced to classify individual samples directly. This is because the calibration curves allow us to identify which samples are more likely to have been correctly attributed. Using such a curve can help an analyst choose a cut-off point which will prevent most misclassifications, at the cost of causing the rejection of some of the more dubious correct attributions.", "creator": "LaTeX with hyperref package"}}}