{"id": "1701.05779", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jan-2017", "title": "Empirical Study of Drone Sound Detection in Real-Life Environment with Deep Neural Networks", "abstract": "This work aims to investigate the use of deep neural network to detect commercial hobby drones in real-life environments by analyzing their sound data. The purpose of work is to contribute to a system for detecting drones used for malicious purposes, such as for terrorism. Specifically, we present a method capable of detecting the presence of commercial hobby drones as a binary classification problem based on sound event detection. We recorded the sound produced by a few popular commercial hobby drones, and then augmented this data with diverse environmental sound data to remedy the scarcity of drone sound data in diverse environments. We investigated the effectiveness of state-of-the-art event sound classification methods, i.e., a Gaussian Mixture Model (GMM), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), for drone sound detection. Our empirical results, which were obtained with a testing dataset collected on an urban street, confirmed the effectiveness of these models for operating in a real environment. In summary, our RNN models showed the best detection performance with an F-Score of 0.8009 with 240 ms of input audio with a short processing time, indicating their applicability to real-time detection systems.", "histories": [["v1", "Fri, 20 Jan 2017 12:48:02 GMT  (542kb,D)", "http://arxiv.org/abs/1701.05779v1", "IEEE 5 Pages, Submitted"]], "COMMENTS": "IEEE 5 Pages, Submitted", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["sungho jeon", "jong-woo shin", "young-jun lee", "woong-hee kim", "younghyoun kwon", "hae-yong yang"], "accepted": false, "id": "1701.05779"}, "pdf": {"name": "1701.05779.pdf", "metadata": {"source": "CRF", "title": "Empirical Study of Drone Sound Detection in Real-Life Environment with Deep Neural Networks", "authors": ["Sungho Jeon", "Jong-Woo Shin", "Young-Jun Lee", "Woong-Hee Kim", "YoungHyoun Kwon", "Hae-Yong Yang"], "emails": ["formant}@nsr.re.kr", "sdeva14@gmail.com"], "sections": [{"heading": null, "text": "I. INTRODUCTIONMotivation. Popularization of commercial hobby drones brings unexpected threats to the environment in which we live, such as terror to people or important facilities. A common four-propeller drone is suitable to enjoy as a hobby and for broadcasting, but at the same time it makes existing defense systems appear surprisingly outdated. Some accidents have already proven that these drones can easily penetrate the highest level of security systems, such as landing in front of the Prime Minister of Germany, on the roof of the official residence of the Prime Minister of Japan, and in the White House in the United States. Thus, the ability to detect the occurrence of a drone is a matter of the highest priority to prevent threats. Although few studies deal with the problem of drone sound detection, the previous work has been carried out in isolated or quiet locations, rather than in a real environment without the polyphonic sonic environment."}, {"heading": "II. METHOD", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Data Augmentation", "text": "Especially in real environments, invisible event sound has a detrimental effect on the deterioration of the detection rate.ar Xiv: 170 1.05 779v 1 [cs.S D] January 20, 2017The biggest difficulty in this work is the lack of public drone noise data for training. Although the supply of commercial hobby drones is available, drone noise collection in various environments is limited as drone flying is restricted in most public or residential areas. The purpose of this extension is to generate drone noise combined with realistic noise data by adding various real environmental sounds from a public dataset [12], [13] and our collection. Drone noise was collected in a quiet location outside. Purpose of this extension is to generate drone noise combined with realistic noise data while preserving the characteristics of drone noise. Data augmentation included an enhancement of drone noise to emphasize the background noise by 5%."}, {"heading": "B. Feature: MFCC and Mel-spectrogram", "text": "In many previous ESC studies, it is known that MFCC has outstanding features for classifiers. MFCC also has useful features for recording the periodicity of the fundamental frequencies caused by the rotor blades of a drone. Our recorded drone sound indicated a noticeable harmonic shape below a frequency of 1500 Hz. In addition, we also observed a noticeable range of influence on the spectrogram between 5000 Hz and 7000 Hz (Figure 1), as previously explained [1], [2]. However, these characteristics were not demonstrated by all drone models used in our experiments. Furthermore, low frequency data is transmitted further in terms of its energy than high frequency data. Therefore, we focus only on low frequency data below 1500 Hz. The other important consideration for feature engineering is the length of instantaneous input data to the model. The minimum length of audio data converted into an MFCC vector is higher than high frequency data, and the best performance with our 240% over 50 M conversion of the best MFCC configuration is shown by our EL models."}, {"heading": "C. Classifier1: Gaussian Mixture Model", "text": "The GMM detector we are constructing consists of two GMMs trained in positive or negative form. For a given length of audio data, they are truncated as fixed windows, described as sample X = x1, x2,..., xl, with l being the frame length. Then, we compare the log probability (L) of both models with a decision threshold to decide the appearance of the drone, Labelpredicted = L1 \u2212 L2 > \u03b8decision. In our experiment, GMM leads to the best detection performance with the number of Gausses than 13, the number of MFCCs as 20, and the number of MFCCs as 40. Higher values for these parameters, as suggested in previous work, lead to the upgrade problem, which shows higher detection performance in training, but produces unsatisfactory results on the test dataset. The type of covariance shape influenced by detection performance is almost 0.1 in our training, although we use a diagonal form to overfit."}, {"heading": "D. Classifier2: Convolutional Neural Network", "text": "CNN for audio-related tasks showed excellent results with spectral properties rather than focusing on feature engineering [9], [10]. CNN's main idea is to use a Convolutionary Layer that performs localized filtering to local connectivity, which is known to be effective in capturing invariance useful patterns and highly correlated values with the time-frequency representation of sound signal data. Our observation that drone sound exhibits noticeable inventory features below 1500 Hz with harmonics (Figure 1). Our proposed simple architecture consists of nine levels as opposed to previous approaches suggested in audio-related tasks (Table I), because instead of improving performance, a more complex model easily leads to the overfitting problem. During training, we regularly checked the accuracy and loss with the test data set and stopped training when accuracy did not improve for three eras. Finally, we chose the model that showed the best accuracy."}, {"heading": "E. Classifier3: Recurrent Neural Network", "text": "The other popular DNN model, RNN, is designed to draw on past information to propel the network, performing the same task repeatedly with the memory that represents the context of the information accumulated up to that point. This memory component has the role of preventing the disappearing gradient problem that diminishes the influence of past data. Based on this idea, long-term short-term memory (LSTM) is often used for standard RNN, replacing simple neurons with LSTM memory blocks consisting of multiple gates, such as a tanh entry gate, a gateway to decide whether it remains, and an output gate to control what value is used to calculate output activation. Finally, the output of the LSTM memory block is calculated as a multiplication of these gates. In this work, the bi-directional LSTM RN model shows the network performance with slightly more complicated STM blocks, and the best three STM blocks to match."}, {"heading": "III. EXPERIMENT", "text": "We evaluated our methodology by answering the following questions: (Q1) Comparing the detection performance of three models, GMM, CNN and RNN. (Q2) Determining the detection performance for invisible types of data, such as detecting different drone models or in different environments. (Q3) Considering the computing costs required for application to real-time detection systems. All reported performance values are averaged over 10 evaluation results. We implemented the model in Python 2.7 with Scikit-learn 0.18, Librosa 0.4.3 and Tensorflow 0.12 on the following system: 4-core 2.6 GHz CPU, SSD and GTX 1070."}, {"heading": "A. Data description", "text": "The background sound consists of data from our own recording and from a public record [12], [13] and the drone sound was collected manually using two popular commercial drones - Phantom3 and Phantom4 from DJI. Our background sound data includes sounds from ordinary real-world situations with common sounds such as chatting, passing cars and aircraft noise, with a total time of 677 seconds. Our drone sound data was recorded in a quiet outdoor location at a distance of 30 m, 70 m and 150 m for two types of behavior, hovering and approaching, with a total time of 64 seconds. The exact labeling of the drone noise was achieved by starting after the drone was activated, and stopping before deactivating it. As a result of the augmentation, the total audio time used for the training is 9556 seconds. Note that we are separating the training and test data to strictly measure the performance of the training, rather than double the technique we normally use for the transverse one half."}, {"heading": "B. Testing: detection performance", "text": "We evaluated the drone's detection performance against the proposed three models with the actual predicted time period (Figure 1), precision, recall, F-score and accuracy (Figure 2).In our experiment, RNN achieves the best detection performance on the training data sets related to the F-score (RNN > CNN > GMM: 0.8009 > 0.6415 > 0.5232).It is evident that our data augmentation is meaningful to address the lack of drone training records due to this high detection performance.Our CNN model is reported as the second best model in terms of precision and recall (0.7953, 0.8066).We note that it remains difficult to determine whether our CNN model exceeds the GMM. Our CNN and GMM show a distinctly different tendency in terms of precision (CNN, GMM: 0.5346 < 0.9031) and recall (CNN, 80M: GMM)."}, {"heading": "C. Testing: unseen types of data", "text": "The disadvantage of the machine learning approach is the possibility of a significant deterioration in detection performance when processing unlearned data. In this experiment, we aim for a deterioration in detection performance for invisible data types and an improved understanding of the tendency of the proposed model. Our RNN continues to achieve the best performance in terms of F score (0.6984) with balanced precision and retrieval (0.5477, 0.9635). Interestingly, our report shows that the CNN model does not classify the data, but treats all data as positive. This misclassification could be caused by invisible, high-noise background sound, which the CNN model could not distinguish from drone noise. Therefore, the CNN model is vulnerable to invisible, loud background data. Our GMM exhibits a more accurate detection performance than CNN, but exhibits a significantly reduced measurement size, so it would not be appropriate to improve in practice the use of additional GMM models based on additional GMM (390,610 additional tendency to GMM)."}, {"heading": "D. Required cost versus Detection performance", "text": "In practice, the two main factors determining the cost of operating this system in practice are processing time and the amount of input data for prediction. Many previous studies have overlooked the practical limitations for improved detection performance, but they can actually dominate the shape of the system in a real environment, such as the difference in processing time between the GMM and DNN models [14]. Statistically, we have measured the execution time for each process only with the exception of the additional time required to execute the Python program. According to the results, all three proposed models appear suitable for application to real-time systems. The most time-consuming phase is feature engineering to create the MFCC vector, but it only takes 145 ms for a one-minute audio clip. Execution time for loading the data varies depending on the target platform and the classification time, not adversely affecting the operation of the real-time system, but it only takes 145 ms for a one-minute audio clip to be loaded."}, {"heading": "IV. CONCLUSION", "text": "In our experiment, the RNN model showed the best F score (0.8009) with 240 ms audio input data. Our experiment also confirmed that the use of data augmentation to synthesize raw drone sound with different background noises can alleviate the lack of drone training data. The other main concern of our work was the influence on the detection performance of increasing drone distance. For practical reasons, we could not evaluate distances greater than 150 m. From our experience, audio data recorded at distances greater than 150 m cannot be transferred to the spectrograph with discernible properties."}], "references": [{"title": "Drone sound detection", "author": ["J. Mezei", "V. Fiaska", "A. Moln\u00e1r"], "venue": "Computational Intelligence and Informatics (CINTI), 2015 16th IEEE International Symposium on. IEEE, 2015, pp. 333\u2013338.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Detection and tracking of drones using advanced acoustic cameras", "author": ["J. Busset", "F. Perrodin", "P. Wellig", "B. Ott", "K. Heutschi", "T. R\u00fchl", "T. Nussbaumer"], "venue": "SPIE Security+ Defence. International Society for Optics and Photonics, 2015, pp. 96 470F\u201396 470F.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Drone sound detection by correlation", "author": ["J. Mezei", "A. Moln\u00e1r"], "venue": "Applied Computational Intelligence and Informatics (SACI), 2016 IEEE 11th International Symposium on. IEEE, 2016, pp. 509\u2013518.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning based doppler radar for micro uas detection and classification", "author": ["G.J. Mendis", "T. Randeny", "J. Wei", "A. Madanayake"], "venue": "Military Communications Conference, MILCOM 2016-2016 IEEE. IEEE, 2016, pp. 924\u2013929.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Investigating cost-effective rf-based detection of drones", "author": ["P. Nguyen", "M. Ravindranatha", "A. Nguyen", "R. Han", "T. Vu"], "venue": "Proceedings of the 2nd Workshop on Micro Aerial Vehicle Networks, Systems, and Applications for Civilian Use. ACM, 2016, pp. 17\u201322.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Acoustic scene classification: Classifying environments from the sounds they produce", "author": ["D. Barchiesi", "D. Giannoulis", "D. Stowell", "M.D. Plumbley"], "venue": "IEEE Signal Processing Magazine, vol. 32, no. 3, pp. 16\u2013 34, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Detection of shouted speech in the presence of ambient noise.", "author": ["J. Pohjalainen", "T. Raitio", "P. Alku"], "venue": "in INTERSPEECH,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Tut database for acoustic scene classification and sound event detection", "author": ["A. Mesaros", "T. Heittola", "T. Virtanen"], "venue": "Signal Processing Conference (EUSIPCO), 2016 24th European. IEEE, 2016, pp. 1128\u2013 1132.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust sound event recognition using convolutional neural networks", "author": ["H. Zhang", "I. McLoughlin", "Y. Song"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 559\u2013563.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Filterbank learning for deep neural network based polyphonic sound event detection", "author": ["E. Cakir", "E.C. Ozan", "T. Virtanen"], "venue": "Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, 2016, pp. 3399\u20133406.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural networks for polyphonic sound event detection in real life recordings", "author": ["G. Parascandolo", "H. Huttunen", "T. Virtanen"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 6440\u20136444.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "DARES-G1: Database of Annotated Real-world Everyday Sounds", "author": ["J.K.M.W.W. Grootel", "T.C. Andringa"], "venue": "Proceedings of the NAG/DAGA Meeting 2009, 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Histogram of gradients of timefrequency representations for audio scene classification", "author": ["A. Rakotomamonjy", "G. Gasso"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), vol. 23, no. 1, pp. 142\u2013153, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic environmental sound recognition: Performance versus computational cost", "author": ["S. Sigtia", "A.M. Stark", "S. Krstulovi\u0107", "M.D. Plumbley"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 11, pp. 2096\u20132107, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Even though few studies have been concerned with the problem of drone sound detection, previous work was conducted in isolated or calm places rather than in a real-life environment without the polyphonic sound environment typical of outside areas, such as on the rooftop of a building in a calm place or isolated environment [1], [2], [3].", "startOffset": 325, "endOffset": 328}, {"referenceID": 1, "context": "Even though few studies have been concerned with the problem of drone sound detection, previous work was conducted in isolated or calm places rather than in a real-life environment without the polyphonic sound environment typical of outside areas, such as on the rooftop of a building in a calm place or isolated environment [1], [2], [3].", "startOffset": 330, "endOffset": 333}, {"referenceID": 2, "context": "Even though few studies have been concerned with the problem of drone sound detection, previous work was conducted in isolated or calm places rather than in a real-life environment without the polyphonic sound environment typical of outside areas, such as on the rooftop of a building in a calm place or isolated environment [1], [2], [3].", "startOffset": 335, "endOffset": 338}, {"referenceID": 3, "context": "Other work differs by using an impressive approach based on radar information or the RF frequency [4], [5], but we need to consider a combined", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "Other work differs by using an impressive approach based on radar information or the RF frequency [4], [5], but we need to consider a combined", "startOffset": 103, "endOffset": 106}, {"referenceID": 5, "context": "The most popular combination of feature and classification is Mel-frequency Cepstrum Coefficients (MFCC) [6] with the Gaussian Mixture Model (GMM) [7], [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "The most popular combination of feature and classification is Mel-frequency Cepstrum Coefficients (MFCC) [6] with the Gaussian Mixture Model (GMM) [7], [8].", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "The most popular combination of feature and classification is Mel-frequency Cepstrum Coefficients (MFCC) [6] with the Gaussian Mixture Model (GMM) [7], [8].", "startOffset": 152, "endOffset": 155}, {"referenceID": 8, "context": "Two popular DNN models, the Convolutional Neural Network (CNN) [9], [10] and Recurrent Neural Network (RNN) [11], have also been highlighted for audio-related tasks.", "startOffset": 63, "endOffset": 66}, {"referenceID": 9, "context": "Two popular DNN models, the Convolutional Neural Network (CNN) [9], [10] and Recurrent Neural Network (RNN) [11], have also been highlighted for audio-related tasks.", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "Two popular DNN models, the Convolutional Neural Network (CNN) [9], [10] and Recurrent Neural Network (RNN) [11], have also been highlighted for audio-related tasks.", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "We therefore remedied the shortage of training data, by augmenting the drone sound with diverse real-life environmental sounds from a public dataset [12], [13] and our collection.", "startOffset": 149, "endOffset": 153}, {"referenceID": 12, "context": "We therefore remedied the shortage of training data, by augmenting the drone sound with diverse real-life environmental sounds from a public dataset [12], [13] and our collection.", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": "In addition, we also observed a noticeable influence area on the spectrogram between 5000 Hz and 7000 Hz (Figure 1) as was previously pointed out [1], [2].", "startOffset": 146, "endOffset": 149}, {"referenceID": 1, "context": "In addition, we also observed a noticeable influence area on the spectrogram between 5000 Hz and 7000 Hz (Figure 1) as was previously pointed out [1], [2].", "startOffset": 151, "endOffset": 154}, {"referenceID": 8, "context": "CNN for audio-related tasks showed outstanding results with spectral features instead of focusing on feature engineering [9], [10].", "startOffset": 121, "endOffset": 124}, {"referenceID": 9, "context": "CNN for audio-related tasks showed outstanding results with spectral features instead of focusing on feature engineering [9], [10].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "The background sound consists of data from our own recording and from a public dataset [12], [13] and the drone sound was collected manually with two popular commercial drones \u2013 Phantom3 and Phantom4 from DJI.", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "The background sound consists of data from our own recording and from a public dataset [12], [13] and the drone sound was collected manually with two popular commercial drones \u2013 Phantom3 and Phantom4 from DJI.", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Many previous studies overlooked the practical constraint for improved detection performance, but it indeed may dominate the form of the system in a real environment, such as the difference in processing time between the GMM and DNN models [14].", "startOffset": 240, "endOffset": 244}], "year": 2017, "abstractText": "This work aims to investigate the use of deep neural network to detect commercial hobby drones in real-life environments by analyzing their sound data. The purpose of work is to contribute to a system for detecting drones used for malicious purposes, such as for terrorism. Specifically, we present a method capable of detecting the presence of commercial hobby drones as a binary classification problem based on sound event detection. We recorded the sound produced by a few popular commercial hobby drones, and then augmented this data with diverse environmental sound data to remedy the scarcity of drone sound data in diverse environments. We investigated the effectiveness of state-of-the-art event sound classification methods, i.e., a Gaussian Mixture Model (GMM), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), for drone sound detection. Our empirical results, which were obtained with a testing dataset collected on an urban street, confirmed the effectiveness of these models for operating in a real environment. In summary, our RNN models showed the best detection performance with an F-Score of 0.8009 with 240 ms of input audio with a short processing time, indicating their applicability to real-time detection systems.", "creator": "LaTeX with hyperref package"}}}