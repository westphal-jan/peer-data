{"id": "1701.05847", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jan-2017", "title": "End-To-End Visual Speech Recognition With LSTMs", "abstract": "Traditional visual speech recognition systems consist of two stages, feature extraction and classification. Recently, several deep learning approaches have been presented which automatically extract features from the mouth images and aim to replace the feature extraction stage. However, research on joint learning of features and classification is very limited. In this work, we present an end-to-end visual speech recognition system based on Long-Short Memory (LSTM) networks. To the best of our knowledge, this is the first model which simultaneously learns to extract features directly from the pixels and perform classification and also achieves state-of-the-art performance in visual speech classification. The model consists of two streams which extract features directly from the mouth and difference images, respectively. The temporal dynamics in each stream are modelled by an LSTM and the fusion of the two streams takes place via a Bidirectional LSTM (BLSTM). An absolute improvement of 9.7% over the base line is reported on the OuluVS2 database, and 1.5% on the CUAVE database when compared with other methods which use a similar visual front-end.", "histories": [["v1", "Fri, 20 Jan 2017 16:36:09 GMT  (691kb)", "http://arxiv.org/abs/1701.05847v1", "Accepted for publication, ICASSP 2017"]], "COMMENTS": "Accepted for publication, ICASSP 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["stavros petridis", "zuwei li", "maja pantic"], "accepted": false, "id": "1701.05847"}, "pdf": {"name": "1701.05847.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["sp104@imperial.ac.uk", "zl4615@imperial.ac.uk", "m.pantic@imperial.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 170 1.05 847v 1 [cs.C V] 20 Jan 20Index Terms - Visual Voice Recognition, Lipreading, End-to-End Training, Long-Term Recurring Neural Networks, Deep Networks"}, {"heading": "1. INTRODUCTION", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "2. DATABASES", "text": "The databases used in this study are the OuluVS2 [16] and CUAVE [17]. The OuluVS2 contains 52 speakers, each saying 10 utterances, for a total of 156 examples per utterance. The utterances are: \"Excuse,\" \"Goodbye,\" \"Hello,\" \"How are you,\" \"Nice to meet you,\" \"I'm sorry,\" \"Thank you,\" \"Have a good time,\" \"You are welcome.\" The mouth ROIs are provided and scaled down to 26 x 44 to keep the aspect ratio constant. The CUAVE data set contains 36 subjects, each speaking the digits from 0 to 9.5 times, making a total of 180 examples per digit. The normal part of the database is used where the subjects are in a frontal position."}, {"heading": "3. END-TO-END VISUAL SPEECH RECOGNITION", "text": "The first stream mainly encodes static information by extracting features directly from the ROI of the raw mouth. The second stream encodes local temporal dynamics by extracting features from the ROI of the diff mouth, which is calculated by taking the difference between two consecutive frames. Both streams follow a bottleneck architecture to compress the high-dimensional input image into a low-dimensional representation on the bottleneck layer. The same architecture as in [19] is used where 3 sigmoid hidden layers are used with sizes 2000, 1000 and 500, followed by a linear bottleneck layer. These coding layers are accepted for publication, ICASSP 2017 is trained in a predictive layer-wise manner by associating them with restricted Boltzmann machines (RBMs)."}, {"heading": "4. EXPERIMENTAL SETUP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Evaluation Protocol", "text": "The protocol proposed by the creators of the OuluVS2 database is used [22], with 40 subjects being used for training and validation and 12 for testing. We randomly divided the 40 subjects into 30 and 10 subjects for training and validation purposes respectively, i.e. there are 900 training expressions, 300 validation expressions and 360 test expressions. The evaluation protocol proposed in [4] was used for experiments with the CUAVE database, whereby the odd subjects (a total of 18) are used for testing and the even subjects for training. Furthermore, we have divided the latter into 12 subjects for training and 6 for validation, i.e. there are 600, 300 and 900 training, validation and test expressions."}, {"heading": "4.2. Preprocessing", "text": "Since all experiments are performed independently of the subjects, we must first reduce the effects of subject-dependent characteristics by subtracting the mean image calculated over the entire utterance from each frame. The next step is the normalization of the data. As recommended in [20], the data should be z-normalized, i.e. the mean and the standard deviation should be equal to 0 or 1 before training an RBM with linear input units. Therefore, each image is z-normalized prior to pre-training of the coding levels."}, {"heading": "4.3. Training", "text": "RBM training: For the first shift, a Gaussian-Bernoulli RBM [20] is used because the input (pixels) has a real value, followed by two Bernoulli-Bernoulli RBM and one Bernoulli-Gaussian RBM for the linear bottleneck layer. Each RBM is trained for 20 epochs with a minichield size of 100 and an L2 regulation coefficient of 0.0002 using contrasting divergence, the learning rate is set at 0.1 for Bernoulli RBMs and 0.001 if a layer (input or bottleneck) has a real value as proposed in [20]. End-to-end training: The AdaDelta algorithm [27], which automatically calculates the learning rate in each epoch, was used for training with a minichar size of 20 utterances."}, {"heading": "5. RESULTS", "text": "The results for the OuluVS2 database are shown in Table 1. Since this database was recently made available only to the baseline results of the creators, the best baseline result, namely 74.8%, is achieved by HMMs in combination with DCT functions. The best overall result is achieved by the end-to-end model considered in Fig. 1, with a classification of layers and the LSTM layer. We should also stress that both streams exceed baseline performance, and the best overall result is achieved by the end-to-end model."}, {"heading": "6. CONCLUSION", "text": "In this paper, we present an end-to-end visual speech recognition system that collectively learns to extract features directly from the pixels and classify them using LSTM networks. Results from non-thematic experiments show that the proposed model achieves state-of-the-art performance in the OuluVS2 and CUAVE databases compared to models that use a similar visual frontend. It can easily be expanded to multiple data streams, so we plan to add an audio stream to evaluate its performance in audiovisual speech recognition tasks."}, {"heading": "7. ACKNOWLEDGEMENTS", "text": "This work was funded by the European Community Horizon 2020 under Funding Agreement No. 645094 (SEWA) and No. 688835 (DE-ENIGMA)."}, {"heading": "8. REFERENCES", "text": "[1] G. Potamianos, C. Neti, G. Gravier, A. Garg, and A. W. Senior, \"Recent Advances in Automatic Recognition of Audiovisual Language,\" Proceedings of the IEEE, vol. 91, no. 9, pp. 1306-1326, September 2003. [2] S. Dupont and J. Luettin, \"Audio-visual speech modeling for continuous speech,\" IEEE Trans. on Multimedia, vol. 2, pp. 141-151, September 2000. [3] S. Zhou, G. Zhao, and M. Pietikinen, \"Towards a practical lipreading system,\" in IEEE CVPR, 2011, pp. 137-144. J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y Ng, \"Multimodal deep learning,\" in Proc. of ICML, 2011, pp."}], "references": [{"title": "Recent advances in the automatic recognition of audiovisual speech", "author": ["G. Potamianos", "C. Neti", "G. Gravier", "A. Garg", "A.W. Senior"], "venue": "Proceedings of the IEEE, vol. 91, no. 9, pp. 1306\u20131326, Sept 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Audio-visual speech modeling for continuous speech recognition", "author": ["S. Dupont", "J. Luettin"], "venue": "IEEE Trans. on Multimedia, vol. 2, no. 3, pp. 141\u2013151, Sep 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Towards a practical lipreading system", "author": ["Z. Zhou", "G. Zhao", "M. Pietikinen"], "venue": "IEEE CVPR, 2011, pp. 137\u2013 144.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A. Y Ng"], "venue": "Proc. of ICML, 2011, pp. 689\u2013696.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Integration of deep bottleneck features for audio-visual speech recognition", "author": ["H. Ninomiya", "N. Kitaoka", "S. Tamura", "Y. Iribe", "K. Takeda"], "venue": "Conf. of the International Speech Communication Association, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Extracting deep bottleneck features for visual speech recognition", "author": ["C. Sui", "R. Togneri", "M. Bennamoun"], "venue": "ICASSP, 2015, pp. 1518\u20131522.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Audio-visual deep learning for noise robust speech recognition", "author": ["J. Huang", "B. Kingsbury"], "venue": "IEEE ICASSP, 2013, pp. 7596\u20137599.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep multimodal learning for audio-visual speech recognition", "author": ["Y. Mroueh", "E. Marcheret", "V. Goel"], "venue": "IEEE ICASSP, 2015, pp. 2130\u20132134.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Listening with your eyes: Towards a practical visual speech recognition system using deep boltzmann machines", "author": ["C. Sui", "M. Bennamoun", "R. Togneri"], "venue": "IEEE ICCV, 2015, pp. 154\u2013162.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Lip reading using a dynamic feature of lip images and convolutional neural networks", "author": ["Y. Li", "Y. Takashima", "T. Takiguchi", "Y. Ariki"], "venue": "IEEE/ACIS Intl. Conf. on Computer and Information Science, 2016, pp. 1\u20136.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep complementary bottleneck features for visual speech recognition", "author": ["S. Petridis", "M. Pantic"], "venue": "IEEE ICASSP, 2016, pp. 2304\u20132308.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Audio-visual speech recognition using deep learning", "author": ["K. Noda", "Y. Yamaguchi", "K. Nakadai", "H.G. Okuno", "T. Ogata"], "venue": "Applied Intelligence, vol. 42, no. 4, pp. 722\u2013 737, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Lipreading with long short-term memory", "author": ["M. Wand", "J. Koutn", "J. Schmidhuber"], "venue": "IEEE ICASSP, 2016, pp. 6115\u20136119.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "An audio-visual corpus for speech perception and automatic speech recognition", "author": ["M. Cooke", "J. Barker", "S. Cunningham", "X. Shao"], "venue": "The Journal of the Acoustical Society of America, vol. 120, no. 5, pp. 2421\u20132424, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning dynamic stream weights for coupled-hmm-based audiovisual speech recognition", "author": ["A.H. Abdelaziz", "S. Zeiler", "D. Kolossa"], "venue": "IEEE Trans. on Audio, Sp., and Lang. Proc., vol. 23, no. 5, pp. 863\u2013876, May 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Ouluvs2: A multi-view audiovisual database for nonrigid mouth motion analysis", "author": ["I. Anina", "Z. Zhou", "G. Zhao", "M. Pietikinen"], "venue": "IEEE FG, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Moving-talker, speaker-independent feature study, and baseline results using the cuave multimodal speech corpus", "author": ["E. Patterson", "S. Gurbuz", "Z. Tufekci", "J. Gowdy"], "venue": "EURASIP J. Appl. Signal Process., vol. 2002, no. 1, pp. 1189\u20131201, Jan. 2002.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "One millisecond face alignment with an ensemble of regression trees", "author": ["V. Kazemi", "J. Sullivan"], "venue": "IEEE CVPR, 2014, pp. 1867\u20131874.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "Salakhutdinov R."], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G. Hinton"], "venue": "Neural Networks: Tricks of the Trade, pp. 599\u2013619. Springer, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "The htk book", "author": ["S. Young", "G. Evermann", "M. Gales", "T. Hain", "D. Kershaw", "X. Liu", "G. Moore", "J. Odell", "D. Ollason", "D. Povey"], "venue": "vol. 3, pp. 175, 2002.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "J. Mach. Learn. Res., vol. 15, no. 1, pp. 2949\u20132980, Jan. 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal fusion and learning with uncertain features applied to audiovisual speech recognition", "author": ["G. Papandreou"], "venue": "Workshop on Multimedia Signal Processing, 2007, pp. 264\u2013267.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Patch-based representation of visual speech", "author": ["P. Lucey", "S. Sridharan"], "venue": "Proc. of the HCSNet Workshop on Use of Vision in HCI, 2006, pp. 79\u201385.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Adaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition", "author": ["G. Papandreou", "A. Katsamanis", "V. Pitsikalis", "P. Maragos"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing, vol. 17, no. 3, pp. 423\u2013435, 2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "vol. http://arxiv.org/abs/1212.5701, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Traditionally, visual speech recognition systems consist of two stages, feature extraction from the mouth region of interest (ROI) and classification [1, 2, 3].", "startOffset": 150, "endOffset": 159}, {"referenceID": 1, "context": "Traditionally, visual speech recognition systems consist of two stages, feature extraction from the mouth region of interest (ROI) and classification [1, 2, 3].", "startOffset": 150, "endOffset": 159}, {"referenceID": 2, "context": "Traditionally, visual speech recognition systems consist of two stages, feature extraction from the mouth region of interest (ROI) and classification [1, 2, 3].", "startOffset": 150, "endOffset": 159}, {"referenceID": 3, "context": "[4] applied principal component analysis (PCA) to the mouth ROI and trained a deep autoencoder to extract bottleneck features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] also applied PCA to the mouth ROIs and used a deep autoencoder to extract bottleneck features but an HMM was used in order to take into account the temporal dynamics.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] extracted local binary patterns from the mouth ROI and used a deep autoencoder to reduce their dimensionality.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "A similar approach has also been followed in audiovisual speech recognition [7, 8, 9] where a shared representation of the input audio and visual features is extracted from the bottleneck layer.", "startOffset": 76, "endOffset": 85}, {"referenceID": 7, "context": "A similar approach has also been followed in audiovisual speech recognition [7, 8, 9] where a shared representation of the input audio and visual features is extracted from the bottleneck layer.", "startOffset": 76, "endOffset": 85}, {"referenceID": 8, "context": "A similar approach has also been followed in audiovisual speech recognition [7, 8, 9] where a shared representation of the input audio and visual features is extracted from the bottleneck layer.", "startOffset": 76, "endOffset": 85}, {"referenceID": 9, "context": "Li [10] used a convolutional neural network (CNN) in order to extract bottleneck features from dynamic representations of images, which are fed to an HMM for classification.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "In our previous work [11], we extracted bottleneck features directly from the raw mouth ROI using a deep feedforward network and then trained an LSTM for classification.", "startOffset": 21, "endOffset": 25}, {"referenceID": 11, "context": "[12] used a CNN to predict the phoneme that corresponds to an input mouth ROI, and then an HMM is used together with audio features in order to classify an utterance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] developed an end-to-end system for lipreading.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The system was tested on a subjectdependent experiment on the GRID corpus[14] and although it outperformed other baseline systems it failed to outperform the state-of-the-art results [15].", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "The system was tested on a subjectdependent experiment on the GRID corpus[14] and although it outperformed other baseline systems it failed to outperform the state-of-the-art results [15].", "startOffset": 183, "endOffset": 187}, {"referenceID": 15, "context": "The databases used in this study are the OuluVS2 [16] and CUAVE [17].", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "The databases used in this study are the OuluVS2 [16] and CUAVE [17].", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "Sixty eight points are tracked on the face using the tracker proposed in [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "The same architecture as in [19] is used, where 3 sigmoid hidden layers are used with sizes of 2000, 1000 and 500, respectively, followed by a linear bottleneck layer.", "startOffset": 28, "endOffset": 32}, {"referenceID": 19, "context": "pre-trained in a greedy layer-wise manner using Restricted Boltzmann Machines (RBMs) [20].", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "The \u2206 (first derivatives) and \u2206\u2206 (second derivatives) [21] features are also computed, based on the bottleneck features, and they are appended to the bottleneck layer.", "startOffset": 54, "endOffset": 58}, {"referenceID": 3, "context": "The evaluation protocol suggested in [4] was used for experiments on the CUAVE database.", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "The end-to-end model is trained using the same protocol as [4, 23] where 18 subjects are used for training and validation and 18 for testing.", "startOffset": 59, "endOffset": 66}, {"referenceID": 21, "context": "The end-to-end model is trained using the same protocol as [4, 23] where 18 subjects are used for training and validation and 18 for testing.", "startOffset": 59, "endOffset": 66}, {"referenceID": 3, "context": "Deep Autoencoder + SVM [4] 68.", "startOffset": 23, "endOffset": 26}, {"referenceID": 21, "context": "Deep Boltzmann Machines + SVM [23] 69.", "startOffset": 30, "endOffset": 34}, {"referenceID": 22, "context": "AAM +HMM [24] \u2020 75.", "startOffset": 9, "endOffset": 13}, {"referenceID": 23, "context": "Patch-based Features + HMM [25] \u2217 77.", "startOffset": 27, "endOffset": 31}, {"referenceID": 24, "context": "Visemic AAM + HMM [26] \u2020 \u2021 83.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "As recommended in [20] the data should be z-normalised, i.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "RBM Training: A Gaussian-Bernoulli RBM [20] is used for the first layer since the input (pixels) is real-valued, followed by two Bernoulli-Bernoulli RBMs and one BernoulliGaussian RBM for the linear bottleneck layer.", "startOffset": 39, "endOffset": 43}, {"referenceID": 19, "context": "001 when one layer (input or bottleneck) is real-valued as suggested in [20].", "startOffset": 72, "endOffset": 76}, {"referenceID": 25, "context": "End-to-End Training: The AdaDelta algorithm [27], which automatically computes the learning rate in each epoch, was used for training with a mini-batch size of 20 utterances.", "startOffset": 44, "endOffset": 48}, {"referenceID": 3, "context": "Only [4] and [23] use the same evaluation protocol as in this study.", "startOffset": 5, "endOffset": 8}, {"referenceID": 21, "context": "Only [4] and [23] use the same evaluation protocol as in this study.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "This includes [24] where a 6fold cross validation was used with 30 subjects for training and validation and 6 for testing, and [25] where 28 subjects were used for training and validation and 10 for testing.", "startOffset": 14, "endOffset": 18}, {"referenceID": 23, "context": "This includes [24] where a 6fold cross validation was used with 30 subjects for training and validation and 6 for testing, and [25] where 28 subjects were used for training and validation and 10 for testing.", "startOffset": 127, "endOffset": 131}, {"referenceID": 24, "context": "Only [26] achieves a higher performance than our end-to-end system, but a much more complicated visual front-end is used, with a cascade of active appearance models (AAM), and the model is evaluated using a 6-fold cross-validation.", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "This is also reported in [13] and it is likely due to the small training sets.", "startOffset": 25, "endOffset": 29}], "year": 2017, "abstractText": "Traditional visual speech recognition systems consist of two stages, feature extraction and classification. Recently, several deep learning approaches have been presented which automatically extract features from the mouth images and aim to replace the feature extraction stage. However, research on joint learning of features and classification is very limited. In this work, we present an end-to-end visual speech recognition system based on Long-Short Memory (LSTM) networks. To the best of our knowledge, this is the first model which simultaneously learns to extract features directly from the pixels and perform classification and also achieves state-of-the-art performance in visual speech classification. The model consists of two streams which extract features directly from the mouth and difference images, respectively. The temporal dynamics in each stream are modelled by an LSTM and the fusion of the two streams takes place via a Bidirectional LSTM (BLSTM). An absolute improvement of 9.7% over the base line is reported on the OuluVS2 database, and 1.5% on the CUAVE database when compared with other methods which use a similar visual front-end.", "creator": "LaTeX with hyperref package"}}}