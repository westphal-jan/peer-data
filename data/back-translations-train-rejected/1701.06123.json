{"id": "1701.06123", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2017", "title": "Optimization on Product Submanifolds of Convolution Kernels", "abstract": "We address a problem of optimization on product of embedded submanifolds of convolution kernels (PEMs) in convolutional neural networks (CNNs). First, we explore metric and curvature properties of PEMs in terms of component manifolds. Next, we propose a SGD method, called C-SGD, by generalizing the SGD methods employed on kernel submanifolds for optimization on product of different collections of kernel submanifolds. Then, we analyze convergence properties of the C-SGD considering sectional curvature properties of PEMs. In the theoretical results, we expound the constraints that can be used to compute adaptive step sizes of the C-SGD in order to assure the asymptotic convergence.", "histories": [["v1", "Sun, 22 Jan 2017 05:35:39 GMT  (181kb)", "http://arxiv.org/abs/1701.06123v1", "3 pages"]], "COMMENTS": "3 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["mete ozay", "takayuki okatani"], "accepted": false, "id": "1701.06123"}, "pdf": {"name": "1701.06123.pdf", "metadata": {"source": "CRF", "title": "Optimization on Product Submanifolds of Convolution Kernels", "authors": ["Mete Ozay", "Takayuki Okatani"], "emails": ["mozay@vision.is.tohoku.ac.jp", "okatani@vision.is.tohoku.ac.jp"], "sections": [{"heading": null, "text": "ar Xiv: 170 1,06 123v 1 [cs.C V] 22 Jan 20"}, {"heading": "1 INTRODUCTION", "text": "Product diversity has been used to solve various optimization problems in machine learning, pattern recognition and computer vision. Furthermore, in recent work stochastic gradient descent (SGD) methods have been applied to embedded kernel manifolds [5] and Grassmann manifolds [4] of folding cores (weights) of Convolutionary Neural Networks (CNNs) [1], [2]. Ozay and Okatani [5] proposed an SGD method to train CNNs with embedded kernel subfolds of folding cores with convergence properties. Boot manifolds products are used for action detection by using CNNs in [3]. In this paper, we deal with a problem of optimizing products of embedded kernel subfolds (PEMs) with convergence properties. Using an SGD method to summarize the effects of product formation of SD-1, we examine the following properties of GD:"}, {"heading": "2 OPTIMIZATION ON PRODUCT MANIFOLDS FOR TRAINING OF CNNS", "text": "We assume that we have received a set of training texts which contain a set of training texts which contain a set of training texts. (Wl) Wl = 1 of the random variables resulting from a distribution P to a measurable space S in which yi is a class name of the Irish image Ii. (Wl) Wl = {Wl = {Wd, l} Dld = 1, and Wd = [Wd, l] Wl = 1, l = [Wc, l] Clc = 1 is a tensorels (weight matrices) Wc, d, l = 1,2., L, for each cth channel c = 1,2,."}, {"heading": "3 CONCLUSION", "text": "In our theoretical results, we first examine the metric and curvature properties of PEM's and show that PEM's have a strictly positive cross-section curvature. Next, we propose an SGD method called C-SGD, by generalizing the SGD methods used on nuclear subdiversity to optimize C-SGD's adaptation step size for the product of different collections of nuclear subdiversity. Then, we analyze the convergence properties of C-SGD for PEM's, taking into account their cross-section properties. In the theoretical results, we explain the constraints that can be used to calculate the adaptive step size of C-SGD to ensure its asymptotic convergence. In addition, we use the proposed methods to calculate C-SGD's adaptive step size for optimizing C-SGD's PEM steps, to compare the proposed SK with the steps here."}], "references": [{"title": "Generalized backpropagation, \u00e9tude de cas: Orthogonality", "author": ["M. Harandi", "B. Fernando"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "A riemannian network for spd matrix learning", "author": ["Z. Huang", "L.V. Gool"], "venue": "In Assoc. for the Adv. of Artificial Intelligence (AAAI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Deep learning on lie groups for skeleton-based action recognition", "author": ["Z. Huang", "C. Wan", "T. Probst", "L.V. Gool"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Building deep networks on grassmann manifolds", "author": ["Z. Huang", "J. Wu", "L.V. Gool"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Optimization on submanifolds of convolution kernels in cnns", "author": ["M. Ozay", "T. Okatani"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "In the recent works, stochastic gradient descent (SGD) methods have been employed on embedded kernel submanifolds [5] and Grassmann manifolds [4] of convolution kernels (weights) of convolutional neural networks (CNNs) [1], [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 3, "context": "In the recent works, stochastic gradient descent (SGD) methods have been employed on embedded kernel submanifolds [5] and Grassmann manifolds [4] of convolution kernels (weights) of convolutional neural networks (CNNs) [1], [2].", "startOffset": 142, "endOffset": 145}, {"referenceID": 0, "context": "In the recent works, stochastic gradient descent (SGD) methods have been employed on embedded kernel submanifolds [5] and Grassmann manifolds [4] of convolution kernels (weights) of convolutional neural networks (CNNs) [1], [2].", "startOffset": 219, "endOffset": 222}, {"referenceID": 1, "context": "In the recent works, stochastic gradient descent (SGD) methods have been employed on embedded kernel submanifolds [5] and Grassmann manifolds [4] of convolution kernels (weights) of convolutional neural networks (CNNs) [1], [2].", "startOffset": 224, "endOffset": 227}, {"referenceID": 4, "context": "Ozay and Okatani [5] proposed a SGD method to train CNNs using embedded kernel submanifolds of convolution kernels with convergence properties.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "Products of the Stiefel manifolds are used for action recognition by employing CNNs in [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "2) Next, we generalize the SGD methods employed on kernel submanifolds [2], [3], [5] for optimization on product of different collections of kernel submanifolds using a SGD algorithm, called C-SGD.", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "2) Next, we generalize the SGD methods employed on kernel submanifolds [2], [3], [5] for optimization on product of different collections of kernel submanifolds using a SGD algorithm, called C-SGD.", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "2) Next, we generalize the SGD methods employed on kernel submanifolds [2], [3], [5] for optimization on product of different collections of kernel submanifolds using a SGD algorithm, called C-SGD.", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "In the SGD algorithms [5], [2], [3], each kernel is assumed to reside on an embedded kernel submanifold Mc,d,l at the l layer", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "In the SGD algorithms [5], [2], [3], each kernel is assumed to reside on an embedded kernel submanifold Mc,d,l at the l layer", "startOffset": 27, "endOffset": 30}, {"referenceID": 2, "context": "In the SGD algorithms [5], [2], [3], each kernel is assumed to reside on an embedded kernel submanifold Mc,d,l at the l layer", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "In this work, we propose a SGD algorithm called C-SGD by generalizing the SGD algorithms employed on kernel submanifolds [5], [2], [3] for optimization on product of different collections of the submanifolds, which are defined next.", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "In this work, we propose a SGD algorithm called C-SGD by generalizing the SGD algorithms employed on kernel submanifolds [5], [2], [3] for optimization on product of different collections of the submanifolds, which are defined next.", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "In this work, we propose a SGD algorithm called C-SGD by generalizing the SGD algorithms employed on kernel submanifolds [5], [2], [3] for optimization on product of different collections of the submanifolds, which are defined next.", "startOffset": 131, "endOffset": 134}], "year": 2017, "abstractText": "We address a problem of optimization on product of embedded submanifolds of convolution kernels (PEMs) in convolutional neural networks (CNNs). First, we explore metric and curvature properties of PEMs in terms of component manifolds. Next, we propose a SGD method, called C-SGD, by generalizing the SGD methods employed on kernel submanifolds for optimization on product of different collections of kernel submanifolds. Then, we analyze convergence properties of the C-SGD considering sectional curvature properties of PEMs. In the theoretical results, we expound the constraints that can be used to compute adaptive step sizes of the C-SGD in order to assure the asymptotic convergence.", "creator": "LaTeX with hyperref package"}}}