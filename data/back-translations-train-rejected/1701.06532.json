{"id": "1701.06532", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2017", "title": "ENIGMA: Efficient Learning-based Inference Guiding Machine", "abstract": "ENIGMA is a learning-based method for guiding given clause selection in saturation-based theorem provers. Clauses from many proof searches are classified as positive and negative based on their participation in the proofs. An efficient classification model is trained on this data, using fast feature-based characterization of the clauses . The learned model is then tightly linked with the core prover and used as a basis of a new parameterized evaluation heuristic that provides fast ranking of all generated clauses. The approach is evaluated on the E prover and the CASC 2016 AIM benchmark, showing a large increase of E's performance.", "histories": [["v1", "Mon, 23 Jan 2017 18:03:52 GMT  (22kb)", "http://arxiv.org/abs/1701.06532v1", "Submitted to LPAR 2017"]], "COMMENTS": "Submitted to LPAR 2017", "reviews": [], "SUBJECTS": "cs.LO cs.AI cs.LG", "authors": ["jan jakub\\r{u}v", "josef urban"], "accepted": false, "id": "1701.06532"}, "pdf": {"name": "1701.06532.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jakubuv@gmail.com", "josef.urban@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 1.06 532v 1 [cs.L O] 23 Jan 2017"}, {"heading": "1 Introduction: Theorem Proving and Learning", "text": "In fact, the stronger the performance of these tools, the more realistic tasks such as fully understanding complex mathematical theories and reviewing software, hardware, and engineering designs become. While the performance of ATPs has steadily increased in recent years, it is still, on average, far below the performance of trained mathematicians. Their advanced knowledge is a mystery that probably cannot be deciphered."}, {"heading": "2 Preliminaries", "text": "If S is a finite set, then | S | denotes its size, and Sn, if n P N is the n-ary cartesian product of S, i.e. the set of all vectors of size n with members of S. If x P is Sn, then we use the notation xris to denote its i-th member, with indexes of 1. We use xT to denote the transposed vector. Multiset M over a set S is represented by a total function from S to N, i.e., Mpsq is the number of s P in M. The union M1 YM2 of two multisets M1 and M2 over S is the multiset represented by the function pM1 Y M2qpsq \"M1psq'M2psq 'for all s P S. We use the notation ts1."}, {"heading": "3 Training Clause Classifiers", "text": "There are many different machine learning methods with different functional areas to explore, different training and evaluation speeds, etc. Based on our previous experiments with premise selection and with leanCoP, we opted for a very fast and scalable learning method for the first ENIGMA instance. While more expressive learning methods usually lead to stronger results from a single strategy, very important aspects of our field are that (i) learning and testing develop together in a feedback loop [23] where rapid learning is useful, and (ii) combinations of multiple strategies - which can be delivered in different ways from different evidence by learning - usually solve many more problems than the best strategies.After several experiments, we opted for LIBLINEAR: Open Source Library [4] for large-scale linear classification. This section describes how we use LIBLINEAR to train a clause classifier to pre-select a clone."}, {"heading": "3.1 Extracting Training Examples from ATP Runs", "text": "Suppose we perform a saturation-based ATP to prove a presumption in theory T. If the ATP succeeds in concluding a proof, we can extract the following training examples from this particular search for evidence. We collect all clauses that were selected as pre-determined clauses during the search for evidence. From these clauses, those that appear in the final proof are classified as positive, while the remaining pre-defined clauses are classified as negative. This gives us two sets of clauses, positive clauses C and negative clauses Ca. Repeating the search for evidence using the information pC: \"Caq to prefer clauses from C\" as pre-defined clauses should significantly shorten the search for evidence. The challenge is to generalize this knowledge in order to prove new problems that are in a sense similar. To achieve this, the positive and negative clauses extracted from evidence for many related problems are combined and learned together."}, {"heading": "3.2 Encoding Clauses by Features", "text": "To use LIBLINEAR for linear classification (Section 3.3), we must present clauses as finite attribute vectors. For our purposes, a attribute vector x representing a sentence C is a vector with a fixed length of natural numbers whose i-th member xris indicates how often the i-th attribute appears in clause C. Several choices of attribute clauses are possible [14], for example sub-concepts, their generalizations, or paths in term trees. In this thesis, we use term paths of length 3 as follows. First, we construct a attribute vector for each literal L in clause C. We write the literal L as a tree in which the symbols from the individual words are labeled. To handle possibly infinite numbers of variables and scolem symbols, we replace all variables and scolem symbols with special symbols."}, {"heading": "3.2.1 A Technical Note on Feature Vector Representation", "text": "To use LIBLINEAR, we transform the attribute multiset \u03a6 into a vector of the length classes. \"We assign a natural index to each attribute and construct a vector whose i-string contains the number of characters where i is the index of the attribute \u03c6.\" Technically, we construct a bijection symbol between \u03a3 and t0,. \"1u, which encodes symbols by natural numbers.\" Then we construct a bijection between attribute and t1,. \"Feature | u, which encodes features by numbers1.\" \"Now it is easy to construct a function vector that encodes IBIB's model into a vector from N | Feature | as follows:\" x, that xrcodepaciq qs, \"which encodes features for all other types of characteristics.\""}, {"heading": "4 Guiding the Proof Search", "text": "Once we have a LIBLINEAR model (classifier) M, we construct a clause weight function that can be used within the clause loop to evaluate the clauses generated. As usual, clauses with lower weight are selected before clauses with higher weight. First, we define the function prediction, which assigns a smaller number to positively classified clauses, as follows: Prediction weight pC, Mq \"# 1 iff predipC, Mq\" 10. In addition, to prefer smaller clauses over larger clauses, we add the sentence length to the weight predicted above. We use lengthpCq to specify the length of C as the number of symbols. In addition, we use a real parameter Prediction clause prediction clause prediction clause clause."}, {"heading": "5 Experimental Evaluation", "text": "We use the AIM2 category of CASC 2016 in different ways to get even more training examples. This benchmark fits our needs as it targets internal steering in ATPs based on training and testing examples. Before the competition, 1020 training problems were provided for training ATPs, while an additional 200 problems were used in the competition. Prover9 evidence was provided along with all training problems. Due to several interesting issues, we decided not to use the training problems and instead to find as many strategies as possible. With quick preliminary evaluations, we selected a strong E4 strategy S0 (see Appendix A), which alone can solve 239 training problems with a 30 s timeout. For comparison, E's Auto Plan Mode (with optimized strategy planning) we can solve 261 problems. We train a clause classifier model M0 (section 3) on the 239 proofs and then perform with the classifications M0 in different ways."}, {"heading": "5.1 Looping and Boosting", "text": "Recent work on the premise selection task has shown that there is typically no single optimal method for guiding the search for evidence; re-learning new evidence, as introduced by MaLARea, and combining evidence and learners usually exceeds a single method; since we are using a very fast classifier here, we can easily experiment with giving it more and other data. Firstly, such an experiment is conducted as follows: We add the evidence obtained from the 52 competition problems solved to the training data obtained from the 337 training problems solved; instead of immediately learning and re-performing them (as in the MaLARea loop), however, we first increase all positive examples (i.e. clauses participating in the evidence) by repeating them ten times in the training data; in this way, we inform the learner to avoid the misclassification of the positive examples as negative rather than vice versa."}, {"heading": "6 Conclusions", "text": "While recent work on premise selection and internal guidance for leanCoP suggests that major improvements are possible, this is the first practical and usable improvement of a state-of-the-art ATP through internal learning-based guidance based on a large CASC benchmark. It is clear that a wide range of future improvements is possible: learning could also be used dynamically during evidence gathering, training problems could be selected according to their similarity to the current problem, 5 more advanced learning and characterization methods could be applied, etc. The extent of improvement is unusually large for the ATP area and comparable to the improvements achieved in MaLARea 0.5 over E-LTB (with the same underlying engine) in CASC 2013 [13]. We believe that this could mark the arrival of ENIGMAs - efficient learning-based inference guides - as a crucial and indispensable technology for building the most automated auditor."}, {"heading": "7 Acknowledgments", "text": "We thank Stephan Schulz for his open and modular implementation of E and his many features that made this work possible. We also thank the Machine Learning Group of National Taiwan University for the open availability of LIBLINEAR. This work was supported by the ERC Consolidator Scholarship No. 649043 AI4REASON."}, {"heading": "A The E Prover Strategy Used in Experiments", "text": "The following fixed E-strategy S0, described by its command-line arguments, was used in the experiments: --defintional-cnf = 24 --destructive-er-aggressive --destructive-er-prefers-initial clauses -F1 --delete-bad-limit = 150000000 --forward-context-sr -winvfreqrank -c1 -Ginvfreq -WSelectComplexG --oriented-simul-paramod -tKBO6 -H (1 * ConjectureRelativeSymbolWeight (SimulateSOS, 0.5,100,100,1,5,5,1), 4 * ConjectureRelativeSymbolWeight (ConstPrio, 0.1,100,100,100,1,5,1,5), 1 * FIFOWeight (PreferProcessed), 1 * ConjectureRelativeSymbolWeight (PreferNonGoals, 0.5,100,100,1,5,1,5,1), Sfinate2,2)."}], "references": [{"title": "A learning-based fact selector for Isabelle/HOL", "author": ["J.C. Blanchette", "D. Greenaway", "C. Kaliszyk", "D. K\u00fchlwein", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Hammering towards QED", "author": ["J.C. Blanchette", "C. Kaliszyk", "L.C. Paulson", "J. Urban"], "venue": "J. Formalized Reasoning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "A training algorithm for optimal margin classifiers", "author": ["B.E. Boser", "I. Guyon", "V. Vapnik"], "venue": "In COLT,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R. Fan", "K. Chang", "C. Hsieh", "X. Wang", "C. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Monte Carlo connection prover", "author": ["M. F\u00e4rber", "C. Kaliszyk", "J. Urban"], "venue": "CoRR, abs/1611.05990,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Global Conference on Artificial Intelligence, GCAI 2015, Tbilisi, Georgia, October 16-19, 2015, volume 36 of EPiC Series in Computing", "author": ["G. Gottlob", "G. Sutcliffe", "A. Voronkov", "editors"], "venue": "EasyChair,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "SEPIA: search for proofs using inferred automata", "author": ["T. Gransden", "N. Walkinshaw", "R. Raman"], "venue": "25th International Conference on Automated Deduction, Berlin,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A dual coordinate descent method for large-scale linear SVM", "author": ["C. Hsieh", "K. Chang", "C. Lin", "S.S. Keerthi", "S. Sundararajan"], "venue": "In ICML, volume 307 of ACM International Conference Proceeding Series,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "BliStrTune: hierarchical invention of theorem proving strategies", "author": ["J. Jakubuv", "J. Urban"], "venue": "Proceedings of the 6th ACM SIGPLAN Conference on Certified Programs and Proofs,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Learning-assisted automated reasoning with Flyspeck", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "FEMaLeCoP: Fairly efficient machine learning connection prover", "author": ["C. Kaliszyk", "J. Urban"], "venue": "Logic for Programming, Artificial Intelligence, and Reasoning - 20th International Conference,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "MizAR 40 for Mizar 40", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Machine learner for automated reasoning 0.4 and 0.5", "author": ["C. Kaliszyk", "J. Urban", "J. Vysko\u010dil"], "venue": "CoRR, abs/1402.2359,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Efficient semantic features for automated reasoning over large theories", "author": ["C. Kaliszyk", "J. Urban", "J. Vysko\u010dil"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "First-order theorem proving and Vampire", "author": ["L. Kov\u00e1cs", "A. Voronkov"], "venue": "CAV, volume 8044 of LNCS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "MaLeS: A framework for automatic tuning of automated theorem provers", "author": ["D. K\u00fchlwein", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Trust region newton method for logistic regression", "author": ["C. Lin", "R.C. Weng", "S.S. Keerthi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "leanCoP: lean connection-based theorem proving", "author": ["J. Otten", "W. Bibel"], "venue": "J. Symb. Comput.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "E - A Brainiac Theorem Prover", "author": ["S. Schulz"], "venue": "AI Commun.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "The 8th IJCAR automated theorem proving system competition - CASC-J8", "author": ["G. Sutcliffe"], "venue": "AI Commun.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "MaLARea SG1 - Machine Learner for Automated Reasoning with Semantic Guidance", "author": ["J. Urban", "G. Sutcliffe", "P. Pudl\u00e1k", "J. Vysko\u010dil"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "MaLeCoP: Machine learning connection prover", "author": ["J. Urban", "J. Vysko\u010dil", "P. \u0160t\u011bp\u00e1nek"], "venue": "editors, TABLEAUX,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "State-of-the-art resolution/superposition automated theorem provers (ATPs) such as Vampire [15] and E [20] are today\u2019s most advanced tools for general reasoning across a variety of mathematical and scientific domains.", "startOffset": 91, "endOffset": 95}, {"referenceID": 18, "context": "State-of-the-art resolution/superposition automated theorem provers (ATPs) such as Vampire [15] and E [20] are today\u2019s most advanced tools for general reasoning across a variety of mathematical and scientific domains.", "startOffset": 102, "endOffset": 106}, {"referenceID": 9, "context": "On large corpora such as Flyspeck, Mizar and Isabelle, the ATP progress has been mainly due to learning how to select the most relevant knowledge, based on many previous proofs [10, 12, 1, 2].", "startOffset": 177, "endOffset": 191}, {"referenceID": 11, "context": "On large corpora such as Flyspeck, Mizar and Isabelle, the ATP progress has been mainly due to learning how to select the most relevant knowledge, based on many previous proofs [10, 12, 1, 2].", "startOffset": 177, "endOffset": 191}, {"referenceID": 0, "context": "On large corpora such as Flyspeck, Mizar and Isabelle, the ATP progress has been mainly due to learning how to select the most relevant knowledge, based on many previous proofs [10, 12, 1, 2].", "startOffset": 177, "endOffset": 191}, {"referenceID": 1, "context": "On large corpora such as Flyspeck, Mizar and Isabelle, the ATP progress has been mainly due to learning how to select the most relevant knowledge, based on many previous proofs [10, 12, 1, 2].", "startOffset": 177, "endOffset": 191}, {"referenceID": 8, "context": "Learning from many proofs has also recently become a very useful method for automated finding of parameters of ATP strategies [22, 9, 19, 16], and for learning of sequences of tactics in interactive theorem provers (ITPs) [7].", "startOffset": 126, "endOffset": 141}, {"referenceID": 15, "context": "Learning from many proofs has also recently become a very useful method for automated finding of parameters of ATP strategies [22, 9, 19, 16], and for learning of sequences of tactics in interactive theorem provers (ITPs) [7].", "startOffset": 126, "endOffset": 141}, {"referenceID": 6, "context": "Learning from many proofs has also recently become a very useful method for automated finding of parameters of ATP strategies [22, 9, 19, 16], and for learning of sequences of tactics in interactive theorem provers (ITPs) [7].", "startOffset": 222, "endOffset": 225}, {"referenceID": 17, "context": "Several experiments with the compact leanCoP [18] system have recently shown that directly using trained machine learner for internal clause selection can significantly prune the search space and solve additional problems [24, 11, 5].", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "Several experiments with the compact leanCoP [18] system have recently shown that directly using trained machine learner for internal clause selection can significantly prune the search space and solve additional problems [24, 11, 5].", "startOffset": 222, "endOffset": 233}, {"referenceID": 10, "context": "Several experiments with the compact leanCoP [18] system have recently shown that directly using trained machine learner for internal clause selection can significantly prune the search space and solve additional problems [24, 11, 5].", "startOffset": 222, "endOffset": 233}, {"referenceID": 4, "context": "Several experiments with the compact leanCoP [18] system have recently shown that directly using trained machine learner for internal clause selection can significantly prune the search space and solve additional problems [24, 11, 5].", "startOffset": 222, "endOffset": 233}, {"referenceID": 19, "context": "This in turn very significantly improves the performance of E in terms of the number of solved problems in the CASC 2016 AIM benchmark [21].", "startOffset": 135, "endOffset": 139}, {"referenceID": 20, "context": "While more expressive learning methods usually lead to stronger single-strategy ATP results, very important aspects of our domain are that (i) the learning and proving evolve together in a feedback loop [23] where fast learning is useful, and (ii) combinations of multiple strategies \u2013 which can be provided by learning in different ways from different proofs \u2013 usually solve much more problems than the best strategy.", "startOffset": 203, "endOffset": 207}, {"referenceID": 3, "context": "After several experiments, we have chosen LIBLINEAR: open source library [4] for large-scale linear classification.", "startOffset": 73, "endOffset": 76}, {"referenceID": 13, "context": "Several choices of clause features are possible [14], for example sub-terms, their generalizations, or paths in term trees.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "LIBLINEAR supports several classification methods, but we have so far used only the default solver L2-SVM (L2-regularized L2-loss Support Vector Classification) [3].", "startOffset": 161, "endOffset": 164}, {"referenceID": 7, "context": "LIBLINEAR implements a coordinate descend method [8] and a trust region Newton method [17].", "startOffset": 49, "endOffset": 52}, {"referenceID": 16, "context": "LIBLINEAR implements a coordinate descend method [8] and a trust region Newton method [17].", "startOffset": 86, "endOffset": 90}, {"referenceID": 12, "context": "5 over E-LTB (sharing the same underlying engine) in CASC 2013 [13].", "startOffset": 63, "endOffset": 67}], "year": 2017, "abstractText": "ENIGMA is a learning-based method for guiding given clause selection in saturationbased theorem provers. Clauses from many proof searches are classified as positive and negative based on their participation in the proofs. An efficient classification model is trained on this data, using fast feature-based characterization of the clauses . The learned model is then tightly linked with the core prover and used as a basis of a new parameterized evaluation heuristic that provides fast ranking of all generated clauses. The approach is evaluated on the E prover and the CASC 2016 AIM benchmark, showing a large increase of E\u2019s performance.", "creator": "easychair.cls-3.4"}}}