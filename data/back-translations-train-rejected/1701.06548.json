{"id": "1701.06548", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2017", "title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers.", "histories": [["v1", "Mon, 23 Jan 2017 18:35:28 GMT  (64kb,D)", "http://arxiv.org/abs/1701.06548v1", "Submitted to ICLR 2017"]], "COMMENTS": "Submitted to ICLR 2017", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["gabriel pereyra", "george tucker", "jan chorowski", "{\\l}ukasz kaiser", "geoffrey hinton"], "accepted": false, "id": "1701.06548"}, "pdf": {"name": "1701.06548.pdf", "metadata": {"source": "CRF", "title": "REGULARIZING NEURAL NETWORKS BY PENALIZING CONFIDENT OUTPUT DISTRIBUTIONS", "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Geoffrey Hinton"], "emails": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"], "sections": [{"heading": null, "text": "We systematically investigate the regulation of neural networks by punishing low entropy output distributions. We show that punishing low entropy output distributions, which has been shown to improve exploration in amplification learning, acts as a strong regulator in supervised learning. In addition, we combine a maximum entropy-based confidence penalty with label smoothing through the direction of KL divergence. We comprehensively evaluate the proposed confidence penalty and label smoothing using six common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT '14 English-German), and language recognition (TIMIT and WSJ). We find that both label smoothing and trust penalty improve the state of the art across benchmarks without modifying existing hyperparameters, which implies broad applicability."}, {"heading": "1 INTRODUCTION", "text": "Large neural networks with millions of parameters achieve strong performance in image classification (Szegedy et al., 2015a), machine translation (Wu et al., 2016), speech modeling (Jozefowicz et al., 2016), and speech recognition (Graves et al., 2013). However, despite using large data sets, neural networks still tend to overadapt. Numerous techniques have been proposed to prevent overadaptation, including early stop, L1 / L2 regulation (weight loss), dropout confidence (Srivastava et al., 2014), and batch normalization (Ioffe & Szegedy, 2015). These techniques, along with most other forms of regulation, act on the hidden activations or weights of a neural network. Alternatively, the regulation of output distribution of large, deep neural networks is largely unexplored."}, {"heading": "2 RELATED WORK", "text": "Maximum entropy (Jaynes, 1957) has a long history with many areas of machine learning in which learning is given priority, and the strengthening of learning. (Se) D \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i"}, {"heading": "3 DIRECTLY PENALIZING CONFIDENCE", "text": "Reliable predictions correspond to low entropy output distributions. A network is overconfident if it relies all likelihood on a single class in the training set, which is often a symptom of over-adaptation (Szegedy et al., 2015b). Confidence penalty is a regularization term that prevents these peak distributions, leading to better generalization.A neural network generates a conditional distribution curve (y | x) over classes y that receive an input x through a Softmax function. Entropy of these conditional distribution curves results from H (p\u03b8 (y | x) = \u2212.To punish the reliability of output distributions, we add negative entropy to the negative protocol probability during the trainingL (\u03b8) = \u2212 \u2211 (y | x) \u2212 \u03b2H (p\u03b8 (y | x)), where the strength of the confidence of the max function is controlled by a soft input."}, {"heading": "3.1 ANNEALING AND THRESHOLDING THE CONFIDENCE PENALTY", "text": "In the field of enhanced learning, low entropy distributions prevent an early convergence of a political network and promote exploration. An easy way to achieve this is to increase the confidence penalty during the training.Another way to strengthen the confidence penalty is to punish production distributions only if they are below a certain entropy threshold. We can achieve this by adding a loss of hinges to the confidence penalty, which leads to a target of the form L (\u03b8) = \u2212 \u2211 log p\u03b8 (y | x) \u2212 \u03b2max (0, \u0445 \u2212 H (p\u03b8 (y | x))), where the entropy threshold below which we begin to apply the confidence sentence.Initial experiments suggest that the threshold of the confidence penalty leads to faster convergence at the expense of the introduction of an additional hyperparameter. For the majority of our experiments, we were able to reach the comparison threshold without the hyperparameters."}, {"heading": "3.2 CONNECTION TO LABEL SMOOTHING", "text": "Label smoothing estimates the marginalized effect of label noise during training. If the previous label distribution is uniform, label smoothing is equivalent to adding the KL divergence between the uniform distribution u and the predicted distribution increase of the network to the negative log LikelihoodL (\u03b8) = \u2212 Looking Log-P\u03b8 (y | x) \u2212 DKL (u \u0109p\u043a (y | x)).By reversing the direction of KL divergence, DKL (p\u03b8 (y | x) \u0432u), we regain the confidence penalty. This interpretation suggests other trust regulators who use alternative target distributions instead of uniform distribution. We leave the research of these regulators to future work."}, {"heading": "4 EXPERIMENTS", "text": "We evaluated the confidence penalty and label smoothing at MNIST and CIFAR-10 for image classification, Penn Treebank for voice modeling, WMT '14 English-German for machine translation, and TIMIT and WSJ for speech recognition. All models were implemented with TensorFlow (Abadi et al., 2016) and trained on NVIDIA Tesla K40 or K80 GPUs."}, {"heading": "4.1 IMAGE CLASSIFICATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 MNIST", "text": "As a preliminary experiment, we evaluated the approaches of the standard MNIST digit recognition task. We used the standard split into 60k training frames and 10k test frames. We used the last 10k frames of the training set as a held-out validation set for hyperparameter setting and then retrained the models throughout the best configuration dataset. We trained fully networked ReLu activation neural networks with 1024 units per shift and two hidden layers. Weights were initialized from a normal distribution with a standard deviation of 0.01. Models were optimized with stochastic gradient deviation with a constant learning rate of 0.05 (with the exception of gradations where we set the learning rate to 0.001). For label smoothing, we varied the smoothing parameter in the range [0.05, 0.1, 0.2, 0.3, 0.4, 0.5] and found to work best by 0.1 and 0.5] for both."}, {"heading": "4.1.2 CIFAR-10", "text": "CIFAR-10 is an image classification dataset consisting of 32x32x3 RGB images of 10 classes. The dataset is divided into 50k training images and 10k test images. We use the last 5k images of the training set as high-level validation sets for adjusting hyperparameters, as is customary in practice.For our experiments, we used a tightly connected Convolutionary Neural Network that represents the current state of the art on CIFAR-10 (Huang et al., 2016a). We use the small configuration of (Huang et al., 2016a), which consists of 40 layers, with a growth rate of 12. All models were trained for 300 epochs, with a stack size of 50 and a learning rate of 0.1. The learning rate has been reduced by a factor of 10 at 150 and 225 epochs. We present results for training without data augmentation."}, {"heading": "4.2 LANGUAGE MODELING", "text": "In language modeling, we found that the confidence penalty significantly exceeds label noise and label smoothing. We used the hyperparameter settings from the large configuration in (Zaremba et al., 2014). In short, we used a two-layer, 1500-unit LSTM with 65% failure rate applied to all non-recurring compounds. We trained for 55 epochs with stochastic gradient, with the learning rate decreasing by 1.15 after 14 epochs, and cut the standard of gradients if they were greater than 10%. For label noise and label smoothing, we performed a grid search using noise and smoothing values of [0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5]. In label noise, we found that label smoothing worked best. In the word group, we set the penalty value to 0.5 from 1, we have fixed the penalty value to 0.5 from 1."}, {"heading": "4.3 MACHINE TRANSLATION", "text": "For machine translation, we rated the confidence penalty for the WMT '14 English-German translation task using Google's production-level translation system Wu et al. (2016). The training set consists of 5 million pairs of sentences, and we used newstest2012 and newstests2013 for validation and newstest2014 for testing. We report tokenized BLEU values as calculated by the multi-bleu.perl script from the Moses translation-machine translation package. Our model was an eight-layer sequence-to-sequence model with attention (Bahdanau et al., 2014). The first encoder was a bidirectional LSTM, the remaining encoder and decoder layers were unidirectional LSTMs, and the attention network was a single-layer feed-forward network with attention. Each layer had 512 units (compared to 2016 units in Wu, 1024)."}, {"heading": "4.4 SPEECH RECOGNITION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.4.1 TIMIT", "text": "In the TIMIT corpus, the training set consisted of 3512 utterances, the validation set of 184 utterances, and the test set of 192 utterances. All 61 phonemes were used during training and decoding, and in scoring, these 61 phonemes were reduced to 39 to calculate the phoneme error rate (PER). As a base model, we used a sequence-to-sequence attentive model. The encoder consisted of 3 bidirectional LSTM layers, the decoder consisted of a single unidirectional LSTM layer, and the attention network consisted of a single-layer feed network. All layers consisted of 256 units. The 15% abort rate was applied as described in Zaremba et al. (2014). We trained the asynchronous SGD layer with 5 replicas. We used a stack size of 32, a learning rate of 0.01, and a dynamics rate of 0.0 at 0.0 at 0.0 (0.0 at 0.0 at 0.0)."}, {"heading": "4.4.2 WALL STREET JOURNAL", "text": "For the WSJ corpus, we used attention-based sequence-to-sequence networks that directly predicted the characters. We used the SI284 subset for training, DEV93 for validation, and EVAL92 for testing. We used 240-dimensional vectors composed of 80-bit filter bank functions that were extended with their deltas and delta deltas. The network encoder consisted of 4 bidirectional LSTM layers, each computed with Kaldi Povey et al. (2011). We did not use text-based data or separate language models during decoding. Details of the network architecture were as follows: 4 bi-directional LSTM layers, each comprising 256 units, interleaved with 3 time subsampling layers, configured to drop every second frame (Bahdanau et al. 2016, Chan al; only one STM, 2015-al, was used)."}, {"heading": "5 CONCLUSION", "text": "Motivated by the recent successes of output regulators (Szegedy et al., 2015b; Xie et al., 2016), we are conducting a systematic evaluation of two output regulators: trust penalty and label smoothing. We show that this form of regulation, which has been shown to improve exploration in the area of enhanced learning, also acts as a strong regulator in the area of supervised learning. We note that both trust penalty and label smoothing improve a wide range of state-of-the-art models without having to change hyperparameters."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Sergey Ioffe, Alex Alemi and Navdeep Jaitly for helpful conversations, Prajit Ramachandran, Barret Zoph, Mohammad Norouzi and Yonghui Wu for technical assistance with the different models used in our experiments, and the anonymous reviewers for revealing comments."}, {"heading": "6 GRADIENT NORMS", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bahdanau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A maximum entropy approach to natural language processing", "author": ["Adam L Berger", "Vincent J Della Pietra", "Stephen A Della Pietra"], "venue": "Computational linguistics,", "citeRegEx": "Berger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "N-gram counts and language models from the common crawl", "author": ["Christian Buck", "Kenneth Heafield", "Bas Van Ooyen"], "venue": "In LREC,", "citeRegEx": "Buck et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Buck et al\\.", "year": 2014}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Latent sequence decompositions", "author": ["William Chan", "Yu Zhang", "Quoc Le", "Navdeep Jaitly"], "venue": "arXiv preprint arXiv:1610.03035,", "citeRegEx": "Chan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Attention-based models for speech recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal"], "venue": "arXiv preprint arXiv:1512.05287,", "citeRegEx": "Gal.,? \\Q2015\\E", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "In ICML,", "citeRegEx": "Graves and Jaitly.,? \\Q2014\\E", "shortCiteRegEx": "Graves and Jaitly.", "year": 2014}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "IEEE international conference on acoustics, speech and signal processing,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q Weinberger"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Information theory and statistical mechanics", "author": ["Edwin T Jaynes"], "venue": "Physical review,", "citeRegEx": "Jaynes.,? \\Q1957\\E", "shortCiteRegEx": "Jaynes.", "year": 1957}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "venue": "arXiv preprint arXiv:1605.07648,", "citeRegEx": "Larsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsson et al\\.", "year": 2016}, {"title": "Learning online alignments with continuous rewards policy gradient", "author": ["Yuping Luo", "Chung-Cheng Chiu", "Navdeep Jaitly", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1608.01281,", "citeRegEx": "Luo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1508.04025,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "venue": "arXiv preprint arXiv:1609.07843,", "citeRegEx": "Merity et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "A global optimization technique for statistical classifier design", "author": ["David Miller", "Ajit V Rao", "Kenneth Rose", "Allen Gersho"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Miller et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1996}, {"title": "Distributional smoothing by virtual adversarial examples", "author": ["Takeru Miyato", "Shin-ichi Maeda", "Masanori Koyama", "Ken Nakae", "Shin Ishii"], "venue": "arXiv preprint arXiv:1507.00677,", "citeRegEx": "Miyato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Acoustic modeling using deep belief networks", "author": ["Abdel-rahman Mohamed", "George E Dahl", "Geoffrey Hinton"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Mohamed et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2012}, {"title": "Reward augmented maximum likelihood for neural structured prediction", "author": ["Mohammad Norouzi", "Dale Schuurmans", "Samy Bengio", "Zhifeng Chen", "Navdeep Jaitly", "Mike Schuster", "Yonghui Wu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Norouzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2016}, {"title": "The kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely"], "venue": "In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society,", "citeRegEx": "Povey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2011}, {"title": "Using the output embedding to improve language models", "author": ["Ofir Press", "Lior Wolf"], "venue": "arXiv preprint arXiv:1608.05859,", "citeRegEx": "Press and Wolf.,? \\Q2016\\E", "shortCiteRegEx": "Press and Wolf.", "year": 2016}, {"title": "Training deep neural networks on noisy labels with bootstrapping", "author": ["Scott Reed", "Honglak Lee", "Dragomir Anguelov", "Christian Szegedy", "Dumitru Erhan", "Andrew Rabinovich"], "venue": "arXiv preprint arXiv:1412.6596,", "citeRegEx": "Reed et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2014}, {"title": "Deterministic annealing for clustering, compression, classification, regression, and related optimization problems", "author": ["Kenneth Rose"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rose.,? \\Q1998\\E", "shortCiteRegEx": "Rose.", "year": 1998}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "arXiv preprint arXiv:1512.00567,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Combining time-and frequency-domain convolution in convolutional neural networkbased phone recognition", "author": ["L\u00e1szl\u00f3 T\u00f3th"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "T\u00f3th.,? \\Q2014\\E", "shortCiteRegEx": "T\u00f3th.", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Function optimization using connectionist reinforcement learning algorithms", "author": ["Ronald J Williams", "Jing Peng"], "venue": "Connection Science,", "citeRegEx": "Williams and Peng.,? \\Q1991\\E", "shortCiteRegEx": "Williams and Peng.", "year": 1991}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Disturblabel: Regularizing cnn on the loss layer", "author": ["Lingxi Xie", "Jingdong Wang", "Zhen Wei", "Meng Wang", "Qi Tian"], "venue": "arXiv preprint arXiv:1605.00055,", "citeRegEx": "Xie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Under review as a conference paper at ICLR", "author": ["Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 40, "context": ", 2015a), machine translation (Wu et al., 2016), language modeling (Jozefowicz et al.", "startOffset": 30, "endOffset": 47}, {"referenceID": 19, "context": ", 2016), language modeling (Jozefowicz et al., 2016), and speech recognition (Graves et al.", "startOffset": 27, "endOffset": 52}, {"referenceID": 12, "context": ", 2016), and speech recognition (Graves et al., 2013).", "startOffset": 32, "endOffset": 53}, {"referenceID": 14, "context": "To motivate output regularizers, we can view the knowledge of a model as the conditional distribution it produces over outputs given an input (Hinton et al., 2015) as opposed to the learned values of its parameters.", "startOffset": 142, "endOffset": 163}, {"referenceID": 14, "context": "Distillation (Hinton et al., 2015; Bucilu et al., 2006) exploits this fact by explicitly training a small network to assign the same probabilities to incorrect classes as a large network or ensemble of networks that generalizes well.", "startOffset": 13, "endOffset": 55}, {"referenceID": 18, "context": "The maximum entropy principle (Jaynes, 1957) has a long history with deep connections to many areas of machine learning including unsupervised learning, supervised learning, and reinforcement learning.", "startOffset": 30, "endOffset": 44}, {"referenceID": 3, "context": "In supervised learning, we can search for the model with maximum entropy subject to constraints on empirical statistics, which naturally gives rise to maximum likelihood in log-linear models (see (Berger et al., 1996) for a review).", "startOffset": 196, "endOffset": 217}, {"referenceID": 27, "context": "This prevents the policy from converging early and leads to improved performance (Mnih et al., 2016).", "startOffset": 81, "endOffset": 100}, {"referenceID": 21, "context": "Penalizing low entropy has also been used when combining reinforcement learning and supervised learning to train a neural speech recognition model to learn when to emit tokens (Luo et al., 2016).", "startOffset": 176, "endOffset": 194}, {"referenceID": 29, "context": "Indeed, in recent work on reward augmented maximum likelihood (Norouzi et al., 2016), this entropy augmented reinforcement learning objective played a direct role in linking maximum likelihood and reinforcement learning objectives.", "startOffset": 62, "endOffset": 84}, {"referenceID": 41, "context": "Simply adding label noise has also been shown to be effective at regularizing neural networks (Xie et al., 2016).", "startOffset": 94, "endOffset": 112}, {"referenceID": 14, "context": "Instead of smoothing the labels with a uniform distribution, as in label smoothing, we can smooth the labels with a teacher model (Hinton et al., 2015) or the model\u2019s own distribution (Reed et al.", "startOffset": 130, "endOffset": 151}, {"referenceID": 32, "context": ", 2015) or the model\u2019s own distribution (Reed et al., 2014).", "startOffset": 40, "endOffset": 59}, {"referenceID": 26, "context": "Virtual adversarial training (VAT) (Miyato et al., 2015) is another promising smoothing regularizer.", "startOffset": 35, "endOffset": 56}, {"referenceID": 3, "context": "In supervised learning, we can search for the model with maximum entropy subject to constraints on empirical statistics, which naturally gives rise to maximum likelihood in log-linear models (see (Berger et al., 1996) for a review). Deterministic annealing Rose (1998) is a general approach for optimization that is widely applicable, avoids local minima, and can minimize discrete objectives, and it can be derived from the maximum entropy principle.", "startOffset": 197, "endOffset": 269}, {"referenceID": 3, "context": "In supervised learning, we can search for the model with maximum entropy subject to constraints on empirical statistics, which naturally gives rise to maximum likelihood in log-linear models (see (Berger et al., 1996) for a review). Deterministic annealing Rose (1998) is a general approach for optimization that is widely applicable, avoids local minima, and can minimize discrete objectives, and it can be derived from the maximum entropy principle. Closely related to our work, Miller et al. (1996) apply deterministic annealing to train multilayer perceptrons, where an entropy based regularizer is introduced and slowly annealed.", "startOffset": 197, "endOffset": 502}, {"referenceID": 0, "context": "All models were implemented using TensorFlow (Abadi et al., 2016) and trained on NVIDIA Tesla K40 or K80 GPUs.", "startOffset": 45, "endOffset": 65}, {"referenceID": 37, "context": "Model Layers Size Test Wan et al. (2013) - Unregularized 2 800 1.", "startOffset": 23, "endOffset": 41}, {"referenceID": 34, "context": "40% Srivastava et al. (2014) - Dropout 3 1024 1.", "startOffset": 4, "endOffset": 29}, {"referenceID": 34, "context": "40% Srivastava et al. (2014) - Dropout 3 1024 1.25% Wan et al. (2013) - DropConnect 2 800 1.", "startOffset": 4, "endOffset": 70}, {"referenceID": 34, "context": "40% Srivastava et al. (2014) - Dropout 3 1024 1.25% Wan et al. (2013) - DropConnect 2 800 1.20% Srivastava et al. (2014) - MaxNorm + Dropout 2 8192 0.", "startOffset": 4, "endOffset": 121}, {"referenceID": 13, "context": "Model Layers Parameters Test He et al. (2015) - Residual CNN 110 1.", "startOffset": 29, "endOffset": 46}, {"referenceID": 13, "context": "Model Layers Parameters Test He et al. (2015) - Residual CNN 110 1.7M 13.63% Huang et al. (2016b) - Stochastic Depth Residual CNN 110 1.", "startOffset": 29, "endOffset": 98}, {"referenceID": 13, "context": "Model Layers Parameters Test He et al. (2015) - Residual CNN 110 1.7M 13.63% Huang et al. (2016b) - Stochastic Depth Residual CNN 110 1.7M 11.66% Larsson et al. (2016) - Fractal CNN 21 38.", "startOffset": 29, "endOffset": 168}, {"referenceID": 13, "context": "Model Layers Parameters Test He et al. (2015) - Residual CNN 110 1.7M 13.63% Huang et al. (2016b) - Stochastic Depth Residual CNN 110 1.7M 11.66% Larsson et al. (2016) - Fractal CNN 21 38.6M 10.18% Larsson et al. (2016) - Fractal CNN (Dropout) 21 38.", "startOffset": 29, "endOffset": 220}, {"referenceID": 13, "context": "Model Layers Parameters Test He et al. (2015) - Residual CNN 110 1.7M 13.63% Huang et al. (2016b) - Stochastic Depth Residual CNN 110 1.7M 11.66% Larsson et al. (2016) - Fractal CNN 21 38.6M 10.18% Larsson et al. (2016) - Fractal CNN (Dropout) 21 38.6M 7.33% Huang et al. (2016a) - Densely Connected CNN 40 1.", "startOffset": 29, "endOffset": 280}, {"referenceID": 13, "context": "Model Layers Parameters Test He et al. (2015) - Residual CNN 110 1.7M 13.63% Huang et al. (2016b) - Stochastic Depth Residual CNN 110 1.7M 11.66% Larsson et al. (2016) - Fractal CNN 21 38.6M 10.18% Larsson et al. (2016) - Fractal CNN (Dropout) 21 38.6M 7.33% Huang et al. (2016a) - Densely Connected CNN 40 1.0M 7.00% Huang et al. (2016a) - Densely Connected CNN 100 7.", "startOffset": 29, "endOffset": 339}, {"referenceID": 23, "context": "We performed word-level language modeling experiments using the Penn Treebank dataset (PTB) (Marcus et al., 1993).", "startOffset": 92, "endOffset": 113}, {"referenceID": 42, "context": "We used the hyper-parameter settings from the large configuration in (Zaremba et al., 2014).", "startOffset": 69, "endOffset": 91}, {"referenceID": 9, "context": "Variational dropout (Gal, 2015) applies a fixed dropout mask (stochastic for each sample) at each time-step, instead of resampling at each time-step as in traditional dropout.", "startOffset": 20, "endOffset": 31}, {"referenceID": 40, "context": "Model Parameters Validation Test Zaremba et al. (2014) - Regularized LSTM 66M 82.", "startOffset": 33, "endOffset": 55}, {"referenceID": 9, "context": "4 Gal (2015) - Variational LSTM 66M 77.", "startOffset": 2, "endOffset": 13}, {"referenceID": 9, "context": "4 Gal (2015) - Variational LSTM 66M 77.9 75.2 Press & Wolf (2016) - Tied Variational LSTM 51M 79.", "startOffset": 2, "endOffset": 66}, {"referenceID": 9, "context": "4 Gal (2015) - Variational LSTM 66M 77.9 75.2 Press & Wolf (2016) - Tied Variational LSTM 51M 79.6 75.0 Merity et al. (2016) - Pointer Sentinel LSTM 21M 72.", "startOffset": 2, "endOffset": 125}, {"referenceID": 9, "context": "4 Gal (2015) - Variational LSTM 66M 77.9 75.2 Press & Wolf (2016) - Tied Variational LSTM 51M 79.6 75.0 Merity et al. (2016) - Pointer Sentinel LSTM 21M 72.4 70.9 Zilly et al. (2016) - Variational RHN 32M 71.", "startOffset": 2, "endOffset": 183}, {"referenceID": 9, "context": "4 Gal (2015) - Variational LSTM 66M 77.9 75.2 Press & Wolf (2016) - Tied Variational LSTM 51M 79.6 75.0 Merity et al. (2016) - Pointer Sentinel LSTM 21M 72.4 70.9 Zilly et al. (2016) - Variational RHN 32M 71.2 68.5 Zilly et al. (2016) - Tied Variational RHN 24M 68.", "startOffset": 2, "endOffset": 235}, {"referenceID": 2, "context": "Our model was an 8-layer sequence-to-sequence model with attention (Bahdanau et al., 2014).", "startOffset": 67, "endOffset": 90}, {"referenceID": 40, "context": "Each layer had 512 units (compared to 1024 in (Wu et al., 2016)).", "startOffset": 46, "endOffset": 63}, {"referenceID": 42, "context": "Dropout of 30% was applied as described in (Zaremba et al., 2014).", "startOffset": 43, "endOffset": 65}, {"referenceID": 40, "context": "Unlike (Wu et al., 2016), we did not use reinforcement learning to fine-tune our model.", "startOffset": 7, "endOffset": 24}, {"referenceID": 40, "context": "For more details, see (Wu et al., 2016).", "startOffset": 22, "endOffset": 39}, {"referenceID": 38, "context": "For machine translation, we evaluated the confidence penalty on the WMT\u201914 English-to-German translation task using Google\u2019s production-level translation system Wu et al. (2016). The training set consists of 5M sentence pairs, and we used newstest2012 and newtests2013 for validation and newstest2014 for testing.", "startOffset": 161, "endOffset": 178}, {"referenceID": 4, "context": "Model Parameters Validation Test Buck et al. (2014) - PBMT 20.", "startOffset": 33, "endOffset": 52}, {"referenceID": 4, "context": "Model Parameters Validation Test Buck et al. (2014) - PBMT 20.7 Cho et al. (2015) - RNNSearch 16.", "startOffset": 33, "endOffset": 82}, {"referenceID": 4, "context": "Model Parameters Validation Test Buck et al. (2014) - PBMT 20.7 Cho et al. (2015) - RNNSearch 16.9 Zhou et al. (2016) - Deep-Att 20.", "startOffset": 33, "endOffset": 118}, {"referenceID": 4, "context": "Model Parameters Validation Test Buck et al. (2014) - PBMT 20.7 Cho et al. (2015) - RNNSearch 16.9 Zhou et al. (2016) - Deep-Att 20.6 Luong et al. (2015) - P-Attention 164M 20.", "startOffset": 33, "endOffset": 154}, {"referenceID": 4, "context": "Model Parameters Validation Test Buck et al. (2014) - PBMT 20.7 Cho et al. (2015) - RNNSearch 16.9 Zhou et al. (2016) - Deep-Att 20.6 Luong et al. (2015) - P-Attention 164M 20.9 Wu et al. (2016) - WPM-16K 167M 24.", "startOffset": 33, "endOffset": 195}, {"referenceID": 4, "context": "Model Parameters Validation Test Buck et al. (2014) - PBMT 20.7 Cho et al. (2015) - RNNSearch 16.9 Zhou et al. (2016) - Deep-Att 20.6 Luong et al. (2015) - P-Attention 164M 20.9 Wu et al. (2016) - WPM-16K 167M 24.4 Wu et al. (2016) - WPM-32K 278M 24.", "startOffset": 33, "endOffset": 232}, {"referenceID": 41, "context": "Dropout of 15% was applied as described in Zaremba et al. (2014). We trained the model with asynchronous SGD with 5 replicas.", "startOffset": 43, "endOffset": 65}, {"referenceID": 29, "context": "For more details, see Norouzi et al. (2016). For label smoothing, we performed a grid search over values [0.", "startOffset": 22, "endOffset": 44}, {"referenceID": 26, "context": "Model Parameters Validation Test Mohamed et al. (2012) - DNN-HMM 20.", "startOffset": 33, "endOffset": 55}, {"referenceID": 26, "context": "Model Parameters Validation Test Mohamed et al. (2012) - DNN-HMM 20.7 Norouzi et al. (2016) - RML 6.", "startOffset": 33, "endOffset": 92}, {"referenceID": 11, "context": "9 Graves et al. (2006) - CTC 6.", "startOffset": 2, "endOffset": 23}, {"referenceID": 11, "context": "9 Graves et al. (2006) - CTC 6.8M 18.4 Graves et al. (2013) - RNN Transducer 4.", "startOffset": 2, "endOffset": 60}, {"referenceID": 11, "context": "9 Graves et al. (2006) - CTC 6.8M 18.4 Graves et al. (2013) - RNN Transducer 4.3M 17.7 T\u00f3th (2014) - CNN 13.", "startOffset": 2, "endOffset": 99}, {"referenceID": 1, "context": "The encoder of the network consisted of 4 bidirectional LSTM layers each having 256 units, interleaved with 3 time-subsampling layers, configured to drop every second frame (Bahdanau et al., 2016; Chan et al., 2015).", "startOffset": 173, "endOffset": 215}, {"referenceID": 5, "context": "The encoder of the network consisted of 4 bidirectional LSTM layers each having 256 units, interleaved with 3 time-subsampling layers, configured to drop every second frame (Bahdanau et al., 2016; Chan et al., 2015).", "startOffset": 173, "endOffset": 215}, {"referenceID": 25, "context": "We used 240-dimensional vectors consisting of 80-bin filterbank features augmented with their deltas and delta-deltas with per-speaker normalized mean and variances computed with Kaldi Povey et al. (2011). We did not use text-only data or separate language models during decoding.", "startOffset": 185, "endOffset": 205}, {"referenceID": 1, "context": "The encoder of the network consisted of 4 bidirectional LSTM layers each having 256 units, interleaved with 3 time-subsampling layers, configured to drop every second frame (Bahdanau et al., 2016; Chan et al., 2015). The decoder used a single LSTM layer with 256 units. The attention vectors were computed with a single layer feedforward network having 64 hidden units and the convolutional filters as described in Chorowski et al. (2015). Weights were initialized from a uniform distribution [\u22120.", "startOffset": 174, "endOffset": 439}, {"referenceID": 6, "context": "2 to 11) improve over the recently proposed Latent Sequence Decompositions (LSD) method (Chan et al., 2016) which reduces the WER from 14.", "startOffset": 88, "endOffset": 107}, {"referenceID": 1, "context": "3 Bahdanau et al. (2016) - seq2seq 5.", "startOffset": 2, "endOffset": 25}, {"referenceID": 1, "context": "3 Bahdanau et al. (2016) - seq2seq 5.7M 18.6 Chan et al. (2016) - Baseline 5.", "startOffset": 2, "endOffset": 64}, {"referenceID": 1, "context": "3 Bahdanau et al. (2016) - seq2seq 5.7M 18.6 Chan et al. (2016) - Baseline 5.1M 14.7 Chan et al. (2016) - LSD 5.", "startOffset": 2, "endOffset": 104}, {"referenceID": 41, "context": "Motivated by recent successes of output regularizers (Szegedy et al., 2015b; Xie et al., 2016), we conduct a systematic evaluation of two output regularizers: the confidence penalty and label smoothing.", "startOffset": 53, "endOffset": 94}], "year": 2017, "abstractText": "We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT\u201914 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers.", "creator": "LaTeX with hyperref package"}}}