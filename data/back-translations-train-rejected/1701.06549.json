{"id": "1701.06549", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2017", "title": "Learning to Decode for Future Success", "abstract": "We introduce a general strategy for improving neural sequence generation by incorporating knowledge about the future. Our decoder combines a standard sequence decoder with a `soothsayer' prediction function Q that estimates the outcome in the future of generating a word in the present. Our model draws on the same intuitions as reinforcement learning, but is both simpler and higher performing, avoiding known problems with the use of reinforcement learning in tasks with enormous search spaces like sequence generation. We demonstrate our model by incorporating Q functions that incrementally predict what the future BLEU or ROUGE score of the completed sequence will be, its future length, and the backwards probability of the source given the future target sequence. Experimental results show that future rediction yields improved performance in abstractive summarization and conversational response generation and the state-of-the-art in machine translation, while also enabling the decoder to generate outputs that have specific properties.", "histories": [["v1", "Mon, 23 Jan 2017 18:36:37 GMT  (378kb,D)", "https://arxiv.org/abs/1701.06549v1", null], ["v2", "Fri, 3 Feb 2017 21:11:31 GMT  (378kb,D)", "http://arxiv.org/abs/1701.06549v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "will monroe", "dan jurafsky"], "accepted": false, "id": "1701.06549"}, "pdf": {"name": "1701.06549.pdf", "metadata": {"source": "CRF", "title": "Learning to Decode for Future Success", "authors": ["Jiwei Li", "Will Monroe", "Dan Jurafsky"], "emails": ["jiweil@stanford.edu", "wmonroe4@stanford.edu", "jurafsky@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "The question that has arisen in recent years is whether and in what form people are able to survive themselves, or whether they are able to survive themselves. (...) The question is only whether people are able to survive themselves. (...) The question is whether people are able to survive themselves. (...) The question is whether people are able to survive themselves. (...) The question is only whether people are able to survive themselves, to survive themselves. (...) The question is whether they are able to survive themselves. (...) The question is only whether people are able to survive themselves. (...) The question is whether people are able to survive themselves. (...) The question is whether they are able to survive themselves. (...) The question is whether they are able to survive themselves \"as they survive themselves.\" (...) Whether they are able to survive themselves. (...) The question is whether they are able to survive themselves."}, {"heading": "2 Model Overview", "text": "In this section we will first review the basics of training and decoding in standard models of the neural generation. Then we will give a sketch of the proposed model."}, {"heading": "2.1 Basics", "text": "Neural sequence-to-sequence models (SEQ2SEQ) aim to generate a sequence of Y characters (Y input sequence X). Using recursive networks, LSTMs (Hochreiter and Schmidhuber, 1997), or CNNs (Krizhevsky et al., 2012; Kim, 2014), X is initially assigned to a vector representation, which is then used as input for the decoder. A neural generation model defines a distribution of results by generally predicting using a1A workaround tokens, using the linear interpolation of MLE-based policies and the value function for a specific property as a reward for RL training. This strategy has the following drawbacks: It requires training different models for different interpolation weights, and again suffers from large training variance."}, {"heading": "2.2 The Value Function Q", "text": "The core of the proposed architecture is to train a future prediction function (or value function) Q to estimate the future result of taking an action (selecting a token) yt in the present.Function Q is then included in S (yt) at each decryption step to advance the model to generate results leading to future success.This results in the following definition of the score S (yt) of the action yt: S (yt) = log p (yt | ht \u2212 1) + \u03b3Q (X, y1: t) (3), where \u03b3 denotes the hyperparameter that controls the trade-off between the local probability prediction p (yt | ht \u2212 1) and the value sequence Q (X, y1: t).Input to Q can take various forms, such as the vector representation of the decoding step after yt has defined the hyperparameter, which can represent the future trade between the local probability function (i.e. 1) and the local probability function \u2212 1 (i.e."}, {"heading": "3 Q for Controlling Sequence Length", "text": "For tasks such as machine translation, abstract summary, and caption generation, the information needed to generate the target sequences is already embedded in the input. Normally, we do not have to worry about the length of the targets, as the model can figure them out for itself; this is a well-known desirable feature of neural models (Shi et al., 2016).However, for tasks such as generating conversation answers and answering non-factoid questions where there is no single correct answer, it is useful to be able to control the length of the targets. In addition, SEQ2SEQ models have a strong tendency to generate short sequences for tasks such as conversation response generation (Sountsov and Sarawagi, 2016), as the standard search algorithm can only afford to explore a very small amount of action space when decoding time. As decoding progresses, only a small number of hypotheses can be maintained."}, {"heading": "3.1 Training Q for Sequence Length", "text": "Shao et al. (2017) provide an efficient method of generating long sequences, consisting of a stochastic search algorithm and a segment-by-segment recalculation of hypotheses. The basic idea is to keep a diverse list of hypotheses on the beam and remove those that are similar to each other in order to explore the space more appropriately. While a more appropriate exploration of the search space can increase the likelihood of generating long sequences, since the beam is more likely to contain a prefix of a long sequence, this method does not offer direct control over the sequence length. Length information appears to be implicitly embedded in the hidden representations of neuronal models (Shi et al., 2016). We are therefore building another neural model to expose this length information and use it to estimate the number of words remaining to generate the sequence length."}, {"heading": "3.2 Decoding", "text": "When decoding step t \u2212 1, we first get the ht \u2212 1 vector representation for the current time step. The score used to rank the decisions for the next token yt is a linear combination of the log probability output from the sequence model and the mean square losses between the number of remaining words (N \u2212 t) and the output of Q (ht): yt = argmax ylog p (y1: t | X) \u2212 \u03bb | | (N \u2212 t) \u2212 Q (ht) | | 2 (4), where y1: t is the concatenation of yt and the previously decoded (given) sequence y1: t \u2212 1, and ht is derived from the sequence model by combining ht \u2212 1 and the word representation of yt as if it were the true next token. This is a hyperparameter that controls the influence of the future length estimator."}, {"heading": "3.3 Experiments", "text": "We evaluate the proposed model for the task of Open Domain Conversation Response Generation (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Mei et al., 2016; Serban et al., 2015b), in which a model must predict the next round of a dialogue as the predicted targets of each group are identified. We use the OpenSubtitles (OSDb) Dataset (Tiedemann, 2009).We compare the proposed model with the standard SEQ2SEQ Radiation Search (SBS) decoder first group test pairs by target length and decoding of each group. At decoding time, for an input with a gold target of length L, we force the model to generate an output of length L. This can be achieved by selecting a hypothesis that predicts an EOS request at time step L + 1.3, if no EOS request is predicted."}, {"heading": "4 Q for Mutual Information", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Background", "text": "Maximum Mutual Information (MMI) has been shown to be better than maximum probability estimation (MLE) as a decoding target for conversation response generation tasks (Li et al., 2016a). Mutual information between source X and target Y is given by log [p (X, Y) / p (X) p (Y)], which measures bidirectional dependence between sources and targets, as opposed to unidirectional dependence on targets on sources in the maximum probability target. Modelling the bidirectional dependence between sources and targets reduces the prevalence of generic responses and leads to more diverse and interesting conversations. 6 Maximizing a weighted generalization of mutual information between source and target can be shown to be equivalent to maximizing a linear combination of forward probability protocol p (Y | X)."}, {"heading": "4.2 Training Q for Mutual Information", "text": "We therefore focus our attention on the second term, log p (X | Y). To integrate the backward probability into intermediate decoding steps, we use a model to determine the future value of p (X | Y) when generating each token y.For example, we assume that we have a source pair with source code X = \"what's your name\" and target Y = \"my name is john.\" The future backward probability of partial sequences \"my,\" \"my name,\" \"my name,\" is thus p (X | Y). Again, we use Q (yt) to name the function that maps a partially generated sequence to its future backward probability, and we can factorize QQ5 as follows: yt = argmax ylog yp (y1: t \u2212 1, y | X)."}, {"heading": "4.3 Experimental Results", "text": "We compare the results of the approach with the standard beam search using the MLE target and the MMI reranking approach of Li et al. (2016a), which only performs reranking after decoding is complete. We report BLEU scores and AdverSuc scores8 on the test set. We also report on diversity values (denoted by Distinct-1 and Distinct-2), which are defined as in Li et al. (2016a) to be the number of unique unigrams and bigrams in the generated responses, divided by the total number of tokens generated (to avoid long sentences). Addi-8The machine vs random scores for the three models are very similar, or 0.947, 0.939, 0.935.5, we divide the dev set into a subset containing longer targets (with a length greater than 8) and a subset containing shorter than 8."}, {"heading": "5 Q for BLEU/ROUGE", "text": "The future result function Q can be trained to predict any characteristics, including BLEU- (Papineni et al., 2002) or ROUGE-values (Lin, 2004). So we train Q to directly predict future BLEU- or ROUGE-values. In this situation, the future prediction function is able to reduce the discrepancy between training (with maximum probability target) and tests (with BLEU or ROUGE) (Wiseman and Rush, 2016; Shen et al., 2015; Ranzato et al., 2016)."}, {"heading": "5.1 Model", "text": "Using a pre-trained sequence generation model, an input sequence X, and a partially decoded sequence y1: t \u2212 1, we will estimate the future reward for selecting the word yt for the current time step. We call this estimate Q ({yt, y1: t \u2212 1, X}), abbreviated to Q (yt) where possible. The future prediction network is trained as follows: We first try yt from the distribution p (yt | X, y1: t \u2212 1), and then decipher the rest of the sequence Y using the beam search. Thus, the future result for the action yt is the score of the final decoded sequence q (Y). After obtaining pairs (q (Y), {X, y1: t}), we train a neural network model that takes X and y1: t as input to predict q (Y)."}, {"heading": "5.2 Experiments", "text": "We are the first ones we put into the world to save the world, \"he said.\" We are the first ones we put into the world to save the world, \"he said.\" We are the first ones we put into the world, \"he said.\" We are the first ones we put into the world, \"he said.\" We are the first ones we put into the world, \"he said.\" We are the first ones we put into the world, \"he said.\" We are the first ones we put into the world. \""}, {"heading": "6 Conclusion", "text": "In this paper, we propose a general strategy that allows a neural decoder to generate output with certain interesting properties. We show how to use a Model Q to optimize three useful properties of output - sequence length, mutual information, and BLEU / ROUGE values - and examine the impact of different designs on the predictor model and decoding algorithm. Our model provides a general and easy-to-implement way to control neural generation models to meet their specific needs, while improving results for a variety of generation tasks."}], "references": [{"title": "An actor-critic algorithm for sequence prediction", "author": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1607.07086 .", "citeRegEx": "Bahdanau et al\\.,? 2016", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Report on the 11th IWSLT evaluation campaign, IWSLT 2014", "author": ["Mauro Cettolo", "Jan Niehues", "Sebastian St\u00fcker", "Luisa Bentivogli", "Marcello Federico."], "venue": "Proceedings of the International Workshop on Spoken Language Translation, Hanoi, Vietnam.", "citeRegEx": "Cettolo et al\\.,? 2014", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Microsoft COCO captions: Data collection and evaluation server", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "arXiv preprint arXiv:1504.00325 .", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Noisy parallel approximate decoding for conditional recurrent language model", "author": ["Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1605.03835 .", "citeRegEx": "Cho.,? 2016", "shortCiteRegEx": "Cho.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Alexander M. Rush."], "venue": "NAACL-HLT .", "citeRegEx": "Chopra et al\\.,? 2016", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "A systematic exploration of diversity in machine translation", "author": ["Kevin Gimpel", "Dhruv Batra", "Chris Dyer", "Gregory Shakhnarovich."], "venue": "EMNLP.", "citeRegEx": "Gimpel et al\\.,? 2013", "shortCiteRegEx": "Gimpel et al\\.", "year": 2013}, {"title": "A survey of actor-critic reinforcement learning: Standard and natural policy gradients", "author": ["Ivo Grondman", "Lucian Busoniu", "Gabriel AD Lopes", "Robert Babuska."], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)", "citeRegEx": "Grondman et al\\.,? 2012", "shortCiteRegEx": "Grondman et al\\.", "year": 2012}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1503.03535 .", "citeRegEx": "Gulcehre et al\\.,? 2015", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882 .", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "NIPS.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "2016a. A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A simple, fast diverse decoding algorithm for neural generation", "author": ["Jiwei Li", "Will Monroe", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1611.08562 .", "citeRegEx": "Li et al\\.,? 2016b", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Adversarial reinforcement learning for neural dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "Dan Jurafsky"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "ROUGE: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "LSTM based conversation models", "author": ["Yi Luan", "Yangfeng Ji", "Mari Ostendorf."], "venue": "arXiv preprint arXiv:1603.09457 .", "citeRegEx": "Luan et al\\.,? 2016", "shortCiteRegEx": "Luan et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "EMNLP.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Coherent dialogue with attention-based language models", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R Walter."], "venue": "arXiv preprint arXiv:1611.06997 .", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Abstractive text summarization using sequence-to-sequence RNNs and beyond", "author": ["Ramesh Nallapati", "Bowen Zhou", "\u00c7a\u011flar G\u00fcl\u00e7ehre", "Bing Xiang."], "venue": "CoNLL.", "citeRegEx": "Nallapati et al\\.,? 2016", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "An efficient A* search algorithm for statistical machine translation", "author": ["Franz Josef Och", "Nicola Ueffing", "Hermann Ney."], "venue": "Proceedings of the Workshop on Data-Driven Methods in Machine Translation.", "citeRegEx": "Och et al\\.,? 2001", "shortCiteRegEx": "Och et al\\.", "year": 2001}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "ACL.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Sequence level training with recurrent neural networks. In ICLR", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1509.00685 .", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909 .", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1507.04808 .", "citeRegEx": "Serban et al\\.,? 2015a", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1507.04808 .", "citeRegEx": "Serban et al\\.,? 2015b", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Generating long and diverse responses with neural conversational models", "author": ["Louis Shao", "Stephan Gouws", "Denny Britz", "Anna Goldie", "Brian Strope", "Ray Kurzweil."], "venue": "arXiv preprint arXiv:1701.03185 .", "citeRegEx": "Shao et al\\.,? 2017", "shortCiteRegEx": "Shao et al\\.", "year": 2017}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "arXiv preprint arXiv:1512.02433 .", "citeRegEx": "Shen et al\\.,? 2015", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Why neural translations are the right length", "author": ["Xing Shi", "Kevin Knight", "Deniz Yuret."], "venue": "EMNLP.", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Length bias in encoder decoder models and a case for global conditioning", "author": ["Pavel Sountsov", "Sunita Sarawagi."], "venue": "arXiv preprint arXiv:1606.03402 .", "citeRegEx": "Sountsov and Sarawagi.,? 2016", "shortCiteRegEx": "Sountsov and Sarawagi.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Richard S Sutton."], "venue": "Machine learning 3(1):9\u201344.", "citeRegEx": "Sutton.,? 1988", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "News from OPUS \u2013 A collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "Recent Advances in Natural Language Processing. volume 5, pages 237\u2013248.", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "Diverse beam", "author": ["Ashwin K Vijayakumar", "Michael Cogswell", "Ramprasath R Selvaraju", "Qing Sun", "Stefan Lee", "David Crandall", "Dhruv Batra"], "venue": null, "citeRegEx": "Vijayakumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vijayakumar et al\\.", "year": 2016}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "Proceedings of the ICML Deep Learning Workshop.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Sequence-to-sequence learning as beam-search optimization", "author": ["Sam Wiseman", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1606.02960 .", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}], "referenceMentions": [{"referenceID": 36, "context": "Neural generation models (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013) learn to map source to target sequences in applications such as machine translation (Sennrich et al.", "startOffset": 25, "endOffset": 122}, {"referenceID": 1, "context": "Neural generation models (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013) learn to map source to target sequences in applications such as machine translation (Sennrich et al.", "startOffset": 25, "endOffset": 122}, {"referenceID": 5, "context": "Neural generation models (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013) learn to map source to target sequences in applications such as machine translation (Sennrich et al.", "startOffset": 25, "endOffset": 122}, {"referenceID": 11, "context": "Neural generation models (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013) learn to map source to target sequences in applications such as machine translation (Sennrich et al.", "startOffset": 25, "endOffset": 122}, {"referenceID": 26, "context": ", 2014; Kalchbrenner and Blunsom, 2013) learn to map source to target sequences in applications such as machine translation (Sennrich et al., 2015; Gulcehre et al., 2015), conversational response generation (Vinyals and Le, 2015; Sordoni et al.", "startOffset": 124, "endOffset": 170}, {"referenceID": 9, "context": ", 2014; Kalchbrenner and Blunsom, 2013) learn to map source to target sequences in applications such as machine translation (Sennrich et al., 2015; Gulcehre et al., 2015), conversational response generation (Vinyals and Le, 2015; Sordoni et al.", "startOffset": 124, "endOffset": 170}, {"referenceID": 40, "context": ", 2015), conversational response generation (Vinyals and Le, 2015; Sordoni et al., 2015), abstractive summarization (Nallapati et al.", "startOffset": 44, "endOffset": 88}, {"referenceID": 34, "context": ", 2015), conversational response generation (Vinyals and Le, 2015; Sordoni et al., 2015), abstractive summarization (Nallapati et al.", "startOffset": 44, "endOffset": 88}, {"referenceID": 21, "context": ", 2015), abstractive summarization (Nallapati et al., 2016; Rush et al., 2015).", "startOffset": 35, "endOffset": 78}, {"referenceID": 25, "context": ", 2015), abstractive summarization (Nallapati et al., 2016; Rush et al., 2015).", "startOffset": 35, "endOffset": 78}, {"referenceID": 30, "context": "This locally incremental nature of the decoding model leads to the following issueL Decoders cannot be tailored to generate target sequences with specific properties of interest, such as pre-specified length constraints (Shao et al., 2017; Shi et al., 2016), which might be useful in tasks like conversational response generation or non-factoid question answering, and cannot deal with important objectives, such as the mutual information between sources and targets (Li et al.", "startOffset": 220, "endOffset": 257}, {"referenceID": 32, "context": "This locally incremental nature of the decoding model leads to the following issueL Decoders cannot be tailored to generate target sequences with specific properties of interest, such as pre-specified length constraints (Shao et al., 2017; Shi et al., 2016), which might be useful in tasks like conversational response generation or non-factoid question answering, and cannot deal with important objectives, such as the mutual information between sources and targets (Li et al.", "startOffset": 220, "endOffset": 257}, {"referenceID": 41, "context": ", REINFORCE or actorcritic models) for sequence generation (Wiseman and Rush, 2016; Shen et al., 2015; Bahdanau et al., 2016; Ranzato et al., 2016).", "startOffset": 59, "endOffset": 147}, {"referenceID": 31, "context": ", REINFORCE or actorcritic models) for sequence generation (Wiseman and Rush, 2016; Shen et al., 2015; Bahdanau et al., 2016; Ranzato et al., 2016).", "startOffset": 59, "endOffset": 147}, {"referenceID": 0, "context": ", REINFORCE or actorcritic models) for sequence generation (Wiseman and Rush, 2016; Shen et al., 2015; Bahdanau et al., 2016; Ranzato et al., 2016).", "startOffset": 59, "endOffset": 147}, {"referenceID": 24, "context": ", REINFORCE or actorcritic models) for sequence generation (Wiseman and Rush, 2016; Shen et al., 2015; Bahdanau et al., 2016; Ranzato et al., 2016).", "startOffset": 59, "endOffset": 147}, {"referenceID": 35, "context": "(Sountsov and Sarawagi, 2016); (2) mutual information between sources and targets: the approach enables modeling the bidirectional dependency between sources and targets at each decoding timestep, significantly improving response quality on a task of conversational response generation and (3) the properties can also take the form of the BLEU and ROUGE scores, yielding consistent improvements in machine translation and summarization, yielding the state-of-the-art result on the IWSLT German-English translation task.", "startOffset": 0, "endOffset": 29}, {"referenceID": 10, "context": "Using recurrent nets, LSTMs (Hochreiter and Schmidhuber, 1997) or CNNs (Krizhevsky et al.", "startOffset": 28, "endOffset": 62}, {"referenceID": 13, "context": "Using recurrent nets, LSTMs (Hochreiter and Schmidhuber, 1997) or CNNs (Krizhevsky et al., 2012; Kim, 2014), X is first mapped to a vector representation, which is then used as the initial input to the decoder.", "startOffset": 71, "endOffset": 107}, {"referenceID": 12, "context": "Using recurrent nets, LSTMs (Hochreiter and Schmidhuber, 1997) or CNNs (Krizhevsky et al., 2012; Kim, 2014), X is first mapped to a vector representation, which is then used as the initial input to the decoder.", "startOffset": 71, "endOffset": 107}, {"referenceID": 37, "context": "It is thus similar to the value function in Q-learning, the role of the critic in actor-critic reinforcement learning (Sutton, 1988; Grondman et al., 2012), the value network for position evaluation in the Monte-Carlo tree search of AlphaGo (Silver et al.", "startOffset": 118, "endOffset": 155}, {"referenceID": 8, "context": "It is thus similar to the value function in Q-learning, the role of the critic in actor-critic reinforcement learning (Sutton, 1988; Grondman et al., 2012), the value network for position evaluation in the Monte-Carlo tree search of AlphaGo (Silver et al.", "startOffset": 118, "endOffset": 155}, {"referenceID": 33, "context": ", 2012), the value network for position evaluation in the Monte-Carlo tree search of AlphaGo (Silver et al., 2016), or the h* function in A\u2217 search.", "startOffset": 93, "endOffset": 114}, {"referenceID": 22, "context": "(Och et al., 2001).", "startOffset": 0, "endOffset": 18}, {"referenceID": 32, "context": "Usually we don\u2019t have to worry about the length of targets, since the model can figure it out itself; this is a known, desirable property of neural generation models (Shi et al., 2016).", "startOffset": 166, "endOffset": 184}, {"referenceID": 35, "context": "Additionally, in tasks like conversational response generation, SEQ2SEQ models have a strong bias towards generating short sequences (Sountsov and Sarawagi, 2016).", "startOffset": 133, "endOffset": 162}, {"referenceID": 32, "context": "Length information seems to be embedded in the hidden representations of neural models in some implicit way (Shi et al., 2016).", "startOffset": 108, "endOffset": 126}, {"referenceID": 40, "context": "(Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Mei et al., 2016; Serban et al., 2015b), in which a model must predict the next turn of a dialogue given the preceding ones.", "startOffset": 0, "endOffset": 106}, {"referenceID": 34, "context": "(Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Mei et al., 2016; Serban et al., 2015b), in which a model must predict the next turn of a dialogue given the preceding ones.", "startOffset": 0, "endOffset": 106}, {"referenceID": 27, "context": "(Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Mei et al., 2016; Serban et al., 2015b), in which a model must predict the next turn of a dialogue given the preceding ones.", "startOffset": 0, "endOffset": 106}, {"referenceID": 20, "context": "(Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Mei et al., 2016; Serban et al., 2015b), in which a model must predict the next turn of a dialogue given the preceding ones.", "startOffset": 0, "endOffset": 106}, {"referenceID": 28, "context": "(Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2015a; Mei et al., 2016; Serban et al., 2015b), in which a model must predict the next turn of a dialogue given the preceding ones.", "startOffset": 0, "endOffset": 106}, {"referenceID": 38, "context": "We use the OpenSubtitles (OSDb) dataset (Tiedemann, 2009).", "startOffset": 40, "endOffset": 57}, {"referenceID": 14, "context": "We also report adversarial success (AdverSuc) and machine-vs-random accuracy, evaluation metrics proposed in Li et al. (2016c). Adversarial success refers to the percentage of machine-generated responses that are able to fool a trained evaluator model into believing that they are generated by a human; machine-vs-random accuracy denotes the accuracy of a (different) trained evaluator model at distinguishing between machinegenerated responses and randomly-sampled responses.", "startOffset": 109, "endOffset": 127}, {"referenceID": 14, "context": "We also report adversarial success (AdverSuc) and machine-vs-random accuracy, evaluation metrics proposed in Li et al. (2016c). Adversarial success refers to the percentage of machine-generated responses that are able to fool a trained evaluator model into believing that they are generated by a human; machine-vs-random accuracy denotes the accuracy of a (different) trained evaluator model at distinguishing between machinegenerated responses and randomly-sampled responses.5 Higher values of adversarial success and machine-vs-random accuracy indicate the superiority of a model. We refer readers to Li et al. (2016c) for more details.", "startOffset": 109, "endOffset": 621}, {"referenceID": 29, "context": "The estimators for AdverSuc and machine-vs-random accuracy are trained using a hierarchical network (Serban et al., 2016) See Li et al.", "startOffset": 100, "endOffset": 121}, {"referenceID": 14, "context": ", 2016) See Li et al. (2016c) for details.", "startOffset": 12, "endOffset": 30}, {"referenceID": 15, "context": "Since hypotheses in beam search are known to lack diversity (Li et al., 2016b; Vijayakumar et al., 2016; Gimpel et al., 2013), after decoding is finished, it is sometimes too late for the reranking model to have significant impact.", "startOffset": 60, "endOffset": 125}, {"referenceID": 39, "context": "Since hypotheses in beam search are known to lack diversity (Li et al., 2016b; Vijayakumar et al., 2016; Gimpel et al., 2013), after decoding is finished, it is sometimes too late for the reranking model to have significant impact.", "startOffset": 60, "endOffset": 125}, {"referenceID": 7, "context": "Since hypotheses in beam search are known to lack diversity (Li et al., 2016b; Vijayakumar et al., 2016; Gimpel et al., 2013), after decoding is finished, it is sometimes too late for the reranking model to have significant impact.", "startOffset": 60, "endOffset": 125}, {"referenceID": 7, "context": ", 2016; Gimpel et al., 2013), after decoding is finished, it is sometimes too late for the reranking model to have significant impact. Shao et al. (2017) confirm this problem and show that the reranking approach helps for short sequences but not longer ones.", "startOffset": 8, "endOffset": 154}, {"referenceID": 14, "context": "We compare the results for the approach with standard beam search using the MLE objective and the MMI reranking approach of Li et al. (2016a), which performs reranking only after decoding is finished.", "startOffset": 124, "endOffset": 142}, {"referenceID": 14, "context": "We compare the results for the approach with standard beam search using the MLE objective and the MMI reranking approach of Li et al. (2016a), which performs reranking only after decoding is finished. We report BLEU scores and AdverSuc scores8 on the test set. We also report diversity scores (denoted by Distinct-1 and Distinct-2); these are defined as in Li et al. (2016a) to be the the number of distinct unigrams and bigrams (respectively) in generated responses, divided by the total number of generated tokens (to avoid favoring long sentences).", "startOffset": 124, "endOffset": 375}, {"referenceID": 14, "context": "Table 4: Sample of responses generated by (1) standard beam search (SBS); (2) the MMI reranking approach of Li et al. (2016a), which performs reranking only after decoding is complete (denoted by MMI); and (3) the future prediction model Q(MMI) with different values of future prediction weight \u03bb.", "startOffset": 108, "endOffset": 126}, {"referenceID": 23, "context": "These features include BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) scores.", "startOffset": 28, "endOffset": 51}, {"referenceID": 17, "context": ", 2002) or ROUGE (Lin, 2004) scores.", "startOffset": 17, "endOffset": 28}, {"referenceID": 41, "context": "In this situation, the future prediction function is able to reduce the discrepancy between training (using maximum likelihood objective) and testing (using BLEU or ROUGE) (Wiseman and Rush, 2016; Shen et al., 2015; Ranzato et al., 2016).", "startOffset": 172, "endOffset": 237}, {"referenceID": 31, "context": "In this situation, the future prediction function is able to reduce the discrepancy between training (using maximum likelihood objective) and testing (using BLEU or ROUGE) (Wiseman and Rush, 2016; Shen et al., 2015; Ranzato et al., 2016).", "startOffset": 172, "endOffset": 237}, {"referenceID": 24, "context": "In this situation, the future prediction function is able to reduce the discrepancy between training (using maximum likelihood objective) and testing (using BLEU or ROUGE) (Wiseman and Rush, 2016; Shen et al., 2015; Ranzato et al., 2016).", "startOffset": 172, "endOffset": 237}, {"referenceID": 2, "context": "Machine Translation We use the GermanEnglish machine translation track of the IWSLT 2014 (Cettolo et al., 2014), which consists of sentence-aligned subtitles of TED and TEDx talks.", "startOffset": 89, "endOffset": 111}, {"referenceID": 0, "context": "Machine Translation We use the GermanEnglish machine translation track of the IWSLT 2014 (Cettolo et al., 2014), which consists of sentence-aligned subtitles of TED and TEDx talks. For fair comparison, we followed exactly the data processing protocols defined in Ranzato et al. (2016), which have also been adopted by Bahdanau et al.", "startOffset": 90, "endOffset": 285}, {"referenceID": 0, "context": "(2016), which have also been adopted by Bahdanau et al. (2016) and Wiseman and Rush (2016).", "startOffset": 40, "endOffset": 63}, {"referenceID": 0, "context": "(2016), which have also been adopted by Bahdanau et al. (2016) and Wiseman and Rush (2016). The training data consists of roughly 150K sentence pairs, in which the average English sentence is 17.", "startOffset": 40, "endOffset": 91}, {"referenceID": 36, "context": "We train two models, a vanilla LSTM (Sutskever et al., 2014) and an attention-based model (Bahdanau et al.", "startOffset": 36, "endOffset": 60}, {"referenceID": 1, "context": ", 2014) and an attention-based model (Bahdanau et al., 2015).", "startOffset": 37, "endOffset": 60}, {"referenceID": 0, "context": ", 2014) and an attention-based model (Bahdanau et al., 2015). For the attention model, we use the input-feeding model described in Luong et al. (2015) with one minor modification: the weighted attention vectors that are used in the softmax token predictions and those fed to the recurrent net at the next step use different sets of parameters.", "startOffset": 38, "endOffset": 151}, {"referenceID": 0, "context": ", 2014) and an attention-based model (Bahdanau et al., 2015). For the attention model, we use the input-feeding model described in Luong et al. (2015) with one minor modification: the weighted attention vectors that are used in the softmax token predictions and those fed to the recurrent net at the next step use different sets of parameters. Their values can therefore be different, unlike in Luong et al. (2015). We find that this small modification significantly improves the capacity of attention models, yielding more than a +1.", "startOffset": 38, "endOffset": 415}, {"referenceID": 0, "context": ", 2014) and an attention-based model (Bahdanau et al., 2015). For the attention model, we use the input-feeding model described in Luong et al. (2015) with one minor modification: the weighted attention vectors that are used in the softmax token predictions and those fed to the recurrent net at the next step use different sets of parameters. Their values can therefore be different, unlike in Luong et al. (2015). We find that this small modification significantly improves the capacity of attention models, yielding more than a +1.0 BLEU score improvement. We use structure similar to that of Wiseman and Rush (2016), a single-layer sequenceto-sequence model with 256 units for each layer.", "startOffset": 38, "endOffset": 620}, {"referenceID": 24, "context": "(2016) and the beam-search training scheme described in WiseREINFORCE (Ranzato et al., 2016) 20.", "startOffset": 70, "endOffset": 92}, {"referenceID": 0, "context": "7 Actor-Critic (Bahdanau et al., 2016) 22.", "startOffset": 15, "endOffset": 38}, {"referenceID": 15, "context": "Baselines employed include the REINFORCE model described in Ranzato et al. (2016), the actor-critic RL model described in Bahdanau et al.", "startOffset": 4, "endOffset": 82}, {"referenceID": 0, "context": "(2016), the actor-critic RL model described in Bahdanau et al. (2016) and the beam-search training scheme described in WiseREINFORCE (Ranzato et al.", "startOffset": 47, "endOffset": 70}, {"referenceID": 0, "context": "(2016), the actor-critic RL model described in Bahdanau et al. (2016) and the beam-search training scheme described in WiseREINFORCE (Ranzato et al., 2016) 20.7 Actor-Critic (Bahdanau et al., 2016) 22.4 Wiseman and Rush (2016) 26.", "startOffset": 47, "endOffset": 227}, {"referenceID": 24, "context": "Abstractive Summarization We follow the protocols described in Rush et al. (2015), in which the source input is the first sentence of a new article and the target output is the headline.", "startOffset": 63, "endOffset": 82}], "year": 2017, "abstractText": "We introduce a simple, general strategy to manipulate the behavior of a neural decoder that enables it to generate outputs that have specific properties of interest (e.g., sequences of a pre-specified length). The model can be thought of as a simple version of the actor-critic model that uses an interpolation of the actor (the MLE-based token generation policy) and the critic (a value function that estimates the future values of the desired property) for decision making. We demonstrate that the approach is able to incorporate a variety of properties that cannot be handled by standard neural sequence decoders, such as sequence length and backward probability (probability of sources given targets), in addition to yielding consistent improvements in abstractive summarization and machine translation when the property to be optimized is BLEU or ROUGE scores.", "creator": "LaTeX with hyperref package"}}}