{"id": "1701.06641", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2017", "title": "Perceptually Optimized Image Rendering", "abstract": "We develop a framework for rendering photographic images, taking into account display limitations, so as to optimize perceptual similarity between the rendered image and the original scene. We formulate this as a constrained optimization problem, in which we minimize a measure of perceptual dissimilarity, the Normalized Laplacian Pyramid Distance (NLPD), which mimics the early stage transformations of the human visual system. When rendering images acquired with higher dynamic range than that of the display, we find that the optimized solution boosts the contrast of low-contrast features without introducing significant artifacts, yielding results of comparable visual quality to current state-of-the art methods with no manual intervention or parameter settings. We also examine a variety of other display constraints, including limitations on minimum luminance (black point), mean luminance (as a proxy for energy consumption), and quantized luminance levels (halftoning). Finally, we show that the method may be used to enhance details and contrast of images degraded by optical scattering (e.g. fog).", "histories": [["v1", "Mon, 23 Jan 2017 21:38:52 GMT  (3851kb,D)", "http://arxiv.org/abs/1701.06641v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.GR", "authors": ["valero laparra", "alex berardino", "johannes ball\\'e", "eero p simoncelli"], "accepted": false, "id": "1701.06641"}, "pdf": {"name": "1701.06641.pdf", "metadata": {"source": "CRF", "title": "Perceptually Optimized Image Rendering", "authors": ["Valero Laparra", "Alex Berardino", "Johannes Ball\u00e9", "Eero P. Simoncelli"], "emails": ["valero.laparra@uv.es"], "sections": [{"heading": null, "text": "We are developing a framework for reproducing photographic images, taking into account imaging limitations, in order to optimize the perception similarity between rendered image and original scene. We formulate this as a limited optimization problem, where we minimize a measure of imaging dissimilarity, the Normalized Laplacian Pyramid Distance (NLPD), which mimics early-stage human visual system transformations. In reproducing images taken with a higher dynamic range than the display, we find that the optimized solution enhances contrast of high-contrast features without introducing significant artifacts, resulting in results of comparable visual quality without manual intervention or parameter adjustments. Finally, we are investigating a variety of other imaging limitations, including limitations on minimum luminance (black dot), average luminance (representing energy consumption), and quantified luminance (refraction)."}, {"heading": "1 Introduction", "text": "This year, we have reached the point where we see ourselves as being able to live in a country where most people are able to move, and where they are able to unfold, to unfold."}, {"heading": "2 Optimal rendering framework", "text": "We choose a perceptual measure that can be differentiated in relation to me, and we use modern high-dimensional optimization tools to numerically solve the optimal image function that we are able to present the image in a way that it has previously adapted in such a way that it remains true to the human perception of the original scene. We formalize this as a limited optimization problem: I can describe many known rendering problems, such as tone imaging or dithering, which is only in the specification of C. In general, the optimization problem expressed in equation cannot be solved analytically, and therefore we will not have an explicit function to calculate it, given S and C. Instead of adopting a functional form for this mapping, we choose a perceptual measure that can be differentiated in relation to me, and use modern high-dimensional optimization tools to resolve the optimal image."}, {"heading": "3 Varying image acquisition conditions", "text": "We performed a series of experiments demonstrating the flexibility of our optimization frame over different image recording conditions, starting with calibrated images for which we know the exact luminance values (cd / m2) in the original scene, then moving on to uncalibrated images for which we have to make an assumption about the luminance values in the original scene, and finally concluding this section by demonstrating that we can manipulate our assumptions about the original scene luminance to solve other rendering problems, such as haze removal and artificial detail improvement. Each example requires us to minimize the perception distance in relation to the rendered image I, subject to display limitations, using the Adaptive Moment Estimation (Adam) algorithm. Deriving the perception distance in relation to me is described in Appendix A. The calculation time scales are linear with the size of the image. Optimizing on a Tesla GU-1000 image point (optimization of 2 seconds took less than 1000 KU)."}, {"heading": "3.1 Rendering calibrated HDR luminances", "text": "The image used here has been extracted from the database of Mark Fairchild [6], and its brightness range is Smin = 0.78 to Smax = 16200 cd / m2. Let us further assume that we want to render this image with a device with a brightness range of Imin = 5 to Imax = 300 cd / m2 (typical of many computer monitors), and that this range is much lower than that of the image. We solve for the perceptively optimal rendered image: I assume that we will render an ID (S, I), s.t. i: Imin \u2264 Ii \u2264 Imax. (7) Results are shown in Fig. 4. We compare the original image intensities, linearly rescaled, to fit within the brightness range [Imin, Imax], to render our perceptibly optimized image, and an image tone mapped with a current state-of-art procedure, Paris etatized, not in detail."}, {"heading": "3.2 Rendering LDR images with an image acquisition model", "text": "Our method can also be used to improve the appearance of images taken with a conventional low dynamic range (LDR) digital camera, which has been calibrated to allow brightness values (in cd / m2) to be restored from recorded pixel values. Generally, for most modern digital cameras, the detection range is still much larger than the display range and is unlikely to match in any case. Therefore, we need to solve the following optimization problem analogous to the previous section: I = argmin ID (S, I), s.t.: i: Imin \u2264 Ii \u2264 Imax (8), where S = g (R), where g is the mapping of recorded pixel values to estimated scene illumination values. Results for two grayscale images from the McGill database [20] are shown in Fig. 5. For each image, we compare the original image intensities anew, linearly converted to fit within the luminance range [Immax]."}, {"heading": "3.3 Rendering uncalibrated HDR images", "text": "In contrast to the situation in Section 3.1, the typical scenario for images taken by HDR cameras is that they are uncalibrated, which means that we have access to measurements L linearly relative to the actual brightness values, but do not have access to the scaling parameters (for example, they could be normalized values ranging from 0 to 1).In order to apply our method, the measurements must be converted linearly to the brightness values, which amounts to knowing, estimating or guessing the minimum and maximum brightness in the original scene (Smin or Smax). Frequently, an educated guess can be used for these values - for example, the brightness of a filament of a clear light bulb is about 106 cd / m2. As in the previous experiments, we solve the resulting optimization problem for the resulting optimization problem: I = argmin ID (S, I), s.t."}, {"heading": "3.4 Artificial detail enhancement and haze removal", "text": "We have shown in the preceding sections that the use of the knowledge we have about the imaging process is very helpful in the automatic recovery of images that are optimized to look like the original scene, given the limitations of screen representation. In some cases, however, the detail visibility in the scene could be unsatisfactory. Intuitively, the photographers know that the amount of detail visible in a scene depends on the amount of light available. If the image has already been captured, it is of course not possible to change the light sources. Since the brightness of the scene, however, scales linearly with the intensity of the light sources, our method allows us to simulate more light post hoc by recalculating the light sources of the scene linearly. Figure 7 shows the results of the modification of our choice of Smax (as in the previous experiment Smin = 0.0001). Note that with increasing values of Smax details become more visible, 0.00000000.0000.0000.0000.0000.00.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.0000.00.0000.0000.00.0000.0000.00.000000.00.00.000000.00.00.00000000.00.00.00.00.000000000000.00.00.00.00.00.0000000000.00.00.00.00.000000000000.00.00.00.00.00.0000000000000000000000.00.00.00.00.00.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"}, {"heading": "4 Varying display constraints", "text": "While in the previous section we examined the effects of different image acquisition scenarios, we only assumed that the luminance of the display is limited; the upper limit is a natural limit for all real displays; and the lower limit is also relevant for a wide range of practical displays, resulting from reflected ambient light and dispersion within the display. In this section, we examine the effects of each of these limitations independently, along with a few more complex limitations. Figure 9 shows the results for different maximum (Imax) and minimum (Imin) luminance limits. Our method amplifies local contrast, whereas recalibration can only manipulate contrast globally. For a variety of display features, optimizing the image to minimize the NLP distance reduces distortion in the rendered images and increases the visibility of perceptual features."}, {"heading": "4.1 Rendering with limited energy consumption", "text": "We assume that the energy consumption is proportional to the luminance, such as in organic light-emitting diodes (OLED) displays used in mobile phones. Thus, we keep the luminance in a lower and upper barrier, while at the same time maintaining the mean luminance constant: I value = Argmin ID (S, I), s.t.: Imin \u2264 Ii \u2264 Imax (10) and 1Ni \u2211 iIi = ImeanFigure 10 shows images optimized for different mean luminance compared to images that are rescaled linearly to achieve the same target average brightness. It is clear that our optimized images retain more detail from the original scene. Figure 11 shows two graphs comparing the visual appearance and energy consumption for both methods."}, {"heading": "4.2 Rendering with a discrete set of gray levels (dithering)", "text": "Most displays have a limited number of available grayscales, but in extreme cases there can be only two (e.g. black and white printers, e-ink devices, etc.).Here we illustrate that the proposed method is flexible enough to achieve good results even under such extreme limitations.The optimization problem is the same as before, but here we limit the pixel values to be derived from a discrete set: Instead, we use a greedy error-diffusion algorithm, analogous to the classic Floyd-Steinberg method. We first initialize the image to the continuous solution of the optimization problem that is achieved for a continuous range of luminances, as in previous experiments, and then select the discrete value for each pixel image."}, {"heading": "5 Contribution of perceptual metric components", "text": "In order to give an intuition about the effect of the individual primary components of the NLP, we also optimized the images for reproduction while removing one of the three components of the transformation: initial point nonlinearity (set \u03b3 = 1), multi-scale decomposition (set Nk = 1), and divisive normalization (set P = 0 and \u03c3 = 1). Figure 13 shows results for each manipulation. Note that we have not refuted each of the partial transformations to predict human perceptions; therefore, these results should be seen as a way to understand the meaning of each calculation, not as a quantitative comparison of the evaluation power of image quality (see details in Appendix B). Each of the three images is noticeably different from the one optimized with the complete transformation. Without the initial point nonlinearity of the images, the algorithm produces images in which low to medium luminance fields of an image are misrepresented."}, {"heading": "6 Discussion", "text": "It's not just a matter of time before there's going to be such a process, it's a matter of time before there's going to be such a process. It's a matter of time before there's going to be such a process. It's a matter of time before there's going to be such a process. It's a matter of time before there's going to be such a process. It's a matter of time before there's going to be such a process. It's a matter of time before there's going to be such a process. It's a matter of time before there's going to be such a process."}, {"heading": "A Derivative of the distance with respect to the rendered image", "text": "Here we provide for the interested reader the derivative of the perception distance D (S \u03b2, I) in relation to the represented image. The distance is defined by: D (S, I) = 1 Nk Nk \u2211 k = 1 (k) cNc \u2211 i = 1 (\u03b2 (k) i) \u03b1 1 \u03b2Here we define the distance between the transformed images, y = f (S) and y = f (I), as: d (k) i = 1 (k) cNk (k) i \u2212 y (k) i (k) i \u2212 y (k) i (k) i (k) i (k) i (f) x (f) x (f) x (f) x (f) x (k) x (k) i \u2212 y (k) i \u2212 y (k) i (k) i (k) i (k) i) i) i (i) i (k) i (k) i (k) i (k) i (k) i (k) i (k) i (k) i (k) i (k) x) x (k) x (k) (k) i) x (k) x (k) i) x (k) (k) i) x (k) x (k) i) x (k) x (k) i) x (k (k) i) x (k) i) x (k (k) i) x (k) i) x (k (k) i) x (k) i) x (i) x (i) x (i) x (i) x (i) x (i) x (k) i) x (i) x (i) x (i) x (i) x (i)."}, {"heading": "Acknowledgments", "text": "JB and EPS are supported by the Howard Hughes Medical Institute. VL is supported by the APOSTD / 2014 / 095 Generalitat Valenciana Grant (Spain) and Analog Devices, Inc. AB is supported by the NEI Visual Neuroscience Training Program, T32 EY007136. We would like to thank the comments of many people who have helped develop this work, in particular Jes\u00fas Malo, Javier Calpe, Pau Segu\u00ed, Jorge P\u00e9rez, Marcelo Bertalm\u00edo, Ted Adelson, Alejandro P\u00e1rraga, Xim Cerd\u00e1, Sylvian Paris, Mark Fairchild and all the people from LCV."}], "references": [{"title": "End-to-end optimization of nonlinear transform codes for perceptual quality", "author": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "venue": "Picture Coding Symposium,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "On the mathematical properties of the structural similarity index", "author": ["Dominique Brunet", "Edward R. Vrscay", "Zhou Wang"], "venue": "Trans. Img. Proc.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "The Laplacian pyramid as a compact image code", "author": ["Peter J. Burt", "Edward H. Adelson"], "venue": "IEEE Transactions on Communications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1983}, {"title": "Normalization as a canonical neural computation", "author": ["M. Carandini", "D.J. Heeger"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Which tone-mapping operator is the best? a comparative study of perceptual quality", "author": ["Xim Cerd\u00e1-Company", "C. Alejandro P\u00e1rraga", "Xavier Otazu"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Dehazing using color-lines", "author": ["Raanan Fattal"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A model of visual adaptation for realistic image synthesis", "author": ["James A. Ferwerda", "Sumanta N. Pattanaik", "Peter Shirley", "Donald P. Greenberg"], "venue": "In Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "An Adaptive Algorithm for Spatial Greyscale", "author": ["Robert W. Floyd", "Louis Steinberg"], "venue": "Proceedings of the Society for Information Display,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1976}, {"title": "Single image haze removal using dark channel prior", "author": ["Kaiming He", "Jian Sun", "Xiaoou Tang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Normalization of cell responses in cat striate cortex", "author": ["D.J. Heeger"], "venue": "Journal of Modern Optics Vis. Neurosci.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba"], "venue": "arXiv e-prints,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Subjective quality assessment database of HDR images compressed with JPEG XT", "author": ["Pavel Korshunov", "Philippe Hanhart", "Thomas Richter", "Alessandro Artusi", "Rafal Mantiuk", "Touradj Ebrahimi"], "venue": "In 7th International Workshop on Quality of Multimedia Experience (QoMEX),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Perceptual image quality assessment using a normalized laplacian pyramid", "author": ["V Laparra", "J Ball\u00e9", "A Berardino", "EP Simoncelli"], "venue": "Proc. IS&T Int\u2019l Symposium on Electronic Imaging, Conf. on Human Vision and Electronic Imaging,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Divisive normalization image quality metric revisited", "author": ["V. Laparra", "J. Mu\u00f1oz Mar\u00ed", "J. Malo"], "venue": "JOSA A,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Categorical image quality (csiq) database", "author": ["EC Larson", "DM Chandler"], "venue": "http://vision.okstate.edu/csiq,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Display adaptive tone mapping", "author": ["Rafa\u0142Mantiuk", "Scott Daly", "Louis Kerofsky"], "venue": "ACM Trans. Graph.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Hdr-vdp-2.2: a calibrated method for objective quality prediction of highdynamic range and standard images", "author": ["Manish Narwaria", "Rafal K. Mantiuk", "Matthieu Perreira Da Silva", "Patrick Le Callet"], "venue": "J. Electronic Imaging,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "A biologically inspired algorithm for the recovery of shading and reflectance", "author": ["A. Olmos", "F.A.A. Kingdom"], "venue": "images. Perception,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Local laplacian filters: edge-aware image processing with a laplacian pyramid", "author": ["Sylvain Paris", "Samuel W. Hasinoff", "Jan Kautz"], "venue": "ACM Trans. Graph.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "A multiscale model of adaptation and spatial vision for realistic image display", "author": ["Sumanta N. Pattanaik", "James A. Ferwerda", "Mark D. Fairchild", "Donald P. Greenberg"], "venue": "In Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Time-dependent visual adaptation for fast realistic image display", "author": ["Sumanta N. Pattanaik", "Jack Tumblin", "Hector Yee", "Donald P. Greenberg"], "venue": "In Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}, {"title": "TID2008 - a database for evaluation of full-reference visual quality assessment metrics", "author": ["N. Ponomarenko", "V. Lukin", "A. Zelensky", "K. Egiazarian", "M. Carli", "F. Battisti"], "venue": "Advances of Modern Radioelectronics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Image database tid2013: Peculiarities, results and perspectives", "author": ["Nikolay N. Ponomarenko", "Lina Jin", "Oleg Ieremeiev", "Vladimir V. Lukin", "Karen O. Egiazarian", "Jaakko Astola", "Benoit Vozel", "Kacem Chehdi", "Marco Carli", "Federica Battisti", "C.-C. Jay Kuo"], "venue": "Sig. Proc.: Image Comm.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Natural signal statistics and sensory gain control", "author": ["O. Schwartz", "E.P. Simoncelli"], "venue": "Nat. Neurosci.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "A statistical evaluation of recent full reference image quality assessment algorithms", "author": ["Hamid R. Sheikh", "Muhammad F. Sabir", "Alan C. Bovik"], "venue": "IEEE Transactions on Image Processing (TIP),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Two methods for display of high contrast images", "author": ["Jack Tumblin", "Jessica K. Hodgins", "Brian K. Guenter"], "venue": "ACM Trans. Graph.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Tone reproduction for realistic images", "author": ["Jack Tumblin", "Holly Rushmeier"], "venue": "IEEE Comput. Graph. Appl.,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1993}, {"title": "Image quality assessment: From error visibility to structural similarity", "author": ["Zhou Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "Trans. Img. Proc.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "Multiscale structural similarity for image quality assessment", "author": ["ZhouWang", "Eero P. Simoncelli", "Alan C. Bovik"], "venue": "In in Proc. IEEE Asilomar Conf. on Signals, Systems, and Computers,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2003}, {"title": "A technique for visual optimization of DCT quantization matrices for individual images. Society for Information Display", "author": ["Andrew B. Watson"], "venue": "Digest of Technical Papers", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1993}], "referenceMentions": [{"referenceID": 26, "context": "A variety of \"tone-mapping\" methods have been proposed to solve this problem by nonlinearly remapping the intensities of the original image into the output range, in a way that remains faithful to the visual appearance of the original scene [29].", "startOffset": 241, "endOffset": 245}, {"referenceID": 27, "context": "Rather than defining a perceptual distance directly (as in SSIM [30], for example), we first define a nonlinear perceptual transform f(\u00b7), which approximates the computations performed within the early stages of the human visual system.", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "Figure 2: Perceptual transform, constructed as a Normalized Laplacian Pyramid (NLP) [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 2, "context": "The transformed luminance image is then decomposed into frequency channels, using the recursive implementation of the Laplacian Pyramid [3].", "startOffset": 136, "endOffset": 139}, {"referenceID": 12, "context": "in the perceptual space defined by the NLP are highly correlated with human judgments [15].", "startOffset": 86, "endOffset": 90}, {"referenceID": 2, "context": "This initial nonlinear transformation is followed by a recursive partition into frequency channels, as in the Laplacian Pyramid [3]:", "startOffset": 128, "endOffset": 131}, {"referenceID": 2, "context": "05), as originally specified in [3].", "startOffset": 32, "endOffset": 35}, {"referenceID": 9, "context": "This function is a simplified variant of divisive normalization, used to describe the responses of neurons in different parts of the visual system [11, 26, 4].", "startOffset": 147, "endOffset": 158}, {"referenceID": 23, "context": "This function is a simplified variant of divisive normalization, used to describe the responses of neurons in different parts of the visual system [11, 26, 4].", "startOffset": 147, "endOffset": 158}, {"referenceID": 3, "context": "This function is a simplified variant of divisive normalization, used to describe the responses of neurons in different parts of the visual system [11, 26, 4].", "startOffset": 147, "endOffset": 158}, {"referenceID": 12, "context": "Figure 3: Summation model, using the Normalized Laplacian Pyramid [15] as perceptual transform f (see Fig.", "startOffset": 66, "endOffset": 70}, {"referenceID": 29, "context": "A similar summation model has been employed in previous perceptual quality metrics [32, 16].", "startOffset": 83, "endOffset": 91}, {"referenceID": 13, "context": "A similar summation model has been employed in previous perceptual quality metrics [32, 16].", "startOffset": 83, "endOffset": 91}, {"referenceID": 21, "context": "All parameters of the perceptual transform and metric were optimized to best explain human perceptual ratings of distorted images in a public database [24].", "startOffset": 151, "endOffset": 155}, {"referenceID": 12, "context": "Unlike in [15], we set the normalization parameters to be identical for all bandpass channels (assuming scale-invariance), but allowed a different set for the lowpass channel.", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "[21] NLP", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Center: result obtained from a state-of-the-art tone mapping algorithm [21].", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "For this, we use the Adaptive Moment Estimation (Adam) algorithm [13].", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "[21] NLP", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] mitigates this problem, rendering an image that reveals detail in both dark and bright regions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Results for two example grayscale images from the McGill database [20] are shown in Fig.", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "[21] result.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21]", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Top center: image processed using [21].", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10]", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Fattal [7] NLP, Smax = 104", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "algorithm [10].", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "Bottom left: image processed using Fattal algorithm [7].", "startOffset": 52, "endOffset": 55}, {"referenceID": 8, "context": "Figure 8 compares the performance of our method with both a classical ([10]) and a", "startOffset": 71, "endOffset": 75}, {"referenceID": 5, "context": "state-of-the-art method ([7]).", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "Center column: Floyd\u2013Steinberg method [9].", "startOffset": 38, "endOffset": 41}, {"referenceID": 26, "context": "For example, Tumblin and Rushmeier\u2019s seminal paper on tone mapping states: \u201cAccurate display methods should compensate for all light dependent changes in the way we see\u201d [29].", "startOffset": 170, "endOffset": 174}, {"referenceID": 6, "context": "Several tone mapping papers have followed this framework (see for instance [8, 22, 28, 23, 18]), each using a different perceptual metric to determine the similarity between the rendered image and the scene.", "startOffset": 75, "endOffset": 94}, {"referenceID": 19, "context": "Several tone mapping papers have followed this framework (see for instance [8, 22, 28, 23, 18]), each using a different perceptual metric to determine the similarity between the rendered image and the scene.", "startOffset": 75, "endOffset": 94}, {"referenceID": 25, "context": "Several tone mapping papers have followed this framework (see for instance [8, 22, 28, 23, 18]), each using a different perceptual metric to determine the similarity between the rendered image and the scene.", "startOffset": 75, "endOffset": 94}, {"referenceID": 20, "context": "Several tone mapping papers have followed this framework (see for instance [8, 22, 28, 23, 18]), each using a different perceptual metric to determine the similarity between the rendered image and the scene.", "startOffset": 75, "endOffset": 94}, {"referenceID": 15, "context": "Several tone mapping papers have followed this framework (see for instance [8, 22, 28, 23, 18]), each using a different perceptual metric to determine the similarity between the rendered image and the scene.", "startOffset": 75, "endOffset": 94}, {"referenceID": 4, "context": "Nowadays, tone mapping methods often do not make explicit use of perceptual metrics (see [5] for a nice review), but rather provide the user with a small set of free", "startOffset": 89, "endOffset": 92}, {"referenceID": 18, "context": "These methods are conceptually simpler than ours, and some of them can produce high quality results in controlled situations (see for instance [21]).", "startOffset": 143, "endOffset": 147}, {"referenceID": 12, "context": "The metric is an extension of the NLP distance presented in [15].", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "As a case in point, it has also been employed to optimize an image compression algorithm [1].", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "In particular, it should be possible to improve the halftoning solution, for which we used a simple greedy method analogous to Floyd-Steinberg [9].", "startOffset": 143, "endOffset": 146}, {"referenceID": 27, "context": "Our use of a simple physiologically-inspired model for assessing perceptual disortion offers opportunities for improvement (note that most image quality models are less physiologically based [30, 31, 19]).", "startOffset": 191, "endOffset": 203}, {"referenceID": 28, "context": "Our use of a simple physiologically-inspired model for assessing perceptual disortion offers opportunities for improvement (note that most image quality models are less physiologically based [30, 31, 19]).", "startOffset": 191, "endOffset": 203}, {"referenceID": 16, "context": "Our use of a simple physiologically-inspired model for assessing perceptual disortion offers opportunities for improvement (note that most image quality models are less physiologically based [30, 31, 19]).", "startOffset": 191, "endOffset": 203}, {"referenceID": 27, "context": "The most widely-used method of assessing IQA models is by measuring their correlation with human quality ratings on a diverse set of distorted images [30, 31, 19].", "startOffset": 150, "endOffset": 162}, {"referenceID": 28, "context": "The most widely-used method of assessing IQA models is by measuring their correlation with human quality ratings on a diverse set of distorted images [30, 31, 19].", "startOffset": 150, "endOffset": 162}, {"referenceID": 16, "context": "The most widely-used method of assessing IQA models is by measuring their correlation with human quality ratings on a diverse set of distorted images [30, 31, 19].", "startOffset": 150, "endOffset": 162}, {"referenceID": 21, "context": "Note that the parameters of our metric were adjusted using the TID 2008 [24] database, the VDP 2.", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "2 metric was trained using HDR images, the TID 2008 [24] and the CSIQ [17] database, and the SSIM and MS-SSIM were trained using LIVE database [27].", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "2 metric was trained using HDR images, the TID 2008 [24] and the CSIQ [17] database, and the SSIM and MS-SSIM were trained using LIVE database [27].", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "2 metric was trained using HDR images, the TID 2008 [24] and the CSIQ [17] database, and the SSIM and MS-SSIM were trained using LIVE database [27].", "startOffset": 143, "endOffset": 147}, {"referenceID": 1, "context": "For example, the application of SSIM to optimization procedures is not straightforward and it involves some modifications of the metric [2].", "startOffset": 136, "endOffset": 139}, {"referenceID": 21, "context": "TID 2008 [24] TID 2013 [25] LIVE [27] CSIQ [17] EPFL [14]", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "TID 2008 [24] TID 2013 [25] LIVE [27] CSIQ [17] EPFL [14]", "startOffset": 23, "endOffset": 27}, {"referenceID": 24, "context": "TID 2008 [24] TID 2013 [25] LIVE [27] CSIQ [17] EPFL [14]", "startOffset": 33, "endOffset": 37}, {"referenceID": 14, "context": "TID 2008 [24] TID 2013 [25] LIVE [27] CSIQ [17] EPFL [14]", "startOffset": 43, "endOffset": 47}, {"referenceID": 11, "context": "TID 2008 [24] TID 2013 [25] LIVE [27] CSIQ [17] EPFL [14]", "startOffset": 53, "endOffset": 57}], "year": 2017, "abstractText": "We develop a framework for rendering photographic images, taking into account display limitations, so as to optimize perceptual similarity between the rendered image and the original scene. We formulate this as a constrained optimization problem, in which we minimize a measure of perceptual dissimilarity, the Normalized Laplacian Pyramid Distance (NLPD), which mimics the early stage transformations of the human visual system. When rendering images acquired with higher dynamic range than that of the display, we find that the optimized solution boosts the contrast of low-contrast features without introducing significant artifacts, yielding results of comparable visual quality to current state-of-the art methods with no manual intervention or parameter settings. We also examine a variety of other display constraints, including limitations on minimum luminance (black point), mean luminance (as a proxy for energy consumption), and quantized luminance levels (halftoning). Finally, we show that the method may be used to enhance details and contrast of images degraded by optical scattering (e.g., fog).", "creator": "LaTeX with hyperref package"}}}