{"id": "1701.06725", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jan-2017", "title": "A Contextual Bandit Approach for Stream-Based Active Learning", "abstract": "Contextual bandit algorithms -- a class of multi-armed bandit algorithms that exploit the contextual information -- have been shown to be effective in solving sequential decision making problems under uncertainty. A common assumption adopted in the literature is that the realized (ground truth) reward by taking the selected action is observed by the learner at no cost, which, however, is not realistic in many practical scenarios. When observing the ground truth reward is costly, a key challenge for the learner is how to judiciously acquire the ground truth by assessing the benefits and costs in order to balance learning efficiency and learning cost. From the information theoretic perspective, a perhaps even more interesting question is how much efficiency might be lost due to this cost. In this paper, we design a novel contextual bandit-based learning algorithm and endow it with the active learning capability. The key feature of our algorithm is that in addition to sending a query to an annotator for the ground truth, prior information about the ground truth learned by the learner is sent together, thereby reducing the query cost. We prove that by carefully choosing the algorithm parameters, the learning regret of the proposed algorithm achieves the same order as that of conventional contextual bandit algorithms in cost-free scenarios, implying that, surprisingly, cost due to acquiring the ground truth does not increase the learning regret in the long-run. Our analysis shows that prior information about the ground truth plays a critical role in improving the system performance in scenarios where active learning is necessary.", "histories": [["v1", "Tue, 24 Jan 2017 04:12:25 GMT  (173kb,D)", "http://arxiv.org/abs/1701.06725v1", "arXiv admin note: text overlap witharXiv:1607.03182"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1607.03182", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["linqi song", "jie xu"], "accepted": false, "id": "1701.06725"}, "pdf": {"name": "1701.06725.pdf", "metadata": {"source": "CRF", "title": "A Contextual Bandit Approach for Stream-Based Active Learning", "authors": ["Linqi Song", "Jie Xu"], "emails": ["songlinqi@ucla.edu", "jiexu@miami.edu"], "sections": [{"heading": null, "text": "It is only a matter of time before that happens, that it happens."}, {"heading": "II. PROBLEM FORMULATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. System Model", "text": "We consider a discrete time system in which the time in slots t = 1, 2,.... The arm space is a limited space K with the coverage dimension dK. The context space is a limited space X with the coverage dimension dX. For each context x-X, the reward for selecting the arm k-K is called r (x, k), which is also unknown. We assume that the reward space [0, 1] is for the ease of exposure [0, 1], but this assumption can be relaxed to take into account any limited interval. In the conventional contextual bandit setting, the following events occur in order in each time slot t: (1) A context X arrives; (2) An arm kt-K is selected; (3) The reward r (xt, kt) is generated after f (xt) and is observed at cost."}, {"heading": "B. Learning Regret", "text": "We use the total expected payout (i.e. the reward minus the query costs) to describe the performance of an algorithm \u03c0. The total expected payout up to the time window T is as follows: U\u03c0 (T) = E T \u2211 t = 1 [r (xt, kt) \u2212 ctqt] (3), with the expectation taking over the context of the arrival process and the reward distributions. We compare an algorithm with the static-best oracle policy. Since the oracle knows the reward distributions a priori, there is no need to question the basic truth in order to learn about it. Therefore, the oracle policy chooses the arm k = argmaxk \u00b5 (xt, kt), which maximizes the expected reward. Since the oracle knows the reward distributions, there is no need for it to query the basic truth in order to learn about it. Therefore, q \u00b2 t = 0, \u0441\u043e = The duration of a learning district (Udic = T) becomes an algorithm district."}, {"heading": "III. THE ALGORITHM", "text": "In this section we describe the proposed contextual active learning bandit algorithm (CB-AL)."}, {"heading": "A. Useful Notions", "text": "Similarly, the division of context space for epoch i is denoted by PX (i) = {X1, X2, XMi}, consisting of the Mi subspaces. The division of context space for epoch i is denoted by PX (i) = {X1, XMi}, consisting of the Mi subspaces. Similarly, the division of context space for epoch i (i) = {K1, KNi}, consisting of the Ni subspaces is denoted by PX (i) = {X1, XMi}. The radius of a context cluster Xm is half the maximum distance between two contexts in the cluster."}, {"heading": "B. The Algorithm", "text": "The aim of the algorithm in the exploration phase is to explore various active clusters in order to improve their performance for the learned system."}, {"heading": "IV. REGRET ANALYSIS", "text": "\u2022 Cluster reward. We define the expected reward for selecting an arm cluster Kn for the context cluster Xm as \u00b5 (m, n) = maxx-xm (x, k). Thus, the reward of the optimal arm cluster (normal) is (normal) arm cluster Xm (m, n). In addition, we define the reward difference as (m, n) m \u2212 \u00b5 (m, n).The reward of the optimal arm cluster (normal).We define the optimal arm cluster in relation to the context cluster Xm as arm cluster Kn, the (m, n).m \u2212 m Similarly, the suboptimal arm clusters are the ones that satisfy the (m)."}, {"heading": "V. NUMERICAL RESULTS", "text": "In our first experiment, we compare the performance of our proposed CB-AL algorithm with the Contextual Bandit algorithm and Contextual Bandit Active Learning without prior information (CB-AL without prior information), and the result is in Figure 3. As we can see, the proposed CB-AL algorithm performs better in terms of payout than the conventional bandit algorithm (by 16%) and the CBAL without prior information (by 13%) at the end of the trial period. In our second experiment, we show the payouts achieved by our proposed CB-AL algorithm when query costs vary (by changing the cost parameters c to e.q. (1)). We show the result in Figure 4. We can see that the payout achieved by c decreases from 0.1 to 1 by 15% for T = 10,000 and by 8% for 20,000 T = 000."}, {"heading": "VI. CONCLUSIONS", "text": "In this thesis, we developed a contextual bandit learning algorithm with active learning capacities.The active learning costs are reduced by providing the annotator with information in advance about the realization of the reward. The algorithm maintains and updates the partitions of the context and the armrooms, and works between exploration and exploitation phases. By precisely controlling the partitioning process and when it comes to questioning the basic truth of the reward, the algorithm gracefully balances the accuracy of learning and the costs of active learning. We prove that the regret of the proposed algorithm is in the same order as those of conventional contextual bandit algorithms in free scenarios."}], "references": [{"title": "and M", "author": ["T. Lu", "D. P\u00e1l"], "venue": "P\u00e1l, \u201cContextual multi-armed bandits,\u201d in Artificial Intelligence and Statistics Conference (AISTATS)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "The epoch-greedy algorithm for multiarmed bandits with side information,", "author": ["J. Langford", "T. Zhang"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Contextual bandits with similarity information.", "author": ["A. Slivkins"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "and M", "author": ["L. Song", "W. Hsu", "J. Xu"], "venue": "van der Schaar, \u201cUsing contextual learning to improve diagnostic accuracy: Application in breast cancer screening,\u201d IEEE Journal of Biomedical and Health Informatics, vol. 20, no. 3, pp. 902\u2013914", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "and M", "author": ["J. Xu", "T. Xing"], "venue": "van der Schaar, \u201cPersonalized course sequence recommendations,\u201d IEEE Transactions on Signal Processing, vol. 64, no. 20, pp. 5340\u20135352", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Active learning literature survey,", "author": ["B. Settles"], "venue": "University of Wisconsin, Madison,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "and M", "author": ["D.A. Cohn", "Z. Ghahramani"], "venue": "I. Jordan, \u201cActive learning with statistical models,\u201d Journal of Artificial Intelligence Research, vol. 4, no. 1, pp. 129\u2013145", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Statistical active learning algorithms,", "author": ["M.-F.F. Balcan", "V. Feldman"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Nigamy, \u201cEmploying em and pool-based active learning for text classification,", "author": ["K.A.K. McCallumzy"], "venue": "in Proc. International Conference on Machine Learning (ICML),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Analysis of a greedy active learning strategy.", "author": ["S. Dasgupta"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "An approximate formula for a partial sum of the divergent p-series,", "author": ["E. Chlebus"], "venue": "Applied Mathematics Letters, vol. 22,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Lectures on analysis on metric spaces", "author": ["J. Heinonen"], "venue": "Springer Science & Business Media", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Contextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5].", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "Contextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5].", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "Contextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5].", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "Contextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5].", "startOffset": 282, "endOffset": 285}, {"referenceID": 4, "context": "Contextual bandits [1][2][3] is a powerful machine learning framework for modeling and solving a large class of sequential decision making problems under uncertainty, ranging from content recommendation, online advertising, stream mining, to decision support for clinical diagnosis [4] and personalized education [5].", "startOffset": 313, "endOffset": 316}, {"referenceID": 5, "context": "Therefore, in addition to carefully deciding which arm to pull, the learner also has to actively and judiciously acquire the ground truth rewards from an annotator by assessing the benefits and costs of obtaining them in these application scenarios [6][7].", "startOffset": 249, "endOffset": 252}, {"referenceID": 6, "context": "Therefore, in addition to carefully deciding which arm to pull, the learner also has to actively and judiciously acquire the ground truth rewards from an annotator by assessing the benefits and costs of obtaining them in these application scenarios [6][7].", "startOffset": 252, "endOffset": 255}, {"referenceID": 7, "context": "This is in stark contrast with conventional active learning literature where the cost of acquiring the ground truth is constant [8][9][10].", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "This is in stark contrast with conventional active learning literature where the cost of acquiring the ground truth is constant [8][9][10].", "startOffset": 131, "endOffset": 134}, {"referenceID": 9, "context": "This is in stark contrast with conventional active learning literature where the cost of acquiring the ground truth is constant [8][9][10].", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "For any context x \u2208 X , the reward of selecting arm k \u2208 K is r(x, k) \u2208 [0, 1], which is sampled according to some underlying but unknown distribution f(x, k).", "startOffset": 71, "endOffset": 77}, {"referenceID": 0, "context": "We assume that the reward value space is [0, 1] for the ease of exposition but this assumption can be relaxed to account for any bounded interval.", "startOffset": 41, "endOffset": 47}, {"referenceID": 0, "context": "As a widely-adopted assumption in contextual bandits literature [1][3], the reward function is assumed to satisfy a Lipschitz condition with respect to both the context and the arm.", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "As a widely-adopted assumption in contextual bandits literature [1][3], the reward function is assumed to satisfy a Lipschitz condition with respect to both the context and the arm.", "startOffset": 67, "endOffset": 70}, {"referenceID": 10, "context": "If 1 \u2264 \u03b21 < 2, the third term on the right hand side of the last inequality in (24) can be bounded by cNi2 5\u03b21/2\u22121 [ln(2T \u03b3+1 i )] 1(S m) 1\u2212\u03b21/2 1\u2212\u03b21/2 , due to the divergent series \u2211T t=1 t \u2212y \u2264 T (1\u2212y)/(1 \u2212 y) for 0 < y < 1 [11].", "startOffset": 226, "endOffset": 230}, {"referenceID": 11, "context": "According to the definition of covering dimensions [12], the maximum number of arm clusters can be bounded by Ni \u2264 CA\u03c1A K,i in epoch i, and the maximum number of context clusters can be bounded by Mi \u2264 CX\u03c1X X,i in epoch i, where CA, CX are covering constants for the arm space and the context space.", "startOffset": 51, "endOffset": 55}, {"referenceID": 0, "context": "Since the proposed algorithm incurs the query cost when it requests a ground truth, the lower bound of the regret cannot be lower than that of the conventional contextual MAB setting where no query cost is incurred [1].", "startOffset": 215, "endOffset": 218}, {"referenceID": 0, "context": "Theorem 1 and Theorem 2 together show that our algorithm is order-optimal and achieves the same order as conventional contextual bandits algorithms in cost-free scenarios [1][2][3].", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "Theorem 1 and Theorem 2 together show that our algorithm is order-optimal and achieves the same order as conventional contextual bandits algorithms in cost-free scenarios [1][2][3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 2, "context": "Theorem 1 and Theorem 2 together show that our algorithm is order-optimal and achieves the same order as conventional contextual bandits algorithms in cost-free scenarios [1][2][3].", "startOffset": 177, "endOffset": 180}], "year": 2017, "abstractText": "Contextual bandit algorithms \u2013 a class of multiarmed bandit algorithms that exploit the contextual information \u2013 have been shown to be effective in solving sequential decision making problems under uncertainty. A common assumption adopted in the literature is that the realized (ground truth) reward by taking the selected action is observed by the learner at no cost, which, however, is not realistic in many practical scenarios. When observing the ground truth reward is costly, a key challenge for the learner is how to judiciously acquire the ground truth by assessing the benefits and costs in order to balance learning efficiency and learning cost. From the information theoretic perspective, a perhaps even more interesting question is how much efficiency might be lost due to this cost. In this paper, we design a novel contextual bandit-based learning algorithm and endow it with the active learning capability. The key feature of our algorithm is that in addition to sending a query to an annotator for the ground truth, prior information about the ground truth learned by the learner is sent together, thereby reducing the query cost. We prove that by carefully choosing the algorithm parameters, the learning regret of the proposed algorithm achieves the same order as that of conventional contextual bandit algorithms in cost-free scenarios, implying that, surprisingly, cost due to acquiring the ground truth does not increase the learning regret in the long-run. Our analysis shows that prior information about the ground truth plays a critical role in improving the system performance in scenarios where active learning is necessary.", "creator": "LaTeX with hyperref package"}}}