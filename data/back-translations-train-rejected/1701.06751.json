{"id": "1701.06751", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jan-2017", "title": "Collective Vertex Classification Using Recursive Neural Network", "abstract": "Collective classification of vertices is a task of assigning categories to each vertex in a graph based on both vertex attributes and link structure. Nevertheless, some existing approaches do not use the features of neighbouring vertices properly, due to the noise introduced by these features. In this paper, we propose a graph-based recursive neural network framework for collective vertex classification. In this framework, we generate hidden representations from both attributes of vertices and representations of neighbouring vertices via recursive neural networks. Under this framework, we explore two types of recursive neural units, naive recursive neural unit and long short-term memory unit. We have conducted experiments on four real-world network datasets. The experimental results show that our frame- work with long short-term memory model achieves better results and outperforms several competitive baseline methods.", "histories": [["v1", "Tue, 24 Jan 2017 07:07:15 GMT  (900kb,D)", "http://arxiv.org/abs/1701.06751v1", "7 pages, 5 figures"]], "COMMENTS": "7 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.SI", "authors": ["qiongkai xu", "qing wang", "chenchen xu", "lizhen qu"], "accepted": false, "id": "1701.06751"}, "pdf": {"name": "1701.06751.pdf", "metadata": {"source": "CRF", "title": "Collective Vertex Classification Using Recursive Neural Network", "authors": ["Qiongkai Xu", "Qing Wang", "Chenchen Xu", "Lizhen Qu"], "emails": ["Xu.Qiongkai@data61.csiro.au", "qing.wang@anu.edu.au", "Xu.Chenchen@data61.csiro.au", "Qu.Lizhen@data61.csiro.au"], "sections": [{"heading": "Introduction", "text": "In fact, most of them are able to decide for themselves what they want."}, {"heading": "Related Work", "text": "There is a growing trend towards graph-based data representation (Angles and Gutierrez 2008), and the discovery of knowledge from graphs is becoming an exciting area of research, such as Vertex Classification (London and Getoor 2014) and Graph Classification (Niepert, Ahmed and Kutzkov 2016). Graph Classification analyses the properties of the graph as a whole, while Vertex Classification focuses on predicting vertices in graphics. In this paper, we discuss the problem of vertex classification. The main flow approaches to vertex classification are collective vertex classification (Lu and Getoor 2003), which classify vertices with information provided by adjacent vertices. Iterative classification approaches (Lu and Getoor 2003) Vertex Classification models are collective vertex classification (Lu and Getoor 2003)."}, {"heading": "Graph-based Recursive Neural Networks", "text": "A diagram G = (V, E) consists of a series of vertices V = {v1, v2,..., vN} and a series of edges E V \u00b7 V. Diagrams can contain cycles in which a cycle is a path from a vertex back to itself. Let X = {x1, x2, \u00b7 \u00b7, xN} be a series of characteristic vectors, each xi-X having a vertex vi-V, L having a series of labels, and vt-V having a vertex designated as a vertex. Then the collective problem of classifying vertex points is to predict the marking yt-X with a vertex vi-V-V, so that this thaty-T = argmax yt-LP3-V (yt | vt, G, X) (1) using a recursive neural network with the parameters.. 6. algorithm 4. Qt-safe-1-Q-V is algorithm-vvt-V."}, {"heading": "Tree Construction", "text": "In a neural network, neurons are arranged in layers and different layers are processed in a predefined order. However, in graphs, especially cyclic graphs, there is no explicit sequence. How to construct an ordered structure from a graph is questionable. Given a graph G = (V, E), a target vertex V and a tree depth d'N, we can construct a tree T = (VT, ET) that is rooted in the width of the first search where VT is a vertex, and (v, w) ET means an edge from a parent vertex v to a child vertex w. The depth of a vertex v in T is the length of the path from vt to v, referred to as T."}, {"heading": "Recursive Neural Network Construction", "text": "Now we construct a recursive neural unit (RNU) for each vertex vk \u0441T. Each RNU takes a characteristic vector xk and hidden states of its infant vertebrae as input. We examine two types of recursive neural units discussed in (Socher et al. 2011; Tai, Socher and Manning 2015)."}, {"heading": "Naive Recursive Neural Unit (NRNU)", "text": "The transition equations of NRNU are given as follows: h-k = max vr-C (vk) {hr} (2) hk = tanh (W (h) xk + U (h) h-k + b (h))) (3), where C (vk) is the set of child vertices of vk, W (h) and U (h) is the weight matrix and b (h) is the bias. The generated hidden state hk of vk is related to the input vector xk and the aggregated hidden state h-k. Unlike summing up all the hidden states as in (Tai, Socher and Manning 2015), we use the maximum aggregation for h-k (see Eq 2). This is because the number of neighbors can be very large for the vertex and some of the hidden states are relevant to the ONLY."}, {"heading": "Long Short-Term Memory Unit (LSTMU)", "text": "LSTMU is a variant of RNU that can solve the long-term q dependency problem by introducing memory cells and gated units (Hochreiter and Schmidhuber 1997). LSTMU consists of an input gate ik, a forget gate fk, an output gate ok, a memory cell ck and a hidden state hk. The transition equations of LSTMU are given as follows: h-k = max vr-C (vk) {hr} (4) ik = \u03c3 (W (i) xk + U (i) h-k + b (i)))) (5) fkr = \u03c3 (W (f) xk + U (f) hr + b (f))))) (6) ok = evolution (W (o) xk + U (o) h-K + b (o) h-K (o) h-K (o)))))) (7) uk = tanh-K (u), the vector W (u-k (u-k) (h) (K)."}, {"heading": "Experimental Setup", "text": "To test the effectiveness of our approach, we conducted experiments on four sets of data and compared our approach with three basic methods. We will describe the data sets, basic methods and experimental settings."}, {"heading": "Datasets", "text": "We tested our approach on four real-world network datasets. \u2022 Cora (McCallum et al. 2000) is a citation network dataset consisting of 2708 scientific publications and 5429 citations between publications. \u2022 All publications are classified into seven classes: Rule Learning (RU), Genetic Algorithms (GE), Reinforcement Learning (RE), Neural Networks (NE), Probabilistic Methods (PR), Case Based (CA) and Theory (TH). \u2022 Citeseer (Giles, Bollacker and Lawrence 1998) is another citation network dataset larger than Cora. Citeseer consists of 3312 scientific publications and 4723 citations. All publications are classified into six classes: Agents, AI, DB, IR, ML and HCI. \u2022 WebKB dataset is larger than Cora."}, {"heading": "Baseline Methods", "text": "We have implemented the following three basic methods: \u2022 Logistic Regression (LR) (Hosmer Jr and Lemeshow2004) predicts the labeling of a vertex using its own attributes through a logistic regression model. \u2022 Iterative Classification Approach (ICA) (Lu and Getoor 2003; London and Getoor 2014) uses the combination of connecting structure and vertex characteristics as input for a statistical machine learning model. We use two variants of ICA: ICA-binary uses the occurrence of adjacent vertex labels, ICA-count uses the frequency of adjacent vertex labels. \u2022 Label propagation (LP) (Wang and Zhang 2008; London and Getoor 2014) uses statistical machine learning to determine a label probability for each vertex, and then propagates the label probability to all its neighbors."}, {"heading": "Experimental Settings", "text": "In our experiments, we divide each data set into two parts: training set and test set, with different proportions (70% to 95% for training); for each proportional setting, we randomly generate 5 pairs of training and test sets; then we report on the averaged results of the experiments with the same proportional setting; after preliminary experiments, the learning rate is set to 0.1 for LR, ICA and LP and 0.01 for all GRNN models (Baeza-Yates, Ribeiro-Neto and others 1999); we empirically set the number of hidden states to 200 for all GRNN models; Adagrad (Duchi, Hazan and Singer 2011) is used as an optimization method for LR, ICA and LP (ICA and LP and 0.01 for all GRNN models)."}, {"heading": "Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Model and Parameter Selection", "text": "Figure 3 illustrates the performance of G-NRNN and GLSTM on four sets of data. We use G-NRNN di and GLSTM di to refer to G-NRNN and G-LSTM on trees with a depth of i, where i = 0, 1, 2.In experiments with G-NRNN and G-LSTM on trees with different steps, d1 and d2 in most cases perform better than d0. This performance difference is also evident in WebKB when the training share is greater than 85%. This means that the introduction of adjacent vertices can improve the performance of classification and adjacent information by increasing the depth3If d = 0, each constructed tree contains only one text.of trees help. With the same RNU, we can improve the vertisings of Websims 1 and Websids 1 not always 1, but of Websids 1."}, {"heading": "Baseline Comparison", "text": "Since our models with d2 provide better performance, we choose G-NRNN d2 and G-LSTM d2 as representative models. In Figure 4.a and Figure 4.b, we compare our approach with the base methods on two citation networks, Cora and Citeseer. Both G-NRNN d2 and G-LSTM d2 consistently outperform all base methods on the citation networks, indicating the effectiveness of our approach. In Figure 4.c and Figure 4.d, we compare our approach with the base methods on WebKB and WebKB-sim. For WebKB, our method achieves competitive results compared to ICA. LP performs worse than LNRR on WebKB, where Micro-F1 exceeds this score with less than 0.7% of the results on WebKB and WebKB-sim."}, {"heading": "Dataset Comparison", "text": "To analyze the juxtaposition of adjacent labels, we calculate the transition probability of target corners to their adjacent corners. First, we calculate the identification probability matrix Md, in which Mdi, j indicates simultaneous times of labels li of target corners vi and labels lj of d-step removed corners vj. Then, we obtain a transition probability matrix T d, in which T di, j = M d i, j / (\u2211 kM d i, k). Thermal images of T d on four data sets are demonstrated in Figure 5. For Cora and Citeseer, adjacent labels tend to share the same labels. If d increases to 2, labels are still closely correlated, which is probably why all ICA, LP G-NRNN and G-LSTM work well on Cora and Citeseer. In this situation, GRNs integrate features of d-step-distant labels that can directly contribute to classify target labels."}, {"heading": "Conclusions and Future work", "text": "We have compared two recursive units, NRNU and LSTMU, within this framework. It has been found that LSTMU works better than NRNU in most experiments. Finally, the performance of our proposed methods has outperformed several state-of-the-art statistical methods of machine learning. In the future, we intend to expand this work in several directions. We intend to apply GRNN to large-scale diagrams, improve the efficiency of GRNN and perform time complexity analyses."}], "references": [{"title": "Survey of graph database models", "author": ["R. Angles", "C. Gutierrez"], "venue": "ACM Computing Surveys (CSUR) 40(1):1.", "citeRegEx": "Angles and Gutierrez,? 2008", "shortCiteRegEx": "Angles and Gutierrez", "year": 2008}, {"title": "Modern information retrieval, volume 463", "author": ["R. Baeza-Yates", "B Ribeiro-Neto"], "venue": null, "citeRegEx": "Baeza.Yates and Ribeiro.Neto,? \\Q1999\\E", "shortCiteRegEx": "Baeza.Yates and Ribeiro.Neto", "year": 1999}, {"title": "Enhanced hypertext categorization using hyperlinks", "author": ["S. Chakrabarti", "B. Dom", "P. Indyk"], "venue": "ACM SIGMOD Record, volume 27, 307\u2013318. ACM.", "citeRegEx": "Chakrabarti et al\\.,? 1998", "shortCiteRegEx": "Chakrabarti et al\\.", "year": 1998}, {"title": "Learning to extract symbolic knowledge from the world wide web", "author": ["M. Craven", "D. DiPasquo", "D. Freitag", "A. McCallum"], "venue": "Proceedings of the 15th National Conference on Artificial Intelligence, 509\u2013516. American Association for Artificial Intelligence.", "citeRegEx": "Craven et al\\.,? 1998", "shortCiteRegEx": "Craven et al\\.", "year": 1998}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research 12(Jul):2121\u2013 2159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Citeseer: An automatic citation indexing system", "author": ["C.L. Giles", "K.D. Bollacker", "S. Lawrence"], "venue": "Proceedings of the 3rd ACM conference on Digital libraries, 89\u201398. ACM.", "citeRegEx": "Giles et al\\.,? 1998", "shortCiteRegEx": "Giles et al\\.", "year": 1998}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Applied logistic regression", "author": ["D.W. Hosmer Jr", "S. Lemeshow"], "venue": "John Wiley & Sons.", "citeRegEx": "Jr and Lemeshow,? 2004", "shortCiteRegEx": "Jr and Lemeshow", "year": 2004}, {"title": "Collective classification of network data", "author": ["B. London", "L. Getoor"], "venue": "Data Classification: Algorithms and Applications 399.", "citeRegEx": "London and Getoor,? 2014", "shortCiteRegEx": "London and Getoor", "year": 2014}, {"title": "Link-based classification", "author": ["Q. Lu", "L. Getoor"], "venue": "Proceedings of the 20th International Conference on Machine Learning, volume 3, 496\u2013503.", "citeRegEx": "Lu and Getoor,? 2003", "shortCiteRegEx": "Lu and Getoor", "year": 2003}, {"title": "Automating the construction of internet portals with machine learning", "author": ["A.K. McCallum", "K. Nigam", "J. Rennie", "K. Seymore"], "venue": "Information Retrieval 3(2):127\u2013163.", "citeRegEx": "McCallum et al\\.,? 2000", "shortCiteRegEx": "McCallum et al\\.", "year": 2000}, {"title": "Recurrent neural collective classification", "author": ["D.D. Monner", "J.A. Reggia"], "venue": "IEEE transactions on neural networks and learning systems 24(12):1932\u20131943.", "citeRegEx": "Monner and Reggia,? 2013", "shortCiteRegEx": "Monner and Reggia", "year": 2013}, {"title": "A practical hypertext categorization method using links and incrementally available class information", "author": ["S.H. Myaeng", "Lee", "M.-h."], "venue": "Proceedings of the 23rd ACM International Conference on Research and Development in Information Retrieval. ACM.", "citeRegEx": "Myaeng et al\\.,? 2000", "shortCiteRegEx": "Myaeng et al\\.", "year": 2000}, {"title": "Learning convolutional neural networks for graphs", "author": ["M. Niepert", "M. Ahmed", "K. Kutzkov"], "venue": "Proceedings of the 33rd International Conference on Machine Learning.", "citeRegEx": "Niepert et al\\.,? 2016", "shortCiteRegEx": "Niepert et al\\.", "year": 2016}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 701\u2013710. ACM.", "citeRegEx": "Perozzi et al\\.,? 2014", "shortCiteRegEx": "Perozzi et al\\.", "year": 2014}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks 61:85\u2013117.", "citeRegEx": "Schmidhuber,? 2015", "shortCiteRegEx": "Schmidhuber", "year": 2015}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C. Lin", "C. Manning", "A.Y. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning, 129\u2013136.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistic. ACL.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Negative link prediction in social media", "author": ["J. Tang", "S. Chang", "C. Aggarwal", "H. Liu"], "venue": "Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, 87\u201396. ACM.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["D. Tang", "B. Qin", "T. Liu"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 1422\u20131432.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Label propagation through linear neighborhoods", "author": ["F. Wang", "C. Zhang"], "venue": "IEEE Transactions on Knowledge and Data Engineering 20(1):55\u201367.", "citeRegEx": "Wang and Zhang,? 2008", "shortCiteRegEx": "Wang and Zhang", "year": 2008}, {"title": "Social influence locality for modeling retweeting behaviors", "author": ["J. Zhang", "B. Liu", "J. Tang", "T. Chen", "J. Li"], "venue": "Proceeding of the 23rd International Joint Conference on Artificial Intelligence, volume 13, 2761\u20132767. Citeseer.", "citeRegEx": "Zhang et al\\.,? 2013", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "Many studies formulate their graph problems as predictive tasks such as vertex classification (London and Getoor 2014), link prediction (Tang et al.", "startOffset": 94, "endOffset": 118}, {"referenceID": 18, "context": "Many studies formulate their graph problems as predictive tasks such as vertex classification (London and Getoor 2014), link prediction (Tang et al. 2015), and graph classification (Niepert, Ahmed, and Kutzkov 2016).", "startOffset": 136, "endOffset": 154}, {"referenceID": 8, "context": "Algorithms for classifying vertices are widely adopted in web page analysis, citation analysis and social network analysis (London and Getoor 2014).", "startOffset": 123, "endOffset": 147}, {"referenceID": 9, "context": "For instance, the Iterative Classification Approach (ICA) integrates the label distribution of neighbouring vertices to assist classification (Lu and Getoor 2003) and the Label Propagation approach (LP) fine-tunes predictions of the vertex using the labels of its neighbouring vertices (Wang and Zhang 2008).", "startOffset": 142, "endOffset": 162}, {"referenceID": 20, "context": "For instance, the Iterative Classification Approach (ICA) integrates the label distribution of neighbouring vertices to assist classification (Lu and Getoor 2003) and the Label Propagation approach (LP) fine-tunes predictions of the vertex using the labels of its neighbouring vertices (Wang and Zhang 2008).", "startOffset": 286, "endOffset": 307}, {"referenceID": 15, "context": "Utilizing representations learned from neural networks instead of neighbouring attributes or labels is one of the possible approaches (Schmidhuber 2015).", "startOffset": 134, "endOffset": 152}, {"referenceID": 0, "context": "There has been a growing trend to represent data using graphs (Angles and Gutierrez 2008).", "startOffset": 62, "endOffset": 89}, {"referenceID": 8, "context": "Discovering knowledge from graphs becomes an exciting research area, such as vertex classification (London and Getoor 2014) and graph classification (Niepert, Ahmed, and Kutzkov 2016).", "startOffset": 99, "endOffset": 123}, {"referenceID": 9, "context": "The main-stream approaches for vertex classification are collective vertex classification (Lu and Getoor 2003) which classify vertices using information provided by neighbouring vertices.", "startOffset": 90, "endOffset": 110}, {"referenceID": 9, "context": "Iterative classification approach (Lu and Getoor 2003) models neighbours\u2019 label distribution as link features to facilitate classification.", "startOffset": 34, "endOffset": 54}, {"referenceID": 20, "context": "Label propagation approach (Wang and Zhang 2008) assigns a probabilistic label for each vertex and then fine-tunes the probability using graph structure.", "startOffset": 27, "endOffset": 48}, {"referenceID": 11, "context": "Recurrent neural collective classification (Monner and Reggia 2013) encodes neighbouring vertices via a recurrent neural network, which is hard to capture the information from vertices that are more than several steps away.", "startOffset": 43, "endOffset": 67}, {"referenceID": 16, "context": "RNN has been implemented in natural scenes parsing (Socher et al. 2011) and tree-structured sentence representation learning (Tai, Socher, and Manning 2015).", "startOffset": 51, "endOffset": 71}, {"referenceID": 16, "context": "We explore two kinds of recursive neural units which are discussed in (Socher et al. 2011; Tai, Socher, and Manning 2015).", "startOffset": 70, "endOffset": 121}, {"referenceID": 21, "context": "This is because, in real-life situations, the number of neighbours for a vertex can be very large and some of them are irrelevant for the vertex to classify (Zhang et al. 2013).", "startOffset": 157, "endOffset": 176}, {"referenceID": 6, "context": "Long Short-Term Memory Unit (LSTMU) LSTMU is one variation on RNU, which can handle the long term dependency problem by introducing memory cells and gated units (Hochreiter and Schmidhuber 1997).", "startOffset": 161, "endOffset": 194}, {"referenceID": 10, "context": "\u2022 Cora (McCallum et al. 2000) is a citation network dataset which is composed of 2708 scientific publications and 5429 citations between publications.", "startOffset": 7, "endOffset": 29}, {"referenceID": 3, "context": "\u2022 WebKB (Craven et al. 1998) is a website network collected from four computer science departments in different universities which consists of 877 web pages, 1608 hyper-links between web pages.", "startOffset": 8, "endOffset": 28}, {"referenceID": 9, "context": "\u2022 Iterative classification approach (ICA) (Lu and Getoor 2003; London and Getoor 2014) utilizes the combination of link structure and vertex features as input of a statistical machine learning model.", "startOffset": 42, "endOffset": 86}, {"referenceID": 8, "context": "\u2022 Iterative classification approach (ICA) (Lu and Getoor 2003; London and Getoor 2014) utilizes the combination of link structure and vertex features as input of a statistical machine learning model.", "startOffset": 42, "endOffset": 86}, {"referenceID": 20, "context": "\u2022 Label propagation (LP) (Wang and Zhang 2008; London and Getoor 2014) uses a statistical machine learning to give a label probability for each vertex, then propagates the label probability to all its neighbours.", "startOffset": 25, "endOffset": 70}, {"referenceID": 8, "context": "\u2022 Label propagation (LP) (Wang and Zhang 2008; London and Getoor 2014) uses a statistical machine learning to give a label probability for each vertex, then propagates the label probability to all its neighbours.", "startOffset": 25, "endOffset": 70}, {"referenceID": 6, "context": "G-LSTM can thus better capture correlations between representations with long dependencies (Hochreiter and Schmidhuber 1997).", "startOffset": 91, "endOffset": 124}], "year": 2017, "abstractText": "Collective classification of vertices is a task of assigning categories to each vertex in a graph based on both vertex attributes and link structure. Nevertheless, some existing approaches do not use the features of neighbouring vertices properly, due to the noise introduced by these features. In this paper, we propose a graphbased recursive neural network framework for collective vertex classification. In this framework, we generate hidden representations from both attributes of vertices and representations of neighbouring vertices via recursive neural networks. Under this framework, we explore two types of recursive neural units, naive recursive neural unit and long short-term memory unit. We have conducted experiments on four real-world network datasets. The experimental results show that our framework with long short-term memory model achieves better results and outperforms several competitive baseline methods.", "creator": "LaTeX with hyperref package"}}}