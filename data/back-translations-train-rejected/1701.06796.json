{"id": "1701.06796", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jan-2017", "title": "Discriminative Neural Topic Models", "abstract": "We propose a neural network based approach for learning topics from text and image datasets. The model makes no assumptions about the conditional distribution of the observed features given the latent topics. This allows us to perform topic modelling efficiently using sentences of documents and patches of images as observed features, rather than limiting ourselves to words. Moreover, the proposed approach is online, and hence can be used for streaming data. Furthermore, since the approach utilizes neural networks, it can be implemented on GPU with ease, and hence it is very scalable.", "histories": [["v1", "Tue, 24 Jan 2017 10:29:31 GMT  (500kb,D)", "https://arxiv.org/abs/1701.06796v1", "6 pages, 9 figures"], ["v2", "Tue, 28 Feb 2017 14:17:16 GMT  (512kb,D)", "http://arxiv.org/abs/1701.06796v2", "6 pages, 9 figures"]], "COMMENTS": "6 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gaurav pandey", "ambedkar dukkipati"], "accepted": false, "id": "1701.06796"}, "pdf": {"name": "1701.06796.pdf", "metadata": {"source": "CRF", "title": "Discriminative Neural Topic Models", "authors": ["Gaurav Pandey"], "emails": ["ad}@csa.iisc.ernet.in"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process, a process in which there is a process, a process in which there is a process, a process in which there is a process, a process in which there is a process in which there is a process, a process in which there is a process in which there is a process, a process in which there is a process, a process in which there is a process, a process, a process in which there is a process, a process, a process, a process and a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process. \""}, {"heading": "2 Discriminative Neural Topic Models", "text": "Let X (1),... X (n) be the observed characteristics for n objects, while Z (1),..., Z (n) are the corresponding latent classes. Let K be the number of latent classes."}, {"heading": "2.1 Modeling the words in a document", "text": "In the case of documents, X (i) j represents the embedding vector of the j th word in the ith document X (i) in an arbitrary order, and Z (i) j represents the corresponding topic or latent class. In particular, there is an embedding vector for each word in the vocabulary. The embedding is randomly initialized and practiced by backpropagating the progression of the topic. The length of the ith document is given by mi. We assume that in view of the words of the document, the topics are distributed independently, that is, the terms (Z) are characterized by backpropagation of the progression of the topic."}, {"heading": "2.2 Regularizing the model", "text": "To ensure that the model does not fit too well with the training data, we use negative samples. Specifically, for text documents, the words in the vocabulary should be randomly sampled to create a fake document. Next, we force the model to perform poorly on the fake document as follows: 1. The distributions over the topics for the words in a fake document should be highly insecure, which is equivalent to maximizing the entropy of the corresponding topic distributions, i.e. H (Z (i) j | X (i)) will be for 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 mi.2. The distribution over the topics for the words in a fake document should have a high variance, which is equivalent to maximizing the KL divergence of the topic distribution from the average of the words in the fake document, i.e. KL (PZ (i) j | PdZ (i); X (i))))."}, {"heading": "2.3 Document Clustering", "text": "To ensure reproducibility, we use a pre-edited version of the dataset 1. We discard documents with less than 2 words. For cluster tasks, we combine the training and test sets. We use two metrics to evaluate the effectiveness of the proposed clustering topic model - purity and normalized mutual information (NMI) 2. We compare the proposed model with several standard algorithms for clustering and topical modeling. These include K-means, normalized sections [10], probable latent semantic indexing (NMI)."}, {"heading": "2.4 The topics", "text": "Although the proposed DNTM model learns only the distribution of the topics given with the words, we can determine the distribution of the words with the topics as follows: First, we calculate the common probability of the topic t and the word w occurring together as follows: P (t, w) = 1n n n \u2211 i = 1 mi mi mi \u2211 j = 1 P (Z (i) j = t | X (i) j = w) P (X (i) j = w) Here P (X (i) j = w) = 1 if the i-th word of the jth document w and otherwise 0. Informally, the above equation simply calculates the probability of observing the topic t each time the word w occurs, and takes the average of all of these probabilities. Finally, to calculate the posterior word, we normalize the above word of the jth document w and 0 otherwise."}, {"heading": "3 Topic modelling in images", "text": "In order to expand the proposed model for images, it is necessary to define \"words\" and \"documents\" for images. The most common approach to extracting words from images involves extracting features (SIFT, HOG, etc.) from images, which are then bundled with K-Means. The corresponding quantified features are used as words [???] for theme modeling. The learning of features occurs independently of theme modeling, resulting in suboptimal features for theme modeling. Another approach taken into account in [?] involves the formation of a deep Boltzmann machine on the pixels of an image. Samples from the deepest latent layer of the DBM are then used as words for the formation of a hierarchical Bayesian model. In this work, however, we learn the words from the pixels of an image through the use of a revolutionary network. Specifically, we let X (i) represent the ith image, Y (j) represent the function of the j-j of the image and the corresponding word i-word in the image."}, {"heading": "3.1 Treating images as documents", "text": "We use the proposed model to learn topics in the CIFAR-10 dataset. The dataset consists of 50,000 training images and 10,000 test images, divided into 10 classes. This dataset is quite difficult as there is a high variability within each class, although the individual images only have 32x32 pixels. We use Convolutionary Neural Networks to obtain the properties from the images. In particular, for the 32x32 CIFAR-10 images we apply 4 layers of Stretched Folding and ReLU Nonlinearity to obtain 64 (8x8) features per image. These features function as words and are fed into the DNTM as input. Note that CNN is trained only by the DNTM, thereby coupling feature extraction and theme modeling. We train the DNTM to extract 100 topics from the CIFAR-10 dataset. 9 of these topics are presented in Figure 3. Specifically, we appear to have the topic most likely to be associated with each of the 24th.We train the DNTM to extract 100 topics from the CIFAR-10 dataset."}], "references": [{"title": "Latent Dirichlet Allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Semi-supervised classification by low density separation", "author": ["Olivier Chapelle", "Alexander Zien"], "venue": "In AISTATS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["Li Fei-Fei", "Pietro Perona"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Yves Grandvalet", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Probabilistic Latent Semantic Indexing", "author": ["Thomas Hofmann"], "venue": "Proceedings of the 22nd annual International ACM SIGIR conference on Research and Development in Information Retrieval,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Discriminative clustering by regularized information maximization", "author": ["Andreas Krause", "Pietro Perona", "Ryan G Gomes"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Investigating task performance of probabilistic topic models: an empirical study of plsa and lda", "author": ["Yue Lu", "Qiaozhu Mei", "ChengXiang Zhai"], "venue": "Information Retrieval,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Inference of population structure using multilocus genotype", "author": ["Jonathan K Pritchard", "Matthew Stephens", "Peter Donnelly"], "venue": "data. Genetics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "IEEE Transactions on pattern analysis and machine intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}], "referenceMentions": [{"referenceID": 5, "context": "A topic is assumed to have a multinomial distribution over the words, while a document is assumed to have a multinomial distribution over the topics [6].", "startOffset": 149, "endOffset": 152}, {"referenceID": 0, "context": "One can further define Dirichlet priors on the parameters of the multinomial distributions, to obtain a full Bayesian treatment of topic modeling [1, 9].", "startOffset": 146, "endOffset": 152}, {"referenceID": 8, "context": "One can further define Dirichlet priors on the parameters of the multinomial distributions, to obtain a full Bayesian treatment of topic modeling [1, 9].", "startOffset": 146, "endOffset": 152}, {"referenceID": 2, "context": "An intermediate preprocessing step is required that transforms all the observed features in the collection to a relatively small set of unique \u2018codewords\u2019 [3].", "startOffset": 155, "endOffset": 158}, {"referenceID": 3, "context": "In order to prevent the model from overfitting, we use adversarial training by coupling it with a generator [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 6, "context": "This is also referred to as cluster assumption, and has been used for clustering [7] and semi supervised learning [5, 2], where it enjoys considerable success.", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "This is also referred to as cluster assumption, and has been used for clustering [7] and semi supervised learning [5, 2], where it enjoys considerable success.", "startOffset": 114, "endOffset": 120}, {"referenceID": 1, "context": "This is also referred to as cluster assumption, and has been used for clustering [7] and semi supervised learning [5, 2], where it enjoys considerable success.", "startOffset": 114, "endOffset": 120}, {"referenceID": 9, "context": "These include K-means, Normalized cuts [10], probabilistic Latent Semantic indexing (pLSI) and Latent Dirichlet Allocation (LDA).", "startOffset": 39, "endOffset": 43}, {"referenceID": 7, "context": "This approach has been shown to be more effective than clustering the topic representations of documents [8].", "startOffset": 105, "endOffset": 108}], "year": 2017, "abstractText": "We propose a neural network based approach for learning topics from text and image datasets. The model makes no assumptions about the conditional distribution of the observed features given the latent topics. This allows us to perform topic modelling efficiently using sentences of documents and patches of images as observed features, rather than limiting ourselves to words. Moreover, the proposed approach is online, and hence can be used for streaming data. Furthermore, since the approach utilizes neural networks, it can be implemented on GPU with ease, and hence it is very scalable.", "creator": "LaTeX with hyperref package"}}}