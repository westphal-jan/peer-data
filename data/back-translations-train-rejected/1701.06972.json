{"id": "1701.06972", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jan-2017", "title": "Deep Network Guided Proof Search", "abstract": "Deep learning techniques lie at the heart of several significant AI advances in recent years including object recognition and detection, image captioning, machine translation, speech recognition and synthesis, and playing the game of Go. Automated first-order theorem provers can aid in the formalization and verification of mathematical theorems and play a crucial role in program analysis, theory reasoning, security, interpolation, and system verification. Here we suggest deep learning based guidance in the proof search of the theorem prover E. We train and compare several deep neural network models on the traces of existing ATP proofs of Mizar statements and use them to select processed clauses during proof search. We give experimental evidence that with a hybrid, two-phase approach, deep learning based guidance can significantly reduce the average number of proof search steps while increasing the number of theorems proved. Using a few proof guidance strategies that leverage deep neural networks, we have found first-order proofs of 7.36% of the first-order logic translations of the Mizar Mathematical Library theorems that did not previously have ATP generated proofs. This increases the ratio of statements in the corpus with ATP generated proofs from 56% to 59%.", "histories": [["v1", "Tue, 24 Jan 2017 16:39:05 GMT  (263kb,D)", "http://arxiv.org/abs/1701.06972v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.LO", "authors": ["sarah loos", "geoffrey irving", "christian szegedy", "cezary kaliszyk"], "accepted": false, "id": "1701.06972"}, "pdf": {"name": "1701.06972.pdf", "metadata": {"source": "CRF", "title": "Deep Network Guided Proof Search", "authors": ["Sarah Loos", "Geoffrey Irving", "Christian Szegedy", "Cezary Kaliszyk"], "emails": ["smloos@google.com", "geoffreyi@google.com", "szegedy@google.com", "cezary.kaliszyk@uibk.ac.at"], "sections": [{"heading": null, "text": "Automated first-order theory testers can help formalize and verify mathematical theorems and play a critical role in program analysis, theory thinking, safety, interpolation, and system verification. Here, we propose deep learning guidance in the sample search of the theory tester E. We train and compare several deep neural network models based on the traces of existing ATP evidence of Mizar statements and use it to select processed clauses during the sample search. We provide experimental evidence that using a hybrid, two-phase approach, a deep learning-based guide significantly reduces the average number of evidence collection steps and at the same time increases the number of proven theorems. With a few evidence-taking strategies utilizing deep neural networks, we have first-order evidence for 7.36% of first-order logical translations of the mathematical library's theorems, which had no prior evidence from the 59% of ATP."}, {"heading": "1 Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Motivation", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "1.2 Contributions", "text": "For the first time, we evaluate deep network models as a method of evidence within an automated theorem tester. We describe the formalities that need to be addressed for the successful integration of relatively slow neural network models into a system based on very fast exploration. Experimental results are given on a large corpus of mathematical statements, which are translated from the mathematical library Mizar (Grabowski et al., 2010) to the first-order logic (Urban, 2006), which covers significant parts of basic mathematics, from discrete mathematics and mathematical logic to calculus and algebra. Applying deep network models within the evidence-finding process is challenging due to the relatively expensive nature of neural network inferences. While one clause is evaluated by a neural network (on CPU), several hundred or thousands of superimposition steps can be performed by the tester. This means that the evidence on neural networks alone is very good quality suggestions we need to describe in order to have a positive effect on this two important aspects."}, {"heading": "2 Related Work", "text": "Denzinger et al. (1999) explored learning and knowledge-based approaches to conduct automated theory auditors. Schulz (2000) proposed a k-NN-based learning methodology with handmade features based on a normalized form of clauses for the selection of the next clause to be processed. Schulz (2000) proposed na\u00efve Bayes and black-based approaches for the internal management of the Givenclause algorithm in Satallax (F\u00e4rber & Brown, 2016). FEMaLeCoP uses na\u00efve Bayes to guide its tableaux proof search (Kaliszyk & Urban, 2015b)."}, {"heading": "3 Theorem Proving Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The First-Order Logic Prover E", "text": "We aim at the first-order saturation-based logic theorem Prover E (Schulz, 2013). E processes all input in clerical normal form (CNF) and performs the proof search in this form. A firstorder formula is in CNF if it is a combination of clauses in which each clause represents a separation of words, each dictionary being a (possibly negated) application of a k predicate symbol to k terms, and in which the terms are applied recursively to either variables or functions on terms. E's proof proof search is parameterized by a variety of arguments, the most important of which is the clause selection heuristic, on which we focus in this work. In a saturation theorem proving two sets of clauses are manipulated: a set of unprocessed clauses initialized on the clerical form of the problem, and an empty set of clauses first."}, {"heading": "3.2 Mizar First-Order Problems", "text": "The Mizar Mathematical Library (MML) (Grabowski et al., 2015) is a library of formal mathematics developed on the Mizar system (Bancerek et al., 2015).Today, the MML is one of the largest libraries of formalized evidence covering most basic areas of mathematics and some computer proofs.The foundations of Mizar have been transferred from version 4.181,1147 of MML to the first target for experiments combining machine learning with automated thinking, and the most comprehensive evaluation of AI-ATP methods to the library has been conducted by Kaliszyk & Urban."}, {"heading": "4 Deep Networks", "text": "Here we describe the data collection, network architectures and training methodology of our deep learning models that are included in the evidence procedure of Schulz et al. \"s Automated Theorem Prover E. We have reused several architecture and training decisions that proved successful in the earlier contiguous presumption selection work by Alemi et al. (2016). Our models have two inputs: a negated assumption that needs to be proven, and an unprocessed clause that is both used in CNF. Each is reduced to a fixed size vector that uses an embedding network, and a combiner network combines the two embedding vectors into a score for use as clause selection heuristic. Ideally, the score would depend on the current clauses processed, but we have limited the inputs to nested conjection and clause for simplicity and speed. Thus, the entire architecture isvc = femb (clause wc = femfemc) conjecture (conjecture) nnegip is vc (wc = femb)."}, {"heading": "4.1 Data Collection", "text": "As described in Section 3.2, we have divided the 32,521 Mizar theorems that have at least one proof into a training and validation set, using a 90% -10% split. Evidence in our data set comes from various first-order ATPs. Furthermore, the pre-processing configuration (scolemization and clausification), even for evidence tracks emanating from E, has a major impact on the symbols that appear in the clauses and their form. Therefore, we have replicated the evidence using a single, consistent configuration of the pre-processing process of E to avoid a mismatch between our training set and the clauses that appear when searching with the trained model. Specifically, we train using evidence created using the Auto208 configuration detailed in the appendix."}, {"heading": "4.2 Architectures", "text": "We look at three architectures for embedding networks femb: simple convolutional models, WaveNet models (van den Oord et al., 2016), and recursive networks (Goller & Kuchler, 1996) All experiments use the same combiner network gcomb with a hidden layer of 1024 units: gcomb (vc, vnc) = W2 relu (W1 concat (vc, vnc) + b2) + b2where W1, W1, R1024 \u00d7 2 dim v, W2, R1, R1 \u00d7 1024, b1, R1024, and b2, R. In data acquisition, we collect all symbols (constants, function names, variable names, logical operations, and brackets) into a vocabulary, and the input layer turns into an embedding vector of the dimension 1024, with embedding of constants, function names, variable names, logical operations, and parentheses."}, {"heading": "4.3 Simple Convolutional Model", "text": "According to the premise selection models by Alemi et al. (2016), our revolutionary models (\"CNNs\" for \"Convolutionary Neural Networks\") have a relatively low depth because they provide good results in this task. They consist of a stack of three one-dimensional Convolutionary Layers, each layer with patch size 5 (the number of inputs to which each output is connected), step 1, and rectified linear activation. We have tried several models with different feature dimensions."}, {"heading": "4.4 WaveNet Model", "text": "WaveNet (van den Oord et al., 2016) is a special type of hierarchical revolutionary network that uses extended coils and residual connections, allowing dependencies on properties with only moderate overhead since higher layers exhibit geometrically increasing dilatation factors (see Figure 2). While van den Oord et al. (2016) uses causally extended coils, we use symmetrical coils as our task discrimination, not generation. Our WaveNet embedding network consists of 3 WaveNet blocks, each block consisting of 7 convolutional layers added by d = 1, 2, 4,.,.,, 64. We use the gated activation tanh (x) of van den Oord et al al al al al al al al al al al al al al al al al al. (2016) and residual connections for both layers and blocks (He et al., 2015) x x x x x."}, {"heading": "4.5 Recursive Neural Network", "text": "Our third model class are recursive neural networks (Socher et al., 2011) constructed through the parse tree of the first order logic formulas. The topology of neural networks reflects the parse tree and is wired by collecting operations in TensorFlow. To simplify the tree, all functional applications are curated, giving rise to the following layer types: \u2022 Application of nodes that perform a function with exactly two children. \u2022 or nodes that calculate the embedding for the separation of exactly two children. \u2022 and nodes that calculate the embedding for the conjunction of exactly two children. These are used only for embedding the negated guess, as the proof clauses do not contain any conjunctions. \u2022 No nodes that calculate the embedding for the negation of a single child node. \u2022 and nodes that calculate the embedding for the conjunction of exactly two children."}, {"heading": "5 Experimental Results", "text": "After training the deep neural networks as described in Section 4, we evaluate them using three metrics, each of which is more conclusive and mathematically expensive than the last; the first metric presented in Section 5.1 checks whether a trained model can accurately predict whether a clause in the final proof has been used; this accuracy test is performed using an extensive set of evidence requirements from the training data; next, in Section 5.2, we perform the same set of 9,159 FOL evidence requirements from Kaliszyk & Urban (2015a) using the trained networks to guide the selection of the clause; these theories all have at least one ATP evidence using classical search heuristics, and 96.6% of these theorems can be detected using one of the integrated automated hayristics of e-testers, so there is little scope for improving these data sets."}, {"heading": "5.1 Accuracy Evaluations", "text": "Table 1 shows the accuracy of a 50-50% portion of the used and unused clauses of the holdout set of assumptions. Here, CNN-N \u00d7 L is a coil mesh with L layers of N dimensions, and Tree-Type-N \u00b7 L is a tree RNN or LSTM with L layers of N dimensions at each node of the input tree. WaveNet-N \u00d7 B \u00b7 L has B blocks with L layers of N dimension. We include (D%) to indicate that a dropout of D% is used during training as a regulator. WaveNet 640 with Dropout has the best accuracy at 81.5% accuracy, but many of the CNN and WaveNet models perform significantly better than the others. Note that the time required to evaluate the models for a number of examples varies greatly. Given that we have the total run-time of the models, rather than the higher number of system evaluations, however, we can gain more specialized comparisons to the number of models carried out, which is more important."}, {"heading": "5.2 Experiments on Statements with Existing ATP Proofs", "text": "With good heuristics, evidence can be found in a relatively small number of search steps. In this section, we will use the models from Section 5.1 with the highest accuracy as a clause selection heuristic within E.2 These models, trained to select clauses most likely to contribute to a proof, now assign a score to each unprocessed sentence, namely the trained value of p (useful | c, nc). In other words, the probability that a clause2Due was unable to experiment with WaveNet-1024 in E-prover.c due to its enormous storage needs is used in the final proof, since the set of negated assumptions nc. E-Prover uses this score to evaluate the set of unprocessed clauses, and then the clause with the best ranking in each step."}, {"heading": "5.2.1 Guidance Design", "text": "In our experiments, we found that our calculation of heavy neural networks works much better when used in accordance with existing, fast heuristics. In this section, we present various approaches to using neural networks to guide existing heuristics. 1. Auto-heuristics is standard in E (see Appendix). This approach is already a hybrid of several weight functions and coordinated parameters, but it does not include conclusions from a deep neural network. 2. A purely neural network of heuristic scores uses clauses that only use the trained neural network. 3. This approach is slow because it needs to evaluate all clauses with the expensive neural network, but cannot draw conclusions from ranking, as the auto-function does, and we observe that it does not work well. 3. A hybrid approach switches between deep network-based guidance and fast auto-heuristics."}, {"heading": "5.2.2 Comparison of Model Performance on Easy Statements", "text": "In this section, we present the performance of four WaveNet and two CNN models that are used to guide the selection of clauses in the e-verifier. These are the models with the highest accuracy from Table 1 that are still small enough to fit into memory. We also include all three models that have been trained on a data set that contains unprocessed clauses as negative examples. These models are marked with (*). All experiments use the e-theorem verifier, version 1.9.1pre014 (Schulz, 2013), with some modifications to allow integration with the trained deep neural networks as heuristics. In Figure 4 on the right and Table 2, we show the performance of each of these models with different processing clauses limitations. All heuristics that benefit from deep neural network management exceed the auto-baseline, regardless of the number of clauses that are processed before evidence is found."}, {"heading": "5.3 Experiments on Hard Statements", "text": "Here we describe our results using a corpus of 25,361 theorems from the Mizar corpus, for which none of the testers, strategies and presumption selection methods considered by Kaliszyk & Urban (2015a) have been found. We will call these \"hard statements\" in the rest of the paper \"hard statements.\" Although all hard statements have human evidence, this subset could not be proved either by the theory tester E (Schulz, 2013) or by Vampir (Kov\u00e1cs & Voronkov, 2013) with a time limit of 15 minutes and default settings (auto or casc heuristics) based on the premises derived from the human evidence. Also, note that the premises submitted to the theorem testers form a very rough approximation of the set of necessary premises. Often, about 10 to 20 dependencies are sufficient to prove the theorems, but the number of dependencies may lie in the backward of the premises."}, {"heading": "5.3.1 The Importance of Premise Selection", "text": "Here we start with a comparison of four different approaches to the hard statements, including guided and unsupervised evidence search with and without selection of the premises. We use the character-level model from the DeepMath paper (Alemi et al., 2016) for the premises selection, which is very similar to our Convolutionary Evidence Model, but was trained using presumption-conclusion pairs from a training set randomly selected from the 56% ATP-tested statements. A major difference is that our Constitutional Leadership Network is word level with an embedding that was learned for each premise, while the premise selection network takes strings as input. This limits the quality, as the DeepMath work suggests, that the use of a word-level model with definite embedding provides the best results. We opted for the qualitatively worse premise selection model to select the premises models that are not exceeded by the number of variants selected for the simple selection of the 2564 premises."}, {"heading": "5.3.2 Comparison of Model Performance on Hard Statements", "text": "All evidence-based methods used the two-phase approach, which initially performs a hybrid network-led phase for 20 minutes, followed by a standard heuristics-based phase for 10 minutes. In this way, we have tried two different premise selection models, \"DeepMath 1\" and \"DeepMath 2,\" where we can evaluate the stability of the result in terms of the premise selection strategy. It can be seen that the two models are designed in the same way and have similar quality, but lead to significantly different theorems, the number of theorems proven with both models is very similar, and the \"switch\" strategy with the simpler CNN cut off best in both premise selection and evidence by a significant margin. Experimental results are given in Table 4. CNN models use simple three-layered folding networks to evaluate the unprocessed clauses, while WaveNet models use the WaveNet architecture, which is 8% lower overall, but 86.0% lower on the approach."}, {"heading": "6 Conclusion", "text": "We have shown that the feasibility of first-order logic detection search by extending the given clause selection method with ranking by deep neural networks. With a properly constructed mix of neural guidance and handmade search strategies, we get significant improvements in first-order logic detection performance, especially for theorems that are more difficult and require deeper searching. Due to the slowness of neural network evaluation, our method leads to an increased number of successful detections only when we use it in a two-phase approach, in which a deep network phase is followed by a faster combinatorial search with specific clauses that are quickly selected using existing handmade clause selection strategies. Furthermore, we have found that both the predictive accuracy of the deep network and its speed are important to increase performance."}], "references": [{"title": "TensorFlow: Large-scale machine learning", "author": ["Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "on heterogeneous systems,", "citeRegEx": "Talwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Talwar et al\\.", "year": 2015}, {"title": "Automated and Human Proofs in General Mathematics: An Initial Comparison", "author": ["Jesse Alama", "Daniel K\u00fchlwein", "Josef Urban"], "venue": "LPAR, volume 7180 of LNCS,", "citeRegEx": "Alama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alama et al\\.", "year": 2012}, {"title": "Deepmath-deep sequence models for premise selection", "author": ["Alexander A Alemi", "Francois Chollet", "Geoffrey Irving", "Niklas Een", "Christian Szegedy", "Josef Urban"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Alemi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alemi et al\\.", "year": 2016}, {"title": "Structure formation in large theories", "author": ["Serge Autexier", "Dieter Hutter"], "venue": "Intelligent Computer Mathematics - International Conference,", "citeRegEx": "Autexier and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Autexier and Hutter.", "year": 2015}, {"title": "Mizar: State-of-the-art and beyond", "author": ["Grzegorz Bancerek", "Czes\u0142aw Byli\u0144ski", "Adam Grabowski", "Artur Korni\u0142owicz", "Roman Matuszewski", "Adam Naumowicz", "Karol P\u0105k", "Josef Urban"], "venue": "Intelligent Computer Mathematics - International Conference,", "citeRegEx": "Bancerek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bancerek et al\\.", "year": 2015}, {"title": "Hammering towards QED", "author": ["Jasmin Christian Blanchette", "Cezary Kaliszyk", "Lawrence C. Paulson", "Josef Urban"], "venue": "J. Formalized Reasoning,", "citeRegEx": "Blanchette et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blanchette et al\\.", "year": 2016}, {"title": "Understanding LSTM networks, 2015. URL https://colah.github.io/posts/ 2015-08-Understanding-LSTMs", "author": ["Chris Olah"], "venue": null, "citeRegEx": "Olah.,? \\Q2015\\E", "shortCiteRegEx": "Olah.", "year": 2015}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.02367,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Learning from Previous Proof Experience", "author": ["J\u00f6rg Denzinger", "Matthias Fuchs", "Christoph Goller", "Stephan Schulz"], "venue": "Technical Report AR99-4,", "citeRegEx": "Denzinger et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Denzinger et al\\.", "year": 1999}, {"title": "Internal guidance for Satallax", "author": ["Michael F\u00e4rber", "Chad E. Brown"], "venue": "International Joint Conference on Automated Reasoning (IJCAR 2016),", "citeRegEx": "F\u00e4rber and Brown.,? \\Q2016\\E", "shortCiteRegEx": "F\u00e4rber and Brown.", "year": 2016}, {"title": "Learning task-dependent distributed representations by backpropagation through structure", "author": ["Christoph Goller", "Andreas Kuchler"], "venue": "In Neural Networks,", "citeRegEx": "Goller and Kuchler.,? \\Q1996\\E", "shortCiteRegEx": "Goller and Kuchler.", "year": 1996}, {"title": "Mizar in a nutshell", "author": ["Adam Grabowski", "Artur Korni\u0142owicz", "Adam Naumowicz"], "venue": "J. Formalized Reasoning,", "citeRegEx": "Grabowski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Grabowski et al\\.", "year": 2010}, {"title": "Four decades of Mizar - foreword", "author": ["Adam Grabowski", "Artur Korni\u0142owicz", "Adam Naumowicz"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Grabowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grabowski et al\\.", "year": 2015}, {"title": "A formal proof of the Kepler conjecture", "author": ["Thomas C. Hales", "Mark Adams", "Gertrud Bauer", "Dat Tat Dang", "John Harrison", "Truong Le Hoang", "Cezary Kaliszyk", "Victor Magron", "Sean McLaughlin", "Thang Tat Nguyen", "Truong Quang Nguyen", "Tobias Nipkow", "Steven Obua", "Joseph Pleso", "Jason Rute", "Alexey Solovyev", "An Hoai Thi Ta", "Trung Nam Tran", "Diep Thi Trieu", "Josef Urban", "Ky Khac Vu", "Roland Zumkeller"], "venue": "CoRR, abs/1501.02155,", "citeRegEx": "Hales et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hales et al\\.", "year": 2015}, {"title": "History of interactive theorem proving", "author": ["John Harrison", "Josef Urban", "Freek Wiedijk"], "venue": "URL http://www.sciencedirect.com/science/article/ pii/B9780444516244500046", "citeRegEx": "Harrison et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Harrison et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath", "Brian Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "MizAR 40 for Mizar 40", "author": ["Cezary Kaliszyk", "Josef Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Kaliszyk and Urban.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk and Urban.", "year": 2015}, {"title": "FEMaLeCoP: Fairly efficient machine learning connection prover", "author": ["Cezary Kaliszyk", "Josef Urban"], "venue": "20th International Conference,", "citeRegEx": "Kaliszyk and Urban.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk and Urban.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "seL4: formal verification of an operating-system kernel", "author": ["Gerwin Klein", "June Andronick", "Kevin Elphinstone", "Gernot Heiser", "David Cock", "Philip Derrin", "Dhammika Elkaduwe", "Kai Engelhardt", "Rafal Kolanski", "Michael Norrish", "Thomas Sewell", "Harvey Tuch", "Simon Winwood"], "venue": "Commun. ACM,", "citeRegEx": "Klein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2010}, {"title": "First-order theorem proving and Vampire", "author": ["Laura Kov\u00e1cs", "Andrei Voronkov"], "venue": "CAV, volume 8044 of LNCS,", "citeRegEx": "Kov\u00e1cs and Voronkov.,? \\Q2013\\E", "shortCiteRegEx": "Kov\u00e1cs and Voronkov.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Overview and evaluation of premise selection techniques for large theory mathematics", "author": ["Daniel K\u00fchlwein", "Twan van Laarhoven", "Evgeni Tsivtsivadze", "Josef Urban", "Tom Heskes"], "venue": "IJCAR, volume 7364 of LNCS,", "citeRegEx": "K\u00fchlwein et al\\.,? \\Q2012\\E", "shortCiteRegEx": "K\u00fchlwein et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Metamath: A Computer Language for Pure Mathematics", "author": ["Norman Megill"], "venue": "URL http://us.metamath.org/ downloads/metamath.pdf", "citeRegEx": "Megill.,? \\Q2007\\E", "shortCiteRegEx": "Megill.", "year": 2007}, {"title": "Wavenet: A generative model for raw audio", "author": ["A\u00e4ron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Learning knowledge base inference with neural theorem provers", "author": ["Tim Rockt\u00e4schel", "Sebastian Riedel"], "venue": "5th Workshop on Automated Knowledge Base Construction (AKBC)", "citeRegEx": "Rockt\u00e4schel and Riedel.,? \\Q2016\\E", "shortCiteRegEx": "Rockt\u00e4schel and Riedel.", "year": 2016}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1509.06664,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Learning search control knowledge for equational deduction, volume 230 of DISKI", "author": ["Stephan Schulz"], "venue": "Infix Akademische Verlagsgesellschaft,", "citeRegEx": "Schulz.,? \\Q2000\\E", "shortCiteRegEx": "Schulz.", "year": 2000}, {"title": "Logic for Programming, Artificial Intelligence, and Reasoning - 19th International Conference, LPAR-19, volume 8312 of LNCS, pp. 735\u2013743", "author": ["Stephan Schulz"], "venue": null, "citeRegEx": "Schulz.,? \\Q2013\\E", "shortCiteRegEx": "Schulz.", "year": 2013}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "The TPTP problem library and associated infrastructure", "author": ["Geoff Sutcliffe"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Sutcliffe.,? \\Q2009\\E", "shortCiteRegEx": "Sutcliffe.", "year": 2009}, {"title": "Automatic acquisition of search guiding heuristics", "author": ["Christian Suttner", "Wolfgang Ertel"], "venue": "In International Conference on Automated Deduction,", "citeRegEx": "Suttner and Ertel.,? \\Q1990\\E", "shortCiteRegEx": "Suttner and Ertel.", "year": 1990}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher DManning"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "MPTP 0.2: Design, implementation, and initial experiments", "author": ["Josef Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Urban.,? \\Q2006\\E", "shortCiteRegEx": "Urban.", "year": 2006}, {"title": "MaLeCoP: Machine learning connection prover", "author": ["Josef Urban", "Ji\u0159\u00ed Vysko\u010dil", "Petr \u0160t\u011bp\u00e1nek"], "venue": "TABLEAUX, volume 6793 of LNCS,", "citeRegEx": "Urban et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Urban et al\\.", "year": 2011}, {"title": "Isar - A generic interpretative approach to readable formal proof documents", "author": ["Markus Wenzel"], "venue": "Theorem Proving in Higher Order Logics, 12th International Conference, TPHOLs\u201999,", "citeRegEx": "Wenzel.,? \\Q1999\\E", "shortCiteRegEx": "Wenzel.", "year": 1999}, {"title": "Holophrasm: a neural automated theorem prover for higher-order logic", "author": ["Daniel Whalen"], "venue": "arXiv preprint arXiv:1608.02644,", "citeRegEx": "Whalen.,? \\Q2016\\E", "shortCiteRegEx": "Whalen.", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "In the past twenty years, various large corpora of computer-understandable reasoning knowledge have been developed (Harrison et al., 2014).", "startOffset": 115, "endOffset": 138}, {"referenceID": 35, "context": "This is either given in the form of premises-conclusion pairs (Sutcliffe, 2009) or as procedures and intermediate steps (Wenzel, 1999).", "startOffset": 62, "endOffset": 79}, {"referenceID": 41, "context": "This is either given in the form of premises-conclusion pairs (Sutcliffe, 2009) or as procedures and intermediate steps (Wenzel, 1999).", "startOffset": 120, "endOffset": 134}, {"referenceID": 39, "context": "Furthermore, the AI methods can be augmented by automated reasoning: progress in the development of efficient first-order automated theorem provers (ATPs) (Kov\u00e1cs & Voronkov, 2013) allows applying them not only as tools that redo the formal proofs, but also to find the missing steps (Urban, 2006).", "startOffset": 284, "endOffset": 297}, {"referenceID": 5, "context": "Together with proof translations from the richer logics of the interactive systems to the simpler logics of the ATPs this becomes a commonly used tool in certain interactive provers (Blanchette et al., 2016).", "startOffset": 182, "endOffset": 207}, {"referenceID": 13, "context": "Examples include the formal proof of the Kepler conjecture (Hales et al., 2015), or the proof of correctness of the seL4 operating system kernel (Klein et al.", "startOffset": 59, "endOffset": 79}, {"referenceID": 22, "context": ", 2015), or the proof of correctness of the seL4 operating system kernel (Klein et al., 2010).", "startOffset": 73, "endOffset": 93}, {"referenceID": 25, "context": "For this reason AI-based heuristic and learning techniques are used to preselect lemmas externally (K\u00fchlwein et al., 2012).", "startOffset": 99, "endOffset": 122}, {"referenceID": 12, "context": "(2012) show that ATPs are only able to find proofs of up to twenty human steps among the proofs in the Mizar Mathematical Library (Grabowski et al., 2015), whereas the library contains a number of proofs with hundreds of steps.", "startOffset": 130, "endOffset": 154}, {"referenceID": 40, "context": "For the tableaux calculus the Machine Learning Connection Prover (MaLeCoP) (Urban et al., 2011) shows that using machine learning to choose the next step can reduce the number of inferences in the proofs on average by a factor of 20.", "startOffset": 75, "endOffset": 95}, {"referenceID": 32, "context": "As the most competitive ATPs today are not based on tableau, but instead rely on the superposition calculus, in this paper we investigate guiding the state-of-the-art automated prover E (Schulz, 2013)1 using deep neural networks.", "startOffset": 186, "endOffset": 200}, {"referenceID": 26, "context": "Deep convolutional neural networks (LeCun et al., 1998) lie at the heart of several recent AI breakthroughs in the past few years.", "startOffset": 35, "endOffset": 55}, {"referenceID": 20, "context": ", 2012a) and natural language processing (Kim, 2014).", "startOffset": 41, "endOffset": 52}, {"referenceID": 24, "context": "Object recognition has reached human level performance on large benchmarks (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2015) due to the inroads of deep convolutional neural networks.", "startOffset": 75, "endOffset": 139}, {"referenceID": 37, "context": "Object recognition has reached human level performance on large benchmarks (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2015) due to the inroads of deep convolutional neural networks.", "startOffset": 75, "endOffset": 139}, {"referenceID": 15, "context": "Object recognition has reached human level performance on large benchmarks (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2015) due to the inroads of deep convolutional neural networks.", "startOffset": 75, "endOffset": 139}, {"referenceID": 33, "context": "DeepMind\u2019s AlphaGo (Silver et al., 2016) demonstrated superhuman performance in playing the game of Go by utilizing deep convolution neural networks that evaluate board positions.", "startOffset": 19, "endOffset": 40}, {"referenceID": 1, "context": "Even with external selection Alama et al. (2012) show that ATPs are only able to find proofs of up to twenty human steps among the proofs in the Mizar Mathematical Library (Grabowski et al.", "startOffset": 29, "endOffset": 49}, {"referenceID": 11, "context": "Experimental results are given on a large corpus of mathematical statements translated from the Mizar Mathematical Library (Grabowski et al., 2010) to first-order logic (Urban, 2006) which covers significant parts of basic mathematics ranging from discrete mathematics and mathematical logic to calculus and algebra.", "startOffset": 123, "endOffset": 147}, {"referenceID": 39, "context": ", 2010) to first-order logic (Urban, 2006) which covers significant parts of basic mathematics ranging from discrete mathematics and mathematical logic to calculus and algebra.", "startOffset": 29, "endOffset": 42}, {"referenceID": 2, "context": "Convolutional neural networks were used successfully for premise selection in Mizar (Alemi et al., 2016), and we use similar models for proof guidance here.", "startOffset": 84, "endOffset": 104}, {"referenceID": 7, "context": "Whalen (2016) proposed a complete neural network based theorem prover architecture leveraging GRU (Chung et al., 2015) networks to guide the proof search of a tableau style proof process for the MetaMath system (Megill, 2007).", "startOffset": 98, "endOffset": 118}, {"referenceID": 27, "context": ", 2015) networks to guide the proof search of a tableau style proof process for the MetaMath system (Megill, 2007).", "startOffset": 100, "endOffset": 114}, {"referenceID": 6, "context": "Denzinger et al. (1999) surveyed learning and knowledge base based approaches to guide automated theorem provers.", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "Denzinger et al. (1999) surveyed learning and knowledge base based approaches to guide automated theorem provers. Schulz (2000) proposed a k-NN based learning methodology with hand-crafted features on a normalized form of the clauses for the selecting the next clause to be processed.", "startOffset": 0, "endOffset": 128}, {"referenceID": 2, "context": "Convolutional neural networks were used successfully for premise selection in Mizar (Alemi et al., 2016), and we use similar models for proof guidance here. Whalen (2016) proposed a complete neural network based theorem prover architecture leveraging GRU (Chung et al.", "startOffset": 85, "endOffset": 171}, {"referenceID": 2, "context": "Convolutional neural networks were used successfully for premise selection in Mizar (Alemi et al., 2016), and we use similar models for proof guidance here. Whalen (2016) proposed a complete neural network based theorem prover architecture leveraging GRU (Chung et al., 2015) networks to guide the proof search of a tableau style proof process for the MetaMath system (Megill, 2007). A fully differential theorem prover was tested on a few toy problems by Rockt\u00e4schel & Riedel (2016). Entailment analysis via deep learning was proposed by Rockt\u00e4schel et al.", "startOffset": 85, "endOffset": 484}, {"referenceID": 2, "context": "Convolutional neural networks were used successfully for premise selection in Mizar (Alemi et al., 2016), and we use similar models for proof guidance here. Whalen (2016) proposed a complete neural network based theorem prover architecture leveraging GRU (Chung et al., 2015) networks to guide the proof search of a tableau style proof process for the MetaMath system (Megill, 2007). A fully differential theorem prover was tested on a few toy problems by Rockt\u00e4schel & Riedel (2016). Entailment analysis via deep learning was proposed by Rockt\u00e4schel et al. (2015).", "startOffset": 85, "endOffset": 565}, {"referenceID": 32, "context": "We target the saturation-based first-order logic theorem prover E (Schulz, 2013).", "startOffset": 66, "endOffset": 80}, {"referenceID": 12, "context": "The Mizar Mathematical Library (MML) (Grabowski et al., 2015) is a library of formal mathematics developed on top of the Mizar system (Bancerek et al.", "startOffset": 37, "endOffset": 61}, {"referenceID": 4, "context": ", 2015) is a library of formal mathematics developed on top of the Mizar system (Bancerek et al., 2015).", "startOffset": 80, "endOffset": 103}, {"referenceID": 4, "context": ", 2015) is a library of formal mathematics developed on top of the Mizar system (Bancerek et al., 2015). The MML is today one of the largest libraries of formalized proofs, covering most basic domains of mathematics and some computer science proofs. The foundations of Mizar have been translated to first-order logic by Urban (2006) making MML the first target for experiments combining machine learning with automated reasoning.", "startOffset": 81, "endOffset": 333}, {"referenceID": 4, "context": ", 2015) is a library of formal mathematics developed on top of the Mizar system (Bancerek et al., 2015). The MML is today one of the largest libraries of formalized proofs, covering most basic domains of mathematics and some computer science proofs. The foundations of Mizar have been translated to first-order logic by Urban (2006) making MML the first target for experiments combining machine learning with automated reasoning. The most extensive evaluation of AI-ATP methods on the library has been performed by Kaliszyk & Urban (2015a) on MML version 4.", "startOffset": 81, "endOffset": 540}, {"referenceID": 4, "context": ", 2015) is a library of formal mathematics developed on top of the Mizar system (Bancerek et al., 2015). The MML is today one of the largest libraries of formalized proofs, covering most basic domains of mathematics and some computer science proofs. The foundations of Mizar have been translated to first-order logic by Urban (2006) making MML the first target for experiments combining machine learning with automated reasoning. The most extensive evaluation of AI-ATP methods on the library has been performed by Kaliszyk & Urban (2015a) on MML version 4.181.1147. The set of 57,882 Mizar theorems translated to first-order logic has been chronologically ordered together with about 90,000 other formulas (mostly to express Mizar\u2019s type system). The first-order problems used to guide E prover in this paper are all distinct proofs found in Kaliszyk & Urban (2015a). This includes both proofs based on the human dependencies that the ATPs can redo and proofs found by running the ATPs on the predicted dependencies.", "startOffset": 81, "endOffset": 868}, {"referenceID": 30, "context": "Here we describe the data collection, network architectures, and training methodology of our deep learning models to be incorporated into the proof search procedure of the automated theorem prover E by Schulz (2013). We have reused several architectural and training choices that proved successful in the earlier related premise selection work by Alemi et al.", "startOffset": 202, "endOffset": 216}, {"referenceID": 2, "context": "We have reused several architectural and training choices that proved successful in the earlier related premise selection work by Alemi et al. (2016). Our models have two inputs: a negated conjecture to be proved and an unprocessed clause, both in CNF.", "startOffset": 130, "endOffset": 150}, {"referenceID": 2, "context": "Unlike Alemi et al. (2016), we avoid character-level embeddings since proof search can involve very large clauses with large computational cost.", "startOffset": 7, "endOffset": 27}, {"referenceID": 2, "context": "Following the premise selection models of Alemi et al. (2016), our convolutional models (\u201cCNNs\u201d for \u201cconvolutional neural networks\u201d) have relatively shallow depth as they give good results on that related task.", "startOffset": 42, "endOffset": 62}, {"referenceID": 15, "context": "(2016), and residual connections for both layers and blocks (He et al., 2015).", "startOffset": 60, "endOffset": 77}, {"referenceID": 27, "context": "WaveNet (van den Oord et al., 2016) is a special type of hierarchical convolutional network that employs dilated convolutions and residual connections. The dilated convolutions allow long range feature dependencies with only moderate overhead, as higher layers have geometrically increasing dilation factors (see Figure 2). While van den Oord et al. (2016) use causal dilated convolutions, we use symmetric convolutions as our task is discrimination, not generation.", "startOffset": 17, "endOffset": 357}, {"referenceID": 27, "context": "WaveNet (van den Oord et al., 2016) is a special type of hierarchical convolutional network that employs dilated convolutions and residual connections. The dilated convolutions allow long range feature dependencies with only moderate overhead, as higher layers have geometrically increasing dilation factors (see Figure 2). While van den Oord et al. (2016) use causal dilated convolutions, we use symmetric convolutions as our task is discrimination, not generation. Our WaveNet embedding network consists of 3 WaveNet blocks, where each block consists of 7 convolutional layers dilated by d = 1, 2, 4, . . . , 64. We use the gated activation tanh(x)\u03c3(x\u2032) of van den Oord et al. (2016), and residual connections for both layers and blocks (He et al.", "startOffset": 17, "endOffset": 686}, {"referenceID": 34, "context": "Our third model class is recursive neural networks (Socher et al., 2011) that are constructed by the parse tree of the first-order logic formulas.", "startOffset": 51, "endOffset": 72}, {"referenceID": 38, "context": "Each of these internal nodes are represented by either a fully connected layer with ReLU activation or tree LSTM network (Tai et al., 2015).", "startOffset": 121, "endOffset": 139}, {"referenceID": 6, "context": "Figures courtesy of Chris Olah (2015).", "startOffset": 26, "endOffset": 38}, {"referenceID": 38, "context": "Our tree LSTMs use separate forget gains per input and include off-diagonal forget gate terms (see Figure 3 and Tai et al. (2015) for details).", "startOffset": 112, "endOffset": 130}, {"referenceID": 39, "context": "2, we run E prover over the same holdout set of 9,159 FOL proof tasks from Kaliszyk & Urban (2015a) using the trained networks to help guide the clause selection.", "startOffset": 86, "endOffset": 100}, {"referenceID": 39, "context": "2, we run E prover over the same holdout set of 9,159 FOL proof tasks from Kaliszyk & Urban (2015a) using the trained networks to help guide the clause selection. These theorems all have at least one ATP proof using classical search heuristics, and 96.6% of these theorems can be proved using one of E prover\u2019s built-in automated heuristics, so there is little room for improvement on this dataset. However, it allows us to perform a sanity check that the neural nets aren\u2019t doing more harm than good when added to the auto heuristic. The final test is the most interesting but computationally expensive: do these deep network guided selection heuristics allow us to prove Mizar statements that do not yet have any ATP proof? For this, we use a corpus of 25,361 theorems from Mizar for which no proof was found in Kaliszyk & Urban (2015a). Because this is the most computationally expensive task, we only run it using the networks that performed best on the previous tasks.", "startOffset": 86, "endOffset": 839}, {"referenceID": 32, "context": "1pre014 (Schulz, 2013), with some modifications to enable integration with the trained deep neural networks as clause selection heuristics.", "startOffset": 8, "endOffset": 22}, {"referenceID": 32, "context": "Although all the hard statements have human proofs, this subset could neither be proved by theorem prover E (Schulz, 2013) nor by Vampire (Kov\u00e1cs & Voronkov, 2013) with a timeout limit of 15 minutes and default settings (auto and casc heuristics respectively) using the premises derived from the human given proofs.", "startOffset": 108, "endOffset": 122}, {"referenceID": 37, "context": "Here we describe our results on a corpus of 25,361 theorems from the Mizar corpus for which no proof was found by any of the provers, strategies, and premise selection methods considered by Kaliszyk & Urban (2015a). We will call these \u201chard statements\u201d in the rest of the paper.", "startOffset": 201, "endOffset": 215}, {"referenceID": 2, "context": "For premise selection, we use the character level model from the DeepMath paper (Alemi et al., 2016).", "startOffset": 80, "endOffset": 100}, {"referenceID": 2, "context": "Here we tried two different premise selection models \u201cDeepMath 1\u201d and \u201cDeepMath 2\u201d using the same character level convolutional architecture as the best character level model described in (Alemi et al., 2016).", "startOffset": 188, "endOffset": 208}], "year": 2017, "abstractText": "Deep learning techniques lie at the heart of several significant AI advances in recent years including object recognition and detection, image captioning, machine translation, speech recognition and synthesis, and playing the game of Go. Automated first-order theorem provers can aid in the formalization and verification of mathematical theorems and play a crucial role in program analysis, theory reasoning, security, interpolation, and system verification. Here we suggest deep learning based guidance in the proof search of the theorem prover E. We train and compare several deep neural network models on the traces of existing ATP proofs of Mizar statements and use them to select processed clauses during proof search. We give experimental evidence that with a hybrid, two-phase approach, deep learning based guidance can significantly reduce the average number of proof search steps while increasing the number of theorems proved. Using a few proof guidance strategies that leverage deep neural networks, we have found first-order proofs of 7.36% of the first-order logic translations of the Mizar Mathematical Library theorems that did not previously have ATP generated proofs. This increases the ratio of statements in the corpus with ATP generated proofs from 56% to 59%.", "creator": "easychair.cls-3.4"}}}