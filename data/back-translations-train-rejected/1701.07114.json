{"id": "1701.07114", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jan-2017", "title": "On the Effectiveness of Discretizing Quantitative Attributes in Linear Classifiers", "abstract": "Learning algorithms that learn linear models often have high representation bias on real-world problems. In this paper, we show that this representation bias can be greatly reduced by discretization. Discretization is a common procedure in machine learning that is used to convert a quantitative attribute into a qualitative one. It is often motivated by the limitation of some learners to qualitative data. Discretization loses information, as fewer distinctions between instances are possible using discretized data relative to undiscretized data. In consequence, where discretization is not essential, it might appear desirable to avoid it. However, it has been shown that discretization often substantially reduces the error of the linear generative Bayesian classifier naive Bayes. This motivates a systematic study of the effectiveness of discretizing quantitative attributes for other linear classifiers. In this work, we study the effect of discretization on the performance of linear classifiers optimizing three distinct discriminative objective functions --- logistic regression (optimizing negative log-likelihood), support vector classifiers (optimizing hinge loss) and a zero-hidden layer artificial neural network (optimizing mean-square-error). We show that discretization can greatly increase the accuracy of these linear discriminative learners by reducing their representation bias, especially on big datasets. We substantiate our claims with an empirical study on $42$ benchmark datasets.", "histories": [["v1", "Tue, 24 Jan 2017 23:57:32 GMT  (8075kb,D)", "http://arxiv.org/abs/1701.07114v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nayyar a zaidi", "yang du", "geoffrey i webb"], "accepted": false, "id": "1701.07114"}, "pdf": {"name": "1701.07114.pdf", "metadata": {"source": "CRF", "title": "On the Effectiveness of Discretizing Quantitative Attributes in Linear Classifiers", "authors": ["Nayyar A. Zaidi", "Yang Du", "Geoffrey I. Webb"], "emails": ["nayyar.zaidi@monash.edu", "ydu32@student.monash.edu", "geoff.webb@monash.edu"], "sections": [{"heading": null, "text": "In this paper, we show that this representational bias can be greatly reduced through discretization. Discretization is a common mechanism in machine learning that is used to transform a quantitative attribute into a qualitative one. It is often motivated by the limitation of some learners to qualitative data. Discretization loses information because fewer differences between instances are possible by using discretized data in relation to non-discretized data. Thus, where discretization is not essential, it may seem desirable to avoid it. However, discretization has been shown to significantly reduce the error of the linear Bajian generative classifier naive Bayes. This motivates a systematic investigation of the effectiveness of discretization of quantitative attributes for other linear classifiers. In this paper, we investigate the effect of discretization on the performance of linear classifiers that optimize three different discrimination functions - a strongly empirical classification layer supported by the loss of empirical classifiers."}, {"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules they have established over the past few years, and they will be able to play by the rules, \"he said.\" But it's not as if they have been able to play by the rules, \"he said.\" But it's not as if they have been able to play by the rules. \""}, {"heading": "2. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Terminology", "text": "In machine learning and data mining research, there are differences in terminology when it comes to characterizing the nature of an attribute (or trait), such as \"continuous versus discrete,\" \"numerical versus categorical,\" and \"quantitative versus qualitative.\" We believe that the distinction \"quantitative versus qualitative\" is best suited for our study in this paper and is therefore used everywhere on paper.Qualitative attributes are attributes to which arithmetic operations cannot be appliced.The values of a qualitative attribute can be divided or categorized into different categories. Sometimes, there is a significant ranking among these categories that leads to a distinction between ordinary and nominal among quantitative attributes. For example, student categories: {HD, D, C, P, F} and Pool Depth: {Very Deep, Shallow, Shallow} are discretionary attributes that can be determined by Americans married {during the marriage}."}, {"heading": "2.2 Discretization", "text": "Discretization is a common process in machine learning that is used to transform a quantitative problem into a qualitative attribute (Liu et al., 2002; Garcia et al., 2013).The necessity of discretization is based on the facts that some classifiers can only handle, and some others sometimes to better operate with qualitative attributes.The process involves finding intersections within the range of the quantitative attribute and group values in intervals based on these intersections, eliminating the ability to distinguish between data points that fall within the same interval. Therefore, disccretization methods entail information losses. Discretization methods can be categorized into two categories: Supervised and Unsupervised case, class information is not used during the cut-point determination process. Popular approaches are equally common and equally wide discreditization of WWW values (the WW-W-W-W-W-W-W-W-W-W-W-W-W-W)."}, {"heading": "2.3 Linear Classifier - CLL", "text": "A Logistic Regression classifier optimizes the conditional log probability (CLL = \u03b2 = \u03b2 \u03b2), which is defined as: CLL (\u03b2) = N \u2211 l = 1 log P (l) | x (l)))), (1) whereP (y (l) | x (l) = exp (\u03b2y, 0 + \u03b2T y x (l)) \u2211 C = 1 exp (\u03b2y, 0 + \u03b2 T x (l))))). (2) The term \u03b2y, 0 + \u03b2 T x (l) is extended as: \u03b2y, 0x (l) 0 + \u03b2y, 1x (l) 1 + \u03b2y, 2x (l) 2 + \u03b2y, nx (l) n, where x0 for all attributes x (l) can be assumed to be 1 for all data points. Since the objective function as defined in Equation 1 is linear in x, it is a linear classification. Equation 2 leads to a multi-class function max."}, {"heading": "2.4 Linear Classifier \u2013 Hinge Loss", "text": "A classifier that optimizes either a hinged loss objective function or its variant is a linear classifier and is known as a Support Vector Classifier (SVC). In this context, we define L2-Loss HL as: HL (\u03b2) = N \u2211 l = 1 max (0, 1 \u2212 y\u03b2Tx) 2. (7) An alternative is L1-Loss HL, which is the same: \u2211 Nl = 1 max (0, 1 \u2212 y\u03b2Tx). In this thesis, we focus only on L2 loss. In practice, a penalty date is added for regulating the objective function as: HL (\u03b2) = 12 | | \u03b2T\u03b2 | 2 + \u03bb N \u0445 l = 1 (max (0, 1 \u2212 y\u03b2Tx) 2, (8), where \u03bb is the regulating parameter. We will discuss the gradient and Hessian of this function later in section 2.6."}, {"heading": "2.5 Linear Classifier \u2013 Mean-Square-Error", "text": "Another linear classifier is based on the optimization of the objective function of the Mean Square Error (MSE) and is defined as: MSE (\u03b2) = 12 N \u2211 l = 1 C \u2211 c = 1 (P (c | x) \u2212 P (c | x))) 2, (9) where P (c | x) is specified in Equation 2 and P (c | x) is the actual probability of the data instance x for class c. This will be a vector of size C with all zeros except at the position of the label of x where it will be 1 (provided there are no duplicate data points in the dataset). The objective function of Equation 9 is similar to that optimized by artificial neural networks (ANN). However, in ANN P (c | x) is defined in relation to multiple layers. We can interpret Equation 9 as the objective function of a zero layer ANN."}, {"heading": "2.6 Optimization", "text": "There is no closed form for optimizing the negative logarithm probability, especially high data. There is no closed form for optimizing the negative logarithm probability, but above all there is no closed form for optimizing the objective function, and therefore one has to resort to iterative minimization methods, such as gradient descent or quasi-Newton. An iterative optimization procedure generates a sequence {\u03b2k} n k = 1 convergence to an optimal solution. The following equation plays the central role as it holds the key to obtaining a system of linear equations: \"2f\" sk \"sk (\u03b2k) sk\" sk \"sk\" sk, \"where f\" s is the objective function that we optimize. There are two very important questions that need to be addressed when it comes to search direction vectors that use equation (Nocedal and Wright, 2006)."}, {"heading": "3. Related Work", "text": "Statistics and many of its related and applied industries (such as epidemiology, medical research, and consumer marketing) can produce different results, which are examined in many studies (Irwin and McClelland, 2003; MacCallum et al., 2002; Greenland, 1995). However, in most of these studies a majority opinion is against the use of dichotomisation - and for categorisation is recommended to be used with caution. The main reason for this is that dichotomisation and categorisation lead to loss of information, as variability among the members of the group is subsumed. For example, Altman and Royston (2006) write that a lot of information is lost, so that statistical performance between patient outcomes is reduced."}, {"heading": "4. Experiments", "text": "In this section, we will compare the performance of linear classifiers with discredited linear classifiers and denote them by Cd.The details of these classifiers are not used. We will compare the performance of linear classifiers with discredited linear classifiers on various data sets from the UCI repository (Lichman, 2017).We will designate linear classifiers that optimize conditional log likelihood as LR, while their discrete counterparts are referred to as LR (d), SVC (d), and ANN0 (d).We will collectively point to LR, SVC, and ANN0 as linear classifiers and denote them by LC. We will collectively point to LR (d), SVC (d), and ANN0 (d) as discredited linear classifiers and denoted classifiers."}, {"heading": "4.2 Comparison of the Bias and Variance", "text": "In fact, most of us are able to go in search of a solution."}, {"heading": "5. Conclusion and Future Works", "text": "This year, it has never been as good as it has been this year."}, {"heading": "6. Code", "text": "Details of the fastLC software library can be found in Appendix A. The library can be downloaded along with a user manual from Github: https: / / github.com / nayyarzaidi / fastLC.git."}, {"heading": "7. Acknowledgments", "text": "This research was conducted by the Australian Research Council (ARC) under the auspices of DP140100087 and the Asian Aerospace Office, the Office of Scientific Research of the Air Force under contract FA2386-15-1-4007.Appendix A. fastLC - Linear Classifiers LibraryThe library can handle both quantitative and qualitative attributes. There is no need to do a one-hot encoding for qualitative attributes, as the LR model can actually handle the data types. You can execute the code in the library by issuing the following command for LR: > java -cp / fastLC.ar fast.BVDcrossvalx -t / datas.arff"}, {"heading": "Appendix B. Details of Datasets", "text": "Domain Case NomAtt NumAtt Class MissVal DescriptionHIGGS 11000000 0 28 2 N This dataset is generated using Monte Carlo simulation, which involves the deposition of particle-producing collisions from a background source, including 21 kinematic properties and 7 high-level attributes. Continued on next page"}], "references": [{"title": "Dangers of using \u201doptimal\u201d cutpoints in the evaluation of prognostic factors", "author": ["D. Altman", "B. Lausen", "W. Sauerbrei", "M. Schumacher"], "venue": "Journal of the National Cancer Institute,", "citeRegEx": "Altman et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Altman et al\\.", "year": 1994}, {"title": "The cost of dichotomising continuous variables", "author": ["D.G. Altman", "P. Royston"], "venue": "BMJ, 332,", "citeRegEx": "Altman and Royston.,? \\Q2006\\E", "shortCiteRegEx": "Altman and Royston.", "year": 2006}, {"title": "The need for low bias algorithms in classification learning from large data sets", "author": ["Damien Brain", "Geoffrey I. Webb"], "venue": "In PKDD,", "citeRegEx": "Brain and Webb.,? \\Q2002\\E", "shortCiteRegEx": "Brain and Webb.", "year": 2002}, {"title": "URL https://www.csie.ntu.edu.tw/~cjlin/ liblinear", "author": ["L. Chih-Jen"], "venue": "Liblinear library,", "citeRegEx": "Chih.Jen.,? \\Q2010\\E", "shortCiteRegEx": "Chih.Jen.", "year": 2010}, {"title": "Supervised and unsupervised discretization of continuous features", "author": ["J. Dougherty", "R. Kohavi", "M. Sahami"], "venue": "In ICML,", "citeRegEx": "Dougherty et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dougherty et al\\.", "year": 1995}, {"title": "On the handling of continuous-valued attributes in decision tree generation", "author": ["U.M. Fayyad", "K.B. Irani"], "venue": "Machine Learning,", "citeRegEx": "Fayyad and Irani.,? \\Q1992\\E", "shortCiteRegEx": "Fayyad and Irani.", "year": 1992}, {"title": "Differential misclassification arising from nondifferential errors in exposure", "author": ["C. Flegal", "P. Keyl", "P. Nieto"], "venue": "measurement. American Journal of Epidemiology,", "citeRegEx": "Flegal et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Flegal et al\\.", "year": 1991}, {"title": "Methodological issues in case-control studies. III: The effect of joint misclassification of risk factors and confounding factors upon estimation and power", "author": ["K.Y. Fung", "G.R. Howe"], "venue": "International Journal of Epidemiology,", "citeRegEx": "Fung and Howe.,? \\Q1984\\E", "shortCiteRegEx": "Fung and Howe.", "year": 1984}, {"title": "A survey of discretization techniques: Taxonomy and empirical analysis in supervised learning", "author": ["S. Garcia", "J. Luengo", "J. Saez", "V. Lopez", "F. Herrera"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Garcia et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Garcia et al\\.", "year": 2013}, {"title": "Dose-response and trend analysis in epidemiology: Alternatives to categorical analysis", "author": ["S. Greenland"], "venue": "Epidemiology, 6:356\u2013365,", "citeRegEx": "Greenland.,? \\Q1995\\E", "shortCiteRegEx": "Greenland.", "year": 1995}, {"title": "Why discretization works for naive bayesian classifiers", "author": ["C.N. Hsu", "H. J Huang", "T.T. Wong"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Hsu et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2000}, {"title": "Implications of the dirichlet assumption for discretization of continuous variables in naive bayesian classifiers", "author": ["C.N. Hsu", "H. J Huang", "T.T. Wong"], "venue": "Machine Learning,", "citeRegEx": "Hsu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2003}, {"title": "Negative consequences of dichotomizing continuous predictor variables", "author": ["J.R. Irwin", "G.H. McClelland"], "venue": "Journal of Marketing Research,", "citeRegEx": "Irwin and McClelland.,? \\Q2003\\E", "shortCiteRegEx": "Irwin and McClelland.", "year": 2003}, {"title": "Error-based and entropy-based discretization of continuous features", "author": ["R. Kohavi", "M. Sahami"], "venue": "In AAAI,", "citeRegEx": "Kohavi and Sahami.,? \\Q1996\\E", "shortCiteRegEx": "Kohavi and Sahami.", "year": 1996}, {"title": "Bias plus variance decomposition for zero-one loss functions", "author": ["R. Kohavi", "D. Wolpert"], "venue": "In ICML,", "citeRegEx": "Kohavi and Wolpert.,? \\Q1996\\E", "shortCiteRegEx": "Kohavi and Wolpert.", "year": 1996}, {"title": "UCI machine learning repository, 2017", "author": ["M. Lichman"], "venue": "URL http://archive.ics.uci.edu/ ml", "citeRegEx": "Lichman.,? \\Q2017\\E", "shortCiteRegEx": "Lichman.", "year": 2017}, {"title": "Discretization: An enabling technique", "author": ["H. Liu", "F. Hussain", "C.L. Tan", "M. Dash"], "venue": "Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2002}, {"title": "Improving classification performance with discretization on biomedical datasets", "author": ["J.L. Lustgarten", "V. Gopalakrishnan", "H. Grover", "S. Visweswaran"], "venue": "In AMIA Symposium Proceedings,", "citeRegEx": "Lustgarten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lustgarten et al\\.", "year": 2008}, {"title": "On the practice of dichotomization of quantitative variables", "author": ["R.C. MacCallum", "S. Zhang", "K.J. Preacher", "D.D. Rucker"], "venue": "Psychological Methods,", "citeRegEx": "MacCallum et al\\.,? \\Q2002\\E", "shortCiteRegEx": "MacCallum et al\\.", "year": 2002}, {"title": "An experimental investigation of the effect of discrete attributes on the precision of classification methods", "author": ["R.E. Maleki", "S.M. Iranmanesh", "B.M. Bidgoli"], "venue": "In Information and Communication Technologies,", "citeRegEx": "Maleki et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Maleki et al\\.", "year": 2009}, {"title": "The need for biases in learning generalizations", "author": ["T.M. Mitchell"], "venue": "Technical Report CBMTR-117,", "citeRegEx": "Mitchell.,? \\Q1980\\E", "shortCiteRegEx": "Mitchell.", "year": 1980}, {"title": "Machine learning: a probabilistic perspective", "author": ["K Murphy"], "venue": null, "citeRegEx": "Murphy.,? \\Q2012\\E", "shortCiteRegEx": "Murphy.", "year": 2012}, {"title": "A survey of truncated newton methods", "author": ["S.G. Nash"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "Nash.,? \\Q2000\\E", "shortCiteRegEx": "Nash.", "year": 2000}, {"title": "Effect of exposure misclassification on regression analysis", "author": ["S. Reade-Christopher", "L. Kupper"], "venue": "Biometrics, 47:535\u2013548,", "citeRegEx": "Reade.Christopher and Kupper.,? \\Q1991\\E", "shortCiteRegEx": "Reade.Christopher and Kupper.", "year": 1991}, {"title": "A note on dichotomization of continuous response variable in the presence of contamination and model specification", "author": ["Y. Shentu", "M. Xie"], "venue": "Statistics in Medicine,", "citeRegEx": "Shentu and Xie.,? \\Q2010\\E", "shortCiteRegEx": "Shentu and Xie.", "year": 2010}, {"title": "A bias-variance analysis of a real world learning problem: The coil challenge", "author": ["P. van der Putten", "M. van Someren"], "venue": "Machine Learning,", "citeRegEx": "Putten and Someren.,? \\Q2000\\E", "shortCiteRegEx": "Putten and Someren.", "year": 2000}, {"title": "Multiboosting: A technique for combining boosting and wagging", "author": ["G.I. Webb"], "venue": "Machine Learning,", "citeRegEx": "Webb.,? \\Q2000\\E", "shortCiteRegEx": "Webb.", "year": 2000}, {"title": "Discretization for naive-Bayes learning: managing discretization bias and variance", "author": ["Y. Yang", "G.I. Webb"], "venue": "Machine Learning,", "citeRegEx": "Yang and Webb.,? \\Q2009\\E", "shortCiteRegEx": "Yang and Webb.", "year": 2009}], "referenceMentions": [{"referenceID": 20, "context": "One of the many factors that affect the error of a learning system is its representation bias (van der Putten and van Someren, 2004), or, as it is also called, its hypothesis language bias (Mitchell, 1980).", "startOffset": 189, "endOffset": 205}, {"referenceID": 21, "context": "Learning algorithms that use linear models, such as logistic regression (LR) (Murphy, 2012) and support vector classifiers (SVC) (Chih-Jen, 2010), are very popular, possibly in part due to their lending themselves to convex optimization.", "startOffset": 77, "endOffset": 91}, {"referenceID": 3, "context": "Learning algorithms that use linear models, such as logistic regression (LR) (Murphy, 2012) and support vector classifiers (SVC) (Chih-Jen, 2010), are very popular, possibly in part due to their lending themselves to convex optimization.", "startOffset": 129, "endOffset": 145}, {"referenceID": 16, "context": "2 Discretization Discretization is a common process in machine learning that is used to convert a quantitative into a qualitative attribute (Liu et al., 2002; Garcia et al., 2013).", "startOffset": 140, "endOffset": 179}, {"referenceID": 8, "context": "2 Discretization Discretization is a common process in machine learning that is used to convert a quantitative into a qualitative attribute (Liu et al., 2002; Garcia et al., 2013).", "startOffset": 140, "endOffset": 179}, {"referenceID": 13, "context": "technique Entropy-Minimization Discretization (EMD) sorts the quantitative attribute\u2019s values and then finds the cut-point such the information gain is maximized across the splits (Kohavi and Sahami, 1996).", "startOffset": 180, "endOffset": 205}, {"referenceID": 22, "context": "This method is known as \u2018Truncated Newton method\u2019 (Nash, 2000).", "startOffset": 50, "endOffset": 62}, {"referenceID": 12, "context": "In Statistics and many of its related and applied branches (such as epidemiology, medical research and consumer marketing), it goes by names of \u2018dichotomization\u2019 and \u2018categorization\u2019 (where the two techniques differ as the former splits the measurement scale into two while the later can have more than two categories) \u2013 and has been examined in many studies (Irwin and McClelland, 2003; MacCallum et al., 2002; Greenland, 1995).", "startOffset": 359, "endOffset": 428}, {"referenceID": 18, "context": "In Statistics and many of its related and applied branches (such as epidemiology, medical research and consumer marketing), it goes by names of \u2018dichotomization\u2019 and \u2018categorization\u2019 (where the two techniques differ as the former splits the measurement scale into two while the later can have more than two categories) \u2013 and has been examined in many studies (Irwin and McClelland, 2003; MacCallum et al., 2002; Greenland, 1995).", "startOffset": 359, "endOffset": 428}, {"referenceID": 9, "context": "In Statistics and many of its related and applied branches (such as epidemiology, medical research and consumer marketing), it goes by names of \u2018dichotomization\u2019 and \u2018categorization\u2019 (where the two techniques differ as the former splits the measurement scale into two while the later can have more than two categories) \u2013 and has been examined in many studies (Irwin and McClelland, 2003; MacCallum et al., 2002; Greenland, 1995).", "startOffset": 359, "endOffset": 428}, {"referenceID": 27, "context": "An exception is for Bayesian classifiers, where it is common practice to discretize numeric attributes (Yang and Webb, 2009).", "startOffset": 103, "endOffset": 124}, {"referenceID": 1, "context": "For example, Altman and Royston (2006) write: .", "startOffset": 13, "endOffset": 39}, {"referenceID": 1, "context": "This may ease interpretation and presentation of results (Altman and Royston, 2006).", "startOffset": 57, "endOffset": 83}, {"referenceID": 7, "context": "\u2022 If there is error in the measurement scale, discretization can improve the performance of the model by reducing the contamination (Flegal and Keyl, 1991; ReadeChristopher and Kupper, 1991; Fung and Howe, 1984; Shentu and Xie, 2010).", "startOffset": 132, "endOffset": 233}, {"referenceID": 24, "context": "\u2022 If there is error in the measurement scale, discretization can improve the performance of the model by reducing the contamination (Flegal and Keyl, 1991; ReadeChristopher and Kupper, 1991; Fung and Howe, 1984; Shentu and Xie, 2010).", "startOffset": 132, "endOffset": 233}, {"referenceID": 27, "context": "By converting a quantitative attribute Xi into a qualitative one X\u2217 i , the probabilities will take the form of P(x \u2217 i | y) which may be reliably estimated from the data as there will be many xi values falling into the same interval (Yang and Webb, 2009).", "startOffset": 234, "endOffset": 255}, {"referenceID": 0, "context": "If those assumptions are correct, discretization will have a negative impact, but if those assumptions are false, discretization may lead to better results (Altman et al., 1994).", "startOffset": 156, "endOffset": 177}, {"referenceID": 10, "context": "The effectiveness of discretization for naive Bayes classifier is relatively well studied (Hsu et al., 2000; Dougherty et al., 1995; Yang and Webb, 2009).", "startOffset": 90, "endOffset": 153}, {"referenceID": 4, "context": "The effectiveness of discretization for naive Bayes classifier is relatively well studied (Hsu et al., 2000; Dougherty et al., 1995; Yang and Webb, 2009).", "startOffset": 90, "endOffset": 153}, {"referenceID": 27, "context": "The effectiveness of discretization for naive Bayes classifier is relatively well studied (Hsu et al., 2000; Dougherty et al., 1995; Yang and Webb, 2009).", "startOffset": 90, "endOffset": 153}, {"referenceID": 14, "context": "The effect of discretization on various classification algorithms such as naive Bayes, Support Vector Machines and Random Forest is discussed in Lustgarten et al. (2008). On many biomedical datasets, it is shown that discretization can greatly improve the performance of the learning algorithm.", "startOffset": 145, "endOffset": 170}, {"referenceID": 14, "context": "The effect of discretization on various classification algorithms such as naive Bayes, Support Vector Machines and Random Forest is discussed in Lustgarten et al. (2008). On many biomedical datasets, it is shown that discretization can greatly improve the performance of the learning algorithm. The role of discretization as feature selection technique is also explored. On various contrived datasets, Maleki et al. (2009) studied the effect of discretization on the precision and recall of various classification methods.", "startOffset": 145, "endOffset": 423}, {"referenceID": 4, "context": ", 2000; Dougherty et al., 1995; Yang and Webb, 2009). Dougherty et al. (1995) conducted an empirical study of naive Bayes with four well-known discretization methods and found that all the discretization methods result in significantly reducing error relative to a naive Bayes that assumes a Gaussian distribution for the continuous variables.", "startOffset": 8, "endOffset": 78}, {"referenceID": 15, "context": "In this section, we compare the performance of linear classifier with discretized linear classifier on various datasets from the UCI repository (Lichman, 2017).", "startOffset": 144, "endOffset": 159}, {"referenceID": 2, "context": "We expect low variance algorithms to have relatively low error for small data and low bias algorithms to have relatively low error for large data (Brain and Webb, 2002).", "startOffset": 146, "endOffset": 168}, {"referenceID": 4, "context": "Since, this is not a comparative study on the relative efficacies of various discretization techniques for linear classifiers, we only report results with supervised entropy-based discretization of Fayyad and Irani (1992), which we found gives better results than other discretization methods such as equal-frequency, equal-width, etc.", "startOffset": 198, "endOffset": 222}, {"referenceID": 2, "context": "We expect low variance algorithms to have relatively low error for small data and low bias algorithms to have relatively low error for large data (Brain and Webb, 2002). There are a number of different bias-variance decomposition definitions. In this research, we use the bias and variance definitions of Kohavi and Wolpert (1996) together with the repeated cross-validation bias-variance estimation method proposed by Webb (2000).", "startOffset": 147, "endOffset": 331}, {"referenceID": 2, "context": "We expect low variance algorithms to have relatively low error for small data and low bias algorithms to have relatively low error for large data (Brain and Webb, 2002). There are a number of different bias-variance decomposition definitions. In this research, we use the bias and variance definitions of Kohavi and Wolpert (1996) together with the repeated cross-validation bias-variance estimation method proposed by Webb (2000). We report Win-Draw-Loss (W-D-L) results when comparing the 0-1 Loss, RMSE, bias and variance of two models.", "startOffset": 147, "endOffset": 431}], "year": 2017, "abstractText": "Learning algorithms that learn linear models often have high representation bias on real-world problems. In this paper, we show that this representation bias can be greatly reduced by discretization. Discretization is a common procedure in machine learning that is used to convert a quantitative attribute into a qualitative one. It is often motivated by the limitation of some learners to qualitative data. Discretization loses information, as fewer distinctions between instances are possible using discretized data relative to undiscretized data. In consequence, where discretization is not essential, it might appear desirable to avoid it. However, it has been shown that discretization often substantially reduces the error of the linear generative Bayesian classifier naive Bayes. This motivates a systematic study of the effectiveness of discretizing quantitative attributes for other linear classifiers. In this work, we study the effect of discretization on the performance of linear classifiers optimizing three distinct discriminative objective functions \u2014 logistic regression (optimizing negative log-likelihood), support vector classifiers (optimizing hinge loss) and a zero-hidden layer artificial neural network (optimizing mean-square-error). We show that discretization can greatly increase the accuracy of these linear discriminative learners by reducing their representation bias, especially on big datasets. We substantiate our claims with an empirical study on 42 benchmark datasets.", "creator": "TeX"}}}