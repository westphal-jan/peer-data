{"id": "1701.07148", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2017", "title": "CP-decomposition with Tensor Power Method for Convolutional Neural Networks Compression", "abstract": "Convolutional Neural Networks (CNNs) has shown a great success in many areas including complex image classification tasks. However, they need a lot of memory and computational cost, which hinders them from running in relatively low-end smart devices such as smart phones. We propose a CNN compression method based on CP-decomposition and Tensor Power Method. We also propose an iterative fine tuning, with which we fine-tune the whole network after decomposing each layer, but before decomposing the next layer. Significant reduction in memory and computation cost is achieved compared to state-of-the-art previous work with no more accuracy loss.", "histories": [["v1", "Wed, 25 Jan 2017 02:58:06 GMT  (3185kb,D)", "http://arxiv.org/abs/1701.07148v1", "Accepted as a conference paper at BigComp 2017"]], "COMMENTS": "Accepted as a conference paper at BigComp 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marcella astrid", "seung-ik lee"], "accepted": false, "id": "1701.07148"}, "pdf": {"name": "1701.07148.pdf", "metadata": {"source": "CRF", "title": "CP-decomposition with Tensor Power Method for Convolutional Neural Networks Compression", "authors": ["Marcella Astrid", "Seung-Ik Lee"], "emails": ["marcella.astrid@ust.ac.kr", "silee@etri.re.kr"], "sections": [{"heading": null, "text": "I don't think we will be able, I don't think we will be able to find a solution, \"he told the German Press Agency.\" I don't think we will be able to bring about a solution. \"He added:\" I don't think we will be able to bring about a solution. \"He added:\" I don't think we will be able to bring about a solution. \"He added:\" I don't think we will be able to bring about a solution. \"He added:\" I don't think we will be able to bring about a solution. \""}, {"heading": "II. METHOD", "text": "Our approach consists of two main steps: decomposition and fine-tuning. We apply the two steps layer by layer until all layers of a CNN are decomposed and fine-tuned. All layers except the fully bonded layers are decomposed by CP decomposition, while the fully bonded layers are decomposed by SVD. After each decomposition, the entire network is refined by backpropagation. Before we start with the details, these are notations we use in this paper. Sensors are notated in upper case calligraphic letters, such as X. Matrices are notated in bold uppercase letters, such as X. Vectors are notated in bold lowercase letters, such as X. Scalars are notated in regular lower case letters, such as X. Regular uppercase letters, such as X, are used for dimensional size."}, {"heading": "A. Kernel Tensor Decomposition", "text": "The number of components is the tensor rank R. Each component is an outer product of n vectors, where n \u00b7 \u00b7 corresponds to the number of possibilities of the target tensor. Rank R determines the extent of the weight reduction, i.e. the smaller the R is, the lower the weights in a convolution layer. X \u00b7 R \u2032 r \u2032 br cr (1) ar Xiv: 170 1.07 148v 1 [cs.L G] 25 Jan 2017Convolution core tensor: Generally, the convolution layers in the CNNs card are a 3-way input tensor X of size S \u00b7 W \u00b7 H into a 3-way output tensor Y of size S \u00d7 W \u2032 H \u2032 with a 4-way type tensor K of size T \u00b7 S \u00b7 D \u00b7 D with different input channels, S \u00b7 W \u00b7 H into a 3-way output tensor Y of size."}, {"heading": "B. Complexity Analysis", "text": "The initial folding process in (2) requires TSD2 parameters and TSD2W \u2032 H \u2032 multiplication process. In CP decomposition, the compression ratio E and the acceleration ratio C are defined by: E = TSD2RS + RD2 + TR (13) C = TSD2W \u2032 H \u2032 RSWH + RD2W \u2032 H \u2032 + TRW \u2032 H \u2032 (14) The initial operations in FC of (8) are defined by MN parameters and require the same number of multiplication operations. Therefore, the compression ratio E and the acceleration ratio C are equal and are given by: E = C = MNMR + RN (15)."}, {"heading": "C. Rank Selection", "text": "Ranks play a key role in the decomposition of CP. If the rank is too high, the compression would not be maximized, and if it is too low, the accuracy would drop too much to be restored by fine-tuning. However, there is no even algorithm to find the optimal tensor rank [6]. In fact, determining the rank of a layer is NP-hard [3]. Therefore, we apply a primitive principle to determining the rank: the higher the loss of accuracy that a layer causes, the higher the rank that the layer needs. Rank ratio is the ratio of the rank of one layer to other layers. To find out how sensitive a layer is to decomposition, we perform a type of prior decomposition of each layer with a very low but constant rank (e.g. 5), and then fine-tune the entire network (an epoch)."}, {"heading": "D. Computation of Tensor Decomposition", "text": "In general, the decomposition of the tensor is an optimization problem, i.e. the minimization of the difference between the decomposed tensor and the target tensor. We apply the Tensor Power Method (TPM) [1]. TPM is known to explain the same variance with lower rank compared to ALS [1], because the tensor-1 tensors found in the early stages of the process explain most of the deviations in the target sort.W. TPM approaches a target sort.W by adding the tensor-1 iteratively. First, TPM finds a rank-1 tensor that is decomposed to approach W by minimizing the tensor-2 in coordinated lineage. The main idea in decomposition is that it uses the remaining tensor Wresidual = W \u2212 Wdeosed, so that the next iteration brings the residual tensor closer by minimizing | Wdual \u2212 W2."}, {"heading": "E. Fine-Tuning", "text": "As the accuracy after decomposition usually decreases due to the decomposition error, fine-tuning is required to restore the accuracy drop. However, as Lebedev et al. [8] pointed out, due to its instability, CP decomposition has not yet been successfully applied to the entire folding layers of a CNN with one-time fine-tuning [5], [8]. To overcome instability, we refine the entire network iteratively after dismantling each layer to prevent the errors from becoming too large to recover. In iterative fine-tuning, no layer is frozen because freezing some layers makes the approach greedy, which is usually stuck in local minima. As shown in the experiment [12], leaving the layers without freezing shows better results than freezing. In this way, all layers, including the already decomposed layer, can adapt to the newly decomposed layer."}, {"heading": "III. EXPERIMENTS", "text": "In this section, we will test our approach on AlexNet [7], one of the representative CNNs in Caffe [4]. Before introducing the most important experiments to all layers, we will briefly introduce AlexNet."}, {"heading": "A. AlexNet Overview", "text": "AlexNet is one of the most famous object recognition architectures and its pre-trained model is available online at the Caffe Model Zoo [4]. As a starting point, we evaluated the accuracy of the pre-trained model using 50,000 validation images from the ImageNet2012 [9] dataset for 1,000 class classifications, with a top-1 accuracy of 56.83% and a top-5 accuracy of 79.95%. AlexNet has a total of eight layers consisting of five folding layers and three fully interconnected layers."}, {"heading": "B. Whole Network Decomposition", "text": "In this section we will explain the results of decomposition of the entire network with CP-TPM and iterative fine tuning. As already mentioned, we apply the fine tuning iteratively to the best of our knowledge, so that the errors from each decomposition are not amplified, which is usually done with a shot at normal fine tuning, which is done after the entire decomposition of the folding coating.Fig.2 shows the steps of our method in AlexNet. Decomposition and fine tuning are performed for each coating.This process iterates from Conv1 to FC8 in succession. To decompose a layer, we must first find out its rank. It is desirable that a layer is as low as possible by assuming the same layer of accuracy, which means that each layer is proportional to its sensitivity, which is defined as the ratio of loss / total layers to layers."}, {"heading": "IV. CONCLUSION", "text": "We have shown that TPM-based low-rank CPdecomposition combined with iterative fine-tuning can cause complete network decomposition that has not yet been tested with CP. This approach surpasses the previous Tucker-based decomposition method used by Kim et al. [5] for total network decomposition. In particular, our method achieves a reduction in parameters of 6.98 and 3.53 in Alexnet, while the Tucker-based method has 5.46 and 2.67 respectively. There is still much to be explored. First, there may be many variations in iterative fine-tuning, such as freezing layers after fine-tuning and thawing in the final fine-tuning or starting fine-tuning from the last shift. Second, this work has focused more on volume layers than on fully connected layers where we have applied SVD."}, {"heading": "ACKNOWLEDGEMENT", "text": "This work was supported by the ICT R & D program of MSIP / IITP [B0101-15-0551, Technology Development of Virtual Creatures with Digital Emotional DNA of Users]."}], "references": [{"title": "Sparse higher-order principal components analysis", "author": ["G. Allen"], "venue": "AISTATS, pages 27\u201336,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems, pages 1269\u20131277,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Most tensor problems are np-hard", "author": ["C.J. Hillar", "L.-H. Lim"], "venue": "Journal of the ACM (JACM), 60(6):45,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y.-D. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "arXiv preprint arXiv:1511.06530,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM review, 51(3):455\u2013500,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Speeding-up convolutional neural networks using fine-tuned cpdecomposition", "author": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I. Oseledets", "V. Lempitsky"], "venue": "arXiv preprint arXiv:1412.6553,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei- Fei"], "venue": "International Journal of Computer Vision (IJCV), 115(3):211\u2013252,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "INTRODUCTION Convolutional neural networks (CNNs) have shown notable results in image recognition: VGG [10] and GoogleNet [11] achieved around 90% accuracy for top-5 classification in ImageNet2012 dataset; AlexNet [7] also achieved around 80% top-5 accuracy with the same dataset.", "startOffset": 103, "endOffset": 107}, {"referenceID": 10, "context": "INTRODUCTION Convolutional neural networks (CNNs) have shown notable results in image recognition: VGG [10] and GoogleNet [11] achieved around 90% accuracy for top-5 classification in ImageNet2012 dataset; AlexNet [7] also achieved around 80% top-5 accuracy with the same dataset.", "startOffset": 122, "endOffset": 126}, {"referenceID": 6, "context": "INTRODUCTION Convolutional neural networks (CNNs) have shown notable results in image recognition: VGG [10] and GoogleNet [11] achieved around 90% accuracy for top-5 classification in ImageNet2012 dataset; AlexNet [7] also achieved around 80% top-5 accuracy with the same dataset.", "startOffset": 214, "endOffset": 217}, {"referenceID": 4, "context": "In order to tackle this problem, several approaches recently have been proposed based on tensor decomposition, including Tucker decomposition [5] and Canonical Polyadic (CP) decomposition [2], [8].", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "In order to tackle this problem, several approaches recently have been proposed based on tensor decomposition, including Tucker decomposition [5] and Canonical Polyadic (CP) decomposition [2], [8].", "startOffset": 188, "endOffset": 191}, {"referenceID": 7, "context": "In order to tackle this problem, several approaches recently have been proposed based on tensor decomposition, including Tucker decomposition [5] and Canonical Polyadic (CP) decomposition [2], [8].", "startOffset": 193, "endOffset": 196}, {"referenceID": 4, "context": "[5] successfully decompose all the layers by using Tucker decomposition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Moreover, CPdecomposition [2], [8] has not been successful in compressing the whole convolution layers of a CNN because of the CP instability issue [8].", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "Moreover, CPdecomposition [2], [8] has not been successful in compressing the whole convolution layers of a CNN because of the CP instability issue [8].", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "Moreover, CPdecomposition [2], [8] has not been successful in compressing the whole convolution layers of a CNN because of the CP instability issue [8].", "startOffset": 148, "endOffset": 151}, {"referenceID": 7, "context": "We expect the followings with our method: \u2022 The whole convolution layer decomposition with CP: To the best of our knowledge, the whole convolution layer decomposition has not been successful because of CPdecomposition\u2019s instability [8].", "startOffset": 232, "endOffset": 235}, {"referenceID": 1, "context": "We expect that CP-TPM can decompose the whole convolution layers in contrast to previous CP-decomposition approaches [2], [8].", "startOffset": 117, "endOffset": 120}, {"referenceID": 7, "context": "We expect that CP-TPM can decompose the whole convolution layers in contrast to previous CP-decomposition approaches [2], [8].", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "However, there is no straight algorithm to find the optimal tensor rank [6].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "In fact, determining the rank is NP-hard [3].", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "We employ Tensor Power Method (TPM) [1].", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "TPM is known to explain the same variance with less rank compared to ALS [1] because the rank-1 tensors found in the early steps of the process explains most of the variances in the target tensor.", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "More details can be found in [1].", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "[8] pointed out, CP decomposition has not yet successfully applied to the whole convolution layers of a CNN with one-time finetuning because of its instability [5], [8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[8] pointed out, CP decomposition has not yet successfully applied to the whole convolution layers of a CNN with one-time finetuning because of its instability [5], [8].", "startOffset": 160, "endOffset": 163}, {"referenceID": 7, "context": "[8] pointed out, CP decomposition has not yet successfully applied to the whole convolution layers of a CNN with one-time finetuning because of its instability [5], [8].", "startOffset": 165, "endOffset": 168}, {"referenceID": 11, "context": "As experimented in [12], letting the layers unfrozen shows better results compared to freezing.", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "EXPERIMENTS In this section, we test our approach on AlexNet [7], one of the representative CNNs using Caffe framework [4].", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "EXPERIMENTS In this section, we test our approach on AlexNet [7], one of the representative CNNs using Caffe framework [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "AlexNet Overview AlexNet is one of famous object recognition architectures and its pre-trained model is available online in Caffe model zoo [4].", "startOffset": 140, "endOffset": 143}, {"referenceID": 8, "context": "As a baseline, we evaluated the accuracy of the pre-trained model using 50,000 validation images from the ImageNet2012 [9] dataset for 1,000 class classification.", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "0M 724M Tucker[5] VBMF 15 128 0.", "startOffset": 14, "endOffset": 17}], "year": 2017, "abstractText": "Convolutional Neural Networks (CNNs) has shown a great success in many areas including complex image classification tasks. However, they need a lot of memory and computational cost, which hinders them from running in relatively low-end smart devices such as smart phones. We propose a CNN compression method based on CP-decomposition and Tensor Power Method. We also propose an iterative fine tuning, with which we fine-tune the whole network after decomposing each layer, but before decomposing the next layer. Significant reduction in memory and computation cost is achieved compared to state-ofthe-art previous work with no more accuracy loss.", "creator": "LaTeX with hyperref package"}}}