{"id": "1701.07149", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2017", "title": "Hierarchical Recurrent Attention Network for Response Generation", "abstract": "We study multi-turn response generation in chatbots where a response is generated according to a conversation context. Existing work has modeled the hierarchy of the context, but does not pay enough attention to the fact that words and utterances in the context are differentially important. As a result, they may lose important information in context and generate irrelevant responses. We propose a hierarchical recurrent attention network (HRAN) to model both aspects in a unified framework. In HRAN, a hierarchical attention mechanism attends to important parts within and among utterances with word level attention and utterance level attention respectively. With the word level attention, hidden vectors of a word level encoder are synthesized as utterance vectors and fed to an utterance level encoder to construct hidden representations of the context. The hidden vectors of the context are then processed by the utterance level attention and formed as context vectors for decoding the response. Empirical studies on both automatic evaluation and human judgment show that HRAN can significantly outperform state-of-the-art models for multi-turn response generation.", "histories": [["v1", "Wed, 25 Jan 2017 03:04:31 GMT  (1052kb,D)", "http://arxiv.org/abs/1701.07149v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chen xing", "wei wu", "yu wu", "ming zhou", "yalou huang", "wei-ying ma"], "accepted": false, "id": "1701.07149"}, "pdf": {"name": "1701.07149.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Recurrent Attention Network for Response Generation", "authors": ["Chen Xing", "Wei Wu", "Yu Wu", "Ming Zhou", "Yalou Huang", "Wei-Ying Ma"], "emails": ["v-chxing@microsoft.com", "wuwei@microsoft.com", "v-wuyu@microsoft.com", "mingzhou@microsoft.com", "wyma@microsoft.com", "ylhuang@nankai.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people are able to survive themselves and themselves, and that they are able to survive themselves by blaming themselves and others. (...) In fact, it is the case that people are able to survive themselves. (...) It is not the case that they are able to survive themselves. (...) It is not the case that they are able to survive themselves. (...) It is the case that they are not able to survive themselves. (...) It is the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) (...) () (...) () (...) () (...) () () (...) () () (...) (...) () () (...) () () (...) () ()) (()) (...) (...) () (...) () () (...) () ()) (())) (...) (()) (...) () ()) (())) ((...) ()) (...) () (...) () ()) () ()) () ()) () ()) ()) () ())) (...) (...) () ()) () () ())) () ()) () ())) () ()) ()) () ())) () ()) ()) (... ())) () ())) () ())) () ()) (... ()) ())) () ()) (... () ())) () ()) () ())) (... () ())) ()) () () ())) (... () ()))) ()) (... (... ()) () ()) ())) ())) (... ())))) (... (... ()) ())))) ((... (... ())))))) (((())))))) (((...)))))"}, {"heading": "2 Related Work", "text": "Most existing response generation efforts relate to one-on-one conversations. Based on the ba-sian sequence to the sequence model (Sutskever et al., 2014), various models (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2015; Xing et al., 2016; Li et al., 2016;?) have been proposed as part of an encoder decoder framework to improve generational quality from different perspectives such as relevance, diversity, and personality. Recently, HRED has been proposed to model contexts within a hierarchical encoder decoder framework. Context information with a multi-layered perceptron (MLP) is encoded under the architecture of HRED."}, {"heading": "3 Problem Formalization", "text": "Suppose we have a record D = {(Ui, Yi)} Ni = 1. Hi, (Ui, Yi) consists of an answer Yi = (yi, 1,.., yi, Ti) and its context Ui = (ui, 1,.., ui, mi) with yi, j the j-th word, ui, mi the message and (ui, 1,..., ui, mi \u2212 1) the utterances in previous phrases. In this work we need mi > 2 and therefore have at least one utterance as a conversation story in each context."}, {"heading": "4 Hierarchical Recurrent Attention Network", "text": "We propose a hierarchical recurring attention network (HRAN) to model the probability of generating p (y1,.., yT | U). Figure 2 gives the architecture of HRAN. Roughly speaking, before generating a word, HRAN uses a word-level encoder to encode information of each utterance in context as hidden vectors. Subsequently, a hierarchical attention mechanism takes care of important parts within and between utterances with attention at the word or statement level. With the two attention levels, HRAN works in a bottom-up way: hidden vectors of utterances are processed by attention at the word level and uploaded to an encoder at the statement level to form hidden vectors of the context. Hidden vectors of the context are further processed by attention at the context level as a context vector and to the decoder uploaded to the text we describe in the following RAN."}, {"heading": "4.1 Word Level Encoder", "text": "Given U = (u1,.., um), we use a bidirectional recursive neural network with gated recurrent units (BiGRU) (Bahdanau et al., 2014) to encode each ui, i,.., m} as hidden vectors (hi, 1,.., hi, Ti). Formally, it is assumed that ui = (wi, 1,.., wi, Ti) is an operation that joins the two arguments together, \u2212 \u2192 h i, k is the k-th hidden state of a forward Gr (Vo et al., 2014), k = concat (\u2212 h, k), (1) where concat (\u00b7, \u00b7 \u00b7) is an operation defined as concatenating the two arguments, \u2212 h i, k is the k-th hidden state of a forward Gr (Vo et al., 2014), and \u2190 \u2212 h \u2212 sk is the k-th hidden state of a backward Gr."}, {"heading": "4.2 Hierarchical Attention and Utterance Encoder", "text": "(note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note. (note). (note). (note). (note. (note). (note). (note. (note). (note). (note). (note). (note). (note). (note. (note). (note). (note. (note). (note). (note). (note). (note). (note. (note). (note). (note). (note). (note). (note). (note. (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (note). (). (note). (note). (). (note). (). (note). (note). (note). (note). (). (). (note. (). (). (note). (). (note). (). (note). (). (). (note). (). (). (). (). (). (). (). ().). (). (). (). ().). (). (). (). ().). ()."}, {"heading": "4.3 Decoding the Response", "text": "The HRAN decoder is an RNN language model (Mikolov et al., 2010) based on the context vectors {ct} Tt = 1 according to Equation (4). Formally, the probability distribution p (y1,..., yT | U) is defined as p (y1,..., yT | U) = p (y1, st \u2212 1, ct) t = 2p (yt | ct, y1,..., yt \u2212 1). (8) where p (yt | ct, y1,..., yt \u2212 1) byst = f (eyt \u2212 1, st \u2212 1, ct) p (yt | ct, y1,..., yt \u2212 1) = Iyt \u00b7 softmax (st, eyt \u2212 1), (9) where st is the hidden state of the decoder at step t, eyt \u2212 1 is the embedding of yt \u2212 1."}, {"heading": "5 Experiments", "text": "We compared HRAN with state-of-the-art methods, both by automatic evaluation and by human judgment."}, {"heading": "5.1 Data Set", "text": "We have built a data set from Douban Group2, which is a popular Chinese social networking service (SNS) that allows users to discuss a wide range of topics in groups by posting and commenting. In Douban Group, two people can talk to each other about a post on a particular topic by posting one answer and the other quotes it and posts another comment. We scanned 20 million conversations between two people with an average number of phrases of 6.32. The data covers many different topics and can be considered a simulation of open domain conversations in a chatbot. In each conversation, we treated the last round as an answer, and the remaining phrases as context. As pre-processing, we first used Stanford-Chinese word segments3 to tokenize every utterance in the data. Then, we removed the conversations whose answer appears more than 50 times in the entire data to prevent them from dominating learning. We removed the conversations shorter than 3 phrases and the conversations longer than 50 words we use the previous one, and the conversations we are sure of 52,000 words after a 656-word response."}, {"heading": "5.2 Baselines", "text": "We compared HRAN with the following models: S2SA: We concatenated all utterances in a context as a long sequence and treated the sequence and the answer as a message response pair. In this way, we transformed the problem of multi-turn response generation into a problem of single-turn response generation and used the standard sequence of attention sequence (Shang et al., 2015) as baseline.HRED: the hierarchical encoder decoder model proposed by (Serban et al., 2016a). VHRED: a modification of HRED (Serban et al., 2016c) in which latent variables are introduced into the generation. In all models, we set the dimensionality of hidden states of encoders and decoders to 1000, and the dimensionality of word embedding to 620. All models were reduced to isotropic Gaussian distributions X (12X), VIX (VIX) and a delta algorithm (12X)."}, {"heading": "5.3 Evaluation Metrics", "text": "We followed the existing work and looked at the following metrics: Perplexity: Following (Vinyals and Le, 2015) we looked at perplexity as a benchmark. Perplexity is defined by equation (11). It measures how well a model predicts human reactions. The lower perplexity generally indicates better generation performance. In our experiments, perplexity was used in validation to determine when the training should end. If the perplexity stops and the difference is less than 2.0 five times in validation, we think that the algorithm has completed convergence and training."}, {"heading": "5.4 Evaluation Results", "text": "Table 1 shows the results of the perplexity test. HRAN achieves the least perplexity in both validation and test. We performed a t-test on test perplexity, and the result shows that the improvement of HRAN is statistically significant across all baseline models (p-value < 0.01).Table 2 shows the results of human annotations, and the ratios were calculated by combining the comments of the three judges. We see that HRAN outperforms all baseline models and all comparisons have relatively high kappa values, indicating that the annotators achieved relatively high agreement in evaluation. Compared with S2SA, HRED and VHRED, HRAN achieves win-loss gains (win-loss) of 6.7%, 6% and 4.8%, respectively. Sign test results show that the improvement is statistically significant (p-value < 0.01 for HRAN v.s. HRED) and of the ModRED &lt.5, respectively."}, {"heading": "5.5 Discussions", "text": "In this year, it has come to the point where one sees oneself in a position to conquer a new country, in which people are able to move into another world, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they live, in which"}, {"heading": "6 Conclusion", "text": "We propose a hierarchical network for recurring attention (HRAN) for generating multi-turn responses in chatbots. Empirical studies on large-scale conversation data show that HRAN can significantly outperform modern models."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Mind as machine: A history of cognitive science", "author": ["Margaret Ann Boden."], "venue": "Clarendon Press.", "citeRegEx": "Boden.,? 2006", "shortCiteRegEx": "Boden.", "year": 2006}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Syntax, Semantics and Structure in Statistical Translation, page 103.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio."], "venue": "Multimedia, IEEE Transactions on, 17(11):1875\u20131886.", "citeRegEx": "Cho et al\\.,? 2015", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability", "author": ["Joseph L Fleiss", "Jacob Cohen."], "venue": "Educational and psychological measurement.", "citeRegEx": "Fleiss and Cohen.,? 1973", "shortCiteRegEx": "Fleiss and Cohen.", "year": 1973}, {"title": "Filter, rank, and transfer the knowledge: Learning to chat", "author": ["Sina Jafarpour", "Christopher JC Burges", "Alan Ritter."], "venue": "Advances in Ranking, 10.", "citeRegEx": "Jafarpour et al\\.,? 2010", "shortCiteRegEx": "Jafarpour et al\\.", "year": 2010}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1510.03055.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1603.06155.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "arXiv preprint", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation", "author": ["Lili Mou", "Yiping Song", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin."], "venue": "arXiv preprint arXiv:1607.00970.", "citeRegEx": "Mou et al\\.,? 2016", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Hierarchical attention networks", "author": ["Paul Hongsuck Seo", "Zhe Lin", "Scott Cohen", "Xiaohui Shen", "Bohyung Han."], "venue": "arXiv preprint arXiv:1606.02393.", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1507.04808.", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence", "citeRegEx": "Serban et al\\.,? 2016a", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Multiresolution recurrent neural networks: An application to dialogue response generation", "author": ["Iulian Vlad Serban", "Tim Klinger", "Gerald Tesauro", "Kartik Talamadupula", "Bowen Zhou", "Yoshua Bengio", "Aaron Courville."], "venue": "arXiv preprint", "citeRegEx": "Serban et al\\.,? 2016b", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1605.06069.", "citeRegEx": "Serban et al\\.,? 2016c", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "arXiv preprint arXiv:1503.02364.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Word reordering and a dynamic programming beam search algorithm for statistical machine translation", "author": ["Christoph Tillmann", "Hermann Ney."], "venue": "Computational linguistics, 29(1):97\u2013133.", "citeRegEx": "Tillmann and Ney.,? 2003", "shortCiteRegEx": "Tillmann and Ney.", "year": 2003}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv preprint arXiv:1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "The anatomy of ALICE", "author": ["Richard S Wallace."], "venue": "Springer.", "citeRegEx": "Wallace.,? 2009", "shortCiteRegEx": "Wallace.", "year": 2009}, {"title": "Topic aware neural response generation", "author": ["Chen Xing", "Wei Wu", "Yu Wu", "Jie Liu", "Yalou Huang", "Ming Zhou", "Wei-Ying Ma."], "venue": "arXiv preprint arXiv:1606.08340.", "citeRegEx": "Xing et al\\.,? 2016", "shortCiteRegEx": "Xing et al\\.", "year": 2016}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "The hidden information state model: A practical framework for pomdp-based spoken dialogue management", "author": ["Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu."], "venue": "Computer Speech & Language, 24(2):150\u2013174.", "citeRegEx": "Mairesse et al\\.,? 2010", "shortCiteRegEx": "Mairesse et al\\.", "year": 2010}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "author": ["Stephanie Young", "Milica Gasic", "Blaise Thomson", "John D Williams."], "venue": "Proceedings of the IEEE, 101(5):1160\u20131179.", "citeRegEx": "Young et al\\.,? 2013", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 26, "context": "Conversational agents include task-oriented dialog systems which are built in vertical domains for specific tasks (Young et al., 2013; Boden, 2006; Wallace, 2009; Young et al., 2010), and non-task-oriented chatbots which aim to realize natural and human-like conversations with people", "startOffset": 114, "endOffset": 182}, {"referenceID": 1, "context": "Conversational agents include task-oriented dialog systems which are built in vertical domains for specific tasks (Young et al., 2013; Boden, 2006; Wallace, 2009; Young et al., 2010), and non-task-oriented chatbots which aim to realize natural and human-like conversations with people", "startOffset": 114, "endOffset": 182}, {"referenceID": 22, "context": "Conversational agents include task-oriented dialog systems which are built in vertical domains for specific tasks (Young et al., 2013; Boden, 2006; Wallace, 2009; Young et al., 2010), and non-task-oriented chatbots which aim to realize natural and human-like conversations with people", "startOffset": 114, "endOffset": 182}, {"referenceID": 5, "context": "regarding to a wide range of issues in open domains (Jafarpour et al., 2010).", "startOffset": 52, "endOffset": 76}, {"referenceID": 17, "context": "from large scale message-response pairs (Shang et al., 2015; Vinyals and Le, 2015).", "startOffset": 40, "endOffset": 82}, {"referenceID": 21, "context": "from large scale message-response pairs (Shang et al., 2015; Vinyals and Le, 2015).", "startOffset": 40, "endOffset": 82}, {"referenceID": 18, "context": "lem, researchers have taken conversation history into consideration and proposed response generation for multi-turn conversation (Sordoni et al., 2015; Serban et al., 2015; Serban et al., 2016b; Serban et al., 2016c).", "startOffset": 129, "endOffset": 216}, {"referenceID": 13, "context": "lem, researchers have taken conversation history into consideration and proposed response generation for multi-turn conversation (Sordoni et al., 2015; Serban et al., 2015; Serban et al., 2016b; Serban et al., 2016c).", "startOffset": 129, "endOffset": 216}, {"referenceID": 15, "context": "lem, researchers have taken conversation history into consideration and proposed response generation for multi-turn conversation (Sordoni et al., 2015; Serban et al., 2015; Serban et al., 2016b; Serban et al., 2016c).", "startOffset": 129, "endOffset": 216}, {"referenceID": 16, "context": "lem, researchers have taken conversation history into consideration and proposed response generation for multi-turn conversation (Sordoni et al., 2015; Serban et al., 2015; Serban et al., 2016b; Serban et al., 2016c).", "startOffset": 129, "endOffset": 216}, {"referenceID": 14, "context": "State-of-the-art methods such as HRED (Serban et al., 2016a) and VHRED (Serban et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 16, "context": ", 2016a) and VHRED (Serban et al., 2016c) focus on modeling the hierarchy of the context, whereas there is little exploration on how to select important parts from the context, although it is often a crucial step for generating a proper response.", "startOffset": 19, "endOffset": 41}, {"referenceID": 16, "context": "utterance importance, the state-of-the-art model VHRED (Serban et al., 2016c) misses important points and gives a response \u201care you a man or a woman\u201d which is OK if there were only u3 left, but nonsense given the whole context.", "startOffset": 55, "endOffset": 77}, {"referenceID": 17, "context": "Inspired by the success of the attention mechanism in single-turn response generation (Shang et al., 2015), we propose a hierarchical recurrent attention network (HRAN) for multi-turn response generation in which we introduce a hierarchical attention mechanism to dynamically highlight important parts of word sequences and the utterance sequence when generating a response.", "startOffset": 86, "endOffset": 106}, {"referenceID": 19, "context": "sic sequence to sequence model (Sutskever et al., 2014), various models (Shang et al.", "startOffset": 31, "endOffset": 55}, {"referenceID": 16, "context": "Under the architecture of HRED, more variants including VHRED (Serban et al., 2016c) and MrRNN (Serban et al.", "startOffset": 62, "endOffset": 84}, {"referenceID": 15, "context": ", 2016c) and MrRNN (Serban et al., 2016b) are proposed in order to introduce latent and explicit variables into", "startOffset": 19, "endOffset": 41}, {"referenceID": 6, "context": ", 2015; Vinyals and Le, 2015; Li et al., 2015; Xing et al., 2016; Li et al., 2016; ?) have been proposed under an encoder-decoder framework to improve generation quality from different perspectives such as relevance, diversity, and personality. Recently, multiturn response generation has drawn attention from academia. For example, Sordoni et al. (2015) proposed DCGM where context information is encoded with a multi-layer perceptron (MLP).", "startOffset": 30, "endOffset": 355}, {"referenceID": 6, "context": ", 2015; Vinyals and Le, 2015; Li et al., 2015; Xing et al., 2016; Li et al., 2016; ?) have been proposed under an encoder-decoder framework to improve generation quality from different perspectives such as relevance, diversity, and personality. Recently, multiturn response generation has drawn attention from academia. For example, Sordoni et al. (2015) proposed DCGM where context information is encoded with a multi-layer perceptron (MLP). Serban et al. (2016a) proposed HRED which models contexts in a hierarchical encoder-decoder framework.", "startOffset": 30, "endOffset": 465}, {"referenceID": 17, "context": ", 2015), and is quickly applied to single-turn response generation afterwards (Shang et al., 2015; Vinyals and Le, 2015).", "startOffset": 78, "endOffset": 120}, {"referenceID": 21, "context": ", 2015), and is quickly applied to single-turn response generation afterwards (Shang et al., 2015; Vinyals and Le, 2015).", "startOffset": 78, "endOffset": 120}, {"referenceID": 17, "context": ", 2015), and is quickly applied to single-turn response generation afterwards (Shang et al., 2015; Vinyals and Le, 2015). Recently, Yang et al. (2016) proposed a hierarchical attention network for document classification in which two levels of", "startOffset": 79, "endOffset": 151}, {"referenceID": 12, "context": "Seo et al. (2016) proposed a hierarchical attention network to precisely attending objects of different scales and shapes in images.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": ", um), we employ a bidirectional recurrent neural network with gated recurrent units (BiGRU) (Bahdanau et al., 2014) to encode each ui, i \u2208 {1, .", "startOffset": 93, "endOffset": 116}, {"referenceID": 2, "context": "catenating the two arguments together, \u2212 \u2192 h i,k is the k-th hidden state of a forward GRU (Cho et al., 2014), and \u2190\u2212 h i,k is the k-th hidden state of a backward GRU.", "startOffset": 91, "endOffset": 109}, {"referenceID": 9, "context": "The decoder of HRAN is a RNN language model (Mikolov et al., 2010) conditioned on the context", "startOffset": 44, "endOffset": 66}, {"referenceID": 20, "context": "In practice, we employ the beam search (Tillmann and Ney, 2003) technique to generate the n-best responses.", "startOffset": 39, "endOffset": 63}, {"referenceID": 17, "context": "By this means, we transformed the problem of multiturn response generation to a problem of singleturn response generation and employed the standard sequence to sequence with attention (Shang et al., 2015) as a baseline.", "startOffset": 184, "endOffset": 204}, {"referenceID": 14, "context": "HRED: the hierarchical encoder-decoder model proposed by (Serban et al., 2016a).", "startOffset": 57, "endOffset": 79}, {"referenceID": 16, "context": "VHRED: a modification of HRED (Serban et al., 2016c) where latent variables are introduced in to generation.", "startOffset": 30, "endOffset": 52}, {"referenceID": 21, "context": "Perplexity: following (Vinyals and Le, 2015), we employed perplexity as an evaluation metric.", "startOffset": 22, "endOffset": 44}, {"referenceID": 4, "context": "Agreements among the annotators were calculated using Fleiss\u2019 kappa (Fleiss and Cohen, 1973).", "startOffset": 68, "endOffset": 92}, {"referenceID": 11, "context": "Note that we do not choose BLEU (Papineni et al., 2002) as an evaluation metric, because (1) Liu et al.", "startOffset": 32, "endOffset": 55}, {"referenceID": 8, "context": "(Liu et al., 2016) have proven that BLEU is not a proper metric for evaluating conversation models as there is weak correlation between BLEU and human judgment; (2) different from the single-turn case, in multi-turn conversation, one context usually has one copy in the whole data.", "startOffset": 0, "endOffset": 18}, {"referenceID": 18, "context": "(Sordoni et al., 2015) did in their work, each context only has a single reference in test.", "startOffset": 0, "endOffset": 22}, {"referenceID": 16, "context": "VHRED is the best baseline model, which is consistent with the existing literatures (Serban et al., 2016c).", "startOffset": 84, "endOffset": 106}, {"referenceID": 23, "context": "(Xing et al., 2016) and Mou et al.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "We study multi-turn response generation in chatbots where a response is generated according to a conversation context. Existing work has modeled the hierarchy of the context, but does not pay enough attention to the fact that words and utterances in the context are differentially important. As a result, they may lose important information in context and generate irrelevant responses. We propose a hierarchical recurrent attention network (HRAN) to model both aspects in a unified framework. In HRAN, a hierarchical attention mechanism attends to important parts within and among utterances with word level attention and utterance level attention respectively. With the word level attention, hidden vectors of a word level encoder are synthesized as utterance vectors and fed to an utterance level encoder to construct hidden representations of the context. The hidden vectors of the context are then processed by the utterance level attention and formed as context vectors for decoding the response. Empirical studies on both automatic evaluation and human judgment show that HRAN can significantly outperform state-of-the-art models for multi-turn response generation.", "creator": "TeX"}}}