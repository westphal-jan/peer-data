{"id": "1701.07166", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2017", "title": "Personalized Classifier Ensemble Pruning Framework for Mobile Crowdsourcing", "abstract": "Ensemble learning has been widely employed by mobile applications, ranging from environmental sensing to activity recognitions. One of the fundamental issue in ensemble learning is the trade-off between classification accuracy and computational costs, which is the goal of ensemble pruning. During crowdsourcing, the centralized aggregator releases ensemble learning models to a large number of mobile participants for task evaluation or as the crowdsourcing learning results, while different participants may seek for different levels of the accuracy-cost trade-off. However, most of existing ensemble pruning approaches consider only one identical level of such trade-off. In this study, we present an efficient ensemble pruning framework for personalized accuracy-cost trade-offs via multi-objective optimization. Specifically, for the commonly used linear-combination style of the trade-off, we provide an objective-mixture optimization to further reduce the number of ensemble candidates. Experimental results show that our framework is highly efficient for personalized ensemble pruning, and achieves much better pruning performance with objective-mixture optimization when compared to state-of-art approaches.", "histories": [["v1", "Wed, 25 Jan 2017 05:22:35 GMT  (346kb)", "http://arxiv.org/abs/1701.07166v1", null]], "reviews": [], "SUBJECTS": "cs.DC cs.HC cs.LG", "authors": ["shaowei wang", "liusheng huang", "pengzhan wang", "hongli xu", "wei yang"], "accepted": false, "id": "1701.07166"}, "pdf": {"name": "1701.07166.pdf", "metadata": {"source": "CRF", "title": "Personalized Classifier Ensemble Pruning Framework for Mobile Crowdsourcing", "authors": ["Shaowei Wang", "Liusheng Huang", "Pengzhan Wang", "Hongli Xu", "Wei Yang"], "emails": ["pzwang}@mail.ustc.edu.cn,", "qubit}@ustc.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 170 1.07 166v 1 [cs.D C] 2Keywords: Ensemble Pruning, Mobile Crowdsourcing, Bi-objective Optimization, Pareto Optimization"}, {"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Preliminaries", "text": "In this section, we briefly present the biobjective Pareto optimization and the formal definition of the ensemble section."}, {"heading": "2.1 Bi-objective Pareto Optimization", "text": "Multi-goal optimization explicitly optimizes for two or more goals and outputs multiple solutions, where each solution has at least one advantage goal compared to other solutions. Conventionally, the Pareto rule is used to determine the advantage or dominance ratio between solutions, and a solution is Pareto-optimal if it is not dominated by other solutions. Specifically, the Pareto dominance relationship in bi-objective optimization is formulated as follows: Definition 1 (Pareto dominance). Let f = (f1, f2): \u0432 \u2192 R 2 be the objective vector. In two solutions S1, S2, S1, S1, S1, S1, S1 dominate S2 iff f1 (S1) \u2264 f1 (S2) and f2 (S1) \u2264 f2 (S1) and f1 (S1) < f1 (S2) < f2 (S1) < f2 (S2) < f2 (S2) < f2 (S2) Objective has an optimal, an optimal, as opposed to an optimized, normally optimized solution."}, {"heading": "2.2 Ensemble Pruning", "text": "Ensemble Crop selects a subset of classifiers S from a pool of basic classifiers T, so that the accuracy of the aggregated classification results is only slightly worsened or even improved, while the number of classifiers (or calculation costs) in S is significantly reduced compared to the total classification pool T. To be precise: Let the number of basic classifiers be in T = {t0,..., tm} m, and selected classifiers in S are rated as follows: Sopt = arg minS, in which each bit indicates whether each ti is selected. Ensemble Crop seeks a balance between the classification error rate E (S) on a validation dataset and the cost of evaluating classifiers in S as follows: Sopt = arg minS, {0.1} m E (S) + \u03b1 \u00b7 S |, (1) Where \u03b1 [0.0, + \u221e) the trade-off function is the cost of classifiers in S (the cost of the S and the actual classifiers in the S), we can also combine the cost of S and the loss in the classifiers in S (the S)."}, {"heading": "3 Model and Framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Personalized Ensemble Pruning Model", "text": "Remember that in crowdsourcing, the server usually has to publish cropped classifier ensembles to crowdsourcing participants. In practice, some crowdsourcing participants may strive for the most achievable classification accuracy in order to achieve a better quality of service, while others pay more attention to saving energy or time in the classification process. This implies that in mobile crowdsourcing scenarios an ensemble section with personalized accuracy and cost compromise is required. Formally, provided that there are no crowdsourcing participants, everyone has a choice between the accuracy and cost component level \u03b1j for ensemble circumcision, then the optimal cropped ensemble Sj, for participant uj, can be formulated as follows: Sj, opt = arg minS (S) + \u03b1j \u00b7 | S |. (2) Consequently, the aggregator should publish personalized optimal classifier ensembles Sj, opt for participant education as shown in Figure 1."}, {"heading": "3.2 Personalized Ensemble Pruning Framework", "text": "We offer our basic personalized ensemble trimming based on bi-objective optimization. Since determining the optimal ensemble for each participant with trade-off level \u03b1j it would be too expensive, especially if the number of participants is large, we resort to bi-objective Pareto optimization, which sets the empirical classification errors E (S) on validation data set as target f1 (S) and calculation costs. Algorithm 1 Personalized ensemble trimming f2 (S) in definition 1, and returns a Pareto-optimal ensemble set P = {p1, p2, pl} as solutions where l set the size of the Pareto-optimal solution. Algorithm 1 Personalized ensemble trimming Pruning FrameworkInput: A set of trained basic classifiers T = {t1, t2, tm}, Participant's trade-off levels A = B, 2,..., Personalized ensembles:..."}, {"heading": "4 Objective-Mixture Optimization", "text": "Remember that in our basic framework, the bi-objective f (S) = (E) = (S), | S | | solver delivers a series of Pareto-optimal ensembles P = {\u03b11, p2,..., pl}, each of which does not have an objective objective objective objective objective for each pi, pj, pi \u00b7 P, pi \u00b7. However, taking into account the concrete value of the target levels of participants A = {\u03b11, p2,..., pl}, we have the non-dominant relationship between pi and pj \u00b7 P, pi \u00b7 A, formally speaking: (E (pi), | pi |) 6 \u2265 (E (pj), pj | pj |); (E (pi) + pi) + pi \u00b7 pi \u00b7 () + pi \u00b7 () 6 \u2265 (E (pj) + \u00b2 objective objective objective objective objective objective objective objective, pj \u00b7 | A). (3) This implies that the pareto-optimal ensembles still have, if redundancy}, pl, pl, pl,..."}, {"heading": "5 Performance Analyses", "text": "In this section, we analyze the theoretical computational complexity and ensemble pruning performance of our personalized ensemble pruning frame. The computational complexity of our frame depends to a large extent on the two-objective solver used in line 2 in algorithm 1. Fortunately, the evolutionary two-objective solver in [13] shows that expected O (m2 logm) iterations are sufficient to approximate the optimal Pareto set, where m is the number of base classifiers. Personalized ensemble selection in line 3 \u2212 5 naively displays the computational complexity of n \u00b7 l, but in algorithm 2 its complexity is reduced to O (n log n + l log l), where l is the size of P and l = O (poly (m). To put it in a nutshell: The total expected computational complexity of our frame is O (poly (m) + n logn), while the naive set of unedited ensemblems (also) is highly personalized (prunbleed)."}, {"heading": "6 Experiments", "text": "In this area, we are in a position to seek a solution that meets the needs of the individual."}, {"heading": "7 Related Work", "text": "Ensemble learning is often used to classify data, in the form of bagging [1], boosting [6], or heterogeneous classifiers (e.g. combination of SVM, k-NN, and Naive Bayes classifiers), primarily to mitigate the inefficiency problem of ensemble learning, ensemble editing has been extensively studied. As ensemble editing itself is computationally hard, genetic algorithms [17] and semi-definite programming [16] have been proposed. Demir et al. [4] and Ulas et al. [15] also consider the cost of calculating ensembles or distinguishing features between classifiers to compose heuristically a circumcised ensemble. Some other ensemble cutting approaches, such as Margineantu et al. [10] and Mart-\u0131nez-Mun-Mun-et-al. [11], are based on ordered aggregation that rearranges the order of the base classifiers."}, {"heading": "8 Conclusion", "text": "In this paper, we address the problem of personalized ensemble pruning of classifiers in mobile crowdsourcing and propose a framework for the problem based on biobjective Pareto optimization with objective mix. In our framework, each participant (the ensemble user) was able to choose a tailor-made trade-off between the classification accuracy of the ensemble and its calculation costs, balancing the need for energy savings on mobile devices and the need for classification accuracy for each participant. Both theoretical and experimental results demonstrate the efficiency and effectiveness of our framework. In particular, the experimental results show that our personalized ensemble pruning system with objective mix reduces runtime by an average of 40% and combined losses by 50% at the same time with the same number of iterations."}], "references": [{"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine learning 24(2), 123\u2013140", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "Classification and regression trees", "author": ["L. Breiman", "J. Friedman", "C.J. Stone", "R.A. Olshen"], "venue": "CRC press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1984}, {"title": "On the use of ensemble of classifiers for accelerometer-based activity recognition", "author": ["C. Catal", "S. Tufekci", "E. Pirmit", "G. Kocabag"], "venue": "Applied Soft Computing 37, 1018\u2013 1022", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Cost-conscious classifier ensembles", "author": ["C. Demir", "E. Alpaydin"], "venue": "Pattern Recognition Letters 26(14), 2206\u20132214", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Poster: Accuracy vs", "author": ["L. Fan", "M. Xue", "S. Chen", "L. Xu", "H. Zhu"], "venue": "time cost: Detecting android malware through pareto ensemble pruning. In: Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. pp. 1748\u20131750. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of computer and system sciences 55(1), 119\u2013139", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Multiple comparator classifier framework for accelerometer-based fall detection and diagnostic", "author": ["R.M. Gibson", "A. Amira", "N. Ramzan", "P. Casaseca-de-la Higuera", "Z. Pervez"], "venue": "Applied Soft Computing 39, 94\u2013103", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "From dynamic classifier selection to dynamic ensemble selection", "author": ["A.H. Ko", "R. Sabourin", "A.S. Britto Jr"], "venue": "Pattern Recognition 41(5), 1718\u20131731", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Activity recognition using cell phone accelerometers", "author": ["J.R. Kwapisz", "G.M. Weiss", "S.A. Moore"], "venue": "ACM SigKDD Explorations Newsletter 12(2), 74\u201382", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Pruning adaptive boosting", "author": ["D.D. Margineantu", "T.G. Dietterich"], "venue": "ICML. vol. 97, pp. 211\u2013218. Citeseer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Pruning in ordered bagging ensembles", "author": ["G. Mart\u0301\u0131nez-Mu\u00f1oz", "A. Su\u00e1rez"], "venue": "ICML. ACM", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V Dubourg"], "venue": "The Journal of Machine Learning Research 12, 2825\u20132830", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Pareto ensemble pruning", "author": ["C. Qian", "Y. Yu", "Z.H. Zhou"], "venue": "AAAI. pp. 2935\u20132941", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "iprotect: Detecting physical assault using smartphone", "author": ["Z. Sun", "S. Tang", "H. Huang", "L. Huang", "Z. Zhu", "H. Guo", "Sun", "Y.e."], "venue": "Wireless Algorithms, Systems, and Applications, pp. 477\u2013486. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Incremental construction of classifier and discriminant ensembles", "author": ["A. Ula\u015f", "M. Semerci", "O.T. Y\u0131ld\u0131z", "E. Alpayd\u0131n"], "venue": "Information Sciences 179(9), 1298\u20131318", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Ensemble pruning via semi-definite programming", "author": ["Y. Zhang", "S. Burer", "W.N. Street"], "venue": "The Journal of Machine Learning Research 7, 1315\u20131338", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Ensembling neural networks: many could be better than all", "author": ["Z.H. Zhou", "J. Wu", "W. Tang"], "venue": "Artificial intelligence 137(1), 239\u2013263", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Feature engineering for semantic place prediction", "author": ["Y. Zhu", "E. Zhong", "Z. Lu", "Q. Yang"], "venue": "Pervasive and mobile computing 9(6), 772\u2013783", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 17, "context": ", place classification[18]) to people-centric scenarios (e.", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": ", activity recognition [3], human fall detection [7][14]).", "startOffset": 23, "endOffset": 26}, {"referenceID": 6, "context": ", activity recognition [3], human fall detection [7][14]).", "startOffset": 49, "endOffset": 52}, {"referenceID": 13, "context": ", activity recognition [3], human fall detection [7][14]).", "startOffset": 52, "endOffset": 56}, {"referenceID": 16, "context": "Hence, ensemble pruning [17] has been proposed to select a subset of all possible classifiers meanwhile avoid harming classification accuracy.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "[13] proposed an evolutionary algorithm for non-personalized ensemble pruning with guaranteed theoretical advantages over former approaches, this outstanding work on nonpersonalized ensemble pruning can naturally be embedded into our framework for personalized ensemble pruning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Fortunately, the evolutionary biobjective solver in [13] shows that expected O(m logm) iterations is sufficient for approximating optimal Pareto set, where m is the number of base classifiers.", "startOffset": 52, "endOffset": 56}, {"referenceID": 12, "context": "The evolutionary bi-objective solver in [13] has been proved to outperform former ensemble pruning approaches, thus the achievable performance of our framework is guaranteed by [13].", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "The evolutionary bi-objective solver in [13] has been proved to outperform former ensemble pruning approaches, thus the achievable performance of our framework is guaranteed by [13].", "startOffset": 177, "endOffset": 181}, {"referenceID": 12, "context": "In this section, we evaluate the computational overheads and pruning performance of our personalized ensemble pruning framework, with comparison to the state-of-art ensemble pruning approach PEP in [13], which is also used by our framework as the bi-objective solver at line 2 in Algorithm 1 (the differences between PEP and our framework is declared in the next section).", "startOffset": 198, "endOffset": 202}, {"referenceID": 8, "context": "In order to simulate ensemble pruning for crowdsourcing participants that equipped with mobile devices, the human activity recognition data set based on mobile phone accelerometers readings from [9] is used for our experiments.", "startOffset": 195, "endOffset": 198}, {"referenceID": 1, "context": "The pool of m = 20 base CART tree [2] classifiers are generated by Bagging [1] on the training set.", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "The pool of m = 20 base CART tree [2] classifiers are generated by Bagging [1] on the training set.", "startOffset": 75, "endOffset": 78}, {"referenceID": 11, "context": "We conduct experiments on the scikit-learn [12] platform.", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": "Ensemble learning is widely used for data classification, in the form of Bagging[1], Boosting[6] or heterogeneous classifiers (e.", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "Ensemble learning is widely used for data classification, in the form of Bagging[1], Boosting[6] or heterogeneous classifiers (e.", "startOffset": 93, "endOffset": 96}, {"referenceID": 16, "context": "As ensemble pruning itself is computational hard, genetic algorithms[17] and semi-definite programming[16] has been proposed.", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "As ensemble pruning itself is computational hard, genetic algorithms[17] and semi-definite programming[16] has been proposed.", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "[4] and Ula\u015f et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15] also consider costs of computations of ensemble or discriminants between classifiers to heuristically compose a pruned ensemble.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] and Mart\u0301\u0131nez-Mu\u00f1oz et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] are based on ordered aggregation, which rearrange the order of classifiers in base classifier pool.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] proposed an ensemble pruning approach using evolutionary bi-objective optimization, and proves its theoretical advantages over former heuristic or ordered based approaches.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The concurrent work [5] considers customized accuracy-cost trade-off on a group of Pareto optimal sets, whereas this work focus improving efficiency and effectiveness of personalized trade-off on one Pareto optimal set.", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "Another topic related to multiple ensembles selection is dynamic ensemble selection [8], which dynamically select a classifier ensemble for one class of input samples, instead of for personalized accuracy-cost trade-off levels.", "startOffset": 84, "endOffset": 87}], "year": 2017, "abstractText": "Ensemble learning has been widely employed by mobile applications, ranging from environmental sensing to activity recognitions. One of the fundamental issue in ensemble learning is the trade-off between classification accuracy and computational costs, which is the goal of ensemble pruning. During crowdsourcing, the centralized aggregator releases ensemble learning models to a large number of mobile participants for task evaluation or as the crowdsourcing learning results, while different participants may seek for different levels of the accuracy-cost trade-off. However, most of existing ensemble pruning approaches consider only one identical level of such trade-off. In this study, we present an efficient ensemble pruning framework for personalized accuracy-cost trade-offs via multi-objective optimization. Specifically, for the commonly used linear-combination style of the trade-off, we provide an objectivemixture optimization to further reduce the number of ensemble candidates. Experimental results show that our framework is highly efficient for personalized ensemble pruning, and achieves much better pruning performance with objective-mixture optimization when compared to state-of-art approaches.", "creator": "LaTeX with hyperref package"}}}