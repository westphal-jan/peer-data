{"id": "1701.07204", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2017", "title": "Fast Exact k-Means, k-Medians and Bregman Divergence Clustering in 1D", "abstract": "The $k$-Means clustering problem on $n$ points is NP-Hard for any dimension $d\\ge 2$, however, for the 1D case there exist exact polynomial time algorithms. The current state of the art is a $O(kn^2)$ dynamic programming algorithm that uses $O(nk)$ space. We present a new algorithm improving this to $O(kn \\log n)$ time and optimal $O(n)$ space. We generalize our algorithm to work for the absolute distance instead of squared distance and to work for any Bregman Divergence as well.", "histories": [["v1", "Wed, 25 Jan 2017 08:44:04 GMT  (385kb,D)", "https://arxiv.org/abs/1701.07204v1", null], ["v2", "Thu, 16 Feb 2017 20:40:50 GMT  (24kb)", "http://arxiv.org/abs/1701.07204v2", null], ["v3", "Fri, 30 Jun 2017 10:37:16 GMT  (22kb)", "http://arxiv.org/abs/1701.07204v3", null]], "reviews": [], "SUBJECTS": "cs.DS cs.AI cs.LG", "authors": ["allan gr{\\o}nlund", "kasper green larsen", "alexander mathiasen", "jesper sindahl nielsen", "stefan schneider", "mingzhou song"], "accepted": false, "id": "1701.07204"}, "pdf": {"name": "1701.07204.pdf", "metadata": {"source": "CRF", "title": "Fast Exact k-Means, k-Medians and Bregman Divergence Clustering in 1D", "authors": ["Allan Gr\u00f8nlund", "Kasper Green Larsen", "Alexander Mathiasen", "Jesper Sindahl Nielsen", "Stefan Schneider", "Mingzhou Song"], "emails": ["jallan@cs.au.dk.", "larsen@cs.au.dk.", "alexander.mathiasen@gmail.com.", "jasn@cs.au.dk.", "stschnei@cs.ucsd.edu.", "joemsong@cs.nmsu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 1.07 204v 3 [cs.D S] 3 0Ju nto n2O (\u221a lg lgn lg k) time. We generalize the new algorithms to work for absolute distance instead of square distance and also to work for any Bregman divergence. \u043a University Aarhus. E-mail: jallan @ cs.au.dk. Supported by MADALGO - Center for Massive Data Algorithmics, a center of the Danish National Research Foundation. \u2020 University Aarhus. E-mail: larsen @ cs.au.dk. Supported by MADALGO, a Villum Young Investigator Grant and an AUFF Starting Grant. E-mail: alexander.mathiasen @ gmail.com. Supported by MADALGO and an AUFF Starting Grant. E-mail: jasn @ cs.au.dk. Supported by MADALGO.mathiasen @ gmail.com. Supported by the MADALGO and an AUFF Starting Grant."}, {"heading": "1 Introduction", "text": "It is one of the, if not the primary problem in the field of machine learning, known as unsupervised learning and not a cluster problem, so famous and widely regarded as the k-Means problem: in the face of a multiple system X = {x1,..., xn} that includes a series of k-Means centroids M = {\u00b51,..., \u00b5k} that require a minimization of x-X min\u00b5, M | x-\u00b5, M | x-\u00b5 [x-\u00b5]. Several NP-Hardness results exist to find the optimal k-Means clustering in general, forcing a turn to heuristics. k-Means is NP-hard even for k = 2 and the general dimension [4], and it is also NP-hard for d = 2 and general k [16]. Even the hardness of the approximation results exist [15, 8]."}, {"heading": "1.1 Our Results", "text": "This is a factor of time and space compared to the existing solution (which we all support). (The k-Means cluster problem in 1D is defined as follows: (The k-Means cluster problem in 1D). (The k-Means cluster problem in 1D). (The k-Means cluster problem in 1D). (The k-Means cluster method used in O (n) n (n) n (n) n (n) space, or O) time when the algorithms for 1D k-Means are already sorted. (The algorithms also calculate the cost of the optimal k-Means cluster formation in O (n) n (n) n (n) space where the optimal O (kn) time is running. (The algorithms also calculate the cost of the optimal cluster solution using all k-Means). (This is relevant for selecting the right kn (n) space model."}, {"heading": "1.2 Outline", "text": "In Section 2, we describe the existing O (kn2) time algorithm for 1D k-Means clustering that uses the O (kn) space. In Section 3, we show how to calculate the same output as the old algorithm by using only O (kn) time and O (n) space. Then, we show how to improve the runtime to O (n2O). In this section, we describe the previous O (kn2) time and the O (kn) space algorithm presented in [22]. We also introduce the definitions and notation we use in our new algorithm. We will always assume that sorted inputs x1 \u2264 xn... xxx. If the input is not sorted, we start by ordering it in O (n lg) time that we use in our new algorithm."}, {"heading": "2.1 Algorithm Sketch", "text": "The algorithm calculates the optimal cluster formation using i-clusters for all prefixes of input points x1,.., xm, for m = 1,.., n, and for all i = 1,.., k using dynamic programming as a sequence.Leave D [i] [m] the cost of optimal cluster formation x1,..., xm in i-cluster. for i = 1 the cost of optimal cluster formation x1,..., xm in a cluster is the cluster price CC (1, m). That is, D [1] [m] = CC (1, m) for all m. This can be calculated in O (n) time by Lemma. For i > 1D [m] [m] = mmin j = 1 D [i \u2212 1] [j \u2212 1] + CC (j, m) (1) hint that D [i \u2212 m] is the optimal cluster."}, {"heading": "3 New Algorithms", "text": "The idea of the first new algorithm is simply to calculate tables D and T faster by shortening the time for calculating each row of D and T to O (n) time, rather than O (n2) time. This improvement takes advantage of a monotonicity property of the values stored in a row of T. This is explained in Section 3.1, which results in an O (kn) time and O (kn) space solution, assuming sorted inputs. Section 3.2 then shows how to reduce space consumption to only O (n) while maintaining the O (kn) runtime. In Section 3.3, we show that the same property allows us to solve 1D k averages for k = year (lgn) in, n2O (\u0445lg lg n lg k) time and linear space and solve the regulated version of 1D k-Means in O (n) time."}, {"heading": "3.1 Faster Algorithm From Monotone Matrices", "text": "In this section, we reduce the problem of calculating a series of D and T to looking for an implicitly defined n \u00b7 n matrix of a special form that allows us to calculate each line of D and T in linear time."}, {"heading": "3.2 Reducing Space Usage", "text": "Subsequently, we show how we reduce space use to only O (n) while combining O (kn) current time with a Hirschberg space reduction technology (11). (1) The problem is that we cannot store all T (1) points and find the optimal solution. (2) Below, we introduce an algorithm that completely avoids Table T (1). (1) Our key observation is the following: Assume k > 1 and that for each prefix x1,. (2) We have the optimal cost of clustering x1,. (2) Note that this is exactly the set of values. (2) We have the optimal cost of clustering x1,. (2) Note that these are exactly the values stored in the group k / 2. (2) Note that we have the optimal cost of clustering x1,. (3) Note that these are exactly the values stored in the group k / 2."}, {"heading": "3.3 Even Faster Algorithm", "text": "In this section, we show that the concave property we have demonstrated for the problem of cluster costs yields yields and algorithm for calculating the optimal k-means weighting for a given k = (lgn) in n2O (\u221a lg lg n lg k) time. The result follows almost directly from [19]. In [19], slider gives an algorithm with the aforementioned runtime for the problem of determining the shortest path of fixed length k in a directed acyclic graph with nodes 1. n, where the weights w (i, j) satisfy the concave property and are represented as a function that returns the weight of a given edge in constant time. \u2212 Theorem 4 ([19]) Calculation of a minimum weight curve k between two nodes in a directed acyclic graph of size n, where the weights satisfy the concave property n."}, {"heading": "3.3.1 Regularized Clustering", "text": "Consider a regulated version of the k-mean cluster problem, where we install the number of clusters and specify the cost of a cluster and ask to minimize the cost of the cluster plus the penalty \u03bb for each cluster used. For convenience, we assume that all input points are different. If we start \u03bb = 0, the optimal cluster has zero cost and use one cluster for each input point. Let us decrease the optimal number of clusters monotonically toward one in the optimal solution (zero cluster is not well defined). Let dmin be the smallest distance between the points in the input and use the cluster. The optimal cost of using n \u2212 1 clusters is then d2min / 2. If we decrease the optimal number of clusters toward 2 min / 2, it is less costly to use only n \u2212 1 clusters, since the optimal cluster is smaller than the cost of a cluster."}, {"heading": "4 Extending to More Distance Measures", "text": "In the following, we show how to generalize our algorithm for Bregman deviations and the sum of absolute distances while maintaining the same runtime and space requirements."}, {"heading": "4.1 Bregman Divergence and Bregman Clustering", "text": "In this section, we show how our algorithm generalizes to each Bregman divergence."}, {"heading": "Acknowledgements", "text": "We would like to thank Pawel Gawrychowski for pointing out important previous work on concave property."}], "references": [{"title": "Geometric applications of a matrixsearching algorithm", "author": ["A. Aggarwal", "M.M. Klawe", "S. Moran", "P. Shor", "R. Wilber"], "venue": "Algorithmica, 2(1):195\u2013208", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1987}, {"title": "Finding a minimum-weightk-link path in graphs with the concave monge property and applications", "author": ["A. Aggarwal", "B. Schieber", "T. Tokuyama"], "venue": "Discrete & Computational Geometry, 12(3):263\u2013280", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Better guarantees for k-means and euclidean k-median by primal-dual algorithms", "author": ["S. Ahmadian", "A. Norouzi-Fard", "O. Svensson", "J. Ward"], "venue": "CoRR, abs/1612.07925", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Np-hardness of euclidean sum-of-squares clustering", "author": ["D. Aloise", "A. Deshpande", "P. Hansen", "P. Popat"], "venue": "Machine Learning, 75(2):245\u2013248", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Analysis of ego network structure in online social networks", "author": ["V. Arnaboldi", "M. Conti", "A. Passarella", "F. Pezzoni"], "venue": "Privacy, security, risk and trust ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "How slow is the k-means method? In Proceedings of the Twenty-second Annual Symposium on Computational Geometry", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SCG \u201906, pages 144\u2013153, New York, NY, USA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027\u20131035. Society for Industrial and Applied Mathematics", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "The hardness of approximation of euclidean k-means", "author": ["P. Awasthi", "M. Charikar", "R. Krishnaswamy", "A.K. Sinop"], "venue": "L. Arge and J. Pach, editors, 31st International Symposium on Computational Geometry, SoCG 2015, June 22-25, 2015, Eindhoven, The Netherlands, volume 34 of LIPIcs, pages 754\u2013767. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Clustering with bregman divergences", "author": ["A. Banerjee", "S. Merugu", "I.S. Dhillon", "J. Ghosh"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Bregman voronoi diagrams", "author": ["J.-D. Boissonnat", "F. Nielsen", "R. Nock"], "venue": "Discrete & Computational Geometry, 44(2):281\u2013307", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "A linear space algorithm for computing maximal common subsequences", "author": ["D.S. Hirschberg"], "venue": "Commun. ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1975}, {"title": "The least weight subsequence problem", "author": ["D.S. Hirschberg", "L.L. Larmore"], "venue": "SIAM Journal on Computing, 16(4):628\u2013638", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1987}, {"title": "From genome mining to phenotypic microarrays: Planctomycetes as source for novel bioactive molecules", "author": ["O. Jeske", "M. Jogler", "J. Petersen", "J. Sikorski", "C. Jogler"], "venue": "Antonie Van Leeuwenhoek, 104(4):551\u2013567", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "A simple linear time algorithm for concave one-dimensional dynamic programming", "author": ["M.M. Klawe"], "venue": "Technical report, Vancouver, BC, Canada, Canada", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1989}, {"title": "Improved and simplified inapproximability for k-means", "author": ["E. Lee", "M. Schmidt", "J. Wright"], "venue": "Information Processing Letters, 120:40\u201343", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "The Planar k-Means Problem is NP-Hard", "author": ["M. Mahajan", "P. Nimbhorkar", "K. Varadarajan"], "venue": "pages 274\u2013285. Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimal interval clustering: Application to bregman clustering and statistical mixture learning", "author": ["F. Nielsen", "R. Nock"], "venue": "IEEE Signal Process. Lett., 21:1289\u20131292", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "The retail market as a complex system", "author": ["D. Pennacchioli", "M. Coscia", "S. Rinzivillo", "F. Giannotti", "D. Pedreschi"], "venue": "EPJ Data Science, 3(1):1", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Computing a minimum weightk-link path in graphs with the concave monge property", "author": ["B. Schieber"], "venue": "Journal of Algorithms, 29(2):204 \u2013 222", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "k-means requires exponentially many iterations even in the plane", "author": ["A. Vattani"], "venue": "Discrete & Computational Geometry, 45(4):596\u2013616", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Ckmeans.1d.dp: Optimal and fast univariate clustering; R package version", "author": ["H. Wang", "J. Song"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "Ckmeans", "author": ["H. Wang", "M. Song"], "venue": "1d. dp: optimal k-means clustering in one dimension by dynamic programming. The R Journal, 3(2):29\u201333", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "The concave least-weight subsequence problem revisited", "author": ["R. Wilber"], "venue": "Journal of Algorithms, 9(3):418 \u2013 425", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1988}, {"title": "Efficient dynamic programming using quadrangle inequalities", "author": ["F.F. Yao"], "venue": "Proceedings of the Twelfth Annual ACM Symposium on Theory of Computing, STOC \u201980, pages 429\u2013435, New York, NY, USA", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1980}], "referenceMentions": [{"referenceID": 3, "context": "k-Means is NP-hard even for k = 2 and general dimension [4] and it is also NP-hard for d = 2 and general k [16].", "startOffset": 56, "endOffset": 59}, {"referenceID": 15, "context": "k-Means is NP-hard even for k = 2 and general dimension [4] and it is also NP-hard for d = 2 and general k [16].", "startOffset": 107, "endOffset": 111}, {"referenceID": 14, "context": "Even hardness of approximation results exist [15, 8].", "startOffset": 45, "endOffset": 52}, {"referenceID": 7, "context": "Even hardness of approximation results exist [15, 8].", "startOffset": 45, "endOffset": 52}, {"referenceID": 7, "context": "In [8] the authors show there exists a \u03b5 > 0 such that it is NP-hard to approximate k-Means to within a factor 1+\u03b5 of optimal, and in [15] it is proved that \u03b5 \u2265 0.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "In [8] the authors show there exists a \u03b5 > 0 such that it is NP-hard to approximate k-Means to within a factor 1+\u03b5 of optimal, and in [15] it is proved that \u03b5 \u2265 0.", "startOffset": 134, "endOffset": 138}, {"referenceID": 2, "context": "357 [3].", "startOffset": 4, "endOffset": 7}, {"referenceID": 5, "context": "In theory, if Lloyd\u2019s algorithm is run to convergence to a local minimum, t could be exponential and there is no guarantee on how well the solution found approximates the optimal solution [6, 20].", "startOffset": 188, "endOffset": 195}, {"referenceID": 19, "context": "In theory, if Lloyd\u2019s algorithm is run to convergence to a local minimum, t could be exponential and there is no guarantee on how well the solution found approximates the optimal solution [6, 20].", "startOffset": 188, "endOffset": 195}, {"referenceID": 6, "context": "Lloyd\u2019s algorithm is often combined with the effective seeding technique for selecting initial centroids due to [7] that gives an expected O(lg k) approximation ratio for the initial clustering, which can then improved further by Lloyd\u2019s algorithm.", "startOffset": 112, "endOffset": 115}, {"referenceID": 21, "context": "In particular, there is an O(kn) time and O(kn) space dynamic programming solution for the 1D case, due to work by [22].", "startOffset": 115, "endOffset": 119}, {"referenceID": 4, "context": "The 1D kMeans problem is encountered surprisingly often in practice, some examples being in data analysis in social networks, bioinformatics and retail market [5, 13, 18].", "startOffset": 159, "endOffset": 170}, {"referenceID": 12, "context": "The 1D kMeans problem is encountered surprisingly often in practice, some examples being in data analysis in social networks, bioinformatics and retail market [5, 13, 18].", "startOffset": 159, "endOffset": 170}, {"referenceID": 17, "context": "The 1D kMeans problem is encountered surprisingly often in practice, some examples being in data analysis in social networks, bioinformatics and retail market [5, 13, 18].", "startOffset": 159, "endOffset": 170}, {"referenceID": 7, "context": "633 [8].", "startOffset": 4, "endOffset": 7}, {"referenceID": 8, "context": "In [9] the authors consider and define clustering with Bregman Divergences.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "Interestingly, the heuristic local search algorithm for Bregman Clustering [9] is basically the same approach as Lloyd\u2019s algorithm for k-Means.", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "We refer the reader to [9] for more about the general problem.", "startOffset": 23, "endOffset": 26}, {"referenceID": 16, "context": "For the 1D version of the problem, [17] generalized the algorithm from [22] to the k-Medians problems and Bregman Divergences achieving the same O(kn) time and O(kn) space bounds.", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "For the 1D version of the problem, [17] generalized the algorithm from [22] to the k-Medians problems and Bregman Divergences achieving the same O(kn) time and O(kn) space bounds.", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "They do however have many redeeming qualities, for instance Bregman Divergences are convex in the first argument, albeit not the second, see [9, 10] for a more comprenhensive treatment.", "startOffset": 141, "endOffset": 148}, {"referenceID": 9, "context": "They do however have many redeeming qualities, for instance Bregman Divergences are convex in the first argument, albeit not the second, see [9, 10] for a more comprenhensive treatment.", "startOffset": 141, "endOffset": 148}, {"referenceID": 8, "context": "The Bregman Clustering problem as defined in [9] is to find k centroids M = {\u03bc1, .", "startOffset": 45, "endOffset": 48}, {"referenceID": 20, "context": "dp [21].", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "2 The O(kn) Dynamic Programming Algorithm In this section, we describe the previous O(kn) time and O(kn) space algorithm presented in [22].", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "That is, D[1][m] = CC(1,m) for all m.", "startOffset": 10, "endOffset": 13}, {"referenceID": 21, "context": "This is exactly what is described in [22].", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "The problem of finding the minimum value in every row of a matrix has been studied before [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "[1] Let A be a matrix with real entries and let argmin(i) be the index of the leftmost column containing the minimum value in row i of A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "In [1], the authors showed the following: Theorem 1.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "[1] Finding argmin(i) for each row i of an arbitrary n\u00d7m monotone matrix requires \u0398(m lgn) time, whereas if the matrix is totally monotone, the time is O(m) when m > n and is O(m(1 + lg(n/m))) when m < n.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "1In [1] the authors use the maximum instead of the minimum", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "As [1] remarks, a matrix A is totally monotone if all its 2 \u00d7 2 submatrices are monotone.", "startOffset": 3, "endOffset": 6}, {"referenceID": 23, "context": "This is the property known as the concave (concave for short) property [24, 12, 23] and has been used to significantly speed up algorithms, including Dynamic Programming algorithms, for for other problems.", "startOffset": 71, "endOffset": 83}, {"referenceID": 11, "context": "This is the property known as the concave (concave for short) property [24, 12, 23] and has been used to significantly speed up algorithms, including Dynamic Programming algorithms, for for other problems.", "startOffset": 71, "endOffset": 83}, {"referenceID": 22, "context": "This is the property known as the concave (concave for short) property [24, 12, 23] and has been used to significantly speed up algorithms, including Dynamic Programming algorithms, for for other problems.", "startOffset": 71, "endOffset": 83}, {"referenceID": 10, "context": "2 Reducing Space Usage In the following, we show how to reduce the space usage to just O(n) while maintaining O(kn) running time using a space reduction technique of Hirschberg [11].", "startOffset": 177, "endOffset": 181}, {"referenceID": 18, "context": "The result follows almost directly from [19].", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "In [19] Schieber gives an algorithm with the aforementioned running time for the problem of finding the shortest path of fixed length k in a directed acyclic graph with nodes 1, .", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "Theorem 4 ([19]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 22, "context": "Schiebers algorithm relies crucially on algorithms that given a directed acyclic graph where the weights satisfy the concave property computes a minimum weight path in O(n) time [23, 14].", "startOffset": 178, "endOffset": 186}, {"referenceID": 13, "context": "Schiebers algorithm relies crucially on algorithms that given a directed acyclic graph where the weights satisfy the concave property computes a minimum weight path in O(n) time [23, 14].", "startOffset": 178, "endOffset": 186}, {"referenceID": 22, "context": "By the algorithms in [23, 14] this takes O(n) time.", "startOffset": 21, "endOffset": 29}, {"referenceID": 13, "context": "By the algorithms in [23, 14] this takes O(n) time.", "startOffset": 21, "endOffset": 29}, {"referenceID": 1, "context": "This means that if the inputs are integers, we can solve the 1D k-Means problem by a simple application of binary search in O(n lgU) time where U is the universe size [2].", "startOffset": 167, "endOffset": 170}, {"referenceID": 8, "context": "The Bregman Clustering problem as defined in [9], is to find a clustering, M = {\u03bc1, .", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "This is in one sense the defining property of Bregman Divergences [9].", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "The second important is the linear separator property, which is very important for clustering with Bregman Divergences but also very relevavant to Bregman Voronoi Diagrams [9, 10].", "startOffset": 172, "endOffset": 179}, {"referenceID": 9, "context": "The second important is the linear separator property, which is very important for clustering with Bregman Divergences but also very relevavant to Bregman Voronoi Diagrams [9, 10].", "startOffset": 172, "endOffset": 179}, {"referenceID": 16, "context": "The prefix sums idea used to implement the data structure used for Lemma 1 generalizes to Bregman Divergences as observed in [17] (under the name Summed Area Tables).", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "This was also observed in [17].", "startOffset": 26, "endOffset": 30}], "year": 2017, "abstractText": "The k-Means clustering problem on n points is NP-Hard for any dimension d \u2265 2, however, for the 1D case there exist exact polynomial time algorithms. Previous literature reported an O(kn) time dynamic programming algorithm that uses O(kn) space. We present a new algorithm computing the optimal clustering in only O(kn) time using linear space. For k = \u03a9(lg n), we improve this even further to n2 \u221a lg lgn lg k) time. We generalize the new algorithm(s) to work for the absolute distance instead of squared distance and to work for any Bregman Divergence as well. \u2217Aarhus University. Email: jallan@cs.au.dk. Supported by MADALGO Center for Massive Data Algorithmics, a Center of the Danish National Research Foundation. \u2020Aarhus University. Email: larsen@cs.au.dk. Supported by MADALGO, a Villum Young Investigator Grant and an AUFF Starting Grant. \u2021Aarhus University. Email: alexander.mathiasen@gmail.com. Supported by MADALGO and an AUFF Starting Grant. \u00a7Aarhus University. Email: jasn@cs.au.dk. Supported by MADALGO. \u00b6University of California, San Diego. Email: stschnei@cs.ucsd.edu. Supported by NSF grant CCF-1213151 from the Division of Computing and Communication Foundations. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. \u2016New Mexico State University. Email: joemsong@cs.nmsu.edu", "creator": "LaTeX with hyperref package"}}}