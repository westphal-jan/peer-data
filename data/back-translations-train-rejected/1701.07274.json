{"id": "1701.07274", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2017", "title": "Deep Reinforcement Learning: An Overview", "abstract": "We give an overview of recent exciting achievements of deep reinforcement learning (RL). We start with background of deep learning and reinforcement learning, as well as introduction of testbeds. Next we discuss Deep Q-Network (DQN) and its extensions, asynchronous methods, policy optimization, reward, and planning. After that, we talk about attention and memory, unsupervised learning, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, spoken dialogue systems (a.k.a. chatbot), machine translation, text sequence prediction, neural architecture design, personalized web services, healthcare, finance, and music generation. We mention topics/papers not reviewed yet. After listing a collection of RL resources, we close with discussions.", "histories": [["v1", "Wed, 25 Jan 2017 11:52:11 GMT  (54kb)", "http://arxiv.org/abs/1701.07274v1", "arXiv admin note: text overlap witharXiv:1605.07669by other authors"], ["v2", "Thu, 26 Jan 2017 16:38:08 GMT  (54kb)", "http://arxiv.org/abs/1701.07274v2", null], ["v3", "Sat, 15 Jul 2017 01:49:43 GMT  (194kb,D)", "http://arxiv.org/abs/1701.07274v3", "major updates for both content and organization"], ["v4", "Sun, 3 Sep 2017 12:39:11 GMT  (162kb,D)", "http://arxiv.org/abs/1701.07274v4", null], ["v5", "Fri, 15 Sep 2017 13:12:26 GMT  (163kb,D)", "http://arxiv.org/abs/1701.07274v5", null]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1605.07669by other authors", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuxi li"], "accepted": false, "id": "1701.07274"}, "pdf": {"name": "1701.07274.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["(yuxili@gmail.com)"], "sections": [{"heading": null, "text": "ar Xiv: 170 1.07 274v 1 [cs.L G] 25 Jan 2017We will present an overview of the recent exciting achievements of deep reinforcement learning (RL), starting with a background of deep learning and reinforcement learning, as well as the introduction of testbeds. Next, we will discuss Deep Q Network (DQN) and its enhancements, asynchronous methods, policy optimization, reward and planning, then we will talk about attention and memory, unattended learning and learning. Then, we will discuss various applications of RL, including games, in particular AlphaGo, robotics, spoken dialogue systems (a.k.a. chatbot), machine translation, text sequence prediction, neural architecture design, personalized web services, healthcare, finance, and music generation. We will mention topics / essays that have not yet been reviewed. After listing a collection of RL resources, we will conclude with discussions."}, {"heading": "1 INTRODUCTION", "text": "Reinforcement learning (RL) is usually about sequential decision making, solving problems in a wide range of fields in science, engineering and arts (Sutton and Barto, 2017).The integration of reinforcement learning and neural networks dated back to 1990s (Tesauro, 1994; Bertsekas and Tsitsiklis, 1996; Schmidhuber, 2015).With the recent exciting achievements of deep learning (LeCun et al., 2015; Goodfellow et al., 2016) benefiting from big data, powerful computational methods and new algorithmic methods, we have considered the renaissance of enhanced learning (Krakovsky, 2016), in particular the combination of enhanced learning and deep neural networks, i.e., deep reinforcement learning (deep RL).We have witnessed breakthroughs, such as deep Q-network (Mnih et al., 2015), AlphaGo (Silver et al., 2016) and differentiable neural (Graves et al, 2016); and novel architectures."}, {"heading": "2 BACKGROUND", "text": "In this section we briefly present concepts and foundations of deep learning (Goodfellow et al., 2016) and reinforcement learning (Sutton and Barto, 2017)."}, {"heading": "2.1 DEEP LEARNING", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "2.2 REINFORCEMENT LEARNING", "text": "Reinforcement learning usually solves sequential decision-making problems. An RL agent interacts with an environment over time. Q = Q = Q = Q function. At each step Q, the agent receives an incremental reward and selects an action at an action space A that follows a policy \u03c0 (at | st) that reflects the behavior of the agent, i.e. a mapping of the probability P (st + 1 | st, at), receives a scalar reward rt and transitions to the next state st + 1, according to the environment dynamics or model, for reward function R (s, a) and state conversion probability P (st + 1 | st, at). In an episodic problem, this process continues until the agent reaches a terminal state and then begins to learn anew. The return Rt = recorded cumulative reward with the discount factor P (st + 1 | st, at) The agent aims to maximize the expectation of such a long-term return."}, {"heading": "2.3 TESTBEDS", "text": "The Arcade Learning Environment (ALE) (Bellemare et al., 2013) is a framework composed of Atari 2600 games to develop and evaluate AI agents. DeepMind has released a first-person 3D gaming platform DeepMind Lab (Beattie et al., 2016). Deepmind and Blizzard will work together to release the Starcraft II AI research environment (goo.gl / Ptiwfg). OpenAI Gym (https: / / gym.openai.com) is a toolkit for developing RL algorithms consisting of environments such as Atari games and simulated robots, and a website for comparing and reproducing results.OpenAI Universe (https: / / universe.openai.com) is used to transform any program into a physical environment (World of games, including many Flash environments, including Atari mini-cars)."}, {"heading": "3 DEEP Q-NETWORK", "text": "Mnih et al. (2015) introduced Deep Q-Network (DQN) = Q-Q function (before DQN). It is well known that RL is unstable or even divergent when the action value Q function is approximated with a nonlinear function like neural networks. DQN made several important contributions: 1) Stabilizing the formation of the Q action value function with deep neural networks (CNN) using experiential replay (Lin, 1992) and target network; 2) Designing an end-to-end RL approach using only pixels and savegame as inputs, requiring minimal domain knowledge; 3) Training a flexible network using the same algorithm, network architecture and hyperparameters to perform many different tasks well, i.e. 49 Atari games (Bellemare et al., 2013) and performance of previous algorithms and Baraj chapters comparable to a human tester (see 16)."}, {"heading": "3.1 DOUBLE DQN", "text": "van Hasselt et al. (2016a) proposed Double DQN (D-DQN) to address the overestimated problem in Q-Learning. In both standard Q-Learning and DQN, the parameters are updated as follows: \u03b8t + 1 = \u03b8t + \u03b1 (y Q t \u2212 Q (st, at; \u03b8t)) and \u03b8tQ (st, at; \u03b8t), where y Q t = rt + 1 + \u03b3maxa Q (st + 1, a; \u03b8t), so that the max operator uses the same values to both select and evaluate an action. As a result, it is more likely to select overestimated values and leads to over-optimistic values. van Hasselt et al. (2016a) proposed to evaluate the greedy policies according to the online network, but to use the target network to estimate its value. This can be achieved with a slight change in the DQN reference algorithm, the Qest + dgt = Qst."}, {"heading": "3.2 PRIORITIZED EXPERIENCE REPLAY", "text": "Schaul et al. (2016) proposed to prioritize experience transitions so that important experience transitions can be repeated more frequently to learn more efficiently; the importance of experience transitions is measured by TD errors; the authors developed stochastic prioritization based on TD errors and used importance tests to avoid distortions in update distribution; the authors used prioritized experience transitions in DQN and D-DQN; and improved their performance in Atari games."}, {"heading": "3.3 DUELING ARCHITECTURE", "text": "Wang et al. (2016b) proposed the dueling network architecture \u03b8 to estimate the state value function V (s) and the associated advantage function A (s, a) and then to combine these to estimate the action value function Q (s, a), faster than Q-Learning. In DQN, a fully connected (FC) layer follows a CNN layer. In the dueling architecture, two streams of FC layers follow a CNN layer to estimate the value function and advantage function separately; then the two streams are combined to estimate the action function. Normally, we use the following methods to combine V (s) and A (s, a) to obtain Q (s, a), Q (s, a), Q (s), Q (s, a), Q (s, a), Q (s, a), B (s, a) = V (s, a) = V (s; \u03b8, \u03b2) + (A (s, a) \u2212 max a (s, a) \u2212 max a (s, a) \u2212 max a (s, a) to replace Q (s, a), 2001 \u2212 A (s), \u03b1 (a), and \u03b1 (a) with both enhanced currents (a)."}, {"heading": "3.4 MORE EXTENSIONS", "text": "Mnih et al. (2016) proposed asynchronous methods for RL methods, in particular the asynchronous advantage-actor-critic algorithm (A3C) as described in Section 4. Osband et al. (2016), designed a better exploration strategy to improve the DQN. O'Donoghue et al. (2017) proposed a policy gradient and Q-Learning (PGQ) as discussed in Section 5.6. He et al. (2017) suggested accelerating the DQN by optimizing, a limited optimization approach to spread the reward faster and improve accuracy over the DQN. Babaeizadeh et al. (2017) proposed a hybrid CPU / GPU implementation of the A3C. Liang et al. (2016) tried to understand the success of the DQN and reproduce the results with shallow RL."}, {"heading": "4 ASYNCHRONOUS METHODS", "text": "Mnih et al. (2016) proposed asynchronous methods for four RL methods, Q-Learning, SARSA, nstep Q-Learning and Advantage Actor-Critic, and the asynchronous Advantage Actor-Critic (A3C) algorithm delivers the best results. Parallel actors use different exploration policies to stabilize education so that experiences are not used. Unlike most deep learning algorithms, asynchronous methods can be run on a single multi-CPU core. For Atari games, A3C ran much faster, but better than or comparable to DQN, Gorila, D-DQN, Dueling D-DQN, and prioritized D-DQN. A3C is also successful with continuous motor control problems: TORCS auto racing games and MujoCo physics games are manipulated and located."}, {"heading": "5 POLICY OPTIMIZATION", "text": "Policies tend to be stochastical, but Silver et al. (2014) introduced the deterministic policy gradient (DPG) to efficiently estimate policy gradients. Lillicrap et al. (2016) expanded the DPG with deep neural networks. We also present several recent work, including Guided Policy Search (Levine et al., 2016a), Trust Region Policy Optimization (Schulman et al., 2015), benchmark results (Duan et al., 2016), and Policy Gradient and Q-Learning (O'Donoghue et al., 2017)."}, {"heading": "5.1 DETERMINISTIC POLICY GRADIENT", "text": "Silver et al. (2014) introduced the deterministic policy gradient (DPG) algorithm for RL problems with continuous spaces of action. Deterministic policy gradient is the expected gradient of the action value function that integrates across the state space; in the stochastic case, on the other hand, the policy gradient integrates across both state and space of action. Consequently, the deterministic policy gradient can be more efficiently estimated than the stochastic policy gradient. Empirical results showed that it is superior to stochastic policy gradients, especially in high-dimensional tasks, for several problems: a high-dimensional bandit; standard benchmark RL tasks for mountain car and pendulum and 2D environments with a high-dimensional action function; and a high-dimensional action space."}, {"heading": "5.2 DEEP DETERMINISTIC POLICY GRADIENT", "text": "Lillicrap et al. (2016) proposed a model-free, deeply deterministic policy gradient (DDPG) algorithm in continuous action spaces by extending DQN (Mnih et al., 2015) and DPG (Silver et al., 2014). With stakeholder critics such as the DPG, the DDPG avoids optimizing action at every step to achieve a greedy Q-learning policy that will make it unfeasible in complex action spaces with large, unlimited functional approaches such as deep neural networks. To make learning stable and robust, DDPQ uses experience repetition and an idea similar to the target network, the \"soft\" target, which does not directly copy the weights as in DQN, but slowly updates the soft target network weights to capture the learned network weights by deriving the physical weights with the help of DQN."}, {"heading": "5.3 GUIDED POLICY SEARCH", "text": "Levine et al. (2016a) proposed to jointly train perception and control systems end-to-end to map raw image observations directly to torques on the robot's motors. Authors introduced Guided Policy Search (GPS) to train strategies that are presented as CNN by transforming political search into supervised learning to achieve data efficiency, with training data provided by a trajectory-centered RL method that operates under unknown dynamics. GPS switches between trajectory-centered RL and supervised learning to obtain the training data that comes from the government's own distribution of strategies to address the problem that supervised learning typically does not achieve good long-term performance. GPS uses pre-training to reduce the amount of experiential data to train visual motor strategies. Good performance has been achieved on a number of real-world manipulation tasks that require localization, visual tracking, and dealing with complex contact dynamics, and previous search dynamics."}, {"heading": "5.4 TRUST REGION POLICY OPTIMIZATION", "text": "Schulman et al. (2015) introduced an iterative process for the monotonous improvement of strategies and proposed a practical algorithm, Trust Region Policy Optimization (TRPO), by making several approaches. Furthermore, the authors combined policy iteration and policy gradients with analysis. In the experiments, TRPO methods performed well for simulated robot tasks such as swimming, jumping and walking, as well as playing Atari games directly from raw images."}, {"heading": "5.5 BENCHMARK RESULTS", "text": "Duan et al. (2016) presented a benchmark for continuous control tasks, including classic tasks such as cart poles, tasks with very large state and action spaces such as 3D humanoid locomotion and tasks with partial observations, as well as tasks with hierarchical structure, implemented various algorithms, including batch algorithms: REINFORCE, Truncated Natural Policy Gradient (TNPG), Reward-Weighted Regression (RWR), Relative Entropy Policy Search (REPS), Trust Region Policy Optimization (TRPO), Cross Entropy Method (CEM), Covariance Matrix Adaptation Evolution Strategy (CMA-ES); online algorithms: Deep Deterministic Policy Gradient (DPG); and recursive variants of batch algorithms (TRPGs, PPGs, Natural Policy TOS). The open source is available at: https / githlab / githlan / rpg.com / Dualgorithms."}, {"heading": "5.6 COMBINING POLICY GRADIENT AND Q-LEARNING", "text": "O'Donoghue et al. (2017) suggested combining policy gradient with off-policy Q-learning (PGQ) in order to benefit from experience repetition. Normally, actor-critic methods are oriented toward policy. Furthermore, the authors showed that action-value fitting techniques and actor-critic methods are equivalent and interpreted regularized policy-gradient techniques as learning algorithms with benefits. Empirically, the authors showed that PGQ surpassed DQN and A3C in Atari games."}, {"heading": "6 REWARD", "text": "Inverse reinforcement learning (IRL) is the problem of determining a reward function based on observations of optimal behavior (Ng and Russell, 2000). In imitation learning, or apprenticeship learning, an agent learns to perform a task from expert demonstrations, with samples of expert trajectories, without amplification signal, without additional data from the expert during training; two main approaches to imitation learning are behavioral cloning and reverse reinforcement learning; behavioral cloning is formulated as a supervised learning problem to map governmental action pairs of expert pathways to policy (Ho and Ermon, 2016)."}, {"heading": "6.1 GENERATIVE ADVERSARIAL NETWORKS", "text": "Goodfellow et al. (2014) proposed generative adversarial networks (GANs) to estimate generative models using an adversarial process by training two models simultaneously, a generative model G to capture the data distribution, and a discriminatory model D to estimate the probability that a sample comes from the training data but not from the generative model G.Goodfellow et al. (2014) modelled G and D with multi-layered perceptual variables: G (z: \u03b8g) and D (x: \u03b8d), which are parameters, x data points, and z input noise variables. First define an input noise variable pz (z). G is a differentiable function, and D (x) gives a scalar as the probability that x comes from the training data, rather than pg, the generative distribution we want to learn. D is trained to calculate the probability of the correct assignment of labels (G) to sample G (and G) from both training data."}, {"heading": "6.2 GENERATIVE ADVERSARIAL IMITATION LEARNING", "text": "Many IRL algorithms have a high complexity of time, with an RL problem in the inner loop. Ho and Ermon (2016) proposed generative, hostile imitation to learn strategies directly from data, bypassing the IRL intermediate step. Generative, hostile training was used to accommodate the discriminator, the distribution of states and actions that define expert behavior, and the generator, the policy. Generative, hostile imitation of learning finds a political process, so that a discriminating DR cannot distinguish states that follow expert policy, and states that follow imitation policy, forcing the DR to take 0.5 in all cases, and which is not distinguishable from the generator that policies. Such a game is called: max."}, {"heading": "7 PLANNING", "text": "Tamar et al. (2016) introduced Value Iteration Networks (VIN), a fully differentiated CNN planning module to approach the value iteration algorithm to learn, e.g. plan guidelines in RL. Unlike conventional planning, VIN is model-free, with reward and transition probabilities being part of the neural network to be learned, avoiding system identification problems. VIN can be trained end-to-end through back propagation. VIN can be generalized in a variety of tasks: simple grid worlds, Mars rover navigation, continuous control, and WebNav challenge for Wikipedia navigation links (Nogueira and Cho, 2016). One merit of Value Iteration Network and Dueling Network (Wang et al., 2016b) is that they design novel architectures for deep neural networks to amplify learning problems."}, {"heading": "8 ATTENTION AND MEMORY", "text": "The authors used RL methods, especially DNA algorithms, to solve the problem that the model cannot be differentiated, and experimented with an image classification task and a dynamic visual control problem. Xu et al. (2015) integrated attention to image capacities, trained attention with the REINFORCE algorithm, and demonstrated the effectiveness of attention on Flickr30k and MS COCO datasets. The attention mechanisms are also used in NLP, e.g. in Bahdanau et al. (2017) and with external memories."}, {"heading": "9 UNSUPERVISED LEARNING", "text": "Jaderberg et al. (2017) suggested that UNsupervised REinforcement and Auxiliary Learning (UNREAL) improve learning efficiency by maximizing pseudo-reward functions in addition to the usual cumulative reward while sharing a common representation. UNREAL benefits from learning from the abundance of possible training signals, especially when the extrinsic reward signals are rarely observed. UNREAL is composed of RNN-LSTM base agent, pixel control, reward prediction, and reproduction of value functions. The base agent is trained with A3C. Experiences from observations, rewards, and actions are stored in a response buffer used for aid tasks. Aid policies utilize the CNN and LSTM base along with a deconvolutionary network to maximize changes in pixel intensity in different regions of the input images."}, {"heading": "10 LEARNING TO LEARN", "text": "Learning to learn is related to transfer learning, multi-task learning or representation learning, and is a core gredient to get strong AI (Lake et al., 2016). Learning to learn is also related to meta learning or one-shot learning.Duan et al. (2017) and Wang et al. (2016a) proposed to learn a flexible RNN model to handle a family of RL tasks, to improve sample efficiency, learn new tasks in some samples, and benefit from previous knowledge. The agent is modeled with RNN, with inputs of observations, rewards, actions, and termination flags; the weights of RNN are trained with RL, TRPO in Duan et al. (2017) and A3C in Wang et al. (2016a) and achieve similar performance for different problems with specific RL algorithms. Duan et al. (2017) experimented with multi-arm bandits, tabular MDPs, and visual navigation."}, {"heading": "11 GAMES", "text": "We discuss Deep Q-Network (DQN) in Section 3 and its extensions, all of which experimented with Atari games. We discuss Mnih et al. (2016) in Section 4, Jaderberg et al. (2017) in Section 9, and Mirowski et al. (2017) in Section 13, and they used Labyrinth as a test bed. Backgammon and Go are perfect information games. We briefly discuss backgammon in Section 11.1 about board games. We talk about video games like Doom in Section 11.2. We use poker, a board game, in Section 11.3 about imperfect information games when it comes to game theory. Video games like Labyrinth and Doom are usually imperfect information games, while game theory is not (yet) used in this work to address the problem. We highlight AlphaGo (Silver et al., 2016) in Section 12 because of its importance."}, {"heading": "11.1 BOARD GAMES", "text": "Board games such as backgammon, go, chess, checkers, and Othello are classic test environments for RL / AI algorithms. Tesauro (1994) approached backgammon by using neural networks to approximate the value function learned with TD learning, and achieved a performance on a human level."}, {"heading": "11.2 VIDEO GAMES", "text": "Wu and Tian (2017) used A3C with CNN to train an agent in a partially observable 3D environment, Doom, using four raw images and game variables to predict the next action and value function, according to the curriculum approach of learning (Bengio et al., 2009), to begin with simple tasks and gradually move on to harder ones. It is not trivial to apply A3C directly to such 3D games, partly because of sparse and long-term reward. The authors won the title in Track 1 of the ViZDoom competition by a large margin and plan the following future work: a map from an unknown environment, localization, a global action plan and visualization of the thought process. Dosovitskiy and Koltun (2017) approached the problem of sensorimotor control in immersive environments with supervised learning and won the title Full Deathmatch Track of the Visual Doom AI Competition."}, {"heading": "11.3 IMPERFECT INFORMATION GAMES", "text": "Heinrich and Silver (2016) proposed Neural Fictitious Self-Play (NFSP) to combine fictional self-play with deep RL to learn approximate Nash balances for games with imperfect information in a scalable end-to-end approach with no prior domain knowledge. NFSP was evaluated on two-tier zero-sum games. In Leduc poker, NFSP approached a Nash equilibrium while common RL methods diverged from each other. In Limit Texas Hold'em, an imperfect information game on a real scale, NFSP demonstrated a similar performance from the ground up to modern superhuman algorithms based on significant domain expertise.Heads-up limit hold'em poker was essentially solved (Bowling et al., 2015), with contrafactual regret, minimization (CFR), de-situational self-play to recreate an extensive NFSP balance between two self-play situations."}, {"heading": "12 ALPHAGO", "text": "AlphaGo (Silver et al., 2016), a computer go program, won the European Human Go Championship in October 2015, 5 games to 0, and became the first computer go program to win a human professional go player without handicaps on a 19 \u00d7 19 board. Shortly thereafter, in March 2016, AlphaGo defeated Lee Sedol, an 18-time world champion in the go game, 4 games to 1, making headlines worldwide. This set a milestone in AI. The challenge of solving Go stems not only from the gigantic search space of about 250150, an astronomical number, but also from the rigor of the position assessment successfully used in solving many other games such as backgammon and chess."}, {"heading": "12.1 TRAINING PIPELINE AND MCTS", "text": "We briefly discuss how AlphaGo works on the basis of Silver et al. (2016) and Sutton and Barto (2017). See Chapter 16 in Sutton and Barto (2017) for a detailed and intuitive description of AlphaGo. See Deepmind description of AlphaGo at goo.gl / lZoQ1d.AlphaGo was built using techniques of deep CNN, supervised learning, reinforcement learning, and Monte Carlo tree search (MCTS) (Browne et al., 2012; Gelly et al., 2012). AlphaGo consists of two phases: neural network education pipeline and MCTS. The training phases include the formation of a supervised learning (SL) political network of expert steps, a rapid rollout policy, and an RL value network network.The SL political network has revolutionary layers, ReLU nonlinearities, and an output softmax layer that represents probability distribution."}, {"heading": "12.2 DISCUSSIONS", "text": "The Deepmind team integrated several existing techniques to develop AlphaGo, and it has achieved tremendous results. However, the RL Policy Network and the RL Value Network are not strong / precise enough, so the RL Value Network, along with the SL Policy Network and the Rollout Network, will assist MCTS in finding the move, which may explain the one game loss against Lee Sedol. Conversely, AlphaGo still requires manually defined features with human knowledge, so it is not yet a complete solution; in contrast, DQN only needs raw pixels and scores as inputs. Such room for improvement would stimulate intellectual inquisition for better computer go programs, potentially only with deep RL, without MCTS, such as TD-Gammon (Sutton and Barto, 2017). This would be based on a novel RL algorithm, a novel deep neural network architecture and powerful computing."}, {"heading": "13 ROBOTICS", "text": "As we discuss in Section 5, Schulman et al. (2015) proposed a Guided Policy Search (GPS) to handle physical robots. Mirowski et al. (2017) gained navigation skills by solving an RL problem to maximize cumulative reward and jointly considering un / self-monitored tasks to improve data efficiency and task performance. The authors addressed the sparse reward problems by increasing the loss by performing two auxiliary tasks: 1) the unattended reconstruction of a low-dimensional depth map for imaging learning to support avoidance of obstacles and short-term trajectory planning; 2) the self-monitored classification of circuits within a local trajectory. The authors built a stacked LSTM to use memory at different time scales. The proposed agent learns to navigate complex situations."}, {"heading": "14 SPOKEN DIALOGUE SYSTEMS", "text": "There are two categories: chat-oriented and task-oriented systems; the former aims to engage with users in a contextually reasonable manner; the latter aims to reward users for specific goals (Su et al., 2016b).Li et al. (2014): Mysticity and misalignment of users for better reward, coherence and simplicity of response to solve the problems in the sequence of sequence models based on Sutskever et al. (2014): Disregard of the perception of the likelihood of creating a dialogue in the sequence of repetitive responses. The authors have designed a reward function to reflect the above characteristics, and optimize policies to optimize long-term reward."}, {"heading": "15 MACHINE TRANSLATION", "text": "He et al. (2016a) proposed dual learning mechanisms to address the problem of data hunger in machine translation, inspired by the observation that information feedback between the origin, the translation of language A into language B, and the dual, the translation from B to A, can help improve both models of neural translation, using the probability of the language model as a reward signal. Experiments showed that the dual learning approach, with only 10% bilingual data for warm-start and monolingual data, works comparably to previous methods of neural machine translation with full bilingual data from English to French. The dual learning mechanism could extend to many tasks if the task has a dual form, e.g. speech recognition and text in language, caption and image generation, answering questions and questions, search and key word extraction, etc. See Sutskever et al. (2014); Bahdanau et al (2015) for the sequence of neural translation sequences."}, {"heading": "16 TEXT SEQUENCE PREDICTION", "text": "Text generation models are typically based on n-gram, feed-forward neural networks, or recurrent neural networks, which are trained to predict the next word as input based on the previous basic truth words; then the trained models are used in the test to generate a sequence word by word, using the generated words as input; errors pile up along the way, causing the problem of exposure bias. Furthermore, these models are trained with word-level losses, e.g. cross-entropy, to maximize the likelihood of the next word; however, the models are evaluated on another metric, such as BLEU.Ranzato et al. (2016), whose proposed Mixed Incremental Cross-Entropy Reinforce (MIXER) is used to predict sequence, with incremental learning and a loss function combining both REINFORCE and crossentropy. MIXER is a sequence algorithm for predicting the next word, rather than the next training algorithm, such as BLACE."}, {"heading": "17 NEURAL ARCHITECTURE DESIGN", "text": "The design of neural networks is a notorious non-trivial engineering problem, and the search for neural architectures offers a promising way to explore it. Zoph and Le (2017) proposed the search for neural architectures to generate neural architectures using an RNN trained by RL, in particular REINFORCE, which searches from the ground up in the variable length space to maximize the expected accuracy of the generated architectures using a validation set. In the RL formulation, a controller generates hyperparameters as a result of tokens, which are actions selected from hyperparameter spaces; each gradient update of the policy parameters corresponds to training a generated network for convergence; an accuracy on a validation set is the reward signal. The search for neural architectures can generate revolutionary layers, with skipped connections or branching layers, recurring and recurring parameters compared to the authors \"10."}, {"heading": "18 PERSONALIZED WEB SERVICES", "text": "Li et al. (2010) formulated the recommendation of personalized news articles as a context-dependent bandit problem to learn an algorithm to sequentially select articles for users based on contextual information from the user and articles, such as user historical activity and descriptive information and content categories, and to use user feedback to maximize article selection policies in the long run. Theocharous et al. (2015) formulated a personalized ad recommendation system as an RL problem to maximize life-time value (LTV) with theoretical guarantees, in contrast to a short-sighted solution with supervised learning or context-dependent bandit formulation, usually using the performance metric of click rate (CTR). Since the models are difficult to learn, the authors used a model-free approach to determine a lower limit for the expected return of a policy to address the non-political evaluation problem (see a detailed usage guide, i.e. a current usage guide)."}, {"heading": "19 HEALTHCARE", "text": "Personalised medicine is becoming increasingly popular in the healthcare sector. It systematically optimises patient healthcare, particularly in chronic diseases and cancers, by using individual patient information, potentially from the electronic health record (EHR / EMR), which are sequential decision-making issues. Some issues in DTRs do not comply with the RL standard. Shortreed et al. (2011) addressed the issue of missing data and developed methods to quantify the evidence for the optimal policy learned. Goldberg and Kosorok (2012) suggested methods for censored data (patients may fail during the study) and a flexible number of stages. See Chakraborty and Murphy (2014) for a recent survey and Kosorok and Moodie (2015) for an edited book on recent advances in DTRs. Currently, Q-Learning is the RL method in DTRs. It is interesting to see the applications of deeper RL methods in this area."}, {"heading": "20 FINANCE", "text": "RL is a natural solution to some financial and economic problems (Hull, 2014; Luenberger, 1997), such as option pricing (Longstaff and Schwartz, 2001; Tsitsiklis and Van Roy, 2001; Li et al., 2009) and multiple portfolio optimization (Brandt et al., 2005), using value-oriented RL methods. Moody and Saffell (2001) suggested using the political gradient to learn trading; Deng et al. (2016) extended it to include deep neural networks. Deep (amplification) learning would provide better solutions to some issues of risk management (Hull, 2014; Yu et al., 2009). The market efficiency hypothesis is fundamental in finance. However, there are known behavioral distortions in human decision-making under uncertainty. A balance is the adaptive market hypothesis (Lo, 2004), which can be addressed by reinforcement."}, {"heading": "21 MUSIC GENERATION", "text": "Jaques et al. (2017) proposed combining maximum probability estimation with RL training, using RL to impose structure on an RNN trained on data by selecting reward functions to try to ensure a coherent global structure in multi-level generated sequences. A Note RNN was trained to predict the next note in a music sequence with a large corpus of songs. Then, the Note RNN was refined with RL to obtain an RL tuner, with a reward function taking into account both the rules of music theory as well as the output of another trained Note RNN. RL Tuner produced more pleasant and subjectively appealing melodies than alternative methods. The proposed approach has the potential to train sequence models outside of music by enabling the coding of high-level expertise in the RNN."}, {"heading": "22 TO-DO LIST", "text": "We list interesting and / or important directions / works that we have not discussed in this overview as described below; Johnson et al. (2016); Maual et al. (2016) \u2022 in the hope that it would provide \u2022 guidance for those interested in further studies.3 This would be part of our future work.4 \u2022 Understanding deep learning, Daniely et al. (2016); Li et al. (2016); Zhang et al. (2017) \u2022 Exploration, e.g. Stadie et al. (2015); Bellemare et al. (2016); Kulkarni et al. (2016); Osband et al. (2016); Nachum et al. (2017) \u2022 Model-based learning, e.g., Oh et al. (2015); Gu et al. (2016b); retrace algorithm, Munos et al. (2016) \u2022 predictron, Silver et al al al al al al al al. (2017) \u2022 Hierarchical RL, e.g., Kulni karal. (2016), al."}, {"heading": "23 RESOURCES", "text": "We list some resources for Deep RL that are by no means complete."}, {"heading": "23.1 BOOKS", "text": "\u2022 The Unique and Intuitive Booklet for Reinforcement Learning by Richard S. Sutton and Andrew G. Barto (Sutton and Barto, 2017) \u2022 Concise and Theoretical, Algorithms for Reinforcement Learning by Csaba Szepesva'ri (Szepesva'ri, 2010) \u2022 A Theoretical Book on Approximate Dynamic Programming by Dimitri P. Bertsekas (Bertsekas, 2012) \u2022 A Book Oriented to Operations Research, Approximate Dynamic Programming, by Warren B. Powell (Powell, 2011) \u2022 Deep Booklet by Ian Goodfellow, Yoshua Bengio and Aaron Courville (Goodfellow et al., 2016)"}, {"heading": "23.2 COURSES", "text": "\u2022 David Silver, Reinforcement Learning, 2015, slides (goo.gl / UqaxlO), video lectures (goo.gl / 7BVRkT) \u2022 Sergey Levine, John Schulman and Chelsea Finn, CS 294: Deep Reinforcement Learning, Spring 2017, http: / / rll.berkeley.edu / deeprlcourse / \u2022 Charles Isbell, Michael Littman and Pushkar Kolhe, Udacity: Machine Learning: Reinforcement Learning, goo.gl / eyvLfg \u2022 Fei-Fei Li, Andrej Karpathy and Justin Johnson, CS231n: Convolutional Neural Networks for Visual Recognition, http: / / cs231n.stanford.edu \u2022 Richard Socher, CS224d: Deep Learning for Natural Language Processing, http: / / cs224d.stanford.edu \u2022 Nando de Freitas Deep Deep Nectuations / Lecups, Lecups / www.Lecops"}, {"heading": "23.3 TUTORIALS", "text": "\u2022 David Silver, Deep Reinforcement Learning, ICML 2016 \u2022 Pieter Abbeel and John Schulman, Deep Reinforcement Learning Through Policy Optimization, NIPS 2016 \u2022 Andrew Ng, Nuts and Bolts of Building Applications using Deep Learning, NIPS 2016 \u2022 John Schulman, The Nuts and Bolts of Deep Reinforcement Learning Research, Deep Reinforcement Learning Workshop, NIPS 2016 \u2022 John Schulman, Deep Reinforcement Learning, Deep Learning School, 2016 \u2022 Pieter Abbeel, Deep Reinforcement Learning, Deep Learning Summer School, 2016; http: / / videolectures.net / deeplearning2016 abbeel deep reinforcement / \u2022 David Silver, Deep Reinforcement Learning, 2nd Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM), Edmonton 2015; http: / / videolectures.net / deeplearningm2015 abbeel Deep Reinforcement Reinforcement / Multidisciplinary Learning 2016 (RLDM)"}, {"heading": "23.4 CONFERENCES, JOURNALS AND WORKSHOPS", "text": "\u2022 NIPS: Neural Information Processing Systems \u2022 ICML: International Conference on Machine Learning \u2022 ICLR: International Conference on Learning Representation \u2022 RLDM: Multidisciplinary Conference on Reinforcement Learning and Decision Making \u2022 AAAI, IJCAI, ACL, EMNLP, SIGDIAL, ICRA, IROS, KDD, SIGIR, CVPR, etc. \u2022 Science Robotics, JMLR, MLJ, AIJ, JAIR, PAMI, etc. \u2022 Nature May 2015, Science July 2015, Survey Papers on Machine Learning / AI \u2022 Deep Reinforcement Learning Workshop, NIPS 2016, IJCAI 2016 \u2022 Deep Learning Workshop, ICML 2016"}, {"heading": "23.5 BLOGS", "text": "\u2022 Andrej Karpathy, karpathy.github.io, esp. goo.gl / 1hkKrb \u2022 Denny Britz, www.wildml.com, esp. goo.gl / MyrwDC \u2022 Junling Hu, Reinforcement learning explained - Learning to act on long-term gains \u2022 Li Deng, How deep reinforcement learning can help chatbots \u2022 Christopher Olah, colah.github.ioIn the current information / social media age, we are overwhelmed by information, e.g. from Twitter, Google +, WeChat, arXiv, etc. The ability to efficiently select the best information is becoming indispensable."}, {"heading": "24 DISCUSSIONS", "text": "It is the first time that a country in which it is a country, in which it is not only a country, but also a country in which it is not a country, in which it is not only a country, but also a country in which it is not a country, but in which it is a country, in which it is a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, country, country, country, country, country, country, country, country, country, country, country, country, in which it is a country, country, country, country, country, country, in which it is a country, country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, a country, in which it is a country, country, country, country, country, country, country, country, country, country, country, country, country, country, country, country, country, country, country, country, country, country, country, in which is a country, country, country, country, in which it is a country, country, country, country, country, country, country, in which is a country, country, in which is a country, country, country, in which is a country, country, country, in which is a country, in which is a country, country, in which is a country, in which is a country, in which is a country, country, in which is a country, in which is a country, country"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We give an overview of recent exciting achievements of deep reinforcement learning (RL). We start with background of deep learning and reinforcement learning, as well as introduction of testbeds. Next we discuss Deep Q-Network (DQN) and its extensions, asynchronous methods, policy optimization, reward, and planning. After that, we talk about attention and memory, unsupervised learning, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, spoken dialogue systems (a.k.a. chatbot), machine translation, text sequence prediction, neural architecture design, personalized web services, healthcare, finance, and music generation. We mention topics/papers not reviewed yet. After listing a collection of RL resources, we close with discussions.", "creator": "LaTeX with hyperref package"}}}