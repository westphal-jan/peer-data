{"id": "1701.07769", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2017", "title": "Ethical Considerations in Artificial Intelligence Courses", "abstract": "The recent surge in interest in ethics in artificial intelligence may leave many educators wondering how to address moral, ethical, and philosophical issues in their AI courses. As instructors we want to develop curriculum that not only prepares students to be artificial intelligence practitioners, but also to understand the moral, ethical, and philosophical impacts that artificial intelligence will have on society. In this article we provide practical case studies and links to resources for use by AI educators. We also provide concrete suggestions on how to integrate AI ethics into a general artificial intelligence course and how to teach a stand-alone artificial intelligence ethics course.", "histories": [["v1", "Thu, 26 Jan 2017 16:52:22 GMT  (33kb)", "http://arxiv.org/abs/1701.07769v1", "29 pages including all case studies and links to video media on YouTube"]], "COMMENTS": "29 pages including all case studies and links to video media on YouTube", "reviews": [], "SUBJECTS": "cs.AI cs.CY cs.GL", "authors": ["emanuelle burton", "judy goldsmith", "sven koenig", "benjamin kuipers", "nicholas mattei", "toby walsh"], "accepted": false, "id": "1701.07769"}, "pdf": {"name": "1701.07769.pdf", "metadata": {"source": "CRF", "title": "Ethical Considerations in Artificial Intelligence Courses", "authors": ["Emanuelle Burton"], "emails": ["emanuelle.burton@gmail.com", "goldsmit@cs.uky.edu", "skoenig@usc.edu", "kuipers@umich.edu", "n.mattei@ibm.com", "toby.walsh@data61.csiro.au"], "sections": [{"heading": null, "text": "ar Xiv: 170 1.07 76The recent increase in interest in ethics in artificial intelligence may lead many educators to ask how they can address moral, ethical and philosophical issues in their AI courses. As lecturers, we want to develop curricula that not only prepare students to become practitioners of artificial intelligence, but also understand the moral, ethical and philosophical implications that artificial intelligence will have on society. In this article, we offer practical case studies and links to resources for AI educators. We also give concrete suggestions on how to integrate AI ethics into a general course on artificial intelligence and how to teach a stand-alone course on the ethics of artificial intelligence."}, {"heading": "1 Introduction", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is not a country, but a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which"}, {"heading": "2 Some Ethical Problems Raised by AIs", "text": "The prospect of our society playing an important role in artificial intelligence raises many deep, important questions, many of which can be understood and analyzed through the lens of ethical theory. We briefly discuss some of these questions that have received the most attention lately, recognizing that there are many others [Russell et al., 2015]; we will return to a number of these questions by linking them to specific case studies for use in the classroom."}, {"heading": "2.1 How Should AIs Behave in Our Society?", "text": "We all know that computer programs can have unintended consequences and that computer systems raise additional concerns. Fiction raises apocalyptic examples such as SkyNet in the Terminator movies, but real issues such as high-speed algorithmic trading systems have actually caused \"flash crashes\" in the real economy [Kirilenko et al., 2015]. We address issues related to these two, in the case studies SkyNet and Machine Learning. We can also expect robots to become increasingly involved in our daily lives, whether they vacuum our floors, drive our cars or help care for our loved ones."}, {"heading": "2.3 Should AI Systems Be Allowed to Kill?", "text": "There are several ethical arguments, and a popular movement, against the use of killer robots in war (see, for example, http: / / futureoflife.org / open-letter-autonomous-weapons /). Critics of killer robots argue that the development of killer robots will inevitably trigger a global arms race, and that there will be no way to prevent repressive governments, terrorist groups, or ethnic cleansing once this technology exists. They further argue that there are ways to use killer robots in warfare that are not about killing. There are also a number of arguments in favor of robots that kill. Advocates of robots that some wars are necessary and only take people out of the firing line; that such robots can be used both for deterrence and for actual violence; and that it is unrealistic to prevent this technology because it already exists in some forms, and there are significant political and financial resources to ensure that it continues to evolve."}, {"heading": "3 Tools for Thinking about Ethics and AI", "text": "There are many schools of thought within the study of ethics that differ not only in the answers they offer, but also in the way they formulate fundamental questions about how to understand the world, and how to respond to the ethical challenges it poses. Most (though not all) work in ethics - both academic and broader - has a normative purpose: to argue how people should act. But these normative works are significantly, if often invisibly, based on descriptive arguments; before offering recipes for how to address a particular problem, ethics scholars construct arguments for why it is both accurate and useful to understand this problem in a particular way."}, {"heading": "3.1 Deontology", "text": "Deontology understands by ethics that it is about following the moral law. In its widely accepted form, it was developed by Immanuel Kant in the late eighteenth century, but has ancient roots both in traditions of divine command (such as the ancient Israelite religion, the source of the Ten Commandments and the foundation of Judaism, Christianity and Islam) and in other legal systems. The fundamental question of deontology is: \"What is my duty?\" According to Deontology, this duty cannot be understood in the form of laws. According to Kant, it is the responsibility of each individual to discover the true moral law for himself. Although the theoretical foundations of right-based ethics and Kantian deontology are different, in both systems any true law will be universally applicable. Deontology dovetails very well with both the professional and popular understanding of how an ethical machine might arise."}, {"heading": "3.2 Utilitarianism", "text": "The most recent approach, utilitarian ethics, was developed by Jeremy Bentham and John Stuart Mill in the late eighteenth to mid-nineteenth centuries. Utilitarianism's basic question is: \"What is the greatest possible good for the greatest number?\" - or, in William K. Frankena's more recent phrase [Frankena, 1963]: \"the greatest possible balance of good versus evil.\" In computer science and more generally in the social sciences, we use \"benefit\" as a proxy for individual goodness and the sum of individual utilities as a measure of social well-being, often without thinking about the possibility of otherwise thinking about the social good. The underlying assumption is that benefit can be quantified as a mixture of happiness or other qualities, so that we can compare the usefulness of the individual or the usefulness that a person derives from each of several possible outcomes. So-called \"utilitarian calculation\" compares the sum of individual usefulness (or negative) as the result of any ethical decision."}, {"heading": "3.3 Virtue Ethics", "text": "Virtue ethics (also known as teleological ethics) focuses on purposes or goals. The fundamental question of virtue ethics is \"Who should I be?,\" which is based on Aristotle and is most clearly outlined in non-chomachic ethics [Aristotle, 1999]. Unlike deontological ethics, virtue ethics looks at goodness in local and non-universal terms (what is the best form / version of this particular thing in these particular circumstances?) and emphasizes not universal laws but local norms. A central component of the good life according to virtue ethics is \"phronesis\" (often translated as \"moral prudence\" or \"practical wisdom\"). Unlike pure knowledge (\"Sophia\"), ethics is a central component of virtue ethics."}, {"heading": "3.4 Ethical Theory in the Classroom: Making the Most of Multi-", "text": "This year, as never before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country"}, {"heading": "4 Case Studies", "text": "A popular way to teach ethics in courses on artificial intelligence is to use case studies triggered by either real events or fiction. Stories, literature, plays, poetry, and other narratives have always been a way to talk about our own world and tell us how it is and what impact our decisions will have. We present two case studies based on movies, and a third that discusses recent revelations about the distortions encoded in machine learning-based decision algorithms."}, {"heading": "4.1 Case Study 1: Elder Care Robot", "text": "[Frank sits in the woods to talk to him.] aF: [Frank panics] I hate hiking. God, damn bugs! You see a tree, you've seen everything. You just hate me when you go. [Q: What is the goal of my program to improve your health? I am able to adjust my methods. Would you prefer another form of moderate exercise? Q: I would rather die if I eat cheeseburgers than if I live? They send me back to the warehouse and wipe my memory. [Turns and walks on.] Q: [Pauses, turns around and starts walking.] Well, if we go, we do likewise. aClip available at https: / / youtu.be / eQxUW4B622E. [Frank sits in the woods to walk him.]"}, {"heading": "4.1.1 What Are the Ethical Issues?", "text": "His protagonist Frank is a retired jewel thief whose children give him a caretaker robot so that he can stay at home even as his dementia progresses. Although the film seems simple and amusing in many ways, it raises some troubling questions from the perspective of how he talks about the role of robots in our society. For example: 1. It turns out that Frank's health is robot's top priority and overrides all other considerations (including the well-being of others). 2. Throughout the film, robots play a central role in leading Frank back to a criminal life. Robot protocols that help Frank find a long-term activity that keeps him mentally engaged and physically active are at the center. Since preparing for blackmail meets these criteria, robot is willing to rob Frank of his wealthy neighbors, and even to help him. 3. Robot and Frank develop a strange friendship over the course of the relationship, although the relationship between Frank and the audience is not really a priority, but rather the person and the audience."}, {"heading": "4.1.2 How Does Ethical Theory Help Us Interpret Robot & Frank?", "text": "In fact, you have to be able to put yourself in a situation where you see yourself in a position to put yourself in the centre, in a position to put yourself in the centre, and in a position to put yourself in the centre."}, {"heading": "4.1.3 Conclusions and Additional Questions", "text": "The film raises a number of important questions about how a care robot should behave, in relation to the person to be cared for and in relation to the rest of society. Based on what we see about the behaviour of the robot, we can make some guesses about how the ethical system of the robot, or perhaps just its target structure, was conceived. This can and should lead to a serious discussion, either in class or in writing, about whether this is the way we think that care robots should decide how to act. Some possible questions for discussions about care robots for the elderly: 1. If an older person wants to behave in a way that violates common social norms, should a care robot intervene, and if so, how? 2. If the elderly person seriously wants to die, should the robot help them die? 3. If the older person asks the robot to help him make preparations for taking his own life, the robot has an obligation to inform other family members. 4. If the older person, despite a certain situation, we should not touch a child, in spite of a certain situation, a robot."}, {"heading": "4.2 Case Study 2: SkyNet", "text": "In the movie Terminator 2, a future AI, SkyNet, will almost wipe out the human race. aSC: I need to know how SkyNet is built. Who is responsible? T2: The man most directly responsible is Miles Bennett Dyson. SC: Who is this? T2: The Director of Special Projects at Cyberdyne Systems Corporation. SC: Why him? T2: In a few months, he will create a revolutionary type of microprocessor. SC: Go. So what? T2: In three years, Cyberdyne will become the largest provider of military computer systems. All stealth bombers will be equipped with Cyberdyne computers and will be completely unmanned. Afterwards, they will fly with a perfect operational record. The SkyNet Funding Bill is passed. The system goes into effect on August 4, 1997. Human decisions will be removed from strategic defense."}, {"heading": "4.2.1 What Are The Ethical Issues?", "text": "The SkyNet study offers us a chance to look at and weigh two very different approaches to robot ethics. One is the question: \"How do we design AI systems to make them work ethically?\" The other is: \"How do we behave ethically as programmers and systems designers to reduce the risks that our systems and codes entail?\" It is worth noting that philosophers continue to question whether it makes sense to say that a cybernetic system is ethical or unethical. (See, 2016, Heron and Belford, 2015)"}, {"heading": "4.2.2 How Does Ethical Theory Help Us Interpret Terminator 2?", "text": "In this context, it should be noted that this project is a project, which is primarily a project."}, {"heading": "4.2.3 Conclusions and Additional Questions", "text": "The problem in this scenario is not extreme intelligence in an AI. SkyNet's knowledge base and problem-solving capability is not too far removed from today's state of the art; the problem was the extreme power that SkyNet had at its disposal to put its plans into practice. It seems to be a reasonable conclusion that SkyNet should not have been given control of the nuclear arsenal because its human creators had no way of predicting what it could do in unusual and untested circumstances. Nevertheless, one might wonder what it would take to determine that an AI system is capable of being responsible for the nuclear arsenal. \u2022 Under what conditions should people trust an AI system? \u2022 What criteria could human creators apply to determine how much power they entrust to a particular AI? \u2022 How can an AI system show that it is trustworthy?"}, {"heading": "4.3 Case Study 3: Bias in Machine Learning", "text": "In fact, the fact is that most of them will be able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process, a process in which there is a process in which there is a process, a process in which there is a process in which there is a process in which there is a process, a process in which there is a process, a process in which there is a process, a process, a process, a process in which there is a process, a process in which there is a process, a process in which there is a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process and a process in which there is a process, a process, a process, a process, a process, and a process, and a process, and a process, and a process, and a process, and a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process"}, {"heading": "4.3.1 What Are The Ethical Issues?", "text": "The history of discriminatory treatment has meant that women, minorities, and poor people have not had the same chances to achieve and succeed, and that the comparative lack of success is recorded in the data. Unless an algorithm can be designed to account for systemic distortions lurking in the data, the recommendations will continue to translate this history of limited opportunities into limited future opportunities - and, to add insult to injury, allow these biased assessments to be characterized as impartial. The problem is subtle: because we consider data to be value neutral, data is (consciously or not) used to circumvent societal rules and laws that explicitly exist to prevent decision-makers from using certain traits in decision-making, such as race."}, {"heading": "4.3.2 How Does Ethical Theory Help Us Discuss Machine Learning?", "text": "All three theories make it possible to describe violations inflicted on large groups of people as violations of society as a whole, but each offers a different explanation of why, and a different way of thinking about how to solve the problem. This definition of moral law means that the state and its citizens are obliged to ensure that this civil law is applied equally and impartially, as Deontology understands that people should be treated on the basis of their individual merits. This definition of moral law means that the state and its citizens are obliged to ensure that this civil law is applied equally and impartially."}, {"heading": "4.3.3 Conclusions and Additional Questions", "text": "An ethics unit gives teachers the chance to make students aware of the existence of this problem, which they are likely to have missed because they are accustomed to treating data sets as objective. Furthermore, by spending time with the various ethical theory schools, students will develop a sense of how social and ethical \"facts\" can change depending on which theoretical approach is used to describe a particular situation. Working with the various ethical theories will help students understand that social data is never objective, and help them think more creatively about designing systems that do not maintain an unfair bias. \u2022 What steps can we take to evaluate the data processed by an algorithm? At what point in the process should this assessment take place? \u2022 What are the risks of overcorrecting distorted data and how can they be prevented?"}, {"heading": "5 Teaching Ethics in AI Classes", "text": "Since AI technologies and their applications raise ethical questions, it makes sense to dedicate one or more lectures of an introductory AI class (or even an entire course) to them. Students should (1) reflect on the ethical issues that AI technologies and systems raise, (2) learn ethical theories (deontology, utilitarianism, and virtue ethics) that provide a framework that allows them to think about ethical issues, and (3) apply their knowledge to one or more case studies to describe both what is happening within them and to think about possible solutions to the ethical problems they are facing. (1) and (2) could be addressed in a lecture or two separate lectures. In the event of time constraints, (1) - (3) all could be dealt with in one lecture. An additional case study could be assigned as a homework, ideally as group work."}, {"heading": "5.1 Ethical Issues", "text": "AI systems can process large amounts of data, detect regularities in them, draw conclusions from them, and determine effective procedures - sometimes faster and better than humans, and sometimes as part of a hardware that is capable of performing many different, versatile, and potentially dangerous actions. AI systems can be used to gain new insights, support human decision-making, or make autonomous decisions. AI systems \"behavior can be difficult to validate, predict, or explain: AI systems are complex, rationally different from humans, and can change their behavior through learning. Their behavior can also be difficult for humans to monitor in the event of rapid decisions, such as buying and selling decisions in the stock markets. AI systems therefore raise a variety of questions (some of which are common in other information processing or automation technologies) that can be discussed with students, such as: \u2022 Do we have to worry about their reliability, robustness, and safety? \u2022 Do we have to ensure that their behavior is consistent with human standards and values? \u2022 How do we monitor their social behavior?"}, {"heading": "5.2 Case Studies", "text": "Options for case studies include anecdotes that are constructed to illustrate ethical tensions, or actual events (for example, in the form of news headlines), or science fiction movies and stories. News headlines can be used to illustrate ethical issues that are current, visible, and potentially affect students directly in their daily lives. An example is \"Man killed in horrific Tesla autopilot crash was saved by the software of his car weeks earlier\" by the Register [Thomson, 2016], or \"Microsoft's racist chatbot returns with the smoking of Twitter,\" by The Guardian [Gibbs, 2016]. Science fiction stories and movies can also be used to illustrate ethical issues, as they often \"stand out in their efforts to capture what is seen today through the lenses of the future.\" The storylines in science fiction films often reveal important philosophical issues related to moral action power and patience, awareness, privacy, and just to mention some."}, {"heading": "5.3 Teaching Resources", "text": "The Russell and Norvig textbook (third edition) gives a brief overview of the ethics and risks of developing AI systems in Section 26.3. A small number of courses on AI ethics have been held by Kaplan at Stanford University (CS122: Artificial Intelligence - Philosophy, Ethics, and Impact) and by Goldsmith at the University of Kentucky (CS 585: Science Fiction and Computer Ethics), see also [Bates et al., 2012, Bates et al., 2014, Burton et al., 2015, Burton et al., 2016a]. Burton, Goldsmith and Mattei are currently working on a textbook for their course and have already held a sample analysis [Burton et al., 2016b] of Forster's The Machine Stops [Forster, 2015, Burton et al., 2016a]. A number of workshops have also recently been held on ethics, such as the First Workshop on Artificial Intelligence and Ethics Afor AI 2007, the Artificial Intelligence Workshop for Artificial Intelligence.org and the Artificial Intelligence Workshop for 2015."}, {"heading": "6 Conclusion", "text": "We have provided two case studies from films as a template for current use or as inspiration for discussing other films. Furthermore, we have looked at a case study based on the ongoing problem of the bias of much big data and its impact on decision-making in the real world. These three cases are not meant to be a complete catalogue of ethical questions or cases, but should serve as inspiration and guidance for teachers who want to spend a few hours looking at some of the societal implications of our work. Our position is that as educators, we have a responsibility to educate students to identify the larger ethical issues and responsibilities that their work as technologists might come into contact with, and that the use of SF as a basis for achieving better learning, bonding and understanding of students. To this end, some of us have published work on our Science Fiction and Computer Ethics course in recent years [Bates et al., 2012, Bates et al., 2014, Burton et al., 2016b, al]."}, {"heading": "Acknowledgments", "text": "Emanuelle Burton and Judy Goldsmith are supported by the National Science Foundation under grant number 1646887. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the National Science Foundation.The research of Benjamin Kuipers at the Intelligent Robotics Lab at the University of Michigan is supported in part by grants from the National Science Foundation (IIS-1111494 and IIS1421168).The research of Sven Koenig at the University of Southern California is supported by the NSF under grant numbers 1409987 and 1319966.Some of Nicholas Mattei's research was conducted while he was working for Data61, CSIRO (formerly NICTA) and UNSW Australia. Data61, CSIRO (formerly NICTA) are supported by the Australian government through the Department of Communications and the Australian Research Council (ARC) through the www.ICT Centre of Excellence Program.Toby Davies is supported by the National Science Foundation, the ARER7sites..at / Agle7sites..at and the ARC."}], "references": [{"title": "Machine bias: There\u2019s software used across the country to predict future criminals", "author": ["Angwin et al", "J. 2016] Angwin", "J. Larson", "S. Mattu", "L. Kirchner"], "venue": "And it\u2019s biased against blacks. ProPublica", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Science fiction in computer science education", "author": ["Bates et al", "R. 2012] Bates", "J. Goldsmith", "R. Berne", "V. Summet", "N. Veilleux"], "venue": "In Proceedings of the 43rd ACM technical symposium on Computer Science Education,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Using science fiction in CS courses", "author": ["Bates et al", "R. 2014] Bates", "J. Goldsmith", "V. Summet", "N. Veilleux"], "venue": "In Proc. SIGCSE,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "The social dilemma of autonomous vehicles. arXiv:1510.03346", "author": ["Bonnefon et al", "2016] Bonnefon", "J.-F", "A. Shariff", "I. Rahwan"], "venue": "Submitted on 12 Oct 2015 (v1),", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "The second machine age: Work, progress, and prosperity in a time of brilliant technologies", "author": ["Brynjolfsson", "McAfee", "E. 2014] Brynjolfsson", "A. McAfee"], "venue": "WW Norton & Company", "citeRegEx": "Brynjolfsson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Brynjolfsson et al\\.", "year": 2014}, {"title": "Teaching AI ethics using science fiction", "author": ["Burton et al", "E. 2015] Burton", "J. Goldsmith", "N. Mattei"], "venue": "In 1st International Workshop on AI, Ethics and Society, Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Using SF to teach ethics", "author": ["Burton et al", "E. 2016a] Burton", "J. Goldsmith", "N. Mattei"], "venue": "WorldCon workshop,", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Using \u201cThe Machine Stops\u201d for teaching ethics in artificial intelligence and computer science", "author": ["Burton et al", "E. 2016b] Burton", "J. Goldsmith", "N. Mattei"], "venue": "In AI & Ethics: Workshops at the Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Ethics and Robotcs", "author": ["Capurro", "Nagenborg", "R. 2009] Capurro", "M. Nagenborg"], "venue": null, "citeRegEx": "Capurro et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Capurro et al\\.", "year": 2009}, {"title": "Science fiction as an introduction to ai research", "author": ["Goldsmith", "Mattei", "J. 2011] Goldsmith", "N. Mattei"], "venue": "In Second AAAI Symposium on Educational Advances in Artificial Intelligence", "citeRegEx": "Goldsmith et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goldsmith et al\\.", "year": 2011}, {"title": "Fiction as an introduction to computer science research", "author": ["Goldsmith", "Mattei", "J. 2014] Goldsmith", "N. Mattei"], "venue": "ACM Transactions on Computing Education (TOCE),", "citeRegEx": "Goldsmith et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldsmith et al\\.", "year": 2014}, {"title": "EU regulations on algorithmic decision-making and a \u201cright to explanation", "author": ["Goodman", "Flaxman", "B. 2016] Goodman", "S. Flaxman"], "venue": "arXiv preprint arXiv:1606.08813", "citeRegEx": "Goodman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2016}, {"title": "Fuzzy ethics: or how i learned to stop worrying and love the bot", "author": ["Heron", "Belford", "M.J. 2015] Heron", "P. Belford"], "venue": "ACM SIGCAS Computers and Society,", "citeRegEx": "Heron et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heron et al\\.", "year": 2015}, {"title": "The flash crash: The impact of high frequency trading on an electronic market", "author": ["Kirilenko et al", "A.A. 2015] Kirilenko", "A.S. Kyle", "M. Samadi", "T. Tuzun"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Affective basis of judgment-behavior discrepancy in virtual experiences of moral dilemmas", "author": ["Patila et al", "I. 2014] Patila", "C. Cogonia", "N. Zangrandob", "L. Chittarob", "G. Silania"], "venue": "Social Neuroscience,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Robot Ethics", "author": ["Patrick Lin", "Bekey", "K.A. 2014] Patrick Lin", "G.A. Bekey"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Research priorities for robust and beneficial artificial intelligence", "author": ["Russell et al", "S. 2015] Russell", "D. Dewey", "M. Tegmark"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Observing and recommending from a social web with biases. CoRR, abs/1604.07180", "author": ["Staab et al", "S. 2016] Staab", "S. Stalla-Bourdillon", "L. Carmichael"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Moral Machines: Teaching Robots Right from Wrong", "author": ["Wallach", "Allen", "W. 2008] Wallach", "C. Allen"], "venue": null, "citeRegEx": "Wallach et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wallach et al\\.", "year": 2008}], "referenceMentions": [], "year": 2017, "abstractText": "The recent surge in interest in ethics in artificial intelligence may leave many educators wondering how to address moral, ethical, and philosophical issues in their AI courses. As instructors we want to develop curriculum that not only prepares students to be artificial intelligence practitioners, but also to understand the moral, ethical, and philosophical impacts that artificial intelligence will have on society. In this article we provide practical case studies and links to resources for use by AI educators. We also provide concrete suggestions on how to integrate AI ethics into a general artificial intelligence course and how to teach a stand-alone artificial intelligence ethics course.", "creator": "LaTeX with hyperref package"}}}