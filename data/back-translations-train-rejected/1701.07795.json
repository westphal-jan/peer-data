{"id": "1701.07795", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2017", "title": "Match-Tensor: a Deep Relevance Model for Search", "abstract": "The application of Deep Neural Networks for ranking in search engines may obviate the need for the extensive feature engineering common to current learning-to-rank methods. However, we show that combining simple relevance matching features like BM25 with existing Deep Neural Net models often substantially improves the accuracy of these models, indicating that they do not capture essential local relevance matching signals. We describe a novel deep Recurrent Neural Net-based model that we call Match-Tensor. The architecture of the Match-Tensor model simultaneously accounts for both local relevance matching and global topicality signals allowing for a rich interplay between them when computing the relevance of a document to a query. On a large held-out test set consisting of social media documents, we demonstrate not only that Match-Tensor outperforms BM25 and other classes of DNNs but also that it largely subsumes signals present in these models.", "histories": [["v1", "Thu, 26 Jan 2017 17:59:38 GMT  (293kb,D)", "http://arxiv.org/abs/1701.07795v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["aaron jaech", "hetunandan kamisetty", "eric ringger", "charlie clarke"], "accepted": false, "id": "1701.07795"}, "pdf": {"name": "1701.07795.pdf", "metadata": {"source": "META", "title": "Match-Tensor: a Deep Relevance Model for Search", "authors": ["Aaron Jaech", "Hetunandan Kamisetty", "Eric Ringger", "Charlie Clarke"], "emails": ["ajaech@uw.edu", "hetu@fb.com", "eringger@fb.com", "claclark@gmail.com"], "sections": [{"heading": null, "text": "The use of deep neural networks for ranking in search engines could eliminate the need for extensive feature engineering, as is common for current learning-to-rank methods. However, we show that combining simple relevance matching features such as BM25 with existing deep neural net models often significantly improves the accuracy of these models, indicating that they do not capture essential local relevance matching signals. We describe a novel, deep recursive neural net-based model, which we call the match tensor. At the same time, the architecture of the match tensor model takes into account both local relevance matching and global timeliness signals, which allow for rich interaction between them when calculating the relevance of a document for a query. On an extensive test set consisting of social media documents, we not only show that match tensor exceeds BM25 and other classes of DNNs, but also that it subsummarizes the signals that are widely present in these models."}, {"heading": "1 INTRODUCTION", "text": "In fact, the approach requires features that capture the presence of source code in a particular document, such as the proximity and location of these terms in the document, the quality of the document and many other important aspects of the source code. In many cases, the approach requires that the source code be included in that document."}, {"heading": "2 BACKGROUND", "text": "Like most learning methods, we want to learn a function that addresses the question of how to translate the relevance of a document into quantum theory. In the traditional way of working, we are dealing with a series of hand-picked characteristics that capture various aspects of relevance and relevance in a single model. The characteristics that come into play in this approach are so simple that they require the presence of a particular work in quantum theory."}, {"heading": "3 MATCH-TENSOR ARCHITECTURE", "text": "The central thesis of the match tensor architecture is that it is crucial to integrate multiple concepts of similarity and capture both immediate and larger contexts in a particular document when calculating the relevance of this document to a query. This goal is achieved by a three-dimensional tensor in which one dimension corresponds to the terms in the query, one dimension for the terms in the document, and a third dimension for different match channels. Each match channel contains a unique estimate of the match between query and document, hence the name \"match tensor.\" The tensor is calculated from the output of a neural network that works on word embedding and is supplemented by a precisely matching channel that works directly on the tokens; a downstream neural network is then used to determine the relevance of the document to the query using the tensor. The entire network is trained end-to-end with a discriminatory target."}, {"heading": "3.1 Input to the Match-Tensor Layer", "text": "First, a word embedding lookup layer converts query and document terms into separate sequences of word embeds.The word embedding table itself is computed offline from a large body of social media documents using the word2vec package [30] in an unattended manner and fixed during the training of the match tensor network.The implementation of word embedsare 256-dimensional vectors of floating point numbers.The word embedsare then passed through a linear projection layer into a reduced l-dimensional space (here l = 40); the same linear projection matrix is applied to both the query and the document word vectors.This linear projection allows you to vary the size of the embedding and to tune it as hyperparameters without moving the embedding from ground to ground."}, {"heading": "3.2 Match-Tensor Layer", "text": "For the words in the query and n words in the document, the actual match tensor - from which the architecture inherited its name - is a m \u00b7 n \u00b7 k + 1 tensor, where k + 1 is the number of channels in the match tensor (specifically 51 in the detailed figure) Each of the k + 1 channels is calculated from a unique representation of the query and the document: all but one of the channels are calculated using the elemental product of the corresponding bi-LSTM states of the query and the document (after applying the subsequent projection).Including each dimension as a separate level, rather than collapsing it into a single level, allows the model to include state-specific (approximately: term-specific) signals in the matching process and to weight different terms according to their importance. While this approach captures most key signals, an omission of the first k layers is their inability to match the vocabulary vocabulary vocabulary problem with the one that matches or the toexaggerated word in the application."}, {"heading": "3.3 From Match-Tensor to Score", "text": "The secondary neural network begins with the match tensor and applies a revolutionary layer: the match tensor is intertwined with three sets of filters over the full depth (k + 1) of the tensor, each of which has a width of three query words and a height of three, four or five document words. As shown in Fig. 1, these 3-D convolution filters allow the model to learn interactions between the representations in a way that would be very difficult to predict as a feature engineering, which gives expressive power to the model architecture. [18] Finally, the model applies a rectified linear unit (ReLU) function to the output of these coils and then folds it with a series of 1 \u00d7 1 filters. The ReLU activation function was chosen because it has proven effective for revolutionary neural networks in computer vision [18]. Finally, the model applies 2-D max pooling to transform the tips of the LU into a fixed size."}, {"heading": "4 ADDITIONAL RELATED WORK", "text": "The paper presented in this paper follows a rich and rapidly growing work using Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44]. Our work comes closest to the so-called match-pyramid models developed by Pang et al. [35, 36]: match-pyramid models construct a single match matrix and then use a revolutionary network (hence \"pyramid\") to calculate a relevance value. In contrast, the match-tensor architecture developed in this paper looks at several channels simultaneously during the matching process, allowing a rich interaction between the different channels in determining the relevance of a document to a query. Match-pyramid models are unable to distinguish between different words with the same matching pattern. Guo et al et al. [14] developed a neural network-based model (DRMM) that uses matching histogrammes and termating."}, {"heading": "5 METHODOLOGY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Data", "text": "We base our experiments on a collection of approximately 1.6 million (query, document, label) triplets collected on a large social media site between 2016-01-01 and 2016-06-01. Each document is a publicly accessible social media post that may include text, videos, photos, and links, but for the experiments reported in this post, we only look at the text. Labels specify the document's relevance level in relation to the query. For these experiments, we use three relevance levels: \"VITAL,\" \"RELEVANT,\" and \"NONRELEVANT.\" We split this data set (by query) into three parts: turn, validation, and test, so that each query string appears in exactly one of the partitions. Details on partitioning are provided in Table 1. The training and validation sets were used to train the models and perform hyperparameter sweeps. The test set was used only to evaluate at the end of this process."}, {"heading": "5.2 Implementation Details", "text": "We implemented our match-tensor model for neural networks using TensorFlow [1]. We used pre-trained 256-dimensional phrase embedding using the word2vec package [29] on a large body of documents with a vocabulary size of about 2 million tokens that contain unique and selected bigrams. Failed words are mapped to a special token. Queries are shortened to a maximum of eight words in length, while documents are shortened to a maximum length of 200 words. Both the query and the documents are then pre-processed by lowercasing and using a simple token to divide words and remove punctuations. As social media documents are structured into separate fields (e.g. title, author, and body), we added special tokens to delimit the boundaries between fields and for the beginning and end of a document."}, {"heading": "5.3 Semantic Similarity Model(SSM)", "text": "We constructed a model using the Siamese network architecture on the basis of the semantic similarity models (SSM) emerging in other work [17, 34, 41]. In this SSM model, which is detailed in Figure 5, we construct a query embedding by linking the latest results from each of the forward and backward directions of the query Bi-LSTM and a document embedded throughout the document by max pooling across the output Bi-LSTM states. Max pooling is used for the document because the documents can be much longer than the query and it is more difficult for the Bi-LSTM to disseminate the relevant information until the end of the document [22]. These fixed document and query embedding are then performed before calculating a point product between them linear projections, which are then used to calculate the end result. The model parameters and hyper parameters were optimized on the basis of the data sets as the data sets."}, {"heading": "5.4 Match-Tensor(Exact-only)+SSM", "text": "In recent work published during the preparation of this manuscript, Mitra et al. [32] show that a combination of local and distributed matching can outperform other models for web search. Since several details of their model are specific to the structure of web documents, we have constructed a model that has similar properties for our settings by combining a single match-tensor component with an SSM component in a single model that exactly matches the hidden layer of the SSM comparison network, as shown in Figure 6. The match-tensor and SSM components share parameters for word embedding and the LSTM part of the model."}, {"heading": "5.5 Match-Tensor+SSM", "text": "We also compare the effects of using all channels in the match tensor architecture in conjunction with the SSM architecture. This model is shown in Figure 6. The only architectural difference between this model and the previous (exact match tensor model) is the number of channels in the tensor layer: the former has one channel, while this model has k + 1 like the match tensor model."}, {"heading": "5.6 bi-LSTMs vs CNNs", "text": "We compared all three listed model architectures with similar ones that use wave layers instead of Bi-LSTMs. We used a mixture of wave filters of width 1 and width 3. Compared to the Bi-LSTMs, which can contain information about a large token span, the representations of the wave filters consider only trigrams (if the width is 3), but are mathematically cheaper."}, {"heading": "5.7 Attention Pooling", "text": "In order to improve the query-agnostic pooling schemes of the SSM, we also implemented an attention pooling mechanism for document embedding as an alternative to max pooling. The hypothesis here is that information from the query is important for the summary of the document. The attention pooling model learns a ReLU-activated transformation from the query embedding and any output from the document-bi-LSTM. Attention weights are determined by taking the dot product between these vectors and normalizing it using the Softmax function. The weighted embedding of the document is the combination of biLSTM outputs. We note that our attention usage is not different from that of Palangi et al. [45], where attention-based pooling was used in a query-agnostic way. In our experiments based on pooling, we did not follow the pooling results to improve it."}, {"heading": "5.8 Ensemble models", "text": "Comparing different model architectures with absolute metrics can reveal the relative importance of signal types for the task at hand. However, one model can surpass another model without capturing the signal in the latter model. So, to test whether one model subsumes another, we train additional ensemble models that use the scores of both models. In addition to the individual models, we measure the accuracy of the ensemble models."}, {"heading": "6 RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Model selection", "text": "We optimized hyperparameters based on a random grid search based on the validation set for each model architecture studied and selected the model with the best score out of 200 runs. Next, we evaluated the best model of the test set for each model architecture. Table 2 shows these hyperparameters for the three main model architectures studied. Critically, the final match-tensor model has fewer parameters than the final SSM model."}, {"heading": "6.2 Sensitivity to Training Size", "text": "To assess the sensitivity of model performance to the amount of training data, we scanned the training set for each of the NN architectures, retrained the models (while maintaining the hyperparameters), and calculated the test loss. Figure 3 shows the test loss of each model depending on its final accuracy. Unsurprisingly, each architecture we are looking at benefits from the availability of large training sets, and the accuracy improves considerably as we increase the size of the training set. However, the relative comparisons between the model architectures seem to be reasonably robust in terms of the size of training data."}, {"heading": "6.3 Performance of Neural Models", "text": "Figure 4 summarizes the performance of the various neural model architectures compared to a BM25 baseline. Overall, the match-tensor model (with bi-LSTMs) is the most accurate single model, with an 11% improvement in the AUC curve (right panel of the figure) over a BM25 baseline and smaller but consistent improvements over NDCG and ERR. We note that while the relative arrangement of the models appears to be robust against deviations in the test set, the values of these relative improvements are sensitive to the composition of the test set: relative improvements when limited to a subset of the test set that are \"hard\" (at most half of the available results are relevant), the SSM architecture exhibited lower NDCG than the BM25 baseline, and this result is consistent with [14] and the models that are more accurate than any other set of tasks."}, {"heading": "6.4 bi-LSTMs vs CNNs for text representation", "text": "Table 3 shows that the use of Bi-LSTMs in the four model architectures considered leads to more accurate models than their CNN counterparts in terms of AUC, NDCG, and ERR. Especially for AUC, the relative advantage in using Bi-LSTMs is between two and three percent. The fact that this increase applies to both SSM and match-tensor architectural variants suggests that the improvements are due to the fact that Bi-LSTMs - across the board - provide more accurate representations at each position, which is not surprising given similar results in other language modeling tasks, and is consistent with the gains in NDCG observed in [34] in the transition from revolutionary to Bi-LSTM-based semantic similarity models."}, {"heading": "6.5 2-D Matching vs. SSM", "text": "As we have seen, the match-tensor architecture often significantly exceeds the SSM architecture. Although both architectures are the most accurate when using Bi-LSTMs, the improvement in the migration from SSM to match tensors is significant, even when using CNNs to represent the state with each query and each document token: AUC increases by 4% when using Bi-LSTMs and by 3% when using CNNs, suggesting that the improvement is a consequence of the fundamental difference in architectures. It is not surprising that the superiority of match-tensor architecture has a much more expressive matching function, and in addition, the combination of the Bi-LSTM architecture at each position does not result in a significant performance gain: as can be seen, small improvements in AUC are offset by a small reduction in NDCG and ERR. The absence of a difference in this hybrid architecture would indicate that each additional BSTM-expands the global context sufficiently."}, {"heading": "6.6 Influence of the exact-match channel", "text": "While we introduced the Exact Match channel to account for incomplete tokens where the Bi-LSTM states may not be correct, it is calculated for all cases. Unsurprisingly, it makes an important contribution to the accuracy of the final model. However, the interaction between all channels further improves the accuracy of the model: the relative NDCG at 1 increases by 2% for the Bi-LSTM channels compared to the Exact Match model alone, where the relative improvement is about 1%, or about half. This approximate doubling of the relative accuracy at the transition from single channel to full match tensor model can be observed across all positions in the NDCG and ERR."}, {"heading": "6.7 Ensemble models", "text": "To determine whether a profound relevance model actually captures all the essential relevance match signals, we trained ensemble models: reinforced trees [9], which combine the output of the neural model as a trait and a BM25 trait using a 5x cross-validation on the existing validation set as inputs. Neural models, which better capture essential relevance match signals, should show relatively little improvement when adding BM25 to the mix, compared to those that do not, as a good model should already capture most of the BM25 signal. As shown in Table 4, match tensor shows the least relative increase when adding BM25 to the mix, compared to all other alternatives. An exact match tensor + SSM model performs better in this respect than SSM alone, although again the full match tensor model is much better, by allowing the interaction between the channels to match only the BM25, even without explicit signals."}, {"heading": "6.8 Model Introspection", "text": "We illustrate the strengths of the match tensor model over other approaches with a few examples in Table 5. SSM focuses on matching representations. As a result, it often overlooks relevant match signals by finding a result on the same large topic, but is different in some crucial details: As an example of a query on a celebrity TV show, the model ranks a document on another celebrity's TV show before a relevant result. Under its term model, BM25 often achieves results that are completely incorrect, but the right set of tokens over a relevant result. This is best illustrated by the \"Low Fat High Carb\" query, in which the model prefers a result on \"Low Carb High Fat\" to a relevant result. Traditional learning methods rank this problem with specifically constructed proximity and order functions. Match tensor, on the other hand, correctly grades these results by looking for the required proximity, order, this particular document, and other relationships (although often only the exact match)."}, {"heading": "7 CONCLUDING DISCUSSION", "text": "While initial developments in several areas focused on absolute accuracy [21, 42] of these models when compared with alternatives, more recently the focus has shifted to the completeness of these models; in fact, in several areas such as speech recognition, computer vision, and machine translation, entire production systems have been completely replaced by neural networks trained end-to-end [15, 43]. Early neural network models for search focused on semantic matching signals that supplemented existing relevance matching functions. By calculating similarities between semantic representations of the query and documents, signals that were difficult to determine with traditional models were detected. However, this general class of models seems to miss critical relevance matching signals [14]."}, {"heading": "APPENDIX: ALTERNATIVE MODEL ARCHITECTURES WITH HYPERPARAMETERS", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "TensorFlow: Large-Scale Machine", "author": ["Mart\u0301\u0131n Abadi"], "venue": "Learning on Heterogeneous Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "From RankNet to LambdaRank to LambdaMART: An overview", "author": ["Christopher J C Burges"], "venue": "Learning", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Learning to rank: From pairwise approach to listwise approach", "author": ["Zhe Cao", "Tao Qin", "Tie-Yan Liu", "Ming-Feng Tsai", "Hang Li"], "venue": "In 24th International Conference on Machine learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Expected reciprocal rank for graded relevance", "author": ["Olivier Chapelle", "Donald Metlzer", "Ya Zhang", "Pierre Grinspan"], "venue": "In 18th ACM Conference on Information and Knowledge Management", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Adaptability of neural networks on varying granularity IR tasks", "author": ["Daniel Cohen", "Qingyao Ai", "W Bruce Croft"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Query expansion with locally-trained word embeddings", "author": ["Fernando Diaz", "Bhaskar Mitra", "Nick Craswell"], "venue": "arXiv preprint arXiv:1605.07891", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Greedy function approximation: A gradient boosting machine", "author": ["Jerome H Friedman"], "venue": "Annals of statistics", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Word embedding based generalized language model for information retrieval", "author": ["Debasis Ganguly", "Dwaipayan Roy", "Mandar Mitra", "Gareth JF Jones"], "venue": "In 38th International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks 18,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Search retargeting using directed query embeddings", "author": ["Mihajlo Grbovic", "Nemanja Djuric", "Vladan Radosavljevic", "Narayan Bhamidipati"], "venue": "In 24th International Conference on the World Wide Web", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Context-and contentaware embeddings for query rewriting in sponsored search", "author": ["Mihajlo Grbovic", "Nemanja Djuric", "Vladan Radosavljevic", "Fabrizio Silvestri", "Narayan Bhamidipati"], "venue": "In 38th International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "A deep relevance matching model for ad-hoc retrieval", "author": ["Jiafeng Guo", "Yixing Fan", "Qingyao Ai", "W Bruce Croft"], "venue": "In 25th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In 22nd ACM International Conference on Conference on Information & Knowledge Management", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Short text similarity with word embeddings", "author": ["Tom Kenter", "Maarten de Rijke"], "venue": "In 24th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Recurrent Convolutional Neural Networks for Text Classification", "author": ["Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao"], "venue": "In 29th AAAI Conference on Artificial Intelligence", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks 3361,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1995}, {"title": "Learning to rank for information retrieval", "author": ["Tie-Yan Liu"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "A deep architecture for matching short texts", "author": ["Zhengdong Lu", "Hang Li"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "The whens and hows of learning to rank for web search", "author": ["Craig Macdonald", "Rodrygo L. Santos", "Iadh Ounis"], "venue": "Information Retrieval 16,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "A markov random field model for term dependencies", "author": ["Donald Metzler", "W. Bruce Croft"], "venue": "In 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and Their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In 26th International Conference on Neural Information Processing Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Exploring session context using distributed representations of queries and reformulations", "author": ["Bhaskar Mitra"], "venue": "In 38th International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Learning to match using local and distributed representations of text for web search", "author": ["Bhaskar Mitra", "Fernando Diaz", "Nick Craswell"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Improving document ranking with dual word embeddings", "author": ["Eric Nalisnick", "Bhaskar Mitra", "Nick Craswell", "Rich Caruana"], "venue": "In 25th International Conference Companion on the World Wide Web. International World Wide Web Conferences Steering Committee,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Semantic Modelling with Long-Short-Term Memory for Information Retrieval", "author": ["Hamid Palangi", "Li Deng", "Yelong Shen", "Jianfeng Gao", "Xiaodong He", "Jianshu Chen", "Xinying Song", "Rabab K. Ward"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "A study of matchpyramid models on ad-hoc retrieval", "author": ["Liang Pang", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Xueqi Cheng"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "Text Matching as Image Recognition", "author": ["Liang Pang", "Yanyan Lan", "Jiafeng Guo", "Jun Xu", "Shengxian Wan", "Xueqi Cheng"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval", "author": ["Stephen E Robertson", "Steve Walker"], "venue": "In 17th Annual International ACM SIGIR Conference on Research and development in Information Retrieval", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1994}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "Andrew W Senior", "Fran\u00e7oise Beaufays"], "venue": "In Interspeech", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Gerard Salton", "Christopher Buckley"], "venue": "Information processing & management 24,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1988}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil"], "venue": "In 23rd ACM International Conference on Conference on Information and Knowledge Management", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Learning semantic representations using convolutional neural networks for web search", "author": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil"], "venue": "In 23rd International Conference on the World Wide Web", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "aNMM: Ranking short answer texts with attention-based neural matching model", "author": ["Liu Yang", "Qingyao Ai", "Jiafeng Guo", "W Bruce Croft"], "venue": "In 25th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Deepintent: Learning attentions for online advertising with recurrent neural networks", "author": ["Shuangfei Zhai", "Keng-hao Chang", "Ruofei Zhang", "Zhongfei Mark Zhang"], "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": "The learning-to-rank models in widespread use for search may require hundreds of such features to be generated for each query/document pair [27].", "startOffset": 140, "endOffset": 144}, {"referenceID": 32, "context": "In many rankers, the feature set includes classic information retrieval models, such as BM25 [37], and term dependence models [28].", "startOffset": 93, "endOffset": 97}, {"referenceID": 23, "context": "In many rankers, the feature set includes classic information retrieval models, such as BM25 [37], and term dependence models [28].", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "Motivated by success in machine translation and other related NLP tasks, DNNs are now being applied to ranking in search [17, 19, 31, 33, 40, 41, 44] .", "startOffset": 121, "endOffset": 149}, {"referenceID": 15, "context": "Motivated by success in machine translation and other related NLP tasks, DNNs are now being applied to ranking in search [17, 19, 31, 33, 40, 41, 44] .", "startOffset": 121, "endOffset": 149}, {"referenceID": 26, "context": "Motivated by success in machine translation and other related NLP tasks, DNNs are now being applied to ranking in search [17, 19, 31, 33, 40, 41, 44] .", "startOffset": 121, "endOffset": 149}, {"referenceID": 28, "context": "Motivated by success in machine translation and other related NLP tasks, DNNs are now being applied to ranking in search [17, 19, 31, 33, 40, 41, 44] .", "startOffset": 121, "endOffset": 149}, {"referenceID": 35, "context": "Motivated by success in machine translation and other related NLP tasks, DNNs are now being applied to ranking in search [17, 19, 31, 33, 40, 41, 44] .", "startOffset": 121, "endOffset": 149}, {"referenceID": 36, "context": "Motivated by success in machine translation and other related NLP tasks, DNNs are now being applied to ranking in search [17, 19, 31, 33, 40, 41, 44] .", "startOffset": 121, "endOffset": 149}, {"referenceID": 38, "context": "Motivated by success in machine translation and other related NLP tasks, DNNs are now being applied to ranking in search [17, 19, 31, 33, 40, 41, 44] .", "startOffset": 121, "endOffset": 149}, {"referenceID": 12, "context": "vance, they can also fail to capture the most salient aspect of the search task, namely local relevance matching [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 30, "context": "which are used as features in traditional learning-to-rank approaches [35, 36].", "startOffset": 70, "endOffset": 78}, {"referenceID": 31, "context": "which are used as features in traditional learning-to-rank approaches [35, 36].", "startOffset": 70, "endOffset": 78}, {"referenceID": 32, "context": "We also demonstrate that this approach largely subsumes other models such as BM25[37] and SSM[17, 41]: the MatchTensor model alone performs nearly as well as meta-models trained with both Match-Tensor and these features as input.", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "We also demonstrate that this approach largely subsumes other models such as BM25[37] and SSM[17, 41]: the MatchTensor model alone performs nearly as well as meta-models trained with both Match-Tensor and these features as input.", "startOffset": 93, "endOffset": 101}, {"referenceID": 36, "context": "We also demonstrate that this approach largely subsumes other models such as BM25[37] and SSM[17, 41]: the MatchTensor model alone performs nearly as well as meta-models trained with both Match-Tensor and these features as input.", "startOffset": 93, "endOffset": 101}, {"referenceID": 2, "context": "using a learning-to-rank approach [3, 4] to predict the labels on training data:", "startOffset": 34, "endOffset": 40}, {"referenceID": 3, "context": "using a learning-to-rank approach [3, 4] to predict the labels on training data:", "startOffset": 34, "endOffset": 40}, {"referenceID": 32, "context": "Furthermore, it is standard to include classic information retrieval models in this feature set, particularly BM25 [37].", "startOffset": 115, "endOffset": 119}, {"referenceID": 20, "context": "Liu [25] provides a thorough overview of traditional learningto-rank methods for search.", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "[27] cover many of the engineering issues associated with deploying learning-to-rank in a search engine.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] introduced the first Deep Neural Network architectures for Web search that operated on (query, title) pairs, using a so-called siamese architecture [23], in which two feed-forward networks NNQ and NND map the query q and the title of a given web document d, respectively, into fixed-length representations:", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[17] introduced the first Deep Neural Network architectures for Web search that operated on (query, title) pairs, using a so-called siamese architecture [23], in which two feed-forward networks NNQ and NND map the query q and the title of a given web document d, respectively, into fixed-length representations:", "startOffset": 153, "endOffset": 157}, {"referenceID": 36, "context": "[41] marks the next notable advancement using the same siamese architecture.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "convolutional networks, more recent approaches have used Recurrent Neural Networks (RNNs), especially those based on Long Short-term Memory (LSTM) units [16].", "startOffset": 153, "endOffset": 157}, {"referenceID": 37, "context": "A popular architecture for machine translation uses the so-called sequence-to-sequence paradigm in which the input text in the source language is \u201cencoded\u201d using an encoder network to produce a fixed-length representation (the RNN state)[42].", "startOffset": 237, "endOffset": 241}, {"referenceID": 14, "context": "[17] and Shen et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[41], the use of RNNs such as those based on LSTMs is critical to their performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": ", focusing attention) on various elements of the source representation during the decoding process, and they have demonstrated considerable improvements over their non-attention counterparts [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 12, "context": "The \u201crepresentation-based\u201c nature of siamese architectures has also been identified as a limitation in search [14] and has led to the development of alternate \u201cinteraction-based\u201d architectures, in which the relationships between query and", "startOffset": 110, "endOffset": 114}, {"referenceID": 31, "context": "[36] construct an interaction matrix between query and document terms, where each entry in the matrix denotes the strength of similarity between the corresponding terms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[32], that appeared while this manuscript was being prepared, uses a \u201cduet\u201d architecture in which two separate networks (one", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "that are not [39].", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "The word embedding table is itself computed offline from a large corpus of social media documents using the word2vec package [30] in an unsupervised manner and are held fixed during the training of the Match-Tensor network.", "startOffset": 125, "endOffset": 129}, {"referenceID": 9, "context": "e, bi-LSTMs) [11, 16], then encode the query (respectively document) word-embedding sequence into a sequence of LSTM states.", "startOffset": 13, "endOffset": 21}, {"referenceID": 13, "context": "e, bi-LSTMs) [11, 16], then encode the query (respectively document) word-embedding sequence into a sequence of LSTM states.", "startOffset": 13, "endOffset": 21}, {"referenceID": 33, "context": "[38], but the best models did", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The work presented in this paper follows a rich and rapidly growing body of work that use Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44].", "startOffset": 121, "endOffset": 152}, {"referenceID": 15, "context": "The work presented in this paper follows a rich and rapidly growing body of work that use Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44].", "startOffset": 121, "endOffset": 152}, {"referenceID": 26, "context": "The work presented in this paper follows a rich and rapidly growing body of work that use Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44].", "startOffset": 121, "endOffset": 152}, {"referenceID": 28, "context": "The work presented in this paper follows a rich and rapidly growing body of work that use Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44].", "startOffset": 121, "endOffset": 152}, {"referenceID": 35, "context": "The work presented in this paper follows a rich and rapidly growing body of work that use Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44].", "startOffset": 121, "endOffset": 152}, {"referenceID": 36, "context": "The work presented in this paper follows a rich and rapidly growing body of work that use Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44].", "startOffset": 121, "endOffset": 152}, {"referenceID": 38, "context": "The work presented in this paper follows a rich and rapidly growing body of work that use Deep Neural Networks in Search [7, 17, 19, 31, 33, 40, 41, 44].", "startOffset": 121, "endOffset": 152}, {"referenceID": 30, "context": "[35, 36]: Match", "startOffset": 0, "endOffset": 8}, {"referenceID": 31, "context": "[35, 36]: Match", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[14] developed", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "al [32] report that a model incorporating an exact-match channel with a representation based \u201cdistributed\u201d model outperforms DRMM on a larger collection of websearch queries.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "[8] who use them in query expansions, Ganguly et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] who use them in smoothing language models, Nalisnick et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[33] who propose dual embeddings and Grbovic et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12, 13] who use them in sponsored search.", "startOffset": 0, "endOffset": 8}, {"referenceID": 11, "context": "[12, 13] who use them in sponsored search.", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "[6] have also studied the utility of DNNs for several IR tasks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "TensorFlow [1].", "startOffset": 11, "endOffset": 14}, {"referenceID": 24, "context": "We used pre-trained 256-dimensional phrase embeddings using the word2vec package [29] on a large corpus of documents with a vocabulary size of around 2 million tokens containing unigrams and selected bigrams.", "startOffset": 81, "endOffset": 85}, {"referenceID": 16, "context": "We employed the Adam optimizer for gradient descent [20] with a learning rate of 0.", "startOffset": 52, "endOffset": 56}, {"referenceID": 21, "context": "While some architectures have been suggested for short text common in some social media[26, 36], studies indicate that they do not beat baseline models such as BM25[35].", "startOffset": 87, "endOffset": 95}, {"referenceID": 31, "context": "While some architectures have been suggested for short text common in some social media[26, 36], studies indicate that they do not beat baseline models such as BM25[35].", "startOffset": 87, "endOffset": 95}, {"referenceID": 30, "context": "While some architectures have been suggested for short text common in some social media[26, 36], studies indicate that they do not beat baseline models such as BM25[35].", "startOffset": 164, "endOffset": 168}, {"referenceID": 14, "context": "In contrast, both early models [17, 41] and recent developments by Mitra et.", "startOffset": 31, "endOffset": 39}, {"referenceID": 36, "context": "In contrast, both early models [17, 41] and recent developments by Mitra et.", "startOffset": 31, "endOffset": 39}, {"referenceID": 26, "context": "al that has shown strong performance [31] have been designed for web-search and are not directly usable in our setting.", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "We constructed an model using the siamese network architecture based on the semantic similarity models (SSM) appearing in other work [17, 34, 41].", "startOffset": 133, "endOffset": 145}, {"referenceID": 29, "context": "We constructed an model using the siamese network architecture based on the semantic similarity models (SSM) appearing in other work [17, 34, 41].", "startOffset": 133, "endOffset": 145}, {"referenceID": 36, "context": "We constructed an model using the siamese network architecture based on the semantic similarity models (SSM) appearing in other work [17, 34, 41].", "startOffset": 133, "endOffset": 145}, {"referenceID": 18, "context": "mation all the way to the end of the document [22].", "startOffset": 46, "endOffset": 50}, {"referenceID": 27, "context": "prepared, Mitra et al [32] show that a combination of local and distributed matching can outperform other models for web-search.", "startOffset": 22, "endOffset": 26}, {"referenceID": 39, "context": "[45] where attention-based pooling", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The figure reports NDCG at various levels as well as Expected Reciprocal Rank (ERR) [5], with all measures computed using the three relevance grades.", "startOffset": 84, "endOffset": 87}, {"referenceID": 12, "context": "This result is consistent with [14] and others who have highlighted the limitation of models that only match semantic representations in relevance-matching tasks.", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "This outcome is not surprising given similar results in other language modeling tasks and is consistent with gains in NDCG observed in [34] in going from convolutional to bi-LSTM-based semantic similarity models.", "startOffset": 135, "endOffset": 139}, {"referenceID": 7, "context": "To determine if a deep relevance model is indeed capturing all essential relevance matching signals, we trained ensemble models: boosted trees [9] that combine as inputs the neural model\u2019s output as a feature and a BM25 feature using 5fold cross-validation on the existing validation set.", "startOffset": 143, "endOffset": 146}, {"referenceID": 17, "context": "ments in several domains were focused on the absolute accuracy [21, 42] of these models when compared to alternatives, the focus has more recently gravitated towards the completeness of these models; indeed in several domains such as speech", "startOffset": 63, "endOffset": 71}, {"referenceID": 37, "context": "ments in several domains were focused on the absolute accuracy [21, 42] of these models when compared to alternatives, the focus has more recently gravitated towards the completeness of these models; indeed in several domains such as speech", "startOffset": 63, "endOffset": 71}, {"referenceID": 12, "context": "However, this general class of models appears to miss critical relevance-matching signals [14].", "startOffset": 90, "endOffset": 94}], "year": 2017, "abstractText": "The application of Deep Neural Networks for ranking in search engines may obviate the need for the extensive feature engineering common to current learning-to-rank methods. However, we show that combining simple relevance matching features like BM25 with existing Deep Neural Net models often substantially improves the accuracy of these models, indicating that they do not capture essential local relevance matching signals. We describe a novel deep Recurrent Neural Net-based model that we call Match-Tensor. The architecture of the Match-Tensor model simultaneously accounts for both local relevance matching and global topicality signals allowing for a rich interplay between them when computing the relevance of a document to a query. On a large held-out test set consisting of social media documents, we demonstrate not only that Match-Tensor outperforms BM25 and other classes of DNNs but also that it largely subsumes signals present in these models.", "creator": "LaTeX with hyperref package"}}}