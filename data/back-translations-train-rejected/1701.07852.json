{"id": "1701.07852", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2017", "title": "An Empirical Analysis of Feature Engineering for Predictive Modeling", "abstract": "Machine learning models, such as neural networks, decision trees, random forests and gradient boosting machines accept a feature vector and provide a prediction. These models learn in a supervised fashion where a set of feature vectors with expected output is provided. It is very common practice to engineer new features from the provided feature set. Such engineered features will either augment, or replace portions of the existing feature vector. These engineered features are essentially calculated fields, based on the values of the other features.", "histories": [["v1", "Thu, 26 Jan 2017 19:29:41 GMT  (32kb,D)", "http://arxiv.org/abs/1701.07852v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jeff heaton"], "accepted": false, "id": "1701.07852"}, "pdf": {"name": "1701.07852.pdf", "metadata": {"source": "CRF", "title": "An Empirical Analysis of Feature Engineering for Predictive Modeling", "authors": ["Jeff Heaton"], "emails": ["jeffheaton@acm.org"], "sections": [{"heading": null, "text": "This paper reports on empirical research to show which types of technical characteristics are best suited to the type of machine learning model, and this is achieved by generating multiple sets of data designed to benefit from a particular type of technical characteristics.The experiment shows the extent to which the machine learning model is able to synthesize the required trait on its own. If a model is capable of synthesizing a technical trait, it is not necessary to provide this trait. Research has shown that the models studied actually perform differently with different types of technical traits. INTRODUCTION Feature Engineering is an important but labor-intensive component of machine learning applications [1]. Most machine-feeding services depend heavily on the representation of the trait vector. As a result, much of the actual effort involved in the use of machine learning algorithms is in the construction of pipelines and the data augmentation by automation [1]."}, {"heading": "II. BACKGROUND AND PRIOR WORK", "text": "The groundbreaking work of George Box and David Cox in 1964 introduced a method of determining which of several power functions could be a useful transformation for the result of linear regression. [8] This transformation became known as the Box-Cox Transformation. However, unlike the Box-Cox Transformation, ACE is able to guarantee optimal transformations for linear regression by applying a mathematical function to each component of the feature vector and result. However, unlike the Box-Cox Transformation, ACE is able to guarantee optimal transformations for linear regression. linear regression is not the only machine learning model to benefit from feature engineering and other transformations. In 1999, it was shown that feature engineering can improve the performance of rule learning for text classification [10]."}, {"heading": "III. EXPERIMENT DESIGN AND METHODOLOGY", "text": "Different types of machine learning have different degrees of mathematical ability. If the model can learn to synthesize a technical trait independently, there was no reason to develop the trait at all. This empirical study determines which type of technical trait performs best with which machine learning model. To achieve this, ten sets of data were generated, each containing the inputs and outputs corresponding to a certain type of technical trait. If the machine learning model can learn to reproduce this trait with a small error, it means that this particular model could have learned this constructed trait without help. The following regression models were investigated in this experiment.ar Xiv: 170 1.07 852v 1 [cs.L G] January 26, 2017 \u2022 Deep neural networks (THEN) \u2022 Gradient-enhanced machines (GBM) \u2022 Random forests \u2022 Support of vector machines for regression (SVR) To replicate the learning model of some of these packet-milder-based SVR results was performed."}, {"heading": "A. Logarithms and Power Functions", "text": "Logarithms and potential functions have long been used to transform input into linear regression [8]. The usefulness of these functions for transformation has been demonstrated for other model types, such as neural networks [18]. The logarithms and potential transformations used in this paper are from the random x-values scanned uniformly in the real number range in Equations 1,2 and 3.y = log (x) (1) y = x2 (2) y = x 1 2 (3) In this paper, the natural log functions, the second power, and the square root are examined. For both the logarithms and the root, random x-values have been scanned uniformly in the real number range [1, 100]. For the second power transformation, the x-values have been scanned uniformly in the real number range [1, 10]. The partial dataset from the resulting logarithm transformations (equation 1) is used in Table Ix1, a single observation becomes one."}, {"heading": "B. Differences and Ratios", "text": "To evaluate this characteristic type, a dataset with two x observations is generated, which are uniformly scanned in the range of the real numbers [0, 1], and a single y prediction is generated, which is either the difference or the ratio of the two observations. In scanning uniform real numbers for the denominator, the range [0,1, 1] is used to avoid a division by zero. Differences and ratio transformations are represented by equations 4 and 5, y = x1 \u2212 x2 (4) y = x1 x2 (5) Several rows demonstrating the difference transformation (Eq.4) are shown in Table II. The x2 value is simply subtracted from the x1 value, which leads to y1."}, {"heading": "C. Counts", "text": "The count engineered feature counts the number of elements in the feature vector that meet a certain condition. y = n \u2211 i = 1 if xi > t else 0 (6) The x vector represents the input vector of length n. The resulting y contains an integer that corresponds to the number of x values that were above the threshold (t). To generate a count dataset, the resulting y number was uniformly sampled from integers in the range [1, 50] and generated corresponding input vectors. This process is performed by algorithm 1.Several sample lines of count of the input vector are shown in Table III. The y1 value simply contains the number of features x1 to x50 that contain a value greater than 0.1."}, {"heading": "D. Polynomials", "text": "This paper examined the ability of machine learning models to synthesize traits that follow the polynomial of the equation 7.y = 1 + 5x + 8x2 (7). An equation such as this shows the ability of models to synthesize traits that contain multiple multiplications and one exponent.The dataset was generated by evenly scanning x from real numbers in the range [0, 2).Example observations from this dataset are in Table IV. The y1 value is simply calculated on the basis of x1 as input in Equation 7."}, {"heading": "E. Rational Differences and Polynomials", "text": "Equations 8 and 9 show the types of rational combinations of differences and polynomials tested by this paper. In order to create a dataset containing rational differences (Eq.8), four observations are sampled uniformly from real numbers of the range [1, 10]. When creating a dataset of rational polynomials, a single observation is sampled uniformly from real numbers of the range [1, 10]. Several sample observations from this training set are shown in Table VI."}, {"heading": "F. The Quadratic Equation", "text": "The final synthesized feature is based on the distance between the roots of a quadratic equation [19]. The distance between the roots of a quadratic equation can be easily calculated by taking the difference between the two outputs of the quadratic formula, as indicated in Eq.10, in its unsimplified form. y = 1 x values from the real numerical range [\u2212 10, 10]. Such a range leads to some invalid results that can be discarded. Table VII shows the occurrence of this data set."}, {"heading": "IV. RESULTS ANALYSIS", "text": "The results of the experiments carried out in this paper clearly indicate that some model types with certain classes of technical characteristics perform much better than other model types. Simple transformations involving only a single characteristic were easy to learn from all four models, including the protocol, the polynomial, the power, and the root. However, none of the models were able to successfully learn the characteristic of the ratio difference. Model-specific results of this experiment are summarized in the following sections."}, {"heading": "A. Neural Network Results", "text": "For each constructed functional experiment, a stochastic gradient drop [20] was used, which trains a deep neural network. A deep neural network contains a number of input neurons corresponding to the number of input neurons required to test this constructed functional type. Likewise, a single output neuron was used to provide the value generated by the specified constructed feature. There are 5 hidden layers, which, when viewed from the input to the output layer, contain 400, 200, 100, 50 and 25 neurons. Each hidden layer uses a rectifiable transmission function [21], whereby each hidden neuron forms a rectified linear unit (ReLU). The results of these deep neural network constructed transmission functions contain 400, 200, 100, 50, and 25 neurons. The results of these transmission variants are considered in the form of neuron networks (all mean square (MSE) values are considered acceptable for all the MSE-1 values."}, {"heading": "B. Support Vector Machine Results", "text": "The two primary hyperparameters of an SVM are C and \u03b3. It is customary to perform a grid search to find an optimal combination of C and \u03b3 [22]. The 3 C values of 0.001, 1, and 100 were tried, in combination with the 3 \u03b3 values of 0.1, 1, and 10. This resulted in 9 different SVMs for evaluation. The experiment results came from the best combination of C and \u03b3 for each function type. A third hyperparameter specifies the type of kernel used by the SVM, which in this case is a Gaussian kernel. Since support vector engines benefit from normalizing their input function vectors to a specific range [22], we normalize all SVM inputs to [0.1]. This required normalization step for the SVM adds additional calculations to the feature under investigation."}, {"heading": "C. Random Forest Results", "text": "Random forests are an ensemble model consisting of decision trees. Training data is randomly sampled to create a forest of trees that together normally exceeds the individual trees. The random forests used in this work use all 100 classification trees. This number of trees is a hyperparameter for the random forest algorithm. The result of the Random Forest Model's attempt to synthesize the constructed traits is shown in Figure 3.The Random Forest Model failed to synthesize the counting and ratio differential traits. Unsurprisingly, the Random Forest model fails to synthesize the synthetic counting function. Trees have no inherent ability to process multiple inputs at the same time except by branching. While a neural network or SVM can generate a count by a simple summation, a tree would have to produce branches for each combination of the 50 binary input variables. This would have terrible scalability and would be limited by a Catalan number."}, {"heading": "D. Gradient Boosted Machine", "text": "This additional optimization sometimes gives the GBM a performance advantage over random forests. The gradient enhancers used in this paper all used the same hyperparameters, the maximum depth was 10 levels, the number of estimators was 100 and the learning rate was 0.05. The results of the features developed by the GBM are shown in Figure 4.Like the random forest model, the gradient enhancer was unable to synthesize the counts and differential features. Although the gradient enhancer achieved a satisfactory result in terms of ratio, it performed worse than the random forest."}, {"heading": "V. CONCLUSION & FURTHER RESEARCH", "text": "Figures 1-4 clearly show that machine learning models such as neural networks, support for vector machines, random forests and gradient-enhancing machines benefit from a different set of synthetic characteristics. Neural networks and supporting vector machines generally benefit from the same types of technical characteristics that are used for a particular type of machine, as well as from the types of models that work well with each other in an ensemble. Based on the experiments conducted in this research, the type of machine learning model used has a major impact on the types of constructed characteristics that need to be taken into account. Developed characteristics are based on a ratio of differences that cannot be well synthesized in any of the models."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1828}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "International conference on artificial intelligence and statistics, 2011, pp. 215\u2013223.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Feature engineering and classifier ensemble for kdd cup 2010", "author": ["H.-F. Yu", "H.-Y. Lo", "H.-P. Hsieh", "J.-K. Lou", "T.G. McKenzie", "J.-W. Chou", "P.-H. Chung", "C.-H. Ho", "C.-F. Chang", "Y.-H. Wei"], "venue": "KDD Cup, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen"], "venue": "Nature, vol. 381, no. 6583, pp. 607\u2013609, 1996.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "An analysis of transformations", "author": ["G.E. Box", "D.R. Cox"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp. 211\u2013252, 1964.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1964}, {"title": "Estimating optimal transformations for multiple regression and correlation", "author": ["L. Breiman", "J.H. Friedman"], "venue": "Journal of the American statistical Association, vol. 80, no. 391, pp. 580\u2013598, 1985.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1985}, {"title": "Feature engineering for text classification", "author": ["S. Scott", "S. Matwin"], "venue": "ICML, vol. 99. Citeseer, 1999, pp. 379\u2013388.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Theory of pattern recognition", "author": ["V.N. Vapnik", "A.J. Chervonenkis"], "venue": "1974.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1974}, {"title": "A training algorithm for optimal margin classifiers", "author": ["B.E. Boser", "I.M. Guyon", "V.N. Vapnik"], "venue": "Proceedings of the fifth annual workshop on Computational learning theory. ACM, 1992, pp. 144\u2013152.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1992}, {"title": "A logical calculus of the ideas immanent in nervous activity", "author": ["W.S. McCulloch", "W. Pitts"], "venue": "The bulletin of mathematical biophysics, vol. 5, no. 4, pp. 115\u2013133, 1943.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1943}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine learning, vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Boosting algorithms as gradient descent in function space.", "author": ["L. Mason", "J. Baxter", "P. Bartlett", "M. Frean"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Jeff Heaton\u2019s GitHub Repository - Conference/Paper Source Code", "author": ["J Heaton"], "venue": "https://github.com/jeffheaton/papers, accessed: 2016-01-31.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic neural network modeling for univariate time series", "author": ["S.D. Balkin", "J.K. Ord"], "venue": "International Journal of Forecasting, vol. 16, no. 4, pp. 509\u2013515, 2000.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "On the distance between roots of integer polynomials", "author": ["Y. Bugeaud", "M. Mignotte"], "venue": "Proceedings of the Edinburgh Mathematical Society (Series 2), vol. 47, no. 03, pp. 553\u2013556, 2004.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Introduction to stochastic search and optimization: estimation, simulation, and control", "author": ["J.C. Spall"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, 2011, pp. 315\u2013323.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "A practical guide to support vector classification", "author": ["C.-W. Hsu", "C.-C. Chang", "C.-J. Lin"], "venue": "2003.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Support vector regression machines", "author": ["A. Smola", "V. Vapnik"], "venue": "Advances in neural information processing systems, vol. 9, pp. 155\u2013161, 1997.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Feature engineering is an important but labor-intensive component of machine learning applications [1].", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "As a result, much of the actual effort in deploying machine learning algorithms goes into the design of preprocessing pipelines and data transformations [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "To make use of feature engineering a model\u2019s feature vector is expanded by adding new features that are calculations based on the other features [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "Feature engineering was successfully applied to the winning KDD Cup 2010 competition entry [3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "Technologies such as deep learning [4] can benefit from feature engineering.", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "Most research into feature engineering in the deep learning space has been in the areas of image and speech recognition [1].", "startOffset": 120, "endOffset": 123}, {"referenceID": 4, "context": "Such techniques are successful in the high-dimension space of image processing and often amount to dimensionality reduction techniques [5] such as PCA [6] and auto-encoders [7].", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "Such techniques are successful in the high-dimension space of image processing and often amount to dimensionality reduction techniques [5] such as PCA [6] and auto-encoders [7].", "startOffset": 151, "endOffset": 154}, {"referenceID": 6, "context": "Such techniques are successful in the high-dimension space of image processing and often amount to dimensionality reduction techniques [5] such as PCA [6] and auto-encoders [7].", "startOffset": 173, "endOffset": 176}, {"referenceID": 7, "context": "The seminal work by George Box and David Cox in 1964 introduced a method for determining which of several power functions might be a useful transformation for the outcome of linear regression [8].", "startOffset": 192, "endOffset": 195}, {"referenceID": 8, "context": "The alternating conditional expectation (ACE) algorithm [9] works similarly to the Box-Cox transformation, in that a mathematical function is applied to each component of the feature vector and outcome.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "In 1999, it was demonstrated that feature engineering could enhance the performance of rules learning for text classification [10].", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "These experiments were conducted in the Python programming language, using the following third-party packages: Scikit-Learn [11], Lasange, and Nolearn.", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "Using this combination of packages, model types of support vector machine (SVM) [12][13], deep neural network [14], random forest [15], and gradient boosting machine (GBM) [16] were evaluated against the following ten selected engineered features:", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "Using this combination of packages, model types of support vector machine (SVM) [12][13], deep neural network [14], random forest [15], and gradient boosting machine (GBM) [16] were evaluated against the following ten selected engineered features:", "startOffset": 84, "endOffset": 88}, {"referenceID": 13, "context": "Using this combination of packages, model types of support vector machine (SVM) [12][13], deep neural network [14], random forest [15], and gradient boosting machine (GBM) [16] were evaluated against the following ten selected engineered features:", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "Using this combination of packages, model types of support vector machine (SVM) [12][13], deep neural network [14], random forest [15], and gradient boosting machine (GBM) [16] were evaluated against the following ten selected engineered features:", "startOffset": 130, "endOffset": 134}, {"referenceID": 15, "context": "Using this combination of packages, model types of support vector machine (SVM) [12][13], deep neural network [14], random forest [15], and gradient boosting machine (GBM) [16] were evaluated against the following ten selected engineered features:", "startOffset": 172, "endOffset": 176}, {"referenceID": 16, "context": "The Python source code for these experiments can be downloaded from the author\u2019s GitHub page [17].", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "Logarithms and power functions have long been used to transform the inputs to linear regression [8].", "startOffset": 96, "endOffset": 99}, {"referenceID": 17, "context": "The usefulness of these functions for transformation has been shown for other model types, such as neural networks [18].", "startOffset": 115, "endOffset": 119}, {"referenceID": 0, "context": "For both the log and root transform, random x values were uniformly sampled in the real number range [1, 100].", "startOffset": 101, "endOffset": 109}, {"referenceID": 0, "context": "For the second power transformation, the x values were uniformly sampled in the real number range [1, 10].", "startOffset": 98, "endOffset": 105}, {"referenceID": 9, "context": "For the second power transformation, the x values were uniformly sampled in the real number range [1, 10].", "startOffset": 98, "endOffset": 105}, {"referenceID": 0, "context": "To evaluate this feature type a dataset is generated with two x observations uniformly sampled in the real number range [0, 1], a single y prediction is also generated that is either the difference or ratio of the two observations.", "startOffset": 120, "endOffset": 126}, {"referenceID": 0, "context": "To generate a count dataset the resulting y-count was uniformly sampled from integers in the range [1, 50], and corresponding input vectors are created.", "startOffset": 99, "endOffset": 106}, {"referenceID": 0, "context": "To generate a dataset containing rational differences (Equation 8), four observations are uniformly sampled from real numbers of the range [1, 10].", "startOffset": 139, "endOffset": 146}, {"referenceID": 9, "context": "To generate a dataset containing rational differences (Equation 8), four observations are uniformly sampled from real numbers of the range [1, 10].", "startOffset": 139, "endOffset": 146}, {"referenceID": 0, "context": "Generating a dataset of rational polynomials, a single observation is uniformly sampled from real numbers of the range [1, 10].", "startOffset": 119, "endOffset": 126}, {"referenceID": 9, "context": "Generating a dataset of rational polynomials, a single observation is uniformly sampled from real numbers of the range [1, 10].", "startOffset": 119, "endOffset": 126}, {"referenceID": 18, "context": "The final synthesized feature is based on the distance between the roots of a quadratic equation [19].", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "For each engineered feature experiment a stochastic gradient descent [20] trained deep neural network was used.", "startOffset": 69, "endOffset": 73}, {"referenceID": 20, "context": "Each hidden layer makes use of a rectifier transfer function [21], making each hidden neuron a rectified linear unit (ReLU).", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "The experiments in this paper used the rectifier transfer function [21] for hidden neurons and a simple identity linear function for output neurons.", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "It is customary to perform a grid search to find an optimal combination of C and \u03b3 [22].", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "Because support vector machines benefit from their input feature vectors being normalized to a specific range [22], we normalized all SVM input to [0,1].", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "Because support vector machines benefit from their input feature vectors being normalized to a specific range [22], we normalized all SVM input to [0,1].", "startOffset": 147, "endOffset": 152}, {"referenceID": 22, "context": "Smola and Vapnik extended the original support vector machine to include regression; the resulting algorithm is called a support vector regression (SVR) [23].", "startOffset": 153, "endOffset": 157}], "year": 2017, "abstractText": "Machine learning models, such as neural networks, decision trees, random forests and gradient boosting machines accept a feature vector and provide a prediction. These models learn in a supervised fashion where a set of feature vectors with expected output is provided. It is very common practice to engineer new features from the provided feature set. Such engineered features will either augment, or replace portions of the existing feature vector. These engineered features are essentially calculated fields, based on the values of the other features. Engineering such features is primarily a manual, timeconsuming task. Additionally, each type of model will respond differently to different types of engineered features. This paper reports on empirical research to demonstrate what types of engineered features are best suited to which machine learning model type. This is accomplished by generating several datasets that are designed to benefit from a particular type of engineered feature. The experiment demonstrates to what degree the machine learning model is capable of synthesizing the needed feature on its own. If a model is capable of synthesizing an engineered feature, it is not necessary to provide that feature. The research demonstrated that the studied models do indeed perform differently with various types of engineered features.", "creator": "LaTeX with hyperref package"}}}