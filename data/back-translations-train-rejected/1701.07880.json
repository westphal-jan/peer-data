{"id": "1701.07880", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2017", "title": "emLam -- a Hungarian Language Modeling baseline", "abstract": "This paper aims to make up for the lack of documented baselines for Hungarian language modeling. Various approaches are evaluated on three publicly available Hungarian corpora. Perplexity values comparable to models of similar-sized English corpora are reported. A new, freely downloadable Hungar- ian benchmark corpus is introduced.", "histories": [["v1", "Thu, 26 Jan 2017 21:18:32 GMT  (151kb)", "http://arxiv.org/abs/1701.07880v1", "Additional resources: - the emLam repository:this https URL- the emLam corpus:this http URL"]], "COMMENTS": "Additional resources: - the emLam repository:this https URL- the emLam corpus:this http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["d\\'avid m\\'ark nemeskey"], "accepted": false, "id": "1701.07880"}, "pdf": {"name": "1701.07880.pdf", "metadata": {"source": "CRF", "title": "emLam \u2013 a Hungarian Language Modeling baseline", "authors": ["D\u00e1vid M\u00e1rk Nemeskey"], "emails": ["nemeskeyd@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that both are very complex issues, and it is a very complex issue."}, {"heading": "2 The Hungarian Datasets", "text": "The body sizes are similar to those of the English corpus files commonly used for LM benchmarks. However, we used a version in which the morphological characteristics were converted to KR codes in order to stay in line with the automatic toolchain described below. Treebank consists of CoNLL-like Tsv files. We used a version in which the morphological characteristics were converted to KR codes to keep the automatic toolchain in line. At about 1.5 million tokens, it is similar to the size of Penn Treebank."}, {"heading": "2.1 Preprocessing", "text": "In fact, it is the case that one sees oneself as being able to live in a country where most people are able to flourish, and where most of them are able to flourish in order to flourish."}, {"heading": "2.2 The Benchmark Corpus", "text": "Of the three aforementioned corpus, the Hungarian web corpus is the only one that can be freely downloaded and is available under a share-like license (Open Content). Therefore, we have decided to provide not only the scripts, but also the pre-edited corpus for researchers, in a similar manner. The corpus can be downloaded as a list of files separated by tabs. The three columns are the word, the lemmas and the unique morphological characteristics. An unigrammatic (word and Lemma) frequency dictionary is also attached to help with the creation of numbered versions. The corpus is available under the Creative Commons Share-alike (CC SA) license. Such a corpus could facilitate language modeling of research in two ways: First, any result published with the corpus is easily reproducible; second, the fact that it is processed similarly to the English 1B corpus, as was made possible in this set."}, {"heading": "3 Language Modeling", "text": "The task of (statistical) language modeling is to assign a probability to a word sequence S = w1,..., wN. In this work, we are only looking at sentences, but other choices (paragraphs, documents, etc.) are also common. Furthermore, we are only looking at generative models where the probability of a word does not depend on subsequent tokens. (1) The probability of S can then be decomposed using the chain rule asP (S) = P (w1,..., wN) = N \u2211 i = 1 P (wi | w1,..., wi \u2212 1). (1) The condition (w1,..., wi \u2212 1) is called the context of wi. One of the challenges of language modeling is that the number of possible contexts is infinite while the training set is not. Therefore, the full context is rarely used; LMs approach it and address the problem of data sparseness in various ways."}, {"heading": "3.1 N-grams", "text": "N-gram models work according to the Markov assumption, i.e. the current word only depends on n \u2212 1 preceding words: P (wi | w1,..., wi \u2212 1) \u2248 P (wi | wi \u2212 n + 1,..., wi \u2212 1). (2) An n-gram model is a collection of such conditional probabilities. The problem of data sparseness is solved by smoothing out the probability estimate in two ways: Backoff models recursively fall back to coarser (n \u2212 1, n \u2212 2,... -gram) models if the context of a word was not seen during training, while interpolated models always include the lower orders in the probability estimate. A variety of smoothing models have been suggested over the years; we chose modified Kneser-Ney (KN) [15, 9] as a starting point, since it supposedly outperforms all other n-gram models [10]. We used the implementation of the SILM-RILM-2-4- unpolated, 4- 4- backward, 4- polated models."}, {"heading": "3.2 Class-based n-grams", "text": "Class-based models take advantage of the fact that certain words resemble the meaning or syntactical function of others. By grouping words into classes C according to these characteristics, a class-based n-gram model estimates the probability of the next word asP (wi | w1,..., wi \u2212 1, c1,..., ci \u2212 1) \u2248 P (ci | ci) P (ci \u2212 n + 1,..., ci \u2212 1). (3) This is a Hidden Markov Model (HMM) where classes are the hidden states and words are the observations. The techniques proposed for class assignment fall into two categories: statistical clustering [6, 17] and the use of existing linguistic information such as POS tags [24]. In this paper, we have chosen the latter, since a complete morphological analysis was already available as a by-product of deglutenisation."}, {"heading": "3.3 RNN", "text": "In recent years, Recurrent Neural Networks (RNN) have become mainstream in language modeling research [19, 20, 32, 14]. Specifically, LSTM [13] models represent the state of the art on the 1B dataset [14]. The power of RNNs comes from two sources: First, words are projected into a continuous vector space, thereby alleviating the problem of sparseness; and second, their ability to encode the entire context into their state and thus \"remember\" back much further than n-grams. The disadvantage is that it can take weeks to train an RNN, while an n-gram model can be calculated in a few hours. We performed two RNN baselines: 1. the medium-regulated LSTM setup in [32]. We used the implementation6 in Tensorflow [1] 2. LSTM-512-512, the smallest configuration described in [14]."}, {"heading": "3.4 Language Model Evaluation", "text": "The default quantity for the quality of language models is perplexity (PPL), which measures how well the model predicts the text data. Intuitively, it shows how many options the LM considers for each word; the lower the better. The helplessness of the sequences1,..., wN becomes asPPL = 2H = 2 \u2211 N i = 1 \u2212 1 N log2 P (wi | w1,..., wi \u2212 1), (4) where H is the cross entropy. 6 https: / / github.com / tensorflow / tensorflow / tree / master / tensorflow / models / rnn / ptbLanguage models typically fare worse when tested on another corpus, due to differences in vocabulary, word distribution, style, etc. To see how significant this effect is, the models were evaluated not only on the test distribution of their training corpus, but also on the other two corpus."}, {"heading": "4 Evaluation", "text": "The aforementioned brainless brainless brainless consecrated braingeechlcsrteeSrtee\u00fccnlrrcnlrllrteeeFnln nvo ende nlhirngeeeeeeeeFnln in rde lrllllllllllcnllllllllllrllllllllllllllllllllllllllllllllllllllllllllllllllllllrllllllrllllllrllllrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrrlrlrlrlrlrlrrlrlrrlrlrlrlrlrlrlrrlrlrlrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "5 Conclusion", "text": "This work contributes to the modeling of the Hungarian language in two ways: First, we reported on state-of-the-art LM baselines for three Hungarian corporations, from millions to gigaword size. We found that raw LMs performed worse at word level than English, but when the text was divided into lemmas and inflexible attachments (the \"gluten-free\" format), the results were comparable to those reported in similarly large English corporations. Second, we introduced a benchmark corpus for language modeling. To our knowledge, this is the first such data set for Hungarian. This specially prepared version of the Hungarian webcorpus is freely available and allows researchers to experiment easily and reproducibly with new language modeling techniques."}, {"heading": "5.1 Future Work", "text": "While the methods described here can be described as state-of-the-art, many similarly effective modeling approaches are missing. Their evaluation could provide additional insights into how Hungarian \"works\" or how Hungarian and English should be modelled differently. To understand the unusual behavior of word models on webcorpus, further research is also needed in terms of language and body structure. Here, the performance of the models has been measured in isolation. Their application (perhaps with some adjustment) in NLP applications such as ASR or ML could answer the question of whether reducing perplexity leads to similar reductions in WHO or BLEU. The most obvious problem addressed but not addressed in this paper is the effect of compounding and derivation on word size. One way to reduce the number of words could be a thorough deglutenisation algorithm that splits compound words into their components and leaves productive derivative suffixes such as h\u00e1z \u00b7 g untouched."}, {"heading": "Acknowledgements", "text": "This work is part of the e-magyar framework and was supported by the Research Infrastructure Development Grant, Category 2, 2015 of the Hungarian Academy of Sciences."}], "references": [{"title": "TensorFlow: Large-scalemachine learning on heterogeneous systems, 2015", "author": ["Mart\u0131n Abadi"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "On the use of morphological analysis for dialectal Arabic speech recognition.", "author": ["Mohamed Afify", "Ruhi Sarikaya", "Hong-Kwang Jeff Kuo", "Laurent Besacier", "Yuqing Gao"], "venue": "INTERSPEECH", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "A Neural Probabilistic Language Model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "Journal of Machine Learning Research", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Compositional Morphology for Word Representations and Language Modelling", "author": ["Jan A Botha", "Phil Blunsom"], "venue": "In: ICML", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Large Language Models in Machine Translation", "author": ["Thorsten Brants", "Ashok C. Popat", "Peng Xu", "Franz J. Och", "Jeffrey Dean"], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). Prague,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Class\u2013 based n\u2013gram models of natural language", "author": ["P.F. Brown", "V.J. Della Pietra", "P.V. de Souza", "J.C. Lai", "R.L. Mercer"], "venue": "Computational Linguistics", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "DanBikel,Maria Shugrina, PatrickNguyen, and ShankarKumar. Large Scale Language Modeling in Automatic Speech Recognition", "author": ["Ciprian Chelba"], "venue": "Tech. rep. Google,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "One billion word benchmark for measuring progress in statistical languagemodeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "In: INTERSPEECH2014, 15th Annual Conference of the International Speech Communication Association, Singapore,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F Chen", "Joshua Goodman"], "venue": "Tech. rep. TR-10-98", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "A bit of progress in language modeling", "author": ["Joshua T. Goodman"], "venue": "Computer Speech & Language", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Tr\u00f3n. \u201cCreating open language resources for Hungarian", "author": ["P\u00e9ter Hal\u00e1csy", "Andr\u00e1s Kornai", "L\u00e1szl\u00f3 N\u00e9meth", "Andr\u00e1s Rung", "Istv\u00e1n Szakad\u00e1t", "Viktor"], "venue": "Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Morphologically Motivated Language Models in Speech Recognition", "author": ["Teemu Hirsim\u00e4ki", "Mathias Creutz", "Vesa Siivola", "Mikko Kurimo"], "venue": "Proceedings of AKRR\u201905, International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning. Espoo, Finland: Helsinki University of Technology, Laboratory of Computer and Information Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Long Short-TermMemory\u201d. In:Neural Computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "(Nov", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Improved backing-off for m-gram language modeling", "author": ["ReinhardKneser andHermannNey"], "venue": "In: International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["Mitchell Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1993}, {"title": "Algorithms for bigram and trigram word clustering", "author": ["Sven Martin", "J\u00f6rg Liermann", "Hermann Ney"], "venue": "Speech communication", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Improved recognition of spontaneous Hungarian speech \u2014 Morphological and acoustic modeling techniques for a less resourced task", "author": ["P\u00e9ter Mihajlik", "Zolt\u00e1n Tuske", "Bal\u00e1zs Tarj\u00e1n", "Botty\u00e1n N\u00e9meth", "Tibor Fegy\u00f3"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Cernock\u1ef3. \u201cEmpirical Evaluation and Combination of Advanced Language Modeling Techniques.", "author": ["Tomas Mikolov", "Anoop Deoras", "Stefan Kombrink", "Lukas Burget", "Jan"], "venue": "INTERSPEECH", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Strategies for training large scale neural network language models", "author": ["Tom\u00e1\u0161 Mikolov", "Anoop Deoras", "Daniel Povey", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u1ef3"], "venue": "Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig"], "venue": "SLT", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Statisztikai \u00e9s szab\u00e1ly alap\u00fa morfol\u00f3giai elemz\u0151k kombin\u00e1ci\u00f3ja besz\u00e9dfelismer\u0151 alkalmaz\u00e1shoz", "author": ["Botty\u00e1n N\u00e9meth", "P\u00e9ter Mihajlik", "Domonkos Tikk", "Viktor Tr\u00f3n"], "venue": "Proceedings of MSZNY", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Comparison of part-of-speech and automatically derived category-based language models for speech recognition", "author": ["Thomas R Niesler", "Edward WD Whittaker", "Philip C Woodland"], "venue": "In: Acoustics, Speech and Signal Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "The Hungarian Gigaword Corpus", "author": ["Csaba Oravecz", "Tam\u00e1s V\u00e1radi", "B\u00e1lint Sass"], "venue": "Proceedings of LREC", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling.", "author": ["Hasim Sak", "Andrew W Senior", "Fran\u00e7oise Beaufays"], "venue": "INTERSPEECH", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "SRILM at sixteen: Update and outlook", "author": ["Andreas Stolcke", "Jing Zheng", "WenWang", "Victor Abrash"], "venue": "In:Proceedings of IEEEAutomatic Speech Recognition and Understanding Workshop", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Magyar nyelv\u0171, \u00e9l\u0151 k\u00f6z\u00e9leti- \u00e9s h\u00edrm\u0171sorok g\u00e9pi feliratoz\u00e1sa", "author": ["Bal\u00e1zs Tarj\u00e1n", "\u00c1d\u00e1mVarga", "Zolt\u00e1n Tobler", "Gy\u00f6rgy Szasz\u00e1k", "Tibor Fegy\u00f3", "Csaba Bord\u00e1s", "P\u00e9ter Mihajlik"], "venue": "Proc. MSZNY 2016. Szegedi Tudoma\u0301nyegyetem,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Hunmorph: Open Source Word Analysis", "author": ["Viktor Tr\u00f3n", "Gy\u00f6gy Gyepesi", "P\u00e9ter Hal\u00e1csky", "Andr\u00e1s Kornai", "L\u00e1szl\u00f3 N\u00e9meth", "D\u00e1niel Varga"], "venue": "Proceedings of the ACL Workshop on Software. Ann Arbor, Michigan: Association for Computational Linguistics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "e-magyar: digit\u00e1lis nyelvfeldolgoz\u00f3 rendszer", "author": ["Tam\u00e1s V\u00e1radi"], "venue": "XIII. Magyar Sza\u0301m\u0131\u0301to\u0301ge\u0301pes Nyelve\u0301szeti Konferencia (MSZNY2017). Szeged,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2017}, {"title": "Szeged Corpus 2.5: Morphological Modifications in a Manually POS-tagged Hungarian Corpus", "author": ["Veronika Vincze", "Viktor Varga", "Katalin Ilona Simk\u00f3", "J\u00e1nos Zsibrita", "\u00c1goston Nagy", "Rich\u00e1rd Farkas", "J\u00e1nos Csirik"], "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914)", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "It has been shown that the quality of the LM has a significant effect on the performance of these systems [5, 7].", "startOffset": 106, "endOffset": 112}, {"referenceID": 6, "context": "It has been shown that the quality of the LM has a significant effect on the performance of these systems [5, 7].", "startOffset": 106, "endOffset": 112}, {"referenceID": 9, "context": "For English, a thorough benchmark of n-gram models was carried out by Goodman [10], while more recent papers report results for advanced models [20, 8].", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "For English, a thorough benchmark of n-gram models was carried out by Goodman [10], while more recent papers report results for advanced models [20, 8].", "startOffset": 144, "endOffset": 151}, {"referenceID": 7, "context": "For English, a thorough benchmark of n-gram models was carried out by Goodman [10], while more recent papers report results for advanced models [20, 8].", "startOffset": 144, "endOffset": 151}, {"referenceID": 7, "context": "Lately, the One Billion Word Benchmark corpus (1B) [8] was published for the sole reason of measuring progress in statistical language modeling.", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "Neural networks [3, 21, 19] overtook n-grams as the language model of choice.", "startOffset": 16, "endOffset": 27}, {"referenceID": 19, "context": "Neural networks [3, 21, 19] overtook n-grams as the language model of choice.", "startOffset": 16, "endOffset": 27}, {"referenceID": 13, "context": "State-of-theart LSTMp networks achieve up to 55% reductions in perplexity compared to 5-gram models [14].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "Aside from an interesting line of work on morphological modeling for speech recognition [23, 18], no study is known to the author that addresses issues of Hungarian language modeling.", "startOffset": 88, "endOffset": 96}, {"referenceID": 17, "context": "Aside from an interesting line of work on morphological modeling for speech recognition [23, 18], no study is known to the author that addresses issues of Hungarian language modeling.", "startOffset": 88, "endOffset": 96}, {"referenceID": 26, "context": "[28] use a 3-gram model that achieves a perplexity of 4001 on the test set \u2014 a far cry from the numbers reported in [8] and here.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[28] use a 3-gram model that achieves a perplexity of 4001 on the test set \u2014 a far cry from the numbers reported in [8] and here.", "startOffset": 116, "endOffset": 119}, {"referenceID": 10, "context": "Second, we present a version of the Hungarian Webcorpus [11] that can be used as a benchmark for LM models.", "startOffset": 56, "endOffset": 60}, {"referenceID": 28, "context": "hu [30].", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "The Szeged Treebank [31] is the largest manually annotated corpus of Hungarian.", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "5 million tokens, it is similar in size to the Penn Treebank [16], allowing us a direct comparison of small-vocabulary LM techniques.", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "The filtered version of the Hungarian Webcorpus [11] is a semi-gigaword corpus at 589m tokens.", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "The downloadable corpus is already tokenized; we further processed it by performing lemmatization, morphological analysis and disambiguation with Hunmorph [29]: ocamorph for the former two and hunlex for the latter.", "startOffset": 155, "endOffset": 159}, {"referenceID": 23, "context": "The Hungarian Gigaword Corpus (MNSZ2) [25] is the largest public Hungarian corpus.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "The most common solution in the literature is to break up the words into smaller segments [12, 2, 4].", "startOffset": 90, "endOffset": 100}, {"referenceID": 1, "context": "The most common solution in the literature is to break up the words into smaller segments [12, 2, 4].", "startOffset": 90, "endOffset": 100}, {"referenceID": 3, "context": "The most common solution in the literature is to break up the words into smaller segments [12, 2, 4].", "startOffset": 90, "endOffset": 100}, {"referenceID": 14, "context": "A variety of smoothing models have been proposed over the years; we chose modified Kneser-Ney (KN) [15, 9] as our baseline, since it reportedly outperforms all other n-gram models [10].", "startOffset": 99, "endOffset": 106}, {"referenceID": 8, "context": "A variety of smoothing models have been proposed over the years; we chose modified Kneser-Ney (KN) [15, 9] as our baseline, since it reportedly outperforms all other n-gram models [10].", "startOffset": 99, "endOffset": 106}, {"referenceID": 9, "context": "A variety of smoothing models have been proposed over the years; we chose modified Kneser-Ney (KN) [15, 9] as our baseline, since it reportedly outperforms all other n-gram models [10].", "startOffset": 180, "endOffset": 184}, {"referenceID": 25, "context": "We used the implementation in the SRILM [27] library, and tested two configurations: a pruned backoff (the default)4 and, similar to [8], an unpruned interpolated model5.", "startOffset": 40, "endOffset": 44}, {"referenceID": 7, "context": "We used the implementation in the SRILM [27] library, and tested two configurations: a pruned backoff (the default)4 and, similar to [8], an unpruned interpolated model5.", "startOffset": 133, "endOffset": 136}, {"referenceID": 5, "context": "The techniques proposed for class assignment fall into two categories: statistical clustering [6, 17] and using pre-existing linguistic information such as POS tags [24].", "startOffset": 94, "endOffset": 101}, {"referenceID": 16, "context": "The techniques proposed for class assignment fall into two categories: statistical clustering [6, 17] and using pre-existing linguistic information such as POS tags [24].", "startOffset": 94, "endOffset": 101}, {"referenceID": 22, "context": "The techniques proposed for class assignment fall into two categories: statistical clustering [6, 17] and using pre-existing linguistic information such as POS tags [24].", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "In the last few years, Recurrent Neural Networks (RNN) have become the mainstream in language modeling research [19, 20, 32, 14].", "startOffset": 112, "endOffset": 128}, {"referenceID": 30, "context": "In the last few years, Recurrent Neural Networks (RNN) have become the mainstream in language modeling research [19, 20, 32, 14].", "startOffset": 112, "endOffset": 128}, {"referenceID": 13, "context": "In the last few years, Recurrent Neural Networks (RNN) have become the mainstream in language modeling research [19, 20, 32, 14].", "startOffset": 112, "endOffset": 128}, {"referenceID": 12, "context": "In particular, LSTM [13] models represent the state-of-the-art on the 1B dataset [14].", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "In particular, LSTM [13] models represent the state-of-the-art on the 1B dataset [14].", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "the Medium regularized LSTM setup in [32].", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "We used the implementation6 in Tensorflow [1] 2.", "startOffset": 42, "endOffset": 45}, {"referenceID": 13, "context": "LSTM-512-512, the smallest configuration described in [14], which uses LSTMs with a projection layer [26].", "startOffset": 54, "endOffset": 58}, {"referenceID": 24, "context": "LSTM-512-512, the smallest configuration described in [14], which uses LSTMs with a projection layer [26].", "startOffset": 101, "endOffset": 105}, {"referenceID": 20, "context": "PTB [22] N/A 141.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "2 1B [8] 3 90", "startOffset": 5, "endOffset": 8}, {"referenceID": 20, "context": "PTB [22] 141.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "2 N/A 1B [8] 90 67.", "startOffset": 9, "endOffset": 12}, {"referenceID": 30, "context": "Medium regularized [32] PTB 82.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "07 LSTM-512-512 [14] 1B 54.", "startOffset": 16, "endOffset": 20}], "year": 2017, "abstractText": "This paper aims to make up for the lack of documented baselines for Hungarian language modeling. Various approaches are evaluated on three publicly available Hungarian corpora. Perplexity values comparable to models of similar-sized English corpora are reported. A new, freely downloadable Hungarian benchmark corpus is introduced.", "creator": "LaTeX with hyperref package"}}}