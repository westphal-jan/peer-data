{"id": "1701.07974", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "Reinforced backpropagation for deep neural network learning", "abstract": "Standard error backpropagation is used in almost all modern deep network training. However, it typically suffers from proliferation of saddle points in high-dimensional parameter space. Therefore, it is highly desirable to design an efficient algorithm to escape from these saddle points and reach a good parameter region of better generalization capabilities, especially based on rough insights about the landscape of the error surface. Here, we propose a simple extension of the backpropagation, namely reinforced backpropagation, which simply adds previous first-order gradients in a stochastic manner with a probability that increases with learning time. Extensive numerical simulations on a toy deep learning model verify its excellent performance. The reinforced backpropagation can significantly improve test performance of the deep network training, especially when the data are scarce. The performance is even better than that of state-of-the-art stochastic optimization algorithm called Adam, with an extra advantage of less computer memory required.", "histories": [["v1", "Fri, 27 Jan 2017 08:49:19 GMT  (89kb,D)", "https://arxiv.org/abs/1701.07974v1", "7 pages and 5 figures"], ["v2", "Tue, 7 Mar 2017 05:34:08 GMT  (626kb,D)", "http://arxiv.org/abs/1701.07974v2", "9 pages and 7 figures, results added, learning rate typo corrected"], ["v3", "Wed, 24 May 2017 09:30:54 GMT  (663kb,D)", "http://arxiv.org/abs/1701.07974v3", "10 pages and 8 figures, results added"], ["v4", "Tue, 19 Sep 2017 02:57:24 GMT  (674kb,D)", "http://arxiv.org/abs/1701.07974v4", "12 pages and 9 figures, extensively revised"]], "COMMENTS": "7 pages and 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["haiping huang", "taro toyoizumi"], "accepted": false, "id": "1701.07974"}, "pdf": {"name": "1701.07974.pdf", "metadata": {"source": "CRF", "title": "Reinforced stochastic gradient descent for deep neural network learning", "authors": ["Haiping Huang", "Taro Toyoizumi"], "emails": [], "sections": [{"heading": null, "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a region and in which it is a country."}, {"heading": "II. FULLY-CONNECTED DEEP NETWORKS", "text": "We look at a toy deep network model with L layers of fully connected feedback architecture. Each layer has nk neurons (the so-called width of layer k). We define the input as n1-dimensional vector v, and the weight matrix Wk specifies the symmetrical connections between layer k and layer k \u2212 1. The symmetry means that the same connections are used to propagate the error backwards during the workout. A bias parameter can also be included in the weight matrix by assuming an additional constant input. The output at the last layer is expressed as: y = fL (WLfL \u2212 1 (W L \u00b7 \u00b7 \u00b7 \u00b7 f2 (W2v)), where fk (\u00b7) is an elementary sigmoid function for neurons on layer k, defined as f (x) = 11 + \u2212 x, unless the learning function is specified differently for the target layer (e.g., the reactivation of the normal layer)."}, {"heading": "III. BACKPROPAGATION AND ITS VARIANTS", "text": "We first introduce the vanilla BackProp [14] to train the deep network defined in Sec. II. We use a quadratic loss (error) function defined as E = 12T, where T denotes a vector (matrix) transposing process and defines the difference between the target and the actual power as = y. To propagate the error, we also define two associated quantities: one is the state of the neurons in the k-th layer defined by sk (e.g. sL = y, s1 = v), and the other is the weighted sum entered into the neurons in the k-th layer, defined by hk-value Wksk \u2212 1. Accordingly, we define two related gradient vectors applied by sk (2a)."}, {"heading": "IV. REINFORCED STOCHASTIC GRADIENT DESCENT", "text": "In the above learning steps, only current gradients are used to update the weight matrix (1). (1) Therefore, the gradients can be very noisy with large fluctuations. (1) If the update policies along some weight components are not able to accumulate the history of the gradient information while other directions still follow the current gradient, learning performance can be increased. This stochastic rule of accumulation can help us handle the uncertainty of updating the weight components. We will test this idea in the following deep neural networks to use previous gradient information, we will define a stochastic process for adjusting the g gradients to each learning step as follows: (g) i (gt) i (t) i (t)."}, {"heading": "V. RESULTS AND DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Learning performance in simple synthetic dataset", "text": "We first test our idea in the simple synthetic datasets described in Sec. II. We use a 4-layer network architecture as 100-400-200-10. Training examples are divided into mini-batches of size B = 100. In simulations we use the parameters (GP 0, GP 0) = (0.9995, 0.0001), unless otherwise stated. Although the chosen parameters are not optimal to achieve the best performance, we still quickly observe the outstanding performance of R-SGD. In Figure 2 (a) we compare the Vanilla BackProp with R-SGD. We clearly see that the test performance is ultimately improved by a significant amount at 100th epoch (about 77.5%). Meanwhile, the training error is also significantly lower than that of BackProp. An outstanding feature of R-SGD is that the amplification strategy SGD comes from possible plateau regions with high error points and is finally a very beautiful region."}, {"heading": "B. Learning performance in MNIST dataset", "text": "Finally, we evaluate the test performance of R-SGD on MNIST datasets within the time period under investigation. The MNIST handwritten digits of the dataset contain 60000 training images and an additional 10000 images for testing. Each image is one of ten handwritten digits (0 to 9) with 28 x 28 pixels. Therefore, the input size is n1 = 784. For simplicity, we choose the network structure as 784-100-200-10.t = 700 t = 500 t = 300 re in Adam. Although the amplification probability is specified by (0, \u03bb) which we use in the synthetic dataset, we are working on the MNIST dataset (see an example in Fig. 7 (c)), we found that the amplification probability (t) = 1 \u2212 1 / 3 t works better (shortens the training time to achieve a lower generalization error) for MNIST datasets beyond and does not reinforce this choice."}, {"heading": "VI. CONCLUDING REMARKS", "text": "In this paper, we propose a new type of effective strategy to overcome the plateau problem typical of learning in deep neural networks, which takes into account previous (cumulative) gradient information in a likely manner when updating the current weight matrix. In fact, we add a time-dependent gradient amplification, thus increasing the exploratory capability of the original SGD. This strategy is essentially different from independently added noise during the learning process. [22, 23] We add that the noise of the gradients during training is compared with hyperparameters in our simulations."}, {"heading": "Acknowledgments", "text": "We are grateful for the anonymous arbitrators for many constructive comments. H.H. Thanks to Dr. Alireza Goudarzi 51 chastic networks for a lunch discussion that later sparked the idea of this work. This work was supported by the Program for Brain Mapping by Integrated Neurotechnologies for Disease Studies (Brain / MINDS) of Japan Agency for Medical Research and Development, AMED, and by RIKEN Brain Science Institute. [1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet Classification with Deep Convolutionary Neural Networks. In P. Bartlett, F.c.n. Pereira, C.j.c. Burges, L. Bottou, and K.q. XiXi., Editors, Advances in Neural Information Processing Systems 25, pp. 1106-1114, 2012. [2] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, Cambridge, MA, 2016."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Deep Learning", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "A stochastic approximation method", "author": ["Herbert Robbins", "Sutton Monro"], "venue": "Ann. Math. Statist.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1951}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-10),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Origin of the computational hardness for learning with binary synapses", "author": ["Haiping Huang", "Yoshiyuki Kabashima"], "venue": "Phys. Rev. E,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Statistics of critical points of gaussian fields on large-dimensional spaces", "author": ["Alan J. Bray", "David S. Dean"], "venue": "Phys. Rev. Lett.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Replica symmetry breaking condition exposed by random matrix calculation of landscape complexity", "author": ["Yan V. Fyodorov", "Ian Williams"], "venue": "Journal of Statistical Physics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G. Ben Arous", "Y. LeCun"], "venue": "ArXiv e-prints", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "ArXiv e-prints", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Escaping From Saddle Points \u2014 Online Stochastic Gradient for Tensor Decomposition", "author": ["R. Ge", "F. Huang", "C. Jin", "Y. Yuan"], "venue": "ArXiv e-prints", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "author": ["Jimmy Ba", "Roger Grosse", "James Martens"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ArXiv e-prints 1412.6980,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George E. Dahl", "Geoffrey E. Hinton"], "venue": "In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Introductory lectures on convex optimization: A basic course, volume 87", "author": ["Y. Nesterov"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Exploring strategies for training deep neural  12 networks", "author": ["Hugo Larochelle", "Yoshua Bengio", "J\u00e9rome Louradour", "Pascal Lamblin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys", "author": ["P. Chaudhari", "A. Choromanska", "S. Soatto", "Y. LeCun", "C. Baldassi", "C. Borgs", "J. Chayes", "L. Sagun", "R. Zecchina"], "venue": "ArXiv e-prints", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "The Marginal Value of Adaptive Gradient Methods in Machine Learning", "author": ["A.C. Wilson", "R. Roelofs", "M. Stern", "N. Srebro", "B. Recht"], "venue": "ArXiv e-prints", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "An Empirical Analysis of Deep Network Loss Surfaces", "author": ["D. Jiwoong Im", "M. Tao", "K. Branson"], "venue": "ArXiv e-prints", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["A. Neelakantan", "L. Vilnis", "Q.V. Le", "I. Sutskever", "L. Kaiser", "K. Kurach", "J. Martens"], "venue": "ArXiv e-prints", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "The effect of gradient noise on the energy landscape of deep networks", "author": ["P. Chaudhari", "S. Soatto"], "venue": "ArXiv e-prints", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Learning by message passing in networks of discrete synapses", "author": ["A. Braunstein", "R. Zecchina"], "venue": "Phys. Rev. Lett,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Subdominant dense clusters allow for simple learning and high computational performance in neural networks with discrete synapses", "author": ["Carlo Baldassi", "Alessandro Ingrosso", "Carlo Lucibello", "Luca Saglietti", "Riccardo Zecchina"], "venue": "Phys. Rev. Lett.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "On the difficulty of training Recurrent Neural Networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ArXiv e-prints", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Multilayer neural networks have achieved state-of-the-art performances in image recognition [1], speech recognition, and even natural language processing [2].", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": "Multilayer neural networks have achieved state-of-the-art performances in image recognition [1], speech recognition, and even natural language processing [2].", "startOffset": 154, "endOffset": 157}, {"referenceID": 2, "context": "This impressive success is based on a simple powerful stochastic gradient descent (SGD) algorithm [3], and its variants.", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "However, the parameter space is highly non-convex for a typical deep network training, and finding a good path for SGD to improve generalization ability of deep neural networks is thus challenging [4].", "startOffset": 197, "endOffset": 200}, {"referenceID": 4, "context": "As found in standard spin glass models of neural networks [5, 6], a non-convex error surface is accompanied by exponentially many local minima, which hides the (isolated) global minima and thus makes any local search algorithms easily get trapped.", "startOffset": 58, "endOffset": 64}, {"referenceID": 5, "context": "In addition, the error surface structure of deep networks might behave similarly to random Gaussian error surface [7, 8], which demonstrates that critical points (defined as zero-gradient points) of high error have a large number of negative eigenvalues of the corresponding Hessian matrix.", "startOffset": 114, "endOffset": 120}, {"referenceID": 6, "context": "In addition, the error surface structure of deep networks might behave similarly to random Gaussian error surface [7, 8], which demonstrates that critical points (defined as zero-gradient points) of high error have a large number of negative eigenvalues of the corresponding Hessian matrix.", "startOffset": 114, "endOffset": 120}, {"referenceID": 7, "context": "Consistent with this theoretical study, empirical studies on deep network training [9, 10] showed that SGD is slowed down by a proliferation of saddle points with many negative curvatures and even plateaus (eigenvalues close to zero in many directions).", "startOffset": 83, "endOffset": 90}, {"referenceID": 8, "context": "Consistent with this theoretical study, empirical studies on deep network training [9, 10] showed that SGD is slowed down by a proliferation of saddle points with many negative curvatures and even plateaus (eigenvalues close to zero in many directions).", "startOffset": 83, "endOffset": 90}, {"referenceID": 9, "context": "The prevalence of saddle points poses an obstacle to attain better generalization properties for a deep network, especially for SGD based on first-order optimization, while second-order optimization relying on Hessian-vector products is more computationally expensive [11].", "startOffset": 268, "endOffset": 272}, {"referenceID": 10, "context": "The second order method that relies on positive-definite curvature approximations, can not follow directions with negative curvature, and is easily trapped by saddle points [12].", "startOffset": 173, "endOffset": 177}, {"referenceID": 11, "context": "The excellent performance of R-SGD is verified first on training a toy fully-connected deep network model to learn a simple non-linear mapping generated by a two-layer feedforward network, and then on a benchmark handwritten digits dataset [13], in comparison to both vanilla backpropagation (BackProp) [14] and state-of-the-art Adam algorithm [15].", "startOffset": 240, "endOffset": 244}, {"referenceID": 12, "context": "The excellent performance of R-SGD is verified first on training a toy fully-connected deep network model to learn a simple non-linear mapping generated by a two-layer feedforward network, and then on a benchmark handwritten digits dataset [13], in comparison to both vanilla backpropagation (BackProp) [14] and state-of-the-art Adam algorithm [15].", "startOffset": 344, "endOffset": 348}, {"referenceID": 13, "context": "In the benchmark dataset, we also clarify the performance difference between R-SGD and SGD with fixed or adaptive momentum parameter [16] and Nesterov\u2019s momentum [17].", "startOffset": 133, "endOffset": 137}, {"referenceID": 14, "context": "In the benchmark dataset, we also clarify the performance difference between R-SGD and SGD with fixed or adaptive momentum parameter [16] and Nesterov\u2019s momentum [17].", "startOffset": 162, "endOffset": 166}, {"referenceID": 15, "context": "forms the vanilla BackProp widely used in training deep networks given the labeled data [18].", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "The second one is Nesterov\u2019s accelerated gradient (NAG) [17], which first implements a partial update to Wt, and then uses the updated Wt to evaluate gradients, i.", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "To show the efficiency of R-SGD, we also compare its performance with that of a state-of-the-art stochastic optimization algorithm, namely adaptive moment estimation (Adam) [15].", "startOffset": 173, "endOffset": 177}, {"referenceID": 12, "context": "We use heuristic parameters of Adam given in [15], except that \u03b70 = 0.", "startOffset": 45, "endOffset": 49}, {"referenceID": 16, "context": "This observation is consistent with a recent study of maximizing local entropy in deep networks [19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "Adaptive methods such as Adam often show faster initial progress, but their performances get quickly trapped by a test error plateau [20].", "startOffset": 133, "endOffset": 137}, {"referenceID": 18, "context": "Using the bilinear interpolation method [21], one can visualize the error surface in 3D subspace spanning four high-dimensional weight configurations.", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": "Based on these four configurations, the error function is varied as a function of a new constructed weight matrix specified by W = \u03b2(\u03b1W1 + (1\u2212 \u03b1)W2)) + (1\u2212 \u03b2)(\u03b1W3 + (1\u2212 \u03b1)W4), where \u03b1 \u2208 [0, 1] and \u03b2 \u2208 [0, 1] are two control parameters.", "startOffset": 186, "endOffset": 192}, {"referenceID": 0, "context": "Based on these four configurations, the error function is varied as a function of a new constructed weight matrix specified by W = \u03b2(\u03b1W1 + (1\u2212 \u03b1)W2)) + (1\u2212 \u03b2)(\u03b1W3 + (1\u2212 \u03b1)W4), where \u03b1 \u2208 [0, 1] and \u03b2 \u2208 [0, 1] are two control parameters.", "startOffset": 201, "endOffset": 207}, {"referenceID": 19, "context": "This strategy is essentially different from an independently random noise added to the gradient during the learning process [22, 23].", "startOffset": 124, "endOffset": 132}, {"referenceID": 20, "context": "This strategy is essentially different from an independently random noise added to the gradient during the learning process [22, 23].", "startOffset": 124, "endOffset": 132}, {"referenceID": 19, "context": "In fact, we add time-dependent Gaussian noise to gradients during training in our simulations using default hyper-parameters [22], whose performance could not be comparable to that of R-SGD within 100-epochs training (or it requires longer convergence time).", "startOffset": 125, "endOffset": 129}, {"referenceID": 21, "context": "A similar reinforcement strategy has been used in a single layer neural network with discrete weights [24], where local fields in a belief propagation equation are reinforced.", "startOffset": 102, "endOffset": 106}, {"referenceID": 22, "context": "The reinforced belief propagation is conjectured to be related to local entropy maximization [6, 25] in discrete neural networks.", "startOffset": 93, "endOffset": 100}, {"referenceID": 12, "context": "The learning step-size of Adam is adaptively changed, which means that the step-size is automatically tuned to be closer to zero when there is greater uncertainty about the direction of the true gradient [15], while RSGD uses the stochastic reinforcement of the gradient information to deal with this kind of uncertainty, and shows comparable and even better performance.", "startOffset": 204, "endOffset": 208}, {"referenceID": 17, "context": "A recent study argued that adaptive learning methods such as Adam generalize worse than SGD or SGDM on CIFAR-10 image classification tasks [20].", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": "R-SGD may be able to avoid vanishing or exploding gradient problem typical in training a very deep network [4], probably thanks to accumulation of gradients used stochastically.", "startOffset": 107, "endOffset": 110}, {"referenceID": 23, "context": "In addition, it may take effect in recurrent neural network training [27].", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Herbert Robbins and Sutton Monro.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Xavier Glorot and Yoshua Bengio.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Haiping Huang and Yoshiyuki Kabashima.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] Alan J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] Yan V.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] Jimmy Ba, Roger Grosse, and James Martens.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] Ilya Sutskever, James Martens, George E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] Hugo Larochelle, Yoshua Bengio, J\u00e9rome Louradour, and Pascal Lamblin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] Carlo Baldassi, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti, and Riccardo Zecchina.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[27] R.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Stochastic gradient descent (SGD) is a standard optimization method to minimize a training error with respect to network parameters in modern neural network learning. However, it typically suffers from proliferation of saddle points in the high-dimensional parameter space. Therefore, it is highly desirable to design an efficient algorithm to escape from these saddle points and reach a parameter region of better generalization capabilities. Here, we propose a simple extension of SGD, namely reinforced SGD, which simply adds previous first-order gradients in a stochastic manner with a probability that increases with learning time. As verified in a simple synthetic dataset, this method significantly accelerates learning compared with the original SGD. Surprisingly, it dramatically reduces over-fitting effects, even compared with state-of-the-art adaptive learning algorithm\u2014Adam. For a benchmark handwritten digits dataset, the learning performance is comparable to Adam, yet with an extra advantage of requiring one-fold less computer memory. The reinforced SGD is also compared with SGD with fixed or adaptive momentum parameter and Nesterov\u2019s momentum, which shows that the proposed framework is able to reach a similar generalization accuracy with less computational costs. Overall, our method introduces stochastic memory into gradients, which plays an important role in understanding how gradient-based training algorithms can work and its relationship with generalization abilities of deep networks.", "creator": "LaTeX with hyperref package"}}}