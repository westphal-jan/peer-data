{"id": "1701.08071", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "Emotion Recognition From Speech With Recurrent Neural Networks", "abstract": "In this paper the task of emotion recognition from speech is considered. Proposed approach uses deep recurrent neural network trained on a sequence of acoustic features calculated over small speech intervals. At the same time special probabilistic-nature CTC loss function allows to consider long utterances containing both emotional and unemotional parts. The effectiveness of such an approach is shown in two ways. First one is the comparison with recent advances in this field. While second way implies measuring human performance on the same task, which also was done by authors.", "histories": [["v1", "Fri, 27 Jan 2017 14:50:36 GMT  (813kb,D)", "http://arxiv.org/abs/1701.08071v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vladimir chernykh", "grigoriy sterling", "pavel prihodko"], "accepted": false, "id": "1701.08071"}, "pdf": {"name": "1701.08071.pdf", "metadata": {"source": "CRF", "title": "Emotion Recognition From Speech With Recurrent Neural Networks", "authors": ["Vladimir Chernykh"], "emails": ["vladimir.chernykh@phystech.edu", "sterling@phystech.edu", "prihodkop@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In recent years, human-computer interaction has become more and more interesting for data scientists. The goal of most research is to make the conversation between humans and computers more native and natural. To achieve this goal, there are obviously two necessary points: to make people understand computers better and vice versa. A great success can be observed in speech recognition (speech to text, STT). Nowadays, machines can understand almost all human language and its related services everywhere - from Siri-like applications in smartphones to voice control services. Overall, the computer can understand what has been said. However, this is not all the data of a voice, but it also contains information about who and how it was spoken. The second question motivates us to learn to recognize emotions from speech. Moreover, text-to-speech services (TTS) have the same challenging problem: machines can only produce soundless speech."}, {"heading": "1.1 Emotion recognition problem description", "text": "The second difficulty concerns the temporal properties of emotions. Often, almost the entire utterance has no emotion (speaker is in neutral state), but emotionality is contained in only a few words or sounds in a utterance. The problem of emotion recognition can be mathematically reformulated as a classification task. In short, a function has to be constructed from the space of the utterance to the amount of emotional states. In this space, the decision rule separates utterances with one emotion from the others. But what if people evaluate utterances differently? If we assume that the emotional states are selected from anar Xiv: 170 1.08 071v 1 [cs.C L] 2unknown probability distribution, then the model should predict and process these probabilities in the most reasonable way."}, {"heading": "1.2 Relation to prior works", "text": "At the moment, the emotion recognition pipeline looks like this: \u2022 Choose an emotional speech corpus \u2022 Divide continuous audio signal into emotional expressions \u2022 Calculate characteristics for each utterance \u2022 Choose and train a classification model \u2022 Develop a set of evaluation metrics and validate constructed modelsThere are a whole range of methods for each of the above steps. However, there is no panacea for a general problem that determines emotion recognition. It is necessary to modify each step from the list in accordance with the specific characteristics of the task. The most effective approach takes into account many aspects such as problem definition, objectives, record structure, evaluation metrics, etc. In this paper, we look at the sequence classification for loudless emotion recognition of natural language. We decided to choose IEMOCAP database as the speech corpus because it fulfils all the characteristics of the task described above and there are several well-developed approaches to compare this emotional corpus."}, {"heading": "2 Data description", "text": "There are a number of databases and their \"overview\" can be found in [3, 4]. We conducted all experiments with audio data from the database The Interactive Emotional Dyadic Motion Capture (IEMOCAP) [5]. It consists of approximately 12 hours of audiovisual data from 10 actors. All recordings have a dialogue structure between a man and a woman, which was either scripted or improvised on the given topic. After capturing these audiovisual signals, the authors divided dialogues into small statements, mainly from 3 to 15 seconds long (see Fig. 1a for details) and then gave them to the evaluators for evaluation. Utterances are divided into two \"streams\" (male and female voices) and therefore they sometimes cut their dialogues (see Fig. 2b) into small statements of greater length, mainly by 3 to 4 evaluators, who were asked to evaluate each statement on the basis of audio and video streams."}, {"heading": "2.1 Technical details", "text": "Normally, digital audio is recorded as a converted analog signal by a recording device. Audio waves are discretized and converted into time series at the given discretization frequency, the so-called sampling rate. In the case of IEMOCAP, we need 16000 integers for one second of a tone at 16 kHz sampling rate. The most common approach to reducing this number is to separate an input signal into 0.1-1 second intervals (frames) and calculate acoustic characteristics above it. This sequence of characteristic vectors represents an input sound in underdimensional space with sufficient precision, but nevertheless some information is lost. The other approach is to work with raw signals that are discretized at a lower frame rate. This approach is more precise, but requires significantly more computing power."}, {"heading": "2.2 Features used", "text": "One of the sticking points in emotion recognition is which characteristics should be used. Almost all possible characteristics can be divided into 3 main classes: \u2022 Acoustic characteristics. They are calculated from the input signal and describe the wave characteristics of a language. Sometimes, pitch-based characteristics such as chromagram power are included in the acoustic set, but in some works they are considered separately or in prosodic groups. \u2022 Prosodic characteristics. In general, they try to measure peculiarities of the language, such as pauses between words, prosodies and volume. Unfortunately, language details such as prosodic characteristics depend heavily on a speaker, and their use in speaker-less emotion recognition is controversial. In some works, prosodic characteristics improve overall efficiency, but we do not use them because the reason for this is mentioned above. \u2022 Linguistic characteristics. The third type of commonly used characteristics such as prosodic characteristics are based on semantic information from the language."}, {"heading": "3 Problem statement", "text": "This reformulation makes it possible to apply a wide range of well-established methods.Let D = {(X, z)} ni = 1 is the training set in which zi-Z = E \u0442 = {0... k \u2212 1} \u043a is the true sequence of labels and Xi-X = (Rf) - corresponding multidimensional sequence of characteristics. It is worth noting that the lengths of these sequences | zi | = Ui and | Xi | = Ti may not be the same in general, the only limitation is the Ui-X condition. Let us also divide the data set into train and validation sets D = Dl-Dv with the sizes Nl and Nv accordingly. Il and Iv show the corresponding division of indices. Furthermore, let us introduce the set of classification functions F = {f: X 7 \u2192 Z} in which we want to find the best model."}, {"heading": "4 Recognition methods", "text": "The general structure remains more or less the same: input layer, a few stacked LSTM layers, dense classification layers, and a softmax layer. For a more detailed description of the architecture, refer to Section 5 of the experiment. Below is an overview of two main approaches to network training that we have used in this paper."}, {"heading": "4.1 One-label approach", "text": "One-label approach implies that every utterance, regardless of its length, has only one emotional label. It is the obvious approach with respect to data sets labeled with the annotation technique of enunciation. [5] In detail, all lengths of the label sequences are equal to 1, Ui = | zi | = 1. Then the vector z becomes scalar. The recurring neural network can be thought of as a mapping from the input mark space X to the probability distribution via the emotional class labels, which with the model weights y = Nw (X) [0; 1] k, where y is the output of the softmax layer. The loss function here is the categorical transverse entropy loss. Below, there is an objective function to minimize this loss: Q (w, Dl, L) = \u2212 1Nl, i, Il k \u2212 1, c = 0 zi, c yi, where class labels end with codification."}, {"heading": "4.2 CTC approach", "text": "The main advantage of CTC is that it selects the most likely label sequence in terms of the different ways to align it with the original sequence. Of course, the probability of the respective label method is added up from the probabilities of each of its alignments. Formally, this label can be interpreted as a lack of emotion. Technically, it reflects in another unit the ability of the last soft-max layer of the neural network. So the final label set is the following: L = E {NULL}. As in the one-label approach, we consider the work of the model as mapping, but here the codomain is also the sequence space Y = Nw (X; 1]."}, {"heading": "5 Emotion recognition experiments", "text": "In a series of experiments, we examined different models and approaches to the task of emotion recognition. All the code can be found in the github repository. [8] As mentioned in Section 2, the data structure is the following - dialogues are broken down into utterances by human assessors. Utterance is the least structural unit with emotion tag in the original labeling, but utterances vary considerably in length, so we decided to divide them into overlapping frames of 0.2 seconds duration with overlaps of 0.1 seconds. The problem is that frames have no labels. While it is obvious that not all frames of angry utterance can also be called angry frames, the test set in this essay consists of 20% randomly selected points from data sets, unless otherwise specified. In addition, two neural network structures are mainly used: \u2022 Simple LSTM structures consisting of two consecutive LSTM layers with hyperbolic activation layers and two tanbolic layers."}, {"heading": "5.1 Framewise classification", "text": "s take two of the loudest frames from each utterance. Volume is synonymous with spectral force in this context. \u2022 Assign this frame to the emotion of the utterance. \u2022 Train the frame classification model on the derived data sets. Here, we assume that the emotion of the utterance is not contained in all the frames of that utterance, but only in the loudest. Experiment shows that 2 frames are the optimal number. In terms of the classification model, we used Random Forest Classifiers from the Scikit Learning Package [9]. In Figure 5, one can observe the results of this method for random utterances from the test set. On reflection, it looks reasonably reasonable if we use short utterances. On the longer ones, it becomes towsaoth and artificial step to classify the next one's utterance."}, {"heading": "5.2 Utterance-level classification", "text": "The idea of the following experiment is to use frame characteristics themselves as input into the neural network model. Behind this is a basic idea, because with the frame-by-frame approach we do not get any additional information after the intermediate stage of the RFC. The key idea in classifying the statement level is that RNN can learn sufficient characteristics from the input characteristics itself and make the final classification based on these characteristics. Here, we experiment with two approaches described in Section 4. To evaluate and compare them, a 5-fold cross-validation technique is used."}, {"heading": "5.2.1 One-label approach", "text": "BLSTM Network outperforms Simple LSTM Network within this approach. Figure 6 shows the performance analysis. As shown in the diagrams in Figure 6a, 20-22 epochs are sufficient for the training and after that the model begins to overlap. Accuracy development curves are shown in Figure 11a in Appendix A. The bar chart 6b shows the results of the CV. The number of epochs for the training was chosen according to the previously obtained number - 20."}, {"heading": "5.2.2 CTC approach", "text": "The performance indicators for the CTC approach are shown in Figures 7 and 11b in Appendix A. It is worth mentioning that BLSTM with CTC loss needs approximately two times more eras to train properly compared to crosstropy loss. An interesting and important feature is that the CTC loss network does not overmatch."}, {"heading": "5.3 Comparison", "text": "As we look at the task of multi-class classification with somewhat unbalanced classes, it is essential to pay attention not only to general accuracy, but also to class accuracy. Therefore, we introduce two types of measurements: \u2022 general (weighted) accuracy. It is an accuracy over the entire test set. \u2022 average (unweighted) accuracy of the class. This is the average classification accuracy for each class.Table 1 shows the above-mentioned measurements for each method we used throughout the work.The last row of the table corresponds to the human assessors who were asked to forward this dataset. Full details of how and why it was carried out can be found in sections 5.4 and 6."}, {"heading": "5.4 Error structure", "text": "We also decided to investigate the CTC model further because Graves in [6] reports huge gulf in quality compared to classic models. While here the gain is about 3-5%. It is still significant, but not the breakthrough. For this reason, the fault structure of our most successful model is studied. First, it is useful to take a look at predictions distribution versus \"basic truth\" from experts. This distribution is shown in Figure 8a. Busso in his work [5] mentions that audio signal plays the main role in sadness detection, while anger and excitement is better accompanied by video signal that audio during assessment work. This hypothesis seems to be true in relation to our model. Sadness recognition percentage is much higher than the others. It is perhaps also interesting to compare it with the human fault structure in Figure 9aSo, as it was said in Section 2 Dataset, not the homogeneous structure in the sense of expert answers reliability."}, {"heading": "6 Markup investigation", "text": "Altrov in [11] collected the corpus of the Estonian language and asked people of different nationalities to rate it. Almost all nationalities (Latvians, Italians, Finns, Swedes, Danes, Norwegians, Russians) were geographically and culturally close to Estonians. Nevertheless, Estonians perform much better than all other nationalities, which have an average class accuracy of about 69%. All other people perform 10-15% worse, and the only emotion they recognize well was sadness. In our research, we developed a simple interface (Fig. 9b) for re-labeling the language corpus to see how well people can perform this task. In Figure 9a you can feel the results of the experiments.Both types of accuracies we look at before this tab confirm that 70% of the term is treated differently (1%)."}, {"heading": "7 Future work", "text": "For the future, we see two main directions of research: \u2022 Improve the properties of the model. Now, there are many ways to do it. First, it is to continue experimenting with the network structure, but based on our experience, it cannot greatly decrease the quality. Second, is to use more complex feature-generation framework or to come up with the new one. First, the last way we see is to try to use the raw audio signal as input for the model. Recently, Google DeepMind group [12] showed that it is possible to use raw audio signal to generate speech, and more than that this algorithm far exceeds all other advances in this field. We believe that technical details of their work allow it to adapt to the task of emotion recognition and in this way looks most promising. \u2022 Improve the markup and collect new data sets. As we have shown in this paper, both the markup and the data collection methodology have a strong influence on the model diversification."}, {"heading": "8 Conclusion", "text": "In this paper, we proposed a novel approach to emotion recognition from audio. There are two main advantages of a method: \u2022 The CTC loss function takes into account the fact that emotionality can only be present in a few images in the utterance \u2022 It can predict the sequence of emotions for an utterance. Furthermore, we showed that the results are comparable to those in this field. Furthermore, we analyzed model responses and error distribution as well as human performance and concluded that emotions are a very subjective notion and even when humans surpass the computer, the difference is not that significant."}, {"heading": "Appendix A", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Speech emotion recognition using deep neural network and extreme learning machine", "author": ["K. Han", "D. Yu", "I. Tashev"], "venue": "Interspeech 2014", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "High-level feature representation using recurrent neural network for speech emotion recognition", "author": ["J. Lee", "I Tashev"], "venue": "Interspeech 2015", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "A review of emotional speech databases", "author": ["D. Ververidis", "C. Kotropoulos"], "venue": "Panhellenic Conference on Informatics, pages 560\u2013574", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Iemocap: Interactive emotional dyadic motion capture database", "author": ["C. Busso", "M. Bulut", "C. Lee", "A. Kazemzadeh", "E. Mower", "S. Kim", "J. Chang", "S. Lee", "S. Narayanan"], "venue": "Journal of Language Resources and Evaluation, 42(4)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fernandez", "F. Gomez", "J. Schmidhuber"], "venue": "Proceedings of the 23rd International Conference on Machine Learning", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE, 77(2)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1989}, {"title": "and P", "author": ["V. Chernykh", "G. Sterling"], "venue": "Prihodko. https://github.com/vladimir-chernykh/ emotion_recognition", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, 12", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ICCV 2015", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "The influence of language and culture on the understanding of vocal emotions", "author": ["R. Altrov", "H. Pajupuu"], "venue": "Journal of Estonian and Finno-Ugric Linguistics, 6(3)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Wavenet: A generative model for raw audio. ArXiv e-prints, 2016. 12  Appendix A (a) Simple LSTM network (b) BLSTM network Figure 10: Network architectures (a) One-label approach  (b) CTC approach Figure 11: Accuracy evolution for BLSTM architecture", "author": ["A. Van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "in [1] where they used DNN-based system with the following aggregation over time using statistical functions.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "In the next year Lee and the same microsoft team [2] trained long short-term memory (LSTM) recurrent neural network with on a feature sequence and achieved about 60% accuracy on IEMOCAP database.", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "There are quite a few databases and its\u2019 overview can be found in [3, 4].", "startOffset": 66, "endOffset": 72}, {"referenceID": 3, "context": "We carried out all experiments with an audio data from The Interactive Emotional Dyadic Motion Capture (IEMOCAP) database [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "It is the obvious approach regarding datasets labelled with the utterance annotation technique [5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "in [6] derived a new efficient forward-backward dynamic programming algorithm for that.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "The initial idea was taken from Rabiner [7] HMM decoding algorithm.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "[6] also suggested differentiation technique naturally embedded into dynamic programming algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "All the code can be found in github repository [8] As it was mentioned in section 2, data structure is the following \u2014 dialogues are broken into utterances by human assessors.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "Regarding the classification model we used Random Forest Classifier from scikit-learn package [9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "We also decided to further investigate the CTC model because Graves in [6] reports about huge gap in quality over classical models.", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "Busso in his work [5] mentions that audio signal plays the main role in sadness recognition while angry and excitement are better detected via video signal which accompanies audio during the assessors work.", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "Recognizing a truck or a flower on a picture can be evaluated objectively and in tasks like that computers have already outperform humans [10].", "startOffset": 138, "endOffset": 142}, {"referenceID": 9, "context": "Altrov in [11] collected the corpus of Estonian speech and asked people of different nationalities to evaluate it.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "Recently Google DeepMind group shows [12] that it is possible to use raw audio signal to generate speech and more than that this algoritm greatly surpasses all other advances in this field.", "startOffset": 37, "endOffset": 41}], "year": 2017, "abstractText": "In this paper the task of emotion recognition from speech is considered. Proposed approach uses deep recurrent neural network trained on a sequence of acoustic features calculated over small speech intervals. At the same time special probabilistic-nature CTC loss function allows to consider long utterances containing both emotional and unemotional parts. The effectiveness of such an approach is shown in two ways. First one is the comparison with recent advances in this field. While second way implies measuring human performance on the same task, which also was done by authors.", "creator": "LaTeX with hyperref package"}}}