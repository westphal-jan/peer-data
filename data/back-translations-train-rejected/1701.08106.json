{"id": "1701.08106", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "Faster Discovery of Faster System Configurations with Spectral Learning", "abstract": "Despite the huge spread and economical importance of configurable software systems, there is unsatisfactory support in utilizing the full potential of these systems with respect to finding performance-optimal configurations. Prior work on predicting the performance of software configurations suffered from either (a) requiring far too many sample configurations or (b) large variances in their predictions. Both these problems can be avoided using the WHAT spectral learner. WHAT's innovation is the use of the spectrum (eigenvalues) of the distance matrix between the configurations of a configurable software system, to perform dimensionality reduction. Within that reduced configuration space, many closely associated configurations can be studied by executing only a few sample configurations. For the subject systems studied here, a few dozen samples yield accurate and stable predictors - less than 10% prediction error, with a standard deviation of less than 2%. When compared to the state of the art, WHAT (a) requires 2 to 10 times fewer samples to achieve similar prediction accuracies, and (b) its predictions are more stable (i.e., have lower standard deviation). Furthermore, we demonstrate that predictive models generated by WHAT can be used by optimizers to discover system configurations that closely approach the optimal performance.", "histories": [["v1", "Fri, 27 Jan 2017 16:36:09 GMT  (457kb,D)", "http://arxiv.org/abs/1701.08106v1", "26 pages, 6 figures"], ["v2", "Thu, 3 Aug 2017 21:15:47 GMT  (1281kb,D)", "http://arxiv.org/abs/1701.08106v2", "26 pages, 6 figures"]], "COMMENTS": "26 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.SE cs.LG", "authors": ["vivek nair", "tim menzies", "norbert siegmund", "sven apel"], "accepted": false, "id": "1701.08106"}, "pdf": {"name": "1701.08106.pdf", "metadata": {"source": "CRF", "title": "Faster Discovery of Faster System Configurations with Spectral Learning", "authors": ["Vivek Nair", "Tim Menzies", "Sven Apel"], "emails": ["vivekaxl@gmail.com", "tim.menzies@gmail.com", "norbert.siegmund@uni-passau.de", "apel@uni-passau.de"], "sections": [{"heading": null, "text": "Keywords Performance Prediction \u00b7 Spectral Learning \u00b7 Decision Trees \u00b7 SearchBased Software Engineering \u00b7 Sampling.Vivek Nair North Carolina State University, Raleigh, USA E-mail: vivekaxl @ gmail.com Tim Menzies North Carolina State University, Raleigh, USA E-mail: tim.menzies @ gmail.com Norbert Siegmund University of Weimar, Germany E-mail: norbert.siegmund @ uni-passau.deSven Apel University of Passau, Germany E-mail: apel @ uni-passau.dear Xiv: 170 1.08 106v 1 [cs.S E] 27 Jan 2017"}, {"heading": "1 Introduction", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution, that is capable of finding a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution."}, {"heading": "2 Background & Related Work", "text": "A configurable software system has a set of X of the Boolean configuration options, 1 also referred to as features or independent variables in our environment. We refer to the number of features of the system S as n. The configuration space of S can be represented by a Boolean space Zn2, which can be either true or false, based on whether the feature is selected or not. Each valid instance of a vector (i.e., a configuration) has a corresponding performance score associated to it.The literature offers two approaches to predicting the performance of software configurations: a maximum sampling method is selected or not. Each valid instance of a vector (i.e., a configuration) has a corresponding performance score assigned to it.The literature offers two approaches to predicting the performance of software configurations: a maximum sampling and a minimum sampling approach: With maximum sampling approach, we compile all possible configurations, we compile all possible configurations."}, {"heading": "5 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 RQ1", "text": "Figure 1 shows the mean errors of the predictors learned after taking X% of the configurations, and then asks WHAT and some sampling methods (S1, S2 and S3) to (a) determine which configurations to measure; then (b) asks CART for a predictor to use these measurements; the horizontal axis of the diagrams shows what X% of the configurations are being examined; the vertical axis shows the mean relative error (\u00b5) from Equation 3. In this figure: 7 Just to clarify a frequently asked question about this work, we point out that our rig \"studies\" 40% of the data do not mean that our predictive models require access to the performance results from the 40% of the data; rather, we think that by \"study\" we reflect a sample of the configurations to determine what minimal subset of that sample is to calculate."}, {"heading": "5.2 RQ2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Do less data used in building prediction models cause larger variances in the predicted values?", "text": "In our study, we report standard deviations (\u03c3) as a measure of deviations in the performance predictions. The less we scan the configuration space, the less we restrict model generation in that space. Therefore, one effect that can be expected is that models learned from too few samples show large deviations, but by sampling from the spectral space, a compensatory effect can be introduced, as this space contains fewer confusing or correlated variables than the raw configuration space. Figure 3 reports that one of these two competing effects is dominant. Figure 3 shows that after some initial fluctuations, after seeing X = 40% of the configurations, the deviations in the forecast errors return to almost zero, which is similar to the results in Figure 1. Based on the results of Figure 3, we answer \"No\" to RQ2: Selecting a small number of samples does not necessarily increase the variance (at least not in that range)."}, {"heading": "5.3 RQ3", "text": "The results of answering RQ1 and RQ2 suggest using WHAT (with S1) to generate runtime predictors from a small sample of data. RQ3 asks whether this predictor can be used by an optimizer to derive which other configurations correspond to system configurations with fast performance values. To answer this question, we executed a random set of 100 configurations 20 times and correlated these baseline values with three optimizers (GALE [21], DE [36] and NSGA-II [6]). When these three optimizers mutated existing configurations to propose new ones, these mutations were checked for validity. Mutants that violated the system configurations (e.g. one feature that excludes another feature) were rejected and the survivors were \"evaluated\" by asking the CART surrogate model. These evaluations declined the mutants to either use the mutants to search for + 1, or to find them better in Q1."}, {"heading": "5.4 RQ4", "text": "We compare WHAT with the three state-of-the-art predictors suggested in the literature [35], [15], [29] as in Section 2. Note that all approaches recommend user regression trees as predictors, except Siegmund's approach, which uses a regression function derived from linear programming; the results were examined using non-parametric tests, which are also used by Arcuri and Briand at ICSE '11 [25]. To test statistical significance, we used non-parametric bootstrap tests 95% confidence [9], followed by an A12 test to verify that all observed differences were non-trivial small effects; i.e., given two lists X and Y, we count how often there are larger numbers in the previous list (and there are bindings, let's add half a mark): a = x > Y # (x > y = y = y)."}, {"heading": "6 Why does it work?", "text": "In fact, it is the case that most of us are in a position to go into a different world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "7 Reliability and Validity", "text": "In order to increase external reliability, we have made sure that we either clearly define our algorithms or use implementations from the public domain (SciKitLearn). [26] All of the data used in this work is also available online in the PROMISE9 code repository, and all of our algorithms are available online at github.com / ai-se / where.Validity refers to the extent to which one part of the research actually examines what the researcher purports to be examining [33]. Internal validity tests, whether the differences found in the work areas in the treatments under study.9 http: / / openscience.us / performance-prediction / cpm.htmlOne threat to the internal validity of our experiments are the choice of the training and test data sets discussed in Figure 1. Recall of this while all of our students used the test data, the internal learners, the learners, the learners, the learners, the learners, the learners, the learners, the learners, the learners, the learners, the learners, the learners, the students, the learners, the learners, the learners, the students, the learners."}, {"heading": "8 Related Work", "text": "In 2000, Shi and Maik [31] claimed the term \"spectral clustering\" as a reference to their normalized sectional image segmentation algorithm, which divides data by a spectral (eigenvalue) analysis of the laplac representation of the similarity graph between instances in the data. In 2003, Kamvar et al. [20] generalized this definition by saying that \"spectral learners\" are any data-mining algorithm that first replaces the raw dimensions with those derived from the spectrum (eigenvalues) of the affinity matrix (also referred to as the distance matrix) of the data. Our cluster based on a first major component splits the data based on an approximation to a eigenvector found at each recursive level of the data (as described in paragraph 3.1). Therefore, this method is a \"spectral cluster\" in the general Kamvar sense. Note that we did not find that our data for our 34 systems are standardized compared to previous ones]."}, {"heading": "9 Conclusions", "text": "Configurable software systems are now widely used in practice, but pose challenges in determining optimal performance configurations. State-of-the-art approaches require too many measurements or tend to differ greatly in their performance predictions. To overcome these limitations, we have proposed a fast spectral learner called WAS, along with three new sampling techniques. The basic idea of WAS is to explore the configuration space with eigenvalues of the features used in a configuration to determine exactly those measurement configurations that reveal key performance characteristics. In this way, we can study many closely linked configurations with just a few measurements. We evaluated our approach using six configurable software systems borrowed from literature in the real world. Our approach achieves similarly low error rates, while remaining stable compared to the state of the art. With the exception of Berkeley DB in particular, our approach is more precise than the state-of-the-art configuration approach."}], "references": [{"title": "Think locally, act globally: Improving defect and effort prediction models", "author": ["Nicolas Bettenburg", "Meiyappan Nagappan", "Ahmed E Hassan"], "venue": "In Proceedings of the 9th IEEE Working Conference on Mining Software Repositories,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Towards improving statistical modeling of software engineering data: think locally, act globally", "author": ["Nicolas Bettenburg", "Meiyappan Nagappan", "Ahmed E Hassan"], "venue": "Empirical Software Engineering,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["Ella Bingham", "Heikki Mannila"], "venue": "In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Principal direction divisive partitioning", "author": ["Daniel Boley"], "venue": "Data mining and knowledge discovery,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Classification and regression trees", "author": ["Leo Breiman", "Jerome Friedman", "Charles J Stone", "Richard A Olshen"], "venue": "CRC press,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1984}, {"title": "A fast and elitist multiobjective genetic algorithm: NSGA-II", "author": ["Kalyanmoy Deb", "Amrit Pratap", "Sameer Agarwal", "T Meyarivan"], "venue": "IEEE Transactions on Evolutionary Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Using spectral clustering to automate identification and optimization of component structures", "author": ["Constanze Deiters", "Andreas Rausch", "Marco Schindler"], "venue": "In Realizing Artificial Intelligence Synergies in Software Engineering (RAISE),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Low-complexity principal component analysis for hyperspectral image compression", "author": ["Qian Du", "James E Fowler"], "venue": "International Journal of High Performance Computing Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "An introduction to the bootstrap", "author": ["Bradley Efron", "Robert J Tibshirani"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "Fastmap: A fast algorithm for indexing, data-mining and visualization of traditional and multimedia", "author": ["Christos Faloutsos", "King-Ip Lin"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1995}, {"title": "FastMap: A fast algorithm for indexing, data-mining and visualization of traditional and multimedia datasets, volume", "author": ["Christos Faloutsos", "King-Ip Lin"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Practical methods of optimization", "author": ["Roger Fletcher"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Revisiting the impact of classification techniques on the performance of defect prediction models", "author": ["B. Ghotra", "S. McIntosh", "A.E. Hassan"], "venue": "IEEE International Conference on Software Engineering,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Measuring the strangeness of strange attractors", "author": ["Peter Grassberger", "Itamar Procaccia"], "venue": "In The Theory of Chaotic Attractors,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Variability-aware performance prediction: A statistical learning approach", "author": ["Jianmei Guo", "Krzysztof Czarnecki", "Sven Apel", "Norbert Siegmund", "Andrzej Wasowski"], "venue": "In IEEE/ACM 28th International Conference on Automated Software Engineering,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Search-based software engineering: Trends, techniques and applications", "author": ["Mark Harman", "S Afshin Mansouri", "Yuanyuan Zhang"], "venue": "ACM Computing Surveys,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Practical approaches to principal component analysis in the presence of missing values", "author": ["Alexander Ilin", "Tapani Raiko"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Principal component analysis", "author": ["Ian Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Spectral learning", "author": ["Kamvar Kamvar", "Sepandar Sepandar", "Klein Klein", "Dan Dan", "Manning Manning", "Christopher Christopher"], "venue": "In International Joint Conference of Artificial Intelligence. Stanford InfoLab,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Gale: Geometric active learning for search-based software engineering", "author": ["Joseph Krall", "Tim Menzies", "Misty Davies"], "venue": "IEEE Transactions on Software Engineering,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Introduction to combinatorial testing", "author": ["D Richard Kuhn", "Raghu N Kacker", "Yu Lei"], "venue": "CRC press,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Surrogate-assisted evolutionary algorithms", "author": ["Ilya Gennadyevich Loshchilov"], "venue": "PhD thesis,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Local versus global lessons for defect prediction and effort estimation", "author": ["Tim Menzies", "Andrew Butcher", "David Cok", "Andrian Marcus", "Lucas Layman", "Forrest Shull", "Burak Turhan", "Thomas Zimmermann"], "venue": "IEEE Transactions on Software Engineering,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Ranking and Clustering Software Cost Estimation Models through a Multiple Comparisons Algorithm", "author": ["Nikolaos Mittas", "Lefteris Angelis"], "venue": "IEEE Transactions of Software Engineering,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Scikit-learn: Machine learning in python", "author": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Fastmap, Metricmap, and Landmark MDS are all nystrom algorithms. pages 261\u2013268", "author": ["John Platt"], "venue": "Society for Artificial Intelligence and Statistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Optimal Design of Experiments, volume 50", "author": ["Friedrich Pukelsheim"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1993}, {"title": "Cost-efficient sampling for performance prediction of configurable systems", "author": ["Atri Sarkar", "Jianmei Guo", "Norbert Siegmund", "Sven Apel", "Krzysztof Czarnecki"], "venue": "In 30th IEEE/ACM International Conference on Automated Software Engineering,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Scalable product line configuration: A straw to break the camel\u2019s back", "author": ["Abdel Salam Sayyad", "Joe Ingram", "Tim Menzies", "Hany Ammar"], "venue": "In IEEE/ACM 28th International Conference on Automated Software Engineering,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2000}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2000}, {"title": "Views on internal and external validity in empirical software engineering", "author": ["Janet Siegmund", "Norbert Siegmund", "Sven Apel"], "venue": "In Proceedings of the 37th International Conference on Software Engineering,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Performance-influence models for highly configurable systems", "author": ["Norbert Siegmund", "Alexander Grebhahn", "Sven Apel", "Christian K\u00e4stner"], "venue": "In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Predicting performance via automated feature-interaction detection", "author": ["Norbert Siegmund", "Sergiy S Kolesnikov", "Christian K\u00e4stner", "Sven Apel", "Don Batory", "Marko Rosenm\u00fcller", "Gunter Saake"], "venue": "In Proceedings of the 34th International Conference on Software Engineering,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Differential evolution\u2013a simple and efficient heuristic for global optimization over continuous spaces", "author": ["Rainer Storn", "Kenneth Price"], "venue": "Journal of global optimization,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1997}, {"title": "Approximating attack surfaces with stack traces", "author": ["Christopher Theisen", "Kim Herzig", "Patrick Morrison", "Brendan Murphy", "Laurie Williams"], "venue": "In Proceedings of the 37th International Conference on Software Engineering,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "A Critique and Improvement of the CL Common Language Effect Size Statistics of McGraw and Wong", "author": ["Andr\u00e1s Vargha", "Harold D Delaney"], "venue": "Journal of Educational and Behavioral Statistics,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2000}, {"title": "Maximizing classifier utility when there are data acquisition and modeling costs", "author": ["Gary M Weiss", "Ye Tian"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2008}, {"title": "Talwadker. Hey, you have given me too many knobs!: Understanding and dealing with over-designed configuration in system software", "author": ["Tianyin Xu", "Long Jin", "Xuepeng Fan", "Yuanyuan Zhou", "Shankar Pasupathy", "Rukma"], "venue": "In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Cross-project defect prediction using a connectivity-based unsupervised classifier", "author": ["Feng Zhang", "Quan Zheng", "Ying Zou", "Ahmed E. Hassan"], "venue": "In Proceedings of the 38th International Conference on Software Engineering", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Performance prediction of configurable software systems by fourier learning", "author": ["Yi Zhang", "Jianmei Guo", "Eric Blais", "Krzysztof Czarnecki"], "venue": "In 30th IEEE/ACM International Conference on Automated Software Engineering,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Active learning for multi-objective optimization", "author": ["Marcela Zuluaga", "Guillaume Sergent", "Andreas Krause", "Markus P\u00fcschel"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}], "referenceMentions": [{"referenceID": 38, "context": "documented the difficulties developers face with understanding the configuration spaces of their systems [40].", "startOffset": 105, "endOffset": 109}, {"referenceID": 38, "context": "As a result, developers tend to ignore over 5/6ths of the configuration options, which leaves considerable optimization potential untapped and induces major economic cost [40].", "startOffset": 171, "endOffset": 175}, {"referenceID": 14, "context": "Addressing the challenge of performance prediction and optimization in the face of large configuration spaces, researchers have developed a number of approaches that rely on sampling and machine learning [15,29,35].", "startOffset": 204, "endOffset": 214}, {"referenceID": 27, "context": "Addressing the challenge of performance prediction and optimization in the face of large configuration spaces, researchers have developed a number of approaches that rely on sampling and machine learning [15,29,35].", "startOffset": 204, "endOffset": 214}, {"referenceID": 33, "context": "Addressing the challenge of performance prediction and optimization in the face of large configuration spaces, researchers have developed a number of approaches that rely on sampling and machine learning [15,29,35].", "startOffset": 204, "endOffset": 214}, {"referenceID": 14, "context": "For example, prior work on predicting performance scores using regression trees had to compile and execute hundreds to thousands of specific system configurations [15].", "startOffset": 163, "endOffset": 167}, {"referenceID": 33, "context": "is able to learn predictors for configurable systems [35] with low mean errors, but with large variances of prediction accuracy (e.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "[15] also proposed an incremental method to build a predictor model, which uses incremental random samples with steps equal to the number of configuration options (features) of the system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] proposed a projective-learning approach (using fewer measurements than Guo at al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35], Guo et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15], and Sarkar et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] by means of six real-world configurable systems: Berkeley DB, the Apache Web server, SQLite, the LLVM compiler, and the x264 video encoder.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "Other researchers have commented that, in real world scenarios, the cost of acquiring the optimal configuration is overly expensive and time consuming [39].", "startOffset": 151, "endOffset": 155}, {"referenceID": 40, "context": "[42] approximate the configuration space as a Fourier series, after which they can derive an expression showing how many configurations must be studied to build predictive models with a given error.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35].", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": ", for how to incorporate numeric options [34].", "startOffset": 41, "endOffset": 45}, {"referenceID": 14, "context": "[15] proposed a progressive random sampling approach, which samples the configuration space in steps of the number of features of the software system in question.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] proposed a cost model for predicting the effort (or cost) required to generate an accurate predictive model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "For example, in models like the Linux Kernel such an enumeration is practically impossible [30].", "startOffset": 91, "endOffset": 95}, {"referenceID": 18, "context": "Spectral methods have been used before for a variety of data mining applications [20].", "startOffset": 81, "endOffset": 85}, {"referenceID": 3, "context": "Algorithms, such as PDDP [4], use spectral methods, such as principle component analysis (PCA), to recursively divide data into smaller regions.", "startOffset": 25, "endOffset": 28}, {"referenceID": 35, "context": "Softwareanalytics researchers use spectral methods (again, PCA) as a pre-processor prior to data mining to reduce noise in software-related data sets [37].", "startOffset": 150, "endOffset": 154}, {"referenceID": 29, "context": "WHAT is somewhat different from other spectral learners explored in, for instance, image processing applications [31].", "startOffset": 113, "endOffset": 117}, {"referenceID": 16, "context": "Also, a standard spectral method requires an O(N2) matrix multiplication to compute the components of PCA [18].", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "Competitive results can be achieved using an O(2N) analysis that we have developed previously [24], which is based on a heuristic proposed by Faloutsos and Lin [10] (which Platt has shown computes a Nystr\u00f6m approximation to the first component of PCA [27]).", "startOffset": 94, "endOffset": 98}, {"referenceID": 9, "context": "Competitive results can be achieved using an O(2N) analysis that we have developed previously [24], which is based on a heuristic proposed by Faloutsos and Lin [10] (which Platt has shown computes a Nystr\u00f6m approximation to the first component of PCA [27]).", "startOffset": 160, "endOffset": 164}, {"referenceID": 25, "context": "Competitive results can be achieved using an O(2N) analysis that we have developed previously [24], which is based on a heuristic proposed by Faloutsos and Lin [10] (which Platt has shown computes a Nystr\u00f6m approximation to the first component of PCA [27]).", "startOffset": 251, "endOffset": 255}, {"referenceID": 7, "context": "\u2013 It is very fast: This process requires only 2|n| distance comparisons per level of recursion, which is far less than the O(N2) required by PCA [8] or other algorithms such as K-Means [16].", "startOffset": 145, "endOffset": 148}, {"referenceID": 4, "context": "2, we use a CART regression-tree learner [5] to build a performance predictor.", "startOffset": 41, "endOffset": 44}, {"referenceID": 9, "context": "In summary, WHAT combines: \u2013 The FASTMAP method of Faloutsos and Lin [10], which rather than N2 comparisons only performs 2N where N is the number of configurations in the configuration space; \u2013 A spectral-learning algorithm initially inspired by Boley\u2019s PDDP system [4], which we modify by replacing PCA with FASTMAP (called \u201cWHERE\u201d in prior work [24]); \u2013 The sampling policy that explores the leaf clusters found by this recursive division; \u2013 The CART regression-tree learner that converts the data from the samples col-", "startOffset": 69, "endOffset": 73}, {"referenceID": 3, "context": "In summary, WHAT combines: \u2013 The FASTMAP method of Faloutsos and Lin [10], which rather than N2 comparisons only performs 2N where N is the number of configurations in the configuration space; \u2013 A spectral-learning algorithm initially inspired by Boley\u2019s PDDP system [4], which we modify by replacing PCA with FASTMAP (called \u201cWHERE\u201d in prior work [24]); \u2013 The sampling policy that explores the leaf clusters found by this recursive division; \u2013 The CART regression-tree learner that converts the data from the samples col-", "startOffset": 267, "endOffset": 270}, {"referenceID": 22, "context": "In summary, WHAT combines: \u2013 The FASTMAP method of Faloutsos and Lin [10], which rather than N2 comparisons only performs 2N where N is the number of configurations in the configuration space; \u2013 A spectral-learning algorithm initially inspired by Boley\u2019s PDDP system [4], which we modify by replacing PCA with FASTMAP (called \u201cWHERE\u201d in prior work [24]); \u2013 The sampling policy that explores the leaf clusters found by this recursive division; \u2013 The CART regression-tree learner that converts the data from the samples col-", "startOffset": 348, "endOffset": 352}, {"referenceID": 4, "context": "lected by sampling policy into a run-time prediction model [5].", "startOffset": 59, "endOffset": 62}, {"referenceID": 21, "context": "To speed up that process, optimizers can use a surrogate model 5 that mimics the outputs of a system of interest, while being computationally cheap(er) to evaluate [23].", "startOffset": 164, "endOffset": 168}, {"referenceID": 33, "context": "[35], Guo et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15], and Sarkar et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29].", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "For the sake of completeness, we explored a range of optimizers seen in the literature: DE [36], NSGA-II [6], and our own GALE [21, 43] system.", "startOffset": 91, "endOffset": 95}, {"referenceID": 5, "context": "For the sake of completeness, we explored a range of optimizers seen in the literature: DE [36], NSGA-II [6], and our own GALE [21, 43] system.", "startOffset": 105, "endOffset": 108}, {"referenceID": 19, "context": "For the sake of completeness, we explored a range of optimizers seen in the literature: DE [36], NSGA-II [6], and our own GALE [21, 43] system.", "startOffset": 127, "endOffset": 135}, {"referenceID": 41, "context": "For the sake of completeness, we explored a range of optimizers seen in the literature: DE [36], NSGA-II [6], and our own GALE [21, 43] system.", "startOffset": 127, "endOffset": 135}, {"referenceID": 11, "context": "Normally, it would be reasonable to ask why we used those three, and not the hundreds of other optimizers described in the literature [12, 17].", "startOffset": 134, "endOffset": 142}, {"referenceID": 15, "context": "Normally, it would be reasonable to ask why we used those three, and not the hundreds of other optimizers described in the literature [12, 17].", "startOffset": 134, "endOffset": 142}, {"referenceID": 19, "context": "To answer this question, we ran a random set of 100 configurations, 20 times, and related that baseline to three optimizers (GALE [21], DE [36] and NSGA-II [6]) using their default parameters.", "startOffset": 130, "endOffset": 134}, {"referenceID": 34, "context": "To answer this question, we ran a random set of 100 configurations, 20 times, and related that baseline to three optimizers (GALE [21], DE [36] and NSGA-II [6]) using their default parameters.", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "To answer this question, we ran a random set of 100 configurations, 20 times, and related that baseline to three optimizers (GALE [21], DE [36] and NSGA-II [6]) using their default parameters.", "startOffset": 156, "endOffset": 159}, {"referenceID": 33, "context": "We compare WHAT with the three state-of-the-art predictors proposed in the literature [35], [15], [29], as discussed in Section 2.", "startOffset": 86, "endOffset": 90}, {"referenceID": 14, "context": "We compare WHAT with the three state-of-the-art predictors proposed in the literature [35], [15], [29], as discussed in Section 2.", "startOffset": 92, "endOffset": 96}, {"referenceID": 27, "context": "We compare WHAT with the three state-of-the-art predictors proposed in the literature [35], [15], [29], as discussed in Section 2.", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "The results were studied using nonparametric tests, which was also used by Arcuri and Briand at ICSE \u201911 [25]).", "startOffset": 105, "endOffset": 109}, {"referenceID": 8, "context": "For testing statistical significance, we used non-parametric bootstrap test 95% confidence [9] followed by an A12 test to check that any observed differences were not", "startOffset": 91, "endOffset": 94}, {"referenceID": 36, "context": "5\u2217#(x=y) |X |\u2217|Y | (as per Vargha [38], we say that a \u201csmall\u201d effect has a < 0.", "startOffset": 34, "endOffset": 38}, {"referenceID": 23, "context": "This use of Scott-Knott is endorsed by Mittas and Angelis [25] and by Hassan et al.", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "al [24] demonstrated how to exploit the underlying dimension to cluster data to find local homogeneous data regions in an otherwise heterogeneous data space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "3), which recurses on two dimensions synthesized in linear time using a technique called FASTMAP [11].", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "The use of underlying dimension has been endorsed by various other researchers [1, 2, 7, 41].", "startOffset": 79, "endOffset": 92}, {"referenceID": 1, "context": "The use of underlying dimension has been endorsed by various other researchers [1, 2, 7, 41].", "startOffset": 79, "endOffset": 92}, {"referenceID": 6, "context": "The use of underlying dimension has been endorsed by various other researchers [1, 2, 7, 41].", "startOffset": 79, "endOffset": 92}, {"referenceID": 39, "context": "The use of underlying dimension has been endorsed by various other researchers [1, 2, 7, 41].", "startOffset": 79, "endOffset": 92}, {"referenceID": 17, "context": "There are numerous other methods in the literature, which are used to learn the underlying dimensionality of the data set such as Principal Component Analysis (PCA) [19] 8, Spectral Learning [32] and Random Projection [3].", "startOffset": 165, "endOffset": 169}, {"referenceID": 30, "context": "There are numerous other methods in the literature, which are used to learn the underlying dimensionality of the data set such as Principal Component Analysis (PCA) [19] 8, Spectral Learning [32] and Random Projection [3].", "startOffset": 191, "endOffset": 195}, {"referenceID": 2, "context": "There are numerous other methods in the literature, which are used to learn the underlying dimensionality of the data set such as Principal Component Analysis (PCA) [19] 8, Spectral Learning [32] and Random Projection [3].", "startOffset": 218, "endOffset": 221}, {"referenceID": 13, "context": "To formalize this notion, we borrow the concept of correlation dimension from the domain of physics [14].", "startOffset": 100, "endOffset": 104}, {"referenceID": 24, "context": "For example, how well independent researchers could reproduce the study? To increase external reliability, we took care to either clearly define our algorithms or use implementations from the public domain (SciKitLearn) [26].", "startOffset": 220, "endOffset": 224}, {"referenceID": 31, "context": "Validity refers to the extent to which a piece of research actually investigates what the researcher purports to investigate [33].", "startOffset": 125, "endOffset": 129}, {"referenceID": 27, "context": "\u2013 Our low \u03bc values are consistent with prior work [29]; \u2013 As to our low \u03c3 values, we note that, when the error values are so close to 0 %, the standard deviation of the error is \u201csqueezed\u201d between zero and those errors.", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "Also, we are aware that measurement bias can cause false interpretations [24].", "startOffset": 73, "endOffset": 77}, {"referenceID": 34, "context": "To further strengthen external validity, we run the model (generated by WHAT + S1) against other optimizers, such as NSGA-II and differential evolution [36].", "startOffset": 152, "endOffset": 156}, {"referenceID": 29, "context": "In 2000, Shi and Maik [31] claimed the term \u201cspectral clustering\u201d as a reference to their normalized cuts image segmentation algorithm that partitions data through a spectral (eigenvalue) analysis of the Laplacian representation of the similarity graph between instances in the data.", "startOffset": 22, "endOffset": 26}, {"referenceID": 18, "context": "[20] generalized that definition saying that \u201cspectral learners\u201d were any data-mining algorithm that first replaced the raw dimensions with those inferred from the spectrum (eigenvalues) of the affinity (a.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Regarding sampling, there are a wide range of methods know as experimental designs or designs of experiments [28].", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "They usually rely on fractional factorial designs as in the combinatorial testing community [22].", "startOffset": 92, "endOffset": 96}, {"referenceID": 32, "context": "Furthermore, there is a recent approach that learns performance-influence models for configurable software systems [34].", "startOffset": 115, "endOffset": 119}, {"referenceID": 33, "context": "While this approach can handle even numeric features, it has similar sampling techniques for the Boolean features as reported in their earlier work [35].", "startOffset": 148, "endOffset": 152}, {"referenceID": 33, "context": "[35] and Guo et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Furthermore, we achieve a similar prediction accuracy and stability as the approach by Sarkar et al [29], while requiring a far smaller number of configurations to be measured.", "startOffset": 100, "endOffset": 104}], "year": 2017, "abstractText": "Despite the huge spread and economical importance of configurable software systems, there is unsatisfactory support in utilizing the full potential of these systems with respect to finding performance-optimal configurations. Prior work on predicting the performance of software configurations suffered from either (a) requiring far too many sample configurations or (b) large variances in their predictions. Both these problems can be avoided using the WHAT spectral learner. WHAT\u2019s innovation is the use of the spectrum (eigenvalues) of the distance matrix between the configurations of a configurable software system, to perform dimensionality reduction. Within that reduced configuration space, many closely associated configurations can be studied by executing only a few sample configurations. For the subject systems studied here, a few dozen samples yield accurate and stable predictors\u2014less than 10 % prediction error, with a standard deviation of less than 2 %. When compared to the state of the art, WHAT (a) requires 2 to 10 times fewer samples to achieve similar prediction accuracies, and (b) its predictions are more stable (i.e., have lower standard deviation). Furthermore, we demonstrate that predictive models generated by WHAT can be used by optimizers to discover system configurations that closely approach the optimal performance.", "creator": "LaTeX with hyperref package"}}}