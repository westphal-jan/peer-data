{"id": "1701.08118", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis", "abstract": "Some users of social media are spreading racist, sexist, and otherwise hateful content. For the purpose of training a hate speech detection system, the reliability of the annotations is crucial, but there is no universally agreed-upon definition. We collected potentially hateful messages and asked two groups of internet users to determine whether they were hate speech or not, whether they should be banned or not and to rate their degree of offensiveness. One of the groups was shown a definition prior to completing the survey. We aimed to assess whether hate speech can be annotated reliably, and the extent to which existing definitions are in accordance with subjective ratings. Our results indicate that showing users a definition caused them to partially align their own opinion with the definition but did not improve reliability, which was very low overall. We conclude that the presence of hate speech should perhaps not be considered a binary yes-or-no decision, and raters need more detailed instructions for the annotation.", "histories": [["v1", "Fri, 27 Jan 2017 17:09:07 GMT  (22kb,D)", "http://arxiv.org/abs/1701.08118v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bj\\\"orn ross", "michael rist", "guillermo carbonell", "benjamin cabrera", "nils kurowsky", "michael wojatzki"], "accepted": false, "id": "1701.08118"}, "pdf": {"name": "1701.08118.pdf", "metadata": {"source": "CRF", "title": "Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis", "authors": ["Bj\u00f6rn Ross", "Michael Rist", "Guillermo Carbonell", "Benjamin Cabrera", "Nils Kurowsky", "Michael Wojatzki"], "emails": ["firstname.lastname@uni-due.de"], "sections": [{"heading": "1 Introduction", "text": "In Europe, the current wave of hate speech has been linked to the ongoing refugee crisis. Legislators and social media sites are increasingly aware of the problem and are developing approaches to deal with it, for example by promising to remove illegal messages within 24 hours of they are reported (Titcomb, 2016).This raises the question of how hate speech can be automatically detected, and such an automatic detection method could be used to scan the large amount of text generated on the Internet for hate-filled content and report it to the relevant authorities. It would also make it easier for researchers to investigate the proliferation of hate content through social media on a large scale. From a natural language processing perspective, detection of hate speech can be considered a classification task: determining whether it contains hate speech or not. Training a classifier requires a large amount of data that is unambiguously hate speech."}, {"heading": "2 Hate Speech", "text": "For the purpose of building a classification, Warner and Hirschberg (2012) define hate speech as \"offensive statements that target specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation.\" Recent approaches rely on guidelines such as a tweet that is considered hate speech when it \"uses sexist or racist slurs\" (Waseem and Hovy, 2016), which are similar in that they leave a lot of room for personal interpretation, since there can be differences in what is considered offensive. For example, while the statement \"refugees live off our money\" is clearly generalized and perhaps unfair, it is unclear whether these are already hate speech. More precise definitions from the law are specific to specific legal systems and therefore do not pose a threat to all forms of offensive, hate-filled speech, see e.g. Matsuda (1993). In practice, social media services use their own definitions, which have been subject to adjustments over the years (Jeong, 2016)."}, {"heading": "3 Compiling A Hate Speech Corpus", "text": "As mentioned above, there is no German hate-tweet corpus for our needs, especially not for the recent issue of the refugee crisis in Europe, so we had to put together our own corpus. We used Twitter as a source because it provides up-to-date comments on current events. In our study, we only took into account the textual content of tweets that contain specific keywords and ignored those that contain images or links. This section provides a detailed description of the approach we used to select the tweets and then comment on them. In order to find a large amount of hate-tweets on the refugee crisis, we used 10 hashtags3 that can be used in an offensive or offensive manner. Using these hashtags, we collected a total of 13,766 tweets that roughly date from February to March 2016. However, these tweets contained a lot of non-textual content that we automatically filtered out by removing tweets that consisted solely of links or images that were not considered original tweets, as retweets or replies to tweets, or other tweets that we could only read together, if both were understandable."}, {"heading": "4 Methods", "text": "In order to assess the reliability of definitions of hate speech on social media more broadly, we developed two online surveys, which were conducted by a total of 56 participants (see Table 1), the main objective of which was to examine how non-experts agree on their understanding of hate speech given the diversity of social media content. In the first survey, we used Twitter's definition of hate behavior, which was presented at the beginning and again above each tweet, and the second survey did not include a definition. Participants were randomly assigned to one of the two surveys. The surveys consisted of 20 tweets presented in a random order. Three questions were asked for each tweet. Depending on the survey, participants were asked (1) to answer (yes / no) if they considered the tweet definition to be hate speech, either based on the definition or their personal opinion, and were then asked (2) to answer the tweet definition of the faculty (yes / no)."}, {"heading": "5 Preliminary Results and Discussion", "text": "Table 1 shows some summary statistics. To assess whether the definition had any effect, we calculated for each participant the percentage of tweets he considered to be hate speech or for which he recommended a ban, and their mean insult rating, allowing us to compare the two samples for each of the three questions. Preliminary Shapiro-Wilk tests showed that some of the data was not distributed normally. Therefore, we used the Wilcoxon-MannWhitney (WMW) test to compare the three pairs of series.The results are shown in Table 1. Participants who were shown the definition were more likely to indicate that the tweet was banned. In fact, participants in group one very rarely gave different answers to questions one and two (18 out of 500 cases or 3.6%), suggesting that the participants in that group aligned their own opinion with the definition."}, {"heading": "6 Conclusion and Future Work", "text": "This paper describes the origin of our hate speech corpus and provides initial insights into how users differ in identifying hate-filled messages. Our findings suggest that hate speech is a vague concept that requires much better definitions and policies in order to be reliably commented on. Based on these findings, we plan to develop a new coding scheme that includes clear criteria for people to distinguish hate speech from other content. Researchers building a hate speech recognition system may want to collect multiple labels for each tweet and record the results on average. Of course, this approach does not make the original data any more reliable (Krippendorff, 2004). Nevertheless, collecting the opinions of more users provides a more detailed picture of objective (or intersubjective) hate. For the same reason, researchers might consider detection of hate speech as a regression problem, for example, by predicting the level of hate in a message rather than a binary yes or no classification."}, {"heading": "Acknowledgments", "text": "This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under funding number RTG 2167, Research Training Group \"UserCentred Social Media.\""}], "references": [{"title": "Hate Speech, Machine Classification and Statistical Modelling of Information Flows on Twitter: Interpretation and Communication for Policy Decision Making", "author": ["Peter Burnap", "Matthew Leighton Williams."], "venue": "Proceedings of IPP 2014, pages 1\u201318.", "citeRegEx": "Burnap and Williams.,? 2014", "shortCiteRegEx": "Burnap and Williams.", "year": 2014}, {"title": "Hate Speech Detection with Comment Embeddings", "author": ["Nemanja Djuric", "Robin Morris Jing Zhou", "Mihajlo Grbovic", "Vladan Radosavljevic", "Narayan Bhamidipati."], "venue": "ICML 2014, volume 32, pages 1188\u20131196.", "citeRegEx": "Djuric et al\\.,? 2014", "shortCiteRegEx": "Djuric et al\\.", "year": 2014}, {"title": "The History of Twitter\u2019s Rules", "author": ["Sarah Jeong."], "venue": "VICE Motherboard.", "citeRegEx": "Jeong.,? 2016", "shortCiteRegEx": "Jeong.", "year": 2016}, {"title": "Reliability in Content Analysis: Some Common Misconceptions and Recommendations", "author": ["Klaus Krippendorff."], "venue": "HCR, 30(3):411\u2013433.", "citeRegEx": "Krippendorff.,? 2004", "shortCiteRegEx": "Krippendorff.", "year": 2004}, {"title": "Anger on the Internet: the Perceived Value of Rant-Sites", "author": ["Ryan C Martin", "Kelsey Ryan Coyier", "Leah M VanSistine", "Kelly L Schroeder."], "venue": "Cyberpsychology, behavior and social networking, 16(2):119\u2013", "citeRegEx": "Martin et al\\.,? 2013", "shortCiteRegEx": "Martin et al\\.", "year": 2013}, {"title": "Words that Wound - Critical Race Theory, Assaultive Speech, and the First Amendment", "author": ["Mari J Matsuda."], "venue": "Westview Press, New York.", "citeRegEx": "Matsuda.,? 1993", "shortCiteRegEx": "Matsuda.", "year": 1993}, {"title": "Analyzing the Targets of Hate in Online Social Media", "author": ["Leandro Silva", "Mainack Mondal", "Denzil Correa", "Fabr\u0131\u0301cio Benevenuto", "Ingmar Weber"], "venue": "In Proceedings of ICWSM", "citeRegEx": "Silva et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2016}, {"title": "Facebook and Twitter promise to crack down on internet hate speech", "author": ["James Titcomb."], "venue": "The Telegraph.", "citeRegEx": "Titcomb.,? 2016", "shortCiteRegEx": "Titcomb.", "year": 2016}, {"title": "Detecting Hate Speech on the World Wide Web", "author": ["William Warner", "Julia Hirschberg."], "venue": "Proceedings of LSM 2012, pages 19\u201326. ACL.", "citeRegEx": "Warner and Hirschberg.,? 2012", "shortCiteRegEx": "Warner and Hirschberg.", "year": 2012}, {"title": "Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter", "author": ["Zeerak Waseem", "Dirk Hovy."], "venue": "Proceedings of NAACL-HLT, pages 88\u201393.", "citeRegEx": "Waseem and Hovy.,? 2016", "shortCiteRegEx": "Waseem and Hovy.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "Lawmakers and social media sites are increasingly aware of the problem and are developing approaches to deal with it, for example promising to remove illegal messages within 24 hours after they are reported (Titcomb, 2016).", "startOffset": 207, "endOffset": 222}, {"referenceID": 8, "context": "The reliability of the human annotations is essential, both to ensure that the algorithm can accurately learn the characteristics of hate speech, and as an upper bound on the expected performance (Warner and Hirschberg, 2012; Waseem and Hovy, 2016).", "startOffset": 196, "endOffset": 248}, {"referenceID": 9, "context": "The reliability of the human annotations is essential, both to ensure that the algorithm can accurately learn the characteristics of hate speech, and as an upper bound on the expected performance (Warner and Hirschberg, 2012; Waseem and Hovy, 2016).", "startOffset": 196, "endOffset": 248}, {"referenceID": 9, "context": "More recent approaches rely on lists of guidelines such as a tweet being hate speech if it \u201cuses a sexist or racial slur\u201d (Waseem and Hovy, 2016).", "startOffset": 122, "endOffset": 145}, {"referenceID": 2, "context": "In practice, social media services are using their own definitions which have been subject to adjustments over the years (Jeong, 2016).", "startOffset": 121, "endOffset": 134}, {"referenceID": 4, "context": "Matsuda (1993). In practice, social media services are using their own definitions which have been subject to adjustments over the years (Jeong, 2016).", "startOffset": 0, "endOffset": 15}, {"referenceID": 0, "context": "It can be a precursor and incentive for hate crimes, and it can be so severe that it can even be a health issue (Burnap and Williams, 2014).", "startOffset": 112, "endOffset": 139}, {"referenceID": 4, "context": "It is also known that hate speech does not only mirror existing opinions in the reader but can also induce new negative feelings towards its targets (Martin et al., 2013).", "startOffset": 149, "endOffset": 170}, {"referenceID": 1, "context": "(Djuric et al., 2014; Burnap and Williams, 2014; Silva et al., 2016) \u2013 but also as a problem to deal with in politics such as the No Hate Speech Movement by the Council of Europe.", "startOffset": 0, "endOffset": 68}, {"referenceID": 0, "context": "(Djuric et al., 2014; Burnap and Williams, 2014; Silva et al., 2016) \u2013 but also as a problem to deal with in politics such as the No Hate Speech Movement by the Council of Europe.", "startOffset": 0, "endOffset": 68}, {"referenceID": 6, "context": "(Djuric et al., 2014; Burnap and Williams, 2014; Silva et al., 2016) \u2013 but also as a problem to deal with in politics such as the No Hate Speech Movement by the Council of Europe.", "startOffset": 0, "endOffset": 68}, {"referenceID": 3, "context": "(Krippendorff, 2004)", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "problematic (Krippendorff, 2004).", "startOffset": 12, "endOffset": 32}, {"referenceID": 3, "context": "Of course this approach does not make the original data any more reliable (Krippendorff, 2004).", "startOffset": 74, "endOffset": 94}], "year": 2017, "abstractText": "Some users of social media are spreading racist, sexist, and otherwise hateful content. For the purpose of training a hate speech detection system, the reliability of the annotations is crucial, but there is no universally agreed-upon definition. We collected potentially hateful messages and asked two groups of internet users to determine whether they were hate speech or not, whether they should be banned or not and to rate their degree of offensiveness. One of the groups was shown a definition prior to completing the survey. We aimed to assess whether hate speech can be annotated reliably, and the extent to which existing definitions are in accordance with subjective ratings. Our results indicate that showing users a definition caused them to partially align their own opinion with the definition but did not improve reliability, which was very low overall. We conclude that the presence of hate speech should perhaps not be considered a binary yes-or-no decision, and raters need more detailed instructions for the annotation.", "creator": "TeX"}}}