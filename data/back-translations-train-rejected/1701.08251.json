{"id": "1701.08251", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jan-2017", "title": "Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation", "abstract": "The popularity of image sharing on social media reflects the important role visual context plays in everyday conversation. In this paper, we present a novel task, Image-Grounded Conversations (IGC), in which natural-sounding conversations are generated about shared photographic images. We investigate this task using training data derived from image-grounded conversations on social media and introduce a new dataset of crowd-sourced conversations for benchmarking progress. Experiments using deep neural network models trained on social media data show that the combination of visual and textual context can enhance the quality of generated conversational turns. In human evaluation, a gap between human performance and that of both neural and retrieval architectures suggests that IGC presents an interesting challenge for vision and language research.", "histories": [["v1", "Sat, 28 Jan 2017 05:06:11 GMT  (244kb,D)", "http://arxiv.org/abs/1701.08251v1", null], ["v2", "Thu, 20 Apr 2017 00:36:35 GMT  (1132kb,D)", "http://arxiv.org/abs/1701.08251v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["nasrin mostafazadeh", "chris brockett", "bill dolan", "michel galley", "jianfeng gao", "georgios p spithourakis", "lucy vanderwende"], "accepted": false, "id": "1701.08251"}, "pdf": {"name": "1701.08251.pdf", "metadata": {"source": "CRF", "title": "Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation", "authors": ["Nasrin Mostafazadeh", "Chris Brockett", "Bill Dolan", "Michel Galley", "Jianfeng Gao", "Georgios P. Spithourakis", "Lucy Vanderwende"], "emails": ["nasrinm@cs.rochester.edu,", "chrisbkt@microsoft.com"], "sections": [{"heading": null, "text": "The popularity of image sharing on social media reflects the important role that visual context plays in everyday conversation. In this paper, we introduce a new task, ImageGrounded Conversations (IGC), in which natural-sounding conversations about shared photographic images are generated. We examine this task using training data from image-based conversations on social media, and present a new dataset of crowd-sourced conversations to benchmark progress. Experiments with deep neural network models trained on social media data show that the combination of visual and textual context can improve the quality of generated conversations. In human assessment, a gap between human performance and that of neural and real architectures suggests that IGC presents an interesting challenge for vision and language research."}, {"heading": "1 Introduction", "text": "In recent years, we have shown that we are able to get to grips with problems, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by solving them, by putting them in check, by putting them in check, by pointing them out, by pointing them out, by pointing them out, by opening them, by opening them, by opening them, by opening them, by opening them, by opening them, by opening them, by opening them, by opening them, by opening them, by opening them, by opening them, by opening them, by opening them, by opening them, by opening them, by opening them, by opening them."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Vision and Language", "text": "When trained on large datasets such as the COCO dataset (Lin et al., 2014), visual features combined with voice modeling performed well both in caption (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) and visual response (VQA) (Antol et al., 2015) and (Malinowski and Fritz, 2014). In the VQA, questions must be able to be answered from the image, i.e. they could be asked by a person who cannot see the image. The et al. (2016) extend the VQA scenario by collecting questions from people who are only shown an automatically generated caption, not the image itself, and show that system performance is improved by treating questions in a dialogue as a series rather than as separate QA pairs."}, {"heading": "2.2 Data-Driven Conversational Modeling", "text": "Ritter et al. (2011) presented response generation as a machine translation task and learned conversations from parallel pairs of messages and responses found in social media. Their work was successfully expanded to include the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most likely response based on the conversation history (i.e., a text-only context). In this paper, we expand the contextual approach to include multimodal functions to develop models that are able to ask questions on topics of interest to a human being and that could enable a conversation partner to proactively advance the conversation."}, {"heading": "3 Image-Grounded Conversations", "text": "For current purposes, we define the scope of IGC as the following two consecutive conversation steps: \u2022 Question Generation: Faced with a visual context I and a textual context T (e.g. the first statement in Figure 1), generate a coherent, natural question Q about the image as the second utterance in conversation. As seen in Figure 1, the question cannot be answered directly from the image. \u2022 Answer Generation: Faced with a visual context I, a textual context T, and a question Q, generate a coherent, natural answer R to the question as the third utterance in conversation. The answer may be an answer as expected in VQA or visual dialogue tasks, or it may be a comment, distraction, or some other type of answer. Current work does not attempt to generate time to generate answers from generated questions, a task that we leave to future work. 4 Data Collection 4.1 IGCTwitter Previous work in neural conversation modeling (Knight et al, 2010; we have used Sordoni)."}, {"heading": "4.2 IGCCrowd", "text": "To enable benchmarking of progress in the ICG task, we constructed test and validation datasets with tighter controlled parameters based on the VQG dataset (Mostafazadeh et al., 2016a). We developed a crowdsourcing platform based on Turkserver (Mao et al., 2012) that enables synchronous and real-time interaction between crowd workers on Amazon Mechanical Turk (Mturk). Several workers wait in a virtual lobby to be paired with another worker who will be their interlocutor. After pairing, one of the users selects an image from a large photo gallery, whereupon the two users enter a chat window in which they have a brief conversation about the selected image. Images were scanned from the VQG dataset by queriing a search engine using event-centric query terms that \"event\" and \"process\" crowdsourced conversations result from crowd-sourced events (i.e., crowd-sourced events often occurring in IGG contexts)."}, {"heading": "4.3 Dataset Characteristics", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5 Models", "text": "We use the VGGNet architecture (Simonyan and Zisserman, 2014) to calculate deep convolutionary image characteristics. We primarily use the 4096-dimensional output of the last fully connected layer (fc7) as input for all models that respond sensitively to the visual context.2Minsky defines \"frame\" as follows: \"When you encounter a new situation, you choose from memory a structure called frame.\" According to Minsky, a frame is a data structure used to represent stereotypical situations, such as a wedding ceremony. Minsky also associates frames with the nature of questions: \"[AFrame] is a collection of questions that are asked about a situation.\" These questions can be asked about the cause, intention, or side effects of a situation depicted. 3In 17% of cases, we could not find a corresponding QFN slot in FrameNet."}, {"heading": "5.1 Generation Models", "text": "The conversation is based on the initial conversation in Table 3.Visual Context Sensitive Model (V-Gen). Similar to the recurrent Neural Network (RNN) caption models (Devlin et al., 2015; Vinyals et al., 2015), (V-Gen), the image feature transforms vector into a 500-dimensional vector that is the first recursive state to be generated into a 500-dimensional single layer gated recurrent unit (GRU) that is the decoder module. The output set is generated at a specific time until the < EOS > (end-of-of-sentence) token is generated. We set the vocabulary size to 6000. Unknown words are mapped to a < UNK > token during training, which is not allowed to decode time.Textual Context Sensitive Model (T-Gen)."}, {"heading": "5.2 Retrieval Models", "text": "In addition to Generation V, we have implemented two retrieval models that are tailored to the tasks of question and answer generation. Vision and language work has shown the effectiveness of retrieval models that use the annotation (e.g. caption) of a nearby neighbor in the training image to comment on a specific test image (Mostafazadeh et al., 2016a; Devlin et al., 2015; Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010).Visual Context Sensitive Model (V-Ret). This model uses only the given image for retrieval. First, we find a set of K nearest training images for the given test image based on cosmic similarity of the fc7 vision function vectors. Then we call up the oseK annotation model as our pool of K candidates. Finally, we calculate the textual similarity between the questions in the pool of a soft resemblance of the cc7 vision characteristics."}, {"heading": "6 Evaluation Setup", "text": "We provide both human and automatic evaluations for our questions and answers. We base our human evaluation on an AMT-like crowdsourcing system by asking seven crowdworkers to rate the quality of candidate questions or answers on a three-tiered Likert-like scale from 1 to 3 (the highest). To ensure calibrated evaluation, we show human judges all system hypotheses for a given test case at the same time. System expenditures were randomly arranged to prevent judges from guessing which systems were which based on the position. After collecting judgments, we averaged the values throughout the test set for each model. As spammers, we dismissed all commentators whose ratings deviated from the mean by more than 2 standard deviations. Although human evaluation is preferable and is currently indispensable, it is useful for open domain generation tasks with intrinsically different results to have an automatic metric for daily evaluation."}, {"heading": "7 Experimental Results", "text": "We are experimenting with all the models presented in Section 5. To answer this question, we are using a visual & textual sensitive model that uses sack-of-words (V & T.BOW gene) to represent the textual context that produces better results. Previous vision & language work such as VQA (Antol et al., 2015) has shown that a sack-of-words baseline provides LSTM-based models for the representation of textual characteristics (Zhou et al., which are needed for textual input of two phrases, we are using the V & T.RNN gene model as the visual and textual input."}, {"heading": "8 Conclusions", "text": "To support this task, we provide the research community with a crowdsourced dataset of 4,222 high-quality conversations about eventful images, each with up to 6 twists and multiple references. Our experiments provide evidence that capturing multimodal relationships improves the quality of generation, and the gap between the performance of our best models and humans opens up further research opportunities in the continuum, from occasional conversations to more task-and topic-oriented vision and speech dialogue. We also expect that the performance of systems can be further improved by adding other types of grounding."}], "references": [{"title": "VQA: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "International Conference on Computer Vision (ICCV).", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "The berkeley framenet project", "author": ["Collin F. Baker", "Charles J. Fillmore", "John B. Lowe."], "venue": "Proceedings of the 17th International Conference on Computational Linguistics - Volume 1, COLING \u201998, pages 86\u201390, Stroudsburg, PA, USA. Association for Com-", "citeRegEx": "Baker et al\\.,? 1998", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "D\u00e9j\u00e0 image-captions: A corpus of expressive descriptions in repetition", "author": ["Jianfu Chen", "Polina Kuznetsova", "David Warren", "Yejin Choi."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Visual dialog", "author": ["Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "Jos\u00e9 M.F. Moura", "Devi Parikh", "Dhruv Batra."], "venue": "CoRR, abs/1611.08669.", "citeRegEx": "Das et al\\.,? 2016", "shortCiteRegEx": "Das et al\\.", "year": 2016}, {"title": "Language models for image captioning: The quirks and what works", "author": ["Jacob Devlin", "Hao Cheng", "Hao Fang", "Saurabh Gupta", "Li Deng", "Xiaodong He", "Geoffrey Zweig", "Margaret Mitchell."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Com-", "citeRegEx": "Devlin et al\\.,? 2015", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell."], "venue": "CoRR, abs/1411.4389.", "citeRegEx": "Donahue et al\\.,? 2014", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["Hao Fang", "Saurabh Gupta", "Forrest N. Iandola", "Rupesh Srivastava", "Li Deng", "Piotr Doll\u00e1r", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C. Platt", "C. Lawrence Zitnick", "Geoffrey Zweig."], "venue": "CoRR,", "citeRegEx": "Fang et al\\.,? 2014", "shortCiteRegEx": "Fang et al\\.", "year": 2014}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth."], "venue": "Proceedings of the 11th European Conference on Computer Vision:", "citeRegEx": "Farhadi et al\\.,? 2010", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier."], "venue": "J. Artif. Int. Res., 47(1):853\u2013899, May.", "citeRegEx": "Hodosh et al\\.,? 2013", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Tuning as ranking", "author": ["Mark Hopkins", "Jonathan May."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352\u20131362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.", "citeRegEx": "Hopkins and May.,? 2011", "shortCiteRegEx": "Hopkins and May.", "year": 2011}, {"title": "Visual storytelling", "author": ["Mitchell."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1233\u20131239, San Diego, California, June. Association for Computa-", "citeRegEx": "Mitchell.,? 2016", "shortCiteRegEx": "Mitchell.", "year": 2016}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational", "citeRegEx": "Li et al\\.,? 2016a", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Li et al\\.,? 2016b", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics", "author": ["Chin-Yew Lin", "Franz Josef Och."], "venue": "Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics, ACL \u201904, Strouds-", "citeRegEx": "Lin and Och.,? 2004", "shortCiteRegEx": "Lin and Och.", "year": 2004}, {"title": "Microsoft COCO: Common Objects in Context, pages 740\u2013755", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C. Lawrence Zitnick"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "A multiworld approach to question answering about realworld scenes based on uncertain input", "author": ["Mateusz Malinowski", "Mario Fritz."], "venue": "Advances in Neural Information Processing Systems 27, pages 1682\u20131690.", "citeRegEx": "Malinowski and Fritz.,? 2014", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2014}, {"title": "Turkserver: Enabling synchronous and longitudinal online experiments", "author": ["Andrew Mao", "David Parkes", "Yiling Chen", "Ariel D. Procaccia", "Krzysztof Z. Gajos", "Haoqi Zhang."], "venue": "Workshop on Human Computation (HCOMP).", "citeRegEx": "Mao et al\\.,? 2012", "shortCiteRegEx": "Mao et al\\.", "year": 2012}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller."], "venue": "Commun. ACM, 38(11):39\u201341, November.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "A framework for representing knowledge", "author": ["Marvin Minsky."], "venue": "Technical report, Cambridge, MA, USA.", "citeRegEx": "Minsky.,? 1974", "shortCiteRegEx": "Minsky.", "year": 1974}, {"title": "with most of it being pictures now, I rarely use it\u201d: Understanding twitter\u2019s evolving accessibility to blind users", "author": ["Meredith Ringel Morris", "Annuska Zolyomi", "Catherine Yao", "Sina Bahram", "Jeffrey P. Bigham", "Shaun K. Kane."], "venue": "Jofish Kaye, Allison Druin,", "citeRegEx": "Morris et al\\.,? 2016", "shortCiteRegEx": "Morris et al\\.", "year": 2016}, {"title": "Generating natural questions about an image", "author": ["Nasrin Mostafazadeh", "Ishan Misra", "Jacob Devlin", "Margaret Mitchell", "Xiaodong He", "Lucy Vanderwende."], "venue": "Proceedings of the Annual Meeting on Association for Computational Linguistics, ACL \u201916. Association", "citeRegEx": "Mostafazadeh et al\\.,? 2016a", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "Generating natural questions about an image", "author": ["Nasrin Mostafazadeh", "Ishan Misra", "Jacob Devlin", "Margaret Mitchell", "Xiaodong He", "Lucy Vanderwende."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:", "citeRegEx": "Mostafazadeh et al\\.,? 2016b", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L. Berg."], "venue": "Neural Information Processing Systems (NIPS).", "citeRegEx": "Ordonez et al\\.,? 2011", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318, Strouds-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "The TIMEBANK corpus", "author": ["J. Pustejovsky", "P. Hanks", "R. Sauri", "A. See", "R. Gaizauskas", "A. Setzer", "D. Radev", "B. Sundheim", "D. Day", "L. Ferro", "M. Lazo."], "venue": "Proceedings of Corpus Linguistics 2003, pages 647\u2013 656, Lancaster, March.", "citeRegEx": "Pustejovsky et al\\.,? 2003", "shortCiteRegEx": "Pustejovsky et al\\.", "year": 2003}, {"title": "Question relevance in VQA: identifying non-visual and false-premise questions", "author": ["Arijit Ray", "Gordon Christie", "Mohit Bansal", "Dhruv Batra", "Devi Parikh."], "venue": "Jian Su, Xavier Carreras, and Kevin Duh, editors, Proceedings of the 2016 Conference on Empirical Meth-", "citeRegEx": "Ray et al\\.,? 2016", "shortCiteRegEx": "Ray et al\\.", "year": 2016}, {"title": "Unsupervised modeling of twitter conversations", "author": ["Alan Ritter", "Colin Cherry", "Bill Dolan."], "venue": "In HLT-NAACL.", "citeRegEx": "Ritter et al\\.,? 2010", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 583\u2013593. Association for Computational Linguistics.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "A database for fine grained activity detection of cooking activities", "author": ["Marcus Rohrbach", "Sikandar Amin", "Mykhaylo Andriluka", "Bernt Schiele."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, IEEE, June.", "citeRegEx": "Rohrbach et al\\.,? 2012", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2012}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1507.04808.", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "ACL-IJCNLP.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman."], "venue": "CoRR, abs/1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of the", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal,", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Saenko."], "venue": "Proceedings the 2015 Conference of the North American Chapter of the Association for Computational Linguistics \u2013 Human Language Technologies (NAACL HLT 2015),", "citeRegEx": "Saenko.,? 2015", "shortCiteRegEx": "Saenko.", "year": 2015}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "Deep Learning Workshop, ICML.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Computer Vision and Pattern Recognition.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Simple baseline for visual question answering", "author": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus."], "venue": "CoRR, abs/1512.02167.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Significant advances in image captioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) have enabled much interdisciplinary research in vision and language, from video transcription (Rohrbach et al.", "startOffset": 41, "endOffset": 120}, {"referenceID": 7, "context": "Significant advances in image captioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) have enabled much interdisciplinary research in vision and language, from video transcription (Rohrbach et al.", "startOffset": 41, "endOffset": 120}, {"referenceID": 6, "context": "Significant advances in image captioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) have enabled much interdisciplinary research in vision and language, from video transcription (Rohrbach et al.", "startOffset": 41, "endOffset": 120}, {"referenceID": 2, "context": "Significant advances in image captioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) have enabled much interdisciplinary research in vision and language, from video transcription (Rohrbach et al.", "startOffset": 41, "endOffset": 120}, {"referenceID": 29, "context": ", 2015) have enabled much interdisciplinary research in vision and language, from video transcription (Rohrbach et al., 2012; Venugopalan et al., 2015), to answering questions about images (Antol et al.", "startOffset": 102, "endOffset": 151}, {"referenceID": 0, "context": ", 2015), to answering questions about images (Antol et al., 2015; Malinowski and Fritz, 2014), to storytelling around series of photos (Huang et al.", "startOffset": 45, "endOffset": 93}, {"referenceID": 16, "context": ", 2015), to answering questions about images (Antol et al., 2015; Malinowski and Fritz, 2014), to storytelling around series of photos (Huang et al.", "startOffset": 45, "endOffset": 93}, {"referenceID": 21, "context": "Visual Question Generation (VQG) (Mostafazadeh et al., 2016a) attempts to address the challenge of how to generate questions that involve such commonsense understanding of image content.", "startOffset": 33, "endOffset": 61}, {"referenceID": 15, "context": "When trained on large datasets, such as the COCO dataset (Lin et al., 2014), Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al.", "startOffset": 57, "endOffset": 75}, {"referenceID": 5, "context": ", 2014), Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) and in Visual Question Answering (VQA) (Antol et al.", "startOffset": 110, "endOffset": 172}, {"referenceID": 7, "context": ", 2014), Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) and in Visual Question Answering (VQA) (Antol et al.", "startOffset": 110, "endOffset": 172}, {"referenceID": 6, "context": ", 2014), Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) and in Visual Question Answering (VQA) (Antol et al.", "startOffset": 110, "endOffset": 172}, {"referenceID": 0, "context": ", 2014) and in Visual Question Answering (VQA) (Antol et al., 2015) and (Malinowski and Fritz, 2014).", "startOffset": 47, "endOffset": 67}, {"referenceID": 16, "context": ", 2015) and (Malinowski and Fritz, 2014).", "startOffset": 12, "endOffset": 40}, {"referenceID": 26, "context": "(Ray et al., 2016) refine VQA by modeling whether the image contains enough information to answer the question; they observe that a model that can comment on the answerability of the question is preferable to a system that always answers.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": ", 2014) and in Visual Question Answering (VQA) (Antol et al., 2015) and (Malinowski and Fritz, 2014). In VQA, questions are constrained to be answerable from the image, i.e., they might be asked by a person who cannot see the image. Das et al. (2016) extend the VQA scenario by collecting questions from people who are shown only an automatically generated caption, not the image itself, and demonstrate that system performance is improved by treating questions as a series in a dialog rather than separate QA pairs.", "startOffset": 48, "endOffset": 251}, {"referenceID": 0, "context": ", 2014) and in Visual Question Answering (VQA) (Antol et al., 2015) and (Malinowski and Fritz, 2014). In VQA, questions are constrained to be answerable from the image, i.e., they might be asked by a person who cannot see the image. Das et al. (2016) extend the VQA scenario by collecting questions from people who are shown only an automatically generated caption, not the image itself, and demonstrate that system performance is improved by treating questions as a series in a dialog rather than separate QA pairs. This form of dialog is best considered a simple one-sided QA exchange, in which only humans can ask questions and the system can only provide answers. (Ray et al., 2016) refine VQA by modeling whether the image contains enough information to answer the question; they observe that a model that can comment on the answerability of the question is preferable to a system that always answers. Mostafazadeh et al. (2016a) introduce the task of visual question generation (VQG), in which the system itself outputs questions about the image.", "startOffset": 48, "endOffset": 935}, {"referenceID": 0, "context": ", 2014) and in Visual Question Answering (VQA) (Antol et al., 2015) and (Malinowski and Fritz, 2014). In VQA, questions are constrained to be answerable from the image, i.e., they might be asked by a person who cannot see the image. Das et al. (2016) extend the VQA scenario by collecting questions from people who are shown only an automatically generated caption, not the image itself, and demonstrate that system performance is improved by treating questions as a series in a dialog rather than separate QA pairs. This form of dialog is best considered a simple one-sided QA exchange, in which only humans can ask questions and the system can only provide answers. (Ray et al., 2016) refine VQA by modeling whether the image contains enough information to answer the question; they observe that a model that can comment on the answerability of the question is preferable to a system that always answers. Mostafazadeh et al. (2016a) introduce the task of visual question generation (VQG), in which the system itself outputs questions about the image. Questions are required to be \u2018natural and engaging\u2019, i.e. a person would find them interesting to answer, and may not be answerable from the image alone. In this work, we build on Mostafazadeh et al. (2016a) by introducing multimodal context when formulating questions and responses.", "startOffset": 48, "endOffset": 1261}, {"referenceID": 33, "context": "Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b).", "startOffset": 77, "endOffset": 198}, {"referenceID": 31, "context": "Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b).", "startOffset": 77, "endOffset": 198}, {"referenceID": 30, "context": "Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b).", "startOffset": 77, "endOffset": 198}, {"referenceID": 36, "context": "Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b).", "startOffset": 77, "endOffset": 198}, {"referenceID": 12, "context": "Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b).", "startOffset": 77, "endOffset": 198}, {"referenceID": 13, "context": "Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b).", "startOffset": 77, "endOffset": 198}, {"referenceID": 25, "context": "Ritter et al. (2011) posed the response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media.", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": ", 2015; Vinyals and Le, 2015; Li et al., 2016a; Li et al., 2016b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.", "startOffset": 30, "endOffset": 89}, {"referenceID": 27, "context": "Previous work in neural conversation modeling (Ritter et al., 2010; Sordoni et al., 2015) has successfully used Twitter as the source of millions of natural conversations.", "startOffset": 46, "endOffset": 89}, {"referenceID": 33, "context": "Previous work in neural conversation modeling (Ritter et al., 2010; Sordoni et al., 2015) has successfully used Twitter as the source of millions of natural conversations.", "startOffset": 46, "endOffset": 89}, {"referenceID": 20, "context": "In recent years, uploading a photo along with an accompanying tweet has become increasingly popular: multimedia tweets have risen 15% per year (as of June 2015, 28% total), with 42% of retweets containing non-verbal context (Morris et al., 2016).", "startOffset": 224, "endOffset": 245}, {"referenceID": 21, "context": "To permit benchmarking of progress in the ICG task, we constructed test and validation datasets with more controlled parameters on the basis of the VQG dataset (Mostafazadeh et al., 2016a).", "startOffset": 160, "endOffset": 188}, {"referenceID": 17, "context": "We designed a crowdsourcing platform based on Turkserver (Mao et al., 2012), which enables synchronous and realtime interactions between crowd workers on Amazon Mechanical Turk (Mturk).", "startOffset": 57, "endOffset": 75}, {"referenceID": 18, "context": "Images were sampled from the VQG dataset by querying a search engine using event-centric query terms that aggregated \u2018event\u2019 and \u2018process\u2019 hyponyms in WordNet (Miller, 1995) and using fre-", "startOffset": 159, "endOffset": 173}, {"referenceID": 25, "context": "quent TimeBank events (Pustejovsky et al., 2003).", "startOffset": 22, "endOffset": 48}, {"referenceID": 22, "context": "It has been shown that humans achieve greater consensus on what a natural question to ask given an image (the task of VQG) than on captioning or asking a visually verifiable question (VQA) (Mostafazadeh et al., 2016b).", "startOffset": 189, "endOffset": 217}, {"referenceID": 14, "context": "The right-most plot in Figure 2 compares the inter-annotation textual similarity of our IGCCrowd questions using a smoothed BLEU metric (Lin and Och, 2004).", "startOffset": 136, "endOffset": 155}, {"referenceID": 1, "context": "annotated the FrameNet (Baker et al., 1998) frame evoked by the image (IFN ), and then textual context (TFN ).", "startOffset": 23, "endOffset": 43}, {"referenceID": 32, "context": "We use the VGGNet architecture (Simonyan and Zisserman, 2014) for computing deep convolutional image features.", "startOffset": 31, "endOffset": 61}, {"referenceID": 19, "context": "Minsky defines \u2018frame\u2019 as follows: \u201cWhen one encounters a new situation, one selects from memory a structure called a Frame\u201d (Minsky, 1974).", "startOffset": 125, "endOffset": 139}, {"referenceID": 5, "context": "Similar to Recurrent Neural Network (RNN) models for image captioning (Devlin et al., 2015; Vinyals et al., 2015), (V-Gen) transforms the image feature vector to a 500-dimensional vector that serves as the initial recurrent state to a 500-dimensional one-layer Gated Recurrent Unit (GRU) which is the decoder module.", "startOffset": 70, "endOffset": 113}, {"referenceID": 37, "context": "Similar to Recurrent Neural Network (RNN) models for image captioning (Devlin et al., 2015; Vinyals et al., 2015), (V-Gen) transforms the image feature vector to a 500-dimensional vector that serves as the initial recurrent state to a 500-dimensional one-layer Gated Recurrent Unit (GRU) which is the decoder module.", "startOffset": 70, "endOffset": 113}, {"referenceID": 3, "context": "This is a neural Machine Translation-like model that maps an input sequence to an output sequence (Seq2Seq model (Cho et al., 2014; Sutskever et al., 2014)) using an encoder and a decoder RNN.", "startOffset": 113, "endOffset": 155}, {"referenceID": 34, "context": "This is a neural Machine Translation-like model that maps an input sequence to an output sequence (Seq2Seq model (Cho et al., 2014; Sutskever et al., 2014)) using an encoder and a decoder RNN.", "startOffset": 113, "endOffset": 155}, {"referenceID": 33, "context": "There are various ways to represent conversational history, including a bag of words model, or a concatenation of all textual utterances into one sentence (Sordoni et al., 2015).", "startOffset": 155, "endOffset": 177}, {"referenceID": 14, "context": "We optimize all the parameters of the scoring function towards maximizing the smoothed-BLEU score (Lin and Och, 2004) using the Pairwise Ranking Optimization algorithm (Hopkins and May, 2011).", "startOffset": 98, "endOffset": 117}, {"referenceID": 10, "context": "We optimize all the parameters of the scoring function towards maximizing the smoothed-BLEU score (Lin and Och, 2004) using the Pairwise Ranking Optimization algorithm (Hopkins and May, 2011).", "startOffset": 168, "endOffset": 191}, {"referenceID": 21, "context": ", caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016a; Devlin et al., 2015; Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010).", "startOffset": 90, "endOffset": 204}, {"referenceID": 5, "context": ", caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016a; Devlin et al., 2015; Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010).", "startOffset": 90, "endOffset": 204}, {"referenceID": 9, "context": ", caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016a; Devlin et al., 2015; Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010).", "startOffset": 90, "endOffset": 204}, {"referenceID": 23, "context": ", caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016a; Devlin et al., 2015; Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010).", "startOffset": 90, "endOffset": 204}, {"referenceID": 8, "context": ", caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016a; Devlin et al., 2015; Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010).", "startOffset": 90, "endOffset": 204}, {"referenceID": 14, "context": "Finally, we compute the textual similarity among the questions in the pool according to a Smoothed-BLEU (Lin and Och, 2004) similarity score, then emit the sentence with the highest similarity to the rest of the pool.", "startOffset": 104, "endOffset": 123}, {"referenceID": 24, "context": "For ease of replicability, we use the standard Machine Translation metric, BLEU (Papineni et al., 2002), which captures n-gram overlap between hypotheses and references.", "startOffset": 80, "endOffset": 103}, {"referenceID": 0, "context": "Earlier vision & language work such as VQA (Antol et al., 2015) has shown that a bag-of-words baseline outperforms LSTM-based models for representing textual input when visual features are available (Zhou et al.", "startOffset": 43, "endOffset": 63}, {"referenceID": 38, "context": ", 2015) has shown that a bag-of-words baseline outperforms LSTM-based models for representing textual input when visual features are available (Zhou et al., 2015).", "startOffset": 143, "endOffset": 162}], "year": 2017, "abstractText": "The popularity of image sharing on social media reflects the important role visual context plays in everyday conversation. In this paper, we present a novel task, ImageGrounded Conversations (IGC), in which natural-sounding conversations are generated about shared photographic images. We investigate this task using training data derived from image-grounded conversations on social media and introduce a new dataset of crowd-sourced conversations for benchmarking progress. Experiments using deep neural network models trained on social media data show that the combination of visual and textual context can enhance the quality of generated conversational turns. In human evaluation, a gap between human performance and that of both neural and retrieval architectures suggests that IGC presents an interesting challenge for vision and language research.", "creator": "LaTeX with hyperref package"}}}