{"id": "1701.08303", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jan-2017", "title": "Drug-Drug Interaction Extraction from Biomedical Text Using Long Short Term Memory Network", "abstract": "A drug can affect the activity of other drugs, when administered together, in both synergistic or antagonistic ways. In one hand synergistic effects lead to improved therapeutic outcomes, antagonistic consequences can be life-threatening, leading to increased healthcare cost, or may even cause death. Thus, identification of unknown drug-drug interaction (DDI) is an important concern for efficient and effective healthcare. Although there exist multiple resources for DDI, they often unable to keep pace with rich amount of information available in fast growing biomedical texts including literature. Most existing methods model DDI extraction from text as classification problem and mainly rely on handcrafted features. Some of these features further depends on domain specific tools. Recently neural network models using latent features has shown to be perform similar or better than the other existing models using handcrafted features. In this paper, we present three models namely, B-LSTM, AB-LSTM and Joint AB-LSTM based on long short-term memory (LSTM) network. All three models utilize word and position embedding as latent features and thus do not rely on feature engineering. Further use of bidirectional long short-term memory (Bi-LSTM) networks allow to extract optimal features from the whole sentence. The two models, AB-LSTM and Joint AB-LSTM also use attentive pooling in the output of Bi-LSTM layer to assign weights to features. Our experimental results on the SemEval-2013 DDI extraction dataset shows that the Joint AB-LSTM model outperforms all the existing methods, including those relying on handcrafted features. The other two proposed models also perform competitively with state-of-the-art methods.", "histories": [["v1", "Sat, 28 Jan 2017 17:04:21 GMT  (352kb,D)", "http://arxiv.org/abs/1701.08303v1", "10 pages, 3 figures"], ["v2", "Sun, 13 Aug 2017 12:56:03 GMT  (444kb,D)", "http://arxiv.org/abs/1701.08303v2", "Under review to the Journal of Biomedical Informatics"]], "COMMENTS": "10 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sunil kumar sahu", "ashish anand"], "accepted": false, "id": "1701.08303"}, "pdf": {"name": "1701.08303.pdf", "metadata": {"source": "CRF", "title": "Drug-Drug Interaction Extraction from Biomedical Text Using Long Short Term Memory Network", "authors": ["Sunil Kumar Sahu", "Ashish Anand"], "emails": ["sunil.sahu@iitg.ernet.in", "anand.ashish@iitg.ernet.in", "permissions@acm.org."], "sections": [{"heading": "Keywords", "text": "Information extraction, relapsing neural network, long-term short-term memory, attention model"}, {"heading": "1. INTRODUCTION", "text": "In fact, most people who are in a position to move to another world, to move to another world, to move to another world in which they move, to feel put back into another world, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in fact, in which they, in which they, in fact, in which they, in fact, live, in which they, live, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in fact, live, in which they, in which they, in which they, in fact, in fact, in which it is in fact, in such a fact, in such a fact, it is such a fact, in such a"}, {"heading": "2. MODEL ARCHITECTURE", "text": "We present three LSTM-based models, namely B-LSTM, AB-LSTM and Joint AB-LSTM for the DDI extraction task. We assume that the two targeted drug names are given in one sentence and that the model must either recognize whether there is an interaction or not, or classify them into one of the following categories: counseling, effect, mechanism, int, negative, depending on the task. We describe the tasks in detail in Section 3. The architecture of the three proposed models is shown in Figure 1. Each model uses embedding characteristics as an input into the first layer and learns the representation of fixed lengths by subsequent layers. The evaluation for each possible class is calculated in the last shift and the final decision is made on the basis of this score. The training of the model is such that the correct class receives a high score after the training."}, {"heading": "Feature Layer", "text": "We represent each word in a sentence with three separate characteristics, namely: word (W), distance1 (P1), distance2 (P2). Here, W is exactly the word that occurs in the sentence. P1 is distance (in words) from the first drug name [9, 30]. This value would be zero for the first targeted drug name. P2 is similar to P1, but takes into account the distance from the second targeted drug name. In this way, a word w: D1, D2, D3, where Di is the dictionary of its local characteristics. This feature layer forms the first layer for all models."}, {"heading": "Embedding Layer", "text": "Suppose M i is the embedding matrix for the ith attribute. Here, each column of M i is a vector for the value in the ith attribute. The illustration can be made by taking the product of a hot vector of the attribute value with its embedding matrix [9]. Suppose a (i) j is the only hot vector for the j th attribute value of the ith attribute, then the embedding layer results in: f (i) j = M i.a (i) j (1) xi = f (i) 1-f (i) 2-f (i) 3 (2) This is a concatenation operation, so that xi-R (n1 +.... n3) is a attribute vector for the ith word in the sentence and nk is the dimension of the kth attribute. Precoached word vectors are used for the word embedding matrix and other attribute matrices are initialized randomly."}, {"heading": "Bi-LSTM Layer", "text": "The recurrent neural network is a powerful model for modeling sequential data [19]. It is a network with loop (b) that enables information to be maintained throughout the sequence. However, due to the long sequence, it may suffer from disappearing or exploding gradient problems [10, 20]. LSTM aims to overcome this problem by (gating and memory mechanism). (LSTMlayer is just another way to calculate a hidden state that introduces a new structure called memory cell (ct) and three gates called input (it), output (ot) and forgotten (ft) gates. These gates consist of sigmoid activation function and are responsible for regulating information in memory cell and final output of LSTM \u2212 l is calculated on the basis of new cell states. Consider x1x2..xm is the sequence of the string of a vector and the length of a set containing the xt and the string."}, {"heading": "Pooling Layer", "text": "We are experimenting with two different types of pooling systems: (A) Max Pooling The intuition behind the use of Max Pooling is to derive an optimal feature from word characteristics of variable length. Bi-LSTM collects information both forward and backward, assuming that each node has complete information of the sentence. Max Pooling assumes the maximum over the entire sentence assuming all important and relevant information in this position. Let us let z1z2... zm (zi-RN) be the sequence of vectors obtained after the concatenation of forward, backward LSTM results of each word: z = max 1 \u2264 i (m) [zi] [zi] [zi] max."}, {"heading": "Fully Connected and Softmax", "text": "The output of the pooling layer would be a fixed length vector, which we do not linearize by using tanh activation and then feeding fully connected neural layers. In fully connected layer, we keep the number of nodes equal to the number of classs.h3 = tanh (h2) p (y | x) = Softmax (W oh3 + bo) (6) Here, h2 would be the output of the pooling layer, W o RN \u00b7 C, bo RC are parameters of fully connected neural networks and C is class number in our model. To perform the classification, we use Softmax function in the output of fully connected layers. Softmax will provide normalized probability values for each class."}, {"heading": "Training and Implementation", "text": "All three models use cross-entropy loss functionality for training the entire network. Adam's technique [17] is used for optimization. We use batch size 500 for the entire training in each model. Implementation is done in Python language with the Tensorflow3 package."}, {"heading": "2.1 B-LSTM Model", "text": "B-LSTM is similar to the model for DDI extraction proposed in [18]. Here we use Bi-LSTM instead of the folding neural network used in [18]. As shown in Figure 1a, this model applies max pooling in the output of Bi-LSTM to obtain optimal features of fixed length. In any case, max pooling is obtained by Equation 4. These features are then applied fully connected followed by Softmax layers to obtain a final classification."}, {"heading": "2.2 AB-LSTM Model", "text": "Figure 1b is the illustration of the AB-LSTM model. In this case, we apply attentive pooling in the BiLSTM output. Attention weights are obtained by equation 5 for each set. The output of an attentive pooling layer was used as a feature to create classifiers by feeding these fully connected and Softmax layers."}, {"heading": "2.3 Joint AB-LSTM Model", "text": "The idea of using Joint AB-LSTM is to take advantage of both maximum and attentive pooling. As shown in Figure 1c, Joint AB-LSTM uses two separate modules, each with a Bi-LSTM network. Both Bi-LSTM use the same feature vectors as input and produce output for each word in the sentence. We have applied max pooling on the first and attentive pooling on the second Bi-LSTM layer to get optimal features from both modules."}, {"heading": "3. DATASET DESCRIPTION", "text": "We get the data set from the SemEval2013 joint challenge task-9 [25]. However, this data set contains commented sentences from two sources, Medline abstracts and DrugBank database. MedLine contains biomedical research articles and DrugBank contains documents written by a physician. The data set is provided with the following four types of interactions: 3https: / / www.tensorflow.orgAdvice: The text indicates an opinion or advice mechanism related to the simultaneous use of the two medications, e.g. \"alpha blockers should not be combined with uroxatral.\" Effect: The sentence states the effect of the drug-drug interaction together with pharmacodynamic effects or interaction mechanisms. For example: \"Warfarin users who initiated fluoxetine had an increased risk of hospitalization for gastrointestinal bleeding.\" Mechanism: The sentence describes a pharmacodynamic mechanism of interaction, such as \"paroxetine concentration in endoxetine concentration of 20%.\""}, {"heading": "3.1 Pre-processing", "text": "The following pre-processing is carried out in the data set before it is used in our model: \u2022 Words are tokenised using the genia tagger4 tool. All digits are normalized by replacing them with a special DG token and all letters are changed to lowercase letters. \u2022 The two targeted drug names are replaced by DRUGA or DRUG-B and other drug names in the same set by DRUG-N. Similar strategies have been followed in previous studies [23, 18]."}, {"heading": "3.2 Negative Instance Filtering", "text": "Since we consider all possible pairs of drug names in a set as separate instances for our model, the resulting data set becomes very unbalanced. We have a ratio of 1: 6.9 for positive and negative instances. However, some strategies can be applied to remove negative instances. Previous studies 4http: / / www.nactem.ac.uk / GENIA / tagger / [36, 16, 18] have shown positive effects of negative instance filtering. We filter negative samples based on the following rules: 1. If both targeted drug mentions have the same name, we remove the appropriate instance. Assumption behind this rule is that the drug does not interact with itself. We use string matching on both drug names to identify such cases. 2. If one drug is one type or a special case of the other in a particular case. To identify such cases, we use regular expression by observing patterns in the dataset. \"UDRUA-UG pattern DRB,\" DRG-DRB pattern, \"DRG-DRB pattern,\" DRG-DRG pattern, \"DRG-DRG pattern are removed several such examples."}, {"heading": "4. EXPERIMENT DESIGN", "text": "We train and evaluate our proposed model for the two tasks separately from each other. As already mentioned, all possible pairs with the set represent separate instances / samples if a set contains more than two drug names. We use the same evaluation scheme as in the challenge [25]."}, {"heading": "4.1 Hyperparameters", "text": "In all our proposed models, we use pre-trained Word embedding of 100 dimensions, distance embedding of dimension 10, the size of hidden layers in B-LSTM and AB-LSTM at 200 and 150 for Joint AB-LSTM. We obtain Word embedding with the GloVe tool [21] on a body of PubMed open source articles [31]. We use both l2 regulation and dropout techniques [28] for regulation. We apply dropout only to the output of pooling layers. Different values of regulation parameters are shown in Table 3."}, {"heading": "Task Models Dropout l2 regu.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2 Baseline Methods for comparison", "text": "We compare the performance of the three proposed models with several basic methods, which include approaches based on conventional features, kernel methods 1,2 and neural networks. In the following, we briefly describe the basic methods, where superscript 1 stands for one step and superscript 2 for two steps: \u2022 Linear methods: In this method class, a linear classifier is used to identify the correct class of interaction for each instance. All instances are usually represented by a vector of manually designed features. UTurku1 used Turku Event Extraction System (TEES) [2] for the drug interaction method. The most important features used by TEES are dependency analysis and domain-dependent resources such as MetaMap. UWMTRIADS2 [23] and Kim2 [16] are two-step methods. In both states, SVM used them with textual, contextual, and structural features."}, {"heading": "5. RESULTS AND DISCUSSIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Effect of Negative Instance Filtering", "text": "This year it has come to the point where it will be able to put itself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said."}, {"heading": "5.2 Comparison with Baseline Methods", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move into another world, in which they are able to move into another world, in which they are able to change the world, in which they are able to move, in which they are able to change the world, in which they are able to leave the world, in which they are able to live in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are located, in which they are able to leave the world, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living, in which they are living,"}, {"heading": "5.3 Feature Analysis", "text": "To confirm the importance of each feature, we further analyzed the performance of the Joint AB-LSTM model by gradually removing features. In Table 10, it can be observed that the use of pre-trained word embeddings is an important feature for both task A and task B. In the case of task A, 2.5% and task B, 5.6% of the relative reductions are observed when the model uses random vectors for words instead of pre-trained word embeddings, which clearly shows the importance of word embeddings. On the other hand, the removal of position features did not affect the performance of the model for task A (less than 1% relative change), but for task B there were relative reductions of about 2.6%."}, {"heading": "Tasks Models Precision Recall F Score", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.4 Error vs Sentence Length", "text": "Apart from the problem of imbalance, we try to find out if there is another factor affecting the performance of the models by looking at the average record lengths of correctly and wrongly classified instances for both tasks and for all three proposed models. We observe that the average record length for wrongly classified instances is always high compared to the correctly classified instances. For example, the average record length of correctly (wrongly) classified instances according to Joint AB-LSTM and B-LSTM is 30 (38.84) and 29.79 (39.61), respectively. Similarly, the average record length of correctly (wrongly) classified instances according to Joint ABLSTM and B-LSTM is 29.43 (37.93) and 28.02 (40.04), respectively. This clearly indicates that all models are confused with increasing record length and therefore a better strategy is needed to deal with it."}, {"heading": "5.5 Visual Analysis", "text": "To confirm that the model is able to learn attention weights based on the importance of words, we visualize the attention weights of some sentences after the training Joint AB-LSTM. Figure 2 is the thermal map of attention weights for 6 instances of the test set. Here, each line is a set of two targeted drug names, which are replaced by special symbols DRUG-A and DRUG-B, and darkness in red color signals alertness. Figure shows that our model can select important words based on the task, e.g. in the sentence DRUG-A can enhance the effect of DRUG-B, DRUG-N and other DRUG-N, the model is able to assign high weights to the words increase and effect."}, {"heading": "6. CONCLUSION AND FUTURE WORK", "text": "In this paper, we proposed three LSTM-based models BLSTM, AB-LSTM and Joint AB-LSTM for DDI extraction tasks. All three models use word and distance embedding as a feature and learn to represent higher features through Bi-LSTM network. Two of our proposed models also used neural attention mechanisms to obtain a higher feature representation. To the best of our knowledge, this is the first study to use LSTM and attention mechanisms for DDI extraction tasks. The performance of all three models is also compared with existing methods from the SemEval 2013 DDI extraction dataset. Common AB-LSTM models achieve state-of-the-art for both DDI detection and classification tasks. The performance of the other two models, B-LSTM and AB-LSTM, is also competitive in both tasks. Analysis of the results shows the following important points: Imbalance and noise imbalance affect all models, are likely to predict incorrect interaction classes and are the easiest models to predict for large interaction classes."}, {"heading": "7. ADDITIONAL AUTHORS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8. REFERENCES", "text": "[1] D. Bahdanau, K. Cho, and Y. Bengio. Neural machinetranslation by joint learning to align and translate. CoRR, abs / 1409.0473, 2014. [2] J. Bjo \u00a8 rne, S. Kaewphan, and T. Salakoski. Uturku: drug named entity recognition and drug interaction extraction. [3] T. Bobic, J. Fluck, and M. Hofmann-Apitius. Scai: Extracting drug interactions using a rich feature vector. Atlanta, Georgia, USA, page 675, 2013. [4] B. Bokharaeian and A. D\u0131az. Nil ucm: Extracting drug interactions from text."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Uturku: drug named entity recognition and drug-drug interaction extraction using svm classification and domain knowledge", "author": ["J. Bj\u00f6rne", "S. Kaewphan", "T. Salakoski"], "venue": "In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Scai: Extracting drug-drug interactions using a rich feature vector", "author": ["T. Bobic", "J. Fluck", "M. Hofmann-Apitius"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "D\u0131az. Nil ucm: Extracting drug-drug interactions from text through combination of sequence and tree kernels", "author": ["A.B. Bokharaeian"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research", "author": ["\u00c0. Bravo", "J. Pi\u00f1ero", "N. Queralt-Rosinach", "M. Rautschka", "L.I. Furlong"], "venue": "BMC bioinformatics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Why we need an efficient and careful pharmacovigilance", "author": ["R. Businaro"], "venue": "J Pharmacovigilance,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Exploiting the scope of negations and heterogeneous features for relation extraction: A case study for drug-drug interaction extraction", "author": ["M.F.M. Chowdhury", "A. Lavelli"], "venue": "In HLT-NAACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Fbk-irst: a multi-phase kernel based approach for drug-drug  interaction detection and classification that exploits linguistic information", "author": ["M.F.M. Chowdhury", "A. Lavelli"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Extraction of potential adverse drug events from medical case reports", "author": ["H. Gurulingappa", "A. Mateen-Rajpu", "L. Toldo"], "venue": "Journal of biomedical semantics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Automatic detection of adverse events to predict drug label changes using text and data mining techniques", "author": ["H. Gurulingappa", "L. Toldo", "A.M. Rajput", "J.A. Kors", "A. Taweel", "Y. Tayrouz"], "venue": "Pharmacoepidemiology and drug safety,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Ucolorado som: extraction of drug-drug interactions from biomedical text using knowledge-rich and knowledge-poor features", "author": ["N.D. Hailu", "L.E. Hunter", "K.B. Cohen"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Text mining for adverse drug events: the promise, challenges, and state of the art", "author": ["R. Harpaz", "A. Callahan", "S. Tamang", "Y. Low", "D. Odgers", "S. Finlayson", "K. Jung", "P. LePendu", "N.H. Shah"], "venue": "Drug safety,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Extracting drug\u2013drug interactions from literature using a rich feature-based linear kernel approach", "author": ["S. Kim", "H. Liu", "L. Yeganova", "W.J. Wilbur"], "venue": "Journal of biomedical informatics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Drug-drug interaction extraction via convolutional neural networks", "author": ["S. Liu", "B. Tang", "Q. Chen", "X. Wang"], "venue": "Computational and mathematical methods in medicine,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Understanding the exploding gradient problem", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "CoRR, abs/1211.5063,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Tree kernel-based protein\u00e2\u0102\u015eprotein interaction extraction from biomedical literature", "author": ["L. Qian", "G. Zhou"], "venue": "Journal of Biomedical Informatics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Uwm-triads: classifying drug-drug interactions with two-stage svm and post-processing", "author": ["M. Rastegar-Mojarad", "R.D. Boyce", "R. Prasad"], "venue": "In Proceedings of the 7th International Workshop on Semantic Evaluation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Automatic extraction of relations between medical concepts in clinical texts", "author": ["B. Rink", "S. Harabagiu", "K. Roberts"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts (ddiextraction", "author": ["I. Segura Bedmar", "P. Mart\u0301\u0131nez", "M. Herrero Zazo"], "venue": "Association for Computational Linguistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "The 1st ddiextraction-2011 challenge task: Extraction of drug-drug interactions from biomedical texts", "author": ["I. Segura Bedmar", "P. Martinez", "D. S\u00e1nchez Cisneros"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["R. Socher", "C.C.-Y. Lin", "C.D. Manning", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1929}, {"title": "Extraction of drug-drug interactions by recursive matrix-vector spaces", "author": ["V. Su\u00e1rez-Paniagua", "I. Segura-Bedmar"], "venue": "In 6thInternational Workshop on Combinations of Intelligent Methods and Applications (CIMA 2016),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Relation extraction from clinical texts using domain invariant convolutional neural network", "author": ["K.O.N.G. Sunil Kumar Sahu", "Ashish Anand"], "venue": "In Proceedings of the 15th Workshop on Biomedical Natural Language Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Evaluating distributed word representations for capturing semantics of biomedical concepts", "author": ["M. TH", "S. Sahu", "A. Anand"], "venue": "In Proceedings of BioNLP", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Wbi-ddi: drug-drug interaction extraction using majority voting", "author": ["P. Thomas", "M. Neves", "T. Rockt\u00e4schel", "U. Leser"], "venue": "In Second Joint Conference on Lexical and Computational Semantics (* SEM),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Large-scale automatic extraction of side effects associated with targeted anticancer drugs from full-text oncological articles", "author": ["R. Xu", "Q. Wang"], "venue": "Journal of biomedical informatics,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Filtering big data from social media\u2013building an early warning system for adverse drug reactions", "author": ["M. Yang", "M. Kiang", "W. Shang"], "venue": "Journal of biomedical informatics,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Drug drug interaction extraction from biomedical literature using syntax convolutional neural network. Bioinformatics, page btw486, 2016", "author": ["Z. Zhao", "Z. Yang", "L. Luo", "H. Lin", "J. Wang"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Attention-based bidirectional long short-term memory networks for relation classification", "author": ["P. Zhou", "W. Shi", "J. Tian", "Z. Qi", "B. Li", "H. Hao", "B. Xu"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Adverse drug reaction (ADR) is an example of antagonistic effect causing immense health risks and sometimes even leading to death [6].", "startOffset": 130, "endOffset": 133}, {"referenceID": 24, "context": "Identifying DDIs in text is the process of recognizing how two drugs in a given sentence are related [25].", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": "While SemEval 2011 task [26] was focused on DDI detection assuming that drugs were already recognized, SemEval 2013 task-9 [25] was de-", "startOffset": 24, "endOffset": 28}, {"referenceID": 24, "context": "While SemEval 2011 task [26] was focused on DDI detection assuming that drugs were already recognized, SemEval 2013 task-9 [25] was de-", "startOffset": 123, "endOffset": 127}, {"referenceID": 2, "context": "In 1-stage methods [3, 13, 32], a multi-class classifier is used to map a sentence with two target drugs either into one of the interacting classes or into the negative class.", "startOffset": 19, "endOffset": 30}, {"referenceID": 12, "context": "In 1-stage methods [3, 13, 32], a multi-class classifier is used to map a sentence with two target drugs either into one of the interacting classes or into the negative class.", "startOffset": 19, "endOffset": 30}, {"referenceID": 31, "context": "In 1-stage methods [3, 13, 32], a multi-class classifier is used to map a sentence with two target drugs either into one of the interacting classes or into the negative class.", "startOffset": 19, "endOffset": 30}, {"referenceID": 22, "context": "On the other hand 2-stage methods [23, 2] breaks the problem into 2 steps.", "startOffset": 34, "endOffset": 41}, {"referenceID": 1, "context": "On the other hand 2-stage methods [23, 2] breaks the problem into 2 steps.", "startOffset": 34, "endOffset": 41}, {"referenceID": 7, "context": "In the first category mainly support vector machines (SVMs) with linear or non-linear kernels have been used in many studies [8, 4, 16].", "startOffset": 125, "endOffset": 135}, {"referenceID": 3, "context": "In the first category mainly support vector machines (SVMs) with linear or non-linear kernels have been used in many studies [8, 4, 16].", "startOffset": 125, "endOffset": 135}, {"referenceID": 15, "context": "In the first category mainly support vector machines (SVMs) with linear or non-linear kernels have been used in many studies [8, 4, 16].", "startOffset": 125, "endOffset": 135}, {"referenceID": 11, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 131, "endOffset": 147}, {"referenceID": 10, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 131, "endOffset": 147}, {"referenceID": 13, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 131, "endOffset": 147}, {"referenceID": 32, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 131, "endOffset": 147}, {"referenceID": 33, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 183, "endOffset": 187}, {"referenceID": 21, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 259, "endOffset": 270}, {"referenceID": 4, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 259, "endOffset": 270}, {"referenceID": 23, "context": "Such methods have been successfully used for other similar relation extraction tasks such as ADRs extraction from biomedical texts [12, 11, 14, 33], ADRs extraction from social media [34], and many relations between different biomedical and clinical entities [22, 5, 24].", "startOffset": 259, "endOffset": 270}, {"referenceID": 17, "context": "Some notable studies [18, 36] for the DDT extraction tasks are based on convolution neural networks (CNNs) and have been shown to achieving superior performance than the existing state-of-the-art methods.", "startOffset": 21, "endOffset": 29}, {"referenceID": 34, "context": "Some notable studies [18, 36] for the DDT extraction tasks are based on convolution neural networks (CNNs) and have been shown to achieving superior performance than the existing state-of-the-art methods.", "startOffset": 21, "endOffset": 29}, {"referenceID": 17, "context": "As opposed to works in [18, 36], we use LSTM based neural network models [15] instead of CNN.", "startOffset": 23, "endOffset": 31}, {"referenceID": 34, "context": "As opposed to works in [18, 36], we use LSTM based neural network models [15] instead of CNN.", "startOffset": 23, "endOffset": 31}, {"referenceID": 14, "context": "As opposed to works in [18, 36], we use LSTM based neural network models [15] instead of CNN.", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "Theoretically a Bi-LSTM can preserve information about past and future words while reading [15].", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "P1 is distance (in terms of words) from the first drug name [9, 30].", "startOffset": 60, "endOffset": 67}, {"referenceID": 29, "context": "P1 is distance (in terms of words) from the first drug name [9, 30].", "startOffset": 60, "endOffset": 67}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "Recurrent neural network is a powerful model for modeling sequential data [19].", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "However because of long sequence it may suffer with vanishing or exploding gradient problems [10, 20].", "startOffset": 93, "endOffset": 101}, {"referenceID": 19, "context": "However because of long sequence it may suffer with vanishing or exploding gradient problems [10, 20].", "startOffset": 93, "endOffset": 101}, {"referenceID": 0, "context": "Weights of the feature vectors are computed through attention mechanism which assign weights based on importance of that features [1, 35, 37].", "startOffset": 130, "endOffset": 141}, {"referenceID": 35, "context": "Weights of the feature vectors are computed through attention mechanism which assign weights based on importance of that features [1, 35, 37].", "startOffset": 130, "endOffset": 141}, {"referenceID": 16, "context": "Adam\u2019s technique [17] is used for optimization.", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "B-LSTM is similar to the model proposed in [18] for DDI extraction task.", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "Here we use Bi-LSTM in place of convolution neural network used in [18].", "startOffset": 67, "endOffset": 71}, {"referenceID": 24, "context": "We obtain the dataset from the shared challenge SemEval2013 task-9 [25].", "startOffset": 67, "endOffset": 71}, {"referenceID": 22, "context": "Similar strategies were followed in earlier studies [23, 18] as well.", "startOffset": 52, "endOffset": 60}, {"referenceID": 17, "context": "Similar strategies were followed in earlier studies [23, 18] as well.", "startOffset": 52, "endOffset": 60}, {"referenceID": 34, "context": "uk/GENIA/tagger/ [36, 16, 18] have shown positive impact of negative instance filtering.", "startOffset": 17, "endOffset": 29}, {"referenceID": 15, "context": "uk/GENIA/tagger/ [36, 16, 18] have shown positive impact of negative instance filtering.", "startOffset": 17, "endOffset": 29}, {"referenceID": 17, "context": "uk/GENIA/tagger/ [36, 16, 18] have shown positive impact of negative instance filtering.", "startOffset": 17, "endOffset": 29}, {"referenceID": 17, "context": "Similar to [18] Our rules have not eliminated any positive instances from the test set.", "startOffset": 11, "endOffset": 15}, {"referenceID": 24, "context": "We use the same evaluation scheme as used in the challenge [25].", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "Word embedding are obtained using GloVe tool [21] on a corpus of PubMed open source articles [31].", "startOffset": 45, "endOffset": 49}, {"referenceID": 30, "context": "Word embedding are obtained using GloVe tool [21] on a corpus of PubMed open source articles [31].", "startOffset": 93, "endOffset": 97}, {"referenceID": 27, "context": "We use both l2 regularization and dropout [28] techniques for regularization.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "UTurku used Turku event extraction system (TEES) [2] for drug interaction extraction.", "startOffset": 49, "endOffset": 52}, {"referenceID": 22, "context": "UWMTRIADS [23] and Kim [16] are two stage methods.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "UWMTRIADS [23] and Kim [16] are two stage methods.", "startOffset": 23, "endOffset": 27}, {"referenceID": 6, "context": "WBI-DDI and FBK irst are two stage methods [7, 32] for DDI extraction.", "startOffset": 43, "endOffset": 50}, {"referenceID": 31, "context": "WBI-DDI and FBK irst are two stage methods [7, 32] for DDI extraction.", "startOffset": 43, "endOffset": 50}, {"referenceID": 17, "context": "CNN [18] and SCNN [36] used convolution neural network with max pooling layer to learn higher level discriminative features over entire sentence.", "startOffset": 4, "endOffset": 8}, {"referenceID": 34, "context": "CNN [18] and SCNN [36] used convolution neural network with max pooling layer to learn higher level discriminative features over entire sentence.", "startOffset": 18, "endOffset": 22}, {"referenceID": 28, "context": "MVRNN [29] is also a neural network base model they used recursive neural network [27] for learning embedding of sentence or part of sentence recursively and final vector will use for classification.", "startOffset": 6, "endOffset": 10}, {"referenceID": 26, "context": "MVRNN [29] is also a neural network base model they used recursive neural network [27] for learning embedding of sentence or part of sentence recursively and final vector will use for classification.", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "Linear Methods UTurku [2] 85.", "startOffset": 22, "endOffset": 25}, {"referenceID": 22, "context": "4 UWM-TRIADS [23] 43.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "0 Kim [16] 77.", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "Kernel Methods NIL UCM [4] 60.", "startOffset": 23, "endOffset": 26}, {"referenceID": 31, "context": "7 WBI-DDI [32] 80.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "9 FBK irst [8] 79.", "startOffset": 11, "endOffset": 14}, {"referenceID": 17, "context": "Neural Network CNN [18] 75.", "startOffset": 19, "endOffset": 23}, {"referenceID": 34, "context": "75 SCNN [36] 74.", "startOffset": 8, "endOffset": 12}, {"referenceID": 34, "context": "0 SCNN [36] 77.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "6 MV-RNN [29] 52.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "Models Precision Recall F1 Score CNN [18] 75.", "startOffset": 37, "endOffset": 41}, {"referenceID": 34, "context": "01 SCNN [36] 68.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "UTurku [2] 63.", "startOffset": 7, "endOffset": 10}, {"referenceID": 22, "context": "UWM-TRIADS[23] 53.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "Kim [16] 72.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "FBK irst [8] 69.", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "NIL UCM [4] 61.", "startOffset": 8, "endOffset": 11}, {"referenceID": 31, "context": "WBI-DDI[32] 63.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "CNN [18] 77.", "startOffset": 4, "endOffset": 8}, {"referenceID": 28, "context": "91 MV-RNN [29] 57.", "startOffset": 10, "endOffset": 14}], "year": 2017, "abstractText": "A drug can affect the activity of other drugs, when administered together, in both synergistic or antagonistic ways. In one hand synergistic effects lead to improved therapeutic outcomes, antagonistic consequences can be life-threatening, leading to increased healthcare cost, or may even cause death. Thus, identification of unknown drug-drug interaction (DDI) is an important concern for efficient and effective healthcare. Although there exist multiple resources for DDI, they often unable to keep pace with rich amount of information available in fast growing biomedical texts including literature. Most existing methods model DDI extraction from text as classification problem and mainly rely on handcrafted features. Some of these features further depends on domain specific tools. Recently neural network models using latent features has shown to be perform similar or better than the other existing models using handcrafted features. In this paper, we present three models namely, B-LSTM, ABLSTM and Joint AB-LSTM based on long short-term memory (LSTM) network. All three models utilize word and position embedding as latent features and thus do not rely on feature engineering. Further use of bidirectional long short-term memory (Bi-LSTM) networks allow to extract optimal features from the whole sentence. The two models, AB-LSTM and Joint AB-LSTM also use attentive pooling in the output of Bi-LSTM layer to assign weights to features. Our experimental results on the SemEval-2013 DDI extraction dataset shows that the Joint AB-LSTM model outperforms all the existing methods, including those relying on handcrafted features. The other two proposed models also perform competitively with state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}