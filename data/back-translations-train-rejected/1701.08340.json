{"id": "1701.08340", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2017", "title": "Extracting Bilingual Persian Italian Lexicon from Comparable Corpora Using Different Types of Seed Dictionaries", "abstract": "Bilingual dictionaries are very important in various fields of natural language processing. In recent years, research on extracting new bilingual lexicons from non-parallel (comparable) corpora have been proposed. Almost all use a small existing dictionary or other resource to make an initial list called the \"seed dictionary\". In this paper we discuss the use of different types of dictionaries as the initial starting list for creating a bilingual Persian-Italian lexicon from a comparable corpus.", "histories": [["v1", "Sun, 29 Jan 2017 00:28:20 GMT  (183kb,D)", "http://arxiv.org/abs/1701.08340v1", "30 pages, accepted to be published in \"Applications of Comparable Corpora\", Berlin: Language Science Press"]], "COMMENTS": "30 pages, accepted to be published in \"Applications of Comparable Corpora\", Berlin: Language Science Press", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ebrahim ansari", "m h sadreddini", "lucio grandinetti", "mehdi sheikhalishahi"], "accepted": false, "id": "1701.08340"}, "pdf": {"name": "1701.08340.pdf", "metadata": {"source": "META", "title": "Extracting Bilingual Persian Italian Lexicon from Comparable Corpora Using Different Types of Seed Dictionaries", "authors": ["Ebrahim Ansari", "M.H. Sadreddini", "Lucio Grandinetti", "Mehdi Sheikhalishahi"], "emails": ["(ansari@iasbs.ac.ir)"], "sections": [{"heading": null, "text": "Accepted for publication in \"Applications of Comparable Corpora,\" Berlin:"}, {"heading": "Language Science Press", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Extracting Bilingual Persian Italian", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Lexicon from Comparable Corpora", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Using Different Types of Seed", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dictionaries", "text": "Ebrahim Ansari Faculty of Computer Science and Information Technology, Institute of Advanced Studies in Basic Sciences (IASBS), Zanjan, Iran Faculty of Computer Science and Technology, Shiraz University, Shiraz, IranM.H. Sadreddini Faculty of Computer Science and Engineering, Shiraz University, Shiraz, IranLucio Grandinetti Faculty of Electronics, Computer Science and Systems, University of Calabria, Rende, ItalyMehdi Sheikhalishahi CREATE-NET Research Center, Trento, ItalyEbrahim Ansari (ansari @ iasbs.ac.ir) et al. 2017. Extracting bilingual persian italian lexicon from comparable corpora using different types of seed dictionaries. In \"Applications of Comparable Corpora\" edited Book Berlin Linguistic Press (ed.).ar Xiv: 170 1.08 340v 1 [cs.C L] 29 January 2017"}, {"heading": "Ebrahim Ansari , M.H. Sadreddini, Lucio Grandinetti & Mehdi Sheikhalishahi", "text": "Bilingual dictionaries are very important in various areas of natural language processing. In recent years, research has been conducted to create new bilingual dictionaries from non-parallel (comparable) corpora. Almost all of them use a small existing dictionary or other resource to create a first list called \"Seed Dictionary.\" In this essay, we discuss the use of different types of dictionaries as a starting list for creating a bilingual Persian-Italian dictionary from a comparable body. The interesting challenge of our approach is to find a way to combine different dictionaries to create a better and more precise lexicon. To combine seed dictionaries, we propose two different combination models and examine the effect of our novel combination models on the comparability of the results we have proposed for the comparability of our models."}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "Ebrahim Ansari , M.H. Sadreddini, Lucio Grandinetti & Mehdi Sheikhalishahi", "text": "The third dictionary arises from our small parallel Persian-Italian corpus. Using these dictionaries, we propose various model combinations and a new weighting method that can be used on these different dictionaries. Compared to the approach of Ansari et al. 2014a, we are introducing some new combination schemes to improve the quality of the result lexicon. In addition, we are also using parallel extracted dictionary as one of our first dictionaries for seeds. Unlike previous work, we are applying our idea to various comparable corpora with varying degrees of comparability. In section 2, we describe work related to our approach; section 3 describes our approach; section 4 describes the methodology and resources used in our work; section 5 shows the experimental results; and section 6 is the conclusion of the essay."}, {"heading": "2 Related works", "text": "In Section 2 we discuss approaches and implementations in three parts and show how they relate to our work. Section 2.1 describes the process of building a bilingual lexicon by using a pivot language with source and pivot target dictionaries. Section 2.2 discusses the idea of using parallel corpora to extract a bilingual dictionary. Section 2.3 examines methods that rely on similar corpora to create a bilingual lexicon."}, {"heading": "2.1 Using Pivot languages", "text": "Over the past twenty years, various approaches have been proposed to build a new source-pivot lexicon that uses a pivot language and hence source-pivot and pivot target dictionaries (Tanaka & Umemura 1994; Istv\u00e1n & Shoichi 2009; Tsunakawa, Okazaki & Tsujii 2008; Tsunakawa, Yamoto & Kaji 2013; Ahn & Frampton 2006). One of the best known and most cited methods is the approach of Tanaka and Umemura (Tanaka & Umemura 1994), where they only use dictionaries to translate into and out of a pivot language to generate a new dictionary."}, {"heading": "2.2 Using Parallel Corpora", "text": "Another way to create a bilingual dictionary is to use parallel corpora to find a word translation (i.e. word alignment) that began with the primitive methods of (Brown et al. 1990) and continued with other word alignment approaches such as (Gale & Church 1991; 1993; Melamed 1997; Ahrenberg, Andersson & Merkel 1998; Tiedemann 1998; Och, Tillmann & Ney 1999).These approaches share a basic strategy: first, align two parallel texts into pair segments, and second, have word combinations calculated based on this alignment, an approach that normally achieves high scores of 90% accuracy at 90% memory (Otero 2007).Many studies show that well-formed parallel corpora can achieve high accuracy rates of up to 99% for both sentence and word combinations. Currently, almost the entire task of bilingual dictionary creation, and in particular the creation of a 99 probability table for any two-word pora well-known two-word alignment, is required."}, {"heading": "Ebrahim Ansari , M.H. Sadreddini, Lucio Grandinetti & Mehdi Sheikhalishahi", "text": "is not applicable, although using our parallel corpora with limited resources to create a small dictionary can be handy. This dictionary could be used as an input into other methods that would use this subordinate input to create a larger dictionary."}, {"heading": "2.3 Using Comparable Corpora", "text": "This year it has come to the point that it is a purely reactionary project, in which it is a reactionary project, in which it is a reactionary project, in which it is a reactionary project, in which it is a reactionary project."}, {"heading": "Ebrahim Ansari , M.H. Sadreddini, Lucio Grandinetti & Mehdi Sheikhalishahi", "text": "Logical (A, B) = K11 * N C1 * R1 + K12 * N C1 * R2 + K21 * K21 * N C2 * R1 + K22 * N C2 * R2 (1,2) Where: C1 = K11 + K12 (1,3) C2 = K21 + K22 (1,4) R1 = K11 + K21 (1,5) R2 = K12 + K22 (1,6) N = C1 + C2 + R2 (1,7) With parameters K ij expressed in corpus Frequency: K 11 = Frequency of frequent occurrence of word A and word K 12 = Corpus Frequency of word A - K 11 K21 = Corpus Frequency of word B - K 11 K22 = Size of word Y22 = Size of corpus (No. of tokens x) - Corpus Frequency of word VII - Corpus Frequency of word B All numbers were normalized in our experiments."}, {"heading": "3 Our Approach", "text": "Our experiments in building a Persian-Italian lexicon are based on the approach of comparable corpora windows discussed in Section 2.3. An interesting challenge of our work is to combine different dictionaries with varying accuracy and use them all as seed dictionaries to create comparable corpora. We approach this problem with different strategies: First, combine dictionaries with some simple priority rules, and then use all translations together without taking into account the differences in their weights. Section 3.1 explains our method of collecting and creating seed dictionaries and thus our implementation of their independent use. Section 3.2 describes the use of comparable corpora to build a new Persian-Italian lexicon. Section 3.3 and 3.4 explains our approaches to combining three different dictionaries. Section 3.5 describes our proposed weighting method."}, {"heading": "3.1 Building Seed Dictionaries", "text": "The first dictionary is a small Persian-Italian dictionary, the second dictionary is based on the pivot-based method introduced in (Sj\u00f6bergh 2005), which contains the best entries with the highest score, and the third dictionary is based on two small parallel Persian-Italian corpora. If there is more than one translation for an entry in the primary dictionary, we should select a translation. Most standard sentences choose the first translation in the existing dictionary or the candidate with the highest score in the extracted (created) dictionary. However, in (Irimia 2012) several definitions for a word could be selected based on their scores at the step of creating the Seed dictionary. Like other standard methods, we chose the first translation among all candidates. In the following three subsections, our three dictionaries and the process of creating them are discussed."}, {"heading": "Ebrahim Ansari , M.H. Sadreddini, Lucio Grandinetti & Mehdi Sheikhalishahi", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 The Existing Dictionary \u2013 DicEx", "text": "We used a small Persian-Italian dictionary as an existing dictionary called DicEx. For each entry, only the first translation is selected to create lemmas. Although DicEx is a manually created dictionary, it is the most accurate dictionary in our experiments, and its size is the smallest compared to other dictionaries."}, {"heading": "3.1.2 The Dictionary created by a Pivot based method \u2013 DicPi", "text": "We used the method introduced in (Sj\u00f6bergh 2005) as the starting point for pivot-based dictionary creation. Translations with the highest scores are selected at this stage, while the low scores are removed. A Persian-English dictionary and an English-Italian dictionary are considered as inputs. All stop words and all non-alphabet characters are removed from the English part of these two dictionaries. Then, the frequency of the reverse document, idf, is calculated for the remaining English words as follows: idf (Pr | | Es | Prw + Itw) (1.9) While the word for which we calculate the weight, Pr | the total number of dictionary entries in the Persian-English dictionary is the total number of dictionary entries in the English-Persian dictionary, the total number of dictionary entries in the English-Italian dictionary is the number of the English-Persian dictionary, the number of the English-Italian dictionary entries in the English dictionary."}, {"heading": "3.1.3 The Dictionary extracted from Parallel corpora \u2013 DicPa", "text": "In this work, we used our low parallel Persian-Italian resources (e.g. movie subtitles) to create a small dictionary by selecting the top translations with the highest probabilities. This parallel corpus-based dictionary called DicPa is used as a seed dictionary, which is then combined with other main dictionaries in the following phases. It is created from a generic domain translation table that is automatically extracted with Giza + + (Och & Ney 2003). If a word has more than one translation, only the most likely translation is selected and others are removed with less probability."}, {"heading": "3.2 Using seed dictionaries to extract lexicon from Comparable Corpora", "text": "Some mathematical and theoretical points of our approach were discussed in Section 2.3. Given the large differences between Persian and Italian words in syntax and grammar, the window-based approach is preferred. Basis of the method implemented in our study is an adaptation of (Rapp 1999). Based on our proposed idea, the seed dictionary could be an existing dictionary, an automatically generated dictionary, or a combination thereof. There are two types of inputs: the seed dictionary and the bilingual comparable corpus. Weighing vectors must be compiled on the basis of corpora and lexicon. Before creating matrices for the Persian and Italian languages, the stop words of corpora are deleted and they should be lemmatized. Two co-occurrence matrix sentences are compiled for the Persian and Italian corpora: one sentence for the simple approach and another for the different portions of our corpora, in order of the two words respectively."}, {"heading": "Ebrahim Ansari , M.H. Sadreddini, Lucio Grandinetti & Mehdi Sheikhalishahi", "text": "In the series-based method, matrices must store the position of each word with the pivot word, in addition to storing the frequency in a window. We create them by dividing each field of the former matrix by 2n fields in which each field has a different position before or after the pivot word, with each new matrix itself being a r \u00b7 n \u00d7 k matrix if the field (i, j, k) shows the number of times word j occurred in position k of the word i. Previous approaches show the need to replace the concurrent frequency in the matrix with measures that are capable of eliminating word frequency effects and thus favoring significant word pairs. Therefore, we use the loglikelihood ratio (i.e. formula 1 (Dunning 1993)."}, {"heading": "3.3 Using simple combination", "text": "This section discusses the process of creating the larger dictionary using a simple combination rule: the accuracy of the existing dictionary, DicEx, is among others highest, and the accuracy of DicPi is higher than the dictionary created from a parallel corpus (i.e. DicPa). Based on the accuracy of the dictionaries, a priority sequence is defined to create the final dictionary: 1 Extract the bilingual Persian-Italian lexicon from comparable corpora using different types of seed dictionaries DicEx > DicPi > DicPa Our simple combination rule is: Suppose that the priority of Dici is higher than the priority of Dicj; if a word w is in both Dici and Dicj, its translation from Dici (i.e. the dictionary with higher priority) is selected."}, {"heading": "3.4 Using independent word combination", "text": "In our simple priority-oriented combination, described in Section 3.3, there is an important issue that should be discussed. Given two words, the first of which occurs in all three dictionaries and the second only in one dictionary, there is no difference between these words in our simple approach. Therefore, a new advanced combination method is proposed. Our advanced combination method is based on the assumption that a word in two different dictionaries should be considered independently as two different words. For example, if a word appears in both dictionaries Dic1 and Dic2, it may have two independent columns in our vector matrix (i.e. it has two different weights in the transferred vectors). Therefore, the new dictionary called DicCoIn is created where its size corresponds to the sum of the size of our three dictionaries. In this new dictionary, if the word x occurs in two dictionaries, there are two different entries named xi and xj, where j and j are the corresponding dictionaries."}, {"heading": "3.5 New weighting method", "text": "Although some dictionaries are more accurate than others, there is no difference in how initial seed dictionaries are handled. To solve this problem, a new weighting model for similarity values is introduced, based on two aspects: (1) We could change the effect of each seed dictionary to adopt higher weights for more accurate dictionaries, which could be adjusted manually."}, {"heading": "Ebrahim Ansari , M.H. Sadreddini, Lucio Grandinetti & Mehdi Sheikhalishahi", "text": "(2) If a word appears in two dictionaries, then it is not necessary to count it twice, since a double count would create an unfair distortion. We could consider its weight a little more than a normal occurrence weight and then divide it between different dictionaries. If there were k different dictionaries in our proposed independent word-based combination, we could calculate the similarity values between bilingual lemas with the proposed equation 1.11: newdiceMin (X, Y) = 2 \u00b7 \u2211 kj = 1 \u2211 Xi, Dicj min (Xi, Yi) \u00b7 wj \u0445 ni = 1 Xi + \u2211 n = 1 Yi (1.11) Where n is the size of the new combined dictionary and wj is the weight of the dictionary j. In our experiments, the size of k is equal to three. This shows that if wj = 1 for j = 1, 2 and 3, then the method is the same as the previous approach described in section 3.4, the new weighting of the results is based on the two."}, {"heading": "4 Preparing The Inputs", "text": "As already mentioned, two primary entries are required to perform comparable corpora-based lexicon generation: Seed Dictionary and comparable corpora / corpora. Procedures for processing this required data are in sections 4.1 and 4.2, a test data set is required to evaluate the result. Assessment of the test is done by two persons, the first assessor is one of the authors who is a Persian native speaker and speaks fluent Italian, and the second is a Persian native speaker who teaches the Italian language. If both assessors agree on a translation word, it will be accepted as a true translation, otherwise the translation will be deemed to be incorrect. We selected 400 objective test words from Nabid Persian-English dictionary 1. As it is not appropriate to apply our approach to words already included in the base lexicon, we removed all entries belonging to the 400 test words from the lexicon. The frequencies of all selected words in our Nabid were larger than the lexicon 1, lexicon of a lexicon of the Nabid."}, {"heading": "4.1 Seed Dictionaries", "text": "The second dictionary, DicPi, is a dictionary extracted from the swivel approach proposed in Sj\u00f6bergh 2005. To extract DicPi, two Persian-English and English-Italian dictionaries are required. The Persian-English dictionary we used is the Nabid dictionary. It contains about 100,000 Persian index words. For the English-Italian dictionary, we used a personal dictionary created by the University of Pisa for their internal experiments.2 This simple dictionary contains about 130,000 words. We removed the few stopwords from the English part of selected dictionaries and all characters that were not letters."}, {"heading": "Ebrahim Ansari , M.H. Sadreddini, Lucio Grandinetti & Mehdi Sheikhalishahi", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2 Comparable Corpora", "text": "In our experiments, three different types of comparable corpora are gathered: the first is a small corpus of Wikipedia5 articles in Persian and Italian, which passes from the first phase to the second. To skip these articles, which are well known and well described in one of our languages, we selected these pairs of articles, in which the difference between their sizes is not greater than 50%. After applying this criterion, the articles in both languages are selected: about 150,000 sentences in Italian and 176,000 sentences in Italian. Both sentences have been tokenized and lemmatized."}, {"heading": "5 Experimental Results", "text": "The results show that the consideration of the order is not very effective in extracting Persian-Italian lexicographies (i.e. it has a slightly positive effect only in a small number of cases), the authors think that the reason for this is the enormous difference between the structures of the Persian and Italian languages. However, in our experiments we applied both schemes. Based on (Irimia 2012) \"s conclusion, all window sizes were set to 5. In our approach we calculated both the simple frequency and the log probability ratio. Despite our expectation, the use of simple co-events in some cases has a more accurate result compared to the use of the log probability ratio. While this difference is very small, in most of the figures presented in this paper the simple frequency ratio is not taken into account and only the log probability ratio is shown as a combination ratio. All the experiments described in this paper were applied to two types of comparable word ratios."}, {"heading": "Ebrahim Ansari , M.H. Sadreddini, Lucio Grandinetti & Mehdi Sheikhalishahi", "text": "The top 10 measurement corresponds to the number of times a correct translation of a word appears in the top 10 translations of the result dictionary, divided by the number of test words. The evaluation process is performed manually by two evaluators. As described in section 4, a translation is considered true if both evaluators agree."}, {"heading": "5.1 Using independent dictionaries", "text": "In the first phase of our experiments, all three of the aforementioned dictionaries are used individually as the Seed Dictionary. These are the preexisting dictionary (DicEx), the Pivot base extracted dictionary (DicPi) and the parallel corpus based dictionary (DicPa). Figures 1 and 2 summarize the evaluation results using these three seed dictionaries with and without word order. The aim of this experiment is to see the effect of using other comparable corpora. Figure 1 shows the results of using Corpus with higher comparative degrees and Figure 2 shows the results of using GenCorpus with lower comparative degrees. A comparison between these two corpora and effects is illustrated in Figure 3. The aim of this classification is to determine the effects on the effect of Corpus."}, {"heading": "5.2 Using composite dictionaries", "text": "In this section, we evaluate our ideas for combining different dictionaries. As already described, our experiments use two different types of combination: The simple combination creates a dictionary using a simple priority rule and the advanced combination for all dictionaries taking into account all translations of a word. Table 2 shows the results of these studies. According to this table, the best results for the top-1 measurement belong to the simple combination model when all dictionaries are combined with each other. The best top 10 results belong to the advanced combination model that combines all dictionaries. In the advanced combination, all words in all dictionaries are selected at the stage of lexicon creation, and this generally leads to the better top-10 results. An important problem for our advanced combination is that all translations in different dictionaries have the same weight and this can reduce the effect of DicEx. Although it is our most accurate dictionary, it is also our smallest problem, which we will solve in the next section."}, {"heading": "Ebrahim Ansari , M.H. Sadreddini, Lucio Grandinetti & Mehdi Sheikhalishahi", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3 Using new weighting", "text": "In this section, we describe the proposed weighting method for sharing different dictionaries. The aim of introducing this measurement is to \"tune\" the effects of a1 Extraction Bilingual Persian Italian Lexicon from Comparable Corpora Using Different Types of Seed Dictionariesdictionary with regard to their accuracy and correctness. Two different heuristics are used to adjust the weights in this part: the first is the matching of weights based on the accuracy of the dictionaries. Accuracy could be determined from the top 10 results calculated in Phase 5.1 (results in the second column of Figure 1). In the first sentence, the weights for DicEx, DicPi and DicPa are 0.7, 0.64 and 0.59, respectively. In our second heuristic sentence, the weights are calculated on the accuracy as well as the dictionary size. This weight set is constructed on the assumption that the larger dictionary should have a lesser effect on the final result."}, {"heading": "Ebrahim Ansari , M.H. Sadreddini, Lucio Grandinetti & Mehdi Sheikhalishahi", "text": "Based on the second heuristic, formula 1.12, and taking into account the results in our study, the weights are as follows: WDicEx = 2.10, WDicPi = 0.64, and WDicPa = 0.59. The results of these experiments based on different weighting sets are shown in Table 3. Wi = 1 represents the classic approach without using the proposed weighting system.Table 3 shows that when we take into account the accuracy of dictionaries, the extracted lexicon has better accuracy compared to the advanced combination. The best efficiency lies in the second weighting set, where we consider both accuracy and size together when the weight of the most accurate dictionary, DicEx, is much higher than the rest. Finally, Figure 5 shows a short illustration to see the effect of our combination methods compared to classical approaches when using only the existing dictionary, DicEx (the most accurate independent dictionary in our study) as a dictionary."}, {"heading": "6 Conclusion", "text": "In order to create a Persian-Italian lexicon, we opted for a comparable corpora-based lexicon creation method. In our study, three different seed dictionaries (and combinations) are used, consisting of an existing dictionary and two extracted dictionaries. The first extracted dictionary is based on methods for creating parallel dictionaries and the second is extracted using Pivot language models. While a seed dictionary requires a small dictionary, we have selected only the best translations from these created dictionaries. In the first part of our study, the effects of using these dictionaries on comparable corpora are examined."}, {"heading": "Ebrahim Ansari , M.H. Sadreddini, Lucio Grandinetti & Mehdi Sheikhalishahi", "text": "We used two different strategies: firstly, the creation of dictionaries with some priority rules; secondly, the use of all dictionaries together using similar words in two dictionaries as a different word. Both strategies were studied and, based on our experimental results, these novel dictionary combinations were able to improve the efficiency of the results; the proposed advanced dictionary combination is almost as precise as our simple combination; furthermore, in all experiments the effect of the degree of comparability of a comparable initial corpus using different types of comparable corpus is investigated; the results show that a higher degree of comparability in the entry corpus has a more accurate lexicon, although the less comparable corpus is larger compared to the higher comparable corpus; although the use of a specific corpus can reduce the universality of the extracted lexicon; and finally, a new weighting method was proposed to increase the efficiency of comparison."}, {"heading": "Ebrahim Ansari , M.H. Sadreddini, Lucio Grandinetti & Mehdi Sheikhalishahi", "text": "In, vol. 2, 862-870. 1699625: Association for Computational Linguistics. Kaji & Toshiko Aizono. 1996. Extracting word correspondences from bilingual comparable corpora. IEICE - Trans. Inf. Syst. E88-D (2). 313-323. Kaji, Hiroyuki & Toshiko Aizono. 1996. Extracting word correspondences from bilingual corpora based onword co-occurrences information. In, 23-28. 992636: Association for Computational Linguistics. Linguistics."}, {"heading": "Appendix", "text": "Various similarity values were used in the variants of the classic approach to extracting the bilingual lexicon from comparable corpora; (Rapp 1999) used City Block as the preferred similarity vector; the cosinal similarity was used by (Fung & McKeown 1997; Fung & Yee 1998; Chiao & Zweigenbaum 2002; San Vicente X Saralegui 2008) and the lin similarity metric by (Lin 1998); the other known similarity metrics are Cube and Jaccard (Chiao & Zweigenbaum 2008)."}, {"heading": "Ebrahim Ansari , M.H. Sadreddini, Lucio Grandinetti & Mehdi Sheikhalishahi", "text": "There are two different forms of Jaccard and Dice; the jaccardMin metric (Grefenstette 1994; Kaji & Aizono 1996) and the diceMin metric (Curran & Moens 2002; Plas & Bouma 2005; Otero 2007), both of which take into account only the smallest association weight. Jaccard and Dice are very similar based on the results collected in (Otero 2008), in which authors discuss the efficiency of several similarity metrics in combination with simple events and probability weight schemes. In (Xi & Langlais 2010), the authors presented some experiments for different parameters such as context, association measure, similarity measure, and seed lexicon. In recent work, the similarity of two vectors, X and Y, is calculated using one of these similarity measures."}, {"heading": "Name index", "text": "Ahn, Kisuh, 2, 4 Ahrenberg, Lars, 5 Aizawa, 6mir, Iriena Li, 6, 7 Aizono, Toshiko, 28 Andersson, Mikael, 5 Ansari, Ebrahim, 3, 4Babych, Bogdan, 6 Bouamor, Dhouha, 6 Bouma, Gosse, 9, 28 Brown, Peter F., 5Campos, Jose 'Ramom Pichel, 6 Chiao, Yun-Chuang, 3, 6, 7, 27 Church, Kenneth W., 5 Curran, James R., 9, 28Dunning, Ted, 7, 12 D\u00e9jean, Herv\u00e9, 6E. Morin, B. Daille, 6 Emmanuel, Morin, 6Frampton, Matthew, 2, 4 Fung, Pascale, 3, 6, 7, 27Gale, William A., 5 Gaussier, Eric, 6, 7 Gaussier, \u00c9ric, 6 Grefenstette, Gregory, 28vael, Amir, Babyse, 69, Bouena, 7, Li, 611, Tosena, 611, Anderena, 628, 611"}, {"heading": "Name index", "text": "5 Tsujii, Jun'ichi, 2, 4 Tsunakawa, Takashi, 2, 4Umemura, Kyoji, 2, 4, 5Yamamoto, Yosuke, 2, 4 Yee, Lo Yuen, 3, 6, 7, 279, Michael, 3, 6 Branch Tree, Pierre, 3, 6, 7, 27"}], "references": [{"title": "Automatic generation of translation dictionaries using intermediary", "author": ["Ahn", "Kisuh", "Matthew Frampton"], "venue": "languages. In,", "citeRegEx": "Ahn et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2006}, {"title": "A simple hybrid aligner for generating lexical correspondences in parallel", "author": ["Ahrenberg", "Lars", "Mikael Andersson", "Magnus Merkel"], "venue": "texts. In,", "citeRegEx": "Ahrenberg et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Ahrenberg et al\\.", "year": 1998}, {"title": "Combining different seed dictionaries to extract lexicon from comparable corpus", "author": ["Ansari", "Ebrahim", "M.H. Sadreddini", "Alireza Tabebordbar", "Mehdi Sheikhalishahi"], "venue": "Indian Journal of Science and Technology", "citeRegEx": "Ansari et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ansari et al\\.", "year": 2014}, {"title": "Extracting persian-english parallel sentences fromdocument level aligned comparable corpus using bi-directional translation", "author": ["Ansari", "Ebrahim", "M.H. Sadreddini", "Alireza Tabebordbar", "RichardWallace"], "venue": "ACSIJ Advances in Computer Science: an International Journal", "citeRegEx": "Ansari et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ansari et al\\.", "year": 2014}, {"title": "Building specialized bilingual lexicons using word sense disambiguation", "author": ["Bouamor", "Dhouha", "Nasredine Semmar", "Pierre Zweigenbaum"], "venue": null, "citeRegEx": "Bouamor et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bouamor et al\\.", "year": 2013}, {"title": "A statistical approach to machine translation", "author": ["Brown", "Peter F", "John Cocke", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Fredrick Jelinek", "John D. Lafferty", "Robert L. Mercer", "Paul S. Roossin"], "venue": "Comput. Linguist", "citeRegEx": "Brown et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1990}, {"title": "Looking for candidate translational equivalents in specialized", "author": ["Chiao", "Yun-Chuang", "Pierre Zweigenbaum"], "venue": "comparable corpora. In,", "citeRegEx": "Chiao et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chiao et al\\.", "year": 2002}, {"title": "Improvements in automatic thesaurus", "author": ["Curran", "James R", "Marc Moens"], "venue": "extraction. In,", "citeRegEx": "Curran et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Curran et al\\.", "year": 2002}, {"title": "Accurate methods for the statistics of surprise and coincidence", "author": ["Dunning", "Ted."], "venue": "Comput. Linguist. 19(1). 61\u201374.", "citeRegEx": "Dunning and Ted.,? 1993", "shortCiteRegEx": "Dunning and Ted.", "year": 1993}, {"title": "Bilingual terminology extraction: an approach based on a multi-lingual thesaurus applicable to comparable corpora", "author": ["D\u00e9jean", "Herv\u00e9", "\u00c9ric Gaussier", "Fatia Sadat"], "venue": null, "citeRegEx": "D\u00e9jean et al\\.,? \\Q2002\\E", "shortCiteRegEx": "D\u00e9jean et al\\.", "year": 2002}, {"title": "Bilingual terminology mining from language for special purposes comparable corpora. In Building and using comparable", "author": ["E. Morin", "B. Daille", "E. Prochasson"], "venue": null, "citeRegEx": "Morin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2013}, {"title": "Looking at unbalanced specialized comparable corpora for bilingual lexicon extraction. In Proceedings of the 52nd annual meeting of the association for computational linguistics (acl), 1284\u20131293", "author": ["Emmanuel", "Morin", "Amir Hazem"], "venue": null, "citeRegEx": "Emmanuel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Emmanuel et al\\.", "year": 2014}, {"title": "Compiling bilingual lexicon entries from a non-parallel englishchinese corpus", "author": ["Fung", "Pascale."], "venue": "In, 173\u2013183.", "citeRegEx": "Fung and Pascale.,? 1995", "shortCiteRegEx": "Fung and Pascale.", "year": 1995}, {"title": "Finding terminology translations fromNonparallel", "author": ["Fung", "Pascale", "KathleenMcKeown"], "venue": "corpora. In,", "citeRegEx": "Fung et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Fung et al\\.", "year": 1997}, {"title": "An IR approach for translating new words from nonparallel, comparable", "author": ["Fung", "Pascale", "Lo Yuen Yee"], "venue": "texts. In,", "citeRegEx": "Fung et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Fung et al\\.", "year": 1998}, {"title": "Identifying word correspondence in parallel texts", "author": ["Gale", "William A", "Kenneth W. Church"], "venue": null, "citeRegEx": "Gale et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Gale et al\\.", "year": 1991}, {"title": "A program for aligning sentences in bilingual corpora", "author": ["Gale", "William A", "Kenneth W. Church"], "venue": "Comput. Linguist", "citeRegEx": "Gale et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Gale et al\\.", "year": 1993}, {"title": "Explorations in automatic thesaurus discovery", "author": ["Grefenstette", "Gregory."], "venue": "Kluwer Academic Publishers. 25", "citeRegEx": "Grefenstette and Gregory.,? 1994", "shortCiteRegEx": "Grefenstette and Gregory.", "year": 1994}, {"title": "Experimenting with extracting lexical dictionaries", "author": ["Irimia", "Elena"], "venue": null, "citeRegEx": "Irimia and Elena.,? \\Q2012\\E", "shortCiteRegEx": "Irimia and Elena.", "year": 2012}, {"title": "Bilingual dictionary generation for low", "author": ["Istv\u00e1n", "Varga", "Yokoyama Shoichi"], "venue": null, "citeRegEx": "Istv\u00e1n et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Istv\u00e1n et al\\.", "year": 2009}, {"title": "Extracting translation equivalents from bilingual compara", "author": ["Kaji", "Hiroyuki"], "venue": null, "citeRegEx": "Kaji and Hiroyuki.,? \\Q2005\\E", "shortCiteRegEx": "Kaji and Hiroyuki.", "year": 2005}, {"title": "Extracting word correspondences", "author": ["Kaji", "Hiroyuki", "Toshiko Aizono"], "venue": null, "citeRegEx": "Kaji et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kaji et al\\.", "year": 1996}, {"title": "Revisiting context-based projection", "author": ["Laroche", "Audrey", "Philippe Langlais"], "venue": null, "citeRegEx": "Laroche et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Laroche et al\\.", "year": 2010}, {"title": "Improving corpus comparability for bilingual lexi", "author": ["Li", "Bo", "Eric Gaussier"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Clustering comparable corpora", "author": ["Li", "Bo", "Eric Gaussier", "Akiko Aizawa"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Automatic retrieval and clustering of similar words", "author": ["Lin", "Dekang."], "venue": "Proc. of acl-98, 768\u2013774. Montreal, Canada. http://www.aclweb.org/anthology/P98-", "citeRegEx": "Lin and Dekang.,? 1998", "shortCiteRegEx": "Lin and Dekang.", "year": 1998}, {"title": "A portable algorithm for mapping bitext correspondence", "author": ["Melamed", "I. Dan"], "venue": null, "citeRegEx": "Melamed and Dan.,? \\Q1997\\E", "shortCiteRegEx": "Melamed and Dan.", "year": 1997}, {"title": "A systematic comparison of various sta", "author": ["Och", "Franz Josef", "Hermann Ney"], "venue": null, "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Improved alignment", "author": ["Och", "Franz Josef", "Christoph Tillmann", "Hermann Ney"], "venue": null, "citeRegEx": "Och et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Och et al\\.", "year": 1999}, {"title": "Learning bilingual lexicons from comparable english", "author": ["Otero", "Pablo Gamallo"], "venue": null, "citeRegEx": "Otero and Gamallo.,? \\Q2007\\E", "shortCiteRegEx": "Otero and Gamallo.", "year": 2007}, {"title": "Evaluating two different methods for the task", "author": ["Otero", "Pablo Gamallo"], "venue": null, "citeRegEx": "Otero and Gamallo.,? \\Q2008\\E", "shortCiteRegEx": "Otero and Gamallo.", "year": 2008}, {"title": "Syntactic contexts for finding", "author": ["Plas", "Lonneke van der", "Gosse Bouma"], "venue": null, "citeRegEx": "Plas et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Plas et al\\.", "year": 2005}, {"title": "Identifying word transla", "author": ["Linguistics. Rapp", "Reinhard", "Serge Sharoff", "Bogdan Babych"], "venue": null, "citeRegEx": "Rapp et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rapp et al\\.", "year": 2012}, {"title": "Utilizing citations of foreign words", "author": ["Rapp", "Reinhard", "Michael Zock"], "venue": null, "citeRegEx": "Rapp et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rapp et al\\.", "year": 2010}, {"title": "Measuring the distance between comparable corpora", "author": ["Sharoff", "Serge"], "venue": null, "citeRegEx": "Sharoff and Serge.,? \\Q2013\\E", "shortCiteRegEx": "Sharoff and Serge.", "year": 2013}, {"title": "Construction of a bilingual dictionary", "author": ["Tanaka", "Kumiko", "Kyoji Umemura"], "venue": null, "citeRegEx": "300", "shortCiteRegEx": "300", "year": 1994}, {"title": "In both dice and jaccard metrics, the association values of two lemmas with the same context are joined using their product. There are two different forms of jaccard and dice; the jaccardMin metric (Grefenstette", "author": ["Ebrahim Ansari", "M.H. Sadreddini", "Lucio Grandinetti", "Mehdi Sheikhalishahi baum"], "venue": "Kaji & Aizono 1996) and diceMin (Curran", "citeRegEx": "Ansari et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Ansari et al\\.", "year": 1994}], "referenceMentions": [{"referenceID": 5, "context": "word alignment) started with primitive methods of (Brown et al. 1990) and continued with some other word alignment approaches such as (Gale & Church 1991; 1993; Melamed 1997; Ahrenberg, Andersson & Merkel 1998; Tiedemann 1998; Och, Tillmann & Ney 1999).", "startOffset": 50, "endOffset": 69}], "year": 2017, "abstractText": null, "creator": "LaTeX with hyperref package"}}}