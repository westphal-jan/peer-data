{"id": "1701.08343", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2017", "title": "Rhythm Transcription of Polyphonic Piano Music Based on Merged-Output HMM for Multiple Voices", "abstract": "In a recent conference paper, we have reported a rhythm transcription method based on a merged-output hidden Markov model (HMM) that explicitly describes the multiple-voice structure of polyphonic music. This model solves a major problem of conventional methods that could not properly describe the nature of multiple voices as in polyrhythmic scores or in the phenomenon of loose synchrony between voices. In this paper we present a complete description of the proposed model and develop an inference technique, which is valid for any merged-output HMMs for which output probabilities depend on past events. We also examine the influence of the architecture and parameters of the method in terms of accuracies of rhythm transcription and voice separation and perform comparative evaluations with six other algorithms. Using MIDI recordings of classical piano pieces, we found that the proposed model outperformed other methods by more than 12 points in the accuracy for polyrhythmic performances and performed almost as good as the best one for non-polyrhythmic performances. This reveals the state-of-the-art methods of rhythm transcription for the first time in the literature. Publicly available source codes are also provided for future comparisons.", "histories": [["v1", "Sun, 29 Jan 2017 01:25:57 GMT  (656kb,D)", "http://arxiv.org/abs/1701.08343v1", "13 pages, 13 figures, version accepted to IEEE/ACM TASLP"]], "COMMENTS": "13 pages, 13 figures, version accepted to IEEE/ACM TASLP", "reviews": [], "SUBJECTS": "cs.AI cs.SD", "authors": ["eita nakamura", "kazuyoshi yoshii", "shigeki sagayama"], "accepted": false, "id": "1701.08343"}, "pdf": {"name": "1701.08343.pdf", "metadata": {"source": "CRF", "title": "Rhythm Transcription of Polyphonic Piano Music Based on Merged-Output HMM for Multiple Voices", "authors": ["Eita Nakamura", "Kazuyoshi Yoshii", "Shigeki Sagayama"], "emails": ["enakamura@sap.ist.i.kyoto-u.ac.jp,", "yoshii@kuis.kyoto-u.ac.jp).", "sagayama@meiji.ac.jp)."], "sections": [{"heading": null, "text": "In fact, most of them are able to survive on their own, without feeling able to survive on their own."}, {"heading": "II. RELATED WORK", "text": "Previous studies on rhythm transcription are reviewed in this section, the purpose of which is twofold: firstly, we describe the historical development of rhythm transcription models, some of which form the basis of our model and others are the subject of our comparative evaluation; secondly, we review how polyphony has been treated in previous studies on rhythm transcription and related areas and explain in detail the reasons for explicit modeling of multiple voices. Part of the discussions in sections II-B and II-C and the numbers are quoted from Ref. [19] to make this section more informative and independent."}, {"heading": "A. Early Studies", "text": "Longuet-Higgins [3] developed a method for simultaneously estimating note values and note structure by recursively dividing a time interval into two or three parts of almost equal size, likely beginning with note start. A similar method for simultaneously dividing a time interval was also proposed [4]. Methods that use preference rules for the ratio of quantified note durations were developed by Chowning et al. [5] and Temperley et al. [6]. Desain et al. [7] proposed a connectionist approach that iteratively converts note durations so that adjacent note lengths tend to have simple integer ratios. Despite some successful results, these methods have principal limitations."}, {"heading": "B. Statistical Methods", "text": "Since about the year 2000, it has become popular to use statistical models that allow us to use the statistical character of scores and performances. Normally, two models, one to describe the probability of a score (score model) and the other to describe the probability of a performance in the face of a score (performance model), are combined as a Bayesian model, and rhythm transcription can be formulated as the maximum a posteriori estimate. In the following, we will consider representative models for rhythm transcription. We are only looking at monophonic performances; polyphonic extensions are in paragraphs II-C. In a class of rhythm transcription HMMs that we call HMMs, a score is represented as a sequence of note values and described using a Markov model (fig. 2) [8]. To describe the temporal fluctuations in performances, a latent variable is introduced that corresponds to a (local) tempo that is also described with a Markov model."}, {"heading": "C. Polyphonic Extensions", "text": "The note HMM has been extended to handle polyphonic performances [10] by presenting a polyphonic score as a linear sequence of chords or, more precisely, clusters of notes consisting of one or more notes. Such a score representation is also used confidentially for music analysis [27] and for score performance matches (28], [29]. Chordal notes can be represented as self-transitions in the score model (fig. 2) and their IOIs can be described with a probability distribution with a peak at zero. Polyphonic extension of metric HMMs is possible in the same way (28). Although this simplified representation of polyphonic scores is logically possible, there are instances where score and performance models based on this representation cannot adequately describe the nature of polyphonic music."}, {"heading": "D. Merged-Output HMM", "text": "Based on the fact that HMM is effective for monophonic music [8] - [13], an HMM-based model has been proposed that can describe the polyphonic structure of symbolic music, known as the merged output of polyphonic HMMs (fig. 4). Mathematically, the model is described as follows: Consider the case of two voices indexed by a variable s = 1, 2, and let i (s) represent the overall polyphony of the music signal as the merged output of polyphonic HMMs (fig. 4). Consider the case of two voices indexed by a variable s = 1, 2, and let i (s) determine the output probability of the respective voice HMM (for an output symbol x)."}, {"heading": "III. PROPOSED METHOD", "text": "We present a complete description of a rhythm transcription method based on merged output HMM [19], which describes polyphonic performances with polyphonic structure, the generative model is presented in paragraph III-A, the determination of model parameters is discussed in paragraph III-B, and its inference algorithm, which leads to rhythm transcription and voice separation at the same time, is derived in paragraph III-C."}, {"heading": "A. Model Formulation", "text": "First, the description of the chord model, which has been explained as \"self-transition\" in the note state space, is given. Since self-transmission is also used to represent repeated note values from two note clusters, it should be handled with care and we introduce a two-step Markov hierarchical model to solve the problem. Second, there is a refinement of the choice of HMM's vocal probability, which has not been mentioned before but is necessary to improve the accuracy of voice separation. Below, a score is specified using multiple sequences that correspond to voices, pitches and note values, and a MIDI power signal is specified by a sequence of pitches and set times. In this paper, we look only at note sets and thus note lengths, and IOI mean the same note model for each voice HMM."}, {"heading": "B. Model Parameters and Their Determination", "text": "Here we summarize the model parameters, explain how they can be derived from data, and describe some reasonable constraints to improve the efficiency of the learning parameters. (InitOnset timePitchTempoNote value (voice 2) s1 = s2 = s3 = s4 = s5 = 2v1 v2 v3 v4t1 t3 t4 t5p1 p3 p3 p5Init (1) s2 (2) s2 (1) s3 (1) Init (2) Voicer (2) 1r (1) s2 p2 p3 p3 p3 (1) 2g p3 p3 (1) 1 g (2). (2) 2Fig. (7) Graphic representation of the proposed summary HMM when voice information is fixed. The variables with a tilde (s) n and g (s) n represent values for each voice."}, {"heading": "IV. EVALUATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Methodology for Systematic Evaluation", "text": "In some studies that reported systematic evaluations of rhythm transcription [8], [9], [13], processing costs (i.e. the number of operations necessary to correct an estimated result) are used as a yardstick. These studies used the shift mode, which changes the score time of a given note or equivalent to count the number of notated rhythmic errors. On the other hand, the relative note values are more important than the absolute note values, and the tempo error should also be taken into account. This is because there is arbitrariness in the selection of the note unit: for example, a quarter note played at a tempo of 60 BPM has the same duration as a half note played at a tempo of 120 BPM. Since the results of rhythm transcription often contain note values that are uniformly scaled by the correct note values, which should not be considered a completely erroneous estimate [34], we must take into account the scaling process as well as the scaling procedures."}, {"heading": "B. Comparisons with Other Methods", "text": "The purpose is to find out that the state-of-the-art method of rhythm transcription and its relation to the proposed model was not different. Among earlier methods described in Section II, the following six methods were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metric HMM [11] and the 2D PCFG model [16], [17]. The first five are quoted relatively frequently and the last one is theoretically important as it provides an alternative way of statistically modelling multi-voice structures (Sec. II-C. Setup: Two sets of classical piano pieces were used. One (polyrhythmic records) consisted of 30 performances of different (excerpts of pieces containing 2 against 3 or 3 against 4 polyrhythmic passages)."}, {"heading": "C. Examining the Proposed Method", "text": "1) Dependence on Nh: In paragraph III-C, we have introduced a cutoff Nh in the inference algorithm to reduce the calculation costs. 1) Dependence on Nh: In an earlier study [24], which discussed the same cutoff, it was empirically confirmed that Nh = 50 results provide almost exact results for piano performances. As it is difficult for our model to use the exact algorithm that corresponds to Nh = 30 for all other evaluations to investigate its dependence, as shown in Fig. 12, the results for Nh \u2265 20 were similar and were exactly the same for Nh \u2265 30. Based on this result, we have used the value Nh = 30 for all other evaluations in this work. 2) Effect Model: Explains that the sufficient value of Nh depends on exact data conclusions and that smaller values with suboptimal estimates could provide better accuracies (than the case of Nh = 20 for our algorithms and data)."}, {"heading": "V. CONCLUSION", "text": "This model has an internal structure consisting of several HMMs to solve the long-standing problem of correctly describing the polyphonic structure of polyphonic music. By using the inference method derived from this paper, the algorithm can perform voice separation and score recognition at the same time. Deriving inference algorithms with reduced computing costs can be applied to other fused-output HMMs with autoregressive voice HMMs, which are expected to be effective models of polyphonic music where the polyphonic structure is significant. By examining the proposed method, we have also confirmed that simultaneous inferencing of voice and rhythm information improves the accuracy of rhythm transcription compared to a cascading approach, although it does not necessarily improve the accuracy of voice separation."}, {"heading": "APPENDIX A CALCULATION OF THE RHYTHM CORRECTION COST", "text": "Let's minimize the minimum number of operations No is determined by minimizing Nc + Nsh for all machining operations. The number of scaling operations and those of displacement operations are formally defined as Nsc = # {n} n. The number of scaling operations and those of displacement operations are formally defined as Nsc = # {n} n. The number of scaling operations and those of displacement operations are formally defined as Nsc = # dn = 6 = dn} and Nsh = {n} and Nsh = {n}. The minimum number of machining operations is determined by minimizing Nc + sh for all machining operations."}, {"heading": "ACKNOWLEDGMENT", "text": "We thank David Temperley, Norihiro Takamune and Henkjan Honing for providing their source code. Author EN thanks Hiroaki Tanaka for useful discussions about merging HMM and Yoshiaki Bando for helping to run computer programs. This work is partially supported by the projects JSPS KAKENHI No. 24220006, 26240025, 26280089, 26700020, 15K16054, 16H01744 and 16J05486 as well as JST CrestMuse, OngaCREST and OngaACCEL."}], "references": [{"title": "Automatic Music Transcription: Challenges and Future Directions", "author": ["E. Benetos", "S. Dixon", "D. Giannoulis", "H. Kirchhoff", "A. Klapuri"], "venue": "J. Intelligent Information Systems, vol. 41, no. 3, pp. 407\u2013434, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Mental Processes: Studies in Cognitive Science", "author": ["H. Longuet-Higgins"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1987}, {"title": "Transcribe: A Comprehensive Autotranscription Program", "author": ["J. Pressing", "P. Lawrence"], "venue": "Proc. ICMC, pp. 343\u2013345, 1993.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1993}, {"title": "Intelligent Systems for the Analysis of Digitized Acoustic Signals", "author": ["J. Chowning", "L. Rush", "B. Mont-Reynaud", "C. Chafe", "W. Schloss", "J. Smith"], "venue": "Tech. Rep. CCRMA, STAN-M-15, 1984.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1984}, {"title": "Modeling Meter and Harmony: A Preference-Rule Approach", "author": ["D. Temperley", "D. Sleator"], "venue": "Comp. Mus. J., vol. 23, no. 1, pp. 10\u201327, 1999.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "The Quantization of Musical Time: A Connectionist Approach", "author": ["P. Desain", "H. Honing"], "venue": "Comp. Mus. J., vol. 13, no. 3, pp. 56\u201366, 1989.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1989}, {"title": "Musical Rhythm Recognition Using Hidden Markov Model (in Japanese)", "author": ["T. Otsuki", "N. Saitou", "M. Nakai", "H. Shimodaira", "S. Sagayama"], "venue": "J. Information Processing Society of Japan, vol. 43, no. 2, pp. 245\u2013255, 2002.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Hidden Markov Model for Automatic Transcription of MIDI Signals", "author": ["H. Takeda", "T. Otsuki", "N. Saito", "M. Nakai", "H. Shimodaira", "S. Sagayama"], "venue": "Proc. MMSP, pp. 428\u2013431, 2002.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Rhythm and Tempo Analysis Toward Automatic Music Transcription", "author": ["H. Takeda", "T. Nishimoto", "S. Sagayama"], "venue": "Proc. ICASSP, vol. 4, pp. 1317\u20131320, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Automated Rhythm Transcription", "author": ["C. Raphael"], "venue": "Proc. ISMIR, pp. 99\u2013 107, 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "A Learning-Based Quantization: Unsupervised Estimation of the Model Parameters", "author": ["M. Hamanaka", "M. Goto", "H. Asoh", "N. Otsu"], "venue": "Proc. ICMC, pp. 369\u2013372, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Monte Carlo Methods for Tempo Tracking and Rhythm Quantization", "author": ["A. Cemgil", "B. Kappen"], "venue": "J. Artificial Intelligence Res., vol. 18, no. 1, pp. 45\u201381, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "A Unified Probabilistic Model for Polyphonic Music Analysis", "author": ["D. Temperley"], "venue": "J. New Music Res., vol. 38, no. 1, pp. 3\u201318, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Improving Metrical Grammar with Grammar Expansion", "author": ["M. Tanji", "D. Ando", "H. Iba"], "venue": "Proc. AI 2008 (Springer LNAI 5360), pp. 180\u2013 191, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Probabilistic Model of Two-Dimensional Rhythm Tree Structure Representation for Automatic Transcription of Polyphonic MIDI Signals", "author": ["M. Tsuchiya", "K. Ochiai", "H. Kameoka", "S. Sagayama"], "venue": "Proc. APSIPA, pp. 1\u20136, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic Transcription from MIDI Signals of Music Performance Using 2-Dimensional LR Parser (in Japanese)", "author": ["N. Takamune", "H. Kameoka", "S. Sagayama"], "venue": "Tech. Rep. SIGMUS, vol. 2014-MUS-104, no. 7, pp. 1\u20136, 2014.  IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. XX, NO. YY, 2017  13", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Rhythm Transcription of MIDI Performances Based on Hierarchical Bayesian Modelling of Repetition and Modification of Musical Note Patterns", "author": ["E. Nakamura", "K. Itoyama", "K. Yoshii"], "venue": "Proc. EUSIPCO, pp. 1946\u2013 1950, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1946}, {"title": "Rhythm Transcription of Polyphonic MIDI Performances Based on a Merged-Output HMM for Multiple Voices", "author": ["E. Nakamura", "K. Yoshii", "S. Sagayama"], "venue": "Proc. SMC, pp. 338\u2013343, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition", "author": ["L. Rabiner"], "venue": "Proc. IEEE, vol. 77, no. 2, pp. 257\u2013286, 1989.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1989}, {"title": "Data Processing in Music Performance Research: Using Structural Information to Improve Score- Performance Matching", "author": ["H. Heijink", "L. Windsor", "P. Desain"], "venue": "Behavior Research Methods, Instruments, & Computers, vol. 32, no. 4, pp. 546\u2013554, 2000.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Improved Score-Performance Matching Using Both Structural and Temporal Information from MIDI Recordings", "author": ["B. Gingras", "S. McAdams"], "venue": "J. New Music Res., vol. 40, no. 1, pp. 43\u201357, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Merged-Output Hidden Markov Model for Score Following of MIDI Performance with Ornaments, Desynchronized Voices, Repeats and Skips", "author": ["E. Nakamura", "Y. Saito", "N. Ono", "S. Sagayama"], "venue": "Proc. Joint ICMC|SMC 2014, pp. 1185\u20131192, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Merged-Output HMM for Piano Fingering of Both Hands", "author": ["E. Nakamura", "N. Ono", "S. Sagayama"], "venue": "Proc. ISMIR, pp. 531\u2013536, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Context-Free 2D Tree Structure Model of Musical Notes for Bayesian Modeling of Polyphonic Spectrograms", "author": ["H. Kameoka", "K. Ochiai", "M. Nakano", "M. Tsuchiya", "S. Sagayama"], "venue": "Proc. ISMIR, pp. 307\u2013312, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Demo Page of Polyphonic Rhythm Transcription", "author": ["E. Nakamura", "K. Yoshii", "S. Sagayama"], "venue": "[Online]. Available: http:// anonymous4721029.github.io/demo.html, Accessed on:", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Representation and Discovery of Vertical Patterns in Music", "author": ["D. Conklin"], "venue": "A. Smaill et al. (eds.), Music and Artificial Intelligence, Lecture Notes in Artificial Intelligence, Springer, pp. 32\u201342, 2002.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "A Stochastic Temporal Model of Polyphonic MIDI Performance with Ornaments", "author": ["E. Nakamura", "N. Ono", "S. Sagayama", "K. Watanabe"], "venue": "J. New Music Res., vol. 44, no. 4, pp. 287\u2013304, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "A Coupled Duration-Focused Architecture for Realtime Music to Score Alignment", "author": ["A. Cont"], "venue": "IEEE Trans. on PAMI, vol. 32, no. 6, pp. 974\u2013987, 2010.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Factorial Hidden Markov Models", "author": ["Z. Ghahramani", "M. Jordan"], "venue": "Machine Learning, vol. 29, pp. 245\u2013273, 1997.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1997}, {"title": "Automatic Segmentation of Acoustic Musical Signals Using Hidden Markov Models", "author": ["C. Raphael"], "venue": "IEEE Trans. on PAMI, vol. 21, no. 4, pp. 360\u2013370, 1999.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1999}, {"title": "A Conditional Random Field Framework for Robust and Scalable Audio-to-Score Matching", "author": ["C. Joder", "S. Essid", "G. Richard"], "venue": "IEEE Trans. on ASLP, vol. 19, no. 8, pp. 2385\u20132397, 2011.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Inferring Metrical Structure in Music Using Particle Filters", "author": ["F. Krebs", "A. Holzapfel", "A.T. Cemgil", "G. Widmer"], "venue": "IEEE Trans. on ASLP, vol. 23, no. 5, pp. 817\u2013827, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "On Tempo Tracking: Tempogram Representation and Kalman Filtering", "author": ["A.T. Cemgil", "B. Kappen", "P. Desain", "H. Honing"], "venue": "J. New Music Res., vol. 29, no. 4, pp. 259\u2013273, 2000.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2000}, {"title": "A New Music Database Describing Deviation Information of Performance Expressions", "author": ["M. Hashida", "T. Matsui", "H. Katayose"], "venue": "Proc. ISMIR, pp. 489\u2013494, 2008.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "piano) music has been receiving much attention [1], [2].", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "To solve the other part of the transcription problem, many studies have been devoted to so-called rhythm transcription, that is, the problem of recognising quantised note lengths (or note values) of the musical notes in MIDI performances [3]\u2013[19].", "startOffset": 238, "endOffset": 241}, {"referenceID": 17, "context": "To solve the other part of the transcription problem, many studies have been devoted to so-called rhythm transcription, that is, the problem of recognising quantised note lengths (or note values) of the musical notes in MIDI performances [3]\u2013[19].", "startOffset": 242, "endOffset": 246}, {"referenceID": 6, "context": "One of the models most frequently used in recent studies [8]\u2013[13], [18], [19] is the hidden Markov model (HMM) [20].", "startOffset": 57, "endOffset": 60}, {"referenceID": 11, "context": "One of the models most frequently used in recent studies [8]\u2013[13], [18], [19] is the hidden Markov model (HMM) [20].", "startOffset": 61, "endOffset": 65}, {"referenceID": 16, "context": "One of the models most frequently used in recent studies [8]\u2013[13], [18], [19] is the hidden Markov model (HMM) [20].", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "One of the models most frequently used in recent studies [8]\u2013[13], [18], [19] is the hidden Markov model (HMM) [20].", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "One of the models most frequently used in recent studies [8]\u2013[13], [18], [19] is the hidden Markov model (HMM) [20].", "startOffset": 111, "endOffset": 115}, {"referenceID": 8, "context": "II, a conventional way of representing a polyphonic score as a linear sequence of chords [10] may not retain sequential regularities within voices, such as those in polyrhythmic scores, nor it can capture the loose synchrony between voices [21], [22] in polyphonic performances.", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "II, a conventional way of representing a polyphonic score as a linear sequence of chords [10] may not retain sequential regularities within voices, such as those in polyrhythmic scores, nor it can capture the loose synchrony between voices [21], [22] in polyphonic performances.", "startOffset": 240, "endOffset": 244}, {"referenceID": 20, "context": "II, a conventional way of representing a polyphonic score as a linear sequence of chords [10] may not retain sequential regularities within voices, such as those in polyrhythmic scores, nor it can capture the loose synchrony between voices [21], [22] in polyphonic performances.", "startOffset": 246, "endOffset": 250}, {"referenceID": 17, "context": "From this point of view, in a recent conference [19], we reported a statistical model that can describe the multiplevoice structure of polyphonic music.", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "The model is based on the merged-output HMM [23], [24], which describes polyphonic performances as merged outputs from multiple component HMMs, called voice HMMs, each of which describes the generative process of music scores and performances of one voice (Fig.", "startOffset": 44, "endOffset": 48}, {"referenceID": 22, "context": "The model is based on the merged-output HMM [23], [24], which describes polyphonic performances as merged outputs from multiple component HMMs, called voice HMMs, each of which describes the generative process of music scores and performances of one voice (Fig.", "startOffset": 50, "endOffset": 54}, {"referenceID": 21, "context": "Due to the large size of the state space and the complex dependencies between variables, the standard Viterbi algorithm or its refined version [23] cannot be applied and a new inference technique is", "startOffset": 143, "endOffset": 147}, {"referenceID": 22, "context": "[24], we develop an inference technique that can work in a practical computer environment and could be applied for any merged-output HMMs with autoregressive voice HMMs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "In addition to two HMM-based methods [8], [9], [11], [12] previously tested in Ref.", "startOffset": 37, "endOffset": 40}, {"referenceID": 7, "context": "In addition to two HMM-based methods [8], [9], [11], [12] previously tested in Ref.", "startOffset": 42, "endOffset": 45}, {"referenceID": 9, "context": "In addition to two HMM-based methods [8], [9], [11], [12] previously tested in Ref.", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "In addition to two HMM-based methods [8], [9], [11], [12] previously tested in Ref.", "startOffset": 53, "endOffset": 57}, {"referenceID": 17, "context": "[19], we tested frequently cited methods and theoretically important methods whose source codes were available: Connectionist Quantizer [7], Melisma Analyzers (version 1 [6] and version 2 [14]) and two-dimensional (2D) probabilistic context-free grammar (PCFG) model [16], [17], [25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[19], we tested frequently cited methods and theoretically important methods whose source codes were available: Connectionist Quantizer [7], Melisma Analyzers (version 1 [6] and version 2 [14]) and two-dimensional (2D) probabilistic context-free grammar (PCFG) model [16], [17], [25].", "startOffset": 136, "endOffset": 139}, {"referenceID": 4, "context": "[19], we tested frequently cited methods and theoretically important methods whose source codes were available: Connectionist Quantizer [7], Melisma Analyzers (version 1 [6] and version 2 [14]) and two-dimensional (2D) probabilistic context-free grammar (PCFG) model [16], [17], [25].", "startOffset": 170, "endOffset": 173}, {"referenceID": 12, "context": "[19], we tested frequently cited methods and theoretically important methods whose source codes were available: Connectionist Quantizer [7], Melisma Analyzers (version 1 [6] and version 2 [14]) and two-dimensional (2D) probabilistic context-free grammar (PCFG) model [16], [17], [25].", "startOffset": 188, "endOffset": 192}, {"referenceID": 14, "context": "[19], we tested frequently cited methods and theoretically important methods whose source codes were available: Connectionist Quantizer [7], Melisma Analyzers (version 1 [6] and version 2 [14]) and two-dimensional (2D) probabilistic context-free grammar (PCFG) model [16], [17], [25].", "startOffset": 267, "endOffset": 271}, {"referenceID": 15, "context": "[19], we tested frequently cited methods and theoretically important methods whose source codes were available: Connectionist Quantizer [7], Melisma Analyzers (version 1 [6] and version 2 [14]) and two-dimensional (2D) probabilistic context-free grammar (PCFG) model [16], [17], [25].", "startOffset": 273, "endOffset": 277}, {"referenceID": 23, "context": "[19], we tested frequently cited methods and theoretically important methods whose source codes were available: Connectionist Quantizer [7], Melisma Analyzers (version 1 [6] and version 2 [14]) and two-dimensional (2D) probabilistic context-free grammar (PCFG) model [16], [17], [25].", "startOffset": 279, "endOffset": 283}, {"referenceID": 17, "context": "[19], is explained in full detail together with its calculation algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "We make public the source codes for the best models found (the proposed model and other two HMMs) as well as the evaluation tool to enable future comparisons [26].", "startOffset": 158, "endOffset": 162}, {"referenceID": 17, "context": "[19] to make this section more informative and self-contained.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Longuet-Higgins [3] developed a method for estimating the note values and the j \u0153 j \u0153 \u0153 \u0153 Note model", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "A similar method of dividing a time interval using template grids and an error function of onsets and inter-onset intervals (IOIs) has also been proposed [4].", "startOffset": 154, "endOffset": 157}, {"referenceID": 3, "context": "[5] and Temperley et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] proposed a connectionist approach that iteratively converts note durations so that adjacent durations tend to have simple integral ratios.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "2) [8], [9].", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "2) [8], [9].", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "In another class of HMMs, which we call metrical HMMs, a different description is used for the score model [11]\u2013[13].", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "In another class of HMMs, which we call metrical HMMs, a different description is used for the score model [11]\u2013[13].", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "PCFG models have also been proposed [15], [16].", "startOffset": 36, "endOffset": 40}, {"referenceID": 14, "context": "PCFG models have also been proposed [15], [16].", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "As in [3], a time interval in a score is recursively divided into shorter intervals until those corresponding to note values are obtained, and probabilities describe what particular divisions are likely.", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "The note HMM has been extended to handle polyphonic performances [10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "Such score representation is also familiarly used for music analysis [27] and score-performance matching [28], [29].", "startOffset": 69, "endOffset": 73}, {"referenceID": 26, "context": "Such score representation is also familiarly used for music analysis [27] and score-performance matching [28], [29].", "startOffset": 105, "endOffset": 109}, {"referenceID": 27, "context": "Such score representation is also familiarly used for music analysis [27] and score-performance matching [28], [29].", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "two hands in piano performances [21]), called voice asynchrony, cannot be described.", "startOffset": 32, "endOffset": 36}, {"referenceID": 19, "context": "the importance of incorporating the multiple-voice structure in the presence of voice asynchrony is well investigated in studies on score-performance matching [21], [22].", "startOffset": 159, "endOffset": 163}, {"referenceID": 20, "context": "the importance of incorporating the multiple-voice structure in the presence of voice asynchrony is well investigated in studies on score-performance matching [21], [22].", "startOffset": 165, "endOffset": 169}, {"referenceID": 14, "context": "To describe the multiple-voice structure of polyphonic scores, an extension of the PCFG model called 2D PCFG model has been proposed [16], [25].", "startOffset": 133, "endOffset": 137}, {"referenceID": 23, "context": "To describe the multiple-voice structure of polyphonic scores, an extension of the PCFG model called 2D PCFG model has been proposed [16], [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 15, "context": "[17] state that this problem is solved using a generalised LR parser.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Based on the fact that HMM is effective for monophonic music [8]\u2013[13], an HMM-based model that can describe multiple-voice structure of symbolic music, called mergedoutput HMM, has been proposed [23], [24].", "startOffset": 61, "endOffset": 64}, {"referenceID": 11, "context": "Based on the fact that HMM is effective for monophonic music [8]\u2013[13], an HMM-based model that can describe multiple-voice structure of symbolic music, called mergedoutput HMM, has been proposed [23], [24].", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "Based on the fact that HMM is effective for monophonic music [8]\u2013[13], an HMM-based model that can describe multiple-voice structure of symbolic music, called mergedoutput HMM, has been proposed [23], [24].", "startOffset": 195, "endOffset": 199}, {"referenceID": 22, "context": "Based on the fact that HMM is effective for monophonic music [8]\u2013[13], an HMM-based model that can describe multiple-voice structure of symbolic music, called mergedoutput HMM, has been proposed [23], [24].", "startOffset": 201, "endOffset": 205}, {"referenceID": 21, "context": "The whole process is described as an HMM with a state space indexed by k = (s, i, i) and the transition and output probabilities (in the non-interacting case [23]) are given as", "startOffset": 158, "endOffset": 162}, {"referenceID": 17, "context": "[19], the merged-output HMM can be seen as a variant of factorial HMM [30] in its most general sense.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[19], the merged-output HMM can be seen as a variant of factorial HMM [30] in its most general sense.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "We present a complete description of a rhythm transcription method based on merged-output HMM [19] that describes polyphonic performances with multiple-voice structure.", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "[19] is reviewed here with additional details.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "1) Model for Each Voice: A voice HMM is constructed based on the note HMM [9], which is extended to explicitly model pitches in order to appropriately describe voices.", "startOffset": 74, "endOffset": 77}, {"referenceID": 26, "context": "To describe the temporal fluctuations, we introduce a tempo variable, denoted by vn, that describes the local (inverse) tempo for the time interval between the n-th and (n+1)-th note onsets [28], [31].", "startOffset": 190, "endOffset": 194}, {"referenceID": 29, "context": "To describe the temporal fluctuations, we introduce a tempo variable, denoted by vn, that describes the local (inverse) tempo for the time interval between the n-th and (n+1)-th note onsets [28], [31].", "startOffset": 196, "endOffset": 200}, {"referenceID": 26, "context": "gn = 0), their IOI approximately obeys an exponential distribution [28] and the probability of the onset time of the (n+1)-th note, denoted by tn+1, is then given as", "startOffset": 67, "endOffset": 71}, {"referenceID": 26, "context": "The method for determining the other parameters based on a principle of minimal prediction error is discussed in a previous study [28] and will not be repeated here.", "startOffset": 130, "endOffset": 134}, {"referenceID": 22, "context": "The inference algorithm of merged-output HMM has been discussed previously [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "Since a merged-output HMM can be seen as an HMM with a product state space, the Viterbi algorithm [20] can be applied for inference in principle.", "startOffset": 98, "endOffset": 102}, {"referenceID": 30, "context": "Discretisation of tempo variables has also been used for audio-toscore alignment [32] and beat tracking [33].", "startOffset": 81, "endOffset": 85}, {"referenceID": 31, "context": "Discretisation of tempo variables has also been used for audio-toscore alignment [32] and beat tracking [33].", "startOffset": 104, "endOffset": 108}, {"referenceID": 6, "context": "In a few studies that reported systematic evaluations of rhythm transcription [8], [9], [13], editing costs (i.", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "In a few studies that reported systematic evaluations of rhythm transcription [8], [9], [13], editing costs (i.", "startOffset": 83, "endOffset": 86}, {"referenceID": 11, "context": "In a few studies that reported systematic evaluations of rhythm transcription [8], [9], [13], editing costs (i.", "startOffset": 88, "endOffset": 92}, {"referenceID": 6, "context": "Since results of rhythm transcription often contain note values that are uniformly scaled from the correct values, which should not be considered as completely incorrect estimations [8], [34], we must take into account the scaling operation as well as the shift operation.", "startOffset": 182, "endOffset": 185}, {"referenceID": 32, "context": "Since results of rhythm transcription often contain note values that are uniformly scaled from the correct values, which should not be considered as completely incorrect estimations [8], [34], we must take into account the scaling operation as well as the shift operation.", "startOffset": 187, "endOffset": 191}, {"referenceID": 5, "context": "II, the following six were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metrical HMM [11] and the 2D PCFG model [16], [17].", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": "II, the following six were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metrical HMM [11] and the 2D PCFG model [16], [17].", "startOffset": 104, "endOffset": 107}, {"referenceID": 12, "context": "II, the following six were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metrical HMM [11] and the 2D PCFG model [16], [17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 7, "context": "II, the following six were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metrical HMM [11] and the 2D PCFG model [16], [17].", "startOffset": 148, "endOffset": 151}, {"referenceID": 9, "context": "II, the following six were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metrical HMM [11] and the 2D PCFG model [16], [17].", "startOffset": 170, "endOffset": 174}, {"referenceID": 14, "context": "II, the following six were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metrical HMM [11] and the 2D PCFG model [16], [17].", "startOffset": 197, "endOffset": 201}, {"referenceID": 15, "context": "II, the following six were directly compared: Connectionist Quantizer [7], Melisma Analyzers (the first [6] and second [14] versions), the note HMM [9], the metrical HMM [11] and the 2D PCFG model [16], [17].", "startOffset": 203, "endOffset": 207}, {"referenceID": 33, "context": "taken from the PEDB database [35], a few were performances we recorded, and the rests were taken from collections in public domain websites2.", "startOffset": 29, "endOffset": 33}, {"referenceID": 26, "context": "Values for the parameters for the performance model, \u03c3v , \u03c3t and \u03bb, were taken from a previous study [28] (which used performance data different from ours).", "startOffset": 101, "endOffset": 105}, {"referenceID": 24, "context": "2The list of used pieces is available on our web page [26].", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 15.", "startOffset": 28, "endOffset": 32}, {"referenceID": 7, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 15.", "startOffset": 42, "endOffset": 45}, {"referenceID": 15, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 15.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 15.", "startOffset": 73, "endOffset": 76}, {"referenceID": 12, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 15.", "startOffset": 91, "endOffset": 95}, {"referenceID": 9, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 8.", "startOffset": 28, "endOffset": 32}, {"referenceID": 7, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 8.", "startOffset": 42, "endOffset": 45}, {"referenceID": 15, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 8.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 8.", "startOffset": 73, "endOffset": 76}, {"referenceID": 12, "context": "Proposed model Metrical HMM [11] Note HMM [9] 2D PCFG [17] Melisma ver 1 [6] Melisma ver 2 [14] 8.", "startOffset": 91, "endOffset": 95}, {"referenceID": 7, "context": "Data set Model Unprocessed R \u2212Rprop [%] Polyrhythmic Note HMM [9] 0 12.", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": "8 Metrical HMM [11] 0 13.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "1 2-dim PCFG [17] 18 23.", "startOffset": 13, "endOffset": 17}, {"referenceID": 4, "context": "9 Melisma ver 1 [6] 9 17.", "startOffset": 16, "endOffset": 19}, {"referenceID": 12, "context": "7 Melisma ver 2 [14] 4 21.", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "3 Connectionist [7] 0 38.", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "Non-polyrhythmic Note HMM [9] 0 \u22120.", "startOffset": 26, "endOffset": 29}, {"referenceID": 9, "context": "50 Metrical HMM [11] 0 \u22120.", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "61 2-dim PCFG [17] 25 1.", "startOffset": 14, "endOffset": 18}, {"referenceID": 4, "context": "64 Melisma ver 1 [6] 4 1.", "startOffset": 17, "endOffset": 20}, {"referenceID": 12, "context": "12 Melisma ver 2 [14] 11 \u22120.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "33 Connectionist [7] 0 30.", "startOffset": 17, "endOffset": 20}, {"referenceID": 24, "context": "3Other examples and sound files are accessible in our demonstration web page [26].", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "In a previous study [24] that discussed the same cutoff, it has been empirically confirmed that Nh = 50 yields almost exact results for piano performances.", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "III-A, we propose a two-level hierarchical HMM for the description of chords, replacing self-transitions used in previous studies [10], [19].", "startOffset": 130, "endOffset": 134}, {"referenceID": 17, "context": "III-A, we propose a two-level hierarchical HMM for the description of chords, replacing self-transitions used in previous studies [10], [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 26, "context": "4) Influence of the Model Parameters: In our implementation, parameters of the tempo variables (mainly \u03c3v , \u03c3t and \u03bb) were not optimised but adjusted to values measured in a completely different experiment [28].", "startOffset": 206, "endOffset": 210}, {"referenceID": 21, "context": "The use of merged-output HMMs with interacting voice HMMs [23] could provide a solution in principle, but how to describe synchrony of global score times while retaining computational tractability is a remaining problem.", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "(29) can be calculated by the Viterbi algorithm [20] with computational complexity O((#\u03a9)N) < O(N rN) where Nr is the number of note-value types.", "startOffset": 48, "endOffset": 52}], "year": 2017, "abstractText": "In a recent conference paper, we have reported a rhythm transcription method based on a merged-output hidden Markov model (HMM) that explicitly describes the multiple-voice structure of polyphonic music. This model solves a major problem of conventional methods that could not properly describe the nature of multiple voices as in polyrhythmic scores or in the phenomenon of loose synchrony between voices. In this paper we present a complete description of the proposed model and develop an inference technique, which is valid for any merged-output HMMs for which output probabilities depend on past events. We also examine the influence of the architecture and parameters of the method in terms of accuracies of rhythm transcription and voice separation and perform comparative evaluations with six other algorithms. Using MIDI recordings of classical piano pieces, we found that the proposed model outperformed other methods by more than 12 points in the accuracy for polyrhythmic performances and performed almost as good as the best one for non-polyrhythmic performances. This reveals the state-ofthe-art methods of rhythm transcription for the first time in the literature. Publicly available source codes are also provided for future comparisons.", "creator": "LaTeX with hyperref package"}}}