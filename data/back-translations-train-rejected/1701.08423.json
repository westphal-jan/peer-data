{"id": "1701.08423", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2017", "title": "On the Local Structure of Stable Clustering Instances", "abstract": "In this paper, we analyze the performance of a simple and standard Local Search algorithm for clustering on well behaved data. Since the seminal paper by Ostrovsky, Rabani, Schulman and Swamy [FOCS 2006], much progress has been made to characterize real-world instances. We distinguish the three main definitions -- Distribution Stability (Awasthi, Blum, Sheffet, FOCS 2010) -- Spectral Separability (Kumar, Kannan, FOCS 2010) -- Perturbation Resilience (Bilu, Linial, ICS 2010) We show that Local Search performs well on the instances with the aforementioned stability properties. Specifically, for the $k$-means and $k$-median objective, we show that Local Search exactly recovers the optimal clustering if the dataset is $3+\\varepsilon$-perturbation resilient, and is a PTAS for distribution stability and spectral separability. This implies the first PTAS for instances satisfying the spectral separability condition. For the distribution stability condition we also go beyond previous work by showing that the clustering output by the algorithm and the optimal clustering are very similar. This is a significant step toward understanding the success of Local Search heuristics in clustering applications and supports the legitimacy of the stability conditions: They characterize some of the structure of real-world instances that make Local Search a popular heuristic.", "histories": [["v1", "Sun, 29 Jan 2017 19:55:27 GMT  (46kb)", "http://arxiv.org/abs/1701.08423v1", null], ["v2", "Fri, 7 Apr 2017 08:46:26 GMT  (112kb,D)", "http://arxiv.org/abs/1701.08423v2", null], ["v3", "Thu, 10 Aug 2017 09:46:07 GMT  (116kb,D)", "http://arxiv.org/abs/1701.08423v3", null]], "reviews": [], "SUBJECTS": "cs.DS cs.CG cs.LG", "authors": ["vincent cohen-addad", "chris schwiegelshohn"], "accepted": false, "id": "1701.08423"}, "pdf": {"name": "1701.08423.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 1.08 423v 1 [cs.D S] \u2022 Distribution stability (Awasthi, Blum, Sheffet, FOCS 2010) \u2022 Spectral separability (Kumar, Kannan, FOCS 2010) \u2022 Perturbation resilience (Bilu, Linial, ICS 2010) and show that local search performs well in cases with the above stability characteristics. In particular, for the k-mean and k-median target, we show that local search accurately restores optimal cluster stability when the dataset is 3 + \u03b5 interference resistant, and is a PTAS for distribution stability and spectral separability. This implies the first PTAS for cases that meet the spectral separability condition. For distribution stability, we also go beyond previous work by showing that cluster results through the algorithm and optimal cluster formation are very similar to the locational search of the loceustic structure, which is a very important step to understanding the loceutical structure in hay centers."}, {"heading": "1 Introduction", "text": "Clustering is a pervasive problem; the goal is to distribute data points according to similarity; from the user's point of view, the appropriateness of a particular objective function depends on the underlying structure; for example, if the data is generated by a mixture of Gaussian units, the problem is often modelled by the k-mean problem; the appropriateness of a model gives clustering its simple in practice, hard in theory quality: on the one hand, a benchmark algorithm often leads to good clustering with a suitable model; on the other, many cluster goals are difficult to approximate. To close this gap, preparatory work usually proceeds in two steps: (1) characterization of the properties of natural clustering of the underlying data; and (2) design of an algorithm that uses such properties that then circumvent traditional hardness outcomes. At this point, there is a wide variety of (1) characterizations of well-behaved instances and (2) algorithms that are well-coordinated."}, {"heading": "1.1 Our Contribution", "text": "Loosely speaking, we show that the three most important stability conditions capture some of the practical input factors that make the results of the search more efficient. (1 + \u03b2) -Approach and \"Restore most of the structure of the optimal solution\" (see Theorem 1.1) - This is the first step towards cost function and a (1 \u2212 \u03b5) -approximate classification for most clusters. Spectral separability Local search achieves a (1 + \u03b5) approach (see Theorem 1.3) - this is the first PTAS for the problem. Perturbation Resilience Every local optimum is a global optimum for the global stability of alpha-perturbation-resilient substances with alpha > 3 (see Theorem 1.2).This results in a unified and simple approach to stability conditions. There are two possible high-level interpretations of our results: (1), since Local Search heuristics are widely used by practitioners."}, {"heading": "1.2 Related Work", "text": "The problems we are examining are NP-hard: k-median and k-means are already NP-hard in the Euclidean plane (see Meggido and Supowit [67], Mahajan et al. [64], and Dasgupta et al. [51], Guruswami et al. [47], and Awasthi et al. [12], on the positive side are constant factor approximations in metric space for both k-median and Khuller [44], Jain et al. [47], Guruswami et al. [47], and Awasthi et al. [12], constant factor approximations are known in metric space (see for example [63, 52, 68]. Given the hardness results, how can one hope to achieve a (1 +) approximation?"}, {"heading": "2 Approach and Techniques", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Distribution Stability", "text": "The first important observation is that only a few clusters have more than a fraction of the total cost of the solution; for these clusters, local search with a corresponding neighborhood size can find the optimal solution; among the remaining clusters, the centers and points near the center are far apart; any locally optimal solution cannot fail on too many of these clusters; the cost of loading the points of the remaining clusters can then be included in the total contribution, which allows us to bind the approximation factor, see Figure 2. Our evidence contains a few ingredients from [10] such as the notion of the inner ring (we work with a more general definition) and the distinction between cheap and expensive clusters. However, our analysis is more general, as it allows us to analyze not only the cost of solving the algorithm, but also the structure of the clusters."}, {"heading": "2.2 Perturbation Resilience", "text": "The narrow approximation factor of 3 + \u03b5 of the local search for k-median implies a localization gap of 4 out of 3. The most important observation is that failure resistance implies that locally optimal solutions for a local neighborhood of appropriate size correspond to the global optimum."}, {"heading": "2.3 Spectral Separability", "text": "In Section 4 we examine the spectral separation conditions for the Euclidean k-mean problem. Indeed, this is the first step of the algorithm of Kumar and Kannan [60] (see algorithm 3). Algorithm 3 k-means with spectral initialization [60] 1: Project points to the best rank k-space 2: Compute a clustering C with constant approximation factor on the projection 3: Initialize centroids of each cluster of C as centers in the original space 4: Run Lloyd's k-means until convergenceIn general, Projecting on the best rank k subspace 2: Compute a clustering C with constant approximation factor on the projection 3: Initialize centroids of each cluster of C as centers in the original space 4: Run Lloyd's k-means until convergenceIn general, Projecting on the best rank k subspace and computing a constant approximation results in the constant approximation of the projection 3: Initialize centroids of each cluster of C as centers in the original space: Run Lloyd's k-means until convergenceIn the best rank k subspace and computing a constant approximation results in the projection 3: Initialister roids in the projection centres each cluster C-means in the original clusters."}, {"heading": "3 Metric Spaces", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Preliminaries", "text": "In this section, we will look at inputs that consist of both a group of client A and a group of candidate centers F, together with a distance function dist: A-F-F-F-F-R + 5. Allow p-M and k-M problems with p = 1 and 2 and p-M problems, respectively. To provide a slightly simpler proof, we will assume that p = 1. Applying the following two terms in different steps of the evidence ensures that the result applies to a higher value of p (by introducing a dependency in 1 / \u03b5O (p) in the neighborhood size of the algorithm). Let p-0 and 1 / 2 > 0."}, {"heading": "3.2 Distribution Stability", "text": "We work with the term \u03b2, \u03b4-distribution stability, which generalizes the stability of the distribution, extending to the costs incurred for each cluster of the optimal solution, with most points meeting \u03b2-distribution stability. Definition 3.1 ((\u03b2, \u03b4) distribution stability) Let (A, F, costs, k) be an instance of k-clustering where A-F is located in a metric space and let C-1,. (A, costs, k) be a division of A and S-1 instance = {c-instance 1,., c-K) F is a series of centers. Further, let \u03b2 > 0 and 1 / 2 > C-client. (A, costs, k), (C, S-instance) is a (\u03b2,) -distribution stable instance, if there is a stable instance."}, {"heading": "3.3 \u03b1-Perturbation-Resilient Instances", "text": "Let I = (A, F, cost, k) be an instance of the k-failure resistance problem. For \u03b1-failure resistance, I is if there is a unique optimal cluster solution. (P, q) The unique optimal cluster solution is {C, q, k} and for each instance I \u2032 = (A, F, cost, k, p). Note that cost \u00b2 P, cost (P, q) \u2264 cost \u00b2 (P, q)."}, {"heading": "4 Spectral Separability", "text": "In this section we will define the spectral separability state for the Euclidean k-mean problem. Our main result will be a proof for the theorem 1.3.Let us first remember the basic terms and definitions for the Euclidean k-mean. Let us then be a series of points in d-dimensional Euclidean space where the series Ai contains the coordinates of the n-dimensional point. The singular value distribution is defined as A = UB value T, where U-Rn \u00b7 d and V-Rd \u00b7 d are orthogonal and V-Rd \u00b7 d is a diagonal matrix containing the singular values, where per convention the singular values are given in descending order, i.e.."}, {"heading": "5 Acknowledgments", "text": "The authors thank their dedicated consultant for this project: Claire Mathieu. Without her, this collaboration would not have been possible."}, {"heading": "B Euclidean Distribution Stability", "text": "In this section we show how the Euclidean problem can be reduced to the discrete version. Our analysis focuses on the k-mean problem, but we point out that discrediting works for all values of cost = distp, where dependence on p grows exponentially. For the constant p, we obtain polynomial candidate solutions in polynomial time. For k-means itself, we could alternatively combine Matousek's approximate centric approach [65] with the Johnson Lindenstrauss dilemma and avoid the following construction; however, this would only require optimal distribution of stable clusterings and proof Theorem 1.3 that there is a well for non-optimal clusterings. First, we describe a discrediting process [65]. It will be important for us that the candidate solution (1) covers the costs of each given group of centers and (2) distribution stability.For a set of points P is a set of P, a set of points Nually."}, {"heading": "C \u03b1-Perturbation Resilience", "text": "We also define the optimal centers induced by clustering (A, F, costs, k). We first show how Theorem 3.14 enables us to prove Theorem 1.2.Proof of Theorem 1.2. Faced with an instance (A, F, costs, k) we define the following instance I (A, F, costs, k) where the costs (a, b) = dist (a, b) are defined for some distance functions defined via A, F and F. For each customer c, NC (C, costs) NL (L), we leave the center of L that serves it in L, for each point p = i, we define the costs (c, p) = costs (p) and costs (c, i) = costs."}], "references": [{"title": "Local Search in Combinatorial Optimization", "author": ["Emile Aarts", "Jan K. Lenstra", "editors"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "On spectral learning of mixtures of distributions", "author": ["Dimitris Achlioptas", "Frank McSherry"], "venue": "In Learning Theory, 18th Annual Conference on Learning Theory, COLT 2005, Bertinoro,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "k-means++ under approximation stability", "author": ["Manu Agarwal", "Ragesh Jaiswal", "Arindam Pal"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Learning mixtures of arbitrary gaussians", "author": ["Sanjeev Arora", "Ravi Kannan"], "venue": "In Proceedings on 33rd Annual ACM Symposium on Theory of Computing, July 6-8,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Approximation schemes for Euclidean k -medians and related problems", "author": ["Sanjeev Arora", "Prabhakar Raghavan", "Satish Rao"], "venue": "In Proceedings of the Thirtieth Annual ACM Symposium on the Theory of Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Smoothed analysis of the k-means method", "author": ["David Arthur", "Bodo Manthey", "Heiko R\u00f6glin"], "venue": "J. ACM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "k-means++: the advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Worst-case and smoothed analysis of the ICP algorithm, with an application to the k-means method", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "SIAM J. Comput.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Local search heuristics for k-median and facility location problems", "author": ["Vijay Arya", "Naveen Garg", "Rohit Khandekar", "Adam Meyerson", "Kamesh Munagala", "Vinayaka Pandit"], "venue": "SIAM J. Comput.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Stability yields a PTAS for k-median and k-means clustering", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "In 51th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Center-based clustering under perturbation stability", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "Inf. Process. Lett.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "The hardness of approximation of Euclidean k-means", "author": ["Pranjal Awasthi", "Moses Charikar", "Ravishankar Krishnaswamy", "Ali Kemal Sinop"], "venue": "In 31st International Symposium on Computational Geometry, SoCG 2015, June 22-25,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Improved spectral-norm bounds for clustering. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques - 15th International Workshop", "author": ["Pranjal Awasthi", "Or Sheffet"], "venue": "APPROX 2012, and 16th International Workshop,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Polynomial time algorithm for 2-stable clustering", "author": ["Ainesh Bakshi", "Nadiia Chepurko"], "venue": "instances. CoRR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Approximate clustering without the approximation", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Clustering under approximation stability", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "J. ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Finding low error clusterings", "author": ["Maria-Florina Balcan", "Mark Braverman"], "venue": "In COLT 2009 - The 22nd Conference on Learning Theory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "k-center clustering under perturbation resilience", "author": ["Maria-Florina Balcan", "Nika Haghtalab", "Colin White"], "venue": "In 43rd International Colloquium on Automata, Languages, and Programming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Clustering under perturbation resilience", "author": ["Maria-Florina Balcan", "Yingyu Liang"], "venue": "SIAM J. Comput.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Agnostic clustering", "author": ["Maria-Florina Balcan", "Heiko R\u00f6glin", "Shang-Hua Teng"], "venue": "In Algorithmic Learning Theory, 20th International Conference,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "On variants of k-means clustering", "author": ["Sayan Bandyapadhyay", "Kasturi R. Varadarajan"], "venue": "CoRR, abs/1512.02985,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Distributed balanced clustering via mapping coresets", "author": ["MohammadHossein Bateni", "Aditya Bhaskara", "Silvio Lattanzi", "Vahab S. Mirrokni"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "In 51th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Data stability in clustering: A closer look", "author": ["Shalev Ben-David", "Lev Reyzin"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "On the practically interesting instances of MAXCUT", "author": ["Yonatan Bilu", "Amit Daniely", "Nati Linial", "Michael E. Saks"], "venue": "In 30th International Symposium on Theoretical Aspects of Computer Science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Are stable instances easy? Combinatorics", "author": ["Yonatan Bilu", "Nathan Linial"], "venue": "Probability & Computing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Parallel approximation algorithms for facilitylocation problems", "author": ["Guy E. Blelloch", "Kanat Tangwongsan"], "venue": "SPAA", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Streaming k-means on well-clusterable data", "author": ["Vladimir Braverman", "Adam Meyerson", "Rafail Ostrovsky", "Alan Roytman", "Michael Shindler", "Brian Tagiku"], "venue": "In Proceedings of the Twenty- Second Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Isotropic PCA and affine-invariant clustering", "author": ["S. Charles Brubaker", "Santosh Vempala"], "venue": "Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Improved combinatorial algorithms for facility location problems", "author": ["Moses Charikar", "Sudipto Guha"], "venue": "SIAM J. Comput.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "Dimensionality reduction for k-means clustering and low rank approximation", "author": ["Michael B. Cohen", "Sam Elder", "Cameron Musco", "Christopher Musco", "Madalina Persu"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "The power of Local Search for clustering", "author": ["Vincent Cohen-Addad", "Philip N. Klein", "Claire Mathieu"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Effectiveness of local search for geometric optimization", "author": ["Vincent Cohen-Addad", "Claire Mathieu"], "venue": "In 31st International Symposium on Computational Geometry, SoCG 2015, June 22-25,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Variational shape approximation", "author": ["David Cohen-Steiner", "Pierre Alliez", "Mathieu Desbrun"], "venue": "ACM Trans. Graph.,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "Graph partitioning via adaptive spectral techniques", "author": ["Amin Coja-Oghlan"], "venue": "Combinatorics, Probability & Computing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Spectral clustering with limited independence", "author": ["Anirban Dasgupta", "John E. Hopcroft", "Ravi Kannan", "Pradipta Prometheus Mitra"], "venue": "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "Annual Symposium on Foundations of Computer Science,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1999}, {"title": "Random projection trees for vector quantization", "author": ["Sanjoy Dasgupta", "Yoav Freund"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "A probabilistic analysis of EM for mixtures of separated, spherical gaussians", "author": ["Sanjoy Dasgupta", "Leonard J. Schulman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "Iterative clustering of high dimensional text data augmented by local search", "author": ["Inderjit S. Dhillon", "Yuqiang Guan", "Jacob Kogan"], "venue": "In Proceedings of the 2002 IEEE International Conference on Data Mining (ICDM 2002),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2002}, {"title": "A unified framework for approximating and clustering data", "author": ["Dan Feldman", "Michael Langberg"], "venue": "In Proceedings of the 43rd ACM Symposium on Theory of Computing,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Local search yields a PTAS for k-means in doubling metrics", "author": ["Zachary Friggstad", "Mohsen Rezapour", "Mohammad R. Salavatipour"], "venue": "CoRR, abs/1603.08976,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Tight analysis of a multiple-swap heurstic for budgeted red-blue median", "author": ["Zachary Friggstad", "Yifeng Zhang"], "venue": "In 43rd International Colloquium on Automata, Languages, and Programming,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Greedy strikes back: Improved facility location algorithms", "author": ["Sudipto Guha", "Samir Khuller"], "venue": "J. Algorithms,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1999}, {"title": "Clustering data streams: Theory and practice", "author": ["Sudipto Guha", "Adam Meyerson", "Nina Mishra", "Rajeev Motwani", "Liadan O\u2019Callaghan"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2003}, {"title": "Simpler analyses of local search algorithms for facility location", "author": ["Anupam Gupta", "Kanat Tangwongsan"], "venue": "CoRR, abs/0809.2554,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2008}, {"title": "Embeddings and non-approximability of geometric problems", "author": ["Venkatesan Guruswami", "Piotr Indyk"], "venue": "In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, January 12-14,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2003}, {"title": "J-means: a new local search heuristic for minimum sum of squares clustering", "author": ["Pierre Hansen", "Nenad Mladenovic"], "venue": "Pattern Recognition,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2001}, {"title": "Smaller coresets for k-median and k-means clustering", "author": ["Sariel Har-Peled", "Akash Kushal"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2007}, {"title": "On coresets for k-means and k-median clustering", "author": ["Sariel Har-Peled", "Soham Mazumdar"], "venue": "In Proceedings of the 36th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2004}, {"title": "A new greedy approach for facility location problems", "author": ["Kamal Jain", "Mohammad Mahdian", "Amin Saberi"], "venue": "In Proceedings on 34th Annual ACM Symposium on Theory of Computing, May 19-21,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2002}, {"title": "Approximation algorithms for metric facility location and k -median problems using the primal-dual schema and Lagrangian relaxation", "author": ["Kamal Jain", "Vijay V. Vazirani"], "venue": "J. ACM,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2001}, {"title": "Analysis of k-means++ for separable data. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques - 15th International Workshop", "author": ["Ragesh Jaiswal", "Nitin Garg"], "venue": "APPROX 2012, and 16th International Workshop,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2012}, {"title": "Extensions of Lipschitz mapping into Hilbert space. In Conf. in modern analysis and probability, volume 26 of Contemporary Mathematics, pages 189\u2013206", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "American Mathematical Society,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1984}, {"title": "The spectral method for general mixture models", "author": ["Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala"], "venue": "SIAM J. Comput.,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2008}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["Tapas Kanungo", "David M. Mount", "Nathan S. Netanyahu", "Christine D. Piatko", "Ruth Silverman", "Angela Y. Wu"], "venue": "Comput. Geom.,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2004}, {"title": "Approximation and streaming algorithms for projective clustering via random projections", "author": ["Michael Kerber", "Sharath Raghvendra"], "venue": "In Proceedings of the 27th Canadian Conference on Computational Geometry,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2015}, {"title": "A nearly linear-time approximation scheme for the euclidean k-median problem", "author": ["Stavros G. Kolliopoulos", "Satish Rao"], "venue": "SIAM J. Comput.,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2007}, {"title": "Analysis of a local search heuristic for facility location problems", "author": ["Madhukar R. Korupolu", "C. Greg Plaxton", "Rajmohan Rajaraman"], "venue": "J. Algorithms,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2000}, {"title": "Clustering with spectral norm and the k-means algorithm", "author": ["Amit Kumar", "Ravindran Kannan"], "venue": "In 51th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2010}, {"title": "Linear-time approximation schemes for clustering problems in any dimensions", "author": ["Amit Kumar", "Yogish Sabharwal", "Sandeep Sen"], "venue": "J. ACM,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2010}, {"title": "Finding meaningful cluster structure amidst background noise", "author": ["Shrinu Kushagra", "Samira Samadi", "Shai Ben-David"], "venue": "In Algorithmic Learning Theory - 27th International Conference,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2016}, {"title": "Approximating k-median via pseudo-approximation", "author": ["Shi Li", "Ola Svensson"], "venue": "In Symposium on Theory of Computing Conference,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2013}, {"title": "The planar k-means problem is NP-hard", "author": ["Meena Mahajan", "Prajakta Nimbhorkar", "Kasturi R. Varadarajan"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2012}, {"title": "On approximate geometric k-clustering", "author": ["Ji\u0155\u0131 Matousek"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2000}, {"title": "Spectral partitioning of random graphs", "author": ["Frank McSherry"], "venue": "In 42nd Annual Symposium on Foundations of Computer Science,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2001}, {"title": "On the complexity of some common geometric location problems", "author": ["Nimrod Megiddo", "Kenneth J. Supowit"], "venue": "SIAM J. Comput.,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 1984}, {"title": "The online median problem", "author": ["Ramgopal R. Mettu", "C. Greg Plaxton"], "venue": "SIAM J. Comput.,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2003}, {"title": "The effectiveness of Lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "venue": "J. ACM,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2012}, {"title": "The volume of convex bodies and Banach space geometry", "author": ["Gilles Pisier"], "venue": "Cambridge Tracts in Mathematics", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1999}, {"title": "Clustering with or without the approximation", "author": ["Frans Schalekamp", "Michael Yu", "Anke van Zuylen"], "venue": "J. Comb. Optim.,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2013}, {"title": "Clustering for edge-cost minimization (extended abstract)", "author": ["Leonard J. Schulman"], "venue": "In Proceedings of the Thirty-Second Annual ACM Symposium on Theory of Computing, May 21-23,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2000}, {"title": "A spectral algorithm for learning mixture models", "author": ["Santosh Vempala", "Grant Wang"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2004}, {"title": "Towards event source unobservability with minimum network traffic in sensor networks", "author": ["Yi Yang", "Min Shao", "Sencun Zhu", "Bhuvan Urgaonkar", "Guohong Cao"], "venue": "In Proceedings of the First ACM Conference on Wireless Network Security,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": ": [1] or [56]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 55, "context": ": [1] or [56]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "This algorithm has a polynomial running time (see [9, 33]).", "startOffset": 50, "endOffset": 57}, {"referenceID": 32, "context": "This algorithm has a polynomial running time (see [9, 33]).", "startOffset": 50, "endOffset": 57}, {"referenceID": 9, "context": "Distribution Stability Awasthi, Blum, Sheffet [10]", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "Approximation Stability Balcan, Blum, Gupta [15, 16] Cost Separation Ostrovsky, Rabani, Schulman, Swamy [69] Jaiswal, Garg [53]", "startOffset": 44, "endOffset": 52}, {"referenceID": 15, "context": "Approximation Stability Balcan, Blum, Gupta [15, 16] Cost Separation Ostrovsky, Rabani, Schulman, Swamy [69] Jaiswal, Garg [53]", "startOffset": 44, "endOffset": 52}, {"referenceID": 68, "context": "Approximation Stability Balcan, Blum, Gupta [15, 16] Cost Separation Ostrovsky, Rabani, Schulman, Swamy [69] Jaiswal, Garg [53]", "startOffset": 104, "endOffset": 108}, {"referenceID": 52, "context": "Approximation Stability Balcan, Blum, Gupta [15, 16] Cost Separation Ostrovsky, Rabani, Schulman, Swamy [69] Jaiswal, Garg [53]", "startOffset": 123, "endOffset": 127}, {"referenceID": 59, "context": "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 56, "endOffset": 60}, {"referenceID": 24, "context": "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 113, "endOffset": 121}, {"referenceID": 25, "context": "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 113, "endOffset": 121}, {"referenceID": 10, "context": "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 145, "endOffset": 149}, {"referenceID": 18, "context": "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 164, "endOffset": 168}, {"referenceID": 10, "context": "Center Proximity Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "Center Proximity Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "For example, if an instance is cost-separated then it is distribution-stable; therefore the algorithm by Awasthi, Blum and Sheffet [10] also works for cost-separated instances.", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": "[10], called \u201cdistribution stability\u201d.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "2 (Distribution Stability [10]).", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 68, "context": "[69], so Local Search is also a PTAS in their setting and also recovers most of the structure of such instances.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "3 (Perturbation Resilience [11]).", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "An optimal algorithm for 2-perturbation resilient clustering for any center-based objective function was very recently given by Bakshi and Chepurko [14].", "startOffset": 148, "endOffset": 152}, {"referenceID": 13, "context": "We do not quite match [14]: One limitation is that Local Search is not necessarily optimal for 2-perturbation resilient instances, see Proposition 3.", "startOffset": 22, "endOffset": 26}, {"referenceID": 59, "context": "4 (Spectral Separation [60]2).", "startOffset": 23, "endOffset": 27}, {"referenceID": 59, "context": "In previous work by Kumar and Kannan [60], an algorithm was given with approximation ratio 1+O(OPTk/OPTk\u22121), where OPTi denotes the value of an optimal solution using i centers.", "startOffset": 37, "endOffset": 41}, {"referenceID": 59, "context": "k/\u03b5)-spectrally separated [60].", "startOffset": 26, "endOffset": 30}, {"referenceID": 55, "context": ": [56]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 59, "context": "Hence if instances are The proximity condition of Kumar and Kannan [60] implies the spectral separation condition.", "startOffset": 67, "endOffset": 71}, {"referenceID": 66, "context": "2 Related Work The problems we study are NP-hard: k-median and k-means are already NP-hard in the Euclidean plane (see Meggido and Supowit [67], Mahajan et al.", "startOffset": 139, "endOffset": 143}, {"referenceID": 63, "context": "[64], and Dasgupta and Freud [38]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[64], and Dasgupta and Freud [38]).", "startOffset": 29, "endOffset": 33}, {"referenceID": 43, "context": "In terms of hardness of approximation, both problems are APX-hard, even in the Euclidean setting when both k and d are part of the input (see Gua and Khuller [44], Jain et al.", "startOffset": 158, "endOffset": 162}, {"referenceID": 50, "context": "[51], Guruswami et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[47] and Awasthi et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "On the positive side, constant factor approximations are known in metric space for both k-median and k-means (see for example [63, 52, 68]).", "startOffset": 126, "endOffset": 138}, {"referenceID": 51, "context": "On the positive side, constant factor approximations are known in metric space for both k-median and k-means (see for example [63, 52, 68]).", "startOffset": 126, "endOffset": 138}, {"referenceID": 67, "context": "On the positive side, constant factor approximations are known in metric space for both k-median and k-means (see for example [63, 52, 68]).", "startOffset": 126, "endOffset": 138}, {"referenceID": 40, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 175, "endOffset": 183}, {"referenceID": 60, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 175, "endOffset": 183}, {"referenceID": 4, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 203, "endOffset": 226}, {"referenceID": 31, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 203, "endOffset": 226}, {"referenceID": 41, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 203, "endOffset": 226}, {"referenceID": 57, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 203, "endOffset": 226}, {"referenceID": 48, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 203, "endOffset": 226}, {"referenceID": 49, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 203, "endOffset": 226}, {"referenceID": 68, "context": "[69] assumed that cost of an optimal clustering with k centers is smaller than an \u03b52-fraction of the cost of an optimal clustering with k\u2212 1 centers, see also Schulman [72] for an earlier condition for two clusters and the irreducibility condition by Kumar et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 71, "context": "[69] assumed that cost of an optimal clustering with k centers is smaller than an \u03b52-fraction of the cost of an optimal clustering with k\u2212 1 centers, see also Schulman [72] for an earlier condition for two clusters and the irreducibility condition by Kumar et al.", "startOffset": 168, "endOffset": 172}, {"referenceID": 60, "context": "[61].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "The popular D2 sampling technique (also known as k-means++) has an improved performance for cost separated instances compared to the worst-case O(log k)-approximation ratio [7, 28, 53, 69].", "startOffset": 173, "endOffset": 188}, {"referenceID": 27, "context": "The popular D2 sampling technique (also known as k-means++) has an improved performance for cost separated instances compared to the worst-case O(log k)-approximation ratio [7, 28, 53, 69].", "startOffset": 173, "endOffset": 188}, {"referenceID": 52, "context": "The popular D2 sampling technique (also known as k-means++) has an improved performance for cost separated instances compared to the worst-case O(log k)-approximation ratio [7, 28, 53, 69].", "startOffset": 173, "endOffset": 188}, {"referenceID": 68, "context": "The popular D2 sampling technique (also known as k-means++) has an improved performance for cost separated instances compared to the worst-case O(log k)-approximation ratio [7, 28, 53, 69].", "startOffset": 173, "endOffset": 188}, {"referenceID": 9, "context": "[10] where the cost of assigning all the points from one cluster in the optimal k-clustering to another center increases the objective by some factor (1 + \u03b1).", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 3, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 22, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 28, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 34, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 35, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 36, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 38, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 54, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 65, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 72, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 14, "context": "[15, 16] gave a deterministic condition called approximation stability under which a target clustering can be retrieved by via any sufficiently good algorithm for the k-means (or kmedian) objective function.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[15, 16] gave a deterministic condition called approximation stability under which a target clustering can be retrieved by via any sufficiently good algorithm for the k-means (or kmedian) objective function.", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "For results using approximation stability, see [16, 17, 71, 3, 10].", "startOffset": 47, "endOffset": 66}, {"referenceID": 16, "context": "For results using approximation stability, see [16, 17, 71, 3, 10].", "startOffset": 47, "endOffset": 66}, {"referenceID": 70, "context": "For results using approximation stability, see [16, 17, 71, 3, 10].", "startOffset": 47, "endOffset": 66}, {"referenceID": 2, "context": "For results using approximation stability, see [16, 17, 71, 3, 10].", "startOffset": 47, "endOffset": 66}, {"referenceID": 9, "context": "For results using approximation stability, see [16, 17, 71, 3, 10].", "startOffset": 47, "endOffset": 66}, {"referenceID": 59, "context": "Another deterministic condition that relates target clustering recovery via the k-means objective was introduced by Kumar and Kannan [60].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "For further spectral based approaches, see also [13].", "startOffset": 48, "endOffset": 52}, {"referenceID": 25, "context": "[26, 25] formalized this as allowing edge weights in a graph to be modified by a factor of at most \u03b3 without changing the maxcut.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "[26, 25] formalized this as allowing edge weights in a graph to be modified by a factor of at most \u03b3 without changing the maxcut.", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "[6, 8] for work on k-means).", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "[6, 8] for work on k-means).", "startOffset": 0, "endOffset": 6}, {"referenceID": 10, "context": "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].", "startOffset": 47, "endOffset": 71}, {"referenceID": 13, "context": "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].", "startOffset": 47, "endOffset": 71}, {"referenceID": 17, "context": "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].", "startOffset": 47, "endOffset": 71}, {"referenceID": 18, "context": "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].", "startOffset": 47, "endOffset": 71}, {"referenceID": 23, "context": "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].", "startOffset": 47, "endOffset": 71}, {"referenceID": 61, "context": "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].", "startOffset": 47, "endOffset": 71}, {"referenceID": 58, "context": "Local Search There exists a large body of bicriteria approximations for k-median and k-means [59, 30, 33, 21].", "startOffset": 93, "endOffset": 109}, {"referenceID": 29, "context": "Local Search There exists a large body of bicriteria approximations for k-median and k-means [59, 30, 33, 21].", "startOffset": 93, "endOffset": 109}, {"referenceID": 32, "context": "Local Search There exists a large body of bicriteria approximations for k-median and k-means [59, 30, 33, 21].", "startOffset": 93, "endOffset": 109}, {"referenceID": 20, "context": "Local Search There exists a large body of bicriteria approximations for k-median and k-means [59, 30, 33, 21].", "startOffset": 93, "endOffset": 109}, {"referenceID": 8, "context": "[9] (see also [46]) gave the first analysis showing that Local Search with a neighborhood size of 1/\u03b5 gives a 3 + 2\u03b5 approximation to k-median and showed that this bound is tight.", "startOffset": 0, "endOffset": 3}, {"referenceID": 45, "context": "[9] (see also [46]) gave the first analysis showing that Local Search with a neighborhood size of 1/\u03b5 gives a 3 + 2\u03b5 approximation to k-median and showed that this bound is tight.", "startOffset": 14, "endOffset": 18}, {"referenceID": 55, "context": "[56] proved an approximation ratio of 9 + \u03b5 for Euclidean k-means clustering by Local Search, currently the best known algorithm with a polynomial running time in metric and Euclidean spaces.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "3 Recently, Local Search with an appropriate neighborhood was shown to be a PTAS for k-means and k-median in certain restricted metrics including constant dimensional Euclidean space [42, 32].", "startOffset": 183, "endOffset": 191}, {"referenceID": 31, "context": "3 Recently, Local Search with an appropriate neighborhood was shown to be a PTAS for k-means and k-median in certain restricted metrics including constant dimensional Euclidean space [42, 32].", "startOffset": 183, "endOffset": 191}, {"referenceID": 21, "context": "Due to its simplicity, Local Search is also a popular subroutine for clustering tasks in various computational models [22, 27, 45].", "startOffset": 118, "endOffset": 130}, {"referenceID": 26, "context": "Due to its simplicity, Local Search is also a popular subroutine for clustering tasks in various computational models [22, 27, 45].", "startOffset": 118, "endOffset": 130}, {"referenceID": 44, "context": "Due to its simplicity, Local Search is also a popular subroutine for clustering tasks in various computational models [22, 27, 45].", "startOffset": 118, "endOffset": 130}, {"referenceID": 39, "context": "For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74] 2 Approach and Techniques 2.", "startOffset": 59, "endOffset": 79}, {"referenceID": 42, "context": "For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74] 2 Approach and Techniques 2.", "startOffset": 59, "endOffset": 79}, {"referenceID": 47, "context": "For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74] 2 Approach and Techniques 2.", "startOffset": 59, "endOffset": 79}, {"referenceID": 33, "context": "For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74] 2 Approach and Techniques 2.", "startOffset": 59, "endOffset": 79}, {"referenceID": 73, "context": "For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74] 2 Approach and Techniques 2.", "startOffset": 59, "endOffset": 79}, {"referenceID": 9, "context": "Our proof includes a few ingredients from [10] such as the notion of inner-ring (we work with a slightly more general definition) and distinguishing between cheap and expensive clusters.", "startOffset": 42, "endOffset": 46}, {"referenceID": 64, "context": "Euclidean inputs can be straightforwardly \u201cdiscretized\u201d by computing an appropriate candidate set of centers, for instance via Matousek\u2019s approximate centroid set [65] and then applying the Johnson-Lindenstrauss lemma, if the dimension is too large.", "startOffset": 163, "endOffset": 167}, {"referenceID": 64, "context": "They combined Local Search with techniques from Matousek [65] for k-means clustering in Euclidean spaces.", "startOffset": 57, "endOffset": 61}, {"referenceID": 59, "context": "Indeed, this is the first step of the algorithm by Kumar and Kannan [60] (see Algorithm 3).", "startOffset": 68, "endOffset": 72}, {"referenceID": 59, "context": "Algorithm 3 k-means with spectral initialization [60] 1: Project points onto the best rank k subspace 2: Compute a clustering C with constant approximation factor on the projection 3: Initialize centroids of each cluster of C as centers in the original space 4: Run Lloyd\u2019s k-means until convergence In general, projecting onto the best rank k subspace and computing a constant approximation on the projection results in a constant approximation in the original space.", "startOffset": 49, "endOffset": 53}, {"referenceID": 59, "context": "Kumar and Kannan [60] and later Awasthi and Sheffet [13] gave tighter bounds if the spectral separation is large enough.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "Kumar and Kannan [60] and later Awasthi and Sheffet [13] gave tighter bounds if the spectral separation is large enough.", "startOffset": 52, "endOffset": 56}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "The following definition is a generalization of the inner-ring definition of [10].", "startOffset": 77, "endOffset": 81}, {"referenceID": 9, "context": "1 in [10].", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "Now, observe that by [9], the cost of L is at most a 5 approximation to the cost of OPT in the worst case.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "This relies on the example from [9].", "startOffset": 32, "endOffset": 35}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "2 (Theorem 7 of [31]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "This can be proven by using techniques from Awasthi and Sheffet [13] (Theorem 3.", "startOffset": 64, "endOffset": 68}, {"referenceID": 59, "context": "1) and Kumar and Kannan [60] (Theorem 5.", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "Recall that by [9], L is a 5-approximation and so there exist at most 40\u03b5\u22121\u03b2\u22121 such clusters.", "startOffset": 15, "endOffset": 18}, {"referenceID": 64, "context": "For k-means itself, we could alternatively combine Matousek\u2019s approximate centroid set [65] with the Johnson Lindenstrauss lemma and avoid the following construction; however this would only work for optimal distribution stable clusterings and the proof Theorem 1.", "startOffset": 87, "endOffset": 91}, {"referenceID": 69, "context": "It is well known that for unit Euclidean ball of dimension d, there exists an \u03b5-net of cardinality (1 + 2/\u03b5)d, see for instance Pisier [70].", "startOffset": 135, "endOffset": 139}, {"referenceID": 53, "context": "To reduce the dependency on the dimension, we combine this statement with the seminal theorem originally due to Johnson and Lindenstrauss [54].", "startOffset": 138, "endOffset": 142}, {"referenceID": 56, "context": "The Johnson-Lindenstrauss lemma can also be applied in these settings, at a slightly worse target dimension of O((p + 1)2 log((p + 1)/\u03b5)\u03b5\u22123 log n), see Kerber and Raghvendra [57].", "startOffset": 174, "endOffset": 178}, {"referenceID": 8, "context": "14 We first introduce some definitions, following the terminology of [9, 46].", "startOffset": 69, "endOffset": 76}, {"referenceID": 45, "context": "14 We first introduce some definitions, following the terminology of [9, 46].", "startOffset": 69, "endOffset": 76}], "year": 2017, "abstractText": "In this paper, we analyze the performance of a simple and standard Local Search algorithm for clustering on well behaved data. Since the seminal paper by Ostrovsky, Rabani, Schulman and Swamy [FOCS 2006], much progress has been made to characterize real-world instances. We distinguish the three main definitions \u2022 Distribution Stability (Awasthi, Blum, Sheffet, FOCS 2010) \u2022 Spectral Separability (Kumar, Kannan, FOCS 2010) \u2022 Perturbation Resilience (Bilu, Linial, ICS 2010) and show that Local Search performs well on the instances with the aforementioned stability properties. Specifically, for the k-means and k-median objective, we show that Local Search exactly recovers the optimal clustering if the dataset is 3+ \u03b5-perturbation resilient, and is a PTAS for distribution stability and spectral separability. This implies the first PTAS for instances satisfying the spectral separability condition. For the distribution stability condition we also go beyond previous work by showing that the clustering output by the algorithm and the optimal clustering are very similar. This is a significant step toward understanding the success of Local Search heuristics in clustering applications and supports the legitimacy of the stability conditions: They characterize some of the structure of real-world instances that make Local Search a popular heuristic. Supported by Deutsche Forschungsgemeinschaft within the Collaborative Research Center SFB 876, project A2", "creator": "LaTeX with hyperref package"}}}