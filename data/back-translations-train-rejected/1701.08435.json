{"id": "1701.08435", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2017", "title": "Transformation-Based Models of Video Sequences", "abstract": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model.", "histories": [["v1", "Sun, 29 Jan 2017 21:39:05 GMT  (1304kb,D)", "https://arxiv.org/abs/1701.08435v1", null], ["v2", "Mon, 24 Apr 2017 20:20:40 GMT  (1304kb,D)", "http://arxiv.org/abs/1701.08435v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["joost van amersfoort", "anitha kannan", "marc'aurelio ranzato", "arthur szlam", "du tran", "soumith chintala"], "accepted": false, "id": "1701.08435"}, "pdf": {"name": "1701.08435.pdf", "metadata": {"source": "CRF", "title": "TRANSFORMATION-BASED MODELS OF VIDEO SEQUENCES", "authors": ["Joost van Amersfoort", "Anitha Kannan", "Marc\u2019Aurelio Ranzato", "Arthur Szlam", "Du Tran"], "emails": ["joost@joo.st,", "soumith}@fb.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "There is an increased interest in unmonitored learning processes of video sequences (Mathieu et al., 2016; Srivastava et al., 2015; Vondrick et al., 2016). A popular formulation of the task is to predict a small number of future frames; the motivation that requires future frames needs to be understood how objects interact and what plausible sequences of motion are. These methods aim directly at predicting pixel values that predict either MSE loss or adequate losses. In this paper, we take a different approach to the problem of the next frame. In particular, our model operates in the space of transformations between frames, directly modelling the source of variability."}, {"heading": "1.1 RELATED WORK", "text": "In fact, most of them are able to follow the rules they have imposed on themselves. (...) Most of them are able to follow the rules. (...) Most of them are able to follow the rules. (...) Most of them are able to follow the rules. (...) Most of them are able to follow the rules. (...) Most of them are not able to follow the rules. (...) Most of them are able to follow the rules. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...)"}, {"heading": "2 MODEL", "text": "The model we propose is based on three main assumptions: 1) the mere estimation of object motion yields sequences that are plausible and relatively sharp; 2) global movements can be estimated by tiling high-resolution video images into fields and estimating movements \"revolutionary\" at the field level; and 3) subjecting fields in the same spatial location to a deformation over two successive time steps that can be well described by affine transformations; the first assumption lies at the heart of the proposed method: by taking into account the uncertainty in the space of transformations, we create sequences that may still seem plausible; and the other two assumptions state that a video sequence can be composed by fields undergoing affine transformations. We agree that these are simplistic assumptions that ignore how objective identity affects motion and do not take into account the transformations resulting from plane rotations and more general forms of deformation."}, {"heading": "2.1 AFFINE TRANSFORM EXTRACTOR", "text": "The fact is that we are able to hide, and that we are able, we will be able, we will be able to put ourselves in a position, we will put ourselves in a position, we will put ourselves in a position, we will put ourselves in a position, we will put ourselves in a position, we will put ourselves in a position, we will put ourselves in a position, we will put ourselves in a position, we will put ourselves in a position, we will put ourselves in a position, we will put ourselves in a position, we will put ourselves in a position."}, {"heading": "2.2 AFFINE TRANSFORM PREDICTOR", "text": "The affinity transformation predictor is used to predict the affinity transformations between the last frame and the next frame in the sequence. A schematic representation of the system is shown in Fig. 2. Input is the affinity transformations between pairs of adjacent frames as described in the previous section. Each transformation is arranged in a grid of size 6 \u00d7 n \u00b7 n, n being the number of patches in a row / column and 6 being the number of parameters of each affinity transformation. Thus, if four frames are used to initialize the model, the actual input consists of 18 cards of size n \u00b7 n, which are the concatenation of At \u2212 2, At \u2212 1, At, where At is the collection of patchwork transformations between the frame at the time t \u2212 1 and t. The model consists of a multi-layered conventional network without any prediction."}, {"heading": "2.3 MULTI-STEP PREDICTION", "text": "In practice, we would like to predict several time steps in the future. A greedy approach would: a) train as described above to minimize the predictive error for affine transformations at the next time step, and b) predict one step ahead at the time of testing, and then re-circulate the model prediction to predict affine transformation two steps ahead, and so on. Unfortunately, errors can accumulate during this process, because the model has never been exposed to its own predictions at the time of training, and the approach we propose replicates the model over time, including during training, as shown in Fig. 3. If we want to predict M steps in the future, we replicate the CNN M times and pass the output of CNN at a time step t as input to the same CNN at a time step t + 1 as we do at the test time. Since predictions live in a continuous space, the entire system is differentiable and equivalent to generate a default error across the entire network, since CNN plays a more accurate role in the propagation method of 3."}, {"heading": "2.4 TESTING", "text": "After extracting the N \u2212 1 affinity transformations from the images to which we have conditioned, we replicate the model M times and feed its own prediction back into the input, as explained in the previous section. Once the affinity transformations are predicted, we can reconstruct the actual pixel values. We use the last frame of the sequence and apply the first set of affinity transformations to each patch in that frame. Each pixel in the output frame is predicted several times depending on the step used. We calculate these predictions on average and reconstruct the entire frame. As needed, we can repeat this process for as many frames as necessary, using the last reconstructed frame and the next affinity transformation process. To evaluate the generation, we suggest passing the generated frames to a trained classifier for an interesting task. For example, we can condition the generation from frames taken from video clips labeled with the corresponding action."}, {"heading": "3 EXPERIMENTS", "text": "In this section, we confirm the main assumptions of our model and compare them with modern generative models based on two sets of data. We strongly encourage the reader to watch the short video clips in the supplementary material to better understand the quality of our generations.In Section 2, we discuss the three main assumptions underlying our model: 1) errors in the transformation space still look plausible, 2) a frame can be broken down into patches, and 3) each patch movement is well modelled by affine transformation.The results in Supplementary Material 3 confirm assumptions 2 and 3. Each line shows a sequence from the UCF101 dataset (Soomro et al., 2012).The left column shows the original video frames and the right the reconstructions from the estimated affine transformations as described in Section 2.1. As you can see, there is little discernible difference between these video sequences, suggesting that video sequences can be very well represented for comparison of the first generations."}, {"heading": "3.1 MOVING MNIST", "text": "For our first experiment, we used the data set of movable MNIST digits (Srivastava et al., 2015) and performed qualitative analyses 4. It consists of one or two MNIST digits that are placed in random places and move at constant speed within a 64 \u00d7 64 frame. If a digit reaches a limit, it jumps, which means that the speed is reversed in this direction. Numbers can mutually obscure each other and bounce off walls, causing the data sets to be challenged. Using scripts created by Srivastava et al. (2015), we generated a fixed data set of 128,000 sequences and used 80% for training, 10% for validation and 10% for testing. Next, we estimated the affine transformations between each pair of adjacent frames x x x x x 4 frames in total, and traced a small CNN frame to the affiner space 4 x x 4, and used 80% for training, 10% for validation and 10% for testing."}, {"heading": "3.2 UCF 101 DATA SET", "text": "It is a matter of time before such a constellation will occur. It is a matter of time before such a constellation will occur. It is a matter of time before such a constellation will occur. It is a matter of time before such a constellation will occur. It is a matter of time before such a constellation will occur. It is a matter of time before such a constellation will occur. It is a matter of time before such a constellation will occur. It is a matter of time before such a constellation will occur. It is a matter of time before such a constellation will occur. It is a matter of time before such a constellation will occur."}, {"heading": "4 CONCLUSIONS", "text": "In this case, we are in a situation in which we are in a position to put ourselves in a situation in which we are in a position to entangle ourselves in a situation in which we are in, in which we are in a situation in which we are in."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Camille Couprie and Michael Mathieu for talks and help in evaluating their models."}], "references": [{"title": "High accuracy optical flow estimation based on a theory for warping", "author": ["REFERENCES Thomas Brox", "Andr\u00e9s Bruhn", "Nils Papenberg", "Joachim Weickert"], "venue": "Vision-ECCV", "citeRegEx": "Brox et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Brox et al\\.", "year": 2004}, {"title": "Se3-nets: Learning rigid body motion using deep neural networks", "author": ["Arunkumar Byravan", "Dieter Fox"], "venue": "arXiv preprint arXiv:1606.02378,", "citeRegEx": "Byravan and Fox.,? \\Q2016\\E", "shortCiteRegEx": "Byravan and Fox.", "year": 2016}, {"title": "Unsupervised learning for physical interaction through video prediction", "author": ["Chelsea Finn", "Ian Goodfellow", "Sergey Levine"], "venue": "arXiv preprint arXiv:1605.07157,", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Dynamic filter networks", "author": ["Xu Jia", "Bert De Brabandere", "Tinne Tuytelaars", "Luc V Gool"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2016}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["Andrej Karpathy", "George Toderici", "Sachin Shetty", "Tommy Leung", "Rahul Sukthankar", "Li FeiFei"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Michael Mathieu", "Camille Couprie", "Yann LeCun"], "venue": "In ICLR,", "citeRegEx": "Mathieu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2016}, {"title": "Modeling deep temporal dependencies with recurrent grammar cells", "author": ["Vincent Michalski", "Roland Memisevic", "Kishore Konda"], "venue": "In NIPS,", "citeRegEx": "Michalski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Michalski et al\\.", "year": 2014}, {"title": "Action-conditional video prediction using deep networks in atari", "author": ["Junhyuk Oh", "Xiaoxiao Guo", "Honglak Lee", "Richard Lewis", "Satinder Singh"], "venue": null, "citeRegEx": "Oh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2015}, {"title": "Spatio-temporal video autoencoder with differentiable memory", "author": ["Viorica Patraucean", "Ankur Handa", "Roberto Cipolla"], "venue": "arXiv preprint arXiv:1511.06309,", "citeRegEx": "Patraucean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Patraucean et al\\.", "year": 2015}, {"title": "Video (language) modeling: a baseline for generative models of natural videos", "author": ["MarcAurelio Ranzato", "Arthur Szlam", "Joan Bruna", "Michael Mathieu", "Ronan Collobert", "Sumit Chopra"], "venue": "arXiv preprint arXiv:1412.6604,", "citeRegEx": "Ranzato et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2014}, {"title": "Ucf101: A dataset of 101 human actions classes from videos in the wild", "author": ["Khurram Soomro", "Amir Roshan Zamir", "Mubarak Shah"], "venue": null, "citeRegEx": "Soomro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Soomro et al\\.", "year": 2012}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1502.04681,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["Du Tran", "Lubomir Bourdev", "Rob Fergus", "Lorenzo Torresani", "Manohar Paluri"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Tran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2015}, {"title": "Generating videos with scene dynamics", "author": ["Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "venue": "arXiv preprint arXiv:1609.02612,", "citeRegEx": "Vondrick et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vondrick et al\\.", "year": 2016}, {"title": "An uncertain future: Forecasting from static images using variational autoencoders", "author": ["Jacob Walker", "Carl Doersch", "Abhinav Gupta", "Martial Hebert"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Walker et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2016}, {"title": "Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks", "author": ["Tianfan Xue", "Jiajun Wu", "Katherine Bouman", "Bill Freeman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Xue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2016}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee"], "venue": "arXiv preprint arXiv:1512.00570,", "citeRegEx": "Yan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "There has been an increased interest in unsupervised learning of representations from video sequences (Mathieu et al., 2016; Srivastava et al., 2015; Vondrick et al., 2016).", "startOffset": 102, "endOffset": 172}, {"referenceID": 12, "context": "There has been an increased interest in unsupervised learning of representations from video sequences (Mathieu et al., 2016; Srivastava et al., 2015; Vondrick et al., 2016).", "startOffset": 102, "endOffset": 172}, {"referenceID": 14, "context": "There has been an increased interest in unsupervised learning of representations from video sequences (Mathieu et al., 2016; Srivastava et al., 2015; Vondrick et al., 2016).", "startOffset": 102, "endOffset": 172}, {"referenceID": 12, "context": "Typically, generative models of video sequences are evaluated in terms of MSE in pixel space (Srivastava et al., 2015), which is not a good choice since this metric favors blurry predictions over other more realistic looking options that just happen to differ from the ground truth.", "startOffset": 93, "endOffset": 118}, {"referenceID": 11, "context": "Our experiments show that our simple and efficient model outperforms other baselines, including much more sophisticated models, on benchmarks on the UCF-101 data set (Soomro et al., 2012).", "startOffset": 166, "endOffset": 187}, {"referenceID": 12, "context": "We also provide qualitative comparisons to the moving MNIST digit data set (Srivastava et al., 2015).", "startOffset": 75, "endOffset": 100}, {"referenceID": 7, "context": "Early work on video modeling focused on predicting small patches (Michalski et al., 2014; Srivastava et al., 2015); unfortunately, these models have not shown to scale to the complexity of highresolution videos.", "startOffset": 65, "endOffset": 114}, {"referenceID": 12, "context": "Early work on video modeling focused on predicting small patches (Michalski et al., 2014; Srivastava et al., 2015); unfortunately, these models have not shown to scale to the complexity of highresolution videos.", "startOffset": 65, "endOffset": 114}, {"referenceID": 7, "context": "Early work on video modeling focused on predicting small patches (Michalski et al., 2014; Srivastava et al., 2015); unfortunately, these models have not shown to scale to the complexity of highresolution videos. Also these models require a significant amount of parameters and computational power for even relatively simple data. In Ranzato et al. (2014), the authors circumvented this problem by quantizing the space of image patches.", "startOffset": 66, "endOffset": 355}, {"referenceID": 3, "context": "(2016) recently proposed to replace MSE in pixel space with a MSE on image gradients, leveraging prior domain knowledge, and further improved using a multi-scale architecture with adversarial training (Goodfellow et al., 2014).", "startOffset": 201, "endOffset": 226}, {"referenceID": 16, "context": "A recent strong result is provided in Xue et al. (2016). This paper describes a model that generates videos which exhibit substantial motion using a motion encoder, an image encoder and a cross convolution part with a decoder.", "startOffset": 38, "endOffset": 56}, {"referenceID": 7, "context": "In Oh et al. (2015), and similarly Finn et al.", "startOffset": 3, "endOffset": 20}, {"referenceID": 2, "context": "(2015), and similarly Finn et al. (2016) for Robot tasks and Byravan & Fox (2016) for 3D objects, frames of a video game are predicted given an action (transformation) taken by an agent.", "startOffset": 22, "endOffset": 41}, {"referenceID": 2, "context": "(2015), and similarly Finn et al. (2016) for Robot tasks and Byravan & Fox (2016) for 3D objects, frames of a video game are predicted given an action (transformation) taken by an agent.", "startOffset": 22, "endOffset": 82}, {"referenceID": 9, "context": "Perhaps most similar to our approach, Patraucean et al. (2015) also separate out motion/content and directly model motion and employs the Spatial Transformer network (Jaderberg et al.", "startOffset": 38, "endOffset": 63}, {"referenceID": 9, "context": "Perhaps most similar to our approach, Patraucean et al. (2015) also separate out motion/content and directly model motion and employs the Spatial Transformer network (Jaderberg et al., 2015). The biggest difference is that our approach is solely convolutional, which makes training fast and the optimization problem simpler. This also allows the model to scale to larger datasets and images, with only modest memory and computational resources. The model directly outputs full affine transforms instead of pixels (rather than only translations as in equation 3 in Patraucean et al. (2015)).", "startOffset": 38, "endOffset": 589}, {"referenceID": 17, "context": "Prior work relating to the evaluation protocol can be found in Yan et al. (2015). The authors generate images using a set of predefined attributes and later show that they can recover these using a pretrained neural network.", "startOffset": 63, "endOffset": 81}, {"referenceID": 11, "context": "Every row shows a sequence from the UCF101 dataset (Soomro et al., 2012).", "startOffset": 51, "endOffset": 72}, {"referenceID": 12, "context": "In the next section, we will first report some results using the toy data set of \u201cmoving MNIST digits\u201d (Srivastava et al., 2015).", "startOffset": 103, "endOffset": 128}, {"referenceID": 12, "context": "For our first experiment, we used the dataset of moving MNIST digits (Srivastava et al., 2015) and perform qualitative analysis4.", "startOffset": 69, "endOffset": 94}, {"referenceID": 12, "context": "Using scripts provided by Srivastava et al. (2015), we generated a fixed dataset of 128,000 sequences and used 80% for training, 10% for validation and 10% for testing.", "startOffset": 26, "endOffset": 51}, {"referenceID": 12, "context": "5 in Srivastava et al. (2015). The generations in fig.", "startOffset": 5, "endOffset": 30}, {"referenceID": 12, "context": "st/ICLR/ReconstructionsFromGroundTruth A quantitative analysis would be difficult for this data set because metrics reported in the literature like MSE (Srivastava et al., 2015) are not appropriate for measuring generation quality, and it would be difficult to use the metric we propose because we do not have labels at the sequence level and the design of a classifier is not trivial.", "startOffset": 152, "endOffset": 177}, {"referenceID": 6, "context": "Next, we show the prediction produced by the adversarially trained CNN proposed by Mathieu et al. (2016). The last two column show the prediction produced by our affine-transformation based approach.", "startOffset": 83, "endOffset": 105}, {"referenceID": 6, "context": "37 Mathieu et al. (2016) 57.", "startOffset": 3, "endOffset": 25}, {"referenceID": 11, "context": "The UCF-101 dataset (Soomro et al., 2012) is a collection of 13320 videos of 101 action categories.", "startOffset": 20, "endOffset": 41}, {"referenceID": 0, "context": "We evaluate several models5: a) a baseline which merely copies the last frame used for conditioning, b) a baseline method which estimates optical flow (Brox et al., 2004) from two consecutive frames and extrapolates flow in subsequent frames under the assumption of constant flow speed, c) an adversarially trained multi-scale CNN (Mathieu et al.", "startOffset": 151, "endOffset": 170}, {"referenceID": 6, "context": ", 2004) from two consecutive frames and extrapolates flow in subsequent frames under the assumption of constant flow speed, c) an adversarially trained multi-scale CNN (Mathieu et al., 2016) and several variants of our proposed approach.", "startOffset": 168, "endOffset": 190}, {"referenceID": 0, "context": "(2016) and optical flow (Brox et al., 2004).", "startOffset": 24, "endOffset": 43}, {"referenceID": 13, "context": "We use C3D network (Tran et al., 2015) as the video action classifier: C3D uses both appearance and temporal information jointly, and is pre-trained with Sports1M (Karpathy et al.", "startOffset": 19, "endOffset": 38}, {"referenceID": 5, "context": ", 2015) as the video action classifier: C3D uses both appearance and temporal information jointly, and is pre-trained with Sports1M (Karpathy et al., 2014) and fine tuned on UCF 101.", "startOffset": 132, "endOffset": 155}, {"referenceID": 5, "context": "The best performance is achieved by using ground truth frames, a result comparable to methods recently appeared in the literature (Karpathy et al., 2014; Tran et al., 2015).", "startOffset": 130, "endOffset": 172}, {"referenceID": 13, "context": "The best performance is achieved by using ground truth frames, a result comparable to methods recently appeared in the literature (Karpathy et al., 2014; Tran et al., 2015).", "startOffset": 130, "endOffset": 172}, {"referenceID": 0, "context": "We evaluate several models5: a) a baseline which merely copies the last frame used for conditioning, b) a baseline method which estimates optical flow (Brox et al., 2004) from two consecutive frames and extrapolates flow in subsequent frames under the assumption of constant flow speed, c) an adversarially trained multi-scale CNN (Mathieu et al., 2016) and several variants of our proposed approach. Qualitative comparisons can be seen in the fig. 5 and in the supplementary material6. The first column on the page shows the input, the second the ground truth, followed by results from our model, Mathieu et al. (2016) and optical flow (Brox et al.", "startOffset": 152, "endOffset": 620}, {"referenceID": 6, "context": "Compared to other methods, our approach performs better than optical flow and even the more sophisticated multi-scale CNN proposed in Mathieu et al. (2016) while being computationally cheaper.", "startOffset": 134, "endOffset": 156}, {"referenceID": 6, "context": "Compared to other methods, our approach performs better than optical flow and even the more sophisticated multi-scale CNN proposed in Mathieu et al. (2016) while being computationally cheaper. For instance, our method has less than half a million parameters and requires about 2G floating point operations to generate a frame at test time, while the multi-scale CNN of Mathieu et al. (2016) has 25 times more parameters (not counting the discriminator used at training time) and it requires more than 100 times more floating point operations to generate a single frame.", "startOffset": 134, "endOffset": 391}, {"referenceID": 12, "context": "Unfortunately, we could not compare against the LSTM-based method in Srivastava et al. (2015) because it does not scale to high-resolution videos, but only to small patches.", "startOffset": 69, "endOffset": 94}], "year": 2017, "abstractText": "In this work we propose a simple unsupervised approach for next frame prediction in video. Instead of directly predicting the pixels in a frame given past frames, we predict the transformations needed for generating the next frame in a sequence, given the transformations of the past frames. This leads to sharper results, while using a smaller prediction model. In order to enable a fair comparison between different video frame prediction models, we also propose a new evaluation protocol. We use generated frames as input to a classifier trained with ground truth sequences. This criterion guarantees that models scoring high are those producing sequences which preserve discriminative features, as opposed to merely penalizing any deviation, plausible or not, from the ground truth. Our proposed approach compares favourably against more sophisticated ones on the UCF-101 data set, while also being more efficient in terms of the number of parameters and computational cost.", "creator": "LaTeX with hyperref package"}}}