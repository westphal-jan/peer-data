{"id": "1701.08511", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2017", "title": "Binary adaptive embeddings from order statistics of random projections", "abstract": "We use some of the largest order statistics of the random projections of a reference signal to construct a binary embedding that is adapted to signals correlated with such signal. The embedding is characterized from the analytical standpoint and shown to provide improved performance on tasks such as classification in a reduced-dimensionality space.", "histories": [["v1", "Mon, 30 Jan 2017 08:37:25 GMT  (358kb,D)", "http://arxiv.org/abs/1701.08511v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["diego valsesia", "enrico magli"], "accepted": false, "id": "1701.08511"}, "pdf": {"name": "1701.08511.pdf", "metadata": {"source": "CRF", "title": "Binary adaptive embeddings from order statistics of random projections", "authors": ["Diego Valsesia", "Enrico Magli"], "emails": [], "sections": [{"heading": null, "text": "In fact, most people who are in a position to do so feel able to move to another world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "II. PROPOSED METHOD", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Preliminaries", "text": "Definition 1. An assignment of the metric spaces with the distances dX and dY is called embedding with distortion C > 0 if LdX (u, v) \u2264 dY (\u03c6 (u), \u03c6 (v)) \u2264 CLdX (u, v) for any constant L > 0 and for all u, v, X etc. A well-known binary embedding is the sign of random projections [3] in which a random matrix of independent and identically distributed (i.d.) Gaussian entries is used to calculate some random projections, which are then quantified by retaining their character to form a binary representation."}, {"heading": "B. Proposed adaptive embedding", "text": "A reference signal is used to generate the adaptive embedding, which is achieved in the following way. (A number mpool of random projections is calculated using an i.i.d. Gaussian system. (1) The m entries of the largest magnitude are identified and their positions are stored in vector l. (1) The resulting m-bit binary parameter is: p = character (1), where the matrix is limited to the rows that are stored as page information of the embedding and used as in (1) whenever a new signal is to be embedded, i.e., q = character (2), where v is a generic signal and its embedding. The first theorem we confirm is that the hamming distance dH (p) = 1m pi, i.e."}, {"heading": "III. APPLICATIONS AND EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Low-contrast approximate nearest neighbors", "text": "One measure of the difficulty [15], [16] of a closest search problem is the contrast r, which is defined as the relationship between the distance of the closest false neighbor and that of the most distant true neighbor. [17] It is therefore a realistic method of embedding a function of the inner product between reference and test signals. [17] It is not the way in which a comparison is made, but the way in which there is a function of the inner product between reference and test signals. [17] It is the way in which it comes to a comparison of the two test signals. [17] It is the way in which it is led to a comparison. [17] It is the way in which there is a function of the inner product between reference and test signals. \"[17] It is the way in which it adjusts.\""}, {"heading": "B. Multiclass linear classifiers", "text": "In this section, we apply adaptive embedding to a linear multi-class classifier to improve its storage and computing power. Linear classifiers are often used in the context of deep neural networks, where the layers of the network are trained to untangle the properties of each class, and a simple linear classifier provides the class names. A k-class linear classifier can be used as l = arg maxi = {1,..., k} wix, with l providing the class label, wi providing the weight vectors, and x providing a feature vector. Weights will be learned during the training phase with a suitable loss function such as the hinge loss [20] for support vector machines or the softmax cross-entropy [21] for multinomic logistic regression, which is more popular in deep neural networks. As the feature vectors are highly dimensional and the number of class levels may be large, we may require a memory space as well as a weight ression."}, {"heading": "IV. CONCLUSIONS", "text": "In this paper, a technique for generating compact binary codes from high-dimensional signals that adapt them to a reference signal was presented, and the resulting embedding shows interesting properties that can improve performance in classification tasks when performed in the reduced dimensionality range. Future work will focus on generalizing the approach for sub-Gaussian and structured sensor matrices."}], "references": [{"title": "Database-friendly random projections: Johnson- Lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of computer and System Sciences, vol. 66, no. 4, pp. 671\u2013687, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "Contemporary Mathematics, vol. 26, 1984.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1984}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "Proceedings of the Thiry-fourth Annual ACM Symposium on Theory of Computing, ser. STOC \u201902. New York, NY, USA: ACM, 2002, pp. 380\u2013388. [Online]. Available: http://doi.acm.org/10.1145/509907.509965", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Robust 1bit compressive sensing via binary stable embeddings of sparse vectors", "author": ["L. Jacques", "J.N. Laska", "P.T. Boufounos", "R.G. Baraniuk"], "venue": "IEEE Trans. Inf. Theory, vol. 59, no. 4, pp. 2082\u20132102, April 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient coding of signal distances using universal quantized embeddings", "author": ["P.T. Boufounos", "S. Rane"], "venue": "Data Compression Conference (DCC), 2013, March 2013, pp. 251\u2013260.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "On the resemblance and containment of documents", "author": ["A. Broder"], "venue": "Proceedings of the Compression and Complexity of Sequences 1997. Washington, DC, USA: IEEE Computer Society, 1997, pp. 21\u201329. [Online]. Available: http://dl.acm.org/citation.cfm?id=829502.830043", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Sparse- Hash: Embedding Jaccard Coefficient between Supports of Signals", "author": ["D. Valsesia", "S. Fosson", "C. Ravazzi", "T. Bianchi", "E. Magli"], "venue": "Proc. of MM-SPARSE Workshop at ICME 2016, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Hamming distance metric learning", "author": ["M. Norouzi", "D.J. Fleet", "R.R. Salakhutdinov"], "venue": "Advances in neural information processing systems, 2012, pp. 1061\u20131069.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems 21, D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, Eds. Curran Associates, Inc., 2009, pp. 1753\u20131760. [Online]. Available: http://papers.nips.cc/ paper/3383-spectral-hashing.pdf", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "K-means hashing: An affinity-preserving quantization method for learning binary compact codes", "author": ["K. He", "F. Wen", "J. Sun"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Query adaptative locality sensitive hashing", "author": ["H. Jegou", "L. Amsaleg", "C. Schmid", "P. Gros"], "venue": "2008 IEEE International Conference on Acoustics, Speech and Signal Processing, March 2008, pp. 825\u2013828.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Probability and computing: Randomized algorithms and probabilistic analysis", "author": ["M. Mitzenmacher", "E. Upfal"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Order statistics & inference: estimation methods", "author": ["N. Balakrishnan", "A.C. Cohen"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Expected values of normal order statistics", "author": ["H.L. Harter"], "venue": "Biometrika, vol. 48, no. 1/2, pp. 151\u2013165, 1961. [Online]. Available: http: //www.jstor.org/stable/2333139", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1961}, {"title": "On the difficulty of nearest neighbor search", "author": ["J. He", "S. Kumar", "S.-F. Chang"], "venue": "ICML 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "When Is \u201cNearest Neighbor", "author": ["K. Beyer", "J. Goldstein", "R. Ramakrishnan", "U. Shaft"], "venue": "Meaningful? Berlin, Heidelberg: Springer Berlin Heidelberg,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Commun. ACM, vol. 51, no. 1, pp. 117\u2013122, Jan. 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Compressed fingerprint matching and camera identification via random projections", "author": ["D. Valsesia", "G. Coluccia", "T. Bianchi", "E. Magli"], "venue": "IEEE Transactions on Information Forensics and Security, vol. 10, no. 7, pp. 1472\u20131485, July 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale image retrieval based on compressed camera identification", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 9, pp. 1439\u2013 1449, Sept 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, vol. 20, no. 3, pp. 273\u2013297, 1995.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1995}, {"title": "Pattern recognition", "author": ["C.M. Bishop"], "venue": "Machine Learning, vol. 128, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The concept of embedding has been successfully used in the context of information retrieval [1], where it is usually called \u201chashing\u201d.", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": "Johnson and Lindenstrauss [2] famously stated that an embedding can be realized with a Lipschitz mapping to approximately preserve Euclidean distances with a dimension of the embedding space that only depends on the desired distortion and logarithmically in the number of signals to be embedded.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "Several extensions have later been proposed, allowing one to approximately preserve the angle between signals [3], [4], control the maximum distance that is embedded [5], or preserve the Jaccard distance [6], [7].", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "Several extensions have later been proposed, allowing one to approximately preserve the angle between signals [3], [4], control the maximum distance that is embedded [5], or preserve the Jaccard distance [6], [7].", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "Several extensions have later been proposed, allowing one to approximately preserve the angle between signals [3], [4], control the maximum distance that is embedded [5], or preserve the Jaccard distance [6], [7].", "startOffset": 166, "endOffset": 169}, {"referenceID": 5, "context": "Several extensions have later been proposed, allowing one to approximately preserve the angle between signals [3], [4], control the maximum distance that is embedded [5], or preserve the Jaccard distance [6], [7].", "startOffset": 204, "endOffset": 207}, {"referenceID": 6, "context": "Several extensions have later been proposed, allowing one to approximately preserve the angle between signals [3], [4], control the maximum distance that is embedded [5], or preserve the Jaccard distance [6], [7].", "startOffset": 209, "endOffset": 212}, {"referenceID": 7, "context": "Recently, some works have studied learning embeddings [8]\u2013[10] from training data to derive compact codes by exploiting the particular geometry of the dataset (e.", "startOffset": 54, "endOffset": 57}, {"referenceID": 9, "context": "Recently, some works have studied learning embeddings [8]\u2013[10] from training data to derive compact codes by exploiting the particular geometry of the dataset (e.", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "[11] empirically explored", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "A well-known binary embedding is the sign random projections [3] where a random matrix \u03a6 made of independent and identically distributed (i.", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "The Hamming distance between the binary vectors approximately preserves the angle between the signals in the original space [3], i.", "startOffset": 124, "endOffset": 127}, {"referenceID": 11, "context": "(2) can be readily obtained using Chernoff bounds [12] for the tails of DH .", "startOffset": 50, "endOffset": 54}, {"referenceID": 1, "context": "(2) yields m = O(\u03b4\u22122 logN), which is exactly the same as classic results on non-adaptive random projections [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 12, "context": "We then notice that that is equivalent [13] to e2(mpool\u2212m+1);2mpool , i.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "As a further remark, according to [14] an approximation of the expected value of the desired order statistic is:", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "(4) can be readily obtained using Chernoff bounds [12] for the tails of DH .", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "The key distinction between sign random projections [3] and the method presented in this paper is that the former provides a linear relationship between the Hamming distance in the embedded space and the angle in the original space.", "startOffset": 52, "endOffset": 55}, {"referenceID": 14, "context": "A measure of difficulty [15], [16] of a nearest neighbor search problem is the contrast r, which is defined as the ratio between the distance of the closest false neighbor and that of the farthest true neighbor.", "startOffset": 24, "endOffset": 28}, {"referenceID": 15, "context": "A measure of difficulty [15], [16] of a nearest neighbor search problem is the contrast r, which is defined as the ratio between the distance of the closest false neighbor and that of the farthest true neighbor.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "Locality sensitive hashing [17] is Fig.", "startOffset": 27, "endOffset": 31}, {"referenceID": 2, "context": "Non-adaptive curve uses sign random projections [3].", "startOffset": 48, "endOffset": 51}, {"referenceID": 17, "context": "This problem is not unrealistic and, in fact, as an example, it occurs in the detection of photoresponse non-uniformity artifacts from camera sensors [18] [19], used to attribute a given picture to a given camera sensor.", "startOffset": 150, "endOffset": 154}, {"referenceID": 18, "context": "This problem is not unrealistic and, in fact, as an example, it occurs in the detection of photoresponse non-uniformity artifacts from camera sensors [18] [19], used to attribute a given picture to a given camera sensor.", "startOffset": 155, "endOffset": 159}, {"referenceID": 4, "context": "[5] which is a kind of adaptive embedding where the quantizer can be parametrized in order to distort the expected value of signal distances, similarly to the embedding proposed in this paper.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "The weights are learned during the training phase using a suitable loss function such as the hinge loss [20] for support vector machines or the softmax cross-entropy [21] for multinomial logistic regression which is more popular in deep neural networks.", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "The weights are learned during the training phase using a suitable loss function such as the hinge loss [20] for support vector machines or the softmax cross-entropy [21] for multinomial logistic regression which is more popular in deep neural networks.", "startOffset": 166, "endOffset": 170}, {"referenceID": 21, "context": "The following experiment is a classification problem on the CIFAR-10 dataset [22] comprising 10 classes.", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "We implemented the same convolutional neural network architecture presented in [23].", "startOffset": 79, "endOffset": 83}, {"referenceID": 23, "context": "This network is composed of 8 convolutional layers followed by 2 fully connected layers all with ReLU activation units [24] and a final linear layer.", "startOffset": 119, "endOffset": 123}, {"referenceID": 2, "context": "It can be noticed that the adaptive embedding allows to achieve a significant dimensionality reduction at a negligible loss in terms of classification accuracy, with respect to both sign random projections [3] and the universal embedding [5].", "startOffset": 206, "endOffset": 209}, {"referenceID": 4, "context": "It can be noticed that the adaptive embedding allows to achieve a significant dimensionality reduction at a negligible loss in terms of classification accuracy, with respect to both sign random projections [3] and the universal embedding [5].", "startOffset": 238, "endOffset": 241}], "year": 2017, "abstractText": "We use some of the largest order statistics of the random projections of a reference signal to construct a binary embedding that is adapted to signals correlated with such signal. The embedding is characterized from the analytical standpoint and shown to provide improved performance on tasks such as classification in a reduced-dimensionality space. Keywords\u2014Binary Embeddings, Random projections", "creator": "LaTeX with hyperref package"}}}