{"id": "1701.08694", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "A Comparative Study on Different Types of Approaches to Bengali document Categorization", "abstract": "Document categorization is a technique where the category of a document is determined. In this paper three well-known supervised learning techniques which are Support Vector Machine(SVM), Na\\\"ive Bayes(NB) and Stochastic Gradient Descent(SGD) compared for Bengali document categorization. Besides classifier, classification also depends on how feature is selected from dataset. For analyzing those classifier performances on predicting a document against twelve categories several feature selection techniques are also applied in this article namely Chi square distribution, normalized TFIDF (term frequency-inverse document frequency) with word analyzer. So, we attempt to explore the efficiency of those three-classification algorithms by using two different feature selection techniques in this article.", "histories": [["v1", "Fri, 27 Jan 2017 13:08:08 GMT  (528kb)", "http://arxiv.org/abs/1701.08694v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["md saiful islam", "fazla elahi md jubayer", "syed ikhtiar ahmed"], "accepted": false, "id": "1701.08694"}, "pdf": {"name": "1701.08694.pdf", "metadata": {"source": "CRF", "title": "A Comparative Study on Different Types of Approaches to Bengali document Categorization", "authors": ["Md. Saiful Islam", "Fazla Elahi", "Md Jubayer", "Syed Ikhtiar Ahmed"], "emails": ["saiful-cse@sust.edu"], "sections": [{"heading": null, "text": "* Corresponding Author: * saiful-cse @ sust.eduKeywords: NB; SVM; SGD; LED; TFIDF; N-Gram; SupervisedLearning.Techniques that are Support Vector Machine (SVM), Na\u00efve Bayes (NB) and Stochastic Gradient Descent (SGD) compared to Bengali document categorization. Besides classification, classification also depends on how features are selected from the dataset. To analyze these classification services when predicting a document against twelve categories, this article also applies different feature selection techniques, namely chi-square distribution, normalized TFIDF (Term Frequency-Inverse Document Frequency) with a word analyzer. Therefore, we try to explore the efficiency of these three classification algorithms by using two different feature selection techniques in this article."}, {"heading": "1. INTRODUCTION", "text": "It is the process of grouping documents into different classes or categories. So document categorization plays an important role in natural language processing, computer science and information science. In the digital library system, search engine and document management system of this automatic categorization, this automatic categorization can be used. Spam filtering (SV\u00efgram et al., 1998), online message filtering (Chan et al., 2008), social media analytics (Melville et al., 2009), survey data grouping etc. are some applications of document categorization. Extensive research has already been conducted in this area for different languages. There are several powerful techniques that are offered by natural language processing. Three ways by which a document can be classified, monitored and semi-monitored techniques. In this paper we use monitored learning."}, {"heading": "3. FEATURE SELECTION ALGORITHM:", "text": "In this section we briefly describe the distribution methods TF-IDF and Chi-Square used in this study."}, {"heading": "TF-IDF:", "text": "The text representation is converted to a vector space model, which consists of the frequency of terms. The basic problem with the term frequency method is that it scales common terms because the meaning of rare terms is ignored, even though low frequency terms are important for distinguishing between classes. Where TFIDF approaches the reduction of common terms while scaling rare or rare terms at the same time. If a term occurs in 10 documents out of a total of 15 documents, it is not as important as what occurs in less than 10 documents or occurs only in one document. Therefore, TFIDF uses the logarithm scale to perform these tricks. Next, the IDF calculates according to the following formula: ln (+ 1 + 1) + 1"}, {"heading": "CHI-SQUARE DISTRIBUTION:", "text": "The chi square distribution is a simple statistical approach. On paper [25], the authors used this method to determine important words in a document. Mathematically, the chi square distribution can be written as follows: 2 = \u2211 {2 03 | Md. Saiful Islam et. al., I C E R I 2 0 1 7Wo 2 represents the chi square value, stands for the observed frequency, stands for the expected frequency and the degree of freedom is 1. To calculate the chi value of each word in a document, the equation can be changed as follows: 2 () = (,) \u2212 TF of a word / the total number of a word in a document. A word occurs when a word appears in a long sentence / total number of a word in a document, as many words occur in a document as are likely to occur together."}, {"heading": "4. CLASSIFICATION ALGORITHM:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "NA\u00cfVE BAYES:", "text": "The naive Bayes algorithm is a kind of probabilistic approach based on the application of the Bayes theorem. Depending on the exact type of probability calculation, this classifier can be productively prepared in a supervised learning method."}, {"heading": "STOCHASTIC GRADIENT DESCENT:", "text": "Stochastic Gradient Descent (SGD) is a simple but extremely productive way to deal with discriminatory learning of linear classifiers under curved accident capacities, for example (linear) Support Vector Machines and Logistic Regression [26]. Despite the fact that SGD has been around for quite a while in the machine learning group, it has only lately found a lot of consideration in terms of expansive learning. It is effectively applied to large-scale and sparse machine learning problems that frequently occur in DC. For sparse data, it easily scales to problems with more than 10 ^ 5 training samples and more than 10 ^ 5 characteristics. SUPORT VECTOR MACHINE: Another supervised learning algorithm is Support Vector Machine (SVM), which is widely and effectively used for DC. While training a classifier is necessary to manage a lot of characteristics. In our case it was 1676 characteristics from Table 28717 of SVM, which is absolutely not classified."}, {"heading": "5. EXPERIMENTAL SETUP AND RESULTS:", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "DATASET USED IN THIS EXPERIMENT:", "text": "All documents used in these experiments come from a Bengali document corpus [27]. All documents come from various Bangladeshi newspapers such as http: / / prothom-alo.com, http: / / bdnews24.com, http: / / dailykalerkantha.com, etc., and are marked with the corresponding category name. From this corpus, we collect for each category the same number of documents for training and testing as shown in Table I. There are twelve categories."}, {"heading": "PREPROCESS DATASET:", "text": "Before using the documents, training the classification model to pre-process the data set is a crucial step. It is necessary because it removes from each document recurring words or symbols that are common to all documents, and plays an important role in reducing feature space and increasing performance. In this way, it finds only those important words that are relevant to the data set information from which the classifier can determine the label of the document. Steps are (i) tokenization (ii) removal of the frequent symbol (iii) stemming and (iv) removal of all pronouns and conjunctions."}, {"heading": "APPLY FEATURE SELECTION AND TRAIN MODEL:", "text": "After pre-processing, two different techniques of feature engineering, as described above, were applied separately with SVM, SGD and NB classifiers and performance was compared with each other. For the chi square distribution system, we sorted all features in ascending order after selecting this top 30% feature. A total of 91,503 features were selected from a total of 28,717 training documents using pre-processing + chi square method. For TFIDF, we used the unique as word analyzer and normalized it with length normalization. A total of 216,576 features were selected from the same tensile documents as before using pre-processing + TFIDF, which is nearly 2.3 times more features than with pre-processing + chi square methodology. While we used the NB classifier, the alpha value was set to 0.01 and the SGD classifier with hinge as a loss function, 2 as a regulation, and 0.01 as a number of alpha."}, {"heading": "6. CONCLUSION:", "text": "After calculating the performance of different approaches, it turns out that the SVM classifier achieved the highest F1score (92.56%), while the normalized TFIDF was used as feature selection, and CHI-SQUARE + NB was the lowest with F1score at 83.36%. It is also observed that all classifiers performed better with the TFIDF feature selection technique than with the chi-square distribution method. The following graph shows the comparisons between them:"}, {"heading": "8. REFERENCES", "text": "In fact, most of them are able to survive themselves if they do not follow the rules they have imposed on themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}], "references": [{"title": "A Bayesian approach to filtering junk email., AAAI Workshop on Learning for Text Categorization, July 1998, Madison, Wisconsin", "author": ["M. Sahami", "S. Dumais", "D. Heckerman", "E. Horvitz"], "venue": "AAAI Technical Report", "citeRegEx": "Sahami et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sahami et al\\.", "year": 2001}, {"title": "Social media analytics: Channeling the power of the blogosphere for marketing insight.,", "author": ["P. Melville", "V. Sindhwani", "R. Lawrence"], "venue": "in Workshop on Information in Networks,", "citeRegEx": "Melville et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Melville et al\\.", "year": 2009}, {"title": "A k-nearest neighbor classification rule based on Dempster-Shafer theory,\" Systems, Man and Cybernetics", "author": ["T. Denoeux"], "venue": "IEEE Transactions on,", "citeRegEx": "Denoeux,? \\Q1995\\E", "shortCiteRegEx": "Denoeux", "year": 1995}, {"title": "Feature selection for text classification with Na\u00efve Bayes,\"Expert", "author": ["J. Chen", "H. Huang", "S. Tian", "Y. Qu"], "venue": "Systems with Applications,", "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Class-based n-gram models of natural language,", "author": ["P.F. Brown", "P.V. Desouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Induction of decision trees,", "author": ["J.R. Quinlan"], "venue": "Machine learning,", "citeRegEx": "Quinlan,? \\Q1986\\E", "shortCiteRegEx": "Quinlan", "year": 1986}, {"title": "Machine learning in automated text categorization,", "author": ["F. Sebastiani"], "venue": "ACM computing surveys (CSUR),", "citeRegEx": "Sebastiani,? \\Q2002\\E", "shortCiteRegEx": "Sebastiani", "year": 2002}, {"title": "Automated classification of web sites using Naive Bayesian algorithm,", "author": ["A.S. Patil", "B. Pawar"], "venue": "Proceedings of the International MultiConference of Engineers and Computer Scientists,", "citeRegEx": "Patil and Pawar,? \\Q2012\\E", "shortCiteRegEx": "Patil and Pawar", "year": 2012}, {"title": "KNN based Machine Learning Approach for Text and Document Mining,", "author": ["V. Bijalwan", "V. Kumar", "P. Kumari", "J. Pascual"], "venue": "International Journal of Database Theory and Application,", "citeRegEx": "Bijalwan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bijalwan et al\\.", "year": 2014}, {"title": "A comparative study of centroid-based, neighborhood-based and statistical approaches for effective document categorization", "author": ["Tam", "A Santoso", "R. Setiono"], "venue": "Proceedings of the 16 th International Conference on Pattern Recognition", "citeRegEx": "Tam et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tam et al\\.", "year": 2002}, {"title": "A Comparative Study on Different Types of Approaches to Text Categorization", "author": ["Pratiksha Y. Pawar", "S.H. Gawande"], "venue": "International Journal of Machine Learning and Computing,", "citeRegEx": "Pawar and Gawande,? \\Q2012\\E", "shortCiteRegEx": "Pawar and Gawande", "year": 2012}, {"title": "Shuicai, \"Study on SVM Compared with the other Text Classification Methods,\" in Education", "author": ["L. Zhijie", "L. Xueqiang", "L. Kun"], "venue": "Technology and Computer Science (ETCS),", "citeRegEx": "Zhijie et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhijie et al\\.", "year": 2010}, {"title": "Text categorization with Support Vector Machines: Learning with many relevant features", "author": ["Thorsten Joachims."], "venue": "Proceedings of the 10th European Conference on Machine Learning, ECML\u201998, pages 137\u2013142.", "citeRegEx": "Joachims.,? 1998", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "Text Classifiers for Cricket", "author": ["T. Zakzouk", "H. Mathkour"], "venue": "Sports News,\" in proceedings of International Conference on Telecommunications Technology and Applications ICTTA,", "citeRegEx": "Zakzouk and Mathkour,? \\Q2011\\E", "shortCiteRegEx": "Zakzouk and Mathkour", "year": 2011}, {"title": "RACHIDI, \"Automatic Arabic Document Categorization Based on the Na\u00efve Bayes Algorithm,\" in proceedings of the Workshop on Computational Approaches to Arabic", "author": ["Mohamed EL KOURDI", "Amine BENSAID", "Tajje-eddine"], "venue": "Script-based Languages,", "citeRegEx": "KOURDI et al\\.,? \\Q2004\\E", "shortCiteRegEx": "KOURDI et al\\.", "year": 2004}, {"title": "Support vector machines based Arabic language text classification system: feature selection comparative study,\" in Advances in Computer and Information Sciences and Engineering", "author": ["A. Moh\u2019d Mesleh"], "venue": null, "citeRegEx": "Mesleh,? \\Q2008\\E", "shortCiteRegEx": "Mesleh", "year": 2008}, {"title": "Automatic classification of Tamil documents using vector space model and artificial neural network,", "author": ["K. Rajan", "V. Ramalingam", "M. Ganesan", "S. Palanivel", "B. Palaniappan"], "venue": "Expert Systems with Applications,", "citeRegEx": "Rajan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rajan et al\\.", "year": 2009}, {"title": "Algorithm for Punjabi Text Classification,", "author": ["N. a. V. Gupta"], "venue": "International Journal of Computer Applications,", "citeRegEx": "Gupta,? \\Q2012\\E", "shortCiteRegEx": "Gupta", "year": 2012}, {"title": "Supervised Learning Methods For Bangla Web Document Categorization,", "author": ["Ashis Kumar Mandal", "Rikta Sen"], "venue": "International Journal of Artificial Intelligence & Applications (IJAIA),", "citeRegEx": "Mandal and Sen,? \\Q2014\\E", "shortCiteRegEx": "Mandal and Sen", "year": 2014}, {"title": "Keyword Extraction from a Single Document using Word Co-occurrence Statistical Information", "author": ["Y. Matsuo", "M. Ishizuka"], "venue": "International journal on Artificial Intelligence Tools, vol.13,", "citeRegEx": "Matsuo and Ishizuka,? \\Q2004\\E", "shortCiteRegEx": "Matsuo and Ishizuka", "year": 2004}], "referenceMentions": [{"referenceID": 1, "context": ", 2008 ), social media analytics (Melville et al., 2009), survey data grouping etc.", "startOffset": 33, "endOffset": 56}, {"referenceID": 2, "context": "Lots of well-established supervised techniques are used most frequently such as Denoeux (1995) used KNearest Neighbor(KNN), Chen et al.", "startOffset": 80, "endOffset": 95}, {"referenceID": 2, "context": "Lots of well-established supervised techniques are used most frequently such as Denoeux (1995) used KNearest Neighbor(KNN), Chen et al. (2009) used Na\u00efve Bays(NB), Brown et al.", "startOffset": 80, "endOffset": 143}, {"referenceID": 2, "context": "Lots of well-established supervised techniques are used most frequently such as Denoeux (1995) used KNearest Neighbor(KNN), Chen et al. (2009) used Na\u00efve Bays(NB), Brown et al.(1992) used N-grams,Quinlan (1986) used Decision Tree(DT), Sebastiani (2002) used Neural Network(NNet), Cortes and Vapnik (1995) used SVM.", "startOffset": 80, "endOffset": 183}, {"referenceID": 2, "context": "Lots of well-established supervised techniques are used most frequently such as Denoeux (1995) used KNearest Neighbor(KNN), Chen et al. (2009) used Na\u00efve Bays(NB), Brown et al.(1992) used N-grams,Quinlan (1986) used Decision Tree(DT), Sebastiani (2002) used Neural Network(NNet), Cortes and Vapnik (1995) used SVM.", "startOffset": 80, "endOffset": 211}, {"referenceID": 2, "context": "Lots of well-established supervised techniques are used most frequently such as Denoeux (1995) used KNearest Neighbor(KNN), Chen et al. (2009) used Na\u00efve Bays(NB), Brown et al.(1992) used N-grams,Quinlan (1986) used Decision Tree(DT), Sebastiani (2002) used Neural Network(NNet), Cortes and Vapnik (1995) used SVM.", "startOffset": 80, "endOffset": 253}, {"referenceID": 2, "context": "Lots of well-established supervised techniques are used most frequently such as Denoeux (1995) used KNearest Neighbor(KNN), Chen et al. (2009) used Na\u00efve Bays(NB), Brown et al.(1992) used N-grams,Quinlan (1986) used Decision Tree(DT), Sebastiani (2002) used Neural Network(NNet), Cortes and Vapnik (1995) used SVM.", "startOffset": 80, "endOffset": 305}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task.", "startOffset": 0, "endOffset": 164}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document.", "startOffset": 0, "endOffset": 335}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM.", "startOffset": 0, "endOffset": 431}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM. In their comparisons they showed that SVM is performed far better than all other approaches they used for 20Newsgroups dataset. Also Zhijie et al. (2010) compare SVM against KNN and NB classifier and their statistics proved that SVM is better than KNN and NB.", "startOffset": 0, "endOffset": 678}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM. In their comparisons they showed that SVM is performed far better than all other approaches they used for 20Newsgroups dataset. Also Zhijie et al. (2010) compare SVM against KNN and NB classifier and their statistics proved that SVM is better than KNN and NB. Joachims (1998) was the first who propose the use of a linear SVM with TFIDF term feature for DC.", "startOffset": 0, "endOffset": 800}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM. In their comparisons they showed that SVM is performed far better than all other approaches they used for 20Newsgroups dataset. Also Zhijie et al. (2010) compare SVM against KNN and NB classifier and their statistics proved that SVM is better than KNN and NB. Joachims (1998) was the first who propose the use of a linear SVM with TFIDF term feature for DC. Zakzouk and Mathkour (2011) used SVM for classify cricket sports news.", "startOffset": 0, "endOffset": 910}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM. In their comparisons they showed that SVM is performed far better than all other approaches they used for 20Newsgroups dataset. Also Zhijie et al. (2010) compare SVM against KNN and NB classifier and their statistics proved that SVM is better than KNN and NB. Joachims (1998) was the first who propose the use of a linear SVM with TFIDF term feature for DC. Zakzouk and Mathkour (2011) used SVM for classify cricket sports news. Besides English document there also done good amount of research on other languages too. KOURDI et al. (2004) used SVM , Mesleh (2008) used NB for automatic text classification in Arabic languages.", "startOffset": 0, "endOffset": 1063}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM. In their comparisons they showed that SVM is performed far better than all other approaches they used for 20Newsgroups dataset. Also Zhijie et al. (2010) compare SVM against KNN and NB classifier and their statistics proved that SVM is better than KNN and NB. Joachims (1998) was the first who propose the use of a linear SVM with TFIDF term feature for DC. Zakzouk and Mathkour (2011) used SVM for classify cricket sports news. Besides English document there also done good amount of research on other languages too. KOURDI et al. (2004) used SVM , Mesleh (2008) used NB for automatic text classification in Arabic languages.", "startOffset": 0, "endOffset": 1088}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM. In their comparisons they showed that SVM is performed far better than all other approaches they used for 20Newsgroups dataset. Also Zhijie et al. (2010) compare SVM against KNN and NB classifier and their statistics proved that SVM is better than KNN and NB. Joachims (1998) was the first who propose the use of a linear SVM with TFIDF term feature for DC. Zakzouk and Mathkour (2011) used SVM for classify cricket sports news. Besides English document there also done good amount of research on other languages too. KOURDI et al. (2004) used SVM , Mesleh (2008) used NB for automatic text classification in Arabic languages. For Tamil (South Indian language) languages Rajan et al. (2009) used Na\u00efve Bayes and Gupta (2012) used Neural networks for this task.", "startOffset": 0, "endOffset": 1215}, {"referenceID": 7, "context": "Patil and Pawar (2012) used Naive Bayes algorithm for classify the content in a web sites. They got average 80% accuracy for ten categories. Bijalwan et al. (2014) used KNN, NB and Term-gram for this task. In their experiment they showed that the accuracy of KNN is better choice than NB and Term-gram. Besides this, Tam et al. (2002) also showed that KNN is performed better than NNet and NB for English document. Y et al. (2012) did an comparative study on DT, NB, KNN, Rocchio\u2019s Algorithm, Back propagation Network, SVM. In their comparisons they showed that SVM is performed far better than all other approaches they used for 20Newsgroups dataset. Also Zhijie et al. (2010) compare SVM against KNN and NB classifier and their statistics proved that SVM is better than KNN and NB. Joachims (1998) was the first who propose the use of a linear SVM with TFIDF term feature for DC. Zakzouk and Mathkour (2011) used SVM for classify cricket sports news. Besides English document there also done good amount of research on other languages too. KOURDI et al. (2004) used SVM , Mesleh (2008) used NB for automatic text classification in Arabic languages. For Tamil (South Indian language) languages Rajan et al. (2009) used Na\u00efve Bayes and Gupta (2012) used Neural networks for this task.", "startOffset": 0, "endOffset": 1249}, {"referenceID": 18, "context": "Mandal and Sen(2014) compared four supervised learning techniques for labeled web documents into five categories.", "startOffset": 0, "endOffset": 21}], "year": 2016, "abstractText": "Learning. Abstract: Document categorization is a technique where the category of a document is determined. In this paper three well-known supervised learning techniques which are Support Vector Machine(SVM), Na\u00efve Bayes(NB) and Stochastic Gradient Descent(SGD) compared for Bengali document categorization. Besides classifier, classification also depends on how feature is selected from dataset. For analyzing those classifier performances on predicting a document against twelve categories several feature selection techniques are also applied in this article namely Chi square distribution, normalized TFIDF (term frequency-inverse document frequency) with word analyzer. So, we attempt to explore the efficiency of those three-classification algorithms by using two different feature selection techniques in this article.", "creator": "Microsoft\u00ae Word 2016"}}}