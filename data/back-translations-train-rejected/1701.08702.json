{"id": "1701.08702", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language Model", "abstract": "In this paper, we describe a research method that generates Bangla word clusters on the basis of relating to meaning in language and contextual similarity. The importance of word clustering is in parts of speech (POS) tagging, word sense disambiguation, text classification, recommender system, spell checker, grammar checker, knowledge discover and for many others Natural Language Processing (NLP) applications. In the history of word clustering, English and some other languages have already implemented some methods on word clustering efficiently. But due to lack of the resources, word clustering in Bangla has not been still implemented efficiently. Presently, its implementation is in the beginning stage. In some research of word clustering in English based on preceding and next five words of a key word they found an efficient result. Now, we are trying to implement the tri-gram, 4-gram and 5-gram model of word clustering for Bangla to observe which one is the best among them. We have started our research with quite a large corpus of approximate 1 lakh Bangla words. We are using a machine learning technique in this research. We will generate word clusters and analyze the clusters by testing some different threshold values.", "histories": [["v1", "Fri, 27 Jan 2017 18:43:31 GMT  (281kb)", "http://arxiv.org/abs/1701.08702v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dipaloke saha", "md saddam hossain", "md saiful islam", "sabir ismail"], "accepted": false, "id": "1701.08702"}, "pdf": {"name": "1701.08702.pdf", "metadata": {"source": "CRF", "title": "Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language Model", "authors": ["Dipaloke Saha", "Md Saddam Hossain", "Saiful Islam", "Sabir Ismail"], "emails": ["dipsustcse12@gmail.com,", "mshossaincse@gmail.com", "saiful-cse@sust.edu", "sabir.ismail01@gmail.com", "dipsustcse12@gmail.com"], "sections": [{"heading": null, "text": "Keywords: Word Cluster, NaturalLanguageProcessing, MachineLearning, N-gram Model, TermFrequency (tf). SUST, ICERIE."}, {"heading": "1. INTRODUCTION", "text": "Although Bangla is a widely used language, it lacks resources in its field of research. Recently, a new research dimension was added in Bangla called Word Clusters. This essay attempts to expand the exploration of word clusters for the Bangla language by assembling a large Bangla corpus of 97,971 individual words to generate the word clusters. In this essay, an unattended technique of machine learning and a method for clustering Bangla words based on similarities in semantics and contexts are proposed. In language processing, Word Clusters has a wide range of uses. A POS tag is one of them. The same bundled words usually contain the same POS tag. Word clusters can also produce suggestions for an inaccurately typed word, which is very helpful for spell checkers. Word Disambiguity, sentence structure with grammatical errors can also be bundled together with the same products, which can be very helpful in the case of bundled spelling systems of the same category."}, {"heading": "2. RELATED WORK", "text": "In Bangla, the implementation of word grouping is in the neophyte stage. In many other languages, different kinds of word grouping techniques are used. Finch and Chater (1992) implemented the Bigram model to calculate the weight matrix of a neural network. In a research proposed by Brown, Desouza, Mercer, Peitra, Lai (1992), the N-gram language model is used to group the words. Another experiment with the n-gram model is undertaken by Korkmaz (1997), in which a similarity function and a greedy algorithm are used to group the words into one and the same cluster. However, using the delete interpolation method of Mori, Nishimura and Itoh (1998), they achieved the better result than the Brown-Desouza method, which was done for the Japanese and English languages. Besides these Chinese words, there are a whole range of other languages, such as Arabic, etc."}, {"heading": "3. PROBLEM DEFINITION", "text": "Clustering is an unsupervised technique of machine learning that does not require any rules or predefined conditions. Objects that are very similar semantically or contextually are grouped in the same cluster and differ in different clusters. The method introduced in this problem focuses on two types of similarities such as semantics and contextual similarity. Consider the following four sentences: They are similar in their semantic meaning in sentences 1 and 2 and there is similarity in sentences 3 and 4. Here, the theory of the N-gram model is implemented. Probability distribution is used here to define n-th items in a sequence form before or after (n-1) items. Tri-gram, 4-th and 5-gram models are defined as sizes of 3, 4 and 5 of N-gram respectively. In this research, word clusters are created by implementing Tri, 4 and 5-gram models. After locating word clusters, the most efficient model is found on the basis of these clusters."}, {"heading": "4. METHODOLOGY", "text": "Similarly, a list of the next three words of a specific word is prepared for Tri-gram, four words for 4-gram, five words for 5-gram. Similarly, a list of the next three words and five words of a specific word is prepared for Tri-gram, four words for 4-gram, five words for 5-gram + list. Similarly, a list of the next three words and five words is determined as follows: In Tri-gram for each word pair Wi, Wj-list the number of matching words from the list (Wi-3, Wi-2, Wi-2, Wi-1, Wj-2, Wj-2, Wj-2, Wj-2, Wj-2, Wj-2, Wj-2, Wj-2, Wj-2, Wj-2, Wj-gram, Wj-1, Wj-1) P (Wi-Wi-Wi-Wi-Wi-Wi-3, Wi-3, Wi-Wi-3, Wi-3 (Wi-list, Wi-list) (Wi-Wi-list, Wi-gram, Wi-3, Wi-gram (Wi-Wi-3, Wi-3, Wi-3, Wi-3, Wi-3)."}, {"heading": "5. RESULT ANALYSIS", "text": "In the three, four and five gram models, we derive a total of 2215, 3327 and 5730 word clusters. After analyzing the word clusters of all three models, we find slight similarities in some word clusters, such as 266 for three grams, 300 for the fourth gram, and 360 for the fifth gram. Thus, we find 1949, 3027, and 5370 clusters in strong similarities for the three, fourth, and fifth gram models. Thus, the accuracy for strong similarity in tri grams: - 88% fourth gram: - 91% fifth gram: - 93% It is observed that the fourth gram is better than tri-gram, and the fifth gram is the best of all."}, {"heading": "6. CONCLUSION", "text": "For this reason, the trigram, 4 gram, and 5 gram models have been implemented in Bangla to continue the previous work on word merging, and the analyses and results presented above on a fairly large Bangla corpus have helped us to find the efficiency among the three mentioned models of word composition. On the basis of the observation, it can be said that better efficiency lies in the higher orders than the previous orders of the N gram model."}, {"heading": "H A S\u00e1nchez, A P Porrata and R B Llavori. \u201cWord sense disambiguation based on word sense clustering\u201d.", "text": "Advances in Artificial Intelligence, Springer Berlin Heidelberg, 2006. P: 472-481.Sabir Ismail, M. Shahidur Rahman. https: / / www.researchgate.net / publication / 261551758 _ Bangla _ Word _ Cl ustering _ Based _ on _ N-gram _ Language _ Model, in press."}, {"heading": "S Finch and N Chater. \u201cAutomatic methods for finding linguisticcategories\u201d. In Igor Alexander and John", "text": "Taylor, editors, ArtificialNeural Networks, Volume 2. Elsevier Science Publishers, 1992.P F Brown, P V Desouza, R L Mercer, V J D Pietra, V J Della. and J CLai. \"Class-based N-gram Models of Natural Language.\" Computationalinguistics, 18 No: 4, 1992, P: 467-479.EEEEKorkmaz. \"A method for improving automatic wordcategorization.\" Dissertation, Middle East Technical University, 1997, in print."}, {"heading": "S Mori, M Nishimura and N Itoh. \u201cWord clustering for a word bi - gramModel\u201d. International Conference on", "text": "Language Processing, 1998, in press.Clustering - Introduction, http: / / home.deib.polimi.it / matteucc / Clustering / tutorial _ html.Clustering - Introduction, \"http: / / www.stanford.edu / class / cs345a / slides / 12-clustering.pdf\" Stanford University Clustering.Similarity in semantics and contexts, http: / / www.ilc.cnr.it / EAGLES96 / rep2 / node37.html"}], "references": [{"title": "Word sense disambiguation based on word sense clustering", "author": ["H A S\u00e1nchez", "A P Porrata", "R B Llavori"], "venue": "Advances in Artificial Intelligence,Springer Berlin Heidelberg,", "citeRegEx": "S\u00e1nchez et al\\.,? \\Q2006\\E", "shortCiteRegEx": "S\u00e1nchez et al\\.", "year": 2006}, {"title": "Automatic methods for finding linguisticcategories", "author": ["S Finch", "N Chater"], "venue": "Elsevier Science Publishers,", "citeRegEx": "Finch and Chater.,? \\Q1992\\E", "shortCiteRegEx": "Finch and Chater.", "year": 1992}, {"title": "Word clustering for a word bi - gramModel", "author": ["S Mori", "M Nishimura", "N Itoh"], "venue": "Technical University,", "citeRegEx": "Mori et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Mori et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 1, "context": "Finch and Chater (1992) implemented bigram model for the calculation of weight matrix of a neural network.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Finch and Chater (1992) implemented bigram model for the calculation of weight matrix of a neural network. N-gram language model is used on word clustering in a research proposed by Brown, Desouza, Mercer, Peitra, Lai (1992). Another effort using n-gram model is introduced by Korkmaz (1997) in which a similarity function and greedy algorithm is used to group the words into same cluster.", "startOffset": 0, "endOffset": 225}, {"referenceID": 1, "context": "Finch and Chater (1992) implemented bigram model for the calculation of weight matrix of a neural network. N-gram language model is used on word clustering in a research proposed by Brown, Desouza, Mercer, Peitra, Lai (1992). Another effort using n-gram model is introduced by Korkmaz (1997) in which a similarity function and greedy algorithm is used to group the words into same cluster.", "startOffset": 0, "endOffset": 292}, {"referenceID": 1, "context": "Finch and Chater (1992) implemented bigram model for the calculation of weight matrix of a neural network. N-gram language model is used on word clustering in a research proposed by Brown, Desouza, Mercer, Peitra, Lai (1992). Another effort using n-gram model is introduced by Korkmaz (1997) in which a similarity function and greedy algorithm is used to group the words into same cluster. However, with the use of delete interpolation method by Mori, Nishimura and Itoh (1998) they got the better result than the Brown, Desouza\u2019s method.", "startOffset": 0, "endOffset": 478}], "year": 2016, "abstractText": "\uf0b7 SUST, ICERIE. Abstract: \u2014 In this paper, we describe a research method that generates Bangla word clusters on the basis of relating to meaning in language and contextual similarity. The importance of word clustering is in parts of speech (POS) tagging, word sense disambiguation, text classification, recommender system, spell checker, grammar checker, knowledge discover and for many others Natural Language Processing (NLP) applications. In the history of word clustering, English and some other languages have already implemented some methods on word clustering efficiently. But due to lack of the resources, word clustering in Bangla has not been still implemented efficiently. Presently, it\u2019s implementation is in the beginning stage. In some research of word clustering in English based on preceding and next five words of a key word they found an efficient result. Now, we are trying to implement the tri-gram, 4-gram and 5-gram model of word clustering for Bangla to observe which one is the best among them. We have started our research with quite a large corpus of approximate 1 lakh Bangla words. We are using a machine learning technique in this research. We will generate word clusters and analyze the clusters by testing some different threshold values.", "creator": "Microsoft\u00ae Office Word 2007"}}}