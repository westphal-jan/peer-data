{"id": "1701.08837", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2017", "title": "Emergence of Selective Invariance in Hierarchical Feed Forward Networks", "abstract": "Many theories have emerged which investigate how in- variance is generated in hierarchical networks through sim- ple schemes such as max and mean pooling. The restriction to max/mean pooling in theoretical and empirical studies has diverted attention away from a more general way of generating invariance to nuisance transformations. We con- jecture that hierarchically building selective invariance (i.e. carefully choosing the range of the transformation to be in- variant to at each layer of a hierarchical network) is im- portant for pattern recognition. We utilize a novel pooling layer called adaptive pooling to find linear pooling weights within networks. These networks with the learnt pooling weights have performances on object categorization tasks that are comparable to max/mean pooling networks. In- terestingly, adaptive pooling can converge to mean pooling (when initialized with random pooling weights), find more general linear pooling schemes or even decide not to pool at all. We illustrate the general notion of selective invari- ance through object categorization experiments on large- scale datasets such as SVHN and ILSVRC 2012.", "histories": [["v1", "Mon, 30 Jan 2017 21:44:27 GMT  (298kb,D)", "http://arxiv.org/abs/1701.08837v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["dipan k pal", "vishnu boddeti", "marios savvides"], "accepted": false, "id": "1701.08837"}, "pdf": {"name": "1701.08837.pdf", "metadata": {"source": "CRF", "title": "Emergence of Selective Invariance in Hierarchical Feed Forward Networks", "authors": ["Dipan K. Pal", "Vishnu N. Boddeti", "Marios Savvides"], "emails": ["marioss}@andrew.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "In the last ten years, we have gained enormous popularity, and despite many studies to improve their generalization capabilities, much of their basic architecture remains the same, showing that the basic hierarchical modules, which include convolution, non-linearity, and pooling operations, are very effective in various areas and modalities. However, there is still much to be done to answer the basic tasks of each of these modules. In this paper, we focus on pooling, which receives relatively little attention compared to the filter weights, overall architecture, and training processes."}, {"heading": "2. A conjecture involving Selective Invariance", "text": "The structure must satisfy four axioms, namely that each subset of the group can be used to define a certain range of transformation to be a valid group. A group can have a limited number of elements that lead to a finite group. In cases where a group is used to model a transformation, each subset of the group can be used to define a certain range of transformations that define a certain range of transformational characteristics that relate to a certain range of transformational characteristics: Each function f via x Rd is an invariant feature w.r.t a group G, if f (x) = f (gx).g It is an equivalent feature or covariant feature w.r.t The group, if f (x) f (x) f (x) is group G, where h is defined a linear function via G, it is not desirable."}, {"heading": "3. Partially Invariant Features through Partial", "text": "Most of them require some basic assumption regarding the structure of transformations. One of the most common assumptions is that the transformations in individual countries not only in the USA but also in the USA, in Europe, Europe, Europe, Europe, the USA, the USA, the USA, the USA, the USA, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, France, Great Britain, Great Britain, France, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain United Kingdom, Great Britain United Kingdom, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain United Kingdom, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain United Kingdom, Great Britain, Great Britain, Great Britain, Great Britain, Great Britain United Kingdom, Great Britain, Great Britain, Great Britain United Kingdom, Great Britain, Great Britain United Kingdom, Great Britain and Northern Ireland, Great Britain, Great Britain and Northern Ireland, Great Britain and Northern Ireland, Great Britain and Northern Ireland, Great Britain and Northern Ireland, Great Britain and Northern Ireland, Great Britain, Great Britain United Kingdom, Great Britain United Kingdom, Great Britain and Northern Ireland, Great Britain and Northern Ireland, Great Britain United Kingdom, Great Britain United Kingdom, Great Britain United Kingdom, Great Britain, Great Britain United Kingdom, Great Britain, Great Britain and Northern Ireland, Great Britain United Kingdom, Great Britain United Kingdom, Great Britain and Northern Ireland, Great Britain United Kingdom, Great Britain United Kingdom, Great Britain United Kingdom, Great Britain United Kingdom, United Kingdom,"}, {"heading": "4. Adaptive Pooling Module for Learning Generalized Linear Pooling", "text": "It is a linear process and can therefore be modeled using a Matrix-A card in this framework by optimizing the support of maximum operation instead. However, we focus on generalized linear pooling in this area. Notation: We let each input layer be denoted into the kth layer of uk-Rm and the output of the layer. Let L be the loss that the network optimizes for. The adaptive pooling matrix is defined by A-Rm-n with n pooling elements.We have vk the pooling. We have the loss that the network optimizes for. The adaptive pooling matrix is defined by A-Rm-n with n pooling elements.We have vk = ATuk."}, {"heading": "5. Emergence of Selective Invariance and Redundant Pooling", "text": "We use standard ConveNets architectures and replace the middle and maximum pooling layers with adaptive pooling. We train the networks to minimize logistical soft-max losses on large classification benchmarks such as the Street View House Numbers (SVHN) and the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012. Mean and maximum pooling field sizes were set at 2 \u00d7 2 for all experiments."}, {"heading": "5.1. Street View House Numbers (SVHN)", "text": "In fact, most of them are able to play by the rules without having to play by the rules."}, {"heading": "5.2. ImageNet Large Scale Visual Recognition", "text": "Challenge (ILSVRC) 2012The ILSVRC 2012 Challenge has about 1,000 classes and over 1.2 million images for training and about 50,000 images for validation. We are using the standard AlexNet architecture [13] for this task. We are naming against standard middle and maximum pooling. To include adaptive pooling, we are replacing all three pooling layers in AlexNet with adaptive pooling.Initialization: Convolution filters for baseline networks with medium- and maximum-pooling are always randomly initialized. We initialized the network in two ways. First, we tried to randomly initiate adaptive pooling parameters along with convolution parameters, resulting in extremely slow learning due to the increased number of parameters, no regulation, and a harder classification task. Second, we trained AlexNet with middle pooling layers for 25 and then 65 epochs (with convolutionary layers)."}, {"heading": "6. Discussion", "text": "This year, it's so far that it will be able to put itself at the top, \"he said.\" It's too early to say what we want to do, \"he said.\" It's still too early to do it, \"he said.\" But it's still too early to do it, \"he said.\" It's still too early to do it. \""}], "references": [{"title": "On invariance and selectivity in representation learning", "author": ["F. Anselmi", "L. Rosasco", "T. Poggio"], "venue": "arXiv preprint arXiv:1503.05938", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning mid-level features for recognition", "author": ["Y.-L. Boureau", "F. Bach", "Y. LeCun", "J. Ponce"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2559\u20132566. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "A theoretical analysis of feature pooling in visual recognition", "author": ["Y.-L. Boureau", "J. Ponce", "Y. LeCun"], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10), pages 111\u2013118", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning stable group invariant representations with convolutional networks", "author": ["J. Bruna", "A. Szlam", "Y. LeCun"], "venue": "arXiv preprint arXiv:1301.3537", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Group equivariant convolutional networks", "author": ["T.S. Cohen", "M. Welling"], "venue": "CoRR, abs/1602.07576", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploiting cyclic symmetry in convolutional neural networks", "author": ["S. Dieleman", "J. De Fauw", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.02660", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploiting cyclic symmetry in convolutional neural networks", "author": ["S. Dieleman", "J.D. Fauw", "K. Kavukcuoglu"], "venue": "CoRR, abs/1602.02660", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep symmetry networks", "author": ["R. Gens", "P.M. Domingos"], "venue": "Advances in neural information processing systems, pages 2537\u20132545", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Fractional max-pooling", "author": ["B. Graham"], "venue": "CoRR, abs/1412.6071", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1406.4729", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning translation invariant recognition in a massively parallel networks", "author": ["G.E. Hinton"], "venue": "PARLE Parallel Architectures and Languages Europe, pages 1\u201313. Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1987}, {"title": "What is the best multi-stage architecture for object recognition? In Computer Vision", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "2009 IEEE 12th International Conference on, pages 2146\u20132153. IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Transformation-invariant convolutional jungles", "author": ["D. Laptev", "J.M. Buhmann"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3043\u20133051", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Subtasks of unconstrained face recognition", "author": ["J.Z. Leibo", "Q. Liao", "T. Poggio"], "venue": "International Joint Conference on Computer Vision, Imaging and Computer Graphics, VISI- GRAPP", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning invariant representations and applications to face verification", "author": ["Q. Liao", "J.Z. Leibo", "T. Poggio"], "venue": "Advances in Neural Information Processing Systems, pages 3057\u20133065", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Group invariant scattering", "author": ["S. Mallat"], "venue": "Communications on Pure and Applied Mathematics, 65(10):1331\u20131398", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "On random weights and unsupervised feature learning", "author": ["A. Saxe", "P.W. Koh", "Z. Chen", "M. Bhand", "B. Suresh", "A.Y. Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), pages 1089\u20131096", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Convex learning with invariances", "author": ["C.H. Teo", "A. Globerson", "S.T. Roweis", "A.J. Smola"], "venue": "Advances in neural information processing systems, pages 1489\u20131496", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Representation theory and invariant neural networks", "author": ["J. Wood", "J. Shawe-Taylor"], "venue": "Discrete applied mathematics, 69(1):33\u201360", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "arXiv preprint arXiv:1301.3557", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Local features and kernels for classification of texture and object categories: A comprehensive study", "author": ["J. Zhang", "M. Marsza\u0142ek", "S. Lazebnik", "C. Schmid"], "venue": "International journal of computer vision, 73(2):213\u2013238", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 14, "context": "Convolutional nets (ConvNets [15]) have gained immense popularity over the past decade.", "startOffset": 29, "endOffset": 33}, {"referenceID": 9, "context": "However, these studies approach pooling either from an engineering standpoint (making ConvNets flexible in terms of input size [10]), or from a regularization perspective [22, 9].", "startOffset": 127, "endOffset": 131}, {"referenceID": 21, "context": "However, these studies approach pooling either from an engineering standpoint (making ConvNets flexible in terms of input size [10]), or from a regularization perspective [22, 9].", "startOffset": 171, "endOffset": 178}, {"referenceID": 8, "context": "However, these studies approach pooling either from an engineering standpoint (making ConvNets flexible in terms of input size [10]), or from a regularization perspective [22, 9].", "startOffset": 171, "endOffset": 178}, {"referenceID": 2, "context": "One of the fundamental objective that pooling tries to optimize for, is hypothesized in many works to be generating invariance to input nuisance transformations [3, 19, 1].", "startOffset": 161, "endOffset": 171}, {"referenceID": 18, "context": "One of the fundamental objective that pooling tries to optimize for, is hypothesized in many works to be generating invariance to input nuisance transformations [3, 19, 1].", "startOffset": 161, "endOffset": 171}, {"referenceID": 0, "context": "One of the fundamental objective that pooling tries to optimize for, is hypothesized in many works to be generating invariance to input nuisance transformations [3, 19, 1].", "startOffset": 161, "endOffset": 171}, {"referenceID": 20, "context": "Invariance to transformations: A plethora of literature exists to show that one of the core problems in pattern recognition is to generate invariance to transformations g in the data, leading to significant improvements in recognition performance [21, 4, 1, 16, 11, 17].", "startOffset": 247, "endOffset": 269}, {"referenceID": 3, "context": "Invariance to transformations: A plethora of literature exists to show that one of the core problems in pattern recognition is to generate invariance to transformations g in the data, leading to significant improvements in recognition performance [21, 4, 1, 16, 11, 17].", "startOffset": 247, "endOffset": 269}, {"referenceID": 0, "context": "Invariance to transformations: A plethora of literature exists to show that one of the core problems in pattern recognition is to generate invariance to transformations g in the data, leading to significant improvements in recognition performance [21, 4, 1, 16, 11, 17].", "startOffset": 247, "endOffset": 269}, {"referenceID": 15, "context": "Invariance to transformations: A plethora of literature exists to show that one of the core problems in pattern recognition is to generate invariance to transformations g in the data, leading to significant improvements in recognition performance [21, 4, 1, 16, 11, 17].", "startOffset": 247, "endOffset": 269}, {"referenceID": 10, "context": "Invariance to transformations: A plethora of literature exists to show that one of the core problems in pattern recognition is to generate invariance to transformations g in the data, leading to significant improvements in recognition performance [21, 4, 1, 16, 11, 17].", "startOffset": 247, "endOffset": 269}, {"referenceID": 16, "context": "Invariance to transformations: A plethora of literature exists to show that one of the core problems in pattern recognition is to generate invariance to transformations g in the data, leading to significant improvements in recognition performance [21, 4, 1, 16, 11, 17].", "startOffset": 247, "endOffset": 269}, {"referenceID": 0, "context": "invariant to a subset of G) to the transformation group G allow for the sample complexity to be reduced [1, 17].", "startOffset": 104, "endOffset": 111}, {"referenceID": 16, "context": "invariant to a subset of G) to the transformation group G allow for the sample complexity to be reduced [1, 17].", "startOffset": 104, "endOffset": 111}, {"referenceID": 0, "context": "Previous work such as [1, 16] show that even though complete invariance is unachievable in practice, (as consolation) partial invariance holds.", "startOffset": 22, "endOffset": 29}, {"referenceID": 15, "context": "Previous work such as [1, 16] show that even though complete invariance is unachievable in practice, (as consolation) partial invariance holds.", "startOffset": 22, "endOffset": 29}, {"referenceID": 4, "context": "It is more common to investigate explicit complete invariance to transformation groups such as the rotation group and/or the translation group [5, 7].", "startOffset": 143, "endOffset": 149}, {"referenceID": 6, "context": "It is more common to investigate explicit complete invariance to transformation groups such as the rotation group and/or the translation group [5, 7].", "startOffset": 143, "endOffset": 149}, {"referenceID": 17, "context": "Group invariant scattering was also proposed as a theoretical framework for modelling entire translation group invariances in ConvNets as contractions acting on the entire space globally [18].", "startOffset": 187, "endOffset": 191}, {"referenceID": 2, "context": "Mean/max pooling was examined in detail in terms of discriminability in [3].", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "The effect of max pooling on hard-vector quantized features was shown to help performance [2].", "startOffset": 90, "endOffset": 93}, {"referenceID": 11, "context": "A study more aligned towards highlighting the importance of pooling (even with random convolution filters) is [12].", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "One of the most common assumptions is that the transformations form a group [1, 14, 18].", "startOffset": 76, "endOffset": 87}, {"referenceID": 13, "context": "One of the most common assumptions is that the transformations form a group [1, 14, 18].", "startOffset": 76, "endOffset": 87}, {"referenceID": 17, "context": "One of the most common assumptions is that the transformations form a group [1, 14, 18].", "startOffset": 76, "endOffset": 87}, {"referenceID": 0, "context": "if two images (x, x\u2032) are equivalent under some g, then their distributions are identical [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "Further, measures of the distribution of {\u3008x, g\u22121t\u3009},\u2200g \u2208 G0 where G0 \u2286 G are also invariant owing to partial integrals over partial groups [1].", "startOffset": 140, "endOffset": 143}, {"referenceID": 19, "context": "A previous study that allowed partial invariance although in a much simplistic non-hierarchical setting is [20].", "startOffset": 107, "endOffset": 111}, {"referenceID": 22, "context": "It has also been argued that local features should only have enough invariance (as opposed to complete invariance) as required by the application [23].", "startOffset": 146, "endOffset": 150}, {"referenceID": 7, "context": "Other network architectures which incorporate more transformations (such as [8, 6]) can also be modelled in this framework as long as the transformation is unitary.", "startOffset": 76, "endOffset": 82}, {"referenceID": 5, "context": "Other network architectures which incorporate more transformations (such as [8, 6]) can also be modelled in this framework as long as the transformation is unitary.", "startOffset": 76, "endOffset": 82}, {"referenceID": 0, "context": "It has been shown that a non-linear feature of the form as Equation 3 is partially invariant given a finite partial group [1].", "startOffset": 122, "endOffset": 125}, {"referenceID": 12, "context": "We use the standard AlexNet architecture [13] for this task.", "startOffset": 41, "endOffset": 45}], "year": 2017, "abstractText": "Many theories have emerged which investigate how invariance is generated in hierarchical networks through simple schemes such as max and mean pooling. The restriction to max/mean pooling in theoretical and empirical studies has diverted attention away from a more general way of generating invariance to nuisance transformations. In this exploratory study, we study the conjecture that hierarchically building selective invariance is important for pattern recognition. We define selective invariance as carefully choosing the range of the transformation to be invariant to at each layer of a hierarchical network. For the purpose of our study, we utilize a novel method called adaptive pooling where the pooling weights are not constrained and in fact can adapt their pooling regions to the data. These networks with the adapted pooling regions maintain performances on object categorization tasks comparable to max/mean pooling networks despite being more prone to overfitting. Interestingly, adaptive pooling regions can converge to mean pooling (even when initialized with random pooling regions), find more general linear pooling schemes or even decide not to pool at all. The pooling regions that emerge from the data are not random but rather contiguous, illustrating invariance to contiguous ranges of transformations. We illustrate the general notion of selective invariance through object categorization experiments on largescale datasets such as SVHN and ILSVRC 2012.", "creator": "LaTeX with hyperref package"}}}