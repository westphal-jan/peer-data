{"id": "1701.08868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2017", "title": "Interaction Information for Causal Inference: The Case of Directed Triangle", "abstract": "Interaction information is one of the multivariate generalizations of mutual information, which expresses the amount information shared among a set of variables, beyond the information, which is shared in any proper subset of those variables. Unlike (conditional) mutual information, which is always non-negative, interaction information can be negative. We utilize this property to find the direction of causal influences among variables in a triangle topology under some mild assumptions.", "histories": [["v1", "Mon, 30 Jan 2017 23:01:15 GMT  (664kb,D)", "http://arxiv.org/abs/1701.08868v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["amiremad ghassami", "negar kiyavash"], "accepted": false, "id": "1701.08868"}, "pdf": {"name": "1701.08868.pdf", "metadata": {"source": "CRF", "title": "Interaction Information for Causal Inference: The Case of Directed Triangle", "authors": ["AmirEmad Ghassami", "Negar Kiyavash"], "emails": ["ghassam2@illinois.edu", "kiyavash@illinois.edu"], "sections": [{"heading": null, "text": "In fact, it is as if most people who are able are able to determine themselves what they want and what they do not want. (...) It is not as if they are able to determine themselves. (...) It is not as if they are able to determine themselves. (...) It is not as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are doing it as if they are able to do it. (...). (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves."}, {"heading": "III. APPLICATION TO CAUSAL INFERENCE", "text": "A directed acyclic graph (DAG) is a finite directed graph without directed cycles. Definition 2. A Bayesian network structure G is a DAG whose nodes represent random variables."}, {"heading": "B. P2 DAG", "text": "Structures in parts (a) and (b) are therefore referred to as chains, (c) is referred to as forks, and (d) is referred to as V structures. In a V structure, the middle variable is referred to as colliders. It is known that in the observation structure, we can identify a DAG at most up to its Markov equivalence class [13]. Once we have the information about the DAG skeleton (which could be derived from the correlations), we can use dependence tests to distinguish a V structure from the other three: If the variables X and Z are dependent, we are independent if the true structure is not observed; otherwise, it will be one of the other three."}, {"heading": "IV. CONCLUSION", "text": "We investigated interaction information, which is a multivariate generalization of reciprocal information and indicates the amount of information that is shared in a set of variables, beyond the information that is shared in a proper subset of these variables. Unlike other conventional measures of information, interaction information can have a negative value. We used this property to uncover causal relationships between a triplet of random variables. We provided a discussion of the signs of interaction information and proposed a strategy for classifying causal relationships that could not be identified by purely conventional dependency tests. Interaction information is not studied as thoroughly as its bivariate counterpart. A more comprehensive study of the benefits of this quantity in the field of causal inference, especially when more than three variables are considered as our future work."}], "references": [{"title": "Information theoretical analysis of multivariate correlation,", "author": ["S. Watanabe"], "venue": "IBM Journal of research and development,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1960}, {"title": "Multivariate information transmission,", "author": ["W.J. McGill"], "venue": "Psychometrika, vol. 19,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1954}, {"title": "The Transmission of Information: A Statistical Theory of Communication", "author": ["R.M. Fano"], "venue": "MIT Press, Cambridge, Massachussets", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1961}, {"title": "A new outlook on shannon\u2019s information measures,", "author": ["R.W. Yeung"], "venue": "IEEE transactions on information theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1991}, {"title": "The co-information lattice,", "author": ["A.J. Bell"], "venue": "Proceedings of the Fifth International Workshop on Independent Component Analysis and Blind Signal Separation: ICA,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Quantifying and visualizing attribute interactions,", "author": ["A. Jakulin", "I. Bratko"], "venue": "arXiv preprint cs/0308002,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "and T", "author": ["C.J. Quinn", "N. Kiyavash"], "venue": "P. Coleman, \u201cEfficient methods to compute optimal tree approximations of directed information graphs,\u201d IEEE Transactions on Signal Processing, vol. 61, no. 12, pp. 3173\u20133182", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "and K", "author": ["J. Etesami", "N. Kiyavash", "K. Zhang"], "venue": "Singhal, \u201cLearning network of multivariate hawkes processes: A time series approach,\u201d arXiv preprint arXiv:1603.04319", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "and T", "author": ["C.J. Quinn", "N. Kiyavash"], "venue": "P. Coleman, \u201cEquivalence between minimal generative model graphs and directed information graphs,\u201d in Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on, pp. 293\u2013297, IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "and B", "author": ["M. Kocaoglu", "A.G. Dimakis", "S. Vishwanath"], "venue": "Hassibi, \u201cEntropic causal inference,\u201d arXiv preprint arXiv:1611.04035", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "and T", "author": ["J. Etesami", "N. Kiyavash"], "venue": "Coleman, \u201cLearning minimal latent directed information polytrees,\u201d Neural Computation", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Causality", "author": ["J. Pearl"], "venue": "Cambridge university press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "On triple mutual information,", "author": ["T. Tsujishita"], "venue": "Advances in applied mathematics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Causation", "author": ["P. Spirtes", "C.N. Glymour", "R. Scheines"], "venue": "prediction, and search. MIT press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "B", "author": ["D. Janzing", "D. Balduzzi", "M. Grosse-Wentrup"], "venue": "Sch\u00f6lkopf, et al., \u201cQuantifying causal influences,\u201d The Annals of Statistics, vol. 41, no. 5, pp. 2324\u20132358", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The most well known generalizations are total correlation [1] (also known as multi-information [2]), and interaction information [3], [4].", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "The most well known generalizations are total correlation [1] (also known as multi-information [2]), and interaction information [3], [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "The most well known generalizations are total correlation [1] (also known as multi-information [2]), and interaction information [3], [4].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "This quantity has been studied from different view points and under different names in the literature [3]\u2013[7].", "startOffset": 102, "endOffset": 105}, {"referenceID": 5, "context": "This quantity has been studied from different view points and under different names in the literature [3]\u2013[7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 1, "context": "In the case of three random variables, interaction information is the gain (or loss) in information transmitted between any two of the variables, due to additional knowledge of the third random variable [3].", "startOffset": 203, "endOffset": 206}, {"referenceID": 6, "context": "Other information-theoretic quantities such as entropy and directed information have also been proposed to infer causality in appropriate settings [8]\u2013[12].", "startOffset": 147, "endOffset": 150}, {"referenceID": 10, "context": "Other information-theoretic quantities such as entropy and directed information have also been proposed to infer causality in appropriate settings [8]\u2013[12].", "startOffset": 151, "endOffset": 155}, {"referenceID": 11, "context": "In an observational setup, where performing interventions is not possible, the main approach to identify direction of influences is to perform some sort of statistical dependency tests on data [13].", "startOffset": 193, "endOffset": 197}, {"referenceID": 11, "context": "In Pearl\u2019s language [13], this is because all triangles are in the same Markov equivalent class.", "startOffset": 20, "endOffset": 24}, {"referenceID": 4, "context": "The general formula for interaction information for a set of variables V is defined as [6]", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "Here, interaction information could be represented in terms of mutual information as follows [3]: ar X iv :1 70 1.", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "Specifically, Yeung [5] showed that", "startOffset": 20, "endOffset": 23}, {"referenceID": 12, "context": "We refer readers to [14], where Tsujishita has provided a more in depth mathematical study of the bounds on interaction information as well as some other properties for this quantity.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "Most of the definitions are adopted from [15].", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "Bayesian networks are commonly used to represent causal relationships among the set of variables [13], [16].", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "Bayesian networks are commonly used to represent causal relationships among the set of variables [13], [16].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "For this purpose, we use the results from [17].", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "Following [17], we define the strength of the causal influence of a set of arrows S as", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "It is known that in the observational setup, we can identify a DAG at most up to its Markov equivalent class [13].", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "We use an example from [15] to illustrate the case of negative interaction information.", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "Here, as mentioned in Subsection III-A, we need to be able to quantify the strength of a causal effect, for which we use the results from [17], which was described in Subsection III-A.", "startOffset": 138, "endOffset": 142}, {"referenceID": 15, "context": "The postulated quantity in [17] for causal influence strength implies that", "startOffset": 27, "endOffset": 31}], "year": 2017, "abstractText": "To be considered for the 2017 IEEE Jack Keil Wolf ISIT Student Paper Award. Interaction information is one of the multivariate generalizations of mutual information, which expresses the amount information shared among a set of variables, beyond the information, which is shared in any proper subset of those variables. Unlike (conditional) mutual information, which is always non-negative, interaction information can be negative. We utilize this property to find the direction of causal influences among variables in a triangle topology under some mild assumptions.", "creator": "LaTeX with hyperref package"}}}