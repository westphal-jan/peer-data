{"id": "1701.08878", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2017", "title": "Deep Reinforcement Learning for Robotic Manipulation-The state of the art", "abstract": "The focus of this work is to enumerate the various approaches and algorithms that center around application of reinforcement learning in robotic ma- ]]nipulation tasks. Earlier methods utilized specialized policy representations and human demonstrations to constrict the policy. Such methods worked well with continuous state and policy space of robots but failed to come up with generalized policies. Subsequently, high dimensional non-linear function approximators like neural networks have been used to learn policies from scratch. Several novel and recent approaches have also embedded control policy with efficient perceptual representation using deep learning. This has led to the emergence of a new branch of dynamic robot control system called deep r inforcement learning(DRL). This work embodies a survey of the most recent algorithms, architectures and their implementations in simulations and real world robotic platforms. The gamut of DRL architectures are partitioned into two different branches namely, discrete action space algorithms(DAS) and continuous action space algorithms(CAS). Further, the CAS algorithms are divided into stochastic continuous action space(SCAS) and deterministic continuous action space(DCAS) algorithms. Along with elucidating an organ- isation of the DRL algorithms this work also manifests some of the state of the art applications of these approaches in robotic manipulation tasks.", "histories": [["v1", "Tue, 31 Jan 2017 00:16:15 GMT  (1483kb,D)", "http://arxiv.org/abs/1701.08878v1", "18 pages"]], "COMMENTS": "18 pages", "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["smruti amarjyoti"], "accepted": false, "id": "1701.08878"}, "pdf": {"name": "1701.08878.pdf", "metadata": {"source": "CRF", "title": "Deep reinforcement learning for robotic manipulation-the state of the art", "authors": ["Smruti Amarjyoti"], "emails": ["samarjyo@andrew.cmu.edu"], "sections": [{"heading": null, "text": "The focus of this work is on the enumeration of the various approaches and algorithms that revolve around the application of reinforcement learning to robotic manipulation tasks. Previous methods used specialized policy representations and human demonstrations to constrain policy. Such methods worked well with the continuous state and policy space of robots, but missed generalized strategies. Subsequently, high-dimensional nonlinear functional approximation systems such as neural networks were used to learn strategies from scratch. Moreover, several recent and recent approaches have embedded efficient perception representation using deep SCLearning in control policy, leading to the emergence of a new branch of dynamic robot control systems called deep reinforcement learning (DRL).This work embodies an overview of recent algorithms, architectures and their implementation in simulations and real robot platforms. The palette of DRL architectures is divided into two distinct branches, namely CAS action algorithms (continuous action algorithms) and CAS (continuous action algorithms)."}, {"heading": "1 Introduction", "text": "This is another approach, which is the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the way, like the"}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Preliminaries", "text": "All reinforcement methods examined in this paper are essentially control problems where an actor must act in a stochastic environment by selecting actions sequentially over several time steps with the intention of maximizing cumulative reward. It is modeled as a Markov decision-making process (MDP) that includes a state space S, a space A, an initial state distribution with density p1 (s1), a stationary transition dynamic model with density p (st + 1 | st, at) that satisfies the Markov property p (st + 1 | s1,.... st, at) = p (st + 1 | st, with each trajectory in the state action space and a reward function R (st, at). A policy can be defined as the allocation of state to action distributions and the goal of an MDP to find the optimal policy."}, {"heading": "2.2 Evolution of RL", "text": "Early reinforcement learning (RL) algorithms for prediction and steering focused on the process of refining optimal policy assessment techniques and reducing the computational complexity of approaches. This led to the emergence of exploration and exploitation techniques, approaches at the political and non-political level, model-free and model-based, and various PAC (Probable approximate correct) methods. Although the algorithms were computationally feasible and converged to optimal strategies in polynomial time, they posed a major obstacle to the creation of strategies for high-dimensional control scenarios such as robotic manipulation. Two techniques stand out from the newly developed RL methods, namely functional approximation and policy search. The philosophy of these two approaches consists in parameterizing the action value function and policy function. Furthermore, the gradient of political value is considered to search for optimal net strategies, which is most appropriate for a global policy based on the expected actualization."}, {"heading": "2.3 RL for motor control", "text": "Most real-world tasks are considered episodic, and it is also difficult to define a precise reward function for a robotic task, which is addressed through the use of a technique called learning by demonstration or apprenticeship learning. [22] One of the methods of solving the uncertain reward problem is inverse reinforcement learning, in which the reward function is continuously updated and appropriate policies are generated at the end. Another effective method of modelling policy is the use of motor strategies to represent stochastic politics (at, t) inspired by the work of Kober and Peters, who developed an expectation-based maximization algorithm called Policy Learning by Weighing Exploration with Returns (PoWER)."}, {"heading": "2.4 RL for visuo-motor control", "text": "Many of the RL methods demonstrated on physical robotic systems used relatively low-dimensional political representations, typically with less than a hundred parameters, due to the difficulty of efficiently optimizing high-dimensional political parameter vectors. However, the work of Mnih et al. introduced an effective approach to combine major policy parameters by combining deep learning and enhanced learning [4]. This concept of generating efficient nonlinear representations is made possible by such an approach, which in turn learns the characteristics relevant to the specific task. One of the problems encountered in learning neural networks of strategies was the convergence of some weights to infinity when trained at similar distances from input observations [20]. Solving this difficulty by using methods to restore QPGs that fundamentally reinforced QP measures to control real life."}, {"heading": "3 DRL topology", "text": "The initial problem of planning in continuous and high-dimensional state spaces can be considered solved due to the extensive use of neural networks with a large number of parameters for functional / policy modeling. However, the problem we face today is the mapping of these continuous states to high-dimensional continuous action spaces. The current concentration in the DRL community on this issue and therefore it seems perfectly suitable to organize the different learning approaches on this basis. Furthermore, it also shows quite clearly the capabilities and limitations of the dominant algorithms. The methods are divided into two sections, namely discrete action spaces (DAS) approaches and continuous action spaces (CAS) approaches. Furthermore, CAS methods are divided into stochastic continuous action spaces (SCAS) and deterministic continuous action spaces (DCAS) methods, in which the various algorithms falling within the sphere of responsibility of the DAS fall deeply rooted approaches to normalized CAS, which fall back on CAS approaches directly."}, {"heading": "4 Discrete action space algorithms (DAS)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Deep Q-network (DQN)", "text": "The DQN architecture was the first successful integration of deep learning with Q-Learning Framework [4] Q-Learning forms the basis for most model-free RL algorithms. It involves exploring the environment with a behavioral policy and learning the Q function for the possible action-state pairs with the experience gained from exploration. The following equation describes Q-Learning, where \u03b1 is the learning rate and the observations achieved after exploration is a, r, s \u2032, where a the measures taken are, r is the rewards received and s \u2032 is the next observed state. Q (s, a) Sample = r (s, a) Sample =. \"maxaQ\" (s, a).maxaQ (s \u2032, a) Sample \"Q\" (s, a) Q \"(s, a) Q\" is the only difference between naive Q-Learning and DQN is the use of CNN as function approximators instead of linear approximators."}, {"heading": "4.2 Double Deep Q-networks", "text": "Double deep Q-networks are an improved version of DQN, first introduced by Hasselet et et al. [11]. In Q-learning and DQN, the max operator uses the same values for both behavioural policy and action assessment. This in turn led to overestimations. To mitigate this, DDQN uses the target as: yDQN = r + \u03b3Q (s \u2032, a \u2032 | \u03b8) | \u03b8 \u2212) (7)"}, {"heading": "4.3 Deep duelling network architecture", "text": "A model-free algorithm developed by Wang et al. [12] draws its inspiration from the remaining RL and the concept of Advantage Learning and Update by Baird [13]. In Advantage Learning, instead of estimating the action-value function, an advantage function is calculated, which is defined as the rate of increase of the amplification when a certain measure is taken. The most important meaning of Advantage Learning is that the advantage values have a higher variance, which leads to a simple convergence. Furthermore, the policy does not change discontinuously with changing values. The dual architecture receives both V (s) and A (s), a) with a single deep model and a simple output operation combining both results to get back the Q (s, a) value. Since the results are the same as DDQN and DQN, this network can be trained with any value sharing method. Considering the end, the Agent-Agent-Agent-Agent-Agent-Agent-evolving, the Agent-Agent-Agent-Agent-Agent, an Agent-Agent-A, and a stream-Agent-evolution, where a stream-V is described."}, {"heading": "5 Continuous action space algorithms (CAS)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Normalized advantage functions (NAF)", "text": "Gu et al. proposed a model-free approach that used Q-Learning to plan in continuous action spaces with deep neural networks, which they called Normalized Advance Functions (NAF).The idea behind NAF is to describe the Q function in such a way that its maximum, argmaxaQ (st, at), can be obtained easily and analytically during Q-Learning update.The inherent processes correspond to the duel network as a separate value function V (s | \u03b8) and the advantage term are estimated, but the difference is that in this case the advantage is parameterized as a quadratic function of nonlinear properties of the state: Q (s, a | sector) = A (s, a | equity) + V (s | equity)."}, {"heading": "5.2 Stochastic policy gradient", "text": "The central idea behind these algorithms is to adjust the parameters of politics in the direction of the gradient of performance, i.e., the basic theorem underlying these algorithms is the political gradient theorem [18]. The interesting aspect of this theorem is that although state distribution depends on network parameters, the political gradient does not depend on the gradient of distribution."}, {"heading": "5.3 Stochastic actor-critic methods", "text": "Actor-critic methods are widely used architectures, which in turn are based on the policy gradient theorem. As presented in the policy gradient equation, the term Q (s, a) is missing from the gradient and must be calculated. Therefore, the critic network estimates this Q (s, a) value in order to find the derivatives of the actor network."}, {"heading": "5.4 Trust Region Policy Optimization", "text": "TRPO is a policy optimization algorithm that limits the search space of the policy by applying constraints on the distribution of output policies by punishing the network parameters by means of a KL divergence loss function to the parameters DmaxKL (\u03b8old, \u03b8). Intuitively, this constraint does not allow for major changes in policy distribution and therefore helps the early convergence of the network.The figure above shows the networks used to control the Swimmerand Hopper tasks in MuJoCo environments. The input state consisted of common angles and robot kinematics and the rewards were linear functions."}, {"heading": "5.5 Deterministic policy gradient algorithm", "text": "The deterministic policy gradient algorithm (DPG) was derived from its counterpart, the stochastic policy gradient algorithm, and depends on a similar deterministic policy gradient theorem. In continuous spheres of action, greedy policy improvement becomes problematic and requires global optimization during the policy improvement phase [9]. Consequently, it is mathematically easier to update policy parameters towards the Q gradient."}, {"heading": "6 Discussion", "text": "It can be concluded that for the purposes of robotic manipulation, continuous domain-of-action algorithms are the most fruitful and applicable. Furthermore, there is a trend towards researching examples that are efficient and time-saving. One reason for this may be that complicated strategies require more samples to learn and even to have a sophisticated reward function. There is a need to learn highly complex reward functions in the field of robotics. Also, the approaches fail due to complex strategies that prove necessary. This observation underscores a void in the field of robotics. There is a need to learn highly complex reward functions and methods to represent highly skilled behaviors."}], "references": [{"title": "A Survey on Policy Search for Robotics", "author": ["M.P. Deisenroth", "G. Neumann", "J. Peters"], "venue": "Foundations and Trends in Robotics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Guided Policy Search", "author": ["S. Levine", "V. Koltun"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105)", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen"], "venue": "Nature, 518(7540),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "October. Neural fitted Q iterationfirst experiences with a data efficient neural reinforcement learning method", "author": ["M. Riedmiller"], "venue": "In European Conference on Machine Learning (pp", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Towards vision-based deep reinforcement learning for robotic motion control", "author": ["F. Zhang", "J. Leitner", "M. Milford", "B. Upcroft", "P. Corke"], "venue": "arXiv preprint arXiv:1511.03791", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Reinforcement learning: An introduction (Vol", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Trust region policy optimization. CoRR, abs/1502.05477", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Deterministic policy gradient algorithms", "author": ["G. Lever"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["S. Levine", "P. Abbeel"], "venue": "In Advances in Neural Information Processing Systems (pp. 1071-1079)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["H. Van Hasselt", "A. Guez", "D. Silver"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "N. de Freitas", "M. Lanctot"], "venue": "arXiv preprint arXiv:1511.06581", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Reinforcement learning through gradient descent (Doctoral dissertation, US Air Force Academy, US)", "author": ["III L.C. Baird"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Learning Hand- Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection", "author": ["S. Levine", "P. Pastor", "A. Krizhevsky", "D. Quillen"], "venue": "arXiv preprint arXiv:1603.02199", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Policy search for motor primitives in robotics. In Advances in neural information processing systems (pp. 849-856)", "author": ["J. Kober", "J.R. Peters"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Deep Reinforcement Learning for Robotic Manipulation", "author": ["S. Gu", "E. Holly", "T. Lillicrap", "S. Levine"], "venue": "arXiv preprint arXiv:1610.00633", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Continuous Deep Q- Learning with Model-based Acceleration", "author": ["S. Gu", "T. Lillicrap", "I. Sutskever", "S. Levine"], "venue": "arXiv preprint arXiv:1603.00748", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation", "author": ["R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Y. Mansour"], "venue": "In NIPS (Vol", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Reinforcement learning in robotics: A survey", "author": ["J. Kober", "J.A. Bagnell", "J. Peters"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv preprint arXiv:1509.02971", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng", "July"], "venue": "In Proceedings of the twenty-first international conference on Machine learning (p", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Intrinsically motivated reinforcement learning", "author": ["N. Chentanez", "A.G. Barto", "S.P. Singh"], "venue": "In Advances in neural information processing systems (pp. 1281-1288)", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}], "referenceMentions": [{"referenceID": 18, "context": "Other than the nature derived inspiration, several successful implementations of reinforcement learning (RL) in controlling dynamic robotic systems for manipulation, locomotion and autonomous driving [19], [1], [15] have proven the previously theoretical concept to be applicable in real time control of physical systems.", "startOffset": 200, "endOffset": 204}, {"referenceID": 0, "context": "Other than the nature derived inspiration, several successful implementations of reinforcement learning (RL) in controlling dynamic robotic systems for manipulation, locomotion and autonomous driving [19], [1], [15] have proven the previously theoretical concept to be applicable in real time control of physical systems.", "startOffset": 206, "endOffset": 209}, {"referenceID": 14, "context": "Other than the nature derived inspiration, several successful implementations of reinforcement learning (RL) in controlling dynamic robotic systems for manipulation, locomotion and autonomous driving [19], [1], [15] have proven the previously theoretical concept to be applicable in real time control of physical systems.", "startOffset": 211, "endOffset": 215}, {"referenceID": 1, "context": "Though efficient there is a loss of generality in adopting such an approach as it constricts the policy space to some specific trajectories [2].", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "Another important development in the field of RL has been indirectly borrowed from enormous successes of deep convolutional neural networks(CNN) [3] in image feature extraction.", "startOffset": 145, "endOffset": 148}, {"referenceID": 4, "context": "The paper by Riedmiller [5] demonstrated that neural networks can effectively be used as q-function approximators using neural fitted q-iteration algorithm.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "[4] turned neural networks based q learning as a base for DRL.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Attempts have been made to use deep q-learning (DQN) for high dimensional robotics tasks but with a very little success [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 6, "context": "The parameters can then be perturbed in order to optimize the performance output [7].", "startOffset": 81, "endOffset": 84}, {"referenceID": 7, "context": "Thus, the policy representation provides probabilities over over action in a continuous space [8].", "startOffset": 94, "endOffset": 97}, {"referenceID": 20, "context": "Initially developed and experimented on low dimensional state spaces, CAS algorithms have been integrated into CNN architecture in algorithms like deep deterministic policy gradients (DDPG) [21].", "startOffset": 190, "endOffset": 194}, {"referenceID": 7, "context": "Even though stochastic policy gradient methods provide a better coverage of the policy search space, they require a large number of training samples in order to learn the policy effectively [8].", "startOffset": 190, "endOffset": 193}, {"referenceID": 8, "context": "But, the discovery of deterministic policy gradients has led to an easier method whose performance surpasses stochastic policy algorithms as proven empirically by Silver et al [9].", "startOffset": 176, "endOffset": 179}, {"referenceID": 18, "context": "Application of RL in robotics have included locomotion, manipulation and autonomous vehicle control [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "This problem is tackled by the use of a technique called learning by demonstration or apprenticeship learning [22].", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "One of the methods to solve the uncertain reward problem is inverse reinforcement learning where the reward function is updated continuously and an appropriate policy is generated in the end Another effective method to model the policies is the use of motor policies to represent stochastic policy \u03c0(at|st, t), that is inspired from the works of Kober and Peters [15].", "startOffset": 363, "endOffset": 367}, {"referenceID": 9, "context": "Certain other approaches like guided policy search [10] also introduced more versatile policy representations like differential dynamic programming (DDP).", "startOffset": 51, "endOffset": 55}, {"referenceID": 3, "context": "introduced an effective approach to combine larger policy parameterizations by combining deep learning and reinforcement learning [4].", "startOffset": 130, "endOffset": 133}, {"referenceID": 13, "context": "[14] and Kober et al [?].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "One of the problems that was encountered with neural network learning of policies was the convergence of some weights to infinity when trained with similar instances of input observations [20].", "startOffset": 188, "endOffset": 192}, {"referenceID": 15, "context": "The current state of the art in deep-reinforcement learning includes the algorithms employed by google deepmind research namely DQN (Deep Q network) for discrete actions and Deep deterministic policy gradients (DDPG) for continuous action spaces [16].", "startOffset": 246, "endOffset": 250}, {"referenceID": 20, "context": "Efficacy of both of these methods have been demonstrated empirically for performing complex robotic manipulation tasks like door opening and ball catching [21].", "startOffset": 155, "endOffset": 159}, {"referenceID": 3, "context": "The DQN architecture was the first successful integration of deep learning with Q-learning framework [4].", "startOffset": 101, "endOffset": 104}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] draws its inspiration from residual RL and the concept of Advantage learning and updating by Baird [13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12] draws its inspiration from residual RL and the concept of Advantage learning and updating by Baird [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "Duelling networks performed 75% better than the naive Q-networks as reported in the paper [12].", "startOffset": 90, "endOffset": 94}, {"referenceID": 16, "context": "Policies learnt with this method showed more precise completion of tasks as compared to deep policy gradient methods [17].", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "The fundamental theorem underpinning these algorithms is the policygradienttheorm [18].", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "[14] for autonomous grasping of objects in cluttered", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In continuous action spaces, greedy policy improvement becomes problematic and needs global optimization during policy improvement step [9].", "startOffset": 136, "endOffset": 139}, {"referenceID": 15, "context": "Methods such as NAF and DDPG have been used for learning complex robotic manipulation tasks in real time [16].", "startOffset": 105, "endOffset": 109}, {"referenceID": 22, "context": "There have been several works in incorporating intrinsic motivation in reinforcement learning as a method to induce temporal abstractions in agents [23].", "startOffset": 148, "endOffset": 152}], "year": 2017, "abstractText": "The focus of this work is to enumerate the various approaches and algorithms that center around application of reinforcement learning in robotic manipulation tasks. Earlier methods utilized specialized policy representations and human demonstrations to constrict the policy. Such methods worked well with continuous state and policy space of robots but failed to come up with generalized policies. Subsequently, high dimensional non-linear function approximators like neural networks have been used to learn policies from scratch. Several novel and recent approaches have also embedded control policy with efficient perceptual representation using deep learning. This has led to the emergence of a new branch of dynamic robot control system called deep reinforcement learning(DRL). This work embodies a survey of the most recent algorithms, architectures and their implementations in simulations and real world robotic platforms. The gamut of DRL architectures are partitioned into two different branches namely, discrete action space algorithms(DAS) and continuous action space algorithms(CAS). Further, the CAS algorithms are divided into stochastic continuous action space(SCAS) and deterministic continuous action space(DCAS) algorithms. Along with elucidating an organisation of the DRL algorithms this work also manifests some of the state of the art applications of these approaches in robotic manipulation tasks.", "creator": "LaTeX with hyperref package"}}}