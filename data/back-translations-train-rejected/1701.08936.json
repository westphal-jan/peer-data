{"id": "1701.08936", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2017", "title": "Deep Reinforcement Learning for Visual Object Tracking in Videos", "abstract": "Convolutional neural network (CNN) models have achieved tremendous success in many visual detection and recognition tasks. Unfortunately, visual tracking, a fundamental computer vision problem, is not handled well using the existing CNN models, because most object trackers implemented with CNN do not effectively leverage temporal and contextual information among consecutive frames. Recurrent neural network (RNN) models, on the other hand, are often used to process text and voice data due to their ability to learn intrinsic representations of sequential and temporal data. Here, we propose a novel neural network tracking model that is capable of integrating information over time and tracking a selected target in video. It comprises three components: a CNN extracting best tracking features in each video frame, an RNN constructing video memory state, and a reinforcement learning (RL) agent making target location decisions. The tracking problem is formulated as a decision-making process, and our model can be trained with RL algorithms to learn good tracking policies that pay attention to continuous, inter-frame correlation and maximize tracking performance in the long run. We compare our model with an existing neural-network based tracking method and show that the proposed tracking approach works well in various scenarios by performing rigorous validation experiments on artificial video sequences with ground truth. To the best of our knowledge, our tracker is the first neural-network tracker that combines convolutional and recurrent networks with RL algorithms.", "histories": [["v1", "Tue, 31 Jan 2017 07:48:56 GMT  (5251kb,D)", "http://arxiv.org/abs/1701.08936v1", null], ["v2", "Mon, 10 Apr 2017 20:34:43 GMT  (3619kb,D)", "http://arxiv.org/abs/1701.08936v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["da zhang", "hamid maei", "xin wang", "yuan-fang wang"], "accepted": false, "id": "1701.08936"}, "pdf": {"name": "1701.08936.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning for Visual Object Tracking in Videos", "authors": ["Da Zhang", "Hamid Maei", "Xin Wang", "Yuan-Fang Wang"], "emails": ["yfwang}@cs.ucsb.edu", "hamid.maei@samsung.com"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own, without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process, a process in which there is a process, a process in which there is a process, a process in which there is a process, a process in which there is a process, a process in which there is a process in which there is a process, a process in which there is a process, a process, a process, a process, a process, a process and a process in which there is a process, a process, a process, a process, a process, a process, a process, a process and a process, a process, a process, a process, a process, a process, a process and a process, and a process."}, {"heading": "2. Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Visual Tracking Algorithms", "text": "For a systematic review and comparison, we refer readers to a current benchmark and a Tracking Challenge Report [43, 21]. Generally, most existing trackers fall into either one of two categories: generative trackers and discriminatory trackers. Generative methods describe the appearance of the target objects using generative models and the search for image regions that best fit the model. Some representative methods are based on the main component analysis [32] and sparse representation [26, 44]. On the other hand, discriminatory trackers learn to separate foreground from background using a classifier. Many advanced machine learning algorithms have been used, including online boosting [11, 12], structured edition SVM [14] and multiple learning [2]. However, almost all existing, appearance-based tracking methods are based on low, handcrafted features that are unable to capture setical information from the target objects, and not only the appearance, but also the appearance."}, {"heading": "2.2. Convolutional Neural Networks", "text": "CNNs have demonstrated their outstanding representational power in a wide range of computer image applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19]. Krizhevsky et al. [22] have achieved a significant performance improvement in image classification by training a deep CNN on ImageNet datasets [6]. CNN [10] has applied a CNN to an object recognition task that can accurately locate and classify objects in natural images. Despite the impressive success of CNNs that use representations of CNNs based on stills, the application of Deep Neural Network (DNN) models in visual tracking has not yet been fully explored. Although DNN trackers have improved the state of the art, so far only a limited number of tracking algorithms have been proposed that use representations of CNNs based on a slow-based approach [9, 18, 24, 41, 40, 16, 29] an early tracking algorithm has not applied N41 as a learning algorithm based on a large-scale data set, although CNN has been specifically classified on a large-scale data set."}, {"heading": "2.3. Recurrent Neural Networks", "text": "Advances in understanding the learning dynamics of RNNs have enabled their successful application in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27. The standard RNN cell consists of an input, a hidden layer, and an output layer, as shown in Figure 2. In addition to traditional RNN, the use of recurring networks with sophisticated hidden units such as Long Short-Term Memory (LSTM) [17] or Gated Recurrent Unit (GRU) [5] has become common in recent years. Mnih et al. [27] applied LSTM to visual attention problems by training a decision network that sufficiently improves computing efficiency for computer visual tasks. In recent work, Donahue et al. [8] proposed a class of recurring revolutionary architectures suitable for large-scale visual tasks, demonstrating the value of these models for recognizing cells that are used in the description of the STM and inscription in the STM."}, {"heading": "2.4. Deep Reinforcement Learning", "text": "The underlying model that RL learns is a Markov Decision Process (MDP): An agent interacts with the environment and selects an action. Applying the action in a single state, and the environment sends out a new state and a reward signal. To maximize the expected rewards in the long run, the agent learns the best strategy to take action [36]. Some work in the computer vision literature and elsewhere, e.g. [1, 3, 7, 31, 28, 27], has adopted vision as a sequential decision task. RL is able to solve sequential decision tasks in particular with the help of good environmental representations learned through DNN models. Some recent work has included training an RL agent to play Atari games from raw input pixels, 31, 28] and a recurring visual attention model [27] to reduce compilation for computer tasks. Our work is similar to the problem that is directly described in the system architecture, whereby RL focuses attention on the RL method."}, {"heading": "3. Deep RL Agent", "text": "In this paper, we formulate the tracking problem as a sequential decision-making process of a targeted agent that interacts with the visual environment: As shown in Figure 3, the visual environment consists of a sequence of video images, the agent is modelled as our neural network, and the decision or action is to determine the location of the target object. At any time, the agent observes the environment and extracts representative characteristics from observations. Since the agent is only looking at one image at a time, he must integrate information over time to determine how he can act most effectively. At each step, the agent receives a scalar reward that depends on his actions, and the agent's goal is to maximize the overall long-term rewards. In our system, the agent builds on an RNN as shown in Figure 1. At each time step, he processes the input image, integrates information over time, and chooses how the proposed Total Agent RNL consists of three components:"}, {"heading": "3.1. Feature Extractor", "text": "This representation can be as simple as a linear transformation or a more complex feature calculated by a CNN, and is used by other components to integrate sequential information and take appropriate action; the extracted features are referred to as: it = fi (xt; Wi) (1), which is a function of the preset xt parameters Wi, and the functional relationship through fi.The Feature Extractor is applied to each individual frame in a video, so the Feature Extractor must be general enough to capture representative features for different objects in different contexts; given the different tracking tasks, the Feature Extractor can either be pre-trained, transferred or initialized by other models, and re-trained from scratch. To make the generic extractor adaptable over time, its Wi parameters are further updated during the end-to-end training, providing better descriptors to maximize overall rewards."}, {"heading": "3.2. Memory Component", "text": "The memory component maintains an internal memory state that summarizes information extracted from the history of past observations; this hidden state encodes the knowledge of the environment and is fundamental to the agent's actions. In a single step t, the feature vector of the input frame is fed into an RNN. The recursive neural network updates its internal memory vector based on the previous memory vector ht \u2212 1 and the current input feature provided by the feature extractor: ht = fh (ht \u2212 1, it; Wh) (2), where fh is a recursive transformation function such as GRU, LSTM or a simple logistic function provided by the Wh.We parameter, using the LSTM cell in Figure 2, which has proven to be better at handling long-term and short-term connections and is robust for processing various sequential data."}, {"heading": "3.3. RL Unit", "text": "The instrumental component is an RL unit that takes action based on the internal memory state and receives rewards from the environment. At each time step t, the agent performs an action to locate the target lt. In this work, the location measures are stochastically selected based on a distribution parameter defined by the RL unit fl (ht; Wl), where p is the probability distribution function determined by fl (ht; Wl), the external input is the internal memory state ht and fl is the functional relationship defined by the RL unit. More precisely, the distribution p (\u00b7 fl (ht; Wl)) is defined as a multivariable Gaussian definition. The variance of the Gaussian state is fixed and determined by our experiments, while the mean \u00b5 = fl (ht; Wl) is the output of the RL unit."}, {"heading": "4. Training the Deep RL Agent", "text": "Training this network to maximize overall tracking performance is not a trivial task, and we use the REINFORCE algorithm [42] from the RL community to solve this problem."}, {"heading": "4.1. Gradient Approximation", "text": "Our deep RL agent is parameterized by W = {Wi, Wh, Wl} and we strive to learn these parameters in order to maximize the overall tracking reward that the agent can expect when taking various actions. Specifically, the agent's goal is to have a political function \u03c0 (lt.: z1: t; W) with parameters W, which at each step t, tell the history of past interactions with the environment z1: t = x1, l1, xt (a sequence of past observations and actions taken by the agent) lead to a distribution of actions for the current time step. Here, the policy \u03c0 is defined by our neural network architecture, and the history of interactions z1: t is summarized in the internal storage state. For simplicity, we will use Zt = z1: t to show all gradients up to time, so the political function can be written as zip (lt.)."}, {"heading": "4.2. Training with Backpropagation", "text": "The only remaining part to calculate the gradient in Eq.8 is the calculation of the gradient via the log probability of the policy function. To simplify the notation, we focus on a single time step, leaving out the usual index subscription of the unit. In our network design, the policy function \u03c0 prints the target position l, which comes from a Gaussian distribution centered on \u00b5 with a fixed variance distribution, and \u00b5 is the output of the deep RL agent parameterized by W. The density function g, which determines the output l in each individual study, is given as follows: g (l, \u00b5, \u03b4) = 1 (2\u03c0) 1 2\u03c3 e \u2212 (l \u2212 \u00b5) 22\u04452 (9) Based on the REINFORCE algorithm [42], the gradient of the policy function with respect to \u00b5 is given by the gradient of the density function:"}, {"heading": "4.3. Overall Procedure", "text": "The general approach of our training algorithm is outlined in Algorithm 1. Initially, network parameters W are randomly initialized to define our initial approach, then we take the first T-frames from a training video as input to our network. We execute current N-times, calculate gradients, and update network parameters. Next, we take consecutive T-frames from the same video and apply the same training procedure. We repeat this for all training videos in our dataset, and stop when we reach the maximum number of epochs or the cumulative reward no longer increases. During the test, network parameters W are set and no online fine-tuning is required. The test time procedure is as simple as calculating the run rate of our deep RL agent, i.e. in the face of a test video, the deep RL agent predicts the position of the target in each frame by processing the video sequence."}, {"heading": "5. Experiments", "text": "We examined the proposed approach of visual object tracking using artificially created datasets. We varied the algorithm 1 Deep RL Agent Training Algorithm Input: Training videos {v1,..., vM} with Ground-Truth Output: Network weights W1: Randomly initialize weights Wi, Wh and Wl 2: Start from the first frame in training dataset 3: repeat 4: Sequentially select T-frames {x1,..., xT} 5: Extract characteristics {i1,..., iT} 6: Generate internal memory states {h1,..., hT} 7: Calculate network output {\u00b51,..., \u00b5T} 8: Random exemplary predictions for N episodes {l1: N1,..., l1: NT} according to Equation 9: Calculate rewards {r1: N1: N1, NT}, NT = 1, NT = 1, NT = 1, NT = 1, NT = 1, NT = 1, NT = 12, NT = 1, NT = 1, NT = 1, NT = 1, NT = 1, NT = 1, NT = 1: 1, NT = 1, NT = 1, NT = 1: 1, NT = 1, NT = 1, NT = 1: 1, NT = 1, NT = 1: 1, NT = 1, NT = 1: 1, NT = 1, NT = 1: 1, NT = 1, NT = 1: 1, NT = 1: 1, NT = 1, NT = 1: 1, NT = 1: 1, NT = 1: 1, NT = 1, NT = 1: 1, NT = 1, 1: 1: 1, NT = 1: 1, NT = 1, 1: 1: 1, NT = 1, NT = 1, 1: 1: 1: 1, 1, 1: 1, 1: 1: 1: 1, NT = 1, NT = 1, 1, 1: 1: 1, 1: 1: 1: 1: 1: 1: 1: 1,"}, {"heading": "5.1. Evaluation Metrics", "text": "We used two performance metrics to quantitatively evaluate our tracking model based on test data: The first was the average central pixel error = \u221a (xpred \u2212 xgt) 2 + (ypred \u2212 ygt) 2 (11), where (xgt, ygt) and (xpred, ypred) were the basic truth and the predicted central pixel coordinate in a 2D image, and the error was calculated as the pixel pitch between these two points. Here, we used the average intersection-over-union (IoU) IoU = | bpred-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-b12 intersection average prediction (IoU) IoU = | bpred-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt (bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-and-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt and-bgt-bgt-bgt-bgt-bgt-bgt-bgt-bgt and-bgt-bgt-bgt-bgt-bgt-bgt-bgt and-bgt-bgt-bgt-bgt-bgt-bgt-b"}, {"heading": "5.2. Implementation Details", "text": "We created videos from MNIST images of handwritten digits [23] by inserting randomly drawn digits into a larger screen area of 100 x 100 pixels and moving the digits from one image to the next. We respected the same data distribution for training and testing as in the original MNIST dataset, i.e., the digits were pulled from the training part to generate the training sequences, and from the test splitter to generate the test sequences. Our tracker performed well in challenging scenarios and achieved better results compared to [20] in the same experimental settings. The parts of CNN, RNN, and RL were all implemented with the TensorFlow toolbox1. We first described the design decisions common to all our experiments: Feature Extractor: To capture a robust representation against small deviations in the MNIST dataset, the NIST Extraction Unit was used as a CNN Extrache."}, {"heading": "5.3. Evaluation on Moving MNIST", "text": "In this experiment, we created videos that were randomly assigned a single 28-page MNIST. The data sets consisted of 500 training sequences and 100 test sequences. Each test sequence contained 100 frames, while each training sequence contained only 20 frames. The training algorithm was the same as the algorithm. We used T = 10 and N = 5 as it provided the best tracking performance."}, {"heading": "5.4. Discussion", "text": "We show that the proposed tracking approach works well in three different scenarios by conducting rigorous validation experiments on artificial video sequences with the basic truth. We found in our experiments that the proposed framework has the following characteristics: \u2022 Generalizability: In all experiments, we evaluate a trained model based on test sequences that are longer than the training sequences, and our model has been shown to be able to generalize to longer test sequences. This is not only because we use RNN in our design, but also because of the RL algorithm that allows us to consider long-term rewards. \u2022 Adaptability: By formulating tracking as a decision-making process, the number of neurons in the output layer depends entirely on the tracking complexity. We have shown that our model can achieve reasonable performance by only changing the architecture of the last shift and leaving other components untouched. This is a minimal feature of our overall machine learning that can be applied to different tasks."}, {"heading": "6. Conclusion", "text": "To the best of our knowledge, we are the first to bring RL to CNN and RNN to solve visual tracking issues, the entire network is endlessly trainable offline, and the entire pipeline is jointly optimized to maximize tracking quality.The deep RL algorithm directly optimizes long-term tracking performance, which depends on the overall tracking video sequence.The proposed RL agent maps raw image pixels for target location decisions, and the spatial temporal representations learned through our model are general and robust to accommodate different tracking scenarios. Extensive experiments have confirmed the applicability of our proposed tracker."}], "references": [{"title": "Searching for objects driven by context", "author": ["B. Alexe", "N. Heess", "Y.W. Teh", "V. Ferrari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Robust object tracking with online multiple instance learning", "author": ["B. Babenko", "M.-H. Yang", "S. Belongie"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Optimal scanning for faster object detection", "author": ["N.J. Butko", "J.R. Movellan"], "venue": "In Computer vision and pattern recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1405.3531,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["M. Denil", "L. Bazzani", "H. Larochelle", "N. de Freitas"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Human tracking using convolutional neural networks", "author": ["J. Fan", "W. Xu", "Y. Wu", "Y. Gong"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Real-time tracking via on-line boosting", "author": ["H. Grabner", "M. Grabner", "H. Bischof"], "venue": "In BMVC,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Semi-supervised on-line boosting for robust tracking", "author": ["H. Grabner", "C. Leistner", "H. Bischof"], "venue": "In European conference on computer vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "IEEE international conference on acoustics, speech and signal processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Struck: Structured output tracking with kernels", "author": ["S. Hare", "A. Saffari", "P.H. Torr"], "venue": "In 2011 International Conference on Computer Vision,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Learning to track at 100 fps with deep regression networks", "author": ["D. Held", "S. Thrun", "S. Savarese"], "venue": "arXiv preprint arXiv:1604.01802,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1997}, {"title": "Online tracking by learning discriminative saliency map with convolutional neural network", "author": ["S. Hong", "T. You", "S. Kwak", "B. Han"], "venue": "arXiv preprint arXiv:1502.06796,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Perceptual losses for real-time style transfer and super-resolution", "author": ["J. Johnson", "A. Alahi", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1603.08155,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Ratm: Recurrent attentive tracking model", "author": ["S.E. Kahou", "V. Michalski", "R. Memisevic"], "venue": "arXiv preprint arXiv:1510.08660,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "The visual object tracking vot2015 challenge results", "author": ["M. Kristan", "J. Matas", "A. Leonardis", "M. Felsberg", "L. Cehovin", "G. Fernandez", "T. Vojir", "G. Hager", "G. Nebehay", "R. Pflugfelder"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision Workshops,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Robust visual tracking", "author": ["X. Mei", "H. Ling"], "venue": "IEEE 12th International Conference on Computer Vision,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Learning multi-domain convolutional neural networks for visual tracking", "author": ["H. Nam", "B. Han"], "venue": "arXiv preprint arXiv:1510.07945,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML (3),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "On learning where to look", "author": ["M. Ranzato"], "venue": "arXiv preprint arXiv:1405.5488,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Incremental learning for robust visual tracking", "author": ["D.A. Ross", "J. Lim", "R.-S. Lin", "M.-H. Yang"], "venue": "International Journal of Computer Vision,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "CoRR, abs/1502.04681,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Deeppose: Human pose estimation via deep neural networks", "author": ["A. Toshev", "C. Szegedy"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Visual tracking with fully convolutional networks", "author": ["L. Wang", "W. Ouyang", "X. Wang", "H. Lu"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Transferring rich feature hierarchies for robust visual tracking", "author": ["N. Wang", "S. Li", "A. Gupta", "D.-Y. Yeung"], "venue": "arXiv preprint arXiv:1501.04587,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1992}, {"title": "Online object tracking: A benchmark", "author": ["Y. Wu", "J. Lim", "M.-H. Yang"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Robust visual tracking via multi-task sparse learning", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "N. Ahuja"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 151, "endOffset": 163}, {"referenceID": 14, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 151, "endOffset": 163}, {"referenceID": 31, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 151, "endOffset": 163}, {"referenceID": 9, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 182, "endOffset": 186}, {"referenceID": 23, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 210, "endOffset": 214}, {"referenceID": 3, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 232, "endOffset": 247}, {"referenceID": 34, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 232, "endOffset": 247}, {"referenceID": 35, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 232, "endOffset": 247}, {"referenceID": 18, "context": "CNNs have recently had great success in significantly advancing the state-of-the-art on challenging computer vision tasks such as image classification [22, 15, 33], object detection [10], semantic segmentation [25], and many others [4, 38, 39, 19].", "startOffset": 232, "endOffset": 247}, {"referenceID": 27, "context": "Recently, some initial works have been done in using neural networks for visual tracking, and these efforts have produced some impact and improved state-of-the-art accuracy [29, 41, 40].", "startOffset": 173, "endOffset": 185}, {"referenceID": 37, "context": "Recently, some initial works have been done in using neural networks for visual tracking, and these efforts have produced some impact and improved state-of-the-art accuracy [29, 41, 40].", "startOffset": 173, "endOffset": 185}, {"referenceID": 36, "context": "Recently, some initial works have been done in using neural networks for visual tracking, and these efforts have produced some impact and improved state-of-the-art accuracy [29, 41, 40].", "startOffset": 173, "endOffset": 185}, {"referenceID": 38, "context": "This procedure uses backpropagation to train the neural-network components and REINFORCE algorithm [42] to train the policy network.", "startOffset": 99, "endOffset": 103}, {"referenceID": 39, "context": "For a systematic review and comparison, we refer the readers to a recent benchmark and a tracking challenge report [43, 21].", "startOffset": 115, "endOffset": 123}, {"referenceID": 20, "context": "For a systematic review and comparison, we refer the readers to a recent benchmark and a tracking challenge report [43, 21].", "startOffset": 115, "endOffset": 123}, {"referenceID": 30, "context": "Some representative methods are based on principal component analysis [32], and sparse representation [26, 44].", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "Some representative methods are based on principal component analysis [32], and sparse representation [26, 44].", "startOffset": 102, "endOffset": 110}, {"referenceID": 40, "context": "Some representative methods are based on principal component analysis [32], and sparse representation [26, 44].", "startOffset": 102, "endOffset": 110}, {"referenceID": 10, "context": "Many advanced machine learning algorithms have been used, including online boosting [11, 12], structured output SVM [14], and multiple-", "startOffset": 84, "endOffset": 92}, {"referenceID": 11, "context": "Many advanced machine learning algorithms have been used, including online boosting [11, 12], structured output SVM [14], and multiple-", "startOffset": 84, "endOffset": 92}, {"referenceID": 13, "context": "Many advanced machine learning algorithms have been used, including online boosting [11, 12], structured output SVM [14], and multiple-", "startOffset": 116, "endOffset": 120}, {"referenceID": 1, "context": "instance learning [2].", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 9, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 21, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 23, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 31, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 34, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 35, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 14, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 27, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 18, "context": "CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications [4, 10, 22, 25, 33, 38, 39, 15, 29, 19].", "startOffset": 110, "endOffset": 149}, {"referenceID": 21, "context": "[22] brought significant performance improvement in image classification by training a deep CNN on ImageNet dataset [6].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[22] brought significant performance improvement in image classification by training a deep CNN on ImageNet dataset [6].", "startOffset": 116, "endOffset": 119}, {"referenceID": 9, "context": "Region CNN [10] applied a CNN to an object detection task, which can accurately locate and classify objects in natural images.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "Although DNN trackers have improved the state-of-the-art, only a limited number of tracking algorithms using representations from CNNs have been proposed so far [9, 18, 24, 41, 40, 16, 29].", "startOffset": 161, "endOffset": 188}, {"referenceID": 17, "context": "Although DNN trackers have improved the state-of-the-art, only a limited number of tracking algorithms using representations from CNNs have been proposed so far [9, 18, 24, 41, 40, 16, 29].", "startOffset": 161, "endOffset": 188}, {"referenceID": 37, "context": "Although DNN trackers have improved the state-of-the-art, only a limited number of tracking algorithms using representations from CNNs have been proposed so far [9, 18, 24, 41, 40, 16, 29].", "startOffset": 161, "endOffset": 188}, {"referenceID": 36, "context": "Although DNN trackers have improved the state-of-the-art, only a limited number of tracking algorithms using representations from CNNs have been proposed so far [9, 18, 24, 41, 40, 16, 29].", "startOffset": 161, "endOffset": 188}, {"referenceID": 15, "context": "Although DNN trackers have improved the state-of-the-art, only a limited number of tracking algorithms using representations from CNNs have been proposed so far [9, 18, 24, 41, 40, 16, 29].", "startOffset": 161, "endOffset": 188}, {"referenceID": 27, "context": "Although DNN trackers have improved the state-of-the-art, only a limited number of tracking algorithms using representations from CNNs have been proposed so far [9, 18, 24, 41, 40, 16, 29].", "startOffset": 161, "endOffset": 188}, {"referenceID": 37, "context": "An early tracking algorithm transferred CNNs pretrained on a large-scale dataset constructed for image classification [41].", "startOffset": 118, "endOffset": 122}, {"referenceID": 27, "context": "A recent approach [29] trained a multi-domain neural network and achieved great performance improvement.", "startOffset": 18, "endOffset": 22}, {"referenceID": 16, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 28, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 12, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 33, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 4, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 32, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 19, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 7, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 25, "context": "Advances in understanding the learning dynamics of RNNs have enabled their successful applications in a wide range of tasks [17, 30, 13, 35, 5, 34, 20, 8, 27].", "startOffset": 124, "endOffset": 158}, {"referenceID": 16, "context": "Besides traditional RNN, the application of recurrent networks with sophisticated hidden units, such as Long Short-Term Memory (LSTM) [17] or Gated Recurrent Unit (GRU) [5], has become common in recent years.", "startOffset": 134, "endOffset": 138}, {"referenceID": 4, "context": "Besides traditional RNN, the application of recurrent networks with sophisticated hidden units, such as Long Short-Term Memory (LSTM) [17] or Gated Recurrent Unit (GRU) [5], has become common in recent years.", "startOffset": 169, "endOffset": 172}, {"referenceID": 25, "context": "[27] applied LSTM in visual attention problems by training a decision making network, which sufficiently improves computational efficiency for computer vision tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] proposed a class of recurrent convolutional architectures which are suitable for ReLU RNN Cell", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": ", [1, 3, 7, 31, 28, 27] have embraced vision as a sequential decision task.", "startOffset": 2, "endOffset": 23}, {"referenceID": 2, "context": ", [1, 3, 7, 31, 28, 27] have embraced vision as a sequential decision task.", "startOffset": 2, "endOffset": 23}, {"referenceID": 6, "context": ", [1, 3, 7, 31, 28, 27] have embraced vision as a sequential decision task.", "startOffset": 2, "endOffset": 23}, {"referenceID": 29, "context": ", [1, 3, 7, 31, 28, 27] have embraced vision as a sequential decision task.", "startOffset": 2, "endOffset": 23}, {"referenceID": 26, "context": ", [1, 3, 7, 31, 28, 27] have embraced vision as a sequential decision task.", "startOffset": 2, "endOffset": 23}, {"referenceID": 25, "context": ", [1, 3, 7, 31, 28, 27] have embraced vision as a sequential decision task.", "startOffset": 2, "endOffset": 23}, {"referenceID": 26, "context": "Some recent works included training an RL agent to play Atari games from raw input pixels [28], and a recurrent visual attention model [27] to reduce computation for computer vision tasks.", "startOffset": 90, "endOffset": 94}, {"referenceID": 25, "context": "Some recent works included training an RL agent to play Atari games from raw input pixels [28], and a recurrent visual attention model [27] to reduce computation for computer vision tasks.", "startOffset": 135, "endOffset": 139}, {"referenceID": 25, "context": "Our work is similar to the attention model described in [27], but we designed our own network architecture especially for solving the visual tracking problem by combining CNN, RNN and RL algorithms.", "startOffset": 56, "endOffset": 60}, {"referenceID": 38, "context": "Training this network to maximize the overall tracking performance is a non-trivial task, and we leverage the REINFORCE algorithm [42] from the RL community to solve this problem.", "startOffset": 130, "endOffset": 134}, {"referenceID": 38, "context": "Here, we bring techniques from the RL community to solve this problem, as shown in [42], the gradient can be first simplified by taking the derivative over log-probability of the policy function \u03c0:", "startOffset": 83, "endOffset": 87}, {"referenceID": 38, "context": "The above training rule is known as the episodic REINFORCE [42] algorithm, and it involves running the deep RL agent with its current policy to obtain samples of interactions and then updating parameters W of the agent such that the log-probability of chosen actions that have led to high overall rewards is increased.", "startOffset": 59, "endOffset": 63}, {"referenceID": 38, "context": "Based on REINFORCE algorithm [42], the gradient of the policy function with respect to \u03bc is given by the gradient of the density function:", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "We generated videos from MNIST images of handwritten digits [23] by placing randomly-drawn digits in a larger", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Single-digit in RATM [20] 63.", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "Our tracker demonstrated good performance in challenging scenarios and achieved better results comparing with [20] in the same experimental settings.", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "The LSTM structure was similar to the one proposed in [17] with all gated controls except that", "startOffset": 54, "endOffset": 58}], "year": 2017, "abstractText": "Convolutional neural network (CNN) models have achieved tremendous success in many visual detection and recognition tasks. Unfortunately, visual tracking, a fundamental computer vision problem, is not handled well using the existing CNN models, because most object trackers implemented with CNN do not effectively leverage temporal and contextual information among consecutive frames. Recurrent neural network (RNN) models, on the other hand, are often used to process text and voice data due to their ability to learn intrinsic representations of sequential and temporal data. Here, we propose a novel neural network tracking model that is capable of integrating information over time and tracking a selected target in video. It comprises three components: a CNN extracting best tracking features in each video frame, an RNN constructing video memory state, and a reinforcement learning (RL) agent making target location decisions. The tracking problem is formulated as a decision-making process, and our model can be trained with RL algorithms to learn good tracking policies that pay attention to continuous, inter-frame correlation and maximize tracking performance in the long run. We compare our model with an existing neural-network based tracking method and show that the proposed tracking approach works well in various scenarios by performing rigorous validation experiments on artificial video sequences with ground truth. To the best of our knowledge, our tracker is the first neural-network tracker that combines convolutional and recurrent networks with RL algorithms.", "creator": "LaTeX with hyperref package"}}}