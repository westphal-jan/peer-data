{"id": "1701.08939", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2017", "title": "Deep Submodular Functions", "abstract": "We start with an overview of a class of submodular functions called SCMMs (sums of concave composed with non-negative modular functions plus a final arbitrary modular). We then define a new class of submodular functions we call {\\em deep submodular functions} or DSFs. We show that DSFs are a flexible parametric family of submodular functions that share many of the properties and advantages of deep neural networks (DNNs). DSFs can be motivated by considering a hierarchy of descriptive concepts over ground elements and where one wishes to allow submodular interaction throughout this hierarchy. Results in this paper show that DSFs constitute a strictly larger class of submodular functions than SCMMs. We show that, for any integer $k&gt;0$, there are $k$-layer DSFs that cannot be represented by a $k'$-layer DSF for any $k'&lt;k$. This implies that, like DNNs, there is a utility to depth, but unlike DNNs, the family of DSFs strictly increase with depth. Despite this, we show (using a \"backpropagation\" like method) that DSFs, even with arbitrarily large $k$, do not comprise all submodular functions. In offering the above results, we also define the notion of an antitone superdifferential of a concave function and show how this relates to submodular functions (in general), DSFs (in particular), negative second-order partial derivatives, continuous submodularity, and concave extensions. To further motivate our analysis, we provide various special case results from matroid theory, comparing DSFs with forms of matroid rank, in particular the laminar matroid. Lastly, we discuss strategies to learn DSFs, and define the classes of deep supermodular functions, deep difference of submodular functions, and deep multivariate submodular functions, and discuss where these can be useful in applications.", "histories": [["v1", "Tue, 31 Jan 2017 08:06:33 GMT  (3646kb,D)", "http://arxiv.org/abs/1701.08939v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jeffrey bilmes", "wenruo bai"], "accepted": false, "id": "1701.08939"}, "pdf": {"name": "1701.08939.pdf", "metadata": {"source": "META", "title": "Deep Submodular Functions", "authors": ["Jeffrey A. Bilmes", "Wenruo Bai"], "emails": [], "sections": [{"heading": null, "text": ""}, {"heading": "1 Introduction 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Background and Motivation 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 Sums of Concave Composed with Modular Functions (SCMMs) 4", "text": "3.1 Feature Based Functions................................................... 6"}, {"heading": "4 Deep Submodular Functions 8", "text": "4.1 Recursively defined DSFs.................................. 10 4.2 DSFs: Practical use and relationship to deep neural networks............ 11ar Xiv: 170 1.08 939v 1 [cs.L G] 31 Jan 20"}, {"heading": "5 Relevant Properties and Special Cases 11", "text": "5.1 Characteristics of concave and submodular functions......................................... 12 5.2 Antitone cards and superdifferentials.................................. 16 5.3 Special matroid case and depth matroid rank............................. 19. 5.4 Over- and absolute redundancy.................................................................................."}, {"heading": "6 The Family of Deep Submodular Functions 24", "text": "6.1 Generalizing DSFs SCMMs......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "7 Applications in Machine Learning and Data Science 36", "text": "......................................................................................................................................................................................"}, {"heading": "8 Conclusions and Future Work 43", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 Acknowledgments 43", "text": "References 44"}, {"heading": "A More General Conditions on Two-Layer Functions: Proofs 52", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B Sums of Weighted Cardinality Truncations is Smaller than SCMMs 55", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 Background and Motivation", "text": "This means that the incremental value (or gain) of the addition of another sample v is reduced to a subset if the context in which v is considered X to Y grows. We can use the gain of v in the context of f (v | X), f (X), f (v), f (v), f (v), f (f), f (f), f (f), f (f), f (f), f (f), f (f), f (f), f (f), f (v), f (f), f (v), f (X), f), f (V), f (V), f (V), f), f (V), f (V), f (V), f (V), f (f), f (V)."}, {"heading": "3 Sums of Concave Composed with Modular Functions (SCMMs)", "text": "A class of submodular functions [141] used in machine learning are the so-called \"decomposable functions\" (\"resolvable functions\"), a set of non-negative modular functions (\"resolvable functions\") (\"resolvable functions\") (\"resolvable functions\") (\"resolvable functions\") (\"resolvable functions\") (\"resolvable functions\") (\"resolvable functions\") (\"resolvable functions\") (\"resolvable functions\") (\"resolvable functions\") (\"resolvable functions\") (\"resolvable functions\") (\"resolvable functions\") (\"resolvable functions\") (\"resolvable functions\"), \"(\" resolvable functions \") (\" resolvable functions \"),\" (\"resolvable functions\") (\"resolvable functions\"), \"(resolvable functions\") () (() () (() () (\"resolvable functions\" (), \"(resolvable functions),\" (\"(\") () () () () (\"resolvable functions\" (), \"() (\" () (), \"() () () (\" (), \"(\" (), \"(),\" (), \"(),\" (), \"(),\" (), \"(),\" (), \"(),\" (), \"(),\" (\"(),\" (), \"(\" (), \"(),\" (), \"(\" (), \"(),\" (\"(),\" (), \"(),\" (\"(),\" (), \"(),\" (\"(),\" (), \"() ((),\" (), \"(\" (), \"() ((() ((),\" (), () (((), (\"((),\" (((), \"(\" (), (((), (), (() (() (() ((), (\"(), () ((((), () ((((), (((), (\" (), (\"((), (), (\" (), ("}, {"heading": "3.1 Feature Based Functions", "text": "A particularly useful way to view SCMMs for machine learning and data science (X) (X) is when data objects are embedded in a \"feature\" that can be converted into a \"feature\" space of a limited number of data (V) in which each of them can be considered a possible feature, concept, or attribute of an object. (V) Any feature that occurs in a non-negative feature vector vector vector vector mU (v) can be converted into a non-negative feature vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector () vector vector vector vector vector vector vector vector ()."}, {"heading": "4 Deep Submodular Functions", "text": "While submodular functions are an all-or-nothing property (as in matter), their weakness lies in the fact that features themselves do not interact, even though one feature may be partially redundant with another feature (such as describing a set of its constituent parts as n-grams containing features of higher order n-grams, so that some features of these features may be partially redundant), one way to reduce this redundancy is to reduce the features themselves to a subset that tends not to interact. However, this can only work in limited cases, namely when the features themselves can be reduced to an \"independent\" group that does not lose information about the data objects, and this only happens when they are an all-or-nothing property."}, {"heading": "4.1 Recursively Defined DSFs", "text": "It is not as if it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way, in which it is about a way and a way, in which it is about a way, in which it is about a way and a way in which it is about a way, in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way it is about a way and a way in which it is about a way and a way it is about a way and a way in which it is about a way and a way and a way it is about a way and a way in which it is about a way and a way it is about a way and a way in which it is about a way and in which it is about a way and a way it is about a way and a way and a way and a way in which it is about and a way in which it is about a way and a way it is about a way and a way and a way and a way in which it is about and a way it is about a way and a way and a way in which it is about a way and a way in which it is about and a way"}, {"heading": "4.2 DSFs: Practical Benefits and Relation to Deep Neural Networks", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5 Relevant Properties and Special Cases", "text": "DSFs represent a family that contains at least the family of SCMMs. Above, we intuitively argued that DSFs could expand SCMMs, since they allow the components themselves to interact directly, and the interactions can spread a multi-layered hierarchy. In this section, we will begin (in Section 5.1) by discussing precursor functions with respect to concave functions. Section 5.2 then discusses specific properties of the multivariate concave function associated with a DSF, in particular the overdifferential property of the antitone gradient, which is a sufficient condition for submodularity. This section also compares this condition with the negativity of the off-diagonal Hessian matrix condition for submodular functions. Section 5.3 discusses special cases of the matrix rank, including the laminar matrix function, which can be considered in the light of this paper (matrix rank) as a form of deep matrix sequence of matrix classification."}, {"heading": "5.1 Properties of Concave and Submodular Functions", "text": "Many of the results in the following sections are based on a number of properties of concave functions. Since we want to look at non-differentiable concave functions, the theorems below look at this more general case, in which we can only assume that concave functions have superdifferentials. In general, more work is needed to show that the properties of concave functions apply in this non-differential case, but since there does not seem to be any consolidated evidence for these properties, we offer them here in full. Let it be: R is a normalized (0) = 0) monotonous, non-decreasing concave function. In any such function, there may be an initial linear part in which x = x x x exists for x."}, {"heading": "5.2 Antitone Maps and Superdifferentials", "text": "It is generally known that a univariate concave function, which is composed with non-negative modular functions, yields a sub-modular function (Theorem 5.4), however, yields a sub-modular concave function, which is not the case. If we consider, for example, a concave function, which offers the following evaluations: 0, 0) = 1, 1) = 0, 1) = 0, 1) = 1, 2, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5"}, {"heading": "5.3 The Special Matroid Case and Deep Matroid Rank", "text": "In this section, the specific cases of Matroids and Matroids are defined by their function as they do: \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"\" A, \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "5.4 Surplus and Absolute Redundancy", "text": "The surplus is a useful concept and is used extensively to show that in Section 6 different characteristics of the DSF family are defined as follows: Sf (A) = [A) = [A) = [A) = [A) = [A) = [A) = [A) = [A) = [A) = [A) = [A) = [A) = [A) = [A) = [A) = [A) = (A) = (A) = (A) = (A) = [S) = (A) = [S) = [S)) as surplus (A) as surplus (or absolute redundancy) of a group of A [A]. We use the term \"surplus\" under an interpretation where A is a series of agents who can either perform their action independently or perform their actions jointly and cooperatively [149]. If an agent performs the action independently, the costs (a) are the total cost of a group A (A)."}, {"heading": "6 The Family of Deep Submodular Functions", "text": "We have seen that SCMMs generalize matroid-rank functions and DSFs generalize laminar matroid-rank functions. We could expect from the above results that DSFs generalize SCMMs strictly - which is not immediately obvious, since SCMMs are much more capable than matroid-rank functions because: (1) concave functions do not have to be simple truncations of integers, (2) each term can have its own non-negative modular function, (3) however, there is no need to partition the basic elements across terms in an SCMM, and (4) with SCMMs we are allowed to add any additional modular function. We have also already seen in Theorem 5.8 that SCMMs are a larger class of submodular functions than just concave across terms in an SCMM, and in Lemma 5.10 that they generalize weighted cardinalityuntruncations."}, {"heading": "6.1 DSFs generalize SCMMs", "text": "It is clear that DSFs contain at least the class of SCMMs, since every single-layer DSF is an SCMM. Next, we show that SCMM DSFs are only laminar matroid-matroid-matroid-matroid-matroid-matroid-matroid-matroid-matroid-matroid-functions-matroid-matroid-matroid-functions-matroid-matroid-matroid-matroid-functions-matroid-matroid-matroid-functions-matroid-matroid-matroid-matroid-matroid-functions-matroid-matroid-matroid-matroid-matroid-matroid-functions-matroid-matroid-matroid-matroid-matroid-matroid-functions-matroid-matroid-matroid-functions-matroid-matroid-matroid-matroid-functions-matroid-matroid-matroid-matroid-functions-matroid-matroid-matroid-matroid-functions-matroid-matroid-matroid-matroid-functions-matroid-matroid-matroid-functions-matroid-matroid-matroid-matroid-functions-matroid-matroid-matroid-functions-matroid-matroid-functions-matroid-matroid-matroid-functions-matroid-matroid-matroid-functions-matroid-matroid-matroid-functions-matroid-functions-matroid-matroid-matroid-functions-matroid-functions-matroid-matroid-matroid-functions-matroid-matroid-functions-matroid-matroid-functions-matroid-functions-matroid-matroid-functions-matroid-matroid-functions-functions-matroid-matroid-matroid-functions-matroid-functions-matroid-functions-matroid-"}, {"heading": "6.1.1 The Laminar Matroid Rank Case", "text": "Our first example DSF is a simple laminar matroid on six elements. We show that SCMMs cannot express this laminar precedence order, and since DSFs generalize the laminar precedence order, the result follows. Consider the following function f: 2V \u2192 R, where V = {a, b, c, e, f}: f (A) = min (A) = min (A), 2) + min (A), 2), 3) (64) The function is a laminar matroid function with F = {V, {a, c}, c} and boundaries kV = 3, b}, c}, k {a, c} = 2, k {d, e, f} = 2.In the following results we assume that g: 2V \u2192 R is an SCMM of the form g (A)."}, {"heading": "6.1.2 A Non-matroid Case", "text": "To avoid thinking that it is only the matroids that cause problems for the SCMMs, let us consider the function f = > A = > A = > R, where again V = {a, b, c, e, f}.f (A) = min (min, b, c, d), 3) + min (A, c, e, f), 5) (76) Here there is an overlap between the two groups B1 = {a, b, d} and B2 = {c, e, f}. This is not a matroid ranking, as for example f (c) = 2. Minimum records of maximum value do not all have the same size, e.g. f (a, c, d}) = 5, while f ({a, c, e}) = 5. Lemma 6.3."}, {"heading": "6.1.3 More General Conditions on Two-Layer Functions", "text": "In this section, we return to the form of DSF in Equation (64), where we saw that there is no corresponding SCMM. Let us generalize the equation (64) in the following. g (A) = 0.5 (0.5) Ms (0.5). Lemma 6.2 does not require that for all \u03c6 (2) the corresponding DSF has no SCMM representation. In fact, it is possible for certain functions. Although we do not give a complete characterization of these DSFs that can or cannot be represented by SCMMs in this paper, we offer the following theorems. Theorem 6.5. The function g (A) in Equation 81 is an SCMM if and only if \u2212 4solve (2) SCMMs are efficient."}, {"heading": "6.2 The DSF Family Grows Strictly with the Number of Layers", "text": "It is clear that a k-layer DSF is able to apply a k-1 layer DSF in all areas where the DSFs have only a limited function. (...) It is also clear that DSFs have only a limited number of layers, and that DSFs have only a limited number of layers. (...) It is also clear that DSFs have only a limited number of layers, and therefore DSFk \u2212 1 layers for all layers, and therefore DSFk \u2212 1 layers. This result is similar to some of the recent results from the DNN literature, in which it is shown that it requires exponentially many hidden units to realize a network with more layers."}, {"heading": "6.3 The Family of Submodular Functions is Strictly Larger than DSFs", "text": "Our next result shows that while DSFs are richer than SCMMs, and the DSF family grows with the number of DSFs, they do not yet include all the polymatroid functions. We demonstrate this by proving that we need to decouple the DSF layer-by-layer until we reach a single-layer DSF layer, which, as shown, is unable to represent a cycle of the DSF layer over K4. Specifically, we need a necessary lack of excess, required linearity, and also a required paired surplus from root to first layer, which shows that the DSF layer up to a size of three sets must be similar to a mixture of concessiondular, and which is then unable to maintain an irrational excess rate."}, {"heading": "7 Applications in Machine Learning and Data Science", "text": "In this section we describe a number of possible DSF applications in the fields of machine learning and data science."}, {"heading": "7.1 Learning DSFs", "text": "As mentioned in Section 1, recent studies [52, 43, 42] show that learning submodular functions can be easier or more difficult, depending on the learning environment. (...) A general sketch of different learning environments is in [76, 43] - here we give only a very brief overview. (...) To begin, learning can include several families of functions F, H, and T, some of which are good in some way. (...) There is a real function f that can be learned on the basis of information obtained via patterns of form (A) for A. (...) One wants an approximation f that is good in some way. (...) Learning submodular functions has been studied in a number of possible variants. (...) Typically, there is a probability distribution Pr over subsets of V (i.e.), Pr (S = A)."}, {"heading": "7.1.1 Training and Testing on Different Ground Sets, and Multimodal Submodularity", "text": "In the training process, one consists of a stack of documents (consisting of a set of sentences) and a set of these sentences that does not overlap with the training set. If the training has submodular functions, this means that the training set could consist of several principles, and the test set could consist of lees that were not seen during the training. A data set could consist of D = {S, Si, yi) i, where Vi is a principle, i.e. Vi 6 = Vi and, if available, yi = fi (Si) is an evaluation of Si by a ground-related submodular function fi. Therefore, there can be no instance in which two principles are the same, so Vi 6 = Vj for i, nor could there be any commonality between training and test sets. The reason why this happens can be summarized on the basis of a document [92]."}, {"heading": "7.2 Deep Supermodular Functions and Deep Differences", "text": "The results in this article show that DSUFs correspond to a larger class than just sums of convex functions that are composed with non-negative modular functions. In [110, 64] it was shown that any arbitrary function h: 2V \u2192 R can be represented as a difference between two submodular functions. If we f1, f2, f5, f5, f5, f5, f5, f5, f5, f6, f6, f6, f6, f6, f5, f6, f6, f6, f6, f6, f6, f6, f6, f6, f6, f6, f5, f5, f6, f6, f6, f6, f6, f6, f6, f5, f5, f5, f5, f5, f5, f5, f5, f5, f5, f5, f5, f6, f5, f5, f5, f5, f5, f5, f5, f5, f5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, f6, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,"}, {"heading": "7.3 Deep Multivariate Submodular Functions", "text": "In this section, we consider certain submodular functions to multimodular functions. Other ways to generalize submodularities are considered as discrete generalizations of properties: A, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B"}, {"heading": "7.4 Simultaneously Learning Hash and Submodular Functions", "text": "It would therefore be useful to have a strategy to obtain as much training data as desired. (The goal is to learn a map from a vector x-bit vector represented in the input space Rd and mapping it in binary space. (e.g. images, documents, music files etc.) that are represented in the input space Rd. (1) b, where b < d and beyond is the space binary, operations such as the closest neighbor search are faster. There are existing approaches that can automatically learn this mapping. (e.g.)"}, {"heading": "8 Conclusions and Future Work", "text": "In this paper, we have presented a complete characterization of our newly proposed class of submodular functions, DSFs. We have introduced the antitonic gradient to establish subclasses of submodular functions. We have shown that DSFs are a strictly larger family than the family of submodular functions obtained by additive combination of concave with modular functions (SCMMs). We have also shown that DSFs do not include all submodular functions, all of this in the specific context of matroid rank functions and also in a more general context. As mentioned at various points in the paper, there are several interesting open problems related to DSFs. An immediate task is the development of practical strategies for successful empirical learning of DSFs, as initiated in [36]. A second task is to set generalization limits for learning DSFs in a WKM framework. A third task is the question of whether there will be a limited number of submodular functions combined with each other in a FS. \""}, {"heading": "9 Acknowledgments", "text": "Thanks to Brian Dolhansky for his help in building an initial implementation of DSFs used in [36], to Reza Eghbali and Kai Wei for useful discussions, and to Jan Vondrak for his suggestions on using surplus and deficit as an analysis strategy. This material is based on work supported by the National Science Foundation under grant number IIS-1162606, the National Institutes of Health under grant number R01GM103544, and a Google, Microsoft, Facebook, and Intel research award. Thanks also to the Simons Institute for the Theory of Computing, Foundations of Machine Learning Program. This work was partially supported by TerraSwarm, one of six centers of STARnet, a program sponsored by the Semiconductor Research Corporation, MARCO and DARPA."}, {"heading": "A More General Conditions on Two-Layer Functions: Proofs", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "B Sums of Weighted Cardinality Truncations is Smaller than SCMMs", "text": "This section shows that Lemma 5.10, namely that G = [1], B'V, B | \u2212 1i = 1 \u03b1B, i min (| A'B |, i), B, i, \u03b1B, i \u2265 0}, SCMM. We assume that the reader is familiar with the notation in Appendix A.Lemma B.1. f5 (A), B'V, B | \u2212 1 i = 1 \u03b1B, i min (| A'B |, i), iEmin (| A \u00b2 B |, i \u00b2 proof). (121) Note that f5 (A) = 1 \u03b1B, i min (| A \u00b2 B |, i) = 1 \u0430B V |, i) = 1 \u03b1B \u00b2, i \u00b2, i \u00b2, i \u00b2, iEmin (A \u00b2 B |, i \u00b2), (121), f5, f5, f5, e, f}, and these terms apply to all terms."}], "references": [{"title": "Notes on using Determinantal Point Processes for Clustering with Applications to Text Clustering", "author": ["A. Agarwal", "Choromanska A", "K. Choromanski"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Graph-based learning for statistical machine translation", "author": ["A. Alexandrescu", "K. Kirchhoff"], "venue": "Proceedings of HLT, pages 119\u2013127,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Methods of information geometry, volume 191", "author": ["Shun-ichi Amari", "Hiroshi Nagaoka"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Deep canonical correlation analysis", "author": ["Galen Andrew", "Raman Arora", "Karen Livescu", "Jeff Bilmes"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Structured sparsity-inducing norms through submodular functions", "author": ["F. Bach"], "venue": "NIPS,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Streaming submodular maximization: Massive data summarization on the fly", "author": ["A. Badanidiyuru", "B. Mirzasoleiman", "A. Karbasi", "A. Krause"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 671\u2013680. ACM,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast algorithms for maximizing submodular functions", "author": ["Ashwinkumar Badanidiyuru", "Jan Vondr\u00e1k"], "venue": "In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Bipartite matching generalizations for peptide identification in tandem mass spectrometry", "author": ["Wenruo Bai", "Jeffrey Bilmes", "William S. Noble"], "venue": "In 7th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM BCB),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Learning submodular functions", "author": ["M. Balcan", "N. Harvey"], "venue": "Technical report, arXiv:1008.2159,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning submodular functions", "author": ["Maria-Florina Balcan", "Nicholas JA Harvey"], "venue": "In Proceedings of the forty-third annual ACM symposium on Theory of computing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Coreference semantics from web features", "author": ["M. Bansal", "D. Klein"], "venue": "Proceedings of ACL, pages 389\u2013398,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "The power of randomization: Distributed submodular maximization on massive datasets", "author": ["Rafael Barbosa", "Alina Ene", "Huy L Nguyen", "Justin Ward"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "A new framework for distributed submodular maximization", "author": ["Rafael Da Ponte Barbosa", "Alina Ene", "Huy L Nguyen", "Justin Ward"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Learning Deep Architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends R \u00a9 in Machine Learning, 2(1):1\u2013127,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Representation Learning: A Review and New Perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Creating robust supervised classifiers via web-scale n-gram data", "author": ["S. Bergsma", "E. Pitler", "D. Lin"], "venue": "Proceedings of ACL, pages 865\u2013874,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Instance selection for machine translation using feature decay algorithms", "author": ["E. Bi\u00e7ici", "D. Yuret"], "venue": "Proceedings of the 6th Workshop on Statistical Machine Translation, pages 272\u2013283,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Convex optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Cambridge Univ Pr,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Discover feature engineering, how to engineer features and how to get good at it, 2014", "author": ["Jason Brownlee"], "venue": "Machine Learning Process", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Submodular maximization with cardinality constraints", "author": ["Niv Buchbinder", "Moran Feldman", "Joseph Seffi Naor", "Roy Schwartz"], "venue": "In Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Maximizing a monotone submodular function subject to a matroid constraint", "author": ["Gruia Calinescu", "Chandra Chekuri", "Martin P\u00e1l", "Jan Vondr\u00e1k"], "venue": "SIAM Journal on Computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Streaming algorithms for submodular function maximization", "author": ["Chandra Chekuri", "Shalmoli Gupta", "Kent Quanrud"], "venue": "In International Colloquium on Automata, Languages, and Programming,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Submodular function maximization via the multilinear relaxation and contention resolution schemes", "author": ["Chandra Chekuri", "Jan Vondr\u00e1k", "Rico Zenklusen"], "venue": "SIAM Journal on Computing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Fast approximate kNN graph construction for high dimensional data via recursive Lanczos bisection", "author": ["J. Chen", "H.-R. Fang", "Y. Saad"], "venue": "JMLR, 10:1989\u20132012,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "On the uncapacitated location problem", "author": ["G. Cornu\u00e9jols", "M. Fisher", "G.L. Nemhauser"], "venue": "Annals of Discrete Mathematics, 1:163\u2013177,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1977}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Boolean functions: Theory, algorithms, and applications", "author": ["Yves Crama", "Peter L Hammer"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Visual categorization with bags of keypoints", "author": ["Gabriella Csurka", "Christopher Dance", "Lixin Fan", "Jutta Willamowski", "C\u00e9dric Bray"], "venue": "In Workshop on statistical learning in computer vision, ECCV,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Testing membership in matroid polyhedra", "author": ["W.H. Cunningham"], "venue": "J Combinatorial Theory B, 36:161\u2013188,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1984}, {"title": "Decomposition of submodular functions", "author": ["William H Cunningham"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1983}, {"title": "Optimal attack and reinforcement of a network", "author": ["William H Cunningham"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1985}, {"title": "Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection", "author": ["A. Das", "D. Kempe"], "venue": "ICML,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2009}, {"title": "Bag-of-visual-words models for adult image classification and filtering", "author": ["Thomas Deselaers", "Lexi Pimenidis", "Hermann Ney"], "venue": "In Pattern Recognition,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Deep submodular functions: Definitions & learning", "author": ["Brian Dolhansky", "Jeff Bilmes"], "venue": "In Neural Information Processing Society (NIPS),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Deep submodular functions: Definitions and learning", "author": ["Brian Dolhansky", "Jeff Bilmes"], "venue": "In Neural Information Processing Society (NIPS),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "The pure theory of monopoly", "author": ["F.Y. Edgeworth"], "venue": "Giornale degli Economisti, 1887. Reprinted in EDGEWORTH, F. Y. Papers relating to political economy. London: Macmillan,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1925}, {"title": "Turning down the noise in the blogosphere", "author": ["Khalid El-Arini", "Gaurav Veda", "Dafna Shahaf", "Carlos Guestrin"], "venue": "In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}, {"title": "The power of depth for feedforward neural networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "CoRR, abs/1512.03965,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Maximizing non-monotone submodular functions", "author": ["Uriel Feige", "Vahab S Mirrokni", "Jan Vondrak"], "venue": "SIAM Journal on Computing,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Optimal bounds on approximation of submodular and XOS functions by juntas", "author": ["V. Feldman", "J. Vondr\u00e1k"], "venue": "CoRR, abs/1307.3301,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Representation, approximation and learning of submodular functions using low-rank decision trees", "author": ["Vitaly Feldman", "Pravesh Kothari", "Jan Vondr\u00e1k"], "venue": "In COLT,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "A visual bag of words method for interactive qualitative localization and mapping", "author": ["David Filliat"], "venue": "In Robotics and Automation,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2007}, {"title": "An analysis of approximations for maximizing submodular set functions\u2014 II", "author": ["M.L. Fisher", "G.L. Nemhauser", "L.A. Wolsey"], "venue": "Polyhedral combinatorics, pages 73\u201387,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1978}, {"title": "Submodular Functions and Optimization", "author": ["S. Fujishige"], "venue": "Number 58 in Annals of Discrete Mathematics. Elsevier Science, 2nd edition,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2005}, {"title": "Minimizing a submodular function arising from a concave function", "author": ["Satoru Fujishige", "Satoru Iwata"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1999}, {"title": "Near-optimal MAP inference for determinantal point processes", "author": ["J. Gillenwater", "A. Kulesza", "B. Taskar"], "venue": "NIPS,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Approximate Inference for Determinantal Point Processes", "author": ["Jennifer Gillenwater"], "venue": "PhD thesis, U. Penn,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "Submodular hamming metrics", "author": ["Jennifer Gillenwater", "Rishabh Iyer", "Bethany Lusch", "Rahul Kidambi", "Jeff Bilmes"], "venue": "In Neural Information Processing Society (NIPS),", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Near-optimal MAP inference for determinantal point processes", "author": ["Jennifer Gillenwater", "Alex Kulesza", "Ben Taskar"], "venue": "Advances in Neural Information", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2012}, {"title": "Approximating submodular functions everywhere", "author": ["M.X. Goemans", "N.J.A. Harvey", "S. Iwata", "V. Mirrokni"], "venue": "SODA, pages 535\u2013544,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithmic graph theory and perfect graphs, volume 57", "author": ["Martin Charles Golumbic"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2004}, {"title": "Sampling from probabilistic submodular models", "author": ["Alkis Gotovos", "S. Hamed Hassani", "Andreas Krause"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2015}, {"title": "Learning binary hash codes for large-scale image search. InMachine learning for computer vision, pages 49\u201387", "author": ["Kristen Grauman", "Rob Fergus"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2013}, {"title": "Active semi-supervised learning using submodular functions", "author": ["Andrew Guillory", "Jeff Bilmes"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2011}, {"title": "Optimal marketing strategies over social networks", "author": ["Jason Hartline", "Vahab Mirrokni", "Mukund Sundararajan"], "venue": "In Proceedings of the 17th international conference on World Wide Web,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2008}, {"title": "Submodular optimization under noise", "author": ["Avinatan Hassidim", "Yaron Singer"], "venue": "arXiv preprint arXiv:1601.03095,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2016}, {"title": "Convex analysis and minimization algorithms I: fundamentals, volume 305", "author": ["Jean-Baptiste Hiriart-Urruty", "Claude Lemar\u00e9chal"], "venue": "Springer science & business media,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 1993}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["Kurt Hornik"], "venue": "Neural networks,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1991}, {"title": "Towards minimizing k-submodular functions", "author": ["Anna Huber", "Vladimir Kolmogorov"], "venue": "CoRR, abs/1309.5469,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2013}, {"title": "Computational geometric approach to submodular function minimization for multiclass queueing systems", "author": ["Toshinari Itoko", "Satoru Iwata"], "venue": "In International Conference on Integer Programming and Combinatorial Optimization,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2007}, {"title": "Algorithms for approximate minimization of the difference between submodular functions, with applications", "author": ["R. Iyer", "J. Bilmes"], "venue": "Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast semidifferential based submodular function optimization", "author": ["R. Iyer", "S. Jegelka", "J. Bilmes"], "venue": "ICML,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2013}, {"title": "Submodular optimization with submodular cover and submodular knapsack constraints", "author": ["Rishabh Iyer", "Jeff Bilmes"], "venue": "In Neural Information Processing Society (NIPS),", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2013}, {"title": "Submodular point processes", "author": ["Rishabh Iyer", "Jeff Bilmes"], "venue": "In 18th International Conference on Artificial Intelligence and Statistics (AISTATS-2015),", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2015}, {"title": "Fast semidifferential-based submodular function optimization", "author": ["Rishabh Iyer", "Stefanie Jegelka", "Jeff A. Bilmes"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2013}, {"title": "Graph construction and b-matching for semi-supervised learning", "author": ["T. Jebara", "J. Wang", "S.-F. Chang"], "venue": "Proceedings of ICML, pages 441\u2013448,", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2009}, {"title": "Reflection methods for user-friendly submodular optimization", "author": ["Stefanie Jegelka", "Francis Bach", "Suvrit Sra"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2013}, {"title": "Submodularity beyond submodular energies: coupling edges in graph cuts", "author": ["Stefanie Jegelka", "Jeff A. Bilmes"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2011}, {"title": "Fast approximate submodular minimization", "author": ["Stefanie Jegelka", "Hui Lin", "Jeff A. Bilmes"], "venue": "In Neural Information Processing Society (NIPS),", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2011}, {"title": "A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), page 655\u00e2\u0102\u015e665", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2014}, {"title": "Canonical parameterizations and zero parameter-effects curvature", "author": ["Robert E Kass"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 1984}, {"title": "Finding groups in data: an introduction to cluster analysis, volume 344", "author": ["Leonard Kaufman", "Peter J Rousseeuw"], "venue": null, "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2009}, {"title": "Toward efficient agnostic learning", "author": ["Michael J Kearns", "Robert E Schapire", "Linda M Sellie"], "venue": "Machine Learning,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 1994}, {"title": "Maximizing the spread of influence through a social network", "author": ["D. Kempe", "J. Kleinberg", "E. Tardos"], "venue": "SIGKDD,", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2003}, {"title": "Submodularity for data selection in machine translation", "author": ["K. Kirchhoff", "J. Bilmes"], "venue": "Proceedings of EMNLP, pages 131\u2013141,", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2014}, {"title": "What energy functions can be minimized via graph cuts", "author": ["V. Kolmogorov", "R. Zabih"], "venue": "IEEE TPAMI,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2004}, {"title": "Submodularity on a tree: Unifying l\u02c6\\ natural-convex and bisubmodular functions", "author": ["Vladimir Kolmogorov"], "venue": "In International Symposium on Mathematical Foundations of Computer Science,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2011}, {"title": "Robust submodular observation selection", "author": ["Andreas Krause", "Brendan McMahan", "Carlos Guestrin", "Anupam Gupta"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2008}, {"title": "Determinantal point processes for machine learning", "author": ["Alex Kulesza", "B Taskar"], "venue": "arXiv preprint arXiv:1207.6083,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2012}, {"title": "Fast greedy algorithms in mapreduce and streaming", "author": ["Ravi Kumar", "Benjamin Moseley", "Sergei Vassilvitskii", "Andrea Vattani"], "venue": "ACM Transactions on Parallel Computing,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2015}, {"title": "Using n-gram and word network features for native language identification", "author": ["S. Lahiri", "R. Mihalcea"], "venue": "Proceedings of NAACL-HLT Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised semantic role induction with graph partitioning", "author": ["J. Lang", "M. Lapata"], "venue": "Proceedings of EMNLP, pages 1320\u20131331,", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2011}, {"title": "Graphical models, volume 17", "author": ["Steffen L Lauritzen"], "venue": null, "citeRegEx": "86", "shortCiteRegEx": "86", "year": 1996}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444, may", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2015}, {"title": "Non-monotone submodular maximization under matroid and knapsack constraints", "author": ["Jon Lee", "Vahab S Mirrokni", "Viswanath Nagarajan", "Maxim Sviridenko"], "venue": "In Proceedings of the forty-first annual ACM symposium on Theory of computing,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2009}, {"title": "Cost-effective outbreak detection in networks", "author": ["Jure Leskovec", "Andreas Krause", "Carlos Guestrin", "Christos Faloutsos", "Jeanne VanBriesen", "Natalie Glance"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2007}, {"title": "Contextual bag-of-words for visual categorization. Circuits and Systems for Video Technology", "author": ["Teng Li", "Tao Mei", "In-So Kweon", "Xian-Sheng Hua"], "venue": "IEEE Transactions on,", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2011}, {"title": "A class of submodular functions for document summarization", "author": ["H. Lin", "J. Bilmes"], "venue": "ACL, pages 510\u2013520,", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning mixtures of submodular shells with application to document summarization", "author": ["H. Lin", "J. Bilmes"], "venue": "Uncertainty in Artificial Intelligence (UAI), Catalina Island, USA, July", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2012}, {"title": "A Class of Submodular Functions for Document Summarization", "author": ["Hui Lin", "Jeff Bilmes"], "venue": null, "citeRegEx": "93", "shortCiteRegEx": "93", "year": 2011}, {"title": "Word alignment via submodular maximization over matroids. In North American chapter of the Association for Computational Linguistics/Human Language Technology", "author": ["Hui Lin", "Jeff Bilmes"], "venue": null, "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2011}, {"title": "An application of the submodular principal partition to training data subset selection", "author": ["Hui Lin", "Jeff A. Bilmes"], "venue": "In Neural Information Processing Society (NIPS) Workshop,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2010}, {"title": "A novel graph-based compact representation of word alignment", "author": ["Q. Liu", "Z. Tu", "S. Lin"], "venue": "Proceedings of ACL, pages 358\u2013363,", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2013}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information retrieval", "author": ["Xiaodong Liu", "Jianfeng Gao", "Xiaodong He", "Li Deng", "Kevin Duh", "Ye-Yi Wang"], "venue": "In Proceedings of NAACL,", "citeRegEx": "97", "shortCiteRegEx": "97", "year": 2015}, {"title": "Submodular feature selection for high-dimensional acoustic score spaces", "author": ["Yuzong Liu", "Kai Wei", "Katrin Kirchhoff", "Yisong Song", "Jeff Bilmes"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 2013}, {"title": "An inequality for rearrangements", "author": ["GG Lorentz"], "venue": "The American Mathematical Monthly, 60(3):176\u2013179,", "citeRegEx": "99", "shortCiteRegEx": null, "year": 1953}, {"title": "Matroid matching and some applications", "author": ["L\u00e1szl\u00f3 Lov\u00e1sz"], "venue": "Journal of Combinatorial Theory, Series B,", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 1980}, {"title": "Structural parse tree features for text representation", "author": ["S. Massung", "C. Zhai", "J. Hockenmaier"], "venue": "Proceedings of IEEE Seventh Conference on Semantic Computing,", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2013}, {"title": "Multivariate information transmission", "author": ["William J McGill"], "venue": "Psychometrika, 19(2):97\u2013116,", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 1954}, {"title": "Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling", "author": ["R. Mihalcea"], "venue": "Proceedings of EMNLP, pages 411\u2013418,", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2005}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems, page 3111a\u0302A\u0306S\u03273119,", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 1995}, {"title": "Distributed submodular cover: Succinctly summarizing massive data", "author": ["Baharan Mirzasoleiman", "Amin Karbasi", "Ashwinkumar Badanidiyuru", "Andreas Krause"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "107", "shortCiteRegEx": "107", "year": 2015}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "108", "shortCiteRegEx": "108", "year": 2013}, {"title": "Discrete convex analysis", "author": ["Kazuo Murota"], "venue": null, "citeRegEx": "109", "shortCiteRegEx": "109", "year": 2003}, {"title": "A submodular-supermodular procedure with applications to discriminative structure learning", "author": ["Mukund Narasimhan", "Jeff Bilmes"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "110", "shortCiteRegEx": "110", "year": 2005}, {"title": "Graph connectivity measures for unsupervised word sense disambiguation", "author": ["R. Navigli", "M. Lapata"], "venue": "Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI), pages 1683\u20131688,", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2007}, {"title": "An analysis of approximations for maximizing submodular set functions i", "author": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher"], "venue": "Mathematical Programming, 14:265\u2013294,", "citeRegEx": "113", "shortCiteRegEx": null, "year": 1978}, {"title": "Introductory lectures on convex optimization: A basic course", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "114", "shortCiteRegEx": "114", "year": 2004}, {"title": "Automatic visual bag-of-words for online robot navigation and mapping", "author": ["Tudor Nicosevici", "Rafael Garcia"], "venue": "Robotics, IEEE Transactions on,", "citeRegEx": "115", "shortCiteRegEx": "115", "year": 2012}, {"title": "Convex functions and their applications: a contemporary approach", "author": ["Constantin Niculescu", "Lars-Erik Persson"], "venue": "Springer Science & Business Media,", "citeRegEx": "116", "shortCiteRegEx": "116", "year": 2006}, {"title": "On the convergence rate of decomposable submodular function minimization", "author": ["R. Nishihara", "S Jegelka", "M.I. Jordan"], "venue": "Advances in Neural Information Processing Systems, pages 640\u2013648,", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2014}, {"title": "Multilinear extensions of games", "author": ["Guillermo Owen"], "venue": "Management Science,", "citeRegEx": "119", "shortCiteRegEx": "119", "year": 1972}, {"title": "Using the mutual k-nearest neighbor graphs for semi-supervised classication of natural language data", "author": ["K. Ozaki", "M. Shimbo", "M. Komachi", "Y. Matsumoto"], "venue": "Proceedings of CoNLL, pages 154\u2013162,", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2011}, {"title": "Max-Margin Tensor Neural Network for Chinese Word Segmentation", "author": ["W. Pei"], "venue": "Transactions of the Association of Computational Linguistics, pages 293\u2013303,", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), page 1532a\u0302A\u0306S\u03271543,", "citeRegEx": "122", "shortCiteRegEx": "122", "year": 2014}, {"title": "Explicit and implicit syntactic features for text classification", "author": ["M. Post", "S. Bergsma"], "venue": "Proceedings of ACL, pages 866\u2013872,", "citeRegEx": "123", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised word sense disambiguation with n-gram features", "author": ["D. Preotiuc-Pietro", "F. Hristea"], "venue": "Artificial Intelligence Review, 41(2):241\u2013260,", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2014}, {"title": "Bisubmodular functions. CORE Discussion Papers 1989001", "author": ["Liqun Qi"], "venue": "Universite\u0301 catholique de Louvain, Center for Operations Research and Econometrics (CORE),", "citeRegEx": "125", "shortCiteRegEx": "125", "year": 1989}, {"title": "Graph propagation for paraphrasing out-ofvocabulary words in statistical machine translation", "author": ["M. Razmara", "M. Siahbani", "G. Haffari", "A. Sarkar"], "venue": "Proceedings of ACL,", "citeRegEx": "126", "shortCiteRegEx": null, "year": 2013}, {"title": "Characterization of the subdifferentials of convex functions", "author": ["Ralph Rockafellar"], "venue": "Pacific Journal of Mathematics,", "citeRegEx": "128", "shortCiteRegEx": "128", "year": 1966}, {"title": "On the maximal monotonicity of subdifferential mappings", "author": ["Ralph Rockafellar"], "venue": "Pacific Journal of Mathematics,", "citeRegEx": "129", "shortCiteRegEx": "129", "year": 1970}, {"title": "Submodular inference of diffusion networks from multiple trees", "author": ["Manuel Gomez Rodriguez", "Bernhard Sch\u00f6lkopf"], "venue": "arXiv preprint arXiv:1205.1671,", "citeRegEx": "130", "shortCiteRegEx": "130", "year": 2012}, {"title": "A potential theory for monotone multivalued operators", "author": ["G Romano", "L Rosati", "F Marotti de Sciarra", "P Bisegna"], "venue": "Quarterly of applied mathematics,", "citeRegEx": "131", "shortCiteRegEx": "131", "year": 1993}, {"title": "Foundations of Economic Analysis", "author": ["Paul A Samuelson"], "venue": null, "citeRegEx": "132", "shortCiteRegEx": "132", "year": 1947}, {"title": "Complementarity: An essay on the 40th anniversary of the hicks-allen revolution in demand theory", "author": ["Paul A Samuelson"], "venue": "Journal of Economic literature,", "citeRegEx": "133", "shortCiteRegEx": "133", "year": 1974}, {"title": "Multi-agent and multivariate submodular optimization", "author": ["Richard Santiago", "F. Bruce Shepherd"], "venue": null, "citeRegEx": "134", "shortCiteRegEx": "134", "year": 2016}, {"title": "Learnability, stability and uniform convergence", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "135", "shortCiteRegEx": "135", "year": 2010}, {"title": "A submodular optimization approach to sentence set selection", "author": ["Yusuke Shinohara"], "venue": "In ICASSP,", "citeRegEx": "136", "shortCiteRegEx": "136", "year": 2014}, {"title": "Differential-geometrical methods in statistics, volume 28", "author": ["Amari Shun-ichi"], "venue": null, "citeRegEx": "137", "shortCiteRegEx": "137", "year": 1985}, {"title": "UHD: Cross-lingual Word Sense Disambiguation using multilingual co-occurrence graphs", "author": ["C. Silberer", "S.P. Ponzetto"], "venue": "Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010), pages 134\u2013137,", "citeRegEx": "138", "shortCiteRegEx": null, "year": 2010}, {"title": "On bisubmodular maximization", "author": ["Ajit Singh", "Andrew Guillory", "Jeff Bilmes"], "venue": "In Fifteenth International Conference on Artificial Intelligence and Statistics (AISTAT),", "citeRegEx": "139", "shortCiteRegEx": "139", "year": 2012}, {"title": "Large-margin learning of submodular summarization models", "author": ["R. Sipos", "P. Shivaswamy", "T. Joachims"], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 224\u2013233. Association for Computational Linguistics,", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient minimization of decomposable submodular functions", "author": ["P. Stobbe", "A. Krause"], "venue": "NIPS,", "citeRegEx": "141", "shortCiteRegEx": null, "year": 2010}, {"title": "Convex Analysis for Minimizing and Learning Submodular Set Functions", "author": ["Peter Stobbe"], "venue": "PhD thesis, California Institute of Technology,", "citeRegEx": "142", "shortCiteRegEx": "142", "year": 2013}, {"title": "The multiinformation function as a tool for measuring stochastic dependence", "author": ["Milan Studen\u1ef3", "Jirina Vejnarov\u00e1"], "venue": "In Learning in graphical models,", "citeRegEx": "143", "shortCiteRegEx": "143", "year": 1998}, {"title": "Efficient graph-based semi-supervised learning of structured tagging models", "author": ["A. Subramanya", "S. Petrov", "F. Pereira"], "venue": "Proceedings of EMNLP, pages 167\u2013176,", "citeRegEx": "144", "shortCiteRegEx": null, "year": 2010}, {"title": "Submodular approximation: Sampling-based algorithms and lower bounds", "author": ["Zoya Svitkina", "Lisa Fleischer"], "venue": "SIAM Journal on Computing,", "citeRegEx": "145", "shortCiteRegEx": "145", "year": 2011}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "146", "shortCiteRegEx": "146", "year": 2005}, {"title": "Language modeling for bag-of-visual words image categorization", "author": ["Pierre Tirilly", "Vincent Claveau", "Patrick Gros"], "venue": "In Proceedings of the 2008 international conference on Content-based image and video retrieval,", "citeRegEx": "147", "shortCiteRegEx": "147", "year": 2008}, {"title": "Minimizing a submodular function on a lattice", "author": ["Donald M Topkis"], "venue": "Operations research,", "citeRegEx": "148", "shortCiteRegEx": "148", "year": 1978}, {"title": "Supermodularity and complementarity", "author": ["Donald M Topkis"], "venue": "Princeton university press,", "citeRegEx": "149", "shortCiteRegEx": "149", "year": 1998}, {"title": "Learning mixtures of submodular functions for image collection summarization", "author": ["S. Tschiatschek", "R. Iyer", "H. Wei", "J. Bilmes"], "venue": "Neural Information Processing Society (NIPS), Montreal, Canada, December", "citeRegEx": "150", "shortCiteRegEx": null, "year": 2014}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, page 384a\u0302A\u0306S\u0327394,", "citeRegEx": "151", "shortCiteRegEx": "151", "year": 2010}, {"title": "Submodularity in combinatorial optimization", "author": ["J. Vondr\u00e1k"], "venue": "PhD thesis, Charles University,", "citeRegEx": "152", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast graph construction using auction algorithm", "author": ["J. Wang", "Y. Xia"], "venue": "arXiv preprint arXiv:1210.4917,", "citeRegEx": "153", "shortCiteRegEx": null, "year": 2012}, {"title": "Information theoretical analysis of multivariate correlation", "author": ["Satosi Watanabe"], "venue": "IBM Journal of research and development,", "citeRegEx": "154", "shortCiteRegEx": "154", "year": 1960}, {"title": "Unsupervised submodular subset selection for speech data", "author": ["K. Wei", "Y. Liu", "K. Kirchhoff", "J. Bilmes"], "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, Florence, Italy,", "citeRegEx": "155", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributional features for text categorization", "author": ["X.B. Xue", "Z.H.Zhou"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "157", "shortCiteRegEx": "157", "year": 2009}, {"title": "Evaluating bag-of-visualwords representations in scene classification", "author": ["Jun Yang", "Yu-Gang Jiang", "Alexander G Hauptmann", "Chong-Wah Ngo"], "venue": "In Proceedings of the international workshop on Workshop on multimedia information retrieval,", "citeRegEx": "158", "shortCiteRegEx": "158", "year": 2007}, {"title": "Sentiment classification in under-resourced languages using graph-based semi-supervised learning methods", "author": ["R. Yong", "K. Nobuhiro", "N. Yoshinaga", "M. Kitsuregawa"], "venue": "IEICE TRANSACTIONS on Information and Systems, 97(4):790\u2013797,", "citeRegEx": "159", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning structural svms with latent variables", "author": ["Chun-Nam John Yu", "Thorsten Joachims"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "160", "shortCiteRegEx": "160", "year": 2009}, {"title": "Exploring syntactic structured features over parse trees for relation extraction using kernel methods", "author": ["M. Zhang", "G. Zhou", "A. Aw"], "venue": "Information Processing & Management, 44(2):687\u2013701,", "citeRegEx": "161", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast kNN graph construction with locality sensitive hashing", "author": ["Y.-M. Zhang", "K. Huang", "G. Geng", "C.-L. Liu"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 660\u2013674,", "citeRegEx": "162", "shortCiteRegEx": null, "year": 2013}, {"title": "A non-shannon-type conditional inequality of information quantities", "author": ["Zhen Zhang", "Raymond W Yeung"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "163", "shortCiteRegEx": "163", "year": 1997}, {"title": "Mastering Feature Engineering: Principles and Techniques for Data Scientists", "author": ["Alice Zheng"], "venue": "O\u2019Reilly Media,", "citeRegEx": "164", "shortCiteRegEx": "164", "year": 2016}], "referenceMentions": [{"referenceID": 44, "context": "For example, submodular functions can be minimized without constraints in polynomial time [46] even though they lie within a 2-dimensional cone in R2n and are parameterized, in their most general form, with a corresponding 2 independent degrees of freedom.", "startOffset": 90, "endOffset": 94}, {"referenceID": 108, "context": ", in the cardinality constrained case, the classic 1\u2212 1/e result of Nemhauser [113] via the greedy algorithm.", "startOffset": 78, "endOffset": 83}, {"referenceID": 20, "context": "Other problems also have guarantees, such as submodular maximization subject to knapsack or multiple matroid constraints [22, 21, 88, 66, 68].", "startOffset": 121, "endOffset": 141}, {"referenceID": 19, "context": "Other problems also have guarantees, such as submodular maximization subject to knapsack or multiple matroid constraints [22, 21, 88, 66, 68].", "startOffset": 121, "endOffset": 141}, {"referenceID": 85, "context": "Other problems also have guarantees, such as submodular maximization subject to knapsack or multiple matroid constraints [22, 21, 88, 66, 68].", "startOffset": 121, "endOffset": 141}, {"referenceID": 63, "context": "Other problems also have guarantees, such as submodular maximization subject to knapsack or multiple matroid constraints [22, 21, 88, 66, 68].", "startOffset": 121, "endOffset": 141}, {"referenceID": 65, "context": "Other problems also have guarantees, such as submodular maximization subject to knapsack or multiple matroid constraints [22, 21, 88, 66, 68].", "startOffset": 121, "endOffset": 141}, {"referenceID": 88, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 128, "endOffset": 132}, {"referenceID": 4, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 169, "endOffset": 172}, {"referenceID": 46, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 244, "endOffset": 260}, {"referenceID": 79, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 244, "endOffset": 260}, {"referenceID": 64, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 244, "endOffset": 260}, {"referenceID": 52, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 244, "endOffset": 260}, {"referenceID": 76, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 288, "endOffset": 292}, {"referenceID": 95, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 302, "endOffset": 306}, {"referenceID": 31, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 332, "endOffset": 336}, {"referenceID": 55, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 354, "endOffset": 358}, {"referenceID": 74, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 401, "endOffset": 405}, {"referenceID": 86, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 428, "endOffset": 432}, {"referenceID": 123, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 456, "endOffset": 461}, {"referenceID": 54, "context": "In recent years, submodular functions have been used for representing diversity functions for the purpose of data summarization [91], for use as structured convex norms [6], for energy functions in tree-width unconstrained probabilistic models [48, 82, 67, 55], useful in computer vision [79], feature [98] and dictionary selection [33], viral marketing [58] and influence modeling in social networks [77], information cascades [89] and diffusion modeling [130], clustering [111], and active and semi-supervised learning [57], to name just a few.", "startOffset": 521, "endOffset": 525}, {"referenceID": 106, "context": "This includes algorithms for optimizing non-submodular functions via the use of submodularity [110, 81, 71, 64], strategies for optimizing submodular functions subject to both combinatorial [65] and submodular level-set constraints [66], and so on.", "startOffset": 94, "endOffset": 111}, {"referenceID": 78, "context": "This includes algorithms for optimizing non-submodular functions via the use of submodularity [110, 81, 71, 64], strategies for optimizing submodular functions subject to both combinatorial [65] and submodular level-set constraints [66], and so on.", "startOffset": 94, "endOffset": 111}, {"referenceID": 68, "context": "This includes algorithms for optimizing non-submodular functions via the use of submodularity [110, 81, 71, 64], strategies for optimizing submodular functions subject to both combinatorial [65] and submodular level-set constraints [66], and so on.", "startOffset": 94, "endOffset": 111}, {"referenceID": 61, "context": "This includes algorithms for optimizing non-submodular functions via the use of submodularity [110, 81, 71, 64], strategies for optimizing submodular functions subject to both combinatorial [65] and submodular level-set constraints [66], and so on.", "startOffset": 94, "endOffset": 111}, {"referenceID": 62, "context": "This includes algorithms for optimizing non-submodular functions via the use of submodularity [110, 81, 71, 64], strategies for optimizing submodular functions subject to both combinatorial [65] and submodular level-set constraints [66], and so on.", "startOffset": 190, "endOffset": 194}, {"referenceID": 63, "context": "This includes algorithms for optimizing non-submodular functions via the use of submodularity [110, 81, 71, 64], strategies for optimizing submodular functions subject to both combinatorial [65] and submodular level-set constraints [66], and so on.", "startOffset": 232, "endOffset": 236}, {"referenceID": 8, "context": "For example, it was shown that learning submodularity in the PMAC setting is fairly hard [10] although in some cases things are a bit easier [42].", "startOffset": 89, "endOffset": 93}, {"referenceID": 40, "context": "For example, it was shown that learning submodularity in the PMAC setting is fairly hard [10] although in some cases things are a bit easier [42].", "startOffset": 141, "endOffset": 145}, {"referenceID": 133, "context": "For example, in [140, 92], it is shown empirically that one can effectively learn mixtures of submodular functions using a max-margin learning framework \u2014 here the components of the mixture are fixed and it is only the mixture parameters that are learnt, leading often to a convex optimization problem.", "startOffset": 16, "endOffset": 25}, {"referenceID": 89, "context": "For example, in [140, 92], it is shown empirically that one can effectively learn mixtures of submodular functions using a max-margin learning framework \u2014 here the components of the mixture are fixed and it is only the mixture parameters that are learnt, leading often to a convex optimization problem.", "startOffset": 16, "endOffset": 25}, {"referenceID": 89, "context": "In some cases, computing gradients of the convex problem can be done using submodular maximization [92], while in other cases, even a gradient requires minimizing a difference of two submodular functions [150].", "startOffset": 99, "endOffset": 103}, {"referenceID": 143, "context": "In some cases, computing gradients of the convex problem can be done using submodular maximization [92], while in other cases, even a gradient requires minimizing a difference of two submodular functions [150].", "startOffset": 204, "endOffset": 209}, {"referenceID": 134, "context": "These include the so-called \u201cdecomposable\u201d submodular functions, namely those that can be represented as a sum of concave composed with modular functions [141].", "startOffset": 154, "endOffset": 159}, {"referenceID": 50, "context": "These matroid rank functions include the truncated matroid rank function [52] that is often used to show theoretical worst-case performance for many constrained submodular minimization problems.", "startOffset": 73, "endOffset": 77}, {"referenceID": 89, "context": "Namely, that they: (1) can leverage the vast amount of practical work on feature engineering that occurs in the machine learning community and its applications; (2) can operate on multi-modal data if the data can be featurized in the same space; (3) allow for training and testing on distinct sets since we can learn a function from the feature representation level on up, similar to the work in [92]; and (4) are useful for streaming [7, 83, 23] and parallel [107, 13, 14] optimization since functions can be evaluated without requiring knowledge of or access to the entire ground set.", "startOffset": 396, "endOffset": 400}, {"referenceID": 5, "context": "Namely, that they: (1) can leverage the vast amount of practical work on feature engineering that occurs in the machine learning community and its applications; (2) can operate on multi-modal data if the data can be featurized in the same space; (3) allow for training and testing on distinct sets since we can learn a function from the feature representation level on up, similar to the work in [92]; and (4) are useful for streaming [7, 83, 23] and parallel [107, 13, 14] optimization since functions can be evaluated without requiring knowledge of or access to the entire ground set.", "startOffset": 435, "endOffset": 446}, {"referenceID": 80, "context": "Namely, that they: (1) can leverage the vast amount of practical work on feature engineering that occurs in the machine learning community and its applications; (2) can operate on multi-modal data if the data can be featurized in the same space; (3) allow for training and testing on distinct sets since we can learn a function from the feature representation level on up, similar to the work in [92]; and (4) are useful for streaming [7, 83, 23] and parallel [107, 13, 14] optimization since functions can be evaluated without requiring knowledge of or access to the entire ground set.", "startOffset": 435, "endOffset": 446}, {"referenceID": 21, "context": "Namely, that they: (1) can leverage the vast amount of practical work on feature engineering that occurs in the machine learning community and its applications; (2) can operate on multi-modal data if the data can be featurized in the same space; (3) allow for training and testing on distinct sets since we can learn a function from the feature representation level on up, similar to the work in [92]; and (4) are useful for streaming [7, 83, 23] and parallel [107, 13, 14] optimization since functions can be evaluated without requiring knowledge of or access to the entire ground set.", "startOffset": 435, "endOffset": 446}, {"referenceID": 103, "context": "Namely, that they: (1) can leverage the vast amount of practical work on feature engineering that occurs in the machine learning community and its applications; (2) can operate on multi-modal data if the data can be featurized in the same space; (3) allow for training and testing on distinct sets since we can learn a function from the feature representation level on up, similar to the work in [92]; and (4) are useful for streaming [7, 83, 23] and parallel [107, 13, 14] optimization since functions can be evaluated without requiring knowledge of or access to the entire ground set.", "startOffset": 460, "endOffset": 473}, {"referenceID": 11, "context": "Namely, that they: (1) can leverage the vast amount of practical work on feature engineering that occurs in the machine learning community and its applications; (2) can operate on multi-modal data if the data can be featurized in the same space; (3) allow for training and testing on distinct sets since we can learn a function from the feature representation level on up, similar to the work in [92]; and (4) are useful for streaming [7, 83, 23] and parallel [107, 13, 14] optimization since functions can be evaluated without requiring knowledge of or access to the entire ground set.", "startOffset": 460, "endOffset": 473}, {"referenceID": 12, "context": "Namely, that they: (1) can leverage the vast amount of practical work on feature engineering that occurs in the machine learning community and its applications; (2) can operate on multi-modal data if the data can be featurized in the same space; (3) allow for training and testing on distinct sets since we can learn a function from the feature representation level on up, similar to the work in [92]; and (4) are useful for streaming [7, 83, 23] and parallel [107, 13, 14] optimization since functions can be evaluated without requiring knowledge of or access to the entire ground set.", "startOffset": 460, "endOffset": 473}, {"referenceID": 133, "context": "We therefore extend the max-margin learning framework of [140, 92] to apply to DSFs.", "startOffset": 57, "endOffset": 66}, {"referenceID": 89, "context": "We therefore extend the max-margin learning framework of [140, 92] to apply to DSFs.", "startOffset": 57, "endOffset": 66}, {"referenceID": 30, "context": "If f is a normalized monotone non-decreasing function, then it is often referred to as a polymatroid function [32, 31, 100] 1 because it carries identical information to that of a polymatroidal polyhedron.", "startOffset": 110, "endOffset": 123}, {"referenceID": 29, "context": "If f is a normalized monotone non-decreasing function, then it is often referred to as a polymatroid function [32, 31, 100] 1 because it carries identical information to that of a polymatroidal polyhedron.", "startOffset": 110, "endOffset": 123}, {"referenceID": 97, "context": "If f is a normalized monotone non-decreasing function, then it is often referred to as a polymatroid function [32, 31, 100] 1 because it carries identical information to that of a polymatroidal polyhedron.", "startOffset": 110, "endOffset": 123}, {"referenceID": 100, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 107, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 1, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 131, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 137, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 82, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 93, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 120, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 151, "context": ", [103, 112, 2, 138, 144, 85, 96, 126, 159].", "startOffset": 2, "endOffset": 43}, {"referenceID": 90, "context": "Graph-based submodular functions include the classic graph cut function f(X) = \u2211 x\u2208X,y\u2208V \\X w(x, y), but also the monotone graph cut function f(X) = \u2211 x\u2208X,y\u2208V w(x, y), the saturated graph cut function [93] f(X) = \u2211 v\u2208V min(Cv(X), \u03b1Cv(V )) where \u03b1 \u2208 (0, 1) is a hyperparameter and where Cv(X) = \u2211 x\u2208X w(v, x).", "startOffset": 201, "endOffset": 205}, {"referenceID": 24, "context": "Another widely used graph-based function is the facility location function [106, 26, 113, 45] f(X) = \u2211 v\u2208V maxx\u2208X w(x, v), the maximization of which is related to the k-median problem [7, 75].", "startOffset": 75, "endOffset": 93}, {"referenceID": 108, "context": "Another widely used graph-based function is the facility location function [106, 26, 113, 45] f(X) = \u2211 v\u2208V maxx\u2208X w(x, v), the maximization of which is related to the k-median problem [7, 75].", "startOffset": 75, "endOffset": 93}, {"referenceID": 43, "context": "Another widely used graph-based function is the facility location function [106, 26, 113, 45] f(X) = \u2211 v\u2208V maxx\u2208X w(x, v), the maximization of which is related to the k-median problem [7, 75].", "startOffset": 75, "endOffset": 93}, {"referenceID": 5, "context": "Another widely used graph-based function is the facility location function [106, 26, 113, 45] f(X) = \u2211 v\u2208V maxx\u2208X w(x, v), the maximization of which is related to the k-median problem [7, 75].", "startOffset": 184, "endOffset": 191}, {"referenceID": 72, "context": "Another widely used graph-based function is the facility location function [106, 26, 113, 45] f(X) = \u2211 v\u2208V maxx\u2208X w(x, v), the maximization of which is related to the k-median problem [7, 75].", "startOffset": 184, "endOffset": 191}, {"referenceID": 89, "context": "It is also useful and learn conic mixtures of graph based functions as done in [92].", "startOffset": 79, "endOffset": 83}, {"referenceID": 49, "context": "An advantage of graph-based submodular functions is that they can be instantiated very easily, using only a similarity score between two objects v1, v2 \u2208 V that does not require metricity or any property (such as non-negative definiteness of the associated matrix, required for using a determinantal point process (DPP) [51, 82, 48, 1, 49] other than non-negativity.", "startOffset": 320, "endOffset": 339}, {"referenceID": 79, "context": "An advantage of graph-based submodular functions is that they can be instantiated very easily, using only a similarity score between two objects v1, v2 \u2208 V that does not require metricity or any property (such as non-negative definiteness of the associated matrix, required for using a determinantal point process (DPP) [51, 82, 48, 1, 49] other than non-negativity.", "startOffset": 320, "endOffset": 339}, {"referenceID": 46, "context": "An advantage of graph-based submodular functions is that they can be instantiated very easily, using only a similarity score between two objects v1, v2 \u2208 V that does not require metricity or any property (such as non-negative definiteness of the associated matrix, required for using a determinantal point process (DPP) [51, 82, 48, 1, 49] other than non-negativity.", "startOffset": 320, "endOffset": 339}, {"referenceID": 0, "context": "An advantage of graph-based submodular functions is that they can be instantiated very easily, using only a similarity score between two objects v1, v2 \u2208 V that does not require metricity or any property (such as non-negative definiteness of the associated matrix, required for using a determinantal point process (DPP) [51, 82, 48, 1, 49] other than non-negativity.", "startOffset": 320, "endOffset": 339}, {"referenceID": 47, "context": "An advantage of graph-based submodular functions is that they can be instantiated very easily, using only a similarity score between two objects v1, v2 \u2208 V that does not require metricity or any property (such as non-negative definiteness of the associated matrix, required for using a determinantal point process (DPP) [51, 82, 48, 1, 49] other than non-negativity.", "startOffset": 320, "endOffset": 339}, {"referenceID": 23, "context": "A drawback of graph-based functions is that building a graph over n samples has complexity O(n) as has querying the function itself, something that does not scale to very large ground set sizes (although there are many approaches to more efficient sparse graph construction [25, 69, 25, 120, 153, 162] to improve upon this complexity).", "startOffset": 274, "endOffset": 301}, {"referenceID": 66, "context": "A drawback of graph-based functions is that building a graph over n samples has complexity O(n) as has querying the function itself, something that does not scale to very large ground set sizes (although there are many approaches to more efficient sparse graph construction [25, 69, 25, 120, 153, 162] to improve upon this complexity).", "startOffset": 274, "endOffset": 301}, {"referenceID": 23, "context": "A drawback of graph-based functions is that building a graph over n samples has complexity O(n) as has querying the function itself, something that does not scale to very large ground set sizes (although there are many approaches to more efficient sparse graph construction [25, 69, 25, 120, 153, 162] to improve upon this complexity).", "startOffset": 274, "endOffset": 301}, {"referenceID": 114, "context": "A drawback of graph-based functions is that building a graph over n samples has complexity O(n) as has querying the function itself, something that does not scale to very large ground set sizes (although there are many approaches to more efficient sparse graph construction [25, 69, 25, 120, 153, 162] to improve upon this complexity).", "startOffset": 274, "endOffset": 301}, {"referenceID": 146, "context": "A drawback of graph-based functions is that building a graph over n samples has complexity O(n) as has querying the function itself, something that does not scale to very large ground set sizes (although there are many approaches to more efficient sparse graph construction [25, 69, 25, 120, 153, 162] to improve upon this complexity).", "startOffset": 274, "endOffset": 301}, {"referenceID": 154, "context": "A drawback of graph-based functions is that building a graph over n samples has complexity O(n) as has querying the function itself, something that does not scale to very large ground set sizes (although there are many approaches to more efficient sparse graph construction [25, 69, 25, 120, 153, 162] to improve upon this complexity).", "startOffset": 274, "endOffset": 301}, {"referenceID": 89, "context": "For machine learning applications, moreover, it is difficult with these functions to train on a training set that may generalize to a test set [92].", "startOffset": 143, "endOffset": 147}, {"referenceID": 134, "context": "A class of submodular functions [141] used in machine learning are the so-called \u201cdecomposable functions.", "startOffset": 32, "endOffset": 37}, {"referenceID": 45, "context": "This class of functions is known to be submodular [47, 46, 141].", "startOffset": 50, "endOffset": 63}, {"referenceID": 44, "context": "This class of functions is known to be submodular [47, 46, 141].", "startOffset": 50, "endOffset": 63}, {"referenceID": 134, "context": "This class of functions is known to be submodular [47, 46, 141].", "startOffset": 50, "endOffset": 63}, {"referenceID": 83, "context": "While such functions have been called \u201cdecomposable\u201d in the past, in this work we will refer to this class of functions as \u201cSums of Concave over non-negative Modular plus Modular\u201d (or SCMMs) in order to avoid confusion with the term \u201cdecomposable\u201d used to describe certain graphical models [86, 53].", "startOffset": 290, "endOffset": 298}, {"referenceID": 51, "context": "While such functions have been called \u201cdecomposable\u201d in the past, in this work we will refer to this class of functions as \u201cSums of Concave over non-negative Modular plus Modular\u201d (or SCMMs) in order to avoid confusion with the term \u201cdecomposable\u201d used to describe certain graphical models [86, 53].", "startOffset": 290, "endOffset": 298}, {"referenceID": 134, "context": "2 SCMMs have been shown to be quite flexible [141], being able to represent a surprisingly diverse set of functions.", "startOffset": 45, "endOffset": 50}, {"referenceID": 69, "context": "It is shown in [72] that any SCMM can be represented with a graph cut function that might optionally utilize additional auxiliary variables that are first minimized over.", "startOffset": 15, "endOffset": 19}, {"referenceID": 60, "context": "SCMMs can represent other functions as well, such as multiclass queuing system functions [63, 142], functions of the form f(A) = m1(A)\u03c6(m2(A)) where m1,m2 : V \u2192 R+ are both non-negative modular functions, and \u03c6 : R \u2192 R is a non-increasing concave function.", "startOffset": 89, "endOffset": 98}, {"referenceID": 135, "context": "SCMMs can represent other functions as well, such as multiclass queuing system functions [63, 142], functions of the form f(A) = m1(A)\u03c6(m2(A)) where m1,m2 : V \u2192 R+ are both non-negative modular functions, and \u03c6 : R \u2192 R is a non-increasing concave function.", "startOffset": 89, "endOffset": 98}, {"referenceID": 37, "context": "Another useful instance is the probabilistic coverage function [39] where we have a set of topics, indexed by i, and V is a set of documents.", "startOffset": 63, "endOffset": 67}, {"referenceID": 134, "context": "Because of their particular form, however, SCMMs yield efficient algorithms for fast minimization [141, 70, 117].", "startOffset": 98, "endOffset": 112}, {"referenceID": 67, "context": "Because of their particular form, however, SCMMs yield efficient algorithms for fast minimization [141, 70, 117].", "startOffset": 98, "endOffset": 112}, {"referenceID": 112, "context": "Because of their particular form, however, SCMMs yield efficient algorithms for fast minimization [141, 70, 117].", "startOffset": 98, "endOffset": 112}, {"referenceID": 29, "context": "Moreover, it appears that there is little loss of generality in handling the nonmonotonicty separately from the polymatroidality, as any non-monotone submodular function can easily be written as a sum of a totally normalized polymatroid function plus a modular function [31, 30].", "startOffset": 270, "endOffset": 278}, {"referenceID": 28, "context": "Moreover, it appears that there is little loss of generality in handling the nonmonotonicty separately from the polymatroidality, as any non-monotone submodular function can easily be written as a sum of a totally normalized polymatroid function plus a modular function [31, 30].", "startOffset": 270, "endOffset": 278}, {"referenceID": 83, "context": "To see 2In fact, the notion of decomposition used in [86, 53], the graphical models community, and related to the notion of the same name used in [31], can also be used to describe a form of decomposability of a submodular function in that the submodular function may be expressed as a sum of terms each one of which corresponds to a clique in a graph, and where the graph is triangulated, but where the terms need not be a concave composed with a modular function.", "startOffset": 53, "endOffset": 61}, {"referenceID": 51, "context": "To see 2In fact, the notion of decomposition used in [86, 53], the graphical models community, and related to the notion of the same name used in [31], can also be used to describe a form of decomposability of a submodular function in that the submodular function may be expressed as a sum of terms each one of which corresponds to a clique in a graph, and where the graph is triangulated, but where the terms need not be a concave composed with a modular function.", "startOffset": 53, "endOffset": 61}, {"referenceID": 29, "context": "To see 2In fact, the notion of decomposition used in [86, 53], the graphical models community, and related to the notion of the same name used in [31], can also be used to describe a form of decomposability of a submodular function in that the submodular function may be expressed as a sum of terms each one of which corresponds to a clique in a graph, and where the graph is triangulated, but where the terms need not be a concave composed with a modular function.", "startOffset": 146, "endOffset": 150}, {"referenceID": 156, "context": "Feature engineering is the study of techniques for transforming raw data objects into feature vectors and is an important step for many machine learning [164, 156, 20] and structured prediction problems [146].", "startOffset": 153, "endOffset": 167}, {"referenceID": 18, "context": "Feature engineering is the study of techniques for transforming raw data objects into feature vectors and is an important step for many machine learning [164, 156, 20] and structured prediction problems [146].", "startOffset": 153, "endOffset": 167}, {"referenceID": 139, "context": "Feature engineering is the study of techniques for transforming raw data objects into feature vectors and is an important step for many machine learning [164, 156, 20] and structured prediction problems [146].", "startOffset": 203, "endOffset": 208}, {"referenceID": 153, "context": ", parse-based features [161, 101, 123]) and unsupervised features such as n-gram and word distribution features (e.", "startOffset": 23, "endOffset": 38}, {"referenceID": 98, "context": ", parse-based features [161, 101, 123]) and unsupervised features such as n-gram and word distribution features (e.", "startOffset": 23, "endOffset": 38}, {"referenceID": 117, "context": ", parse-based features [161, 101, 123]) and unsupervised features such as n-gram and word distribution features (e.", "startOffset": 23, "endOffset": 38}, {"referenceID": 149, "context": ", [157, 17, 12, 84, 124]) are available.", "startOffset": 2, "endOffset": 24}, {"referenceID": 15, "context": ", [157, 17, 12, 84, 124]) are available.", "startOffset": 2, "endOffset": 24}, {"referenceID": 10, "context": ", [157, 17, 12, 84, 124]) are available.", "startOffset": 2, "endOffset": 24}, {"referenceID": 81, "context": ", [157, 17, 12, 84, 124]) are available.", "startOffset": 2, "endOffset": 24}, {"referenceID": 118, "context": ", [157, 17, 12, 84, 124]) are available.", "startOffset": 2, "endOffset": 24}, {"referenceID": 42, "context": ", [44, 158, 90, 115, 29, 147, 35]).", "startOffset": 2, "endOffset": 33}, {"referenceID": 150, "context": ", [44, 158, 90, 115, 29, 147, 35]).", "startOffset": 2, "endOffset": 33}, {"referenceID": 87, "context": ", [44, 158, 90, 115, 29, 147, 35]).", "startOffset": 2, "endOffset": 33}, {"referenceID": 110, "context": ", [44, 158, 90, 115, 29, 147, 35]).", "startOffset": 2, "endOffset": 33}, {"referenceID": 27, "context": ", [44, 158, 90, 115, 29, 147, 35]).", "startOffset": 2, "endOffset": 33}, {"referenceID": 140, "context": ", [44, 158, 90, 115, 29, 147, 35]).", "startOffset": 2, "endOffset": 33}, {"referenceID": 33, "context": ", [44, 158, 90, 115, 29, 147, 35]).", "startOffset": 2, "endOffset": 33}, {"referenceID": 144, "context": ", [151, 104, 108, 122, 73, 97]) \u2014 this is essentially the main message in the name ICLR (International Conference on Learning Representations), one of the main venues for deep model research today.", "startOffset": 2, "endOffset": 30}, {"referenceID": 101, "context": ", [151, 104, 108, 122, 73, 97]) \u2014 this is essentially the main message in the name ICLR (International Conference on Learning Representations), one of the main venues for deep model research today.", "startOffset": 2, "endOffset": 30}, {"referenceID": 104, "context": ", [151, 104, 108, 122, 73, 97]) \u2014 this is essentially the main message in the name ICLR (International Conference on Learning Representations), one of the main venues for deep model research today.", "startOffset": 2, "endOffset": 30}, {"referenceID": 116, "context": ", [151, 104, 108, 122, 73, 97]) \u2014 this is essentially the main message in the name ICLR (International Conference on Learning Representations), one of the main venues for deep model research today.", "startOffset": 2, "endOffset": 30}, {"referenceID": 70, "context": ", [151, 104, 108, 122, 73, 97]) \u2014 this is essentially the main message in the name ICLR (International Conference on Learning Representations), one of the main venues for deep model research today.", "startOffset": 2, "endOffset": 30}, {"referenceID": 94, "context": ", [151, 104, 108, 122, 73, 97]) \u2014 this is essentially the main message in the name ICLR (International Conference on Learning Representations), one of the main venues for deep model research today.", "startOffset": 2, "endOffset": 30}, {"referenceID": 129, "context": "Alternatively, defining g(X) , logm(X)\u2212D(p||{m\u0304u(X)}) = \u2211 u\u2208U pu log(mu(X)) as done in [136], we have a submodular function g that represents a combination of its quantity of X via its features (i.", "startOffset": 87, "endOffset": 92}, {"referenceID": 130, "context": "The KL-divergence can be generalized in various ways, one of which is known as the f -divergence, or in particular the \u03b1-divergence [137, 3].", "startOffset": 132, "endOffset": 140}, {"referenceID": 2, "context": "The KL-divergence can be generalized in various ways, one of which is known as the f -divergence, or in particular the \u03b1-divergence [137, 3].", "startOffset": 132, "endOffset": 140}, {"referenceID": 71, "context": "Using the reparameteriation \u03b1 = 1\u2212 2\u03b4 [74], the \u03b1-divergence (or now \u03b4-divergence [165]) can be expressed as", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "For \u03b4 \u2208 (0, 1) we see that the optimization problem minX\u2286V :m(X)\u2264bD\u03b4(p, p\u0304(X)) where b is a budget constraint is the same as the constrained submodular maximization problem maxX\u2286V :m(X)\u2264b g(X) where g(X) = \u2211 u\u2208U p \u03b4 u(mu(X)) 1\u2212\u03b4 is a feature-based function since \u03c6u(\u03b1) = \u03b11\u2212\u03b4 is concave on \u03b1 \u2208 [0, 1] for \u03b4 \u2208 (0, 1).", "startOffset": 294, "endOffset": 300}, {"referenceID": 3, "context": "Examples include the following: (1) the power functions, such as \u03c6(\u03b1) = \u03b11\u2212\u03b4 that we just encountered (\u03b4 = 1/2 in Figures 1 (I)-(IV)); (2) the other non-saturating nonlinearities such as \u03c6(x) = \u03bd\u22121(x) where \u03bd(y) = y/3 + y [4] and the log functions \u03c6\u03b3(\u03b1) = \u03b3 log(1 + \u03b1/\u03b3) with \u03b3 > 0 is a parameter; (3) the saturating functions such as \u03c6(\u03b1) = 1 \u2212 exp(\u2212\u03b1), the logistic function \u03c6(\u03b1) = 1/(1 + exp(\u2212\u03b1)) and other \u201cs\u201d-shaped sigmoids (which are concave over the non-negative reals) such as the hyperbolic tangent, or \u03c6(\u03b1) = [ 1\u2212 1 ln(b) ln ( 1 + exp ( \u2212\u03b1 ln(b) ))] as used in [18, 78]; (4) and the hard truncation functions such as \u03c6(\u03b1) = min(\u03b1, \u03b3) for some constant \u03b3.", "startOffset": 222, "endOffset": 225}, {"referenceID": 16, "context": "Examples include the following: (1) the power functions, such as \u03c6(\u03b1) = \u03b11\u2212\u03b4 that we just encountered (\u03b4 = 1/2 in Figures 1 (I)-(IV)); (2) the other non-saturating nonlinearities such as \u03c6(x) = \u03bd\u22121(x) where \u03bd(y) = y/3 + y [4] and the log functions \u03c6\u03b3(\u03b1) = \u03b3 log(1 + \u03b1/\u03b3) with \u03b3 > 0 is a parameter; (3) the saturating functions such as \u03c6(\u03b1) = 1 \u2212 exp(\u2212\u03b1), the logistic function \u03c6(\u03b1) = 1/(1 + exp(\u2212\u03b1)) and other \u201cs\u201d-shaped sigmoids (which are concave over the non-negative reals) such as the hyperbolic tangent, or \u03c6(\u03b1) = [ 1\u2212 1 ln(b) ln ( 1 + exp ( \u2212\u03b1 ln(b) ))] as used in [18, 78]; (4) and the hard truncation functions such as \u03c6(\u03b1) = min(\u03b1, \u03b3) for some constant \u03b3.", "startOffset": 572, "endOffset": 580}, {"referenceID": 75, "context": "Examples include the following: (1) the power functions, such as \u03c6(\u03b1) = \u03b11\u2212\u03b4 that we just encountered (\u03b4 = 1/2 in Figures 1 (I)-(IV)); (2) the other non-saturating nonlinearities such as \u03c6(x) = \u03bd\u22121(x) where \u03bd(y) = y/3 + y [4] and the log functions \u03c6\u03b3(\u03b1) = \u03b3 log(1 + \u03b1/\u03b3) with \u03b3 > 0 is a parameter; (3) the saturating functions such as \u03c6(\u03b1) = 1 \u2212 exp(\u2212\u03b1), the logistic function \u03c6(\u03b1) = 1/(1 + exp(\u2212\u03b1)) and other \u201cs\u201d-shaped sigmoids (which are concave over the non-negative reals) such as the hyperbolic tangent, or \u03c6(\u03b1) = [ 1\u2212 1 ln(b) ln ( 1 + exp ( \u2212\u03b1 ln(b) ))] as used in [18, 78]; (4) and the hard truncation functions such as \u03c6(\u03b1) = min(\u03b1, \u03b3) for some constant \u03b3.", "startOffset": 572, "endOffset": 580}, {"referenceID": 148, "context": "Feature based submodular functions, in particular, have been useful for tasks in speech recognition [155], machine translation [78], and computer vision [71].", "startOffset": 100, "endOffset": 105}, {"referenceID": 75, "context": "Feature based submodular functions, in particular, have been useful for tasks in speech recognition [155], machine translation [78], and computer vision [71].", "startOffset": 127, "endOffset": 131}, {"referenceID": 68, "context": "Feature based submodular functions, in particular, have been useful for tasks in speech recognition [155], machine translation [78], and computer vision [71].", "startOffset": 153, "endOffset": 157}, {"referenceID": 5, "context": "Finally, unlike the facility location and other graph-based functions, feature-based functions do not require the use of the entire ground set for each evaluation and hence are appropriate for streaming algorithms [7, 23] where future ground elements are unavailable at the time one needs a function evaluation, as well as parallel submodular optimization [107, 13, 14].", "startOffset": 214, "endOffset": 221}, {"referenceID": 21, "context": "Finally, unlike the facility location and other graph-based functions, feature-based functions do not require the use of the entire ground set for each evaluation and hence are appropriate for streaming algorithms [7, 23] where future ground elements are unavailable at the time one needs a function evaluation, as well as parallel submodular optimization [107, 13, 14].", "startOffset": 214, "endOffset": 221}, {"referenceID": 103, "context": "Finally, unlike the facility location and other graph-based functions, feature-based functions do not require the use of the entire ground set for each evaluation and hence are appropriate for streaming algorithms [7, 23] where future ground elements are unavailable at the time one needs a function evaluation, as well as parallel submodular optimization [107, 13, 14].", "startOffset": 356, "endOffset": 369}, {"referenceID": 11, "context": "Finally, unlike the facility location and other graph-based functions, feature-based functions do not require the use of the entire ground set for each evaluation and hence are appropriate for streaming algorithms [7, 23] where future ground elements are unavailable at the time one needs a function evaluation, as well as parallel submodular optimization [107, 13, 14].", "startOffset": 356, "endOffset": 369}, {"referenceID": 12, "context": "Finally, unlike the facility location and other graph-based functions, feature-based functions do not require the use of the entire ground set for each evaluation and hence are appropriate for streaming algorithms [7, 23] where future ground elements are unavailable at the time one needs a function evaluation, as well as parallel submodular optimization [107, 13, 14].", "startOffset": 356, "endOffset": 369}, {"referenceID": 102, "context": ", WordNet [105]), or a visual hierarchy in computer vision (e.", "startOffset": 10, "endOffset": 15}, {"referenceID": 32, "context": ", ImageNet [34]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 90, "context": "Submodularity follows since a composition of a monotone non-decreasing function h and a monotone non-decreasing concave function \u03c6 (g(\u00b7) = \u03c6(h(\u00b7))) is submodular (Theorem 1 in [93] and repeated, with proof, in Theorem 5.", "startOffset": 176, "endOffset": 180}, {"referenceID": 29, "context": "As mentioned above, from the perspective of defining a submodular function, there is no loss of generality by adding the final modular function m\u00b1 to a polymatroid function [31, 30].", "startOffset": 173, "endOffset": 181}, {"referenceID": 28, "context": "As mentioned above, from the perspective of defining a submodular function, there is no loss of generality by adding the final modular function m\u00b1 to a polymatroid function [31, 30].", "startOffset": 173, "endOffset": 181}, {"referenceID": 13, "context": "For example, a one-layer DSF must construct a valuation over a set of objects from a large number of low-level features which can lead to fewer opportunities for feature sharing while a deeper network fosters distributed representations, also analogous to DNNs [15, 16].", "startOffset": 261, "endOffset": 269}, {"referenceID": 14, "context": "For example, a one-layer DSF must construct a valuation over a set of objects from a large number of low-level features which can lead to fewer opportunities for feature sharing while a deeper network fosters distributed representations, also analogous to DNNs [15, 16].", "startOffset": 261, "endOffset": 269}, {"referenceID": 92, "context": "In one instance [95], a square root was applied to a subset of the right hand nodes in a bipartite neighborhood function in order to offer reduced cost for these nodes being indirectly selected in the graph.", "startOffset": 16, "endOffset": 20}, {"referenceID": 148, "context": "In [155] a two-layer DSF was used to introduce higher-level interaction between features, an act that yielded benefits in speech data summarization.", "startOffset": 3, "endOffset": 8}, {"referenceID": 50, "context": "3, have been used to show worst case performance of various constrained submodular minimization problems [52, 145, 66].", "startOffset": 105, "endOffset": 118}, {"referenceID": 138, "context": "3, have been used to show worst case performance of various constrained submodular minimization problems [52, 145, 66].", "startOffset": 105, "endOffset": 118}, {"referenceID": 63, "context": "3, have been used to show worst case performance of various constrained submodular minimization problems [52, 145, 66].", "startOffset": 105, "endOffset": 118}, {"referenceID": 68, "context": ", [71]) to formulate submodular functions from concave functions that have an initial linear part followed by either a saturation or by a smooth concave part.", "startOffset": 2, "endOffset": 6}, {"referenceID": 121, "context": "The superdifferential of \u03c6 at x is the set of vectors defined as follows: \u2202\u03c6(x) = {s \u2208 R : f(y)\u2212 f(x) \u2264 \u3008s, y \u2212 x\u3009,\u2200y \u2208 R} (12) The superdifferential of a concave function is guaranteed always to exist [128, 129, 60, 114].", "startOffset": 202, "endOffset": 221}, {"referenceID": 122, "context": "The superdifferential of \u03c6 at x is the set of vectors defined as follows: \u2202\u03c6(x) = {s \u2208 R : f(y)\u2212 f(x) \u2264 \u3008s, y \u2212 x\u3009,\u2200y \u2208 R} (12) The superdifferential of a concave function is guaranteed always to exist [128, 129, 60, 114].", "startOffset": 202, "endOffset": 221}, {"referenceID": 57, "context": "The superdifferential of \u03c6 at x is the set of vectors defined as follows: \u2202\u03c6(x) = {s \u2208 R : f(y)\u2212 f(x) \u2264 \u3008s, y \u2212 x\u3009,\u2200y \u2208 R} (12) The superdifferential of a concave function is guaranteed always to exist [128, 129, 60, 114].", "startOffset": 202, "endOffset": 221}, {"referenceID": 109, "context": "The superdifferential of \u03c6 at x is the set of vectors defined as follows: \u2202\u03c6(x) = {s \u2208 R : f(y)\u2212 f(x) \u2264 \u3008s, y \u2212 x\u3009,\u2200y \u2208 R} (12) The superdifferential of a concave function is guaranteed always to exist [128, 129, 60, 114].", "startOffset": 202, "endOffset": 221}, {"referenceID": 109, "context": "13, page 54, [114]).", "startOffset": 13, "endOffset": 18}, {"referenceID": 111, "context": "A function is concave if and only if it is continuous and midpoint concave [116] (or midconcave [127]), defined as for any x, y \u2208 R f((x+ y)/2) \u2265 (f(x) + f(y))/2).", "startOffset": 75, "endOffset": 80}, {"referenceID": 90, "context": "We next restate Theorem 1 from [93] but also provide a proof which was missing.", "startOffset": 31, "endOffset": 35}, {"referenceID": 57, "context": "From the monotonicity of the supergradient [60, 114], we always have dmin a , min d\u2208\u2202\u03c6(a) d \u2265 dab \u2265 max d\u2208\u2202\u03c6(b) d , dmax b (24)", "startOffset": 43, "endOffset": 52}, {"referenceID": 109, "context": "From the monotonicity of the supergradient [60, 114], we always have dmin a , min d\u2208\u2202\u03c6(a) d \u2265 dab \u2265 max d\u2208\u2202\u03c6(b) d , dmax b (24)", "startOffset": 43, "endOffset": 52}, {"referenceID": 134, "context": "9 (Sums of Modular Truncations [141]).", "startOffset": 31, "endOffset": 36}, {"referenceID": 17, "context": "2 Antitone Maps and Superdifferentials Thanks to concave composition closure rules [19], the root function \u03c8r(x) : R \u2192 R in Eqn.", "startOffset": 83, "endOffset": 87}, {"referenceID": 57, "context": "4 when k = 1 \u2014 this is because \u03c6 : R\u2192 R being concave is, in the univariate case, synonymous with it having an antitone superdifferential (which is synonymous with monotone supergradients [60, 114]).", "startOffset": 188, "endOffset": 197}, {"referenceID": 109, "context": "4 when k = 1 \u2014 this is because \u03c6 : R\u2192 R being concave is, in the univariate case, synonymous with it having an antitone superdifferential (which is synonymous with monotone supergradients [60, 114]).", "startOffset": 188, "endOffset": 197}, {"referenceID": 145, "context": "Consider the following concave extension of a monotone non-decreasing submodular function [152, 113, 45], \u03c8(x) = minS\u2286V [f(S) + \u2211 v\u2208V x(v)f(v|S)].", "startOffset": 90, "endOffset": 104}, {"referenceID": 108, "context": "Consider the following concave extension of a monotone non-decreasing submodular function [152, 113, 45], \u03c8(x) = minS\u2286V [f(S) + \u2211 v\u2208V x(v)f(v|S)].", "startOffset": 90, "endOffset": 104}, {"referenceID": 43, "context": "Consider the following concave extension of a monotone non-decreasing submodular function [152, 113, 45], \u03c8(x) = minS\u2286V [f(S) + \u2211 v\u2208V x(v)f(v|S)].", "startOffset": 90, "endOffset": 104}, {"referenceID": 145, "context": "This function is concave and is tight f(A) = \u03c8(1A),\u2200A at the vertices of the unit hypercube, but is not the concave closure of f [152].", "startOffset": 129, "endOffset": 134}, {"referenceID": 36, "context": "In fact, the following weaker sufficient condition for submodularity (an old result, going back more than a hundred years [5, 38, 132, 99, 133, 148, 149]) is well established: Theorem 5.", "startOffset": 122, "endOffset": 153}, {"referenceID": 125, "context": "In fact, the following weaker sufficient condition for submodularity (an old result, going back more than a hundred years [5, 38, 132, 99, 133, 148, 149]) is well established: Theorem 5.", "startOffset": 122, "endOffset": 153}, {"referenceID": 96, "context": "In fact, the following weaker sufficient condition for submodularity (an old result, going back more than a hundred years [5, 38, 132, 99, 133, 148, 149]) is well established: Theorem 5.", "startOffset": 122, "endOffset": 153}, {"referenceID": 126, "context": "In fact, the following weaker sufficient condition for submodularity (an old result, going back more than a hundred years [5, 38, 132, 99, 133, 148, 149]) is well established: Theorem 5.", "startOffset": 122, "endOffset": 153}, {"referenceID": 141, "context": "In fact, the following weaker sufficient condition for submodularity (an old result, going back more than a hundred years [5, 38, 132, 99, 133, 148, 149]) is well established: Theorem 5.", "startOffset": 122, "endOffset": 153}, {"referenceID": 142, "context": "In fact, the following weaker sufficient condition for submodularity (an old result, going back more than a hundred years [5, 38, 132, 99, 133, 148, 149]) is well established: Theorem 5.", "startOffset": 122, "endOffset": 153}, {"referenceID": 121, "context": "Not only does an antitone map alone not ensure concavity (a result established originally in [128, 129]), an antitone map need not be a gradient field (a property that, if true, would make it a conservative field).", "startOffset": 93, "endOffset": 103}, {"referenceID": 122, "context": "Not only does an antitone map alone not ensure concavity (a result established originally in [128, 129]), an antitone map need not be a gradient field (a property that, if true, would make it a conservative field).", "startOffset": 93, "endOffset": 103}, {"referenceID": 113, "context": "For an example related to submodular functions, the multilinear extension [119], defined as:", "startOffset": 74, "endOffset": 79}, {"referenceID": 39, "context": "It has been used as a extension of a submodular function, surrogate to the true concave envelope, for use in submodular maximization problems [41, 24, 8].", "startOffset": 142, "endOffset": 153}, {"referenceID": 22, "context": "It has been used as a extension of a submodular function, surrogate to the true concave envelope, for use in submodular maximization problems [41, 24, 8].", "startOffset": 142, "endOffset": 153}, {"referenceID": 6, "context": "It has been used as a extension of a submodular function, surrogate to the true concave envelope, for use in submodular maximization problems [41, 24, 8].", "startOffset": 142, "endOffset": 153}, {"referenceID": 26, "context": "Also, any function defined only on the vertices of the unit hypercube has an infinite number of both concave and convex extensions [28].", "startOffset": 131, "endOffset": 135}, {"referenceID": 124, "context": "15 does not require concavity, however, this suggests that there may be a way to define submodular functions using generalized line integrals of antitone maps without needing concavity [131].", "startOffset": 185, "endOffset": 190}, {"referenceID": 36, "context": "15 is typically stated as both necessary and sufficient conditions for submodularity [38, 132, 133, 148, 149], as it is used to define submodularity on those lattices, including the reals (and hence this is sometimes called continuous submodularity), where twice differentiability everywhere is well defined.", "startOffset": 85, "endOffset": 109}, {"referenceID": 125, "context": "15 is typically stated as both necessary and sufficient conditions for submodularity [38, 132, 133, 148, 149], as it is used to define submodularity on those lattices, including the reals (and hence this is sometimes called continuous submodularity), where twice differentiability everywhere is well defined.", "startOffset": 85, "endOffset": 109}, {"referenceID": 126, "context": "15 is typically stated as both necessary and sufficient conditions for submodularity [38, 132, 133, 148, 149], as it is used to define submodularity on those lattices, including the reals (and hence this is sometimes called continuous submodularity), where twice differentiability everywhere is well defined.", "startOffset": 85, "endOffset": 109}, {"referenceID": 141, "context": "15 is typically stated as both necessary and sufficient conditions for submodularity [38, 132, 133, 148, 149], as it is used to define submodularity on those lattices, including the reals (and hence this is sometimes called continuous submodularity), where twice differentiability everywhere is well defined.", "startOffset": 85, "endOffset": 109}, {"referenceID": 142, "context": "15 is typically stated as both necessary and sufficient conditions for submodularity [38, 132, 133, 148, 149], as it is used to define submodularity on those lattices, including the reals (and hence this is sometimes called continuous submodularity), where twice differentiability everywhere is well defined.", "startOffset": 85, "endOffset": 109}, {"referenceID": 145, "context": "Ordinarily the concave closure of a submodular function is computationally hard to evaluate [152] and this is disappointing since such a construct would be useful for relaxation schemes for maximizing submodular functions (and as result surrogates, such as the multilinear extension are used).", "startOffset": 92, "endOffset": 97}, {"referenceID": 44, "context": "A matroid M [46] is a set system M = (V, I) where I = {I1, I2, .", "startOffset": 12, "endOffset": 16}, {"referenceID": 44, "context": "All monotone non-decreasing non-negative integral submodular functions can be exactly represented by grouping and then evaluating grouped ground elements in a matroid [46].", "startOffset": 167, "endOffset": 171}, {"referenceID": 91, "context": "A useful matroid in machine learning applications [94, 9] is the partition matroid, where a partition (V1, V2, .", "startOffset": 50, "endOffset": 57}, {"referenceID": 7, "context": "A useful matroid in machine learning applications [94, 9] is the partition matroid, where a partition (V1, V2, .", "startOffset": 50, "endOffset": 57}, {"referenceID": 50, "context": "Thus, within the family of DSFs lie the truncated matroid rank functions used to show information theoretic hardness for many constrained submodular optimization problems [52], i.", "startOffset": 171, "endOffset": 175}, {"referenceID": 67, "context": "Since this function is used to show hardness for many constrained submodular minimization problems, and since DSFs generalize laminar matroid ranks, this portends poorly for algorithms of the kind found in [70, 117] to achieve fast DSF minimization.", "startOffset": 206, "endOffset": 215}, {"referenceID": 112, "context": "Since this function is used to show hardness for many constrained submodular minimization problems, and since DSFs generalize laminar matroid ranks, this portends poorly for algorithms of the kind found in [70, 117] to achieve fast DSF minimization.", "startOffset": 206, "endOffset": 215}, {"referenceID": 142, "context": "We use the term \u201csurplus\u201d under an interpretation where A is a set of agents that can perform their action either independently of each other, or may perform their actions jointly and cooperatively [149].", "startOffset": 198, "endOffset": 203}, {"referenceID": 99, "context": "For the entropy function, this idea was first defined in [102].", "startOffset": 57, "endOffset": 62}, {"referenceID": 147, "context": "4 Absolute redundancy is also called \u201ctotal correlation\u201d [154] and also the \u201cmulti-information\u201d function [143].", "startOffset": 57, "endOffset": 62}, {"referenceID": 136, "context": "4 Absolute redundancy is also called \u201ctotal correlation\u201d [154] and also the \u201cmulti-information\u201d function [143].", "startOffset": 105, "endOffset": 110}, {"referenceID": 142, "context": "How fairly to redistribute surplus back to the individual agents is called the \u201csurplus sharing problem\u201d and is studied in [149].", "startOffset": 123, "endOffset": 128}, {"referenceID": 142, "context": "3In [149], surplus is defined as f(A)\u2212 \u2211 a\u2208A f(a) where f is a supermodular function, but the same idea still applies.", "startOffset": 4, "endOffset": 9}, {"referenceID": 99, "context": "4Incidentally, in 1954, [102] was also the first, to the authors knowledge, to provide inequalities on the entropy function that are identical to the submodularity condition.", "startOffset": 24, "endOffset": 29}, {"referenceID": 25, "context": "When f(A) = H(XA) is the entropy function, then the pairwise surplus I (2) f (A;B) is the well-known mutual information [27] between random variable sets XA and XB .", "startOffset": 120, "endOffset": 124}, {"referenceID": 38, "context": "This result is similar to some of the recent results from the DNN literature where it is shown that in some cases, it would require exponentially many hidden units to implement a network with more layers [40].", "startOffset": 204, "endOffset": 208}, {"referenceID": 58, "context": "This is different than standard neural networks where it is shown that even a shallow neural network is a universal approximator [61].", "startOffset": 129, "endOffset": 133}, {"referenceID": 0, "context": "Hence, f\u0302k is like a [0, 1]-normalized laminar matroid rank function with the laminar family of sets Fk = {Vk, Vk1, Vk2, Vk3, Vk11, Vk12, Vk13, Vk21, .", "startOffset": 21, "endOffset": 27}, {"referenceID": 29, "context": "Hence, all members of Fk are totally normalized in this sense [31, 30].", "startOffset": 62, "endOffset": 70}, {"referenceID": 28, "context": "Hence, all members of Fk are totally normalized in this sense [31, 30].", "startOffset": 62, "endOffset": 70}, {"referenceID": 29, "context": "The properties of total normalization [31, 30] will be further useful in the below, so we define functional operators that totally normalize a given function.", "startOffset": 38, "endOffset": 46}, {"referenceID": 28, "context": "The properties of total normalization [31, 30] will be further useful in the below, so we define functional operators that totally normalize a given function.", "startOffset": 38, "endOffset": 46}, {"referenceID": 29, "context": ", (T f)(v|V \\ {v}) = 0), and we have the identity f = T f +Mf , meaning that any submodular function can be decomposed into a totally normalized polymatroid function plus a modular function [31, 30].", "startOffset": 190, "endOffset": 198}, {"referenceID": 28, "context": ", (T f)(v|V \\ {v}) = 0), and we have the identity f = T f +Mf , meaning that any submodular function can be decomposed into a totally normalized polymatroid function plus a modular function [31, 30].", "startOffset": 190, "endOffset": 198}, {"referenceID": 38, "context": "It remains an open question to determine if, when the ground set size is constant and fixed, if DSFk comprises a larger family, or if expressing certain DSFks with k \u2212 1 layers requires an exponential number of hidden units, analogous to [40].", "startOffset": 238, "endOffset": 242}, {"referenceID": 155, "context": "It is also worth noting that in [163] it is shown that the entropy function f(A) = H(XA) when seen as a set function must satisfy inequalities that are not required for an arbitrary polymatroid function, thus implying that entropy also does not comprise all submodular function.", "startOffset": 32, "endOffset": 37}, {"referenceID": 50, "context": "1 Learning DSFs As mentioned in Section 1, recent studies [52, 11, 43, 42] show that learning submodular functions can be easier or harder depending on the learning setting.", "startOffset": 58, "endOffset": 74}, {"referenceID": 9, "context": "1 Learning DSFs As mentioned in Section 1, recent studies [52, 11, 43, 42] show that learning submodular functions can be easier or harder depending on the learning setting.", "startOffset": 58, "endOffset": 74}, {"referenceID": 41, "context": "1 Learning DSFs As mentioned in Section 1, recent studies [52, 11, 43, 42] show that learning submodular functions can be easier or harder depending on the learning setting.", "startOffset": 58, "endOffset": 74}, {"referenceID": 40, "context": "1 Learning DSFs As mentioned in Section 1, recent studies [52, 11, 43, 42] show that learning submodular functions can be easier or harder depending on the learning setting.", "startOffset": 58, "endOffset": 74}, {"referenceID": 73, "context": "A general outline of various learning settings is given in [76, 43] \u2014 here, we give only a very brief overview.", "startOffset": 59, "endOffset": 67}, {"referenceID": 41, "context": "A general outline of various learning settings is given in [76, 43] \u2014 here, we give only a very brief overview.", "startOffset": 59, "endOffset": 67}, {"referenceID": 9, "context": "The distribution Pr might be unknown [11], or might be known (and in such case, might be assumed to be uniform [43, 42]).", "startOffset": 37, "endOffset": 41}, {"referenceID": 41, "context": "The distribution Pr might be unknown [11], or might be known (and in such case, might be assumed to be uniform [43, 42]).", "startOffset": 111, "endOffset": 119}, {"referenceID": 40, "context": "The distribution Pr might be unknown [11], or might be known (and in such case, might be assumed to be uniform [43, 42]).", "startOffset": 111, "endOffset": 119}, {"referenceID": 0, "context": "The quality of learning could be judged over all 2 points or over some fraction, say 1\u2212\u03b2, of the points, for \u03b2 \u2208 [0, 1].", "startOffset": 113, "endOffset": 119}, {"referenceID": 73, "context": "For example, in agnostic learning [76], we acknowledge that it might be difficult to show that learning is good relative to all of F (say due to noise) but still feasible to show that learning is good relative to the best within T .", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "In [11], goodness is judged multiplicatively, meaning for a set A \u2286 V we wish that f\u0303(A) \u2264 f(A) \u2264 g(n)f(A) for some function g(n), and this is typically a probabilistic condition (i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "In the PMAC (probably mostly approximately correct) model [11], we also \u201cmostly\u201d \u03b2 > 0 learn.", "startOffset": 58, "endOffset": 62}, {"referenceID": 133, "context": "Empirically, experiments that learn submodularity for various data science applications [140, 92], has been more successful than simply hand-designing a fixed submodular function.", "startOffset": 88, "endOffset": 97}, {"referenceID": 89, "context": "Empirically, experiments that learn submodularity for various data science applications [140, 92], has been more successful than simply hand-designing a fixed submodular function.", "startOffset": 88, "endOffset": 97}, {"referenceID": 143, "context": "This is true both for image [150] and document [92] summarization tasks.", "startOffset": 28, "endOffset": 33}, {"referenceID": 89, "context": "This is true both for image [150] and document [92] summarization tasks.", "startOffset": 47, "endOffset": 51}, {"referenceID": 89, "context": "There also has been some initial work on learnability bounds in [92].", "startOffset": 64, "endOffset": 68}, {"referenceID": 38, "context": "Even in cases where a DSF can be represented by an SCMM, DSFs may be a far more parsimonious representation of classes of submodular functions and hence a more efficient family over which to learn, analogous to results in DNNs showing the need for exponentially many hidden units for shallow networks to implement a network with more layers [40].", "startOffset": 341, "endOffset": 345}, {"referenceID": 128, "context": "Under this approach, and with an appropriate regularizer, it may be feasible to obtain generalization bounds in some form [135] as is often found in statistical machine learning settings.", "startOffset": 122, "endOffset": 127}, {"referenceID": 56, "context": "Note that, depending on the loss L used, this approach may be tolerant of noisy estimates of the function, where, say, yi = fw(Si) + and where is noise, somewhat analogous to how it is possible to optimize a noisy submodular function [59].", "startOffset": 234, "endOffset": 238}, {"referenceID": 89, "context": "One example is in summarization applications [92, 150] where we wish to learn a submodular function fw that, when maximized subject to a cardinality constraint, produces a set that is valuated highly by the true submodular function relative to other sets of that size.", "startOffset": 45, "endOffset": 54}, {"referenceID": 143, "context": "One example is in summarization applications [92, 150] where we wish to learn a submodular function fw that, when maximized subject to a cardinality constraint, produces a set that is valuated highly by the true submodular function relative to other sets of that size.", "startOffset": 45, "endOffset": 54}, {"referenceID": 0, "context": "More precisely, instead of trying to learn f everywhere, we seek only to learn the parameters w of a function so that if B \u2208 argmaxA\u2286V :|A|\u2264k fw(A), then h(B) \u2265 \u03b1h(A\u2217) for some \u03b1 \u2208 [0, 1] where A\u2217 \u2208 argmaxA\u2286V :|A|\u2264k h(A).", "startOffset": 181, "endOffset": 187}, {"referenceID": 133, "context": "The max-margin approach [140, 92, 150] is appropriate to this problem and is applicable to learning DSFs.", "startOffset": 24, "endOffset": 38}, {"referenceID": 89, "context": "The max-margin approach [140, 92, 150] is appropriate to this problem and is applicable to learning DSFs.", "startOffset": 24, "endOffset": 38}, {"referenceID": 143, "context": "The max-margin approach [140, 92, 150] is appropriate to this problem and is applicable to learning DSFs.", "startOffset": 24, "endOffset": 38}, {"referenceID": 139, "context": "The task of finding the maximizing set is known as loss-augmented inference (LAI) [146, 160], which for general `(A) is", "startOffset": 82, "endOffset": 92}, {"referenceID": 152, "context": "The task of finding the maximizing set is known as loss-augmented inference (LAI) [146, 160], which for general `(A) is", "startOffset": 82, "endOffset": 92}, {"referenceID": 133, "context": "If it is the case that fw(S) is linear in w (such as when w are mixture parameters in an SCMM as was done in [140, 92, 150]), and if the maximization can is done exactly, then this constitutes a convex minimization procedure.", "startOffset": 109, "endOffset": 123}, {"referenceID": 89, "context": "If it is the case that fw(S) is linear in w (such as when w are mixture parameters in an SCMM as was done in [140, 92, 150]), and if the maximization can is done exactly, then this constitutes a convex minimization procedure.", "startOffset": 109, "endOffset": 123}, {"referenceID": 143, "context": "If it is the case that fw(S) is linear in w (such as when w are mixture parameters in an SCMM as was done in [140, 92, 150]), and if the maximization can is done exactly, then this constitutes a convex minimization procedure.", "startOffset": 109, "endOffset": 123}, {"referenceID": 89, "context": "Given a submodular function for the loss, as was done in [92], then the greedy algorithm offers the standard 1 \u2212 1/e approximation guarantee for LAI.", "startOffset": 57, "endOffset": 61}, {"referenceID": 106, "context": "If f\u0303 is submodular, then \u03ba \u2212 f\u0303 is supermodular, and in this case solving maxA\u22082V \\S [f(A) + `(A)] involves maximizing the difference between two submodular functions, and the submodular-supermodular procedure [110, 64] can be used although this procedure does not have guarantees in general.", "startOffset": 211, "endOffset": 220}, {"referenceID": 61, "context": "If f\u0303 is submodular, then \u03ba \u2212 f\u0303 is supermodular, and in this case solving maxA\u22082V \\S [f(A) + `(A)] involves maximizing the difference between two submodular functions, and the submodular-supermodular procedure [110, 64] can be used although this procedure does not have guarantees in general.", "startOffset": 211, "endOffset": 220}, {"referenceID": 115, "context": "For a DSF, this subgradient can be easily computed using backpropagation, similar to the approach of [121].", "startOffset": 101, "endOffset": 106}, {"referenceID": 35, "context": "Preliminary experiments in learning DSFs in this fashion were reported in [37] and show encouraging results.", "startOffset": 74, "endOffset": 78}, {"referenceID": 84, "context": "Given the ongoing research on the non-convex learning of DNNs, which have achieved remarkable results on a plethora of machine learning tasks [87, 54], and given the similarity between DSFs and DNNs, we may leverage the same DNN learning techniques to learn DSFs.", "startOffset": 142, "endOffset": 150}, {"referenceID": 89, "context": "The reason this occurs can be explained using a document summarization example [92].", "startOffset": 79, "endOffset": 83}, {"referenceID": 89, "context": "In this section, we discuss how to addresses this problem for DSFs via a strategy that generalizes [92, 150].", "startOffset": 99, "endOffset": 108}, {"referenceID": 143, "context": "In this section, we discuss how to addresses this problem for DSFs via a strategy that generalizes [92, 150].", "startOffset": 99, "endOffset": 108}, {"referenceID": 89, "context": "This process analogous to the \u201cshells\u201d of [92].", "startOffset": 42, "endOffset": 46}, {"referenceID": 106, "context": "In [110, 64] it was shown that any set function h : 2 \u2192 R can be represented as a difference between two submodular functions.", "startOffset": 3, "endOffset": 12}, {"referenceID": 61, "context": "In [110, 64] it was shown that any set function h : 2 \u2192 R can be represented as a difference between two submodular functions.", "startOffset": 3, "endOffset": 12}, {"referenceID": 63, "context": "For example, after learning, we can utilize submodular level-set constrained submodular optimization of the kind developed in [66] for optimization.", "startOffset": 126, "endOffset": 130}, {"referenceID": 105, "context": "Other ways to generalize submodularity considers discrete generalizations of properties such as midpoint convexity over integer lattices [109].", "startOffset": 137, "endOffset": 142}, {"referenceID": 132, "context": "1 (Simple Bisubmodularity [139]).", "startOffset": 26, "endOffset": 31}, {"referenceID": 119, "context": "3 (Directed Bisubmodularity [125]).", "startOffset": 28, "endOffset": 33}, {"referenceID": 77, "context": "Directed bisubodularity functions have been generalized to what is known as k-submodular functions in [80, 62].", "startOffset": 102, "endOffset": 110}, {"referenceID": 59, "context": "Directed bisubodularity functions have been generalized to what is known as k-submodular functions in [80, 62].", "startOffset": 102, "endOffset": 110}, {"referenceID": 132, "context": "More recently, simple bisubmodularity [139] has been generalized to multivariate submodular functions [134].", "startOffset": 38, "endOffset": 43}, {"referenceID": 127, "context": "More recently, simple bisubmodularity [139] has been generalized to multivariate submodular functions [134].", "startOffset": 102, "endOffset": 107}, {"referenceID": 59, "context": "These are not the same as k-submodular functions [62] but for k = 1 we obtain standard submodular functions and for k = 2 we obtain simple bisubmodular functions.", "startOffset": 49, "endOffset": 53}, {"referenceID": 53, "context": ", [56]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 48, "context": "This section describes a strategy for learning hash functions that utilizes DSFs, the Lov\u00e1sz extension, and the submodular Hamming metric [50].", "startOffset": 138, "endOffset": 142}, {"referenceID": 0, "context": "\u2022 For two arbitrary vectors z1, z2 \u2208 [0, 1] , we can define a relaxed form of metric as follows: d(z1, z2) = f\u0306(z1 + z2 \u2212 2z1 \u2297 z2), and for a DSF, this can be expressed as df\u0306w(z1, z2) = f\u0306w(z1 + z2 \u2212 2z1 \u2297 z2).", "startOffset": 37, "endOffset": 43}, {"referenceID": 0, "context": "\u2022 Let us suppose that h\u0303\u03b8 : R \u2192 [0, 1] is a mapping from real vectors to vectors in the hypercube (e.", "startOffset": 32, "endOffset": 38}, {"referenceID": 34, "context": "An immediate task is to further develop practical strategies for successfully empirically learning DSFs, as was initiated in [36].", "startOffset": 125, "endOffset": 129}, {"referenceID": 155, "context": "And lastly, it remains to compare the DSF family with the family of all entropy functions [163].", "startOffset": 90, "endOffset": 95}, {"referenceID": 34, "context": "Thanks to Brian Dolhansky for helping with building an initial implementation of learning DSFs that was used in [36].", "startOffset": 112, "endOffset": 116}], "year": 2017, "abstractText": "We start with an overview of a class of submodular functions called SCMMs (sums of concave composed with non-negative modular functions plus a final arbitrary modular). We then define a new class of submodular functions we call deep submodular functions or DSFs. We show that DSFs are a flexible parametric family of submodular functions that share many of the properties and advantages of deep neural networks (DNNs), including many-layered hierarchical topologies, representation learning, distributed representations, opportunities and strategies for training, and suitability to GPU-based matrix/vector computing. DSFs can be motivated by considering a hierarchy of descriptive concepts over ground elements and where one wishes to allow submodular interaction throughout this hierarchy. In machine learning and data science applications, where there is often either a natural or an automatically learnt hierarchy of concepts over data, DSFs therefore naturally apply. Results in this paper show that DSFs constitute a strictly larger class of submodular functions than SCMMs, thus justifying their mathematical and practical utility. Moreover, we show that, for any integer k > 0, there are k-layer DSFs that cannot be represented by a k\u2032-layer DSF for any k\u2032 < k. This implies that, like DNNs, there is a utility to depth, but unlike DNNs (which can be universally approximated by shallow networks), the family of DSFs strictly increase with depth. Despite this property, however, we show that DSFs, even with arbitrarily large k, do not comprise all submodular functions. We show this using a technique that \u201cbackpropagates\u201d certain requirements if it was the case that DSFs comprised all submodular functions. In offering the above results, we also define the notion of an antitone superdifferential of a concave function and show how this relates to submodular functions (in general), DSFs (in particular), negative second-order partial derivatives, continuous submodularity, and concave extensions. To further motivate our analysis, we provide various special case results from matroid theory, comparing DSFs with forms of matroid rank, in particular the laminar matroid. Lastly, we discuss strategies to learn DSFs, and define the classes of deep supermodular functions, deep difference of submodular functions, and deep multivariate submodular functions, and discuss where these can be useful in applications.", "creator": "LaTeX with hyperref package"}}}