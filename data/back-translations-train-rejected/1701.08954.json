{"id": "1701.08954", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2017", "title": "CommAI: Evaluating the first steps towards a useful general AI", "abstract": "With machine learning successfully applied to new daunting problems almost every day, general AI starts looking like an attainable goal. However, most current research focuses instead on important but narrow applications, such as image classification or machine translation. We believe this to be largely due to the lack of objective ways to measure progress towards broad machine intelligence. In order to fill this gap, we propose here a set of concrete desiderata for general AI, together with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum.", "histories": [["v1", "Tue, 31 Jan 2017 09:20:17 GMT  (21kb)", "http://arxiv.org/abs/1701.08954v1", "Submitted to ICLR 2017 Workshop Track"], ["v2", "Mon, 27 Mar 2017 18:47:01 GMT  (21kb)", "http://arxiv.org/abs/1701.08954v2", "Published in ICLR 2017 Workshop Track"]], "COMMENTS": "Submitted to ICLR 2017 Workshop Track", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL", "authors": ["marco baroni", "armand joulin", "allan jabri", "germ\\`an kruszewski", "angeliki lazaridou", "klemen simonic", "tomas mikolov"], "accepted": false, "id": "1701.08954"}, "pdf": {"name": "1701.08954.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Allan Jabri", "Germ\u00e1n Kruszewski", "Angeliki Lazaridou", "Klemen Simonic"], "emails": ["mbaroni@fb.com", "ajoulin@fb.com", "ajabri@fb.com", "germank@fb.com", "angelikil@fb.com", "klemen@fb.com", "tmikolov@fb.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 1.08 954v 1 [cs.L G] 31 Jan 2017 Workshop track - ICLR 2017"}, {"heading": "1 DESIDERATA FOR THE EVALUATION OF MACHINE INTELLIGENCE", "text": "This year, the time has come for us to be able to retaliate, to retaliate."}, {"heading": "2 THE COMMAI FRAMEWORK", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "3 RELATED WORK", "text": "We can identify two broad approaches to benchmarking general AI. Some researchers, like us, take a top-down view and derive their requirements from psychological or mathematical considerations (e.g. Adams et al., 2012, Lake et al., 2016, and see the in-depth review in Herna \u0301 ndez-Orallo, 2017). We sympathize with this principled approach, but are unaware that others have emphasized the same practical desiderates we outlined above, and also do not propose a concrete evaluation framework as we do with CommAI-env.Others focus on existing applications that are considered sufficiently complex to measure progress toward universal intelligence. For example, games like Go (Silver et al., 2016) and StarCraft (Ontan o'n et al., 2013) require sophisticated planning capabilities that are intuitively associated with intelligence. While recent results in these areas are impressive, we think this approach is simultaneously too simple and too complex to be a benchmark I."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Gemma Boleda, Stan Dehaene, Emmanuel Dupoux, Jan Feyereisl, Amac Herdag delen, Jose Herna ndez-Orallo, Iasonas Kokkinos, Martin Poliak, Marek Rosa, our FAIR colleagues and the participants of the MAIN @ NIPS 2016 workshop for feedback.2E.g.: https: / / openai.com / blog / universe /, https: / / github.com / deepmind / lab, http: / / www.ggp.org, http: / / www.gvgai.net /"}, {"heading": "SUPPLEMENTARY MATERIALS: THE COMMAI-MINI TASKS", "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "TASK SET #1", "text": "The following examples illustrate Set # 1 (here and below we show only positive examples where the student should answer true): description: C; verify: CCCC.description: AB; verify: ABAB.description: FJG; verify: FJG.Tasks in Set # 1 contain strictly local descriptions. There is a natural hierarchy within the set regarding the length of the n-grams that need to be memorized: Checking the (AB) + language requires less memory than checking the (FJG) + language."}, {"heading": "TASK SET #2", "text": "Examples: Description: anything; Verification: ANFHG.Description: AB or CD; Verification: ABABAB.Description: FAB or GH or MIL; Verification of FABABAB.The tasks in sentence # 2 are also based on purely local descriptions. However, their solution requires the storage of several n-grams in memory due to the operator or operator. Thus, the # 2 tasks imply the skills required to solve # 1 tasks (storing n-grams and verifying their presence in a string), but they generalize them (up to storing and using several n-grams).The # 2 tasks vary in terms of the number of splintered n-grams that include the description and the maximum length of n-grams in the description."}, {"heading": "TASK SET #3", "text": "Examples: Description: AB and CF; Verification: ABCFAB.Description: HL and RM and BT; Verification: RMBTBTHLHLBT.Description: AB and something; Verification: FKGABJKJKJKSD.Description: AB and CF and something; Verification: FJGKJKJKJKJKJKJDCDJKJKJKJJJJAB.Set # 3 tasks include locally verifiable languages. Verification that the target string contains only ngrams from the description is no longer sufficient. The learner must verify that all n-grams in the description have been used. Thus, these tasks generalize # 2 tasks. They also require the storage of several n-grams in memory, but they still need a device to verify that all n-grams have been used in the search table. There is again an obvious hierarchy, as many of them must be stored in the memory, except for the length and whether they must include the resolution of the memory."}, {"heading": "TASK SET #4", "text": "Examples: Description: not AB and anything; Verification: ADFCFHGHADDDB.Description: not AB and CF and anything; Verification: DJFKJKJKJSCFDSFG.Description: not AB and not CF and anything; Verification: DJFKJKJKJKJSCEFDSFG.Tasks in Set # 4 also include locally verifiable languages. In addition to the conjunction, however, they include a negation operator. In our setup, conjunction always takes up more space than negation to avoid the need for an open bracket in the descriptions. The fact that a negated n-gram corresponds to the confirmation of its complement is explicitly expressed by always adding the and everything else condition."}, {"heading": "TASK SET #5", "text": "Examples: Description: C; Product Description: AB; Product Description: FJG; Product Description: anything; Product Description: AB or CD; Product Description: FAB or GH or MIL; Product Description: AB and CF; Product Description: HL and RM and BT; Product Description: AB and anything; Product Description: AB and CF and anything; Product Description: not AB and CF and anything; Product Description: not CF and anything; Product Description: not AB and not CF and anything; Productivity Description: We look at the production contracts of all detection tasks. The learner is asked to create a string that meets the conditions in the description. We expect a compositional learner to complete the production tasks much faster if he has already been exposed to the detection tasks (and vice versa)."}, {"heading": "FURTHER TASKS", "text": "The production tasks in sentence 5 can be solved by always generating the shortest string in the description. In the simpler tasks (without conjunction), this amounts to generating the first uppercase string in the description. We can force the learner out of this strategy by asking him to generate two different strings that correspond to the description, e.g.: Description: C; generating two different strings. Tasks of this kind would obviously build on skills acquired in sentence 5 and add the requirement that the learner stores his own past productions in memory and uses them in planning what is to be produced afterwards. It is easy to imagine other tasks that a learner could quickly solve by using the skills acquired in sentence 1-5, e.g. by changing the capitalization conventions:"}, {"heading": "DESCRIPTION: ab; PRODUCE.", "text": "More ambitious would be to provide the learner with a sample of strings and ask him to formulate a description that accepts them (even extremely loose descriptions, like anything else, would be an impressive achievement)."}], "references": [{"title": "Mapping the landscape of human-level artificial general intelligence", "author": ["Sam Adams", "Itamar Arel", "Joscha Bach", "Robert Coop", "Rod Furlan", "Ben Goertzel", "Storrs Hall", "Alexei Samsonovich", "Matthias Scheutz", "Matthew Schlesinger", "Stuart Shapiro", "John Sowa"], "venue": "AI Magazine,", "citeRegEx": "Adams et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2012}, {"title": "Modular multitask reinforcement learning with policy sketches", "author": ["Jacob Andreas", "Dan Klein", "Sergey Levine"], "venue": null, "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "A survey of robot learning from demonstration", "author": ["Brenna Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "The Compositionality Papers", "author": ["Jerry Fodor", "Ernest Lepore"], "venue": null, "citeRegEx": "Fodor and Lepore.,? \\Q2002\\E", "shortCiteRegEx": "Fodor and Lepore.", "year": 2002}, {"title": "The Measure of All Minds", "author": ["Jos\u00e9 Hern\u00e1ndez-Orallo"], "venue": null, "citeRegEx": "Hern\u00e1ndez.Orallo.,? \\Q2017\\E", "shortCiteRegEx": "Hern\u00e1ndez.Orallo.", "year": 2017}, {"title": "Formal language theory: refining the Chomsky hierarchy", "author": ["Gerhard J\u00e4ger", "James Rogers"], "venue": "Philosophical Transactions of the Royal Society of London B: Biological Sciences,", "citeRegEx": "J\u00e4ger and Rogers.,? \\Q1970\\E", "shortCiteRegEx": "J\u00e4ger and Rogers.", "year": 1970}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Brenden Lake", "Ruslan Salakhutdinov", "Joshua Tenenbaum"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Building machines that learn and think like people", "author": ["Brenden Lake", "Tomer Ullman", "Joshua Tenenbaum", "Samuel Gershman"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "Counter-Free Automata", "author": ["Robert McNaughton", "Seymour Papert"], "venue": null, "citeRegEx": "McNaughton and Papert.,? \\Q1971\\E", "shortCiteRegEx": "McNaughton and Papert.", "year": 1971}, {"title": "A roadmpap towards machine intelligence", "author": ["Tomas Mikolov", "Armand Joulin", "Marco Baroni"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2016}, {"title": "The Society of Mind", "author": ["Marvin Minsky"], "venue": null, "citeRegEx": "Minsky.,? \\Q1986\\E", "shortCiteRegEx": "Minsky.", "year": 1986}, {"title": "A survey of real-time strategy game AI research and competition in StarCraft", "author": ["Santiago Onta\u00f1\u00f3n", "Gabriel Synnaeve", "Alberto Uriarte", "Florian Richoux", "David Churchill", "Mike Preuss"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games,", "citeRegEx": "Onta\u00f1\u00f3n et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Onta\u00f1\u00f3n et al\\.", "year": 2013}, {"title": "Implicit learning of artificial grammars", "author": ["Arthur Reber"], "venue": "Verbal Learning and Verbal Behavior,", "citeRegEx": "Reber.,? \\Q1967\\E", "shortCiteRegEx": "Reber.", "year": 1967}, {"title": "CHILD: A first step towards continual learning", "author": ["Mark Ring"], "venue": "Machine Learning,", "citeRegEx": "Ring.,? \\Q1997\\E", "shortCiteRegEx": "Ring.", "year": 1997}, {"title": "Cognitive and sub-regular complexity", "author": ["James Rogers", "Jeffrey Heinz", "Margaret Fero", "Jeremy Hurst", "Dakotah Lambert", "Sean Wibel"], "venue": "Formal Grammar: 17th and 18th International Conferences,", "citeRegEx": "Rogers et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rogers et al\\.", "year": 2013}, {"title": "On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models", "author": ["J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "Lifelong machine learning systems: Beyond learning algorithms", "author": ["Daniel Silver", "Qiang Yang", "Lianghao Li"], "venue": "In Proceedings of the AAAI Spring Symposium on Lifelong Machine Learning,", "citeRegEx": "Silver et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2013}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Christopher Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Dialog-based language learning", "author": ["Jason Weston"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Weston.,? \\Q2016\\E", "shortCiteRegEx": "Weston.", "year": 2016}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "We must distinguish this learning to learn ability, pertaining to generalization across tasks (Ring, 1997; Schmidhuber, 2015; Silver et al., 2013; Thrun & Pratt, 1997), from 1-shot learning, that is, the challenging but more limited ability to generalize to new classes within the same task (e.", "startOffset": 94, "endOffset": 167}, {"referenceID": 15, "context": "We must distinguish this learning to learn ability, pertaining to generalization across tasks (Ring, 1997; Schmidhuber, 2015; Silver et al., 2013; Thrun & Pratt, 1997), from 1-shot learning, that is, the challenging but more limited ability to generalize to new classes within the same task (e.", "startOffset": 94, "endOffset": 167}, {"referenceID": 16, "context": "We must distinguish this learning to learn ability, pertaining to generalization across tasks (Ring, 1997; Schmidhuber, 2015; Silver et al., 2013; Thrun & Pratt, 1997), from 1-shot learning, that is, the challenging but more limited ability to generalize to new classes within the same task (e.", "startOffset": 94, "endOffset": 167}, {"referenceID": 6, "context": ", 2013; Thrun & Pratt, 1997), from 1-shot learning, that is, the challenging but more limited ability to generalize to new classes within the same task (e.g., extending an object classifier to recognize unseen objects from just a few examples; Lake et al., 2015).", "startOffset": 152, "endOffset": 262}, {"referenceID": 7, "context": "It\u2019s generally agreed that, in order to generalize across tasks, a program should be capable of compositional learning, that is, of storing and re-combining solutions to sub-problems across tasks (Fodor & Lepore, 2002; Lake et al., 2016; Minsky, 1986).", "startOffset": 196, "endOffset": 251}, {"referenceID": 10, "context": "It\u2019s generally agreed that, in order to generalize across tasks, a program should be capable of compositional learning, that is, of storing and re-combining solutions to sub-problems across tasks (Fodor & Lepore, 2002; Lake et al., 2016; Minsky, 1986).", "startOffset": 196, "endOffset": 251}, {"referenceID": 12, "context": "The tasks are different from standard artificial grammar learning (Reber, 1967), in that the learner is given explicit instructions in simplified English about the target stringset and what to do with it (description:, verify:, produce.", "startOffset": 66, "endOffset": 79}, {"referenceID": 17, "context": "For example, games such as Go (Silver et al., 2016) and StarCraft (Onta\u00f1\u00f3n et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 11, "context": ", 2016) and StarCraft (Onta\u00f1\u00f3n et al., 2013) require sophisticated planning skills intuitively associated with intelligence.", "startOffset": 22, "endOffset": 44}, {"referenceID": 19, "context": "The bAbI tasks (Weston et al., 2015) are superficially similar to CommAI tasks, but they evaluate general text understanding phenomena, rather than compositional learning-to-learn abilities.", "startOffset": 15, "endOffset": 36}, {"referenceID": 9, "context": "ACKNOWLEDGMENTS This abstract summarizes and refines ideas we originally presented in an unpublished manuscript (Mikolov et al., 2016).", "startOffset": 112, "endOffset": 134}, {"referenceID": 4, "context": "Jos\u00e9 Hern\u00e1ndez-Orallo. The Measure of All Minds. Cambridge University Press, Cambridge, UK, 2017. Gerhard J\u00e4ger and James Rogers. Formal language theory: refining the Chomsky hierarchy. Philosophical Transactions of the Royal Society of London B: Biological Sciences, 367(1598):1956\u2013 1970, 2012.", "startOffset": 5, "endOffset": 278}], "year": 2017, "abstractText": "With machine learning successfully applied to new daunting problems almost every day, general AI starts looking like an attainable goal (LeCun et al., 2015). However, most current research focuses instead on important but narrow applications, such as image classification or machine translation. We believe this to be largely due to the lack of objective ways to measure progress towards broad machine intelligence. In order to fill this gap, we propose here a set of concrete desiderata for general AI, together with a platform to test machines on how well they satisfy such desiderata, while keeping all further complexities to a minimum. 1 DESIDERATA FOR THE EVALUATION OF MACHINE INTELLIGENCE Rather than trying to define intelligence in abstract terms, we take a pragmatic approach: we would like to develop AIs that are useful for us. This naturally leads to the following desiderata. Communication through natural language An AI will be useful to us only if we are able to communicate with it: assigning it tasks, understanding the information it returns, and teaching it new skills. Since natural language is by far the easiest way for us to communicate, we require our useful AI to be endowed with basic linguistic abilities. The language the machine is exposed to in the testing environment will inevitably be very limited. However, given that we want the machine to also be a powerful, fast learner (see next point), humans should later be able to teach it more sophisticated language skills as they become important to instruct the machine in new domains. In concrete, the environment should not only expose the machine to a set of tasks, but provide instructions and feedback about the tasks in simple natural language. The machine should rely on this form of linguistic interaction to efficiently solve the tasks. Learning to learn Flexibility is a core requirement for a useful AI. As our needs change, the AI should help us with the new challenges we face: from solving a scientific problem in the morning at work to stocking our fridge at night. Progress towards AI should thus be measured on the ability to master a continuous flow of new tasks, with data-efficiency in solving new tasks as a fundamental evaluation component, and without distinguishing train and test phases. We must distinguish this learning to learn ability, pertaining to generalization across tasks (Ring, 1997; Schmidhuber, 2015; Silver et al., 2013; Thrun & Pratt, 1997), from 1-shot learning, that is, the challenging but more limited ability to generalize to new classes within the same task (e.g., extending an object classifier to recognize unseen objects from just a few examples; Lake et al., 2015). It\u2019s generally agreed that, in order to generalize across tasks, a program should be capable of compositional learning, that is, of storing and re-combining solutions to sub-problems across tasks (Fodor & Lepore, 2002; Lake et al., 2016; Minsky, 1986). The testing environment should thus feature sets of related tasks, such that a compositional learner can bootstrap skills from one task to the other. Finally, mastering language skills might be a crucial component of learning to learn, since understanding linguistic instructions allows us to quickly learn how to accomplish tasks we have never performed before. Feedback As we grow up, we learn to master complex tasks with decreasing amounts of explicit reward. A useful AI should possess similar capabilities. Consequently, in our testing environment, \u2217Now at DeepMind.", "creator": "LaTeX with hyperref package"}}}