{"id": "1701.08978", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2017", "title": "Mixed Low-precision Deep Learning Inference using Dynamic Fixed Point", "abstract": "We propose a cluster-based quantization method to convert pre-trained full precision weights into ternary weights with minimal impact on the accuracy. In addition, we also constrain the activations to 8-bits thus enabling sub 8-bit full integer inference pipeline. Our method uses smaller clusters of N filters with a common scaling factor to minimize the quantization loss, while also maximizing the number of ternary operations. We show that with a cluster size of N=4 on Resnet-101, can achieve 71.8% TOP-1 accuracy, within 6% of the best full precision results while replacing \\approx 85% of all multiplications with 8-bit accumulations. Using the same method with 4-bit weights achieves 76.3% TOP-1 accuracy which within 2% of the full precision result. We also study the impact of the size of the cluster on both performance and accuracy, larger cluster sizes N=64 can replace \\approx 98% of the multiplications with ternary operations but introduces significant drop in accuracy which necessitates fine tuning the parameters with retraining the network at lower precision. To address this we have also trained low-precision Resnet-50 with 8-bit activations and ternary weights by pre-initializing the network with full precision weights and achieve 68.9% TOP-1 accuracy within 4 additional epochs. Our final quantized model can run on a full 8-bit compute pipeline, with a potential 16x improvement in performance compared to baseline full-precision models.", "histories": [["v1", "Tue, 31 Jan 2017 10:28:37 GMT  (70kb,D)", "https://arxiv.org/abs/1701.08978v1", null], ["v2", "Wed, 1 Feb 2017 04:09:31 GMT  (70kb,D)", "http://arxiv.org/abs/1701.08978v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["naveen mellempudi", "abhisek kundu", "dipankar das", "dheevatsa mudigere", "bharat kaul"], "accepted": false, "id": "1701.08978"}, "pdf": {"name": "1701.08978.pdf", "metadata": {"source": "CRF", "title": "Mixed Low-precision Deep Learning Inference using Dynamic Fixed Point", "authors": ["Naveen Mellempudi", "Abhisek Kundu", "Dipankar Das", "Dheevatsa Mudigere", "Bharat Kaul"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "2 Related Work", "text": "To address this problem, there was great interest in using lower precision for deep learning to determine the minimum accuracy required to ensure functional correctness within acceptable thresholds. In the past, many research has suggested low precision alternatives to perform deeper learning tasks. Vanhoucke et al. [12] showed that the use of 8-bit fixed-point arithmetic folding networks for speech recognition tasks on universal CPU hardware can be accelerated by up to 10 times. Gupta et al. [4] have successfully trained 16-bit fixed-component networks on custom hardware. Miyashita et al. [8] used protocol quantization on pre-rehearsed models and achieved good accuracy by setting the bit length for each shift."}, {"heading": "3 Low Precision Inference", "text": "Previous work by Miyashita et al. [8] has shown that by compressing the dynamic range of input, it is possible to minimize quantization loss and achieve high accuracy. We take a different approach to minimize the impact of the dynamic range on quantization. We propose a cluster-based quantization method that groups weights into smaller clusters and quantifies each cluster with a unique scaling factor. We use static clusters to group group filters that focus on the same output characteristic to simplify folding operations. Empirical evidence also suggests that these clusters that learn similar characteristics tend to have a smaller dynamic range. By using dynamic fixed point representation, this method can effectively minimize quantization errors and improve the inference accuracy of quantified networks."}, {"heading": "3.1 2-bit Ternary Weights", "text": "We use the ternary representation for weights based on the threshold proposed by Li et al. [7], i.e., the approximate fully precise number of weights W \u00b2 \u03b1W \u00b2 in '2 normal algorithm 1 Ternarize Weights1: Input: Learned fully precise number of weights W of a layer with d filters. 2: Group filter in k cluster: {Gj}, j = 1, k. Let N = | Gj | (number of filters in Gj). 3: For each cluster Gj 4: Run algorithm 2 on each filter W \u00b2 Gj, and store the thresholds as vector \u03b1. 5: For t = 1, Tt = {i:."}, {"heading": "3.2 C1 and BatchNorm Layers", "text": "In our experiments, we keep the weights of the first folding layers at 8-bit to prevent losses from accumulating, while the remaining layers, including fully bonded layers, operate with less precision. We also recalculate the parameters of the batch standard during the inference phase to compensate for the variance shift caused by quantization, which is essential for it to work unless we retrain with less precision. We are exploring the possibility of merging batch normalization layers with the folding layers before quantification to avoid this additional calculation."}, {"heading": "3.3 Performance Implications", "text": "Choosing the right cluster size is a compromise between performance and accuracy, while one cluster per layer favors a higher computing density by eliminating all multiplications, it is not ideal to achieve high accuracy. Although previous research in this area [13] has shown that it is possible to recover some of this lost accuracy by retraining, this is not always ideal, since the cost of retraining these networks is associated with low precision, not to mention the technical difficulties in achieving a reasonable solution in these networks. We examined the accuracy-performance compromise with different cluster sizes. Our experiments on Resnet-101 show that using a cluster size of N = 4 71.8% TOP-1 accuracy can achieve within 6% of the full precision result. This result is significant because to the best of knowledge this is the highest accuracy achieved on Imagenet dataset [3] without retraining the network in low precision."}, {"heading": "4 Training with Low-precision", "text": "We trained the low-precision ResNet-50 on ImageNet datasets using 2-bit weights and 8-bit activations by initializing the network with a pre-trained precision model. We followed the approach proposed by Marcel et al. [10] and replaced data preprocessing steps such as medium subtraction and blurring with batch normalization layers inserted directly after the data. We obtained the models presented by Marcel et al. [10] and refined the parameters of our low-precision network. In the run-up, the weights are converted to 2-bit ternary values using the algorithm described in 1, except for the first layer, in which the weights are quantified to 8-bit fixed point representation. Activations are quantified to 8-bit fixed point in all layers, including ReLU, BatchNorm layers. We quantified the updated weights of the FC in the learning layer to 1, we did not perform the correction of the problem layer for 7 in the full training layer in the 4% during the 1."}, {"heading": "5 Conclusion", "text": "We propose a cluster-based quantization method that utilizes local correlations in the dynamic range of parameters to minimize the effects of quantization on overall accuracy. We demonstrate near SOTA accuracy on Imagenet datasets using pre-trained models with quantified networks without low precision training. On Resnet-101 with 8-bit activations, the error is the best published full precision result (FP32) for external weights and within \u2248 6% for 4-bit weights. To the best of our knowledge, this is the best achieved accuracy with external weights for Imagenet datasets. Our cluster-based approach enables tailored solutions for specific hardware based on the accuracy and performance requirements. Smaller cluster sizes achieve the best accuracy, with N = 4 \u2248 85% of the calculations being considered low precision operations (simple 8-bit accumulations) for a hardware that is better suited to 64-bit implementations with lower accuracy."}], "references": [{"title": "Deep learning. Book in preparation for", "author": ["Yoshua Bengio", "Ian Goodfellow", "Aaron Courville"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Binarized neural networks", "author": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Quantized neural networks: Training neural networks with low precision weights and activations", "author": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.07061,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Ternary weight networks", "author": ["Fengfu Li", "Bo Zhang", "Bin Liu"], "venue": "arXiv preprint arXiv:1605.04711,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Convolutional neural networks using logarithmic data representation", "author": ["Daisuke Miyashita", "Edward H Lee", "Boris Murmann"], "venue": "arXiv preprint arXiv:1603.01025,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Imagenet pre-trained models with batch normalization", "author": ["Marcel Simon", "Erik Rodner", "Joachim Denzler"], "venue": "arXiv preprint arXiv:1612.01452v2,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Finn: A framework for fast, scalable binarized neural network inference", "author": ["Yaman Umuroglu", "Nicholas J Fraser", "Giulio Gambardella", "Michaela Blott", "Philip Leong", "Magnus Jahre", "Kees Vissers"], "venue": "arXiv preprint arXiv:1612.07119,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z Mao"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Accelerating deep convolutional networks using low-precision and sparsity", "author": ["Ganesh Venkatesh", "Eriko Nurvitadhi", "Debbie Marr"], "venue": "arXiv preprint arXiv:1610.00324,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["Shuchang Zhou", "Yuxin Wu", "Zekun Ni", "Xinyu Zhou", "He Wen", "Yuheng Zou"], "venue": "arXiv preprint arXiv:1606.06160,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Trained ternary quantization", "author": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1612.01064,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Deep Learning models are used for achieving state-of-the-art results on a wide variety of tasks including Computer Vision, Natural Language Processing, Automatic Speech Recognition and Reinforcement Learning [1].", "startOffset": 208, "endOffset": 211}, {"referenceID": 5, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 175, "endOffset": 194}, {"referenceID": 1, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 175, "endOffset": 194}, {"referenceID": 4, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 175, "endOffset": 194}, {"referenceID": 13, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 175, "endOffset": 194}, {"referenceID": 7, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 175, "endOffset": 194}, {"referenceID": 6, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 175, "endOffset": 194}, {"referenceID": 3, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 281, "endOffset": 296}, {"referenceID": 14, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 281, "endOffset": 296}, {"referenceID": 12, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 281, "endOffset": 296}, {"referenceID": 10, "context": "Due to the large and somewhat unique compute requirements for both deep learning training and inference operations, it motivates the use of non-standard customized arithmetic [6, 2, 5, 14, 8, 7] and specialized compute hardware to run these computations as efficiently as possible [4, 15, 13, 11].", "startOffset": 281, "endOffset": 296}, {"referenceID": 2, "context": "To the best of our knowledge this is the best reported accuracy with ternary weights on ImageNet dataset[3], without retraining the network.", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "9% TOP-1 accuracy on ImageNet dataset[3] within 4-epochs of fine-tuning.", "startOffset": 37, "endOffset": 40}, {"referenceID": 11, "context": "[12] showed that using 8-bit fixed-point arithmetic convolution networks can be sped up by up to 10x on speech recognition tasks on general purpose CPU hardware.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] have successfully trained networks using 16-bit fixed point on custom hardware.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] used log quantization on pre-trained models and achieved good accuracy by tuning the bit length for each layer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13] achieved near state of the art results using 32b activations with 2-bit ternary weights on Imagenet dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] have demonstrated that with weights as binary values training from scratch can achieve near state-of-the-art results for ILSVRC 2012 image classification task[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[5] have demonstrated that with weights as binary values training from scratch can achieve near state-of-the-art results for ILSVRC 2012 image classification task[9].", "startOffset": 162, "endOffset": 165}, {"referenceID": 7, "context": "[8] showed that by compressing the dynamic range of the input, it is possible to minimize the quantization loss and achieve high accuracy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "3% TOP-1 accuracy on ImageNet dataset[3], without any retraining.", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "Going below 4-bits we use the the ternary representation for weights, following the threshold based approximation proposed by Li et al [7], i.", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": "3: For \u03c4 \u2208 [0, 1], I\u03c4 = {i : |Wi| belongs to the top b \u03c4 \u00b7 n c elements of sorted list}.", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "6: Compute \u03b1\u03c4\u2217 that minimizes \u2016W \u2212 \u03b1\u03c4\u0174\u2016F , for \u03c4 \u2208 [0, 1].", "startOffset": 51, "endOffset": 57}, {"referenceID": 6, "context": "Our method differs from [7] in the approximation used for computing scaling factor (\u03b1).", "startOffset": 24, "endOffset": 27}, {"referenceID": 12, "context": "Although, previous research in this space[13] showed that it is possible to recover some of this lost accuracy through retraining.", "startOffset": 41, "endOffset": 45}, {"referenceID": 2, "context": "This result significant because this is to the best of our knowledge highest accuracy achieved on Imagenet dataset[3] without retraining the network in low-precision.", "startOffset": 114, "endOffset": 117}, {"referenceID": 9, "context": "[10], and replace data pre-processing steps such as mean-subtraction and jittering with batch normalization layer inserted right after the data later.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] and fine-tune the parameters of our low-precision network.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "We propose a cluster-based quantization method to convert pre-trained full precision weights into ternary weights with minimal impact on the accuracy. In addition we also constrain the activations to 8-bits thus enabling sub 8-bit full integer inference pipeline. Our method uses smaller clusters of N filters with a common scaling factor to minimize the quantization loss, while also maximizing the number of ternary operations. We show that with cluster size of N=4 on Resnet-101, can achieve 71.8% TOP-1 accuracy, within 6% of the best full precision result, while replacing \u2248 85% of all multiplications with 8-bit accumulations. Using the same method with 4-bit weights achieves 76.3% TOP-1 accuracy which within 2% of the full precision result. We also study the impact of the size of the cluster on both performance and accuracy, larger cluster sizes N=64 can replace \u2248 98% of the multiplications with ternary operations but introduces significant drop in accuracy which necessitates fine tuning the parameters with retraining the network at lower precision. To address this we have also trained low-precision Resnet-50 with 8-bit activations and ternary weights by pre-initializing the network with full precision weights and achieve 68.9% TOP-1 accuracy within 4 additional epochs. Our final quantized model can run on a full 8-bit compute pipeline, with a potential 16x improvement in performance compared to baseline full-precision models.", "creator": "LaTeX with hyperref package"}}}