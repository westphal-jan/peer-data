{"id": "1701.09042", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2017", "title": "Comparing Dataset Characteristics that Favor the Apriori, Eclat or FP-Growth Frequent Itemset Mining Algorithms", "abstract": "Frequent itemset mining is a popular data mining technique. Apriori, Eclat, and FP-Growth are among the most common algorithms for frequent itemset mining. Considerable research has been performed to compare the relative performance between these three algorithms, by evaluating the scalability of each algorithm as the dataset size increases. While scalability as data size increases is important, previous papers have not examined the performance impact of similarly sized datasets that contain different itemset characteristics. This paper explores the effects that two dataset characteristics can have on the performance of these three frequent itemset algorithms. To perform this empirical analysis, a dataset generator is created to measure the effects of frequent item density and the maximum transaction size on performance. The generated datasets contain the same number of rows. This provides some insight into dataset characteristics that are conducive to each algorithm. The results of this paper's research demonstrate Eclat and FP-Growth both handle increases in maximum transaction size and frequent itemset density considerably better than the Apriori algorithm.", "histories": [["v1", "Mon, 30 Jan 2017 12:34:02 GMT  (232kb,D)", "http://arxiv.org/abs/1701.09042v1", null]], "reviews": [], "SUBJECTS": "cs.DB cs.AI", "authors": ["jeff heaton"], "accepted": false, "id": "1701.09042"}, "pdf": {"name": "1701.09042.pdf", "metadata": {"source": "CRF", "title": "Comparing Dataset Characteristics that Favor the Apriori, Eclat or FP-Growth Frequent Itemset Mining Algorithms", "authors": ["Jeff Heaton"], "emails": ["jeffheaton@acm.org"], "sections": [{"heading": null, "text": "This paper examines the impact that two sets of data have on the performance of these three common itemset algorithms. To perform this empirical analysis, a dataset generator is created that measures the effects of frequent iteration density and maximum transaction size on performance. The generated datasets contain the same number of rows that are conducive to each algorithm. Results of this work show that Eclat and FP growth are both significantly better increases in maximum transaction size and frequent itemset density than the Apriori algorithms."}, {"heading": "II. FREQUENT ITEMSET MINING", "text": "Frequent Itemset Mining was introduced to find common groupings of items in a database containing baskets / transactions of these items. [10] The database consists of a series of baskets that correspond to customers \"orders, and these orders are individual baskets composed of a certain number of items. Companies such as Amazon, Netflix and other online retailers often use item sets to suggest additional items a consumer would like to purchase based on their past purchasing history and the history of others with similar baskets. [11] The following data show baskets that could be used for frequent item mining, where each line represents a single basket of goods. [mp3player usb \u2212 c h a r r book \u2212 d c t h h s] [mp3player usb \u2212 c h h h h h h] [mp3player usc h] mp3player usc h mp3player book \u2212 c h mp3player book \u2212 t h \u2212 c h \u2212 c h h \u2212 c h h"}, {"heading": "III. SURVEY OF APRIORI, ECLAT AND FP-GROWTH", "text": "This paper shows how Apriori, Eclat and FP-Growth address some of the shortcomings of the na\u00efve algorithm. All four algorithms must compute statistics on item sets that can ultimately be included in the final collection of common item sets. A statistic common to all four of these algorithms is support. Support for a candidate who frequently uses item sets is the total number of the database baskets supporting that candidate. A basket should cover a candidate if the candidate is a subset or equal to the basket. Support is sometimes expressed as a percentage of the total number of baskets in the database (N) covering a candidate item set (X). The following formula calculates the support percentage of a candidate basket: supp (X) = XcountN (1) This equation can be used as a minimum of support for the candidate (3)."}, {"heading": "IV. NAIVE ALGORITHM FOR FINDING FREQUENT ITEMSETS", "text": "It is not difficult to extract common item sets from shopping basket data. However, it is difficult to do this efficiently. For the algorithms presented here, let J represent a set of items, let D represent a database of shopping baskets consisting of these items. Algorithm 1 is a summary of the na\u00efve, common item basket algorithm provided by Garcia-Molina, Ullman and Widom. Algorithm 1 Naive Common item set algorithm 1: INPUT: A file D consisting of shopping baskets of items. 2: OUTPUT: The item set sets F1, F2,.... Fq, where fiis the set of all item set sizes I, which are in at least s baskets of D.3: METHOD: 4: R: Integer array, all item sets in D, sizes 2 | D | 5: for any threshold value of METHO: Any size of METHOD | 3: METHOD | 3."}, {"heading": "V. NAIVE ALGORITHM EXAMPLE", "text": "The book is a book about whether it is a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book, a book."}, {"heading": "VI. APRIORI ALGORITHM FOR FINDING FREQUENT ITEMSETS", "text": "The naive algorithm is a theoretical concept and is not applied in practice. aprioiri is the classic implementation of aprioiri is the classic implementation of aprioiri as defined by Goethals (2003), as algorithm 2 [14].Algorithm 2 apriori Frequent Itemset Algorithm 1: INPUT: A file D consisting of baskets of items, a supply threshold of items. (2: OUTPUT: A list of itemsets F [14].Algorithm Frequent Itemset Algorithm 1: INPUT: A file D consisting of baskets of items, a supply threshold. (2: OUTPUT: A list of itemsets F).3: METHOD: 4: C1 Frequent Itemset Algorithm 1: (i)."}, {"heading": "VII. APRIORI ALGORITHM EXAMPLE", "text": "This section shows how the Apriori algorithm handles the shopping cart set specified in this thesis. Apriori algorithm performs a comprehensive initial search for the articles. Figure 1 shows a segment of this search for the articles usbcable, mp3 player and book-dct. The candidate set begins blank and begins with the addition of all individual items that have sufficient individual support. For simplicity, it is assumed that only USB cable, mp3 player and book-dct have sufficient support. The next layer consists of combinations of the previous layer that had sufficient support. For simplicity, it is also assumed that all three combinations had sufficient support. Finally, a triple item set with all three elements is evaluated."}, {"heading": "VIII. ECLAT ALGORITHM FOR FINDING FREQUENT ITEMSETS", "text": "It is not only the question of \"why,\" but also the question of \"why,\" of \"why,\" of \"why,\" of \"why,\" of \"why,\" of \"why,\" of \"why,\" of \"why,\" of \"why,\" of \"why,\" of \"why,\" of \"what,\" of \"what,\" of \"what,\" of \"what,\" of \"what,\" of \"what,\" of \"what.\""}, {"heading": "IX. ECLAT ALGORITHM EXAMPLE", "text": "This section shows how the Eclat algorithm would deal with the shopping cart set previously specified in this thesis. Part of the Eclat-created Trie is shown in Figure 2. The above Trie part encodes a total of 7 different support values for a common article. To find support for a specific common article, simply traverse the graph by following the elements from left to right. Frequently used mp3 player, usb-charger, would have a support value of 3. Likewise, the frequently used mp3 player, usb-charger, book-dct, would have a support value of 2. Once the algorithm is complete, the Trie is traversed and all common elements are extracted from it."}, {"heading": "X. FP-GROWTH ALGORITHM FOR FINDING FREQUENT ITEMSETS", "text": "Frequent pattern growth (FP growth) was introduced in 2000 by Han, Pei and Yin to do without the candidate generation altogether [16]. This is done by using a trial to store the actual baskets, rather than storing candidates such as Apriori and Eclatdo. Aprori is very strongly horizontal, width first, algorithm. Similarly, Eclat is very strongly vertical, depth first, algorithm. The Trie structure of FP growth provides a vertical view of the data. However, FP growth also adds a header table for each individual element that has support above the threshold support level. This header table contains a linked list by the Trie to connect each node of the same type. The header table gives FP growth a horizontal view of the data, in addition to the header table: [FP growth:] a header table for each individual element that has support above the threshold level."}, {"heading": "XI. FP-GROWTH ALGORITHM EXAMPLE", "text": "This section shows how the FP Growth Algorithm would handle the basket set described earlier in this paper. Figure 3 shows a portion of the FP Growth Trie and header table generated for the earlier sample data. This figure illustrates the horizontal and vertical nature of the FP Growth Algorithm. The trine on the right contains the encoded baskets along with their supports. The header on the left contains the elements and provides horizontal access to the data."}, {"heading": "XII. EMPIRICAL COMPARISON OF APRIORI, ECLAT AND FP-GROWTH", "text": "There are a number of papers that compare the computing power of Apriori, Eclat and FP-Growth. These papers typically focus on comparing the differences between algorithms on one or more datasets and at different support thresholds. Essays by Borgelt (2012) and Goethals (2003) are examples of papers that compare different implementations of Apriori, Eclat and FP-Growth [9]. This paper tries a different approach. The aim of this paper is to see what effect the dataset has on the algorithm. Average basket size and frequent article density are used as independent variables to evaluate the total processing time as a dependent variable. Apriori, Eclat and FP-Growth are evaluated independently to determine which performance is best at different basket sizes and frequent article densities. Performance is measured as a shorter total run time."}, {"heading": "XIII. DATASET GENERATION", "text": "Generated datasets are used to perform this evaluation, and this generated data allows the two independent variables to be adjusted to create a total of 20 different datasets to perform the evaluations.The datasets were created using a simple Python script that can be found for this paper on GitHub [8].This Python script accepts the following parameters to produce the dataset: \u2022 Transaction / Cart Counting: 10 million standard \u2022 Number of items can be easily varied in the script: 100 default values \u2022 Max transaction / Cart Size: independent variable, 5-100 range \u2022 Frequent density: independent variable, 0.1 to 0.8 range During basket counting, number of frequent sets, and number of items are varied in the script, for the purposes of this paper they will remain at the above values. Informal experiments found that the shopping cart count had only a small positive correlation to the processing time."}, {"heading": "XIV. EFFECTS OF DATASET DENSITY", "text": "The density of the data sets indicates the percentage of baskets containing intentionally frequent item sets. As the density of the items increases, so does the processing time of Apriori, Eclat and FP-Growth, as shown in Figure 4.This graph shows the results of operating 10 million baskets with an average basket size of 50 at different densities. Eclat and FP-Growth algorithms both show very similar growth with increasing item density. Apriori's algorithm also behaves very similar to Eclat and FP-Growth until the density exceeds 70%. As mentioned in this paper, Apriori has a considerably larger memory requirement than the other algorithms. At 70%, Apriori had allocated all 8 gigabytes of RAM to the test machine, which necessitated the exchange of physical memory and had a drastic effect on the runtime of the algorithm. Interestingly, Eclat is also slightly ahead of the actual density at low densities."}, {"heading": "XV. EFFECTS OF BASKET SIZE", "text": "Larger basket sizes mean that the common transaction sizes will also be larger, increasing the size of the data structures used to store these items. These larger data structures require more memory for storage and greater processing time to pass through them. Figure 5 illustrates the effect of increasing transaction sizes on the performance of the three algorithms. This graph shows the results of operating 10 million baskets with a frequent article density of 50% at different maximum basket sizes. The three algorithms show almost exactly the same O (N) performance for basket sizes up to 60. Once over 60, Apriori appears to grow much faster than the other two. This may be due to the increased memory consumption of Apriori. Interestingly, Apriori has achieved the best results between 60 and 70 maximum transaction sizes. Further research is needed to determine why Apriori is superior in this small range."}, {"heading": "XVI. CONCLUSIONS", "text": "For this reason, Apriori is a popular starting point for frequent itemset studies. However, Apriori has serious scaling problems and depletes available memory much faster than Eclat and RP Growth. Therefore, Apriori should not be used for large datasets. Most common itemset applications should consider using RP Growth or Eclat. These two algorithms performed similar research in this paper, although RP Growth performed slightly better than Eclat. Other papers also recommend RP Growth in most cases [9]. Frequent itemset mining is an area of active research. New algorithms as well as modifications to existing algorithms are often introduced. For an application where performance is critical, it is important to evaluate the dataset with newer algorithms as they are introduced and prove to be more powerful than FPGrowth or Eclat."}], "references": [{"title": "Frequent pattern mining: Current status and future directions", "author": ["J. Han", "H. Cheng", "D. Xin", "X. Yan"], "venue": "Data Mining Knowledge Discovery, vol. 15, no. 1, pp. 55\u201386, Aug. 2007.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD explorations newsletter, vol. 11, no. 1, pp. 10\u201318, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "arules \u2013 A computational environment for mining association rules and frequent item sets", "author": ["M. Hahsler", "B. Gruen", "K. Hornik"], "venue": "Journal of Statistical Software, vol. 14, no. 15, pp. 1\u201325, October 2005. [Online]. Available: http://www.jstatsoft.org/v14/i15/", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Mafia: A maximal frequent itemset algorithm for transactional databases", "author": ["D. Burdick", "M. Calimlim", "J. Gehrke"], "venue": "Proceedings of the 17th International Conference on Data Engineering, 2001. IEEE, 2001, pp. 443\u2013452.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Real world performance of association rule algorithms", "author": ["Z. Zheng", "R. Kohavi", "L. Mason"], "venue": "Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2001, pp. 401\u2013406.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "UCI machine learning repository", "author": ["A. Asuncion", "D. Newman"], "venue": "2007. [Online]. Available: http://www.ics.uci.edu/$\\sim$mlearn/{MLR} epository.html", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Market basket synthetic data generator", "author": ["A. Pitman"], "venue": "2011, http://mloss. org/software/view/294/.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Jeff Heaton\u2019s GitHub Repository - Conference/Paper Source Code", "author": ["J Heaton"], "venue": "https://github.com/jeffheaton/papers, accessed: 2016-01-31.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Frequent item set mining", "author": ["C. Borgelt"], "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 2, no. 6, pp. 437\u2013456, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Mining association rules between sets of items in large databases", "author": ["R. Agrawal", "T. Imieli\u0144ski", "A. Swami"], "venue": "ACM SIGMOD Record, vol. 22, no. 2. ACM, 1993, pp. 207\u2013216.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Mining of massive datasets", "author": ["J. Leskovec", "A. Rajaraman", "J.D. Ullman"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Database systems: the complete book", "author": ["H. Garcia-Molina"], "venue": "Pearson Education India,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Fast algorithms for mining association rules", "author": ["R. Agrawal", "R. Srikant"], "venue": "Proceedings of the 20th international conference of very large data bases, VLDB, vol. 1215, 1994, pp. 487\u2013499.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "Survey on frequent pattern mining", "author": ["B. Goethals"], "venue": "University of Helsinki, 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "New algorithms for fast discovery of association rules", "author": ["M.J. Zaki", "S. Parthasarathy", "M. Ogihara", "W. Li"], "venue": "KDD, vol. 97, 1997, pp. 283\u2013 286.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Mining frequent patterns without candidate generation", "author": ["J. Han", "J. Pei", "Y. Yin"], "venue": "ACM SIGMOD Record, vol. 29, no. 2. ACM, 2000, pp. 1\u201312.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "These algorithms have several popular implementations[1], [2], [3].", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "These algorithms have several popular implementations[1], [2], [3].", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "These algorithms have several popular implementations[1], [2], [3].", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "Most research into frequent itemset mining focuses upon the performance differences between frequent itemset algorithms on a single dataset[4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "The effects of hyper-paramaters, such as minimum support, upon the performance of frequent itemset mining algorithms has also been explored[5].", "startOffset": 139, "endOffset": 142}, {"referenceID": 5, "context": "Some papers make use of common datasets from the UCI Machine Learning Repository[6].", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "Many papers make use of the IBM Quest Synthetic Data Generator[7] or some variant of it.", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "makes use of a Python-based generator that is based on IBM\u2019s work[8].", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "and FP-Growth frequent itemset mining algorithms implemented by Christian Borgelt in 2012[9].", "startOffset": 89, "endOffset": 92}, {"referenceID": 9, "context": "Frequent itemset mining was introduced as a means to find frequent groupings of items in a database containing baskets/transactions of these items[10].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "Companies, such as Amazon, Netflix and other online retailers, make use of frequent itemsets to suggest additional items that a consumer might want to purchase, based on their past purchasing history and the history of others with similar baskets[11].", "startOffset": 246, "endOffset": 250}, {"referenceID": 11, "context": "Algorithm 1 is a summarization of the naive frequent itemset algorithm provided by Garcia-Molina, Ullman, and Widom[12].", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "Agrawal and Srikant initially introduced the Apriori algorithm to provide performance improvements over a naive itemset search[13].", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": "Aprioiri, as defined by Goethals (2003) is presented as Algorithm 2[14].", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "Likewise, a superset of an infrequent itemset must also be infrequent[13].", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "Papers by Goethals (2003) and others do not represent Apriori\u2019s performance in terms of big-O notation[14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "Eclat was introduced by Zaki, Parthasarathy, Ogihara, and Li in 1997[15].", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "Eclat, as defined by Goethals (2003) is presented as Algorithm 3[14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "This is the approach used by Borgelt (2012) to implement the versions of Apriori, Eclat and FP-Growth investigated in this research paper[9].", "startOffset": 137, "endOffset": 140}, {"referenceID": 15, "context": "Frequent pattern growth (FP-Growth) was introduced by Han, Pei, and Yin in 2000 to forego candidate generation altogether[16].", "startOffset": 121, "endOffset": 125}, {"referenceID": 13, "context": "FP-Growth, as defined by Goethals, is presented as Algorithm 4[14].", "startOffset": 62, "endOffset": 66}, {"referenceID": 8, "context": "Papers by Borgelt (2012) and Goethals (2003) are examples of papers that compare various implementations of Apriori, Eclat and FP-Growth[9], [14].", "startOffset": 136, "endOffset": 139}, {"referenceID": 13, "context": "Papers by Borgelt (2012) and Goethals (2003) are examples of papers that compare various implementations of Apriori, Eclat and FP-Growth[9], [14].", "startOffset": 141, "endOffset": 145}, {"referenceID": 8, "context": "This paper focuses on a single implementation of these algorithms using the implementations of Apriori, Eclat and FP-Growth by Borgelt in 2012[9].", "startOffset": 142, "endOffset": 145}, {"referenceID": 7, "context": "The datasets were generated using a simple Python script created for this paper that can be found on GitHub[8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 8, "context": "Other papers also recommend FP-Growth for most cases[9].", "startOffset": 52, "endOffset": 55}], "year": 2017, "abstractText": "Frequent itemset mining is a popular data mining technique. Apriori, Eclat, and FP-Growth are among the most common algorithms for frequent itemset mining. Considerable research has been performed to compare the relative performance between these three algorithms, by evaluating the scalability of each algorithm as the dataset size increases. While scalability as data size increases is important, previous papers have not examined the performance impact of similarly sized datasets that contain different itemset characteristics. This paper explores the effects that two dataset characteristics can have on the performance of these three frequent itemset algorithms. To perform this empirical analysis, a dataset generator is created to measure the effects of frequent item density and the maximum transaction size on performance. The generated datasets contain the same number of rows. This provides some insight into dataset characteristics that are conducive to each algorithm. The results of this paper\u2019s research demonstrate Eclat and FP-Growth both handle increases in maximum transaction size and frequent itemset density considerably better than the Apriori algorithm.", "creator": "LaTeX with hyperref package"}}}