{"id": "1701.09123", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2017", "title": "Robust Multilingual Named Entity Recognition with Shallow Semi-Supervised Features", "abstract": "We present a multilingual Named Entity Recognition approach based on a robust and general set of features across languages and datasets. Our system combines shallow local information with clustering semi-supervised features induced on large amounts of unlabeled text. Understanding via empirical experimentation how to effectively combine various types of clustering features allows us to seamlessly export our system to other datasets and languages. The result is a simple but highly competitive system which obtains state of the art results across five languages and twelve datasets. The results are reported on standard shared task evaluation data such as CoNLL for English, Spanish and Dutch. Furthermore, and despite the lack of linguistically motivated features, we also report best results for languages such as Basque and German. In addition, we demonstrate that our method also obtains very competitive results even when the amount of supervised data is cut by half, alleviating the dependency on manually annotated data. Finally, the results show that our emphasis on clustering features is crucial to develop robust out-of-domain models. The system and models are freely available to facilitate its use and guarantee the reproducibility of results.", "histories": [["v1", "Tue, 31 Jan 2017 16:36:06 GMT  (37kb)", "http://arxiv.org/abs/1701.09123v1", "26 pages, 19 tables (submitted for publication on September 2015), Artificial Intelligence (2016)"]], "COMMENTS": "26 pages, 19 tables (submitted for publication on September 2015), Artificial Intelligence (2016)", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["rodrigo agerri", "german rigau"], "accepted": false, "id": "1701.09123"}, "pdf": {"name": "1701.09123.pdf", "metadata": {"source": "CRF", "title": "Robust Multilingual Named Entity Recognition with Shallow Semi-Supervised Features", "authors": ["Rodrigo Agerriand", "German Rigau"], "emails": ["rodrigo.agerri@ehu.eus"], "sections": [{"heading": null, "text": "ar Xiv: 170 1.09 123v 1 [cs.C LWe present a multilingual approach to named entity recognition based on a robust and general set of characteristics across languages and datasets. Our system combines flat local information with clusters of semi-monitored characteristics generated by large amounts of unlabeled text. By empirically experimenting with how to effectively combine different types of cluster functions, we can seamlessly export our system to other datasets and languages, resulting in a simple but highly competitive system that is state-of-the-art in five languages and twelve datasets. Results are reported via standard evaluation data for common tasks such as CoNLL for English, Spanish and Dutch. In addition, and despite the lack of linguistically motivated characteristics, we also report best results for languages such as Basque and German. In addition, we show that our method achieves very competitive results even when the amount of monitored data is reduced by half."}, {"heading": "1 Introduction", "text": "A designated entity recognition with a wide variety of surface shapes (Barack Obama, President Obama, Mr. Li, B. Obama, etc.) is usually an appropriate surface shape that can refer to a variety of designated entities. For example, the form \"Europe\" is for a variety of different entities, including the continent, the European Union, various Greek mythological entities, a rock band, some music albums, a magazine, a short story, etc.1 In addition, it is possible to refer to a designated entity using anaphorical pronouns and coreferent expressions such as \"he,\" \"she,\" her, \"her,\" her, \"35 years old,\" etc., in order to refer to this paper as: R. Agerri, G. Rigau, G. Rigust Multilingual Named Entity Recognition with semi-shallow functions, artificial intelligence (2016)."}, {"heading": "1.1 Contributions", "text": "The main contributions of this paper are the following: First, we show how to develop easily robust NERC systems via datasets and languages with minimal human intervention (including for languages with declination and / or complex morphology); second, we show empirically how to effectively use different types of simple word representation features, thus providing a clear methodology for selecting and combining these features; third, we show that our system still achieves very competitive results, even if the monitored data is reduced by half (in some cases even less), thereby alleviating the dependence on costly hand-written data; these three main contributions are based on: 1. A simple and flat set of features across languages and datasets, even in out-of-domain evaluations; 2. The lack of linguistically motivated features, even for languages with agglutinative features (e.g. basque) and / or complex morphologies (e.g. German).3 A clear methodology for combining word expressions and different types of use with each other by combining them."}, {"heading": "2 Related Work", "text": "The Named Entity Recognition and Classification (NERC) task was first defined for the Sixth Message Understanding Conference (MUC 6) (Nadeau and Sekine, 2007).The MUC 6 task focused on information extraction (IE) from unstructured text, and NERC was considered an important IE subtask with the goal of identifying and classifying nominal mentions of individuals, organizations, and locations, as well as numerical expressions of data, money, percentage, and time. In the following years, research on NERC 6 increased as it was considered a critical source of information for other tasks in processing natural languages, such as answering questions (QA) and textual adaptation (RTE) (Nadeau and Sekine, 2007).Furthermore, while MUC 6 was exclusively devoted to English as the target language, the CoNLL tasks (2002 and 2003) shared research on language-independent NERC languages."}, {"heading": "2.1 Datasets", "text": "The first half of the band is characterized by the ability to put oneself and others at the center. The second half of the band is characterized by the ability to put oneself at the center. The third half of the band, the second half of the band, the second half of the band, the third half of the band, the third half of the band, the third half of the band, the third half of the band, the second half of the band, the third half of the band, the third half of the band, the third half of the band, the third half of the band, the third half of the band, the third half of the band, the third half of the band, the third half of the band, the third half of the band, the second half of the band, the second half of the band, the third, the third, the third half of the band, the third half of the band, the third half of the band, the fourth, fourth, fourth, fourth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth."}, {"heading": "2.2 Related Approaches", "text": "In fact, it is the case that you are able to go to another world, where you have to go to another world, where you have to go to another world, where you have to go to another world, where you have to go to another world, where you have to go to another world, where you can go to another world."}, {"heading": "3 System Description", "text": "The design of ixa-pipe-nerc aims to establish a simple and flat feature set to avoid linguistic features, with the aim of eliminating any dependence on costly additional gold annotations (POS tags, lemmas, syntax, semantics) and / or cascade errors when using automatic language processors. The underlying motivation is to obtain robust models to facilitate the development of NERC systems for other languages and records / domains while achieving state-of-the-art results. \u2022 Our system consists of: \u2022 Local, flat features, which are largely based on orthographic, word form and n-gram features and their context. \u2022 Three types of simple cluster functions based on the conformity of unigrams. \u2022 Publicly available gazetteers, which are widely used in previous NERC systems (Tjong Kim Seuline and Seuline Seulin 2003, and the Nadeau 7th, for example)."}, {"heading": "3.1 Local Features", "text": "The local features represent our base system on which the cluster features are added. We implement the following features, some of which were inspired by previous work (Zhang and Johnson, 2003): \u2022 Token: Current Lowercase Token (w), namely ekuadorko in Table 3. \u2022 Token Shape: Current Lowercase Token (w) plus Current Token Form (wc), where the token form consists of either lowercase or a two-digit word or a four-digit word. \u2022 Token Shape: If the token contains digits, then whether it also contains letters, slashes, or hyphens, or commas or digits. (iii) The token is either a two-digit word or a single-digit uppercase word, or begins with uppercase letters."}, {"heading": "3.2 Gazetteers", "text": "We only add gazetters to our system when they are easy to use, but our approach is not fundamentally dependent on them. We perform a search in the gazetteer to check if there is a named entity in the sentence. The result of the search is presented using the same encoding that was chosen for the training process, namely the BIO or BILOU scheme 8. Therefore, we add the following characteristics for the current token: 1. The currently named entity class in the encoding scheme. Thus, in the BILOU encoding we would have \"unit,\" \"beginning,\" \"last,\" \"inside,\" or if no match is found, \"outside,\" combined with the specifically named entity type (LOC, ORG, PER, MISC, etc.).2. The currently named entity class as above and the current token."}, {"heading": "3.3 Clustering Features", "text": "The general idea is that by using some kind of semantic similarity or word clusters induced by large, unlabeled corpora, predictions for invisible words in the test sentence can be improved. This kind of semi-supervised learning can aim to improve performance over a set amount of training data or, at a set target performance level, determine how much supervised data is actually required to achieve such performance (Koo et al., 2008). So far, the most successful approaches have used only one type of word representation (Passos et al., 2014; Faruqui et al., 2010; Ratinov et al., 2009). However, our simple basic data combined with one type of word representation characteristics cannot compete with previous, more complex systems. Instead of encoding more complicated characteristics, we have developed a simple method to combine and stack different types of cluster characteristics that can be induced via different data sources or word representations of any kind of our method can, in principle."}, {"heading": "3.3.1 Brown Features", "text": "In fact, it is a matter of a kind of confusion that is able to hide, one that is able to hide itself, and one that is able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be in the position they are in, to be in, and to be able to hide."}, {"heading": "3.3.2 Clark Features", "text": "Clark (2003) presents a number of unattended algorithms based on distribution and morphological information to cluster words in classes of unlabeled text. The focus is on clustering rare words on a small number of clusters of comparatively small amounts of data. In particular Clark (2003) introduces an algorithm that combines distribution information with morphological information of words \"by assembling the cluster model of Ney food with a model of morphology within a Bayesian framework.\" The aim is to distort the distribution information to place words that are morphologically similar in the same cluster. We use the code released by Clark (2003) from the shelf f10 to induce Clark clusters using the Ney-Essen method with morphological information. Input the algorithm is a sequence of lowercase letters without punctuation, one character per line with clause breaks, our current search for cluster characteristics is very simple if we find a corresponding cluster icon in the Clark (if we are looking for a corresponding one)."}, {"heading": "3.3.3 Word2vec Features", "text": "Another family of language models that produce word representations are the neural language models. These approaches produce the representation of words as continuous vectors (Collobert and Weston, 2008; Mnih and Hinton, 2007), also called word embeddings, and perhaps the most popular of them today is the Skip-gram algorithm (Mikolov et al., 2013). The Skip-gram algorithm uses flat log-linear models to calculate the vector representation of words that are more efficient than previous word representations that were generated in neural language models. Its goal is to generate word embeddings by calculating the probability of each n-gram as the product of the conditional probabilities of each contextual word in the n-gram conditioned on its central word (Mikolov et al., 2013). Instead of using continuous vectors as real numbers, we induce clusters or word classes from the word vectors by using these two-dimensional word clusters by using these simple K-means."}, {"heading": "3.3.4 Stacking and Combining Clustering Features", "text": "We successfully combine cluster functions from different word representations. In addition, we also stack or accumulate features of the same type of word representation generated from different 9https: / / github.com / percyliang / brown-cluster 10https: / / github.com / ninjin / clark _ pos _ induction 11https: / / code.google.com / p / Word2vec / data sources, trusting each cluster lexicon to varying degrees, as shown by the five encoded cluster functions in Table 3: two Clark and Word2vec features from different source data and a Brown function. When using word representations as semi-monitored features for a task such as NERC, two main factors must be considered: (i) the source data or corpus used to represent the word representations and (ii) the actual word representation used to encode our characteristics, which in turn will change the weight of our model in English."}, {"heading": "4 Experimental Results", "text": "The experiments are conducted in 5 languages: Basque, Dutch, English, German and Spanish. For comparison purposes, in-domain results are presented in Section 4.1, using the most common NERC data sets for each language as summarized in Table 1. Section 4.2 analyzes performance in reducing training data and Section 4.3 presents eight out-of-domain ratings for three languages: Dutch, English and Spanish.The results for Dutch, English and Spanish do not include trigrams and characters n-grams in the local functionality described in Section 3.1, except for the models in each in-domain rating marked \"charngram 1: 6.\" We also experiment with dictionary features, but unlike previous approaches such as Passos et al. (2014), we only use available gazetteers off-the-domain rating."}, {"heading": "4.1 In-domain evaluation", "text": "In this section, the results are presented by language. In two cases, the Dutch and the German, we use two different sets of data, resulting in a total of seven in-domain ratings."}, {"heading": "4.1.1 English", "text": "We tested our system in the highly competitive CoNLL 2003 dataset al. Table 5 shows that three of our models outperform the previous best results of the individual systems in the CoNLL 2003 dataset (Passos et al., 2014). Results also show that these models improve the basis provided by the local characteristics by about 7 points in the Formula 1 standings. The most significant gain is in terms of recall, almost 9 points better than the baseline data. We also report very competitive results, only marginally lower than Passos et al. (2014), based on stacking and the combination of cluster functions as described in Section 3.3.4. Both best cluster and comp models, based on local plus cluster functions, outperform very competitive and more complex systems such as those of Ratinov and Roth (2009), and get only marginally lower results."}, {"heading": "4.1.3 Spanish", "text": "The best system so far on the CoNLL 2002 dataset, originally published by Carreras et al. (2002), is distributed as part of the Freeling Library (Padro \u0301 and Stanilovsky, 2012).Table 9 lists four models that improve on their reported results, in the case of the e-cluster model, by almost 3 points (with ours excluding trigram and character-n-gram features)."}, {"heading": "4.1.4 Dutch", "text": "Although only clusters from a single data source are used (see Table 4), the results in Table 10 show that our nl cluster model outperforms the best result published on CoNLL 2002 (Clark and Curran, 2003) by 3.83 points in the F1 score. Adding the English NER Gazetteers from Illinois (Ratinov and Roth, 2009) and trigram and character characteristics, the score increases to 85.04 F1, 5.41 points better than previous published work on this data set. In addition, we compared our system with the recently developed SONAR-1 corpus and the associated NERD system distributed within its release (Desmet and Hosts, 2014), reporting 84.91 F1 for the six main named entity types using 10-fold cross-validation. For this comparison, we selected the local nl cluster and nl cluster dict configurations from Table 10 and executed them on SONAR-1 using the same results as reported in our earlier table 11."}, {"heading": "4.1.5 Basque", "text": "Table 12 reports on the experiments with the Egunkaria NER dataset by Alegria et al. (2006). Due to the rarity of the MISC class mentioned in Section 2.1, we have decided to train our models in only three classes (location, organization and person), and the results are therefore obtained in the usual manner by training our models and evaluating them in three classes. However, for direct comparison with previous work (Alegria et al., 2006), we also evaluate our best eu cluster model (trained in three classes) in four classes. Results show that our eu cluster model significantly improves on previous work by 4 points on F1 scale (75.40 vs 71.35). These results are particularly interesting as it has previously been assumed that complex linguistic features and language-specific rules were required to perform well for agglutinative languages such as Basque (Alegria et al., 2006)."}, {"heading": "4.2 Reducing training data", "text": "So far, we have seen how, given a fixed amount of supervised training data, the use of undescribed data using multiple cluster sources has helped to obtain state-of-the-art results in seven different indomain settings for five languages. In this section, we will examine to what extent our system can reduce dependence on supervised training data. We will first use the English CoNLL 2003 data sets for this experiment. The training set consists of about 204K words and we will use various smaller versions of them to test the performance of our best cluster model, which is shown in Table 5. Table 13 shows the F1 results of the baseline system, consisting of local features and the best cluster model. The column refers to the gains of our best cluster model in terms of the baseline model for each part of the training set.While we have already commented on the significant gains we have achieved simply by adding our cluster functions, it is also interesting to note that the gains are significant when less supervised training data is available."}, {"heading": "4.3 Out-of-domain evaluations", "text": "NERC systems are often used outside the domain to comment on data that differs greatly from the data from which the NERC models were learned. These differences may affect text genres and / or domains, but also because assumptions about what constitutes a named entity may differ. Therefore, it is interesting to develop robust NERC systems in both domains and datasets. In this section, we will show that our approach, consisting of basic, generic local characteristics and the combination and stacking of clusters, produces robust NERC systems in three external evaluation settings: \u2022 Class differences: Named units are assigned to different classes in training and testing. \u2022 Different text genres: The text genre differs from training and test data. \u2022 Annotation guidelines: The gold annotation of the test data follows different guidelines from the training data. This is generally reflected in different named entities. The data used for these experiments is directly distributed to the data sets of earlier experiments and to the other data sets."}, {"heading": "4.3.1 Class Disagreements", "text": "MUC 7 comments on seven entity types, including four that are not included in the CoNLL data: DATE, MONEY, NUMBER, and TIME entities. In addition, CoNLL includes the MISC class that was not included in MUC 7. This means that there are class differences in the gold standard annotation between the educational and test data sets. Consider, for example, the following set of the MUC-7 gold standard (example from Ratinov and Roth (2009):... \"balloon, called Virgin Global Challenger.\" The gold annotation in MUC 7 specifies that there is a designated entity in MUC 7:... \"balloon, called MISAR-S1, called MISC.\""}, {"heading": "4.3.2 Text Genre", "text": "In this setting, the out-of-domain character is given by the differences in the text genre between the English CoNLL 2003 set and the Wikigold corpus. We compare our system with English models trained on large amounts of silver standard text (3.5 million tokens) automatically generated from Wikipedia (Nothman et al., 2013). They report results on Wikigold showing that they have outperformed their own CoNLL 2003 gold standard model by 10 points in the F1 score. We compare their result with our best cluster model in Table 17. While the results of our base model confirm theirs, our cluster model score is slightly higher. This result is interesting because it is probably easier to induce the clusters we use to train ixa-pipe-nerc than to create the silver standard training set from Wikipedia, as described in Nothman et al. (2013)."}, {"heading": "4.3.3 Annotation Guidelines", "text": "This year, we are in a position to put ourselves at the top."}, {"heading": "5 Discussion", "text": "This year it is more than ever before."}, {"heading": "6 Conclusion and Future Work", "text": "We have demonstrated how to develop robust NERC systems across languages and datasets with minimal human intervention, even for languages with flexed named units. This is based on an adequate combination of word representation features across superficial and general local characteristics. Crucially, we have demonstrated empirically how different types of simple word representation features can be effectively combined depending on the available source data, resulting in a clear methodology for using the three types of clustering characteristics, which produces very competitive results in both domain and out-domain settings. Thus, despite the relative simplicity of our approach in seven in-domain evaluations, we report on the state-of-the-art for Dutch, English, German, Spanish and Basque results. In addition, we are surpassing previous work in eight out-of-domain evaluations, showing that our clustering capabilities will dramatically improve the robustness of NERC systems across our datasets, as we have measured the amount of data sets over time."}, {"heading": "Acknowledgments", "text": "We would like to thank the anonymous reviewers for their comments on improving this paper and Sebastian Pado \u0301 for his help in training the Clark clusters. This work was supported by the European projects NewsReader, EC / FP7 / 316404 and QTLeap - EC / FP7 / 610516 as well as by the Spanish Ministry of Science and Innovation (MICINN) SKATER, grant no. TIN201238584-C06-01 and TUNER, TIN2015-65308-C5-1-R."}], "references": [{"title": "IXA pipeline: Efficient and ready to use multilingual NLP tools", "author": ["R. Agerri", "J. Bermudez", "G. Rigau"], "venue": "in: Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914),", "citeRegEx": "Agerri et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agerri et al\\.", "year": 2014}, {"title": "The stages of event extraction, in: Proceedings of the Workshop on Annotating and Reasoning about Time and Events, Association for Computational Linguistics", "author": ["D. Ahn"], "venue": null, "citeRegEx": "Ahn,? \\Q2006\\E", "shortCiteRegEx": "Ahn", "year": 2006}, {"title": "Translating named entities using monolingual and bilingual resources", "author": ["Y. Al-Onaizan", "K. Knight"], "venue": "in: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL", "citeRegEx": "Al.Onaizan and Knight,? \\Q2002\\E", "shortCiteRegEx": "Al.Onaizan and Knight", "year": 2002}, {"title": "Lessons from the development of a named entity recognizer for Basque", "author": ["I. Alegria", "O. Arregi", "N. Ezeiza", "I. Fern\u00e1ndez"], "venue": "Procesamiento del lenguaje natural", "citeRegEx": "Alegria et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Alegria et al\\.", "year": 2006}, {"title": "Improving machine translation quality with automatic named entity recognition", "author": ["B. Babych", "A. Hartley"], "venue": "in: Proceedings of the 7th International EAMT workshop on MT and other Language Technology Tools,", "citeRegEx": "Babych and Hartley,? \\Q2003\\E", "shortCiteRegEx": "Babych and Hartley", "year": 2003}, {"title": "The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation", "author": ["M. Baroni", "S. Bernardini", "A. Ferraresi", "E. Zanchetta"], "venue": null, "citeRegEx": "Baroni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2009}, {"title": "Germeval 2014 named entity recognition shared task: Companion paper, in: Proceedings of the GermEval 2014 Named Entity Recognition", "author": ["D. Benikova", "C. Biemann", "M. Kisselew", "S. Pad\u00f3"], "venue": "Shared Task,", "citeRegEx": "Benikova et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Benikova et al\\.", "year": 2014}, {"title": "Germaner: Free open german named entity recognition", "author": ["D. Benikova", "S.M. Yimam", "P. Santhanam", "C. Biemann"], "venue": "Proceedings of the International Conference of the German Society for Computational Linguistics and Language Technology", "citeRegEx": "Benikova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Benikova et al\\.", "year": 2015}, {"title": "Unsupervised part-of-speech tagging in the large", "author": ["C. Biemann"], "venue": "Research on Language and Computation", "citeRegEx": "Biemann,? \\Q2009\\E", "shortCiteRegEx": "Biemann", "year": 2009}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. Desouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational linguistics", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "New avenues in opinion mining and sentiment analysis", "author": ["E. Cambria", "B. Schuller", "Y. Xia", "C. Havasi"], "venue": "IEEE Intelligent Systems", "citeRegEx": "Cambria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cambria et al\\.", "year": 2013}, {"title": "Named entity extraction using AdaBoost", "author": ["X. Carreras", "L. Marquez", "L. Padro"], "venue": "in: Proceedings of the 6th conference on Natural language learning-Volume", "citeRegEx": "Carreras et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2002}, {"title": "Muc-7 information extraction task definition, in: Proceeding of the Seventh Message Understanding Conference (MUC-7), Appendices", "author": ["N. Chinchor", "E. Marsh"], "venue": null, "citeRegEx": "Chinchor and Marsh,? \\Q1998\\E", "shortCiteRegEx": "Chinchor and Marsh", "year": 1998}, {"title": "Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger", "author": ["M. Ciaramita", "Y. Altun"], "venue": "in: Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ciaramita and Altun,? \\Q2006\\E", "shortCiteRegEx": "Ciaramita and Altun", "year": 2006}, {"title": "Combining distributional and morphological information for part of speech induction, in: Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics-Volume 1, Association for Computational Linguistics", "author": ["A. Clark"], "venue": null, "citeRegEx": "Clark,? \\Q2003\\E", "shortCiteRegEx": "Clark", "year": 2003}, {"title": "Language Independent NER using a Maximum Entropy Tagger", "author": ["S. Clark", "J. Curran"], "venue": "in: Proceedings of the Seventh Conference on Natural Language Learning (CoNLL-03),", "citeRegEx": "Clark and Curran,? \\Q2003\\E", "shortCiteRegEx": "Clark and Curran", "year": 2003}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms, in: Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume", "author": ["M. Collins"], "venue": null, "citeRegEx": "Collins,? \\Q2002\\E", "shortCiteRegEx": "Collins", "year": 2002}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "in: Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Large-scale named entity disambiguation based on wikipedia", "author": ["S. Cucerzan"], "venue": "Proceedings of EMNLP-CoNLL,", "citeRegEx": "Cucerzan,? \\Q2007\\E", "shortCiteRegEx": "Cucerzan", "year": 2007}, {"title": "Semantic relations between events and their time, locations and participants for event coreference resolution", "author": ["A. Cybulska", "P. Vossen"], "venue": "in: Proceedings of the International Conference Recent Advances in Natural Language Processing RANLP", "citeRegEx": "Cybulska and Vossen,? \\Q2013\\E", "shortCiteRegEx": "Cybulska and Vossen", "year": 2013}, {"title": "Fine-grained dutch named entity recognition. Language resources and evaluation", "author": ["B. Desmet", "V. Hoste"], "venue": null, "citeRegEx": "Desmet and Hoste,? \\Q2014\\E", "shortCiteRegEx": "Desmet and Hoste", "year": 2014}, {"title": "The automatic content extraction (ace) program-tasks, data, and evaluation", "author": ["G.R. Doddington", "A. Mitchell", "M.A. Przybocki", "L.A. Ramshaw", "S. Strassel", "R.M. Weischedel"], "venue": "in: LREC,", "citeRegEx": "Doddington et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Doddington et al\\.", "year": 2004}, {"title": "Training and evaluating a german named entity recognizer with semantic generalization", "author": ["M. Faruqui", "S. Pad\u00f3", "M. Sprachverarbeitung"], "venue": "in: Proceedings of KONVENS,", "citeRegEx": "Faruqui et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2010}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "author": ["J.R. Finkel", "T. Grenager", "C. Manning"], "venue": "in: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Named entity recognition through classifier combination", "author": ["R. Florian", "A. Ittycheriah", "H. Jing", "T. Zhang"], "venue": "Proceedings of CoNLL-2003,", "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "Evaluating entity linking with wikipedia", "author": ["B. Hachey", "W. Radford", "J. Nothman", "M. Honnibal", "J.R. Curran"], "venue": "Artificial intelligence", "citeRegEx": "Hachey et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hachey et al\\.", "year": 2013}, {"title": "A generative entity-mention model for linking entities with knowledge base", "author": ["X. Han", "L. Sun"], "venue": "in: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Han and Sun,? \\Q2011\\E", "shortCiteRegEx": "Han and Sun", "year": 2011}, {"title": "Modular classifier ensemble architecture for named entity recognition on low resource systems", "author": ["C. H\u00e4nig", "S. Bordag", "S. Thomas"], "venue": "Proceedings of the KONVENS Germ-Eval Shared Task on Named Entity Recognition,", "citeRegEx": "H\u00e4nig et al\\.,? \\Q2014\\E", "shortCiteRegEx": "H\u00e4nig et al\\.", "year": 2014}, {"title": "Robust disambiguation of named entities in text", "author": ["J. Hoffart", "M. Yosef", "I. Bordino", "H. Frstenau", "M. Pinkal", "M. Spaniol", "B. Taneva", "S. Thater", "G. Weikum"], "venue": "in: Proc. of EMNLP,", "citeRegEx": "Hoffart et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 2011}, {"title": "Using cross-entity inference to improve event extraction", "author": ["Y. Hong", "J. Zhang", "B. Ma", "J. Yao", "G. Zhou", "Q. Zhu"], "venue": "in: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Hong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hong et al\\.", "year": 2011}, {"title": "Refining event extraction through cross-document inference", "author": ["H. Ji", "R. Grishman"], "venue": "in: ACL,", "citeRegEx": "Ji and Grishman,? \\Q2008\\E", "shortCiteRegEx": "Ji and Grishman", "year": 2008}, {"title": "Knowledge base population: Successful approaches and challenges, in: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, Association for Computational Linguistics", "author": ["H. Ji", "R. Grishman"], "venue": null, "citeRegEx": "Ji and Grishman,? \\Q2011\\E", "shortCiteRegEx": "Ji and Grishman", "year": 2011}, {"title": "Simple semi-supervised dependency parsing, in: Proceedings of ACL-08: HLT, Association for Computational Linguistics, Columbus, Ohio", "author": ["T. Koo", "X. Carreras", "M. Collins"], "venue": null, "citeRegEx": "Koo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Name-aware machine translation", "author": ["H. Li", "J. Zheng", "H. Ji", "Q. Li", "W. Wang"], "venue": "ACL", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Semi-supervised learning for natural language", "author": ["P. Liang"], "venue": "Ph.D. thesis. Massachusetts Institute of Technology", "citeRegEx": "Liang,? \\Q2005\\E", "shortCiteRegEx": "Liang", "year": 2005}, {"title": "Sentiment analysis and opinion mining", "author": ["B. Liu"], "venue": "Synthesis Lectures on Human Language Technologies", "citeRegEx": "Liu,? \\Q2012\\E", "shortCiteRegEx": "Liu", "year": 2012}, {"title": "Evaluating DBpedia Spotlight for the TACKBP entity linking task", "author": ["P.N. Mendes", "J. Daiber", "M. Jakob", "C. Bizer"], "venue": "in: Proceedings of the TAC-KBP 2011 Workshop,", "citeRegEx": "Mendes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mendes et al\\.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "in: Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Name tagging with word clusters and discriminative training", "author": ["S. Miller", "J. Guinness", "A. Zamanian"], "venue": "in: HLT-NAACL,", "citeRegEx": "Miller et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2004}, {"title": "Semeval-2015 task 4: Timeline: Cross-document event ordering", "author": ["A.L. Minard", "M. Speranza", "E. Agirre", "I. Aldabe", "M. van Erp", "B. Magnini", "G. Rigau", "R. Urizar"], "venue": "in: Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Minard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Minard et al\\.", "year": 2015}, {"title": "MEANTIME, the NewsReader Multilingual Event and Time Corpus", "author": ["A.L. Minard", "M. Speranza", "R. Urizar", "B. Altuna", "M. van Erp", "A. Schoen", "C. van Son"], "venue": "in: Proceedings of LREC 2016", "citeRegEx": "Minard et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Minard et al\\.", "year": 2016}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "in: Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Mnih and Hinton,? \\Q2007\\E", "shortCiteRegEx": "Mnih and Hinton", "year": 2007}, {"title": "A survey of named entity recognition and classification", "author": ["D. Nadeau", "S. Sekine"], "venue": "Lingvisticae Investigationes", "citeRegEx": "Nadeau and Sekine,? \\Q2007\\E", "shortCiteRegEx": "Nadeau and Sekine", "year": 2007}, {"title": "Comparison between tagged corpora for the named entity task", "author": ["C. Nobata", "N. Collier", "J. Tsujii"], "venue": "in: Proceedings of the workshop on Comparing corpora,", "citeRegEx": "Nobata et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Nobata et al\\.", "year": 2000}, {"title": "Learning multilingual named entity recognition from wikipedia", "author": ["J. Nothman", "N. Ringland", "W. Radford", "T. Murphy", "J.R. Curran"], "venue": "Artificial Intelligence", "citeRegEx": "Nothman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nothman et al\\.", "year": 2013}, {"title": "Towards wider multilinguality", "author": ["L. Padr\u00f3", "E. Stanilovsky"], "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation", "citeRegEx": "Padr\u00f3 and Stanilovsky,? \\Q2012\\E", "shortCiteRegEx": "Padr\u00f3 and Stanilovsky", "year": 2012}, {"title": "Lexicon infused phrase embeddings for named entity resolution, in: Proceedings of the Eighteenth Conference on Computational Natural Language Learning, Association for Computational Linguistics, Ann Arbor, Michigan", "author": ["A. Passos", "V. Kumar", "A. McCallum"], "venue": null, "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Semeval2015 task 12: Aspect based sentiment analysis", "author": ["M. Pontiki", "D. Galanis", "H. Papageorgiou", "S. Manandhar", "I. Androutsopoulos"], "venue": "in: Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Pontiki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pontiki et al\\.", "year": 2015}, {"title": "Semeval-2014 task 4: Aspect based sentiment analysis", "author": ["M. Pontiki", "H. Papageorgiou", "D. Galanis", "I. Androutsopoulos", "J. Pavlopoulos", "S. Manandhar"], "venue": "in: Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Pontiki et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pontiki et al\\.", "year": 2014}, {"title": "CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes", "author": ["S. Pradhan", "A. Moschitti", "N. Xue", "O. Uryupina", "Y. Zhang"], "venue": "in: Proceedings of the Sixteenth Conference on Computational Natural Language Learning (CoNLL", "citeRegEx": "Pradhan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2012}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["L. Ratinov", "D. Roth"], "venue": "in: Proceedings of the Thirteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Ratinov and Roth,? \\Q2009\\E", "shortCiteRegEx": "Ratinov and Roth", "year": 2009}, {"title": "Nested named entity recognition with neural networks", "author": ["N. Reimers", "J. Eckle-Kohler", "C. Schnober", "J. Kim", "I. Gurevych"], "venue": "Proceedings of the KONVENS Germ-Eval Shared Task on Named Entity Recognition,", "citeRegEx": "Reimers et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reimers et al\\.", "year": 2014}, {"title": "Named entity recognition in tweets: an experimental study", "author": ["A. Ritter", "S. Clark", "O. Etzioni"], "venue": "in: Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data, in: Proceedings of ACL-08: HLT, Association for Computational Linguistics, Columbus, Ohio", "author": ["J. Suzuki", "H. Isozaki"], "venue": null, "citeRegEx": "Suzuki and Isozaki,? \\Q2008\\E", "shortCiteRegEx": "Suzuki and Isozaki", "year": 2008}, {"title": "AnCora: Multilevel Annotated Corpora for Catalan and Spanish", "author": ["M. Taul\u00e9", "M.A. Mart\u0301\u0131", "M. Recasens"], "venue": "in: Proceedings of LREC", "citeRegEx": "Taul\u00e9 et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Taul\u00e9 et al\\.", "year": 2008}, {"title": "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition", "author": ["E.F. Tjong Kim Sang"], "venue": "in: Proceedings of CoNLL-2002,", "citeRegEx": "Sang,? \\Q2002\\E", "shortCiteRegEx": "Sang", "year": 2002}, {"title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition", "author": ["E.F. Tjong Kim Sang", "F. De Meulder"], "venue": "in: Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume", "citeRegEx": "Sang and Meulder,? \\Q2003\\E", "shortCiteRegEx": "Sang and Meulder", "year": 2003}, {"title": "NewsReader Guidelines for Annotation at Document Level", "author": ["S. Tonelli", "R. Sprugnoli", "M. Speranza", "A.L. Minard"], "venue": null, "citeRegEx": "Tonelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tonelli et al\\.", "year": 2014}, {"title": "Word representations: A simple and general method for semi-supervised learning, in: Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, Uppsala, Sweden", "author": ["J. Turian", "L.A. Ratinov", "Y. Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "A robust risk minimization based named entity recognition system, in: Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003Volume 4, Association for Computational Linguistics", "author": ["T. Zhang", "D. Johnson"], "venue": null, "citeRegEx": "Zhang and Johnson,? \\Q2003\\E", "shortCiteRegEx": "Zhang and Johnson", "year": 2003}], "referenceMentions": [{"referenceID": 49, "context": "Nowadays NERC systems are widely being used in research for tasks such as Coreference Resolution (Pradhan et al., 2012), Named Entity Disambiguation (Cucerzan, 2007; Han and Sun, 2011; Hoffart et al.", "startOffset": 97, "endOffset": 119}, {"referenceID": 18, "context": ", 2012), Named Entity Disambiguation (Cucerzan, 2007; Han and Sun, 2011; Hoffart et al., 2011; Mendes et al., 2011; Hachey et al., 2013) for which a lot of interest has been created by the TAC KBP shared tasks (Ji and Grishman, 2011), Machine Translation (Al-Onaizan and Knight, 2002; Koehn et al.", "startOffset": 37, "endOffset": 136}, {"referenceID": 26, "context": ", 2012), Named Entity Disambiguation (Cucerzan, 2007; Han and Sun, 2011; Hoffart et al., 2011; Mendes et al., 2011; Hachey et al., 2013) for which a lot of interest has been created by the TAC KBP shared tasks (Ji and Grishman, 2011), Machine Translation (Al-Onaizan and Knight, 2002; Koehn et al.", "startOffset": 37, "endOffset": 136}, {"referenceID": 28, "context": ", 2012), Named Entity Disambiguation (Cucerzan, 2007; Han and Sun, 2011; Hoffart et al., 2011; Mendes et al., 2011; Hachey et al., 2013) for which a lot of interest has been created by the TAC KBP shared tasks (Ji and Grishman, 2011), Machine Translation (Al-Onaizan and Knight, 2002; Koehn et al.", "startOffset": 37, "endOffset": 136}, {"referenceID": 36, "context": ", 2012), Named Entity Disambiguation (Cucerzan, 2007; Han and Sun, 2011; Hoffart et al., 2011; Mendes et al., 2011; Hachey et al., 2013) for which a lot of interest has been created by the TAC KBP shared tasks (Ji and Grishman, 2011), Machine Translation (Al-Onaizan and Knight, 2002; Koehn et al.", "startOffset": 37, "endOffset": 136}, {"referenceID": 25, "context": ", 2012), Named Entity Disambiguation (Cucerzan, 2007; Han and Sun, 2011; Hoffart et al., 2011; Mendes et al., 2011; Hachey et al., 2013) for which a lot of interest has been created by the TAC KBP shared tasks (Ji and Grishman, 2011), Machine Translation (Al-Onaizan and Knight, 2002; Koehn et al.", "startOffset": 37, "endOffset": 136}, {"referenceID": 31, "context": ", 2013) for which a lot of interest has been created by the TAC KBP shared tasks (Ji and Grishman, 2011), Machine Translation (Al-Onaizan and Knight, 2002; Koehn et al.", "startOffset": 81, "endOffset": 104}, {"referenceID": 2, "context": ", 2013) for which a lot of interest has been created by the TAC KBP shared tasks (Ji and Grishman, 2011), Machine Translation (Al-Onaizan and Knight, 2002; Koehn et al., 2007; Babych and Hartley, 2003; Li et al., 2013), Aspect Based Sentiment Analysis (Liu, 2012; Cambria et al.", "startOffset": 126, "endOffset": 218}, {"referenceID": 4, "context": ", 2013) for which a lot of interest has been created by the TAC KBP shared tasks (Ji and Grishman, 2011), Machine Translation (Al-Onaizan and Knight, 2002; Koehn et al., 2007; Babych and Hartley, 2003; Li et al., 2013), Aspect Based Sentiment Analysis (Liu, 2012; Cambria et al.", "startOffset": 126, "endOffset": 218}, {"referenceID": 33, "context": ", 2013) for which a lot of interest has been created by the TAC KBP shared tasks (Ji and Grishman, 2011), Machine Translation (Al-Onaizan and Knight, 2002; Koehn et al., 2007; Babych and Hartley, 2003; Li et al., 2013), Aspect Based Sentiment Analysis (Liu, 2012; Cambria et al.", "startOffset": 126, "endOffset": 218}, {"referenceID": 35, "context": ", 2013), Aspect Based Sentiment Analysis (Liu, 2012; Cambria et al., 2013; Pontiki et al., 2014, 2015), Event Extraction (Doddington et al.", "startOffset": 41, "endOffset": 102}, {"referenceID": 10, "context": ", 2013), Aspect Based Sentiment Analysis (Liu, 2012; Cambria et al., 2013; Pontiki et al., 2014, 2015), Event Extraction (Doddington et al.", "startOffset": 41, "endOffset": 102}, {"referenceID": 21, "context": ", 2014, 2015), Event Extraction (Doddington et al., 2004; Ahn, 2006; Ji and Grishman, 2008; Cybulska and Vossen, 2013; Hong et al., 2011) and Event Ordering (Minard et al.", "startOffset": 32, "endOffset": 137}, {"referenceID": 1, "context": ", 2014, 2015), Event Extraction (Doddington et al., 2004; Ahn, 2006; Ji and Grishman, 2008; Cybulska and Vossen, 2013; Hong et al., 2011) and Event Ordering (Minard et al.", "startOffset": 32, "endOffset": 137}, {"referenceID": 30, "context": ", 2014, 2015), Event Extraction (Doddington et al., 2004; Ahn, 2006; Ji and Grishman, 2008; Cybulska and Vossen, 2013; Hong et al., 2011) and Event Ordering (Minard et al.", "startOffset": 32, "endOffset": 137}, {"referenceID": 19, "context": ", 2014, 2015), Event Extraction (Doddington et al., 2004; Ahn, 2006; Ji and Grishman, 2008; Cybulska and Vossen, 2013; Hong et al., 2011) and Event Ordering (Minard et al.", "startOffset": 32, "endOffset": 137}, {"referenceID": 29, "context": ", 2014, 2015), Event Extraction (Doddington et al., 2004; Ahn, 2006; Ji and Grishman, 2008; Cybulska and Vossen, 2013; Hong et al., 2011) and Event Ordering (Minard et al.", "startOffset": 32, "endOffset": 137}, {"referenceID": 39, "context": ", 2011) and Event Ordering (Minard et al., 2015).", "startOffset": 27, "endOffset": 48}, {"referenceID": 43, "context": "This has seriously hindered the development of robust high performing NERC systems for many languages but also for other domains and text genres (Nobata et al., 2000; Ritter et al., 2011), in what we will henceforth call \u2018out-of-domain\u2019 evaluations.", "startOffset": 145, "endOffset": 187}, {"referenceID": 52, "context": "This has seriously hindered the development of robust high performing NERC systems for many languages but also for other domains and text genres (Nobata et al., 2000; Ritter et al., 2011), in what we will henceforth call \u2018out-of-domain\u2019 evaluations.", "startOffset": 145, "endOffset": 187}, {"referenceID": 3, "context": "For this type of languages, it had usually been assumed that linguistic features (typically Part of Speech (POS) and lemmas, but also semantic features based on WordNet, for example) and perhaps specific hand-crafted rules, were a necessary condition for good NERC performance as they would allow to capture better the most recurrent declensions (cases) of named entities for Basque (Alegria et al., 2006) or to address problems such as sparsity and capitalization of every noun for German (Faruqui et al.", "startOffset": 383, "endOffset": 405}, {"referenceID": 22, "context": ", 2006) or to address problems such as sparsity and capitalization of every noun for German (Faruqui et al., 2010; Benikova et al., 2014, 2015).", "startOffset": 92, "endOffset": 143}, {"referenceID": 44, "context": "This suggests that without fine-tuning for each corpus and language, the systems did not generalize well across languages (Nothman et al., 2013).", "startOffset": 122, "endOffset": 144}, {"referenceID": 9, "context": "Our approach consists of shallow local features complemented by three types of word representation (clustering) features: Brown clusters (Brown et al., 1992), Clark clusters (Clark, 2003) and K-means clusters on top of the word vectors obtained by using the Skip-gram algorithm (Mikolov et al.", "startOffset": 137, "endOffset": 157}, {"referenceID": 14, "context": ", 1992), Clark clusters (Clark, 2003) and K-means clusters on top of the word vectors obtained by using the Skip-gram algorithm (Mikolov et al.", "startOffset": 24, "endOffset": 37}, {"referenceID": 37, "context": ", 1992), Clark clusters (Clark, 2003) and K-means clusters on top of the word vectors obtained by using the Skip-gram algorithm (Mikolov et al., 2013).", "startOffset": 128, "endOffset": 150}, {"referenceID": 3, "context": "We also report best results for German using the GermEval 2014 shared task data and for Basque using the Egunkaria testset (Alegria et al., 2006).", "startOffset": 123, "endOffset": 145}, {"referenceID": 50, "context": "We report out-of-domain evaluations in three languages (Dutch, English and Spanish) using four different datasets to compare our system with the best publicly available systems for those languages: Illinois NER (Ratinov and Roth, 2009) for English, Stanford NER (Finkel et al.", "startOffset": 211, "endOffset": 235}, {"referenceID": 23, "context": "We report out-of-domain evaluations in three languages (Dutch, English and Spanish) using four different datasets to compare our system with the best publicly available systems for those languages: Illinois NER (Ratinov and Roth, 2009) for English, Stanford NER (Finkel et al., 2005) for English and Spanish, SONAR-1 NERD for Dutch (Desmet and Hoste, 2014) and Freeling for Spanish (Padr\u00f3 and Stanilovsky, 2012).", "startOffset": 262, "endOffset": 283}, {"referenceID": 20, "context": ", 2005) for English and Spanish, SONAR-1 NERD for Dutch (Desmet and Hoste, 2014) and Freeling for Spanish (Padr\u00f3 and Stanilovsky, 2012).", "startOffset": 56, "endOffset": 80}, {"referenceID": 45, "context": ", 2005) for English and Spanish, SONAR-1 NERD for Dutch (Desmet and Hoste, 2014) and Freeling for Spanish (Padr\u00f3 and Stanilovsky, 2012).", "startOffset": 106, "endOffset": 135}, {"referenceID": 32, "context": "Finally, and inspired by previous work (Koo et al., 2008; Biemann, 2009) we measure how much supervision is required to obtain state of the art results.", "startOffset": 39, "endOffset": 72}, {"referenceID": 8, "context": "Finally, and inspired by previous work (Koo et al., 2008; Biemann, 2009) we measure how much supervision is required to obtain state of the art results.", "startOffset": 39, "endOffset": 72}, {"referenceID": 16, "context": "Our system learns Perceptron models (Collins, 2002) using the Machine Learning machinery provided by the Apache OpenNLP project with our own customized (local and clustering) features.", "startOffset": 36, "endOffset": 51}, {"referenceID": 0, "context": "0 License and part of the IXA pipes tools (Agerri et al., 2014).", "startOffset": 42, "endOffset": 63}, {"referenceID": 42, "context": "The Named Entity Recognition and Classification (NERC) task was first defined for the Sixth Message Understanding Conference (MUC 6) (Nadeau and Sekine, 2007).", "startOffset": 133, "endOffset": 158}, {"referenceID": 42, "context": "In the following years, research on NERC increased as it was considered to be a crucial source of information for other Natural Language Processing tasks such as Question Answering (QA) and Textual Entailment (RTE) (Nadeau and Sekine, 2007).", "startOffset": 215, "endOffset": 240}, {"referenceID": 42, "context": "Thus, while in the MUC 6 competition 5 out of 8 systems were rule-based, in CoNLL 2003 16 teams participated in the English task all using statistical-based NERC (Nadeau and Sekine, 2007).", "startOffset": 162, "endOffset": 187}, {"referenceID": 44, "context": "The Wikigold corpus consists of 39K words of English Wikipedia manually annotated following the CoNLL 2003 guidelines (Nothman et al., 2013).", "startOffset": 118, "endOffset": 140}, {"referenceID": 54, "context": "0 (Taul\u00e9 et al., 2008) and SONAR-1 (Desmet and Hoste, 2014) respectively.", "startOffset": 2, "endOffset": 22}, {"referenceID": 20, "context": ", 2008) and SONAR-1 (Desmet and Hoste, 2014) respectively.", "startOffset": 20, "endOffset": 44}, {"referenceID": 3, "context": "In Basque the only gold standard corpus is Egunkaria (Alegria et al., 2006).", "startOffset": 53, "endOffset": 75}, {"referenceID": 6, "context": "The GermEval 2014 NER shared task (Benikova et al., 2014) aimed at improving the state of the art of German NERC which was perceived to be comparatively lower than the English NERC.", "startOffset": 34, "endOffset": 57}, {"referenceID": 40, "context": "Finally, the MEANTIME corpus (Minard et al., 2016) is a multilingual (Dutch, English, Italian and Spanish) publicly available evaluation set annotated within the Newsreader project.", "startOffset": 29, "endOffset": 50}, {"referenceID": 15, "context": "In any case, while the machine learning method is important, it has also been demonstrated that good performance might largely be due to the feature set used (Clark and Curran, 2003).", "startOffset": 158, "endOffset": 182}, {"referenceID": 55, "context": "As argued by the CoNLL 2003 organizers, no feature set was deemed to be ideal for NERC (Tjong Kim Sang and De Meulder, 2003), although many approaches for English refer to Zhang and Johnson (2003) as a useful general approach.", "startOffset": 98, "endOffset": 197}, {"referenceID": 11, "context": "Linguistic Information Some of the CoNLL participants use linguistic information (POS, lemmas, chunks, but also specific rules or patterns) for Dutch and English (Carreras et al., 2002; Clark and Curran, 2003), although these type of features was deemed to be most important for German, for which the use of linguistic features is pervasive (Benikova et al.", "startOffset": 162, "endOffset": 209}, {"referenceID": 15, "context": "Linguistic Information Some of the CoNLL participants use linguistic information (POS, lemmas, chunks, but also specific rules or patterns) for Dutch and English (Carreras et al., 2002; Clark and Curran, 2003), although these type of features was deemed to be most important for German, for which the use of linguistic features is pervasive (Benikova et al.", "startOffset": 162, "endOffset": 209}, {"referenceID": 6, "context": ", 2002; Clark and Curran, 2003), although these type of features was deemed to be most important for German, for which the use of linguistic features is pervasive (Benikova et al., 2014).", "startOffset": 163, "endOffset": 186}, {"referenceID": 22, "context": "This is caused by the sparsity caused by the declension cases, the tendency to form compounds containing named entities and by the capitalization of every noun (Faruqui et al., 2010).", "startOffset": 160, "endOffset": 182}, {"referenceID": 27, "context": "For example, the best system among the 11 participants in GermEval 2014, ExB, uses morphological features and specific suffix lists aimed at capturing frequent patterns in the endings of named entities (H\u00e4nig et al., 2014).", "startOffset": 202, "endOffset": 222}, {"referenceID": 3, "context": "The only previous work for Basque developed Eihera, a rule-based NERC system formalized as finite state transducers to take into account declension classes (Alegria et al., 2006).", "startOffset": 156, "endOffset": 178}, {"referenceID": 11, "context": "The best performing systems carefully compile their own gazetteers from a variety of sources (Carreras et al., 2002).", "startOffset": 93, "endOffset": 116}, {"referenceID": 11, "context": "The best performing systems carefully compile their own gazetteers from a variety of sources (Carreras et al., 2002). Ratinov and Roth (2009) leverage a collection of 30 gazetteers and matches against each one are weighted as a separate feature.", "startOffset": 94, "endOffset": 142}, {"referenceID": 11, "context": "The best performing systems carefully compile their own gazetteers from a variety of sources (Carreras et al., 2002). Ratinov and Roth (2009) leverage a collection of 30 gazetteers and matches against each one are weighted as a separate feature. In this way they trust each gazetteer to a different degree. Passos et al. (2014) carefully compiled a large collection of English gazetteers extracted from US Census data and Wikipedia and applied them to the process of inducing word embeddings with very good results.", "startOffset": 94, "endOffset": 328}, {"referenceID": 11, "context": "Carreras et al. (2002) proposed a method to produce the set of named entities for the whole sentence, where the optimal set of named entities for the sentence is the coherent set of named entities which maximizes the summation of confidences of the named entities in the set.", "startOffset": 0, "endOffset": 23}, {"referenceID": 11, "context": "Carreras et al. (2002) proposed a method to produce the set of named entities for the whole sentence, where the optimal set of named entities for the sentence is the coherent set of named entities which maximizes the summation of confidences of the named entities in the set. Ratinov and Roth (2009) developed three types of non-local features, analyzing global dependencies in a window of between 200 and 1000 tokens.", "startOffset": 0, "endOffset": 300}, {"referenceID": 9, "context": "More specifically, it had been previously shown how to apply Brown clusters (Brown et al., 1992) for ChineseWord Segmentation (Liang, 2005), dependency parsing (Koo et al.", "startOffset": 76, "endOffset": 96}, {"referenceID": 34, "context": ", 1992) for ChineseWord Segmentation (Liang, 2005), dependency parsing (Koo et al.", "startOffset": 37, "endOffset": 50}, {"referenceID": 32, "context": ", 1992) for ChineseWord Segmentation (Liang, 2005), dependency parsing (Koo et al., 2008), NERC (Suzuki and Isozaki, 2008) and POS tagging (Biemann, 2009).", "startOffset": 71, "endOffset": 89}, {"referenceID": 53, "context": ", 2008), NERC (Suzuki and Isozaki, 2008) and POS tagging (Biemann, 2009).", "startOffset": 14, "endOffset": 40}, {"referenceID": 8, "context": ", 2008), NERC (Suzuki and Isozaki, 2008) and POS tagging (Biemann, 2009).", "startOffset": 57, "endOffset": 72}, {"referenceID": 17, "context": "(2010) made a rather exhaustive comparison of Brown clusters, Collobert and Weston\u2019s embeddings (Collobert and Weston, 2008) and HLBL embeddings (Mnih and Hinton, 2007) to improve chunking and NERC.", "startOffset": 96, "endOffset": 124}, {"referenceID": 41, "context": "(2010) made a rather exhaustive comparison of Brown clusters, Collobert and Weston\u2019s embeddings (Collobert and Weston, 2008) and HLBL embeddings (Mnih and Hinton, 2007) to improve chunking and NERC.", "startOffset": 145, "endOffset": 168}, {"referenceID": 8, "context": ", 2008), NERC (Suzuki and Isozaki, 2008) and POS tagging (Biemann, 2009). Ratinov and Roth (2009) used Brown clusters as features obtaining what was at the time the best published result of an English NERC system on the CoNLL 2003 testset.", "startOffset": 58, "endOffset": 98}, {"referenceID": 8, "context": ", 2008), NERC (Suzuki and Isozaki, 2008) and POS tagging (Biemann, 2009). Ratinov and Roth (2009) used Brown clusters as features obtaining what was at the time the best published result of an English NERC system on the CoNLL 2003 testset. Turian et al. (2010) made a rather exhaustive comparison of Brown clusters, Collobert and Weston\u2019s embeddings (Collobert and Weston, 2008) and HLBL embeddings (Mnih and Hinton, 2007) to improve chunking and NERC.", "startOffset": 58, "endOffset": 261}, {"referenceID": 8, "context": ", 2008), NERC (Suzuki and Isozaki, 2008) and POS tagging (Biemann, 2009). Ratinov and Roth (2009) used Brown clusters as features obtaining what was at the time the best published result of an English NERC system on the CoNLL 2003 testset. Turian et al. (2010) made a rather exhaustive comparison of Brown clusters, Collobert and Weston\u2019s embeddings (Collobert and Weston, 2008) and HLBL embeddings (Mnih and Hinton, 2007) to improve chunking and NERC. They show that in some cases the combination of word representation features was positive but, although they used Ratinov and Roth\u2019s (2009) system as starting point, they did not manage to improve over the state of the art.", "startOffset": 58, "endOffset": 593}, {"referenceID": 8, "context": ", 2008), NERC (Suzuki and Isozaki, 2008) and POS tagging (Biemann, 2009). Ratinov and Roth (2009) used Brown clusters as features obtaining what was at the time the best published result of an English NERC system on the CoNLL 2003 testset. Turian et al. (2010) made a rather exhaustive comparison of Brown clusters, Collobert and Weston\u2019s embeddings (Collobert and Weston, 2008) and HLBL embeddings (Mnih and Hinton, 2007) to improve chunking and NERC. They show that in some cases the combination of word representation features was positive but, although they used Ratinov and Roth\u2019s (2009) system as starting point, they did not manage to improve over the state of the art. Furthermore, they reported that Brown clustering features performed better than the word embeddings. Passos et al. (2014) extend the Skip-gram algorithm to learn 50-dimensional lexicon infused phrase embeddings from 22 different gazetteers and the Wikipedia.", "startOffset": 58, "endOffset": 799}, {"referenceID": 8, "context": ", 2008), NERC (Suzuki and Isozaki, 2008) and POS tagging (Biemann, 2009). Ratinov and Roth (2009) used Brown clusters as features obtaining what was at the time the best published result of an English NERC system on the CoNLL 2003 testset. Turian et al. (2010) made a rather exhaustive comparison of Brown clusters, Collobert and Weston\u2019s embeddings (Collobert and Weston, 2008) and HLBL embeddings (Mnih and Hinton, 2007) to improve chunking and NERC. They show that in some cases the combination of word representation features was positive but, although they used Ratinov and Roth\u2019s (2009) system as starting point, they did not manage to improve over the state of the art. Furthermore, they reported that Brown clustering features performed better than the word embeddings. Passos et al. (2014) extend the Skip-gram algorithm to learn 50-dimensional lexicon infused phrase embeddings from 22 different gazetteers and the Wikipedia. The resulting embeddings are used as features by scaling them by a hyper-parameter which is a real number tuned on the development data. Passos et al. (2014) report best results up to date for English NERC on CoNLL 2003 test data, 90.", "startOffset": 58, "endOffset": 1094}, {"referenceID": 23, "context": "They trained the Stanford NER system (Finkel et al., 2005), which uses a linear-chain Conditional Random Field (CRF) with a variety of features, including lemma, POS tag, etc.", "startOffset": 37, "endOffset": 58}, {"referenceID": 14, "context": "Crucially, they included \u201cdistributional similarity\u201d features in the form of Clark clusters (Clark, 2003) induced from large unlabeled corpora: the Huge German Corpus (HGC) of around 175M tokens of newspaper text and the deWac corpus (Baroni et al.", "startOffset": 92, "endOffset": 105}, {"referenceID": 5, "context": "Crucially, they included \u201cdistributional similarity\u201d features in the form of Clark clusters (Clark, 2003) induced from large unlabeled corpora: the Huge German Corpus (HGC) of around 175M tokens of newspaper text and the deWac corpus (Baroni et al., 2009) consisting of 1.", "startOffset": 234, "endOffset": 255}, {"referenceID": 20, "context": "The best German CoNLL 2003 system (an ensemble) was outperformed by Faruqui et al. (2010). They trained the Stanford NER system (Finkel et al.", "startOffset": 68, "endOffset": 90}, {"referenceID": 24, "context": "Ensemble Systems The best participant of the English CoNLL 2003 shared task used the results of two externally trained NERC taggers to create an ensemble system (Florian et al., 2003).", "startOffset": 161, "endOffset": 183}, {"referenceID": 27, "context": "The best system of the GermEval 2014 task built an ensemble of classifiers and pattern extractors to find the most likely tag sequence (H\u00e4nig et al., 2014).", "startOffset": 135, "endOffset": 155}, {"referenceID": 24, "context": "Ensemble Systems The best participant of the English CoNLL 2003 shared task used the results of two externally trained NERC taggers to create an ensemble system (Florian et al., 2003). Passos et al. (2014) develop a stacked linear-chain CRF system: they train two CRFs with roughly the same features; the second CRF can condition on the predictions made by the first CRF.", "startOffset": 162, "endOffset": 206}, {"referenceID": 24, "context": "Ensemble Systems The best participant of the English CoNLL 2003 shared task used the results of two externally trained NERC taggers to create an ensemble system (Florian et al., 2003). Passos et al. (2014) develop a stacked linear-chain CRF system: they train two CRFs with roughly the same features; the second CRF can condition on the predictions made by the first CRF. Their \u201cbaseline\u201d system uses a similar local featureset as Ratinov and Roth\u2019s (2009) but complemented with gazetteers.", "startOffset": 162, "endOffset": 457}, {"referenceID": 42, "context": "\u2022 Publicly available gazetteers, widely used in previous NERC systems (Tjong Kim Sang and De Meulder, 2003; Nadeau and Sekine, 2007).", "startOffset": 70, "endOffset": 132}, {"referenceID": 59, "context": "We implement the following feature set, partially inspired by previous work (Zhang and Johnson, 2003):", "startOffset": 76, "endOffset": 101}, {"referenceID": 32, "context": "This type of semi-supervised learning may be aimed at improving performance over a fixed amount of training data or, given a fixed target performance level, to establish how much supervised data is actually required to reach such performance (Koo et al., 2008).", "startOffset": 242, "endOffset": 260}, {"referenceID": 46, "context": "So far the most successful approaches have only used one type of word representation (Passos et al., 2014; Faruqui et al., 2010; Ratinov and Roth, 2009).", "startOffset": 85, "endOffset": 152}, {"referenceID": 22, "context": "So far the most successful approaches have only used one type of word representation (Passos et al., 2014; Faruqui et al., 2010; Ratinov and Roth, 2009).", "startOffset": 85, "endOffset": 152}, {"referenceID": 50, "context": "So far the most successful approaches have only used one type of word representation (Passos et al., 2014; Faruqui et al., 2010; Ratinov and Roth, 2009).", "startOffset": 85, "endOffset": 152}, {"referenceID": 50, "context": "However, for comparison purposes, we decided to use word representations previously used in successful NERC approaches: Brown clusters (Ratinov and Roth, 2009; Turian et al., 2010), Word2vec clusters (Passos et al.", "startOffset": 135, "endOffset": 180}, {"referenceID": 58, "context": "However, for comparison purposes, we decided to use word representations previously used in successful NERC approaches: Brown clusters (Ratinov and Roth, 2009; Turian et al., 2010), Word2vec clusters (Passos et al.", "startOffset": 135, "endOffset": 180}, {"referenceID": 46, "context": ", 2010), Word2vec clusters (Passos et al., 2014) and Clark clusters (Finkel et al.", "startOffset": 27, "endOffset": 48}, {"referenceID": 23, "context": ", 2014) and Clark clusters (Finkel et al., 2005; Faruqui et al., 2010).", "startOffset": 27, "endOffset": 70}, {"referenceID": 22, "context": ", 2014) and Clark clusters (Finkel et al., 2005; Faruqui et al., 2010).", "startOffset": 27, "endOffset": 70}, {"referenceID": 9, "context": "The Brown clustering algorithm (Brown et al., 1992) is a hierarchical algorithm which clusters words to maximize the mutual information of bigrams.", "startOffset": 31, "endOffset": 51}, {"referenceID": 38, "context": "Therefore, by using paths of various lengths, we obtain clustering features of different granularities (Miller et al., 2004).", "startOffset": 103, "endOffset": 124}, {"referenceID": 50, "context": "We use paths of length 4, 6, 10 and 20 as features (Ratinov and Roth, 2009).", "startOffset": 51, "endOffset": 75}, {"referenceID": 34, "context": "Furthermore, we follow previous work and remove all sentences which consist of less than 90% lowercase characters (Liang, 2005; Turian et al., 2010) before inducing the Brown clusters.", "startOffset": 114, "endOffset": 148}, {"referenceID": 58, "context": "Furthermore, we follow previous work and remove all sentences which consist of less than 90% lowercase characters (Liang, 2005; Turian et al., 2010) before inducing the Brown clusters.", "startOffset": 114, "endOffset": 148}, {"referenceID": 34, "context": "We use the publicly available tool implemented by Liang (2005) with default settings.", "startOffset": 50, "endOffset": 63}, {"referenceID": 17, "context": "These approaches produce representation of words as continuous vectors (Collobert and Weston, 2008; Mnih and Hinton, 2007), also called word embeddings.", "startOffset": 71, "endOffset": 122}, {"referenceID": 41, "context": "These approaches produce representation of words as continuous vectors (Collobert and Weston, 2008; Mnih and Hinton, 2007), also called word embeddings.", "startOffset": 71, "endOffset": 122}, {"referenceID": 37, "context": "Nowadays, perhaps the most popular among them is the Skip-gram algorithm (Mikolov et al., 2013).", "startOffset": 73, "endOffset": 95}, {"referenceID": 37, "context": "Their objective is to produce word embeddings by computing the probability of each n-gram as the product of the conditional probabilities of each context word in the n-gram conditioned on its central word (Mikolov et al., 2013).", "startOffset": 205, "endOffset": 227}, {"referenceID": 16, "context": "These approaches produce representation of words as continuous vectors (Collobert and Weston, 2008; Mnih and Hinton, 2007), also called word embeddings. Nowadays, perhaps the most popular among them is the Skip-gram algorithm (Mikolov et al., 2013). The Skip-gram algorithm uses shallow log-linear models to compute vector representation of words which are more efficient than previous word representations induced on neural language models. Their objective is to produce word embeddings by computing the probability of each n-gram as the product of the conditional probabilities of each context word in the n-gram conditioned on its central word (Mikolov et al., 2013). Instead of using continuous vectors as real numbers, we induce clusters or word classes from the word vectors by applying K-means clustering. In this way we can use the cluster classes as simple binary features by injecting unigram match features. We use the Word2vec tool released by Mikolov et al. (2013) with a 5 window context to train 50-dimensional word embeddings and to obtain the word clusters on top of them.", "startOffset": 72, "endOffset": 978}, {"referenceID": 5, "context": "de Wikipedia (20140725) 650 190 190 650 deWac (Baroni et al., 2009) 1100 500 500 1100", "startOffset": 46, "endOffset": 67}, {"referenceID": 50, "context": "For every model marked with \u201cdict\u201d we use the thirty English Illinois NER gazetteers (Ratinov and Roth, 2009), irrespective of the target language.", "startOffset": 85, "endOffset": 109}, {"referenceID": 0, "context": "The pre-processing and tokenization is performed with the IXA pipes tools (Agerri et al., 2014).", "startOffset": 74, "endOffset": 95}, {"referenceID": 44, "context": "We also experiment with dictionary features but, in contrast to previous approaches such as Passos et al. (2014), we only use currently available gazetteers off-the-shelf.", "startOffset": 92, "endOffset": 113}, {"referenceID": 46, "context": "Table 5 shows that three of our models outperform previous best results reported for English in the CoNLL 2003 dataset (Passos et al., 2014).", "startOffset": 119, "endOffset": 140}, {"referenceID": 50, "context": "As the development data is closer in style and genre to the training data (Ratinov and Roth, 2009), this may suggest that our system generalizes better on test data that is not close to the training data; indeed, the results reported in Section 4.", "startOffset": 74, "endOffset": 98}, {"referenceID": 46, "context": "Table 5 shows that three of our models outperform previous best results reported for English in the CoNLL 2003 dataset (Passos et al., 2014). Note that the best F1 score (91.36) is obtained by adding trigrams and character n-gram features to the best model (91.18). The results also show that these models improve the baseline provided by the local features by around 7 points in F1 score. The most significant gain is in terms of recall, almost 9 points better than the baseline. We also report very competitive results, only marginally lower than Passos et al. (2014), based on the stacking and combination of clustering features as described in Section 3.", "startOffset": 120, "endOffset": 570}, {"referenceID": 46, "context": "Table 5 shows that three of our models outperform previous best results reported for English in the CoNLL 2003 dataset (Passos et al., 2014). Note that the best F1 score (91.36) is obtained by adding trigrams and character n-gram features to the best model (91.18). The results also show that these models improve the baseline provided by the local features by around 7 points in F1 score. The most significant gain is in terms of recall, almost 9 points better than the baseline. We also report very competitive results, only marginally lower than Passos et al. (2014), based on the stacking and combination of clustering features as described in Section 3.3.4. Thus, both best cluster and comp models, based on local plus clustering features only, outperform very competitive and more complex systems such as those of Ratinov and Roth (2009) and Turian et al.", "startOffset": 120, "endOffset": 844}, {"referenceID": 46, "context": "Table 5 shows that three of our models outperform previous best results reported for English in the CoNLL 2003 dataset (Passos et al., 2014). Note that the best F1 score (91.36) is obtained by adding trigrams and character n-gram features to the best model (91.18). The results also show that these models improve the baseline provided by the local features by around 7 points in F1 score. The most significant gain is in terms of recall, almost 9 points better than the baseline. We also report very competitive results, only marginally lower than Passos et al. (2014), based on the stacking and combination of clustering features as described in Section 3.3.4. Thus, both best cluster and comp models, based on local plus clustering features only, outperform very competitive and more complex systems such as those of Ratinov and Roth (2009) and Turian et al. (2010), and obtain only marginally lower results than Passos et al.", "startOffset": 120, "endOffset": 869}, {"referenceID": 46, "context": "Table 5 shows that three of our models outperform previous best results reported for English in the CoNLL 2003 dataset (Passos et al., 2014). Note that the best F1 score (91.36) is obtained by adding trigrams and character n-gram features to the best model (91.18). The results also show that these models improve the baseline provided by the local features by around 7 points in F1 score. The most significant gain is in terms of recall, almost 9 points better than the baseline. We also report very competitive results, only marginally lower than Passos et al. (2014), based on the stacking and combination of clustering features as described in Section 3.3.4. Thus, both best cluster and comp models, based on local plus clustering features only, outperform very competitive and more complex systems such as those of Ratinov and Roth (2009) and Turian et al. (2010), and obtain only marginally lower results than Passos et al. (2014). The stacking and combining effect manifests itself very clearly when we compare the single clustering feature models (BR, CW600, W2VG200 and W2VW400) with the light, comp and best cluster models which improve the overall F1 score by 1.", "startOffset": 120, "endOffset": 937}, {"referenceID": 14, "context": "50 L + Clark wiki 600 (CW600) 93.98 90.58 92.24 90.85 87.16 88.97 L + Word2vec giga 200 (W2VG200) 93.16 89.90 91.45 89.64 85.06 87.29 L + Word2vec wiki 400 (W2VW400) 93.22 90.02 91.59 88.98 85.09 86.99 L + BR + CW600 + W2VW400 (light) 94.16 91.96 93.04 91.20 89.36 90.27 light + CR600 + W2VG200 (comp) 94.32 92.22 93.26 91.75 89.64 90.69 comp + BW (best cluster) 94.21 92.23 93.26 91.67 89.98 90.82 comp + dict 94.60 92.78 93.68 91.86 90.53 91.19 BR+CR600-CW600+W2VG200+dict 94.58 92.53 93.54 92.20 90.19 91.18 charngram 1:6 + en-91-18 94.56 92.81 93.68 92.16 90.56 91.36 Stanford NER (distsim-conll03) 93.64 92.27 92.95 89.37 87.95 88.65 Illinois NER - - 93.50 n/a n/a 90.57 Turian et al. (2010) 94.", "startOffset": 7, "endOffset": 697}, {"referenceID": 14, "context": "50 L + Clark wiki 600 (CW600) 93.98 90.58 92.24 90.85 87.16 88.97 L + Word2vec giga 200 (W2VG200) 93.16 89.90 91.45 89.64 85.06 87.29 L + Word2vec wiki 400 (W2VW400) 93.22 90.02 91.59 88.98 85.09 86.99 L + BR + CW600 + W2VW400 (light) 94.16 91.96 93.04 91.20 89.36 90.27 light + CR600 + W2VG200 (comp) 94.32 92.22 93.26 91.75 89.64 90.69 comp + BW (best cluster) 94.21 92.23 93.26 91.67 89.98 90.82 comp + dict 94.60 92.78 93.68 91.86 90.53 91.19 BR+CR600-CW600+W2VG200+dict 94.58 92.53 93.54 92.20 90.19 91.18 charngram 1:6 + en-91-18 94.56 92.81 93.68 92.16 90.56 91.36 Stanford NER (distsim-conll03) 93.64 92.27 92.95 89.37 87.95 88.65 Illinois NER - - 93.50 n/a n/a 90.57 Turian et al. (2010) 94.11 93.81 93.95 90.10 90.61 90.36 Passos et al. (2014) - - 94.", "startOffset": 7, "endOffset": 754}, {"referenceID": 23, "context": "We evaluated their CoNLL model and, while the result is substantially better than their reference paper (Finkel et al., 2005), our clustering models obtain better results.", "startOffset": 104, "endOffset": 125}, {"referenceID": 23, "context": "We evaluated their CoNLL model and, while the result is substantially better than their reference paper (Finkel et al., 2005), our clustering models obtain better results. The Illinois NER tagger is used by Ratinov and Roth (2009) and Turian et al.", "startOffset": 105, "endOffset": 231}, {"referenceID": 23, "context": "We evaluated their CoNLL model and, while the result is substantially better than their reference paper (Finkel et al., 2005), our clustering models obtain better results. The Illinois NER tagger is used by Ratinov and Roth (2009) and Turian et al. (2010), both of which are outperformed by our system.", "startOffset": 105, "endOffset": 256}, {"referenceID": 27, "context": "20 ExB (H\u00e4nig et al., 2014) 80.", "startOffset": 7, "endOffset": 27}, {"referenceID": 51, "context": "09 UKP (Reimers et al., 2014) 79.", "startOffset": 7, "endOffset": 29}, {"referenceID": 7, "context": "We also compare our system, in the last three rows, with the publicly available GermaNER (Benikova et al., 2015), which reports results for the 4 main outer level entity types (person, location, organization and other).", "startOffset": 89, "endOffset": 112}, {"referenceID": 14, "context": "57 L + Clark deWac 500 (CWac500) 82.40 65.69 73.11 82.76 67.33 74.25 L + Word2vec deWac 100 (W2VWac100) 81.50 63.52 71.40 81.96 66.46 73.41 CWac500 + W2VWac100 (de-cluster) 83.35 68.38 75.13 83.43 68.96 75.51 de-cluster + dict 85.20 70.54 77.18 83.72 70.30 76.42 Florian et al. (2003) 84.", "startOffset": 7, "endOffset": 285}, {"referenceID": 14, "context": "57 L + Clark deWac 500 (CWac500) 82.40 65.69 73.11 82.76 67.33 74.25 L + Word2vec deWac 100 (W2VWac100) 81.50 63.52 71.40 81.96 66.46 73.41 CWac500 + W2VWac100 (de-cluster) 83.35 68.38 75.13 83.43 68.96 75.51 de-cluster + dict 85.20 70.54 77.18 83.72 70.30 76.42 Florian et al. (2003) 84.60 61.93 71.51 80.19 63.71 72.41 Faruqui and Pad\u00f3 (2010) 86.", "startOffset": 7, "endOffset": 345}, {"referenceID": 22, "context": "Our best CoNLL 2003 model obtains results similar to the state of the art performance with respect to the best system published up to date (Faruqui et al., 2010) using public data.", "startOffset": 139, "endOffset": 161}, {"referenceID": 21, "context": "Our best CoNLL 2003 model obtains results similar to the state of the art performance with respect to the best system published up to date (Faruqui et al., 2010) using public data. Faruqui et al. (2010) also report 78.", "startOffset": 140, "endOffset": 203}, {"referenceID": 45, "context": "(2002), is distributed as part of the Freeling library (Padr\u00f3 and Stanilovsky, 2012).", "startOffset": 55, "endOffset": 84}, {"referenceID": 11, "context": "The best system up to date on the CoNLL 2002 dataset, originally published by Carreras et al. (2002), is distributed as part of the Freeling library (Padr\u00f3 and Stanilovsky, 2012).", "startOffset": 78, "endOffset": 101}, {"referenceID": 11, "context": "14 Carreras et al. (2002) 79.", "startOffset": 3, "endOffset": 26}, {"referenceID": 15, "context": "Despite using clusters from one data source only (see Table 4), results in Table 10 show that our nl-cluster model outperforms the best result published on CoNLL 2002 (Clark and Curran, 2003) by 3.", "startOffset": 167, "endOffset": 191}, {"referenceID": 50, "context": "Adding the English Illinois NER gazetteers (Ratinov and Roth, 2009) and trigram and character n-gram features increases the score to 85.", "startOffset": 43, "endOffset": 67}, {"referenceID": 11, "context": "04 Carreras et al. (2002) 76.", "startOffset": 3, "endOffset": 26}, {"referenceID": 11, "context": "04 Carreras et al. (2002) 76.52 74.82 75.66 77.83 76.29 77.05 Curran and Clark (2003) - - - 79.", "startOffset": 3, "endOffset": 86}, {"referenceID": 20, "context": "We also compared our system with the more recently developed SONAR-1 corpus and the companion NERD system distributed inside its release (Desmet and Hoste, 2014).", "startOffset": 137, "endOffset": 161}, {"referenceID": 3, "context": "However, for direct comparison with previous work (Alegria et al., 2006), we also evaluate our best eu-cluster model (trained on 3 classes) on 4 classes.", "startOffset": 50, "endOffset": 72}, {"referenceID": 3, "context": "These results are particularly interesting as it had been so far assumed that complex linguistic features and language-specific rules were required to perform well for agglutinative languages such as Basque (Alegria et al., 2006).", "startOffset": 207, "endOffset": 229}, {"referenceID": 3, "context": "Table 12 reports on the experiments using the Egunkaria NER dataset provided by Alegria et al. (2006). Due to the sparsity of the MISC class mentioned in Section 2.", "startOffset": 80, "endOffset": 102}, {"referenceID": 3, "context": "40 Alegria et al. (2006) 72.", "startOffset": 3, "endOffset": 25}, {"referenceID": 50, "context": "We have also re-trained the Illinois NER system (Ratinov and Roth, 2009) and our best CoNLL 2003 model (en-91-18 ) for comparison.", "startOffset": 48, "endOffset": 72}, {"referenceID": 50, "context": "Finally, training on just a quarter of the training set (60K) results in a very competitive model when compared with other publicly available NER systems for English trained on the full training set: it roughly matches Stanford NER\u2019s performance, it outperforms models using external knowledge or non-local features reported by Ratinov and Roth (2009), and also several models reported by Turian et al.", "startOffset": 328, "endOffset": 352}, {"referenceID": 50, "context": "Finally, training on just a quarter of the training set (60K) results in a very competitive model when compared with other publicly available NER systems for English trained on the full training set: it roughly matches Stanford NER\u2019s performance, it outperforms models using external knowledge or non-local features reported by Ratinov and Roth (2009), and also several models reported by Turian et al. (2010), which use one type of word representations on top of the baseline system.", "startOffset": 328, "endOffset": 410}, {"referenceID": 50, "context": "For example, consider the following sentence of the MUC 7 gold standard (example taken from Ratinov and Roth (2009)):", "startOffset": 92, "endOffset": 116}, {"referenceID": 50, "context": "Following previous work (Ratinov and Roth, 2009), every named entity that is not LOC, ORG, PER or MISC is labeled as \u2018O\u2019.", "startOffset": 24, "endOffset": 48}, {"referenceID": 20, "context": "For Dutch we compare our SONAR-1 system with the companion system distributed with the SONAR-1 corpus (Desmet and Hoste, 2014).", "startOffset": 102, "endOffset": 126}, {"referenceID": 49, "context": "Following previous work (Ratinov and Roth, 2009), every named entity that is not LOC, ORG, PER or MISC is labeled as \u2018O\u2019. Additionally for MUC 7 every MISC named entity is changed to \u2018O\u2019. For English we used the models reported in Section 4.1.1. For Spanish and Dutch we trained our system with the Ancora and SONAR-1 corpora using the configurations described in Sections 4.1.3 and 4.1.4 respectively. Table 16 compares our results with previous approaches: using MUC 7, Turian et al. (2010) provide standard phrase results whereas Ratinov and Roth (2009) score token based F1 results, namely, each token is considered a chunk, instead of considering multi-token spans too.", "startOffset": 25, "endOffset": 493}, {"referenceID": 49, "context": "Following previous work (Ratinov and Roth, 2009), every named entity that is not LOC, ORG, PER or MISC is labeled as \u2018O\u2019. Additionally for MUC 7 every MISC named entity is changed to \u2018O\u2019. For English we used the models reported in Section 4.1.1. For Spanish and Dutch we trained our system with the Ancora and SONAR-1 corpora using the configurations described in Sections 4.1.3 and 4.1.4 respectively. Table 16 compares our results with previous approaches: using MUC 7, Turian et al. (2010) provide standard phrase results whereas Ratinov and Roth (2009) score token based F1 results, namely, each token is considered a chunk, instead of considering multi-token spans too.", "startOffset": 25, "endOffset": 557}, {"referenceID": 44, "context": "Wikipedia (Nothman et al., 2013).", "startOffset": 10, "endOffset": 32}, {"referenceID": 44, "context": "Wikipedia (Nothman et al., 2013). They report results on Wikigold showing that they outperformed their own CoNLL 2003 gold-standard model by 10 points in F1 score. We compare their result with our best cluster model in Table 17. While the results of our baseline model confirms theirs, our clustering model score is slightly higher. This result is interesting because it is arguably more simple to induce the clusters we use to train ixa-pipe-nerc rather than create the silver standard training set from Wikipedia as described in Nothman et al. (2013).", "startOffset": 11, "endOffset": 553}, {"referenceID": 44, "context": "12 en-wiki2 (Nothman et al. 2013) 64.", "startOffset": 12, "endOffset": 33}, {"referenceID": 57, "context": "However, differences in the gold standard annotation result in significant disagreements regarding the span of the named entities (Tonelli et al., 2014).", "startOffset": 130, "endOffset": 152}, {"referenceID": 50, "context": "Instead, injecting unigram knowledge from the combination and stacking of clusters allows to obtain a robust NERC system across languages, outperforming other, more complex (Ratinov and Roth, 2009; Turian et al., 2010; Desmet and Hoste, 2014; Passos et al., 2014) and language-specific systems.", "startOffset": 173, "endOffset": 263}, {"referenceID": 58, "context": "Instead, injecting unigram knowledge from the combination and stacking of clusters allows to obtain a robust NERC system across languages, outperforming other, more complex (Ratinov and Roth, 2009; Turian et al., 2010; Desmet and Hoste, 2014; Passos et al., 2014) and language-specific systems.", "startOffset": 173, "endOffset": 263}, {"referenceID": 20, "context": "Instead, injecting unigram knowledge from the combination and stacking of clusters allows to obtain a robust NERC system across languages, outperforming other, more complex (Ratinov and Roth, 2009; Turian et al., 2010; Desmet and Hoste, 2014; Passos et al., 2014) and language-specific systems.", "startOffset": 173, "endOffset": 263}, {"referenceID": 46, "context": "Instead, injecting unigram knowledge from the combination and stacking of clusters allows to obtain a robust NERC system across languages, outperforming other, more complex (Ratinov and Roth, 2009; Turian et al., 2010; Desmet and Hoste, 2014; Passos et al., 2014) and language-specific systems.", "startOffset": 173, "endOffset": 263}, {"referenceID": 3, "context": ") has so far been pervasive (Alegria et al., 2006; Benikova et al., 2014, 2015).", "startOffset": 28, "endOffset": 79}, {"referenceID": 44, "context": "This is even the case for the NER tasks organized at CoNLL 2002 and 2003: \u201cFor instance, Spanish marks no lowercase adjectival nationalities and includes 192 instances where surrounding quotes are included in the entity annotation; Dutch has as PER the initials of photographers; and English has lots of financial and sports data in tables\u201d (Nothman et al., 2013).", "startOffset": 341, "endOffset": 363}, {"referenceID": 58, "context": "Choosing the right corpus and clustering method Contrary to previous suggestions that the larger the number of classes and the corpus used to induced the clusters the better (Turian et al., 2010), our results provide a number of interesting pointers to choose the appropriate type of corpus and clustering method required for optimal performance.", "startOffset": 174, "endOffset": 195}, {"referenceID": 14, "context": "For best performance, Clark (2003) recommends that the proportion of clusters k with respect to the source data should be of k \u2248 n where n is the number of words in the corpus.", "startOffset": 22, "endOffset": 35}, {"referenceID": 14, "context": "For best performance, Clark (2003) recommends that the proportion of clusters k with respect to the source data should be of k \u2248 n where n is the number of words in the corpus. Instead, we systematically induce, for every corpus, Clark clusters in the range of 100-600 classes, because preliminary experiments proved that over 600 classes, even if the proportion proposed by Clark holds, performance starts to deteriorate. Following this, we are now in better position to address the questions posed by Turian et al. (2010):", "startOffset": 22, "endOffset": 524}, {"referenceID": 58, "context": "Previous approaches to NERC combining clusters or word embeddings have obtained mixed results (Turian et al., 2010).", "startOffset": 94, "endOffset": 115}, {"referenceID": 46, "context": "Up until now best results have been based on rather complex systems which also used one type clustering or embedding feature (Passos et al., 2014; Ratinov and Roth, 2009; Faruqui et al., 2010; Benikova et al., 2014).", "startOffset": 125, "endOffset": 215}, {"referenceID": 50, "context": "Up until now best results have been based on rather complex systems which also used one type clustering or embedding feature (Passos et al., 2014; Ratinov and Roth, 2009; Faruqui et al., 2010; Benikova et al., 2014).", "startOffset": 125, "endOffset": 215}, {"referenceID": 22, "context": "Up until now best results have been based on rather complex systems which also used one type clustering or embedding feature (Passos et al., 2014; Ratinov and Roth, 2009; Faruqui et al., 2010; Benikova et al., 2014).", "startOffset": 125, "endOffset": 215}, {"referenceID": 6, "context": "Up until now best results have been based on rather complex systems which also used one type clustering or embedding feature (Passos et al., 2014; Ratinov and Roth, 2009; Faruqui et al., 2010; Benikova et al., 2014).", "startOffset": 125, "endOffset": 215}, {"referenceID": 58, "context": "In our opinion, these results are quite interesting as previous experiments combining features of different word representations for NERC (Turian et al., 2010), while increasing the overall result, did not improve over the state of the art at the time (Ratinov and Roth, 2009).", "startOffset": 138, "endOffset": 159}, {"referenceID": 50, "context": ", 2010), while increasing the overall result, did not improve over the state of the art at the time (Ratinov and Roth, 2009).", "startOffset": 100, "endOffset": 124}, {"referenceID": 6, "context": ", 2010; Benikova et al., 2014). In other sequence labeling tasks, Biemann (2009) reports a slight improvement (from 97.", "startOffset": 8, "endOffset": 81}, {"referenceID": 6, "context": ", 2010; Benikova et al., 2014). In other sequence labeling tasks, Biemann (2009) reports a slight improvement (from 97.33 to 97.43 word accuracy) in POS tagging combining two types of clustering methods (one of them was Clark (2003)) for German.", "startOffset": 8, "endOffset": 233}, {"referenceID": 6, "context": ", 2010; Benikova et al., 2014). In other sequence labeling tasks, Biemann (2009) reports a slight improvement (from 97.33 to 97.43 word accuracy) in POS tagging combining two types of clustering methods (one of them was Clark (2003)) for German. Our system displays two important differences with respect to previous approaches. First, the differences between our baseline system and the, for example, Clark features are much larger than in previous work (with the exception of Faruqui et al. (2010)), ranging from 2.", "startOffset": 8, "endOffset": 500}, {"referenceID": 31, "context": "Robust reducing training data Koo et al. (2008) present learning curves showing the increase in performance when using Brown clusters for dependency parsing whereas Biemann (2009) provides learning curves to measure the impact of clusters for NERC and chunking.", "startOffset": 30, "endOffset": 48}, {"referenceID": 8, "context": "(2008) present learning curves showing the increase in performance when using Brown clusters for dependency parsing whereas Biemann (2009) provides learning curves to measure the impact of clusters for NERC and chunking.", "startOffset": 124, "endOffset": 139}, {"referenceID": 8, "context": "(2008) present learning curves showing the increase in performance when using Brown clusters for dependency parsing whereas Biemann (2009) provides learning curves to measure the impact of clusters for NERC and chunking. Inspired by those two previous works we measured the performance when training data is reduced. Unlike these two approaches, the differences between adding clusters or not to our system with less training data is huge. Table 14 shows that differences adding the clustering features with half the data is around 8 points in F1 score (for Spanish the difference is 3.42 F1). Another common point of our clustering features with Koo et al. (2008) is that when gold training data is reduced, the system still obtains competitive results with respect to previous approaches or publicly available systems using only a fraction (half or a quarter) of the data.", "startOffset": 124, "endOffset": 665}, {"referenceID": 13, "context": ", 2014, 2015) and Super Sense tagging (Ciaramita and Altun, 2006).", "startOffset": 38, "endOffset": 65}], "year": 2017, "abstractText": "We present a multilingual Named Entity Recognition approach based on a robust and general set of features across languages and datasets. Our system combines shallow local information with clustering semi-supervised features induced on large amounts of unlabeled text. Understanding via empirical experimentation how to effectively combine various types of clustering features allows us to seamlessly export our system to other datasets and languages. The result is a simple but highly competitive system which obtains state of the art results across five languages and twelve datasets. The results are reported on standard shared task evaluation data such as CoNLL for English, Spanish and Dutch. Furthermore, and despite the lack of linguistically motivated features, we also report best results for languages such as Basque and German. In addition, we demonstrate that our method also obtains very competitive results even when the amount of supervised data is cut by half, alleviating the dependency on manually annotated data. Finally, the results show that our emphasis on clustering features is crucial to develop robust out-of-domain models. The system and models are freely available to facilitate its use and guarantee the reproducibility of results.", "creator": "LaTeX with hyperref package"}}}