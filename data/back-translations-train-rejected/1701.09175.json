{"id": "1701.09175", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2017", "title": "Skip Connections Eliminate Singularities", "abstract": "Skip connections made the training of very deep neural networks possible and have become an indispendable component in a variety of neural architectures. A satisfactory explanation for their success remains elusive. Here, we present an explanation for the benefits of skip connections in training very deep neural networks. We argue that skip connections help break symmetries inherent in the loss landscapes of deep networks, leading to drastically simplified landscapes. In particular, skip connections between adjacent layers in a multilayer network break the permutation symmetry of nodes in a given layer, and the recently proposed DenseNet architecture, where each layer projects skip connections to every layer above it, also breaks the rescaling symmetry of connectivity matrices between different layers. This hypothesis is supported by evidence from a toy model with binary weights and from experiments with fully-connected networks suggesting (i) that skip connections do not necessarily improve training unless they help break symmetries and (ii) that alternative ways of breaking the symmetries also lead to significant performance improvements in training deep networks, hence there is nothing special about skip connections in this respect. We find, however, that skip connections confer additional benefits over and above symmetry-breaking, such as the ability to deal effectively with the vanishing gradients problem.", "histories": [["v1", "Tue, 31 Jan 2017 18:41:07 GMT  (486kb,D)", "http://arxiv.org/abs/1701.09175v1", "16 pages, 12 figures, 1 supplementary figure"], ["v2", "Thu, 2 Feb 2017 17:53:43 GMT  (486kb,D)", "http://arxiv.org/abs/1701.09175v2", "18 pages, 12 figures, 1 supplementary figure"], ["v3", "Thu, 9 Feb 2017 18:33:24 GMT  (486kb,D)", "http://arxiv.org/abs/1701.09175v3", "18 pages, 12 figures, 1 supplementary figure"], ["v4", "Mon, 13 Feb 2017 16:47:10 GMT  (487kb,D)", "http://arxiv.org/abs/1701.09175v4", "18 pages, 12 figures, 1 supplementary figure"], ["v5", "Mon, 22 May 2017 15:18:09 GMT  (766kb,D)", "http://arxiv.org/abs/1701.09175v5", "16 pages, 8 figures, 2 supplementary figure"], ["v6", "Wed, 24 May 2017 16:36:39 GMT  (766kb,D)", "http://arxiv.org/abs/1701.09175v6", "16 pages, 8 figures, 2 supplementary figures, v6 adds Acknowledgements"], ["v7", "Tue, 20 Jun 2017 17:50:41 GMT  (766kb,D)", "http://arxiv.org/abs/1701.09175v7", "16 pages, 8 figures, 2 supplementary figures, v7 fixes typos"]], "COMMENTS": "16 pages, 12 figures, 1 supplementary figure", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["a emin orhan", "xaq pitkow"], "accepted": false, "id": "1701.09175"}, "pdf": {"name": "1701.09175.pdf", "metadata": {"source": "CRF", "title": "Skip Connections as Effective Symmetry-Breaking", "authors": ["Emin Orhan"], "emails": ["aeminorhan@gmail.com"], "sections": [{"heading": null, "text": ""}, {"heading": "1 Introduction 1", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Results 2", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3 Discussion 13", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction", "text": "The introduction of skip (or residual) connections has greatly improved the training of very deep neural networks [6,7,9]. Despite informal intuitions and sometimes worryingly instringent metaphors (\"keeping a\" clean \"information path\" [7] to... \"improve the flow of information between layers\" [9]) that are put forward to motivate skip connections, there is a lack of a clear understanding of how these connections improve training. Such an understanding is invaluable both for itself and for the possibilities it could offer for further improvements in the training of very deep neural networks. A number of recent papers have addressed various aspects of this question [4, 11, 14]. In this paper, we seek to shed further light on this question. We argue that skip connections help break symmetries inherent in the loss of deep neural networks. Symmetries lead to saddle structures in the landscape and cause problems with optimization-based [1,17]."}, {"heading": "2 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Symmetries in fully-connected networks", "text": "In fact, it is the case that it will be able to be in a position in which it is able to move, in which it is able to move, and in which it is able to be in a position."}, {"heading": "2.2 Landscapes of small networks with binary weights", "text": "To illustrate how the connections between the different levels of architecture change, we must first look at a model with seven levels and two nodes in each level, with the exception of the last layer, which has a single node (Figure 3). To fully describe the landscape, we must all engage in a model with 23 binary parameters and a landscape with 223 possible parameters, which we reduce to a two-dimensional regression. We consider the four architectures shown in Figure 3 to be a fully connected architecture."}, {"heading": "2.3 Dynamics of learning in linear networks with skip connections", "text": "Next, we examine how skipped connections affect learning dynamics in linear networks. We remember that in a linear pure L-layer network, the input-output mapping is given by: xL = WL \u2212 1WL \u2212 2.. W1x1 (5), where x1 and xL are the input and output vectors, respectively. In linear residual networks with identity-skipped connections between adjacent layers, the input-output mapping is finally given by: xL = (WL \u2212 1 + I) (WL \u2212 2 + I)... (W1 + I) x1 (6) Finally, in hyperresidual linear networks, the input-output mapping is given by: xL = (WL \u2212 1 + (L \u2212 1) I) (WL \u2212 2 + (L \u2212 2) I). (W1 + I) x1 (7) In the following derivatives, we do not have to assume that the connectivity matrices are quadratic."}, {"heading": "2.3.1 Three-layer networks", "text": "For a three-layer network (L = 3), learning dynamics can be expressed by the following differential equations [18]: \u03c4 ddt a\u03b1 = (s\u03b1 \u00b7 b\u03b1) b\u03b1 \u2212 \u2211 6 = \u03b1 (a\u03b1 \u00b7 b\u03b3) b\u03b1 (8) Number of hidden units (where n is the number of hidden units) that connect the hidden layer to the input and output modes, or to the input correlation matrix and s\u03b1 is the corresponding singular value (see [18] for more details. The first term on the right side of the equations?? facilitates cooperation between a\u03b1 and b\u03b1, which corresponds to the same output mode, is the corresponding singular value."}, {"heading": "2.3.2 Networks with more than three-layers", "text": "As shown in [18], the energy function changes in linear networks with more than a single hidden layer, assuming that there are additional orthogonal matrices Rl and Rl + 1 for each layer l, which diagonalize the initial weight matrix of the corresponding layer (i.e. R > l + 1Wl (0) Rl = Dl is a diagonal matrix), the dynamics of different singular modes decouple from each other, and each mode \u03b1 develops according to the gradient descendant dynamics in an energy landscape described by [18]: Eplain = 12\u03c4 (s\u03b1 \u2212 Nl \u2212 1 = a\u03b1l) 2 (21), a\u03b1l being the strength of mode \u03b1 and Nl representing the total number of layers. In residual networks, it is further assumed that the orthogonal matrices Rl cause the satisfaction of R > l + Relf = hyperfunction \u2212 12res."}, {"heading": "2.4 Experiments with fully-connected networks", "text": "To test the symmetry-breaking hypothesis in more realistic networks, we conducted several experiments with deeply connected, fully networked feed nets. In this section, we present the results of these experiments. We remember that the equations describing the simple networks are given by: xl + 1 = f (Wlxl + bl + 1) (24), where Ql denotes the skip connectivity matrix, which may differ from the identity matrix, and the equations describing the hyper-residual networks by: xl + 1 = f (Wlxl + bl + 1) + Qlxl + 1l \u2212 1 [Ql \u2212 1xl \u2212 1 +.. + Q1x1] (26), where each layer extends over all layers. We divided the contribution from the non-adjacent layers by l \u2212 1, as we chose this layer better than the normal layer (12f version)."}, {"heading": "2.4.1 Alternative ways of breaking the permutation symmetry of hidden units", "text": "If the success of the hidden network architecture is worse in this case than in cases with properly broken symmetry, although the symmetry can be broken, then alternative ways to break the permutation symmetry of the hidden units at the same level are primarily to improve it. We have tested this hypothesis by introducing a particularly simple method to break the permutation symmetry of the hidden units of the hidden units of the hidden units of the hidden units. In particular, we apply an l2 standard penalty on deviations from these biometric values. This imposes a special order on the hidden units according to their target biases and hence breaks their permutation symmetry. Note that setting the l2 standard corresponds to biometry, the biometry biometry biometry does not force the hidden units of the hidden units of the hidden values."}, {"heading": "2.4.2 Non-identity skip connections", "text": "sorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsorsor"}, {"heading": "2.4.3 Hyper-residual networks and breaking the rescaling symmetry", "text": "The results for the hyper-residual networks are shown in Figure 11. In these networks, we chose the constant connectivity matrix between adjacent layers (Ql in Eq.26) to be the identity matrix. To jump connectivity matrices between non-adjacent layers, we used dense random matrices that were double folded starting from an orthogonal matrix (corresponding to matrices called \"32\" in Figure 9).The results show that these hyper-residual networks work better than residential networks with identical connections only between adjacent layers (represented by the solid red lines in Figure 11).In order to prove that the rupture of rescale symmetry contributes to improving the performance of the hyper-residual architecture, we introduced an alternative way to break rescale symmetry and test its effectiveness."}, {"heading": "2.5 Symmetry-breaking in recurrent neural networks", "text": "It is known that a recursive neural network unfolded over time is equivalent to a feedback-forward network with divided weights between successive layers. This suggests that recursive neural networks should have optimization difficulties similar to multilayered feedback networks. In particular, the permutation symmetry of hidden units still contains unfolded recursive networks; however, the rescalation symmetry does not, since the connections between different \"layers\" cannot be reconstructed independently due to the temporal distribution. How can the permutation symmetry of hidden units in recursive neural networks be broken? In recursive vanilla networks, the analogy of adding identity skip connections between adjacent layers would be to switch the recurrent dynamics to (ignoring the distortions in terms of the simplicity of notation): rt = rt \u2212 1 + f (compared to Wrrt \u2212 1 of which this model would differ from the xtr and xtr models) in the model of xtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxtxt"}, {"heading": "3 Discussion", "text": "We argue that connections between adjacent layers in a multi-layer network help to break the permutation symmetry between the hidden units and skip connections between one layer and all layers. This result can be understood from a symmetrical perspective, in which connections between the individual layers are used to disambiguate the hidden units in the next layer, whereas the hidden units in the next layer function better than the typical connections between the layers. This result can be understood from a symmetrical perspective as identity-building connections."}, {"heading": "Acknowledgments", "text": "I would like to thank Xaq Pitkow for helpful discussions and the HPC facilities at NYU for making the experiments reported in this paper possible."}], "references": [{"title": "Singularities affect dynamics of learning in neuromanifolds", "author": ["S Amari", "H Park", "T Ozeki"], "venue": "Neural Comput", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y Bengio", "P Simard", "P Frasconi"], "venue": "IEEE Trans Neural Netw", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Reducing overfitting in deep networks by decorrelating representations. arXiv:1511.06068", "author": ["M Cogswell", "F Ahmed", "R Girshick", "L Zitnick", "D Batra"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K He", "X Zhang", "S Ren", "J Sun"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Identity mappings in deep residual networks. arXiv:1603.05027", "author": ["K He", "X Zhang", "S Ren", "J Sun"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["S Hochreiter"], "venue": "Diploma thesis, Institut f. Informatik,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "Densely connected convolutional networks. arXiv:1608.06993", "author": ["G Huang", "Z Liu", "KQ Weinberger", "L van der Maaten"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S Ioffe", "C Szegedy"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Bridging the gaps between residual learning, recurrent neural networks and visual cortex", "author": ["Q Liao", "T Poggio"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Learning deep parsimonious representations", "author": ["R Liao", "AG Schwing", "RS Zemel", "R Urtasun"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "The loss surface of residual networks: ensembles and the role of batch normalization", "author": ["E Littwin", "L Wolf"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Path-SGD: Path-normalized optimization in deep neural networks. arXiv:1506.02617", "author": ["B Neyshabur", "R Salakhutdinov", "N Srebro"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "On-line learning in soft committee machines", "author": ["D Saad", "SA Solla"], "venue": "Phys Rev E", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv:1312.6120", "author": ["AM Saxe", "JM McClelland", "S Ganguli"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "Introduction of skip (or residual) connections has substantially improved the training of very deep neural networks [6,7,9].", "startOffset": 116, "endOffset": 123}, {"referenceID": 4, "context": "Introduction of skip (or residual) connections has substantially improved the training of very deep neural networks [6,7,9].", "startOffset": 116, "endOffset": 123}, {"referenceID": 6, "context": "Introduction of skip (or residual) connections has substantially improved the training of very deep neural networks [6,7,9].", "startOffset": 116, "endOffset": 123}, {"referenceID": 4, "context": "Despite informal intuitions and sometimes worryingly non-rigorous metaphors (\u201ckeeping a \u201cclean\u201d information path\u201d [7], \u201cto .", "startOffset": 114, "endOffset": 117}, {"referenceID": 6, "context": "improve the information flow between layers\u201d [9]) put forward to motivate skip connections, a clear understanding of how these connections improve training has been lacking.", "startOffset": 45, "endOffset": 48}, {"referenceID": 10, "context": "A number of recent papers addressed different aspects of this question [4, 11, 14].", "startOffset": 71, "endOffset": 82}, {"referenceID": 0, "context": "Symmetries lead to saddle structures in the landscape, causing problems for gradient-based optimization methods [1,17].", "startOffset": 112, "endOffset": 118}, {"referenceID": 12, "context": "Symmetries lead to saddle structures in the landscape, causing problems for gradient-based optimization methods [1,17].", "startOffset": 112, "endOffset": 118}, {"referenceID": 6, "context": "We show that skip connections between adjacent layers break the permutation symmetry of nodes at a given layer, whereas the more recently introduced DenseNet architecture [9], where each layer projects skip connections to every", "startOffset": 171, "endOffset": 174}, {"referenceID": 11, "context": "A similar rescaling symmetry has been discussed before [15], where it has been noted that the incoming weights of any given node can be scaled up by a constant and its outgoing weights scaled down by the same constant without changing the function computed by the network and a modified stochastic gradient descent (SGD) algorithm has been proposed to effectively break this rescaling symmetry.", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Additional skip connections between each layer and all layers above it, as in the DenseNet architecture introduced in [9], break the rescaling symmetry of the weight matrices by adding distinct sets of skip vectors to each layer (Figure 2b).", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "This architecture is inspired by the DenseNet architecture introduced in [9].", "startOffset": 73, "endOffset": 76}, {"referenceID": 13, "context": "1 Three-layer networks Dynamics of learning in plain linear networks with no skip connections was analyzed in [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "For a three-layer network (L = 3), the learning dynamics can be expressed by the following differential equations [18]:", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "Here a and b are n-dimensional column vectors (where n is the number of hidden units) connecting the hidden layer to the \u03b1-th input and output modes, respectively, of the input-output correlation matrix and s\u03b1 is the corresponding singular value (see [18] for further details).", "startOffset": 251, "endOffset": 255}, {"referenceID": 0, "context": "Formally, these correspond to the singularities of the Fisher information matrix, or the Hessian [1].", "startOffset": 97, "endOffset": 100}, {"referenceID": 0, "context": "The effect of such saddle structures on the learning dynamics has previously been analyzed in shallow non-linear feedforward networks [1,17]: in particular, it has been shown that they significantly slow down learning.", "startOffset": 134, "endOffset": 140}, {"referenceID": 12, "context": "The effect of such saddle structures on the learning dynamics has previously been analyzed in shallow non-linear feedforward networks [1,17]: in particular, it has been shown that they significantly slow down learning.", "startOffset": 134, "endOffset": 140}, {"referenceID": 0, "context": "In practice, the variables are initialized randomly and hence they eventually escape the vicinity of the slow manifolds, but the manifolds can exert their effect for a long time [1,17].", "startOffset": 178, "endOffset": 184}, {"referenceID": 12, "context": "In practice, the variables are initialized randomly and hence they eventually escape the vicinity of the slow manifolds, but the manifolds can exert their effect for a long time [1,17].", "startOffset": 178, "endOffset": 184}, {"referenceID": 13, "context": "The derivation proceeds essentially identically to the corresponding derivation for plain networks in [18].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "2 Networks with more than three-layers As shown in [18], in linear networks with more than a single hidden layer, assuming that there are orthogonal matrices Rl and Rl+1 for each layer l that diagonalize the initial weight matrix of the corresponding layer (i.", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "R > l+1Wl(0)Rl = Dl is a diagonal matrix), dynamics of different singular modes decouple from each other and each mode \u03b1 evolves according to gradient descent dynamics in an energy landscape described by [18]:", "startOffset": 204, "endOffset": 208}, {"referenceID": 2, "context": "More complicated joint priors over hidden unit responses that favor decorrelated [3] or clustered [13] responses have been proposed before.", "startOffset": 81, "endOffset": 84}, {"referenceID": 9, "context": "More complicated joint priors over hidden unit responses that favor decorrelated [3] or clustered [13] responses have been proposed before.", "startOffset": 98, "endOffset": 102}, {"referenceID": 1, "context": "One of those advantages is its ability to deal effectively with the vanishing gradients problem encountered in training deep networks [2, 8].", "startOffset": 134, "endOffset": 140}, {"referenceID": 5, "context": "One of those advantages is its ability to deal effectively with the vanishing gradients problem encountered in training deep networks [2, 8].", "startOffset": 134, "endOffset": 140}, {"referenceID": 8, "context": "How can the permutation symmetry of hidden units be broken in recurrent neural networks? In vanilla recurrent networks, the analog of adding identity skip connections between adjacent layers would be to change the equation describing the recurrent dynamics to (ignoring the biases for simplicity of notation): rt = rt\u22121 + f(Wrrt\u22121 + Wxxt) (27) This model, and variations thereof, were proposed in [12] with performance comparisons suggesting that they perform competitively with state-of-the-art feedforward models in standard image recognition benchmarks.", "startOffset": 397, "endOffset": 401}], "year": 2017, "abstractText": "Skip connections made the training of very deep neural networks possible and have become an indispendable component in a variety of neural architectures. A satisfactory explanation for their success remains elusive. Here, we present an explanation for the benefits of skip connections in training very deep neural networks. We argue that skip connections help break symmetries inherent in the loss landscapes of deep networks, leading to drastically simplified landscapes. In particular, skip connections between adjacent layers in a multilayer network break the permutation symmetry of nodes in a given layer, and the recently proposed DenseNet architecture, where each layer projects skip connections to every layer above it, also breaks the rescaling symmetry of connectivity matrices between different layers. This hypothesis is supported by evidence from a toy model with binary weights and from experiments with fully-connected networks suggesting (i) that skip connections do not necessarily improve training unless they help break symmetries and (ii) that alternative ways of breaking the symmetries also lead to significant performance improvements in training deep networks, hence there is nothing special about skip connections in this respect. We find, however, that skip connections confer additional benefits over and above symmetry-breaking, such as the ability to deal effectively with the vanishing gradients problem.", "creator": "LaTeX with hyperref package"}}}