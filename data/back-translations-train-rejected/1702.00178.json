{"id": "1702.00178", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2017", "title": "On the Futility of Learning Complex Frame-Level Language Models for Chord Recognition", "abstract": "Chord recognition systems use temporal models to post-process frame-wise chord preditions from acoustic models. Traditionally, first-order models such as Hidden Markov Models were used for this task, with recent works suggesting to apply Recurrent Neural Networks instead. Due to their ability to learn longer-term dependencies, these models are supposed to learn and to apply musical knowledge, instead of just smoothing the output of the acoustic model. In this paper, we argue that learning complex temporal models at the level of audio frames is futile on principle, and that non-Markovian models do not perform better than their first-order counterparts. We support our argument through three experiments on the McGill Billboard dataset. The first two show 1) that when learning complex temporal models at the frame level, improvements in chord sequence modelling are marginal; and 2) that these improvements do not translate when applied within a full chord recognition system. The third, still rather preliminary experiment gives first indications that the use of complex sequential models for chord prediction at higher temporal levels might be more promising.", "histories": [["v1", "Wed, 1 Feb 2017 09:44:44 GMT  (46kb,D)", "http://arxiv.org/abs/1702.00178v1", null], ["v2", "Fri, 31 Mar 2017 11:24:42 GMT  (69kb,D)", "http://arxiv.org/abs/1702.00178v2", "Published at AES Conference on Semantic Audio 2017"]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["filip korzeniowski", "gerhard widmer"], "accepted": false, "id": "1702.00178"}, "pdf": {"name": "1702.00178.pdf", "metadata": {"source": "CRF", "title": "On the Futility of Learning Complex Frame-Level Language Models for Chord Recognition", "authors": ["Filip Korzeniowski", "Gerhard Widmer"], "emails": ["filip.korzeniowski@jku.at"], "sections": [{"heading": null, "text": "Chord recognition systems use temporal models to frame chord predictions from acoustic models. Traditionally, first-order models such as Hidden Markov Models have been used for this task, with recent work suggesting the use of recurrent neural networks instead. Due to their ability to learn longer-term dependencies, these models are designed to learn and apply musical knowledge rather than simply smoothing out the results of the acoustic model. In this paper, we argue that learning complex temporal models at the level of audio frames is basically meaningless, and that non-Markovian models do no better than their first-order counterparts. We support our argument through three experiments with the McGill Billboard dataset. The first two show that when learning complex temporal models at the frame level, improvements in chord sequence modeling are marginal; and that these improvements cannot be translated if applied within a full chord recognition system."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to go to another world, to go to another world, to go to another world."}, {"heading": "2 Experiment 1: Chord Se-", "text": "quence ModelingIn this experiment, we want to directly quantify the modeling force of temporal models. A temporal model predicts the next chord symbol in a sequence that has already been observed. Since we are dealing with frame data and assume a frame rate of 10 fps, a chord sequence consists of 10 chord symbols per second. In more formal terms: For a chord sequence y = (y1,.., yK), a model M gives a probability distribution PM (yk | y1,..., yk \u2212 1) for each Yk. From this, we can calculate the probability of the chord sequence PM (y) = PM (y1) \u00b7 \u0445Kk = 2PM (y1,.. yk \u2212 1). (1) To measure how well a model M predicts the chord sequence in a dataset, we calculate the average log probability that it will be associated with the sequence of a dataset: Y-Y, Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y-Y (N)."}, {"heading": "2.1 Temporal Models", "text": "We compare two temporal models in this experiment: a first order Markov chain and an RNN with units of the LongShort Term Memory (LSTM). For the Markov chain, due to the Markov property yk \u2212 1 in Eq. 1, it is simplified to PM (yk | yk \u2212 1) = Ayk, yk \u2212 1 and PM (y1) = \u03c0y1. Entering the network in time step k is the chord symbol yk \u2212 1 in the train set.For the LSTM RNN, we follow exactly the design, parameterization and training procedure proposed in [10] and refer the reader to their paper for details. Entering the network step k is the chord symbol yk \u2212 1 in a hot coding, the output is the probability distribution PM (yk | y1)."}, {"heading": "2.2 Data", "text": "We evaluate the models on the McGill billboard record [12]. We filter out duplicate songs to avoid overlap between train and test, and use songs with IDs less than 1000 for training and the rest for testing. Filtering duplicate songs reduces the number of tracks from 890 to 742, of which 571 are used for training and validation, and 171 for testing."}, {"heading": "2.3 Results", "text": "In addition to L (M, Y), we give Ls (M, Y) and Lc (M, Y). These numbers represent the average log probability that the model assigns chord symbols in the dataset if the current symbol is the same as the previous one and if it has changed. They are calculated in a similar way to L (M, Y), but the product in Equation 1 only captures k, where yk = yk \u2212 1 or yk 6 = yk \u2212 1, respectable. They allow us to think about how well a model will flatten the predictions when the chord is stable, and how well it can predict chords when they change (this is where \"musical knowledge\" might come into play). We can see that the RNN symbol performs only slightly better than the Markov chain, despite its higher modeling capacity. This improvement is rooted in better predictions when the chord changes (-5.MC-N44 chance for this symbol to change completely)."}, {"heading": "3 Experiment 2: Frame-Level", "text": "In this experiment, we will evaluate the temporal models in the context of a complete chord recognition frame, the task being to predict the correct chord symbol for each audio frame. We will limit ourselves to major and minor chords, resulting in 24 (12 keys \u00d7 {major, minor}) chord symbols and a special \"no-chord\" symbol or 25 classes. Next, [2] we will map all ground truth chords with a minor term as the first interval to minor chords and all others to major chord symbols. Our chord recognition pipeline includes spectrogram computation, an automatically learned feature extractor and chord predictor, and finally the temporal model. The first two stages are based on our previous work [7, 13]. We will extract a logged and logged spectrograph between 65 and 100 2z per acoustic spectra per 1.5-second of one acoustic frame in three."}, {"heading": "3.1 Temporal Models", "text": "We test three temporal models of increasing complexity. The simplest is Majority Voting within a context of 1.3 s, the others are exactly the same as those we used in the previous experiment. If we combine the temporal model of the Markov chain with the predictions of the acoustic model, we obtain a Hidden Markov Model (HMM). To link the temporal model of the RNN with the predictions of the acoustic model, we apply the search algorithm introduced in [10] for hooked bars with a bar width of 25, a hash length of 3 symbols and a maximum of 4 solutions per hash-bin. The algorithm only approximates the chord sequence (there are no efficient and exact algorithms, since the output of the network depends on all previous inputs)."}, {"heading": "3.2 Results", "text": "Table 2 shows the Weighted Chord Symbol Recall (WCSR) of major and minor chords for all combinations of acoustic and temporal models. WCSR is defined as R = tc / ta, where tc is the total time in which the prediction matches the note, and ta is the total duration of the notes of the respective chord classes (major and minor chords, and in our case the \"chord-free\" class. We used the implementation provided in the \"Mir Eternity\" library [14]. The results show that the complex RNN time model does not exceed the simpler first-order chord models. They improve compared to the non-use of a time model at all and to a simple majority decision. The results suggest that the RNN time model does not have its (marginal) advantage in chord sequence modeling when it assumes that within the full chord system there are no obvious reasons for the improvement."}, {"heading": "4 Experiment 3: Modelling", "text": "In the final experiment, we want to support our argument that the RNN does not learn musical structure because of the hierarchical level (timeframe) on which it is applied. To this end, we conduct an experiment similar to the first - an RNN is required to predict the next chord symbol in the sequence. However, this time, the sequence is not sampled at frame level, but at chord level (i.e. regardless of how long a particular chord is played, it is reduced to a single instance in the sequence).The results confirm that in such a scenario, the RNN significantly exceeds the Markov chain (average Log-P of -1.62 vs. -2.28).Furthermore, we observe that the RNN not only learns static dependencies between successive chords; it is also able to adapt to a song and detect chord sequences that are detected in that song without online training."}, {"heading": "5 Conclusion", "text": "We argue that complex temporary models for chord marking 3 are futile on the basis of a timeframe; the first experiment focused on how well a complex temporary model can prepare chord marking compared to a simple first step; the second experiment showed that there was no improvement in chord marking within a chord marking system."}, {"heading": "Acknowledgements", "text": "This work is supported by the European Research Council (ERC) under the EU's Horizon 2020 Framework Programme (ERC Grant Agreement number 670035, \"Con Espressione\" project), and the Tesla K40 used for this research was donated by NVIDIA Corporation."}], "references": [{"title": "Automatic Chord Estimation from Audio: A Review of the State of the Art", "author": ["M. McVicar", "R. Santos-Rodriguez", "Y. Ni", "T.D. Bie"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(2), pp. 556\u2013575, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "On the Relative Importance of Individual Components of Chord Recognition Systems", "author": ["T. Cho", "J.P. Bello"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(2), pp. 477\u2013492, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "An End-to-End Machine Learning System for Harmonic Analysis of Music", "author": ["Y. Ni", "M. McVicar", "R. Santos-Rodriguez", "T. De Bie"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 20(6), pp. 1771\u20131783, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Combining Musicological Knowledge About Chords and Keys in a Simultaneous Chord and Local Key Estimation System", "author": ["J. Pauwels", "Martens", "J.-P."], "venue": "Journal of New Music Research, 43(3), pp. 318\u2013330, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Simultaneous Estimation of Chords and Musical Context From Audio", "author": ["M. Mauch", "S. Dixon"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 18(6), pp. 1280\u20131289, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Exploring Common Variations in State of the Art Chord Recognition Systems", "author": ["T. Cho", "R.J. Weiss", "J.P. Bello"], "venue": "Proceedings of the Sound and Music Computing Conference (SMC), Barcelona, Spain, 2010. 6", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A Fully Convolutional Deep Auditory Model for Musical Chord Recognition", "author": ["F. Korzeniowski", "G. Widmer"], "venue": "Proceedings of the IEEE International Workshop on Machine Learning for Signal Processing (MLSP), Salerno, Italy, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Chord Recognition Using Duration- Explicit Hidden Markov Models", "author": ["R. Chen", "W. Shen", "A. Srinivasamurthy", "P. Chordia"], "venue": "Proceedings of the 13th International Society for Music Information Retrieval Conference (ISMIR), Porto, Portugal, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Audio Chord Recognition with Recurrent Neural Networks", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "Proceedings of the 14th International Society for Music Information Retrieval Conference (ISMIR), Curitiba, Brazil, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Audio Chord Recognition with a Hybrid Recurrent Neural Network", "author": ["S. Sigtia", "N. Boulanger-Lewandowski", "S. Dixon"], "venue": "16th International Society for Music Information Retrieval Conference (ISMIR), M\u00e1laga, Spain, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "An Endto-End Neural Network for Polyphonic Piano Music Transcription", "author": ["S. Sigtia", "E. Benetos", "S. Dixon"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 24(5), pp. 927\u2013939, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "An Expert Ground Truth Set for Audio Chord Recognition and Music Analysis.", "author": ["J.A. Burgoyne", "J. Wild", "I. Fujinaga"], "venue": "Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR), Miami, USA,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Feature Learning for Chord Recognition: The Deep Chroma Extractor", "author": ["F. Korzeniowski", "G. Widmer"], "venue": "Proceedings of the 17th International Society for Music Information Retrieval Conference (ISMIR), New York, USA, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Mir eval: A Transparent Implementation of Common MIR Metrics,", "author": ["C. Raffel", "B. McFee", "E.J. Humphrey", "J. Salamon", "O. Nieto", "D. Liang", "D.P.W. Ellis"], "venue": "Proceedings of the 15th International Conference on Music Information Retrieval (ISMIR), Taipei, Taiwan,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Recurrent Neural Network Based Language Model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "Proceedings of INTERSPEECH 2010, Makuhari, Japan, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Sequence Transduction with Recurrent Neural Networks", "author": ["A. Graves"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML), Edinburgh, Scotland, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Segmental Recurrent Neural Networks for End-to-End Speech Recognition", "author": ["L. Lu", "L. Kong", "C. Dyer", "N.A. Smith", "S. Renals"], "venue": "Proceedings of INTERSPEECH 2016, San Francisco, USA, 2016. 7", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "We refer the reader to [1, 2] for an overview.", "startOffset": 23, "endOffset": 29}, {"referenceID": 1, "context": "We refer the reader to [1, 2] for an overview.", "startOffset": 23, "endOffset": 29}, {"referenceID": 2, "context": "[3, 4, 5]) implemented hand-designed temporal models for chord recognition.", "startOffset": 0, "endOffset": 9}, {"referenceID": 3, "context": "[3, 4, 5]) implemented hand-designed temporal models for chord recognition.", "startOffset": 0, "endOffset": 9}, {"referenceID": 4, "context": "[3, 4, 5]) implemented hand-designed temporal models for chord recognition.", "startOffset": 0, "endOffset": 9}, {"referenceID": 5, "context": "Here, it common to use simple Hidden Markov Models [6] or Conditional Random Fields [7] with states corresponding to chord labels.", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "Here, it common to use simple Hidden Markov Models [6] or Conditional Random Fields [7] with states corresponding to chord labels.", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "However, recent research showed that first-order models are unable to encode musical knowledge and focus on ensuring stability between consecutive predictions [8, 2] (i.", "startOffset": 159, "endOffset": 165}, {"referenceID": 1, "context": "However, recent research showed that first-order models are unable to encode musical knowledge and focus on ensuring stability between consecutive predictions [8, 2] (i.", "startOffset": 159, "endOffset": 165}, {"referenceID": 8, "context": "To overcome this, a number of recent papers suggested to use Recurrent Neural Networks (RNNs) as temporal models for a number of music-related tasks, such as chord recognition [9, 10] or multi-f0 tracking [11].", "startOffset": 176, "endOffset": 183}, {"referenceID": 9, "context": "To overcome this, a number of recent papers suggested to use Recurrent Neural Networks (RNNs) as temporal models for a number of music-related tasks, such as chord recognition [9, 10] or multi-f0 tracking [11].", "startOffset": 176, "endOffset": 183}, {"referenceID": 10, "context": "To overcome this, a number of recent papers suggested to use Recurrent Neural Networks (RNNs) as temporal models for a number of music-related tasks, such as chord recognition [9, 10] or multi-f0 tracking [11].", "startOffset": 205, "endOffset": 209}, {"referenceID": 9, "context": "For the LSTM-RNN, we follow closely the design, parametrisation and training procedure proposed in [10], and we refer the reader to their paper for details.", "startOffset": 99, "endOffset": 103}, {"referenceID": 9, "context": "As in [10], we show the network sequences of 100 symbols (corresponding to 10 seconds) during training.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "We evaluate the models on the McGill Billboard dataset [12].", "startOffset": 55, "endOffset": 59}, {"referenceID": 1, "context": "Following [2], we map all ground truth chord labels with a minor 3rd as their first interval to minor chords, and all", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "The first two stages are based on our previous work [7, 13].", "startOffset": 52, "endOffset": 59}, {"referenceID": 12, "context": "The first two stages are based on our previous work [7, 13].", "startOffset": 52, "endOffset": 59}, {"referenceID": 6, "context": "5s into one of three acoustic models: a logistic regression classifier (LogReg), a deep neural network (DNN) with 3 fully connected hidden layers of 256 rectifier units, and a convolutional neural network (ConvNet) with the exact architecture we presented in [7].", "startOffset": 259, "endOffset": 262}, {"referenceID": 9, "context": "To connect the RNN temporal model to the predictions of the acoustic model, we apply the hashed beam search algorithm, as introduced in [10], with a beam width of 25, hash length of 3 symbols and a maximum of 4 solutions per hash bin.", "startOffset": 136, "endOffset": 140}, {"referenceID": 13, "context": "We used the implementation provided in the \u201cmir eval\u201d library [14].", "startOffset": 62, "endOffset": 66}, {"referenceID": 14, "context": "We know from other domains such as natural language modelling that RNNs are capable of learning state-of-the-art language models [15].", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "Such language models can then be used in sequence classification framework such as sequence transduction [16] or segmental recurrent neural networks [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": "Such language models can then be used in sequence classification framework such as sequence transduction [16] or segmental recurrent neural networks [17].", "startOffset": 149, "endOffset": 153}], "year": 2017, "abstractText": "Chord recognition systems use temporal models to post-process frame-wise chord preditions from acoustic models. Traditionally, first-order models such as Hidden Markov Models were used for this task, with recent works suggesting to apply Recurrent Neural Networks instead. Due to their ability to learn longer-term dependencies, these models are supposed to learn and to apply musical knowledge, instead of just smoothing the output of the acoustic model. In this paper, we argue that learning complex temporal models at the level of audio frames is futile on principle, and that non-Markovian models do not perform better than their first-order counterparts. We support our argument through three experiments on the McGill Billboard dataset. The first two show 1) that when learning complex temporal models at the frame level, improvements in chord sequence modelling are marginal; and 2) that these improvements do not translate when applied within a full chord recognition system. The third, still rather preliminary experiment gives first indications that the use of complex sequential models for chord prediction at higher temporal levels might be more promising.", "creator": "LaTeX with hyperref package"}}}