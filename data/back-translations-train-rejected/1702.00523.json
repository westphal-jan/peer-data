{"id": "1702.00523", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2017", "title": "Deep Learning the Indus Script", "abstract": "Standardized corpora of undeciphered scripts, a necessary starting point for computational epigraphy, requires laborious human effort for their preparation from raw archaeological records. Automating this process through machine learning algorithms can be of significant aid to epigraphical research. Here, we take the first steps in this direction and present a deep learning pipeline that takes as input images of the undeciphered Indus script, as found in archaeological artifacts, and returns as output a string of graphemes, suitable for inclusion in a standard corpus. The image is first decomposed into regions using Selective Search and these regions are classified as containing textual and/or graphical information using a convolutional neural network. Regions classified as potentially containing text are hierarchically merged and trimmed to remove non-textual information. The remaining textual part of the image is segmented using standard image processing techniques to isolate individual graphemes. This set is finally passed to a second convolutional neural network to classify the graphemes, based on a standard corpus. The classifier can identify the presence or absence of the most frequent Indus grapheme, the \"jar\" sign, with an accuracy of 92%. Our results demonstrate the great potential of deep learning approaches in computational epigraphy and, more generally, in the digital humanities.", "histories": [["v1", "Thu, 2 Feb 2017 01:56:22 GMT  (9254kb,D)", "http://arxiv.org/abs/1702.00523v1", "17 pages, 10 figures, 7 supporting figures (2 pages)"]], "COMMENTS": "17 pages, 10 figures, 7 supporting figures (2 pages)", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["satish palaniappan", "ronojoy adhikari"], "accepted": false, "id": "1702.00523"}, "pdf": {"name": "1702.00523.pdf", "metadata": {"source": "CRF", "title": "Deep Learning the Indus Script", "authors": ["Satish Palaniappan", "Ronojoy Adhikari"], "emails": ["tpsatish95@gmail.com,", "satish12095@cse.ssn.edu.in"], "sections": [{"heading": null, "text": "Standardized corpora of undeciphered fonts, a necessary starting point for computer-aided epigraphics, require laborious human efforts to create them from raw archaeological data. Automating this process through machine learning algorithms can be of significant help to epigraphic research. Here, we are taking the first steps in this direction and presenting a deep learning pipeline that takes images of the undeciphered Indus script found in archaeological artifacts as input and returns as output a set of graphs suitable for inclusion in a standard corpus. Initially, the image is dissected by selective search into regions, and these regions are classified as textual and / or graphic information using a revolutionary neural network. Regions that contain potentially text are hierarchically merged and trimmed to remove non-textual information, and the remaining text portion of the image is segmented using standardized image processing techniques."}, {"heading": "Introduction", "text": "These must emerge from their original context in the archaeological record, most of them based on the corpus, with references to the original context, which is rarely necessary. However, the task of compiling a corpus requires a high degree of care and expertise, and this also applies to most other aspects of epigraphic research."}, {"heading": "Results and Discussion", "text": "Our main result is Figure 1, where the \"flow\" of the image of an Indus seal is shown over the course of the pipeline. A typical seal shown above left in Figure 1 contains a text region (usually at the top of the seal) and non-textual iconographic elements (usually below the text).This image is entered into the deep learning pipeline and the final output shown above right is the sequence of graphics contained in the textual area of the image, along with a classification of the individual graphics. Currently, we classify graphics as part of the \"glass\" class or its complement. A full classifier will be implemented in the future. We will now describe the various components of the pipeline, with specific details of the components shown below Figure 1."}, {"heading": "Region Proposal", "text": "In the first phase of the pipeline, interesting regions are proposed that are highly likely to contain a symbol, animal, deity or any iconographic element engraved on the Indus Seals. In this phase, there are three sub-stages, namely Extract Seal, Selective Search and Region Grouping for the gradual extraction of the interesting regions. Figure 2 illustrates this step of the pipeline.PLOS 2 / 17"}, {"heading": "Extract Seal", "text": "This step takes the image as input and extracts the seal portion only by removing the irrelevant background information from the image using conventional image processing techniques. First, the input image is scaled and smoothed grayish using a multidimensional Gaussian filter with a core of standard deviation 3.0, since the Indus seals are characterized by heavy wear that leads to noise images. Then, each pixel is switched to the middle pixel value of the seal, which is followed by a further smoothing with Gaussian blur of 7x7 grain size. This repeated smoothing ensures that only the most prominent edges that make up the entire seal are detected when we perform an optimized insidious edge detection [12] to suggest contours for calculating the rectangular frame around the seal (see Materials and Methods)."}, {"heading": "Selective Search", "text": "These cropped seal images are taken as input for the next step, which proposes all possible regions of interest likely to have the Indus script symbols or representations of animals such as bull, unicorn, deities and other iconographies. To accomplish this task, we use the selective search algorithm [13] (see Materials and Methods). However, the vanilla version of this algorithm had to be refined by grid search using the four free parameters to better fit our use case of extracting text only regions."}, {"heading": "Region Grouping", "text": "In order to reduce the number of proposals and increase the quality of the proposed regions, a four-step hierarchy of the methodology of grouping the regions has been developed in order to improvise the results of the selective search; at the first level of concentric proposals, all these proposals, which were merely approximations and generalizations of each other, have been replaced by a single enclosing area corresponding to the average of all concentric proposals; secondly, at the level of the contained boxes, the proposals that were 100% contained in another proposal of the region have been removed, taking into account only the regional proposal, which includes whole symbols or objects, rather than each part of them; and all proposals that overlapped with more than 40% of the respective other range have been replaced by the drawing of the superbox, so that a supposedly unified region has been proposed as two different but partially overlapping regions, with a single minimal superbox that delimited both proposals."}, {"heading": "Text Region Extraction", "text": "In this phase of the pipeline, the candidate region is used as input for interest proposals from the previous phase, and precise regions are produced that contain only text information by removing the non-symbolic parts from the regional proposals. To achieve this, we have two successive sub-stages, the region classification and the formulation of text regions. Figure 3 illustrates an example flow of this phase of the pipeline."}, {"heading": "Region Classification", "text": "This sub-stage of the pipeline receives proposals from the Proposal Level region as input and classifies them into three types, namely \"text,\" \"no-text\" and \"both\" regions. To achieve this, we have developed a machine-learning model, the text / no-text classification, which can distinguish a text part from a non-text part. The two regions consist of a script and non-script parts within themselves. Ultimately, this is an image classification problem with three classes, namely \"text\" and \"both.\" To distinguish clearly between these classes, we need to learn deeply in the areas where the label is attached."}, {"heading": "Text Region Formulation", "text": "To achieve this, we have developed a two-tier hierarchy to truncate the non-text regions and group all text regions into a single strip. First, the TextBox Layer Draw brings together those pairs of regional proposals where two \"text regions\" or a \"text region\" and a \"both\" region overlap in height and width along the same horizontal or vertical axis within the PLOS 5 / 17 axis, whether they overlap or not, to bring whole text regions into a single proposal called \"text boxes.\" The dynamic threshold over the merge process is calculated by calculating the arithmetic mean from the width and height of the two candidate regions, and either 25% or 20% of the text regions or 20% of the text regions overlap. Second, the TextBox layer is responsible for truncating the text regions if those \"text regions\" are \"overlapping,\" and \"non-overlapping.\""}, {"heading": "Symbol Segmentation", "text": "This is the penultimate stage of the pipeline, which contains the precise suggestions of each region as input and snippet from each graph. To achieve this, it is necessary to design the individual areas of the image in their entirety as they imagined it to be. Ultimately, this phase reduces the problem of optical character recognition to a much simpler problem by filtering out the individual symbols."}, {"heading": "Empirical Analysis", "text": "To evaluate the performance of our Script OCR deep learning pipeline, we isolated a set of 50 images from the original Indus Seals Dataset (see Materials and Methods) collected by PLOS 7 / 17Roja Muthiah Research Library (RMRL), which was not used to train or optimize any stage of the pipeline. The various stages in the pipeline were evaluated using this set of 50 images. Firstly, these images were fed into the Region Proposal Phase of the pipeline. Seal extract uses basic image processing techniques and performs pretty much exactly in all cases."}, {"heading": "Discussion", "text": "There are seven cases challenging the performance of this deep pipeline, the effectiveness of the system in all these cases has not been discussed."}, {"heading": "Materials and Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Datasets", "text": "Building a deep-rooted pipeline for the effective execution of OCR scripts requires an image corpus of Indus artifacts. The basis of the images, either scans or photographs of Indus artifacts used to formulate the other data sets, was very limited in size and variety. They were taken mainly from the library (RMRL), along with their corresponding text folders to Mahadevan, by scraping the web with the API [23], especially from Harappa.com [24], represents the Indus Seals Dataset Dataset."}, {"heading": "Canny Edge Detection", "text": "The canny edge detection [12] is a multi-level algorithm with two significant features, namely Non-Maximum Suppression and the hysteresis process, in which edge candidates that are not dominant in their neighborhood are not considered edges and where a candidate is in the neighborhood of an edge, the threshold is lowered as he moves along the candidates. These characteristics best adapt to the respective application of the Indus artifacts. To further optimize this for the purpose of seal extraction from given images, the lower and upper thresholds of the algorithm are adjusted according to the present image by calculating the median of the pixel intensity of a channel."}, {"heading": "Selective Search Algorithm", "text": "It combines the advantages of an exhaustive search and segmentation. As with segmentation, the image structure is used to guide the sample, and as with an exhaustive search, all possible object locations are invariably captured in size and scale, making it an optimal choice for our case. Under the hood, it performs a hierarchical grouping of area proposals based on color, texture, size and fillings to suggest the best candidate regions. It was also used as a mechanism for the area proposal with CNN [25], proving its compatibility with deep architectures. Furthermore, the four free parameters of the algorithm were fine-tuned by manual grid search and set to: \"Scale\" - 350, 450, 500 (higher the value than the clusters in the field waltz segment minimum segment b - 26 \"for the minimum segment b,\" Sigma \")."}, {"heading": "Convolutional Neural Networks", "text": "The Convolution Neural Networks (CNN) [9] are powerful deep learning algorithms that work explicitly on images. They are inspired by the actual functioning of the animal visual cortex eyePLOS 11 / 17 and by how the neurons process images and react to intersecting regions tiling the visual field. Unlike other artificial neural networks that have feed-forward, the CNNs have different layer types, the folding layer, pooling layer, LRN [27], ReLU [27], dropout layer [28], fully networked layer and SoftMax layer, to name a few. The plan that describes how these layers are stacked together with the hyperparameter configurations is referred to as the CNN architecture. Both CNNs described in our work here are built, trained, learned and fine-tuned using the \"Caffe Deep Learning Modules\" from Google + Modules Frameworks, which was first developed in Nanging GB."}, {"heading": "Region Classification", "text": "The sub-stage of the text region extraction stage in the pipeline employs the two best levels of the GoogLeNet [30] CNN architecture for classifying the input regions into \"text,\" \"no-text\" and \"both.\" We also examined other famous CNN architectures that provide state-of-the-art technology in the ImageNet [27], and VGGNet [32], which was designed for image classification problems, but GoogLeNet was the deepest, most computationally efficient and lightweight architecture, roughly designed for the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). Specifically, it is about using the network-in-network architecture to enhance the representational power of a neural network. This 22-layer deep network has 9 distinctive inception networks that outperform each other."}, {"heading": "Conclusion", "text": "In conclusion, the proposed deep learning processes are capable of transforming a complex visual characteristic of the availability of information into a relatively simple educational problem and detecting the presence and absence of the \"Jar\" mark. However, the results are not precise if the sealing is too complex to detect. All the discussions show that the text / no-text classifier in the region is very mature and works well, contributing to a stable pipeline, even if it is overlooked by this module."}, {"heading": "S1 Fig. A snapshot of the \u201cIndus Seals Dataset\u201d (RMRL)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "S2 Fig. A snapshot of the \u201cIndus Seals Dataset\u201d (Crawled)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "S3 Fig. A snapshot of the \u201cText-NoText Dataset\u201d", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "S4 Fig. A snapshot of the \u201cJar-NoJar Dataset\u201d", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "S5 Fig. Selective Search applied to an Indus seal image", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "S6 Fig. The GoogLeNet CNN Architecture [30]", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "S7 Fig. The Inception Module in GoogLeNet [30]", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Acknowledgments", "text": "The research of the second author is partially supported by a Google Faculty Research Award on \"Machine Learning of Syntax in Undecrypted Scripts.\" We thank Suresh Babu of the Indus Research Centre and G. Sundar of the Roja Muthiah Research Library for their generous assistance in providing images of the Indus Seals. We thank Harappa.com [24] for their kind permission to use an image of the Indus Seal in this paper."}], "references": [{"title": "Proto-Indica: Brief Report on the Investigation of the Proto-Indian Texts", "author": ["Y Knorozov", "B Volchok", "N. Gurov"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1968}, {"title": "A method to classify characters of unknown ancient scripts", "author": ["S Koskenniemi", "A Parpola", "S. Parpola"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1970}, {"title": "Syntactic methods in the study of the Indus script", "author": ["K. Koskenniemi"], "venue": "Studia Orientalia Electronica", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1981}, {"title": "Entropic evidence for linguistic structure in the Indus script", "author": ["RP Rao", "N Yadav", "MN Vahia", "H Joglekar", "R Adhikari", "I. Mahadevan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "A Markov model of the Indus script", "author": ["RP Rao", "N Yadav", "MN Vahia", "H Joglekar", "R Adhikari", "I. Mahadevan"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Ancient Cities of the Indus Valley Civilization. American Institute of Pakistan studies", "author": ["Kenoyer JM"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y LeCun", "L Bottou", "Y Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE. 1998;86(11):2278\u20132324", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "The Indus script: texts, concordance, and tables. Memoirs of the Archaeological Survey of India", "author": ["I. Mahadevan"], "venue": "Archaeological Survey of India;", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1977}, {"title": "OCR on Indus Seals; 2016", "author": ["S Palaniappan", "R. Adhikari"], "venue": "Available from: https://github. com/tpsatish95/OCR-on-Indus-Seals", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "A computational approach to edge detection", "author": ["J. Canny"], "venue": "IEEE Transactions on pattern analysis and machine intelligence", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1986}, {"title": "Selective search for object recognition", "author": ["JR Uijlings", "KE van de Sande", "T Gevers", "AW. Smeulders"], "venue": "International journal of computer vision", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O Russakovsky", "J Deng", "H Su", "J Krause", "S Satheesh", "S Ma"], "venue": "International Journal of Computer Vision", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "A threshold selection method from gray-level histograms. Automatica", "author": ["N. Otsu"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1975}, {"title": "Minimum cross entropy thresholding", "author": ["CH Li", "C. Lee"], "venue": "Pattern Recognition", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1993}, {"title": "A new criterion for automatic multilevel thresholding", "author": ["JC Yen", "FJ Chang", "S. Chang"], "venue": "IEEE Transactions on Image Processing", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Picture thresholding using an iterative selection method", "author": ["T Ridler", "S. Calvard"], "venue": "IEEE trans syst Man Cybern", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1978}, {"title": "Statistical analysis of the Indus script using n-grams", "author": ["N Yadav", "H Joglekar", "RP Rao", "MN Vahia", "R Adhikari", "I. Mahadevan"], "venue": "PLoS One", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R Girshick", "J Donahue", "T Darrell", "J. Malik"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition;", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Efficient graph-based image segmentation", "author": ["Felzenszwalb PF", "Huttenlocher DP"], "venue": "International Journal of Computer Vision", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing", "author": ["A Krizhevsky", "I Sutskever", "GE. Hinton"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["GE Hinton", "N Srivastava", "A Krizhevsky", "I Sutskever", "RR. Salakhutdinov"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Y Jia", "E Shelhamer", "J Donahue", "S Karayev", "J Long", "R Girshick"], "venue": "arXiv preprint arXiv:14085093", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C Szegedy", "W Liu", "Y Jia", "P Sermanet", "S Reed", "D Anguelov"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition;", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K Simonyan", "A. Zisserman"], "venue": "arXiv preprint", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "How transferable are features in deep neural networks? In: Advances in neural information processing", "author": ["J Yosinski", "J Clune", "Y Bengio", "H. Lipson"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Automation via computer programs has been used in the past [1] [2] [3] [4] [5] to reduce human effort in epigraphical research.", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "Automation via computer programs has been used in the past [1] [2] [3] [4] [5] to reduce human effort in epigraphical research.", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "Automation via computer programs has been used in the past [1] [2] [3] [4] [5] to reduce human effort in epigraphical research.", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "Automation via computer programs has been used in the past [1] [2] [3] [4] [5] to reduce human effort in epigraphical research.", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "Automation via computer programs has been used in the past [1] [2] [3] [4] [5] to reduce human effort in epigraphical research.", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "However, without exception, these have been confined to classification [2] and search for graphemic patterns [3] [4] [5] in a prepared corpus.", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "However, without exception, these have been confined to classification [2] and search for graphemic patterns [3] [4] [5] in a prepared corpus.", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "However, without exception, these have been confined to classification [2] and search for graphemic patterns [3] [4] [5] in a prepared corpus.", "startOffset": 113, "endOffset": 116}, {"referenceID": 4, "context": "However, without exception, these have been confined to classification [2] and search for graphemic patterns [3] [4] [5] in a prepared corpus.", "startOffset": 117, "endOffset": 120}, {"referenceID": 5, "context": "We use images of archaeological records of the Indus script [8], mostly found inscribed on terracotta seals, to demonstrate the efficacy of the pipeline.", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "The first is a convolutional neural network [9] that identifies three types of regions in the image, those that contain a sequence of graphemes (henceforth abbreviated as \u201ctext\u201d), those that contain images or other non-graphemic components (abbreviated as \u201cno-text\u201d) and those that contain both graphemic and non-graphemic components (abbreviated as \u201cboth\u201d).", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "The output of this classifier is then received by a second convolutional neural network [9] which identifies graphemes from the textual region of the image and maps then to a standard set of graphemes.", "startOffset": 88, "endOffset": 91}, {"referenceID": 7, "context": "In the present work, this second stage has been implemented to recognize only the most frequent Indus grapheme, the \u201cjar sign\u201d (sign 342 in the Mahadevan corpus [10]).", "startOffset": 161, "endOffset": 165}, {"referenceID": 8, "context": "The code and other resources used in constructing this deep learning pipeline has been open sourced and is available on GitHub [11].", "startOffset": 127, "endOffset": 131}, {"referenceID": 9, "context": "This repeated smoothing ensures that only the most prominent edges constituting the entire seal will be detected when we perform an optimized canny edge [12] detection (see Materials and Methods) to propose contours for calculating the bounding rectangular box around the seal.", "startOffset": 153, "endOffset": 157}, {"referenceID": 10, "context": "We use the Selective Search [13] (see Materials and Methods) region proposal algorithm to achieve this task.", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "In order to reduce the numbers and increase the quality of the proposed regions, a four-level hierarchy of region grouping methodology was devised to improvise the Selective Search [13] results.", "startOffset": 181, "endOffset": 185}, {"referenceID": 11, "context": "We opted for applying transfer-learning and fine-tuning approaches over the GoogLeNet CNN, initialized with extremely feature-rich filters (weights) pre-trained over the ImageNet dataset [14], given the need for developing a highly discriminative classifier with a comparatively very small and unconventional base dataset consisting of just 2091 data samples.", "startOffset": 187, "endOffset": 191}, {"referenceID": 12, "context": "This grayscaled image is then binarized using Otsu\u2019s thresholding [15] technique in the second level; we settled on Otsu\u2019s thresholding technique for our purpose, after experimenting with various techniques like, Mean, Triangle, Li [16], Yen [17], IsoData [18] and Minimum thresholding, as this suited our purpose more accurately.", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "This grayscaled image is then binarized using Otsu\u2019s thresholding [15] technique in the second level; we settled on Otsu\u2019s thresholding technique for our purpose, after experimenting with various techniques like, Mean, Triangle, Li [16], Yen [17], IsoData [18] and Minimum thresholding, as this suited our purpose more accurately.", "startOffset": 232, "endOffset": 236}, {"referenceID": 14, "context": "This grayscaled image is then binarized using Otsu\u2019s thresholding [15] technique in the second level; we settled on Otsu\u2019s thresholding technique for our purpose, after experimenting with various techniques like, Mean, Triangle, Li [16], Yen [17], IsoData [18] and Minimum thresholding, as this suited our purpose more accurately.", "startOffset": 242, "endOffset": 246}, {"referenceID": 15, "context": "This grayscaled image is then binarized using Otsu\u2019s thresholding [15] technique in the second level; we settled on Otsu\u2019s thresholding technique for our purpose, after experimenting with various techniques like, Mean, Triangle, Li [16], Yen [17], IsoData [18] and Minimum thresholding, as this suited our purpose more accurately.", "startOffset": 256, "endOffset": 260}, {"referenceID": 7, "context": "This is the final stage of the pipeline, it takes the individually cropped images of graphemes identified from the previous stage as input and should ideally label (classify) those symbols into one of the 417 classes (the known number of Indus graphemes [10]) according to the Mahadevan corpus (M77).", "startOffset": 254, "endOffset": 258}, {"referenceID": 16, "context": "However, in our current work, we restrict ourselves to just identifying the presence or absence of the most frequent grapheme [19], the \u201cjar\u201d in the input images.", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "Case 5: Seal images with background noise; the pipeline developed is not tolerant to the seal being surrounded by some background, but this case is successfully handled by the Extract Seal sub-stage of the Region Proposal stage, which uses a modified Canny Edge detection [12] based algorithm to cut off the given seal image only from its background noise.", "startOffset": 272, "endOffset": 276}, {"referenceID": 7, "context": "They were mainly sourced from Roja Muthiah Research Library (RMRL) [22] along with their corresponding text mappings according to Mahadevan\u2019s corpus [10] and by scraping the web with the Google Image Search API [23], especially from the harappa.", "startOffset": 149, "endOffset": 153}, {"referenceID": 10, "context": "The candidates for this dataset were obtained by applying the vanilla version of Selective Search [13] algorithm over the images of the base dataset.", "startOffset": 98, "endOffset": 102}, {"referenceID": 9, "context": "The Canny edge detection [12] is a multi-stage algorithm with two significant features namely, Non-Maximum Suppression and the Hysteresis Process, wherein the edges\u2019 candidates which are not dominant in their neighborhood aren\u2019t considered to be edges and given a candidate is in the neighborhood of an edge the threshold is lowered, while moving along the candidates.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "It is a region proposal algorithm [13] used in the Selective Search level of the Region Proposal stage of the pipeline.", "startOffset": 34, "endOffset": 38}, {"referenceID": 17, "context": "It was also used as the Region Proposal mechanism for Regions with CNN [25], proving its compatibility with deep architectures.", "startOffset": 71, "endOffset": 75}, {"referenceID": 18, "context": "Further to suit our needs, the algorithm\u2019s four free parameters were fine-tuned by manual grid search, and finalized on: \u201cScale\u201d - 350, 450, 500 (higher the value larger the clusters in Felzenszwalb segmentation [26]), \u201cSigma\u201d 0.", "startOffset": 212, "endOffset": 216}, {"referenceID": 18, "context": "8 (width of Gaussian kernel for Felzenszwalb segmentation [26]), \u201cMinimum Size\u201d - 30, 60, 120 (Minimum component size for Felzenszwalb segmentation [26]), \u201cMinimum Area\u201d - 2000 (Minimum area of a region proposed).", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "8 (width of Gaussian kernel for Felzenszwalb segmentation [26]), \u201cMinimum Size\u201d - 30, 60, 120 (Minimum component size for Felzenszwalb segmentation [26]), \u201cMinimum Area\u201d - 2000 (Minimum area of a region proposed).", "startOffset": 148, "endOffset": 152}, {"referenceID": 6, "context": "The Convolution Neural Networks (CNN) [9] are powerful deep learning algorithms that operate explicitly on images.", "startOffset": 38, "endOffset": 41}, {"referenceID": 19, "context": "Unlike other feed-forward artificial neural networks, the CNNs have different types of layers, the convolution layer, pooling layer, LRN [27], ReLU [27], dropout layer [28], fully-connected layer and SoftMax layer, to name a few.", "startOffset": 137, "endOffset": 141}, {"referenceID": 19, "context": "Unlike other feed-forward artificial neural networks, the CNNs have different types of layers, the convolution layer, pooling layer, LRN [27], ReLU [27], dropout layer [28], fully-connected layer and SoftMax layer, to name a few.", "startOffset": 148, "endOffset": 152}, {"referenceID": 20, "context": "Unlike other feed-forward artificial neural networks, the CNNs have different types of layers, the convolution layer, pooling layer, LRN [27], ReLU [27], dropout layer [28], fully-connected layer and SoftMax layer, to name a few.", "startOffset": 168, "endOffset": 172}, {"referenceID": 21, "context": "Both the CNNs described in our work here were built, trained, transfer learned and fine-tuned using the \u201cCaffe\u201d deep learning framework developed by \u201cYangqing Jia\u201d at the Berkeley Vision and Learning Center [29].", "startOffset": 207, "endOffset": 211}, {"referenceID": 22, "context": "The Region Classification sub-stage of the Text Region Extraction stage in the pipeline employs the GoogLeNet [30] CNN architecture for classifying the input regions into \u201ctext\u201d, \u201cno-text\u201d and \u201cboth\u201d.", "startOffset": 110, "endOffset": 114}, {"referenceID": 6, "context": "We also surveyed other famous CNN architectures [31] such as LeNet [9], AlexNet [27], and VGGNet [32], designed for image classification problems, but GoogLeNet was the most deepest, computationally efficient and lightweight architecture that delivered state-of-the-art results in the ImageNet Large-Scale Visual Recognition Challenge 2014 [14] (ILSVRC14).", "startOffset": 67, "endOffset": 70}, {"referenceID": 19, "context": "We also surveyed other famous CNN architectures [31] such as LeNet [9], AlexNet [27], and VGGNet [32], designed for image classification problems, but GoogLeNet was the most deepest, computationally efficient and lightweight architecture that delivered state-of-the-art results in the ImageNet Large-Scale Visual Recognition Challenge 2014 [14] (ILSVRC14).", "startOffset": 80, "endOffset": 84}, {"referenceID": 23, "context": "We also surveyed other famous CNN architectures [31] such as LeNet [9], AlexNet [27], and VGGNet [32], designed for image classification problems, but GoogLeNet was the most deepest, computationally efficient and lightweight architecture that delivered state-of-the-art results in the ImageNet Large-Scale Visual Recognition Challenge 2014 [14] (ILSVRC14).", "startOffset": 97, "endOffset": 101}, {"referenceID": 11, "context": "We also surveyed other famous CNN architectures [31] such as LeNet [9], AlexNet [27], and VGGNet [32], designed for image classification problems, but GoogLeNet was the most deepest, computationally efficient and lightweight architecture that delivered state-of-the-art results in the ImageNet Large-Scale Visual Recognition Challenge 2014 [14] (ILSVRC14).", "startOffset": 340, "endOffset": 344}, {"referenceID": 24, "context": "Therefore, we opted for the practice of Transfer Learning [34], where the weights and rich filters learned by the CNN on some other larger and generic dataset are fine tuned and transfer learned to suit our primary dataset.", "startOffset": 58, "endOffset": 62}, {"referenceID": 22, "context": "Therefore, we tweaked the GoogLeNet [30] architecture\u2019s parameters to disable learning until the first 6 inception layers (3a, 3b, 4a, 4b, 4c, and 4d) by setting their learning rates to 0, thus preserving the rich lower level ImageNet features.", "startOffset": 36, "endOffset": 40}, {"referenceID": 20, "context": "This layer is followed by a Dropout [28] layer, to prevent from over-fitting the small dataset in hand.", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "The GoogLeNet CNN Architecture [30]", "startOffset": 31, "endOffset": 35}, {"referenceID": 22, "context": "The Inception Module in GoogLeNet [30]", "startOffset": 34, "endOffset": 38}], "year": 2017, "abstractText": "Standardized corpora of undeciphered scripts, a necessary starting point for computational epigraphy, requires laborious human effort for their preparation from raw archaeological records. Automating this process through machine learning algorithms can be of significant aid to epigraphical research. Here, we take the first steps in this direction and present a deep learning pipeline that takes as input images of the undeciphered Indus script, as found in archaeological artifacts, and returns as output a string of graphemes, suitable for inclusion in a standard corpus. The image is first decomposed into regions using Selective Search and these regions are classified as containing textual and/or graphical information using a convolutional neural network. Regions classified as potentially containing text are hierarchically merged and trimmed to remove non-textual information. The remaining textual part of the image is segmented using standard image processing techniques to isolate individual graphemes. This set is finally passed to a second convolutional neural network to classify the graphemes, based on a standard corpus. The classifier can identify the presence or absence of the most frequent Indus grapheme, the \u201cjar\u201d sign, with an accuracy of 92%. Our results demonstrate the great potential of deep learning approaches in computational epigraphy and, more generally, in the digital humanities.", "creator": "LaTeX with hyperref package"}}}