{"id": "1702.00764", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2017", "title": "Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey", "abstract": "Natural language and symbols are intimately correlated. Recent advances in machine learning (ML) and in natural language processing (NLP) seem to contradict the above intuition: symbols are fading away, erased by vectors or tensors called distributed and distributional representations. However, there is a strict link between distributed/distributional representations and symbols, being the first an approximation of the second. A clearer understanding of the strict link between distributed/distributional representations and symbols will certainly lead to radically new deep learning networks. In this paper we make a survey that aims to draw the link between symbolic representations and distributed/distributional representations. This is the right time to revitalize the area of interpreting how symbols are represented inside neural networks.", "histories": [["v1", "Thu, 2 Feb 2017 17:53:29 GMT  (129kb,D)", "http://arxiv.org/abs/1702.00764v1", "25 pages"]], "COMMENTS": "25 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lorenzo ferrone", "fabio massimo zanzotto"], "accepted": false, "id": "1702.00764"}, "pdf": {"name": "1702.00764.pdf", "metadata": {"source": "CRF", "title": "A Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a Survey", "authors": ["LORENZO FERRONE", "FABIO MASSIMO ZANZOTTO"], "emails": [], "sections": [{"heading": null, "text": "A Symbolic, Distributed and Distributional Representations for Natural Language Processing in the Era of Deep Learning: a SurveyLORENZO FERRONE, University of Rome Tor Vergata and FABIO MASSIMO ZANZOTTO, University of Rome Tor VergataNatural Language and Symbols are closely related. Recent advances in Machine Learning (ML) and Natural Language Processing (NLP) seem to contradict the above intuition: Symbols fade away, are deleted by vectors or tensors called distributed and distributed representations. However, there is a close link between distributed / distributed representations and symbols, the first being an approximation of the second. A clearer understanding of the strict connection between distributed / distributed representations and symbols will certainly lead to radically new, deep learning networks. In this paper, we lead to a survey aimed at establishing the link between symbolic representations and distributed representations within the distributed / distributed networks."}, {"heading": "1. INTRODUCTION", "text": "They all have a different conception of themselves than they feel able to learn a different language than they can speak a different language; they have a different conception of themselves than they speak a different language; they have a different conception of themselves than they speak a different language; they have a different conception of themselves than they speak a different language; they have a different conception of themselves than they speak a different language; they have a different conception of themselves that is not based on symbols, and they seem to have a different conception of themselves; they have a different conception of themselves that they understand a different language; they have a different conception of themselves and a different conception of themselves; they have a different conception of themselves that is not based on symbols; they seem to have a different conception of themselves."}, {"heading": "2. SYMBOLIC AND DISTRIBUTED REPRESENTATIONS: INTERPRETABILITY AND COMPOSABILITY", "text": "In fact, it is such that the greater number of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "3. STRATEGIES TO OBTAIN DISTRIBUTED REPRESENTATIONS FROM SYMBOLS", "text": "There is a wide range of techniques to transform symbolic representations into distributed representations."}, {"heading": "3.1. Dimensionality reduction", "text": "The question that arises is whether the question of whether the question of whether the question of whether the question is about the question, the question that it is about, whether it is at all about the question of whether the question of why is at all a question of truth, whether the question of why is at all a question of truth or whether the question of why is at all a question of perception, a question of perception, a question of perception and not a question of perception of perception, whether the question of why is a question of perception, whether it is only a question of perception, whether it is a question of perception of perception or a question of perception of perception, a question of perception of perception of perception and not a question of perception of perception, whether it is a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception, a question of perception."}, {"heading": "3.2. Learned representation", "text": "The learned representations differ from the dimensionality reduction techniques in the fact that: (1) encoding / decoding functions must not be linear; (2) learning can optimize functions that are different in terms of the goal of the PCA; and (3) solutions are not derived in a closed form, but are decentred using optimization techniques such as stochastic gradients. Learned representation can be further classified into: - task-dependent representations that are learned with a standalone algorithm (as in autocoders [Socher et al. 2011; Liou et al. 2014]) that are independent of each task and that learn a representation that only depends on the datasets used; - task-dependent representations that are learned as the first step of another algorithm (this is called end-to-end-to-end training), usually the first layer of a deep neural network; - task-dependent representations that are divided by a 2.autocoding technique."}, {"heading": "4. DISTRIBUTIONAL REPRESENTATIONS AS ANOTHER SIDE OF THE COIN", "text": "Distribution semantics is an important area of research in natural language processing aimed at describing the meaning of words and sentences with vectorial representations (see [Turney and Pantel 2010] for a survey), which are referred to as distributional representations. It is a strange historical coincidence that two similar-sounding terms were given - distributed and distributed - two terms that should not be confused for many. Perhaps, this is because the two terms are definitely related to each other. We argue that distribution-specific representations are nothing more than a subset of distributed representations, and in fact, they can be neatly categorized into the features presented in the previous sectoral semantics. Distribution semantics is based on a famous slogan - \"You will judge a word by the company that holds it.\" [Firth 1957] - and on the distribution hypothesis [Harris 1964] - words have a similar meaning when they are used in contexts similar to, or used with similar words."}, {"heading": "4.1. Building distributional representations for words from a corpus", "text": "In this sector, we will be able to be in a position to be in, to be in a position to be in."}, {"heading": "4.2. Compacting distributional representations", "text": "Since distributed representations, distribution-specific representations can go through the process of dimensionality reduction (see Section 3.1) with Principal Component Analysis and Random Indexing, this process is used for two problems: the first is the classic problem of reducing dimensions of representation in order to get more compact representations; the second is to help representation focus on more discriminatory dimensions; the latter problem focuses on feature selection and merging, which is an important task in making these representations more effective for the final task of similarity detectory.Principal Component Analysis (PCA) is mostly applied in the compression of distribution representations: Latent Semantic Analysis (LSA) is a prominent example of how these representations are more effectively tailored to the final task of similarity detection. LSA was born in Information Retrieval with the idea of reducing word-to-document matrices."}, {"heading": "4.3. Learning representations: word2vec", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "5. COMPOSING DISTRIBUTED REPRESENTATIONS", "text": "In the previous sections, we described how a symbol or bag of symbols can be transformed into distributed representations aimed at determining whether these distributed representations are capable of being interpreted. In this section, we want to examine a second and important aspect of these representations, that is, are these aspects composable as symbolic representations? And if these representations are still interpretable, the composition can be used for subsequent steps as a symbolic representation to describe sequences or structures by composing symbols with specific rules. In this process, symbols remain clear and rules compose clear."}, {"heading": "5.1. Compositional Distributional Semantics", "text": "In distribution semantics, there should be vectors similar to those discussed in the 2010 [Zanzotto] issue. In distribution semantics, we proceeded as follows: Prior to Distribution Semantics Models (CDSMs) [Baroni et al. 2014; Mitchell and Lapata 2010] and aim to apply the principle of compositionality [Frege 1884; Montague 1974] to calculate distribution-relevant semantic vectors for phrases. These models generally use structural or syntactical representations of phrases to derive their distributional meaning. Hence, CDSMs aim to provide a complete semantic model for distributional semantics. In the distribution semantics, the goal of CDSMs is to produce similar sentences to derive their distributional meanings. CDSMs aim to provide a complete semantic model for distributional semantics of words."}, {"heading": "5.2. Holographic Representations", "text": "In fact, most of them will be able to move to another world in which they are able, in which they are able to move, and in which they are able to move."}, {"heading": "5.3. Compositional Models in Neural Networks", "text": "When neural networks are applied to sequences or structured networks, these networks are in fact models that form a complex network. However, these models lead to models that are not interpretable. In fact, composition functions rely on specific tasks and not on the possibility of reconstructing structured input, unless in some rare cases [Socher et al. 2011]. The output of these networks are sequences or structured data in which basic symbols are embedded in local representations or distributed representations (see Sec. 4.3). The output are distributed vectors for specific tasks. Therefore, these models are not interpretable in our sense and for the fact that non-linear functions are adopted in the specification of neural networks. In this section, we review some prominent neural network architectures that can be interpreted as models."}, {"heading": "6. CONCLUSIONS", "text": "Recent advances in Machine Learning (ML) and Natural Language Processing (NLP) seem to contradict the above-mentioned intuition: Symbols fade, are deleted by vectors or tensors known as distributed and distributed representations. We have conducted this survey to demonstrate the unsurprising link between symbolic representations and distributed / distributed representations, and this is the right time to revive the area of interpreting the representation of symbols within neural networks. We believe that this survey will help to develop new deep neural networks that can exploit existing and novel symbolic models of classical tasks of natural language processing. We believe that a clearer understanding of the strict link between distributed / distributed representations and symbols will certainly lead to radically new deep learning networks."}], "references": [{"title": "Database-friendly random projections: Johnson-Lindenstrauss with binary coins", "author": ["Dimitris Achlioptas."], "venue": "Journal of computer and System Sciences 66, 4 (2003), 671\u2013687.", "citeRegEx": "Achlioptas.,? 2003", "shortCiteRegEx": "Achlioptas.", "year": 2003}, {"title": "SEM 2013 shared task: Semantic Textual Similarity", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo."], "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity. Association for Computational Linguistics, Atlanta, Georgia, USA, 32\u201343. http://www. aclweb.org/anthology/S13-1004", "citeRegEx": "Agirre et al\\.,? 2013", "shortCiteRegEx": "Agirre et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 (2014).", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Frege in space: A program of compositional distributional semantics", "author": ["Marco Baroni", "Raffaela Bernardi", "Roberto Zamparelli."], "venue": "LiLT (Linguistic Issues in Language Technology) 9 (2014).", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Marco Baroni", "Alessandro Lenci."], "venue": "Comput. Linguist. 36, 4 (Dec. 2010), 673\u2013721. DOI:http://dx.doi.org/10.1162/coli a 00016", "citeRegEx": "Baroni and Lenci.,? 2010", "shortCiteRegEx": "Baroni and Lenci.", "year": 2010}, {"title": "Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space", "author": ["Marco Baroni", "Roberto Zamparelli."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Cambridge, MA, 1183\u20131193. http://www.aclweb.org/anthology/D10-1115", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "In NIPS,", "citeRegEx": "Belkin and Niyogi.,? \\Q2001\\E", "shortCiteRegEx": "Belkin and Niyogi.", "year": 2001}, {"title": "Dynamic Programming", "author": ["R. Bellman", "Rand Corporation."], "venue": "Princeton University Press. https://books. google.it/books?id=wdtoPwAACAAJ", "citeRegEx": "Bellman and Corporation.,? 1957", "shortCiteRegEx": "Bellman and Corporation.", "year": 1957}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["Ella Bingham", "Heikki Mannila."], "venue": "Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 245\u2013250.", "citeRegEx": "Bingham and Mannila.,? 2001", "shortCiteRegEx": "Bingham and Mannila.", "year": 2001}, {"title": "Word Sequence Kernels", "author": ["Nicola Cancedda", "Eric Gaussier", "Cyril Goutte", "Jean Michel Renders."], "venue": "J. Mach. Learn. Res. 3 (March 2003), 1059\u20131082. http://dl.acm.org/citation.cfm?id=944919.944963", "citeRegEx": "Cancedda et al\\.,? 2003", "shortCiteRegEx": "Cancedda et al\\.", "year": 2003}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["Sharan Chetlur", "Cliff Woolley", "Philippe Vandermersch", "Jonathan Cohen", "John Tran", "Bryan Catanzaro", "Evan Shelhamer."], "venue": "arXiv preprint arXiv:1410.0759 (2014).", "citeRegEx": "Chetlur et al\\.,? 2014", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Aspect of Syntax Theory", "author": ["Naom Chomsky."], "venue": "MIT Press, Cambridge, Massachussetts.", "citeRegEx": "Chomsky.,? 1957", "shortCiteRegEx": "Chomsky.", "year": 1957}, {"title": "A Compositional Distributional Model of Meaning", "author": ["Stephen Clark", "Bob Coecke", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the Second Symposium on Quantum Interaction (QI-2008) (2008), 133\u2013140.", "citeRegEx": "Clark et al\\.,? 2008", "shortCiteRegEx": "Clark et al\\.", "year": 2008}, {"title": "Mathematical Foundations for a Compositional Distributional Model of Meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark."], "venue": "CoRR abs/1003.4394 (2010).", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "Convolution Kernels for Natural Language", "author": ["Michael Collins", "Nigel Duffy."], "venue": "NIPS. 625\u2013632.", "citeRegEx": "Collins and Duffy.,? 2001", "shortCiteRegEx": "Collins and Duffy.", "year": 2001}, {"title": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods", "author": ["Nello Cristianini", "John Shawe-Taylor."], "venue": "Cambridge University Press. http://www.amazon.ca/exec/obidos/ redirect?tag=citeulike09-20&amp; path=ASIN/0521780195", "citeRegEx": "Cristianini and Shawe.Taylor.,? 2000", "shortCiteRegEx": "Cristianini and Shawe.Taylor.", "year": 2000}, {"title": "Scalable deep learning on distributed GPUs with a GPU-specialized parameter server", "author": ["Henggang Cui", "Gregory R Ganger", "Phillip B Gibbons."], "venue": "Technical Report. CMU PDL Technical Report (CMU-PDL15-107).", "citeRegEx": "Cui et al\\.,? 2015", "shortCiteRegEx": "Cui et al\\.", "year": 2015}, {"title": "Recognizing Textual Entailment: Models and Applications", "author": ["Ido Dagan", "Dan Roth", "Mark Sammons", "Fabio Massimo Zanzotto."], "venue": "Morgan & Claypool Publishers. 1\u2013220 pages.", "citeRegEx": "Dagan et al\\.,? 2013", "shortCiteRegEx": "Dagan et al\\.", "year": 2013}, {"title": "Curse of dimensionality and particle filters", "author": ["Fred Daum", "Jim Huang."], "venue": "Aerospace Conference, 2003. Proceedings. 2003 IEEE, Vol. 4. IEEE, 4 1979\u20134 1993.", "citeRegEx": "Daum and Huang.,? 2003", "shortCiteRegEx": "Daum and Huang.", "year": 2003}, {"title": "Towards Syntax-aware Compositional Distributional Semantic Models", "author": ["Lorenzo Ferrone", "Fabio Massimo Zanzotto."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association for Computational Linguistics, Dublin, Ireland, 721\u2013730. http://www.aclweb.org/anthology/C14-1068", "citeRegEx": "Ferrone and Zanzotto.,? 2014", "shortCiteRegEx": "Ferrone and Zanzotto.", "year": 2014}, {"title": "Decoding Distributed Tree Structures", "author": ["Lorenzo Ferrone", "Fabio Massimo Zanzotto", "Xavier Carreras."], "venue": "Statistical Language and Speech Processing - Third International Conference, SLSP 2015, Budapest, Hungary, November 24-26, 2015, Proceedings. 73\u201383. DOI:http://dx.doi.org/10.1007/978-3-319-25789-1 8", "citeRegEx": "Ferrone et al\\.,? 2015", "shortCiteRegEx": "Ferrone et al\\.", "year": 2015}, {"title": "Papers in Linguistics", "author": ["John R. Firth."], "venue": "Oxford University Press., London.", "citeRegEx": "Firth.,? 1957", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "A Survey of Dimension Reduction Techniques", "author": ["Imola Fodor."], "venue": "Technical Report.", "citeRegEx": "Fodor.,? 2002", "shortCiteRegEx": "Fodor.", "year": 2002}, {"title": "Die Grundlagen der Arithmetik (The Foundations of Arithmetic): eine logischmathematische Untersuchung ber den Begriff der Zahl", "author": ["Gottlob Frege."], "venue": "Breslau.", "citeRegEx": "Frege.,? 1884", "shortCiteRegEx": "Frege.", "year": 1884}, {"title": "On bias, variance, 0/1loss, and the curse-of-dimensionality", "author": ["Jerome H Friedman."], "venue": "Data mining and knowledge discovery 1, 1 (1997), 55\u201377.", "citeRegEx": "Friedman.,? 1997", "shortCiteRegEx": "Friedman.", "year": 1997}, {"title": "word2vec Explained: deriving Mikolov et al.\u2019s negative-sampling word-embedding method", "author": ["Yoav Goldberg", "Omer Levy"], "venue": "arXiv preprint arXiv:1402.3722", "citeRegEx": "Goldberg and Levy.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg and Levy.", "year": 2014}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."], "venue": "Advances in Neural Information Processing Systems. 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Generating Sequences With Recurrent Neural Networks", "author": ["Alex Graves."], "venue": "CoRR abs/1308.0850 (2013). http://arxiv.org/abs/1308.0850", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP \u201911). Association for Computational Linguistics, Stroudsburg, PA, USA, 1394\u20131404. http://dl.acm.org/citation.cfm?id=2145432.2145580", "citeRegEx": "Grefenstette and Sadrzadeh.,? 2011", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "A Regression Model of Adjective-Noun Compositionality in Distributional Semantics", "author": ["Emiliano Guevara."], "venue": "Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics. Association for Computational Linguistics, Uppsala, Sweden, 33\u201337. http://www.aclweb.org/anthology/W10-2805", "citeRegEx": "Guevara.,? 2010", "shortCiteRegEx": "Guevara.", "year": 2010}, {"title": "Distributional Structure", "author": ["Zellig Harris."], "venue": "The Philosophy of Linguistics, Jerrold J. Katz and Jerry A. Fodor (Eds.). Oxford University Press, New York.", "citeRegEx": "Harris.,? 1964", "shortCiteRegEx": "Harris.", "year": 1964}, {"title": "Convolution kernels on discrete structures", "author": ["David Haussler."], "venue": "Technical Report. University of California at Santa Cruz. http://www.cbse.ucsc.edu/staff/haussler pubs/convolutions.pdf", "citeRegEx": "Haussler.,? 1999", "shortCiteRegEx": "Haussler.", "year": 1999}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv preprint arXiv:1603.05027 (2016).", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Distributed representations", "author": ["G.E. Hinton", "J.L. McClelland", "D.E. Rumelhart."], "venue": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, D. E. Rumelhart and J. L. McClelland (Eds.). MIT Press, Cambridge, MA.", "citeRegEx": "Hinton et al\\.,? 1986", "shortCiteRegEx": "Hinton et al\\.", "year": 1986}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9, 8 (1997), 1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W. Johnson", "J. Lindenstrauss."], "venue": "Contemp. Math. 26 (1984), 189\u2013206.", "citeRegEx": "Johnson and Lindenstrauss.,? 1984", "shortCiteRegEx": "Johnson and Lindenstrauss.", "year": 1984}, {"title": "Recurrent Convolutional Neural Networks for Discourse Compositionality", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Workshop on Continuous Vector Space Models and their Compositionality (2013).", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Curse of dimensionality", "author": ["Eamonn Keogh", "Abdullah Mueen."], "venue": "Encyclopedia of Machine Learning. Springer, 257\u2013258.", "citeRegEx": "Keogh and Mueen.,? 2011", "shortCiteRegEx": "Keogh and Mueen.", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems. 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A Solution to Plato\u2019s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge", "author": ["Thomas K. Landauer", "Susan T. Dumais."], "venue": "Psychological Review 104, 2 (April 1997), 211\u2013240. http://www.sciencedirect.com/science/article/B6X04-46P4NMC-Y/2/ f82804d09e673bd79321d50d30279792", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Deep learning", "author": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton."], "venue": "Nature 521, 7553 (2015), 436\u2013444.", "citeRegEx": "LeCun et al\\.,? 2015", "shortCiteRegEx": "LeCun et al\\.", "year": 2015}, {"title": "Autoencoder for words", "author": ["Cheng-Yuan Liou", "Wei-Chen Cheng", "Jiun-Wei Liou", "Daw-Ran Liou."], "venue": "Neurocomputing 139 (2014), 84 \u2013 96. DOI:http://dx.doi.org/10.1016/j.neucom.2013.09.055", "citeRegEx": "Liou et al\\.,? 2014", "shortCiteRegEx": "Liou et al\\.", "year": 2014}, {"title": "Text classification using string kernels", "author": ["Huma Lodhi", "Craig Saunders", "John Shawe-Taylor", "Nello Cristianini", "Chris Watkins."], "venue": "J. Mach. Learn. Res. 2 (March 2002), 419\u2013444. DOI:http://dx.doi.org/10.1162/153244302760200687", "citeRegEx": "Lodhi et al\\.,? 2002", "shortCiteRegEx": "Lodhi et al\\.", "year": 2002}, {"title": "Low Rank Approximation: Algorithms, Implementation, Applications", "author": ["Ivan Markovsky."], "venue": "(January 2012). http://eprints.soton.ac.uk/273101/", "citeRegEx": "Markovsky.,? 2012", "shortCiteRegEx": "Markovsky.", "year": 2012}, {"title": "Stacked convolutional autoencoders for hierarchical feature extraction", "author": ["Jonathan Masci", "Ueli Meier", "Dan Cire\u015fan", "J\u00fcrgen Schmidhuber."], "venue": "International Conference on Artificial Neural Networks. Springer, 52\u201359.", "citeRegEx": "Masci et al\\.,? 2011", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR abs/1301.3781 (2013). http://arxiv.org/abs/1301.3781", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based Models of Semantic Composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Proceedings of ACL-08: HLT. Association for Computational Linguistics, Columbus, Ohio, 236\u2013244. http://www. aclweb.org/anthology/P/P08/P08-1028", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in Distributional Models of Semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science (2010). DOI:http://dx.doi.org/10.1111/j.1551-6709.2010.01106.x", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller."], "venue": "arXiv preprint arXiv:1312.5602 (2013).", "citeRegEx": "Mnih et al\\.,? 2013", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "English as a Formal Language", "author": ["Richard Montague."], "venue": "Formal Philosophy: Selected Papers of Richard Montague, Richmond Thomason (Ed.). Yale University Press, New Haven, 188\u2013221.", "citeRegEx": "Montague.,? 1974", "shortCiteRegEx": "Montague.", "year": 1974}, {"title": "Holistic processing of hierarchical structures in connectionist networks", "author": ["Jane Neumann."], "venue": "Ph.D. Dissertation. University of Edinburgh.", "citeRegEx": "Neumann.,? 2001", "shortCiteRegEx": "Neumann.", "year": 2001}, {"title": "Dependency-based construction of semantic space models", "author": ["Sebastian Pado", "Mirella Lapata."], "venue": "Computational Linguistics 33, 2 (2007), 161\u2013199.", "citeRegEx": "Pado and Lapata.,? 2007", "shortCiteRegEx": "Pado and Lapata.", "year": 2007}, {"title": "Principal components analysis", "author": ["Karl Pearson."], "venue": "The London, Edinburgh and Dublin Philosophical Magazine and Journal 6, 2 (1901), 566.", "citeRegEx": "Pearson.,? 1901", "shortCiteRegEx": "Pearson.", "year": 1901}, {"title": "Distributed Representations and Nested Compositional Structure", "author": ["T.A. Plate."], "venue": "Ph.D. Dissertation. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.5527", "citeRegEx": "Plate.,? 1994", "shortCiteRegEx": "Plate.", "year": 1994}, {"title": "Holographic reduced representations", "author": ["T.A. Plate."], "venue": "IEEE Transactions on Neural Networks 6, 3 (1995), 623\u2013641. DOI:http://dx.doi.org/10.1109/72.377968", "citeRegEx": "Plate.,? 1995", "shortCiteRegEx": "Plate.", "year": 1995}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain", "author": ["Frank Rosenblatt."], "venue": "Psychological Reviews 65, 6 (November 1958), 386\u2013408. http://www.ncbi.nlm.nih.gov/ pubmed/13602029", "citeRegEx": "Rosenblatt.,? 1958", "shortCiteRegEx": "Rosenblatt.", "year": 1958}, {"title": "Unsupervised Classification with Dependency Based Word Spaces", "author": ["Klaus Rothenh\u00e4usler", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics (GEMS \u201909). Association for Computational Linguistics, Stroudsburg, PA, USA, 17\u201324. http://dl.acm.org/ citation.cfm?id=1705415.1705418", "citeRegEx": "Rothenh\u00e4usler and Sch\u00fctze.,? 2009", "shortCiteRegEx": "Rothenh\u00e4usler and Sch\u00fctze.", "year": 2009}, {"title": "An introduction to random indexing", "author": ["Magnus Sahlgren."], "venue": "Proceedings of the Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering TKE. Copenhagen, Denmark.", "citeRegEx": "Sahlgren.,? 2005", "shortCiteRegEx": "Sahlgren.", "year": 2005}, {"title": "Automatic text processing: the transformation, analysis and retrieval of information by computer", "author": ["G. Salton."], "venue": "Addison-Wesley.", "citeRegEx": "Salton.,? 1989", "shortCiteRegEx": "Salton.", "year": 1989}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber."], "venue": "Neural Networks 61 (2015), 85\u2013117.", "citeRegEx": "Schmidhuber.,? 2015", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "arXiv preprint arXiv:1409.1556 (2014).", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Advances in Neural Information Processing Systems 24.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic Compositionality Through Recursive Matrix-Vector Spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "A survey of dimensionality reduction techniques", "author": ["Carlos Oscar S\u00e1nchez Sorzano", "Javier Vargas", "A Pascual Montano."], "venue": "arXiv preprint arXiv:1403.2877 (2014).", "citeRegEx": "Sorzano et al\\.,? 2014", "shortCiteRegEx": "Sorzano et al\\.", "year": 2014}, {"title": "Similarity of Semantic Relations", "author": ["Peter D. Turney."], "venue": "Comput. Linguist. 32, 3 (2006), 379\u2013416. DOI:http://dx.doi.org/10.1162/coli.2006.32.3.379", "citeRegEx": "Turney.,? 2006", "shortCiteRegEx": "Turney.", "year": 2006}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "J. Artif. Intell. Res. (JAIR) 37 (2010), 141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol."], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, 1096\u20131103.", "citeRegEx": "Vincent et al\\.,? 2008", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol."], "venue": "J. Mach. Learn. Res. 11 (Dec. 2010), 3371\u20133408. http://dl.acm.org/citation.cfm?id=1756006. 1953039", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Grammar as a Foreign Language", "author": ["Oriol Vinyals", "L ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems 28, C. Cortes, N. D.", "citeRegEx": "Vinyals et al\\.,? 2015a", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 3156\u20133164.", "citeRegEx": "Vinyals et al\\.,? 2015b", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "arXiv preprint arXiv:1506.06158 (2015).", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Beyond regression: New tools for prediction and analysis in the behavioral sciences", "author": ["Paul Werbos."], "venue": "(1974).", "citeRegEx": "Werbos.,? 1974", "shortCiteRegEx": "Werbos.", "year": 1974}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1502.03044 2, 3 (2015), 5.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Distributed tree kernels", "author": ["F.M. Zanzotto", "L. Dell\u2019Arciprete"], "venue": "In Proceedings of International Conference on Machine Learning. Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Zanzotto and Dell.Arciprete.,? \\Q2012\\E", "shortCiteRegEx": "Zanzotto and Dell.Arciprete.", "year": 2012}, {"title": "Distributed Tree Kernels", "author": ["Fabio Massimo Zanzotto", "Lorenzo Dell\u2019Arciprete"], "venue": "In Proceedings of International Conference on Machine", "citeRegEx": "Zanzotto and Dell.Arciprete.,? \\Q2012\\E", "shortCiteRegEx": "Zanzotto and Dell.Arciprete.", "year": 2012}, {"title": "When the Whole is Not Greater Than the Combination of Its Parts: A \u201dDecompositional\u201d Look at Compositional Distributional Semantics", "author": ["Fabio Massimo Zanzotto", "Lorenzo Ferrone", "Marco Baroni."], "venue": "Comput. Linguist. 41, 1 (March 2015), 165\u2013173. DOI:http://dx.doi.org/10.1162/COLI a 00215", "citeRegEx": "Zanzotto et al\\.,? 2015", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2015}, {"title": "Estimating Linear Models for Compositional Distributional Semantics", "author": ["Fabio Massimo Zanzotto", "Ioannis Korkontzelos", "Francesca Fallucchi", "Suresh Manandhar."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (COLING).", "citeRegEx": "Zanzotto et al\\.,? 2010", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2010}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus."], "venue": "European Conference on Computer Vision. Springer, 818\u2013833.", "citeRegEx": "Zeiler and Fergus.,? 2014", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}, {"title": "Bilingual Word Embeddings for Phrase-Based Machine Translation", "author": ["Will Y Zou", "Richard Socher", "Daniel M Cer", "Christopher D Manning"], "venue": null, "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 40, "context": "In ML, distributed representations are pushing deep learning models [LeCun et al. 2015; Schmidhuber 2015] towards amazing results in many high-level tasks such as image recognition [He et al.", "startOffset": 68, "endOffset": 105}, {"referenceID": 32, "context": "2015; Schmidhuber 2015] towards amazing results in many high-level tasks such as image recognition [He et al. 2016; Simonyan and Zisserman 2014; Zeiler and Fergus 2014], image generation [Goodfellow et al.", "startOffset": 99, "endOffset": 168}, {"referenceID": 26, "context": "2016; Simonyan and Zisserman 2014; Zeiler and Fergus 2014], image generation [Goodfellow et al. 2014] and image captioning [Vinyals et al.", "startOffset": 77, "endOffset": 101}, {"referenceID": 69, "context": "2014] and image captioning [Vinyals et al. 2015b; Xu et al. 2015], machine translation [Bahdanau et al.", "startOffset": 27, "endOffset": 65}, {"referenceID": 72, "context": "2014] and image captioning [Vinyals et al. 2015b; Xu et al. 2015], machine translation [Bahdanau et al.", "startOffset": 27, "endOffset": 65}, {"referenceID": 2, "context": "2015], machine translation [Bahdanau et al. 2014; Zou et al. 2013], syntactic parsing [Vinyals et al.", "startOffset": 27, "endOffset": 66}, {"referenceID": 78, "context": "2015], machine translation [Bahdanau et al. 2014; Zou et al. 2013], syntactic parsing [Vinyals et al.", "startOffset": 27, "endOffset": 66}, {"referenceID": 68, "context": "2013], syntactic parsing [Vinyals et al. 2015a; Weiss et al. 2015] and even game playing at human level [Silver et al.", "startOffset": 25, "endOffset": 66}, {"referenceID": 70, "context": "2013], syntactic parsing [Vinyals et al. 2015a; Weiss et al. 2015] and even game playing at human level [Silver et al.", "startOffset": 25, "endOffset": 66}, {"referenceID": 48, "context": "2015] and even game playing at human level [Silver et al. 2016; Mnih et al. 2013].", "startOffset": 43, "endOffset": 81}, {"referenceID": 12, "context": "Instead, as a complete semantic representation, vectors for phrases [Mitchell and Lapata 2008; Baroni and Zamparelli 2010; Clark et al. 2008; Grefenstette and Sadrzadeh 2011; Zanzotto et al. 2010] and sentences [Socher et al.", "startOffset": 68, "endOffset": 196}, {"referenceID": 76, "context": "Instead, as a complete semantic representation, vectors for phrases [Mitchell and Lapata 2008; Baroni and Zamparelli 2010; Clark et al. 2008; Grefenstette and Sadrzadeh 2011; Zanzotto et al. 2010] and sentences [Socher et al.", "startOffset": 68, "endOffset": 196}, {"referenceID": 61, "context": "2010] and sentences [Socher et al. 2011; Socher et al. 2012; Kalchbrenner and Blunsom 2013] are obtained by composing vectors for words.", "startOffset": 20, "endOffset": 91}, {"referenceID": 62, "context": "2010] and sentences [Socher et al. 2011; Socher et al. 2012; Kalchbrenner and Blunsom 2013] are obtained by composing vectors for words.", "startOffset": 20, "endOffset": 91}, {"referenceID": 40, "context": "The success of distributed and distributional representations over symbolic approaches is mainly due to the advent of new parallel paradigms that pushed neural networks [Rosenblatt 1958; Werbos 1974] towards deep learning [LeCun et al. 2015; Schmidhuber 2015].", "startOffset": 222, "endOffset": 259}, {"referenceID": 10, "context": "Massively parallel algorithms running on Graphic Processing Units (GPUs) [Chetlur et al. 2014; Cui et al. 2015] crunch vectors, matrices and tensors faster than decades ago.", "startOffset": 73, "endOffset": 111}, {"referenceID": 16, "context": "Massively parallel algorithms running on Graphic Processing Units (GPUs) [Chetlur et al. 2014; Cui et al. 2015] crunch vectors, matrices and tensors faster than decades ago.", "startOffset": 73, "endOffset": 111}, {"referenceID": 20, "context": "However, there is a strict link between distributed/distributional representations and symbols, being the first an approximation of the second [Plate 1994; Plate 1995; Ferrone and Zanzotto 2014; Ferrone et al. 2015].", "startOffset": 143, "endOffset": 215}, {"referenceID": 12, "context": "Finally we discuss more in depth the general issue of compositionality, analyzing three different approaches to the problem: compositional distributional semantics [Clark et al. 2008; Baroni et al. 2014], holographic reduced representations [Plate 1994; Neumann 2001], and recurrent neural networks [Kalchbrenner and Blunsom 2013; Socher et al.", "startOffset": 164, "endOffset": 203}, {"referenceID": 3, "context": "Finally we discuss more in depth the general issue of compositionality, analyzing three different approaches to the problem: compositional distributional semantics [Clark et al. 2008; Baroni et al. 2014], holographic reduced representations [Plate 1994; Neumann 2001], and recurrent neural networks [Kalchbrenner and Blunsom 2013; Socher et al.", "startOffset": 164, "endOffset": 203}, {"referenceID": 62, "context": "2014], holographic reduced representations [Plate 1994; Neumann 2001], and recurrent neural networks [Kalchbrenner and Blunsom 2013; Socher et al. 2012].", "startOffset": 101, "endOffset": 152}, {"referenceID": 42, "context": "To clarify what are distributed representations, we first describe what are local representations (as referred in [Plate 1995]), that is, common feature vectors used to represent symbols, sequences of symbols [Lodhi et al. 2002; Cancedda et al. 2003] or symbolic structures [Haussler 1999; Collins and Duffy 2001] in kernel machines [Cristianini and Shawe-Taylor 2000] or in searching models such as the vector space model [Salton 1989].", "startOffset": 209, "endOffset": 250}, {"referenceID": 9, "context": "To clarify what are distributed representations, we first describe what are local representations (as referred in [Plate 1995]), that is, common feature vectors used to represent symbols, sequences of symbols [Lodhi et al. 2002; Cancedda et al. 2003] or symbolic structures [Haussler 1999; Collins and Duffy 2001] in kernel machines [Cristianini and Shawe-Taylor 2000] or in searching models such as the vector space model [Salton 1989].", "startOffset": 209, "endOffset": 250}, {"referenceID": 33, "context": "In a distributed representation [Plate 1995; Hinton et al. 1986] the informational content is distributed (hence the name) among multiple units, and at the same time each unit can contribute to the representation of multiple elements.", "startOffset": 32, "endOffset": 64}, {"referenceID": 3, "context": "In distributed or, better, in distributional semantic representations [Baroni et al. 2014; Mitchell and Lapata 2010] these two concepts often coincide.", "startOffset": 70, "endOffset": 116}, {"referenceID": 63, "context": "The decoding function is: \u03b4(v\u2032) = W d v \u2032 and W d Wd = I if d is the rank of the matrix X, otherwise it is a degraded approximation (for more details refer to [Fodor 2002; Sorzano et al. 2014]).", "startOffset": 159, "endOffset": 192}, {"referenceID": 61, "context": "\u2014 task-independent representations learned with a standalone algorithm (as in autoencoders [Socher et al. 2011; Liou et al. 2014]) which is independent from any task, and which learns a representation that only depends from the dataset used;", "startOffset": 91, "endOffset": 129}, {"referenceID": 41, "context": "\u2014 task-independent representations learned with a standalone algorithm (as in autoencoders [Socher et al. 2011; Liou et al. 2014]) which is independent from any task, and which learns a representation that only depends from the dataset used;", "startOffset": 91, "endOffset": 129}, {"referenceID": 61, "context": "Autoencoders are a task independent technique to learn a distributed representation encoder \u03b7 : R \u2192 R by using local representations of a set of examples [Socher et al. 2011; Liou et al. 2014].", "startOffset": 154, "endOffset": 192}, {"referenceID": 41, "context": "Autoencoders are a task independent technique to learn a distributed representation encoder \u03b7 : R \u2192 R by using local representations of a set of examples [Socher et al. 2011; Liou et al. 2014].", "startOffset": 154, "endOffset": 192}, {"referenceID": 67, "context": "Autoencoders have been further improved with denoising autoencoders [Vincent et al. 2010; Vincent et al. 2008; Masci et al. 2011] that are a variant of autoencoders where the goal is to reconstruct the input from a corrupted version.", "startOffset": 68, "endOffset": 129}, {"referenceID": 66, "context": "Autoencoders have been further improved with denoising autoencoders [Vincent et al. 2010; Vincent et al. 2008; Masci et al. 2011] that are a variant of autoencoders where the goal is to reconstruct the input from a corrupted version.", "startOffset": 68, "endOffset": 129}, {"referenceID": 44, "context": "Autoencoders have been further improved with denoising autoencoders [Vincent et al. 2010; Vincent et al. 2008; Masci et al. 2011] that are a variant of autoencoders where the goal is to reconstruct the input from a corrupted version.", "startOffset": 68, "endOffset": 129}, {"referenceID": 38, "context": "This is particularly visible with convolutional network [Krizhevsky et al. 2012] applied to computer vision tasks.", "startOffset": 56, "endOffset": 80}, {"referenceID": 45, "context": "In the rest of the section, we present how to build matrices representing words in context, we will shortly recap on how dimensionality reduction techniques have been used in distributional semantics, and, finally, we report on word2vec [Mikolov et al. 2013], which is a novel distributional semantic techniques based on deep learning.", "startOffset": 237, "endOffset": 258}, {"referenceID": 45, "context": "Recently, distributional hypothesis has invaded neural networks: word2vec [Mikolov et al. 2013] uses contextual information to learn word vectors.", "startOffset": 74, "endOffset": 95}, {"referenceID": 12, "context": "This latter is the case for compositional distributional semantics [Clark et al. 2008; Baroni et al. 2014].", "startOffset": 67, "endOffset": 106}, {"referenceID": 3, "context": "This latter is the case for compositional distributional semantics [Clark et al. 2008; Baroni et al. 2014].", "startOffset": 67, "endOffset": 106}, {"referenceID": 75, "context": "Although the \u201csemantic\u201d aspect seems to be predominant in models-that-compose, the convolution conjecture [Zanzotto et al. 2015] hypothesizes that the two aspects coexist and the representational aspect plays always a crucial role.", "startOffset": 106, "endOffset": 128}, {"referenceID": 3, "context": "In distributional semantics, models-that-compose have the name of compositional distributional semantics models (CDSMs) [Baroni et al. 2014; Mitchell and Lapata 2010] and aim to apply the principle of compositionality [Frege 1884; Montague 1974] to compute distributional semantic vectors for phrases.", "startOffset": 120, "endOffset": 166}, {"referenceID": 76, "context": "For example, words and word definitions in dictionaries should have similar vectors as discussed in [Zanzotto et al. 2010].", "startOffset": 100, "endOffset": 122}, {"referenceID": 17, "context": "The applications of these CDSMs encompass multi-document summarization, recognizing textual entailment [Dagan et al. 2013] and, obviously, semantic textual similarity detection [Agirre et al.", "startOffset": 103, "endOffset": 122}, {"referenceID": 1, "context": "2013] and, obviously, semantic textual similarity detection [Agirre et al. 2013].", "startOffset": 60, "endOffset": 80}, {"referenceID": 75, "context": "The convolution conjecture [Zanzotto et al. 2015] suggests that many CDSMs produce distributional vectors where structural information and vectors for individual words can be still intepreted.", "startOffset": 27, "endOffset": 49}, {"referenceID": 76, "context": "where AR and BR are two square matrices depending on the grammatical relation R which may be learned from data [Zanzotto et al. 2010; Guevara 2010].", "startOffset": 111, "endOffset": 147}, {"referenceID": 76, "context": "2 with a matrix AV N , that is: AV Nfr(cows eat animal extracts) \u2248 2eat In general, matrices derived for compositional distributional semantic models [Guevara 2010; Zanzotto et al. 2010] do not have this property but it is possible to obtain matrices with this property by applying thee Jonson-Linderstrauss Tranform [Johnson and Lindenstrauss 1984] or similar techniques as discussed also in [Zanzotto et al.", "startOffset": 150, "endOffset": 186}, {"referenceID": 75, "context": "2010] do not have this property but it is possible to obtain matrices with this property by applying thee Jonson-Linderstrauss Tranform [Johnson and Lindenstrauss 1984] or similar techniques as discussed also in [Zanzotto et al. 2015].", "startOffset": 212, "endOffset": 234}, {"referenceID": 13, "context": "These models have solid mathematical background linking Lambek pregroup theory, formal semantics and distributional semantics [Coecke et al. 2010].", "startOffset": 126, "endOffset": 146}, {"referenceID": 75, "context": "However, using the convolution conjecture [Zanzotto et al. 2015], it is possible to know whether subparts are contained in some final vectors obtained with these models.", "startOffset": 42, "endOffset": 64}, {"referenceID": 61, "context": "In fact, composition functions are trained on specific tasks and not on the possibility of reconstructing the structured input, unless in some rare cases [Socher et al. 2011].", "startOffset": 154, "endOffset": 174}, {"referenceID": 38, "context": "In this section, we revise some prominent neural network architectures that can be interpreted as models-that-compose: the recurrent neural networks [Krizhevsky et al. 2012; He et al. 2016; Vinyals et al. 2015a; Graves 2013] and the recursive neural networks [Socher et al.", "startOffset": 149, "endOffset": 224}, {"referenceID": 32, "context": "In this section, we revise some prominent neural network architectures that can be interpreted as models-that-compose: the recurrent neural networks [Krizhevsky et al. 2012; He et al. 2016; Vinyals et al. 2015a; Graves 2013] and the recursive neural networks [Socher et al.", "startOffset": 149, "endOffset": 224}, {"referenceID": 68, "context": "In this section, we revise some prominent neural network architectures that can be interpreted as models-that-compose: the recurrent neural networks [Krizhevsky et al. 2012; He et al. 2016; Vinyals et al. 2015a; Graves 2013] and the recursive neural networks [Socher et al.", "startOffset": 149, "endOffset": 224}, {"referenceID": 62, "context": "2015a; Graves 2013] and the recursive neural networks [Socher et al. 2012].", "startOffset": 54, "endOffset": 74}, {"referenceID": 38, "context": "At the moment the most powerful network architectures are convolutional neural networks [Krizhevsky et al. 2012; He et al. 2016] for vision related tasks and LSTM-type network for language related task [Vinyals et al.", "startOffset": 88, "endOffset": 128}, {"referenceID": 32, "context": "At the moment the most powerful network architectures are convolutional neural networks [Krizhevsky et al. 2012; He et al. 2016] for vision related tasks and LSTM-type network for language related task [Vinyals et al.", "startOffset": 88, "endOffset": 128}, {"referenceID": 68, "context": "2016] for vision related tasks and LSTM-type network for language related task [Vinyals et al. 2015a; Graves 2013].", "startOffset": 79, "endOffset": 114}, {"referenceID": 62, "context": "The last class of models-that-compose that we present is the class of recursive neural networks [Socher et al. 2012].", "startOffset": 96, "endOffset": 116}], "year": 2017, "abstractText": "Natural language and symbols are intimately correlated. Recent advances in machine learning (ML) and in natural language processing (NLP) seem to contradict the above intuition: symbols are fading away, erased by vectors or tensors called distributed and distributional representations. However, there is a strict link between distributed/distributional representations and symbols, being the first an approximation of the second. A clearer understanding of the strict link between distributed/distributional representations and symbols will certainly lead to radically new deep learning networks. In this paper we make a survey that aims to draw the link between symbolic representations and distributed/distributional representations. This is the right time to revitalize the area of interpreting how symbols are represented inside neural networks.", "creator": "LaTeX with hyperref package"}}}