{"id": "1702.00783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2017", "title": "Pixel Recursive Super Resolution", "abstract": "We present a pixel recursive super resolution model that synthesizes realistic details into images while enhancing their resolution. A low resolution image may correspond to multiple plausible high resolution images, thus modeling the super resolution process with a pixel independent conditional model often results in averaging different details--hence blurry edges. By contrast, our model is able to represent a multimodal conditional distribution by properly modeling the statistical dependencies among the high resolution image pixels, conditioned on a low resolution input. We employ a PixelCNN architecture to define a strong prior over natural images and jointly optimize this prior with a deep conditioning convolutional network. Human evaluations indicate that samples from our proposed model look more photo realistic than a strong L2 regression baseline.", "histories": [["v1", "Thu, 2 Feb 2017 18:59:17 GMT  (3565kb,D)", "http://arxiv.org/abs/1702.00783v1", null], ["v2", "Wed, 22 Mar 2017 16:13:21 GMT  (4255kb,D)", "http://arxiv.org/abs/1702.00783v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["ryan dahl", "mohammad norouzi", "jonathon shlens"], "accepted": false, "id": "1702.00783"}, "pdf": {"name": "1702.00783.pdf", "metadata": {"source": "CRF", "title": "Pixel Recursive Super Resolution", "authors": ["Ryan Dahl", "Mohammad Norouzi", "Jonathon Shlens"], "emails": ["rld@google.com", "mnorouzi@google.com", "shlens@google.com"], "sections": [{"heading": "1. Introduction", "text": "The problem of super-resolution leads to an artificial magnification of a low-resolution photo in order to obtain a plausible high-resolution version of it. If the zoom factor is large, the input image does not contain all the information necessary to construct a high-resolution image. However, a super-resolution model must take into account the complex variations of objects, namely illumination, mainly because the zoom factor increases. The challenge lies not only in the exposure of an image, but also in the generation of objects."}, {"heading": "2. Related work", "text": "The methods that rely on interpolation are easy to implement and widely used, but these methods suffer from a lack of expressiveness because they cannot express complex interdependencies between inputs and outputs. In practice, such methods are often unable to explore high-resolution image files that cause image quality to be blurred. [31, 26, 25, 17] Many of them work on algorithms that form a database of patches and combinations to create plausible details in zoomed images. [7, 13] Recent patch-based work has focused on improving basic interpolation methods by building on images of aforementioned persons."}, {"heading": "3. Probabilistic super resolution", "text": "We aim to learn a probabilistic superresolution model that distinguishes the statistical dependencies between a high-resolution image and a corresponding low-resolution image. Let x and y denote a low-resolution image and a high-resolution image, where y * denotes a ground-level high-resolution image. To learn a parametric model of p\u03b8 (y | x), we use a large dataset of low-resolution pairs and ground-level high-resolution outputs, denominated. To optimize the parameters of the conditional distribution p, we maximize a conditional log likelihood target defined as, O (TB | D) = 1, such a large dataset by starting from a set of high-resolution images and reducing their resolution as necessary. The key problem discussed in this paper is the exact shape of log likelihood objectives defined as, O (\u00b2 | \u00b2), as, O (2), x \u00b2 x, x \u00b2 x, x \u00b2, x, x \u00b2 (TB, x \u00b2)."}, {"heading": "3.1. Pixel independent super resolution", "text": "The simplest form of a probabilistic superresolution model assumes that the output of pixels is conditionally independent of the input pixels. Suppose an RGB output y has M-pixels with three color channels each, i.e. y-x channels. Then, we log p (y-x) = 3M p (yi | x) = 1 log p (yi | x). (2) Two general forms of pixel prediction models have been researched in literature: Gaussian and multinomial distributions to model continuous and discrete pixel values. In the Gaussian case, p (yi | x) = \u2212 12\u0432yi \u2212 Ci (x) \u043222 \u2212 log \u04322\u04322\u043c, (3) where Ci (x) defines the ith element of a nonlinear transformation of x via a conditional neural network."}, {"heading": "3.2. Synthetic multimodal task", "text": "To show how the above pixel-independent models can fail in conditional image modeling, we have created a synthetic dataset that is explicitly multimodal. For many generative tasks, such as superresolution, coloring, and depth estimation, models are desirable that are able to predict a mode without average effects. In this synthetic task, for example, the input is a MNIST digit (1st line of Figure 2), and the output is the same input digit, but scaled and translated either to the upper left corner or the upper right corner (2nd and 3rd lines of Figure 2). The dataset has an equal ratio of left and right upper output, which we call MNIST cornerstone data."}, {"heading": "4. Pixel recursive super resolution", "text": "There are two general methods for modelling statistical correlations between output pixels [8]. One approach is to jointly define the conditional distribution of output pixels by a multivariate Gaussian mixture (36) or an uncritical graphical model such as conditional random fields [8]. With these approaches, one has to commit to a certain form of statistical dependence between the initial pixels, for which inferences can be computationally expensive. The second approach we follow in this paper is to factoring the conditional distribution by means of a chain rule as, log p) = M (1 log p) (yi | x, y < i), (5) where the generation of each initial dimension is conditioned."}, {"heading": "4.1. Implementation details", "text": "The conditioning network is an upstream Convolutionary Neural Network transmitting an 8 x 8 RGB image. We use a non-standard temperature concept that represents 1 \u03c4 in the standard notation. A series of ResNet [10] blocks and transposed folding layers while maintaining a total of 32 channels. The last layer uses a 1 x 1 folding to increase the channels to 256 x 3 and uses the resulting activations to predict multinomic distribution over 256 possible sub-pixel values using a Softmax operator, but we have no way to take coherent samples from it. Sampling sub-pixels provides the ability to absorb the global structure of the image in the marginal probability distribution of pixels. Due to the Softmax layer, it can capture the rich subtleties of the high-resolution distribution, but we have no way to extract them."}, {"heading": "5. Experiments", "text": "We evaluate the effectiveness of the proposed pixel recursive super-resolution method using two sets of small faces and bedroom images. The first set is a version of the CelebA dataset [19], consisting of a series of celebrity faces cropped around the face. In the second set, LSUN Bedrooms [32], images are cropped in the middle. In both sets, we reduce the images to 32 x 32 with bicubic interpolation and again to 8 x 8, representing the output and input pairs for training and evaluation. We present representative super-resolution examples based on high-level test sets and report on human ratings of our predictions in Table 1. We compare our results with two baseline: a pixel-independent L2 regression (\"regression\") and a search near neighbors (\"NN\"). The architecture used for the regression basis is identical to the condition network resource used in our resource model, which is used in multiple blocks."}, {"heading": "5.1. Sampling", "text": "Sampling from the model several times results in different high-resolution images for a given low-resolution image (Figure 5). A given model will identify many plausible high-resolution images that correspond to a given low-resolution image. Each of these samples may contain different qualitative characteristics, and each of these modes is contained within PixelCNN. Note that the differences between the samples for the facedataset are much less dramatic than in our synthetic dataset, where failure to accurately predict modes meant complete failure. Sampling at Should = 1.0, the exact probability given by the model, tends to be more nervous with high frequency content (\u03c4 = \u221e), the samples are of poor quality, they look smooth with horizontal and vertical line artifacts. Sampling at Should = 1.0, the exact probability sampled by the model, tends to be more nervous with high frequency content."}, {"heading": "5.2. Image similarity", "text": "There are many methods of quantifying image similarity that attempt to measure human perceptions of similarity [29, 30, 20]. We quantified the predictive accuracy of our model compared to soil truth using pSNR and MS-SSIM (Table 1). We found that our own visual assessment of the predicted image quality did not correspond to these image similarities. For example, bicubic interpolation reached relatively high measurements, although the samples appeared quite poor. This result is consistent with recent observations suggesting that pSNR and SSIM provide poor assessments of superresolution quality [18, 14] when new details are synthesized. Specifically, we measured the L2 distance between the low-resolution input image and a high-resolution input image (Table 1, \"Consistency\")."}, {"heading": "5.3. Human study", "text": "After the setup in Zhang et al. [34], we present each image for a second before they can answer. Workers are started with 10 pairs of exercises, in which they receive feedback on whether they choose correctly or not. The pairs of exercises are not counted in the results. After the pairs of exercises, they are shown 45 additional pairs, 5 of which are golden questions, to test whether the person is paying attention. The golden question asks a bicubically uploaded image (very blurry) against the basic truth. Without the golden and practical questions, we count forty answers per session. Sessions in which they do not ask golden questions are thrown out. Workers were only allowed to participate in one of our studies. We continued sessions until forty different workers were tested on each of the four algorithms.We report in Table 1 percent of the time in which the subjects select an algorithm that completely confuses the 50% truth."}, {"heading": "6. Conclusion", "text": "As with many image transformation tasks, the central problem of super-resolution is the hallucination of sharp details by choosing a mode of output distribution. We investigated this sub-specific problem using small images and showed that even the smallest 8 x 8 images can be magnified to sharp 32 x 32 images. We introduced a toy dataset with a small number of explicit modes to demonstrate the error cases of two common pixel-independent probability models. In this model, the conditioning network brings us most of the way to predicting a high-resolution image, but the results are blurred where the model is uncertain. Combining the conditioning network with a pixel-based CNN model provides a strong prediction of the output pixels and allows the model to make crisp predictions. Our human evaluations suggest that samples from our model look more realistic on average than a strongly regression-based conditioning network alone."}, {"heading": "B. LSUN bedrooms samples", "text": "Input Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NNInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NNInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NNInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NNInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NN"}, {"heading": "C. Cropped CelebA faces", "text": "Input Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NNInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NNInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NNInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NNInput Bicubic Regression \u03c4 = 1.0 \u03c4 = 1.1 \u03c4 = 1.2 Truth NN"}], "references": [{"title": "and X", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu"], "venue": "Zheng. Tensor- Flow: Large-scale machine learning on heterogeneous systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Svdd: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "Trans. Sig. Proc.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Super-resolution with deep convolutional sufficient statistics", "author": ["J. Bruna", "P. Sprechmann", "Y. LeCun"], "venue": "CoRR, abs/1511.05666", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "A. Szlam", "R. Fergus"], "venue": "NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Image superresolution using deep convolutional networks", "author": ["C. Dong", "C.C. Loy", "K. He", "X. Tang"], "venue": "CoRR, abs/1501.00092", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Image upsampling via imposed edge statistics", "author": ["R. Fattal"], "venue": "ACM Trans. Graph.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Examplebased super-resolution", "author": ["W.T. Freeman", "T.R. Jones", "E.C. Pasztor"], "venue": "IEEE Computer graphics and Applications", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Markov networks for superresolution", "author": ["W.T. Freeman", "E.C. Pasztor"], "venue": "CISS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "A neural algorithm of artistic style", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "CoRR, abs/1508.06576", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Cubic splines for image interpolation and digital filtering", "author": ["H. Hou", "H. Andrews"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Statistics of natural images and models", "author": ["J. Huang", "D. Mumford"], "venue": "Computer Vision and Pattern Recognition, 1999. IEEE Computer Society Conference on., volume 1. IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Single image superresolution from transformed self-exemplars", "author": ["J.-B. Huang", "A. Singh", "N. Ahuja"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Perceptual losses for real-time style transfer and super-resolution", "author": ["J. Johnson", "A. Alahi", "F. Li"], "venue": "CoRR, abs/1603.08155", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Amortised MAP Inference for Image Superresolution", "author": ["C. Kaae S\u00f8nderby", "J. Caballero", "L. Theis", "W. Shi", "F. Husz\u00e1r"], "venue": "ArXiv e-prints,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Accurate image superresolution using very deep convolutional networks", "author": ["J. Kim", "J.K. Lee", "K.M. Lee"], "venue": "CoRR, abs/1511.04587", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Single-image super-resolution using sparse regression and natural image prior", "author": ["K.I. Kim", "Y. Kwon"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(6):1127\u2013 1133", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Photo-realistic single im-  age super-resolution using a generative adversarial network", "author": ["C. Ledig", "L. Theis", "F. Huszar", "J. Caballero", "A. Aitken", "A. Tejani", "J. Totz", "Z. Wang", "W. Shi"], "venue": "arXiv:1609.04802", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "Proceedings of International Conference on Computer Vision (ICCV)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Group mad competition - a new methodology to compare objective image quality models", "author": ["K. Ma", "Q. Wu", "Z. Wang", "Z. Duanmu", "H. Yong", "H. Li", "L. Zhang"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Unrolled generative adversarial networks", "author": ["L. Metz", "B. Poole", "D. Pfau", "J. Sohl-Dickstein"], "venue": "CoRR, abs/1611.02163", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Super-resolution: A comprehensive survey", "author": ["K. Nasrollahi", "T.B. Moeslund"], "venue": "Mach. Vision Appl.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "RAISR: rapid and accurate image super resolution", "author": ["Y. Romano", "J. Isidoro", "P. Milanfar"], "venue": "CoRR, abs/1606.01299", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Pixelcnn++: A pixelcnn implementation with discretized logistic mixture likelihood and other modifications. under review at ICLR 2017", "author": ["T. Salimans", "A. Karpathy", "X. Chen", "D.P. Kingma", "Y. Bulatov"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}, {"title": "Fast image/video upsampling", "author": ["Q. Shan", "Z. Li", "J. Jia", "C.-K. Tang"], "venue": "ACM Transactions on Graphics (TOG), 27(5):153", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Image super-resolution using gradient profile prior", "author": ["J. Sun", "Z. Xu", "H.-Y. Shum"], "venue": "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1\u20138. IEEE", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Pixel recurrent neural networks", "author": ["A. van den Oord", "N. Kalchbrenner", "K. Kavukcuoglu"], "venue": "ICML, 2016", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A. van den Oord", "N. Kalchbrenner", "O. Vinyals", "L. Espeholt", "A. Graves", "K. Kavukcuoglu"], "venue": "NIPS, 2016", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "IEEE transactions on image processing, 13(4):600\u2013612", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Multiscale structural similarity for image quality assessment", "author": ["Z. Wang", "E.P. Simoncelli", "A.C. Bovik"], "venue": "Signals, Systems and Computers, 2004. Conference Record of the Thirty-Seventh Asilomar Conference on, volume 2, pages 1398\u20131402. Ieee", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Structured face hallucination", "author": ["C.Y. Yang", "S. Liu", "M.H. Yang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop", "author": ["F. Yu", "Y. Zhang", "S. Song", "A. Seff", "J. Xiao"], "venue": "arXiv preprint arXiv:1506.03365", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Ultra-Resolving Face Images by Discriminative Generative Networks", "author": ["X. Yu", "F. Porikli"], "venue": "pages 318\u2013333. Springer International Publishing, Cham", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "ECCV", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "From learning models of natural image patches to whole image restoration", "author": ["D. Zoran", "Y. Weiss"], "venue": "Proceedings of the 2011 International Conference on Computer Vision, ICCV \u201911, pages 479\u2013486, Washington, DC, USA", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "From learning models of natural image patches to whole image restoration", "author": ["D. Zoran", "Y. Weiss"], "venue": "CVPR", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 9, "context": "Our conditioning network consists of a deep stack of ResNet [10] blocks, while our prior network comprises a PixelCNN [28] architecture.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "Our conditioning network consists of a deep stack of ResNet [10] blocks, while our prior network comprises a PixelCNN [28] architecture.", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "Super resolution has a long history in computer vision [22].", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "Methods relying on interpolation [11] are easy to implement and widely used, however these methods suffer from a lack of expressivity since linear models cannot express complex dependencies between the inputs and outputs.", "startOffset": 33, "endOffset": 37}, {"referenceID": 1, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 65, "endOffset": 68}, {"referenceID": 34, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 243, "endOffset": 266}, {"referenceID": 25, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 243, "endOffset": 266}, {"referenceID": 5, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 243, "endOffset": 266}, {"referenceID": 11, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 243, "endOffset": 266}, {"referenceID": 24, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 243, "endOffset": 266}, {"referenceID": 16, "context": "Enhancing linear methods with rich image priors such as sparsity [2] or Gaussian mixtures [35] have substantially improved the quality of the methods; likewise, leveraging low-level image statistics such as edge gradients improves predictions [31, 26, 6, 12, 25, 17].", "startOffset": 243, "endOffset": 266}, {"referenceID": 6, "context": "Much work has been done on algorithms that search a database of patches and combine them to create plausible high frequency details in zoomed images [7, 13].", "startOffset": 149, "endOffset": 156}, {"referenceID": 12, "context": "Much work has been done on algorithms that search a database of patches and combine them to create plausible high frequency details in zoomed images [7, 13].", "startOffset": 149, "endOffset": 156}, {"referenceID": 22, "context": "Recent patch-based work has focused on improving basic interpolation methods by building a dictionary of pre-learned filters on images and selecting the appropriate patches by an efficient hashing mechanism [23].", "startOffset": 207, "endOffset": 211}, {"referenceID": 4, "context": "[5] employed a three layer CNN with MSE loss.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16] improved accuracy by increasing the depth to 20 layers and learning only the residuals between the high resolution image and an interpolated low resolution image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Most recently, SRResNet [18] uses many ResNet blocks to achieve state of the art pSNR and SSIM on standard super resolution benchmarks\u2013we employ a similar design for our conditional network and catchall regression baseline.", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "[14] use Euclidean distance between activations of a pre-trained CNN for model\u2019s predictions vs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] also use perceptual loss to train a super resolution network, but inference is done via gradient propagation to the low-res input (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": ", [9]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 17, "context": "[18] and Yu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] use GANs to create compelling super resolution results showing the ability of the model to predict plausible high frequency details.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] also investigate GANs for super resolution using a learned affine transformation that ensures the models only generate images that downscale back to the low resolution inputs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] also explore a masked autoregressive model like PixelCNN [27] but without the gated layers and using a mixture of gaussians instead of a multinomial distribution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[15] also explore a masked autoregressive model like PixelCNN [27] but without the gated layers and using a mixture of gaussians instead of a multinomial distribution.", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "[4] use a multi-scale adversarial network for image synthesis, but the architecture also seems beneficial for super resolution.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[27, 28] are probabilistic generative models that impose an order on image pixels representing them as a long sequence.", "startOffset": 0, "endOffset": 8}, {"referenceID": 27, "context": "[27, 28] are probabilistic generative models that impose an order on image pixels representing them as a long sequence.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "One of their common failure cases is mode collapsing were samples are not diverse enough [21].", "startOffset": 89, "endOffset": 93}, {"referenceID": 26, "context": "Finally we describe our pixel recursive super resolution model that generates output pixels one at a time to enable modeling the statistical dependencies between the output pixels using PixelCNN [27, 28], and synthesizes sharp images from very blurry input.", "startOffset": 195, "endOffset": 203}, {"referenceID": 27, "context": "Finally we describe our pixel recursive super resolution model that generates output pixels one at a time to enable modeling the statistical dependencies between the output pixels using PixelCNN [27, 28], and synthesizes sharp images from very blurry input.", "startOffset": 195, "endOffset": 203}, {"referenceID": 4, "context": ", [5, 16, 18]) fall within this family of pixel independent models, where the outputs of a neural network parameterize a set of Gaussians with fixed bandwidth.", "startOffset": 2, "endOffset": 13}, {"referenceID": 15, "context": ", [5, 16, 18]) fall within this family of pixel independent models, where the outputs of a neural network parameterize a set of Gaussians with fixed bandwidth.", "startOffset": 2, "endOffset": 13}, {"referenceID": 17, "context": ", [5, 16, 18]) fall within this family of pixel independent models, where the outputs of a neural network parameterize a set of Gaussians with fixed bandwidth.", "startOffset": 2, "endOffset": 13}, {"referenceID": 35, "context": "One approach is to define the conditional distribution of the output pixels jointly by either a multivariate Gaussian mixture [36] or an undirected graphical model such as conditional random fields [8].", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "One approach is to define the conditional distribution of the output pixels jointly by either a multivariate Gaussian mixture [36] or an undirected graphical model such as conditional random fields [8].", "startOffset": 198, "endOffset": 201}, {"referenceID": 27, "context": "The prior network, a PixelCNN [28], makes predictions based on previous stochastic predictions (indicated by dashed line).", "startOffset": 30, "endOffset": 34}, {"referenceID": 23, "context": "with Gaussian or Logistic (mixture) conditionals as proposed in [24].", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "a series of ResNet [10] blocks and transpose convolution layers while maintaining 32 channels throughout.", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "Our model is built by using TensorFlow [1] and trained across 8 GPUs with synchronous SGD updates.", "startOffset": 39, "endOffset": 42}, {"referenceID": 18, "context": "The first dataset is a version of the CelebA dataset [19] composed of a set of celebrity faces, which are cropped around the face.", "startOffset": 53, "endOffset": 57}, {"referenceID": 31, "context": "In the second dataset LSUN Bedrooms [32], images are center cropped.", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "The regression architecture is similar in design to to SRResNet [18], which reports state of the art scores in image similarity metrics.", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "The residuals are computed based on bicubic interpolation of the input, and are known to work better to provide superior predictions [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "Many methods exist for quantifying image similarity that attempt to measure human perception judgements of similarity [29, 30, 20].", "startOffset": 118, "endOffset": 130}, {"referenceID": 29, "context": "Many methods exist for quantifying image similarity that attempt to measure human perception judgements of similarity [29, 30, 20].", "startOffset": 118, "endOffset": 130}, {"referenceID": 19, "context": "Many methods exist for quantifying image similarity that attempt to measure human perception judgements of similarity [29, 30, 20].", "startOffset": 118, "endOffset": 130}, {"referenceID": 17, "context": "This result matches recent observations that suggest that pSNR and SSIM provide poor judgements of super resolution quality [18, 14] when new details are synthesized.", "startOffset": 124, "endOffset": 132}, {"referenceID": 13, "context": "This result matches recent observations that suggest that pSNR and SSIM provide poor judgements of super resolution quality [18, 14] when new details are synthesized.", "startOffset": 124, "endOffset": 132}, {"referenceID": 33, "context": "Workers were asked \u201cWhich image, would you guess, is from a camera?\u201d Following the setup in Zhang et al [34], we present each image for one second at a time before allowing them to answer.", "startOffset": 104, "endOffset": 108}, {"referenceID": 0, "context": "Consistency lists the MSE between the input low-res image and downsampled samples on a [0, 1] scale.", "startOffset": 87, "endOffset": 93}], "year": 2017, "abstractText": "We present a pixel recursive super resolution model that synthesizes realistic details into images while enhancing their resolution. A low resolution image may correspond to multiple plausible high resolution images, thus modeling the super resolution process with a pixel independent conditional model often results in averaging different details\u2013 hence blurry edges. By contrast, our model is able to represent a multimodal conditional distribution by properly modeling the statistical dependencies among the high resolution image pixels, conditioned on a low resolution input. We employ a PixelCNN architecture to define a strong prior over natural images and jointly optimize this prior with a deep conditioning convolutional network. Human evaluations indicate that samples from our proposed model look more photo realistic than a strong L2 regression baseline.", "creator": "LaTeX with hyperref package"}}}