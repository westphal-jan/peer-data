{"id": "1702.00956", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2017", "title": "KU-ISPL Speaker Recognition Systems under Language mismatch condition for NIST 2016 Speaker Recognition Evaluation", "abstract": "Korea University Intelligent Signal Processing Lab. (KU-ISPL) developed speaker recognition system for SRE16 fixed training condition. Data for evaluation trials are collected from outside North America, spoken in Tagalog and Cantonese while training data only is spoken English. Thus, main issue for SRE16 is compensating the discrepancy between different languages. As development dataset which is spoken in Cebuano and Mandarin, we could prepare the evaluation trials through preliminary experiments to compensate the language mismatched condition. Our team developed 4 different approaches to extract i-vectors and applied state-of-the-art techniques as backend. To compensate language mismatch, we investigated and endeavored unique method such as unsupervised language clustering, inter language variability compensation and gender/language dependent score normalization.", "histories": [["v1", "Fri, 3 Feb 2017 10:15:29 GMT  (413kb)", "http://arxiv.org/abs/1702.00956v1", "SRE16, NIST SRE 2016 system description"], ["v2", "Mon, 6 Feb 2017 03:37:28 GMT  (414kb)", "http://arxiv.org/abs/1702.00956v2", "SRE16, NIST SRE 2016 system description"]], "COMMENTS": "SRE16, NIST SRE 2016 system description", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["suwon shon", "hanseok ko"], "accepted": false, "id": "1702.00956"}, "pdf": {"name": "1702.00956.pdf", "metadata": {"source": "META", "title": "KU-ISPL SPEAKER RECOGNITION SYSTEMS", "authors": ["Suwon Shon", "Hanseok Ko"], "emails": ["swshon@ispl.korea.ac.kr,", "hsko@korea.ac.kr"], "sections": [{"heading": null, "text": "Data for evaluation studies are collected outside North America, in Tagalog and Cantonese, while the training data is only spoken in English. Therefore, the main problem for SRE16 is to compensate for the discrepancy between different languages. As a developmental dataset, spoken in Cebuano and Mandarin, we were able to prepare the evaluation studies through preliminary experiments to compensate for the linguistic discrepancy. Our team developed 4 different approaches to extract i-vectors and apply state-of-the-art techniques as backend. To compensate for linguistic discrepancies, we investigated and attempted unique methods such as uncontrolled language clustering, variability compensation between languages, and gender / language normalization. Index terms - SRE16, i-vector, linguistic discrepancy"}, {"heading": "1. INTRODUCTION", "text": "This document is a description of the Korea University - Intelligent Signal Processing Laboratory (KU-ISPL) Speaker Recognition System for NIST 2016 Speaker Recognition Evaluation (SRE16). Within the i-Vector Framework, new approaches are introduced using the Bottleneck Feature (BNF) and Deep Neural Network (DNN), which have successfully validated their performance improvement on ASR. In this study, we developed the state-of-the-art i-Vector systems for validating performance in case of speech mismatch using the SRE16 dataset. Based on previous studies on domain matching and compensation, Inter Dataset Variability Compensation (IDVC) and unattended domain matching using interpolated PLDAs were also applied. After studying previous work, we proposed additional techniques to compensate for speech mismatch, in order to obtain robust performance on the overall R16 dataset, we submitted a training system for official Evaluation below E16."}, {"heading": "2. DATASET PREPARATION FOR FIXED TRAINING CONDITION", "text": "For fixed training conditions, we use Fisher English, SRE 04 ~ 10 and SWB-2 (phase1 ~ 3, cellular 1 ~ 2) dataset for the training set. Language of all datasets in the training set is English. The dataset for SRE 16 evaluation studies is collected by speakers based outside North America who speak Tagalog and Cantonese (referred to as the main language). Before the evaluation dataset is available, a development dataset is created that reflects the evaluation conditions in order to prepare the linguistic inmatch condition for the evaluation set. The development dataset is collected by speakers based outside North America who speak Cebuano and Mandarin (referred to as the secondary language). In addition, participants are given unlabeled datasets for development. The development dataset is available for any purpose and detailed statistics on evaluation and development are presented in Table 1."}, {"heading": "3. SYSTEM COMPONENT DESCRIPTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Acoustic features", "text": "For the training of loudspeaker recognition systems on this paper, Mel-Frequency Cepstral Coefficient (MFCC) is used to generate 60-dimensional acoustic characteristics. It consists of 20 ceptral coefficients, including log-energy C0, then it is appended with its delta and its acceleration. For the training of DNN-based acoustic models inspired by the Automatic Speech Recognition (ASR) range, a different configuration was adopted to generate 40 ceptral coefficients without energy components for high resolution acoustic characteristics (ASR-MFCC). For the normalization of characteristics, Cepstral Mean Normalization is applied with 3-second gliding wind. After extracting acoustic characteristics, Voice Activity Algorithm was used to remove silence and low energy segments from the speech dataset. Simple energy-based VAD was used with log-mean scaled threshold."}, {"heading": "3.2. I-vector extraction", "text": "Four different approaches for the extraction of i-vectors are being developed for the performance comparison of SRE 16 studies."}, {"heading": "3.2.1. GMM-UBM (GU)", "text": "For the general i-vector extraction approach [1] by modification of Kaldi's recipe (sre08 / v1) GMM-UBM and Total Variability Matrix, SRE (04-10, part of 12) and Control Panel Data Set (p2-1-3, cellular 1-2) were used."}, {"heading": "3.2.2. DNN-UBM (DU)", "text": "Based on Kaldi's recipe (sre10 / v2), Fisher English was used for training Time Delay Neural Network with ASRMFCC function. After training TDNN, the DNN-UBM is estimated to be DNN-MFCC function, a high resolution version of MFCC. SRE (04-10, part of 12) and switchboard dataset were used for training DNN-UBM and the total variability matrix [2]."}, {"heading": "3.2.3. Supervised GMM-UBM (SU)", "text": "Based on Kaldi's recipe (sre10 / v2), Supervised GMMUBM [2] was trained using the rear TDNN network. The same data set was used as GMM-UBM system for training Supervised GMM-UBM and Total Variability Matrix."}, {"heading": "3.2.4. Bottleneck Feature based GMM-UBM (BU)", "text": "The structure of the DNN layer was set to 1500-1500-80-1500 with a total of 4 layers and the MFCC function of all datasets was converted to BNF function (80 dim). After extracting the BNF function, it follows general i-vector extraction approaches such as the GMM-UBM system in Sec. 3.2.1 and the same dataset was used for the GMMUBM total variability matrix."}, {"heading": "3.3. Backend procedures", "text": "3.3.1. Inter Dataset Variability Compensation (IDVC) [4] SRE and Switchboard (SWB) Dataset Sub-Corpora Label and Gender Label are used to obtain the average i-vectors of each data set by gender. SRE can be divided into 5 sub-corpora (SRE-04, 05, 06, 08, 10) and SWB can be divided into 5 sub-corpora (Switchboard-2 Phase 1,2,3 and cell part 1, 2). Finally, 600 dimensional i-vectors are projected to 580 dimensions by removing the record-dependent dimension. 3.3.2 Whitening Transform and Length Normalization using unlabeled Dataset (WTLN) [5] Whitening transformation and length normalization are simple and powerful techniques to improve the performance of the speaker recognition system by using the discrepancy between enroller and test vector based on the usability adjustment process, as well as to compensate the usable length of a domain system."}, {"heading": "3.3.3. Interpolated PLDA (SRE04-08) + PLDA (speaker", "text": "The approach of the Agglomerative Hierarchical Cluster for unmarked In-Domain Data Sets to estimate the PLDA model was introduced by Garcia-Romero. Using the cluster information, the In-Domain Covariance (WC) and the Across Speaker Covariance (AC) of the PLDA model were interpolated from unmarked WC and AC data sets. We applied this approach to the unmarked data set of the secondary and main languages. By experiments, 30 and 450 clusters were used for clustering unmarked smaller and larger data sets. 450 clusters (Speaker) of unmarked large data sets could be used for calibration as Sec.4.5.3.4. S standard [7] Symmetric Normalization (S standard) for the normalization of scores."}, {"heading": "4. STUDIES FOR COMPENSATING LANGUAGE MISMATCH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Gender Classification and unsupervised Language Classification of minor/major unlabeled dataset (GCLC)", "text": "Gender classification could be done by comparing cosinal similarity between the i-vector of gender and the i-vector of the input we want to classify the gender.Language classification can be done by an uncontrolled clustering algorithm such as AHC or k-mean clustering. Since clustering performance is strongly dependent on the starting point as Figure 1, AHC is often used on i-vector trait segments.For high accuracy and reliability of the clustering algorithm, we suggested a 2-step approach by running the k-mean algorithm twice on different i-vector representation spaces.If the kmeans algorithm ensures good initial clusters representing the mean of i-vectors from each language, we can maintain a high language classification performance, while running the risk of misclassification by random initial clusters.We check this with low test data and low language registration."}, {"heading": "4.2. Inter Language Variability Compensation for gender and minor/major language (ILVC)", "text": "If the system has gender and language information, the variability factor between languages can be removed by the same scheme of the IDVC. Due to the high performance of the GCLC approach that we proposed in Section 4.1, we could have valuable gender and language names for unlabeled data sets of minor and major languages. Finally, i-vector subspace removal can be done by the mean i-vector of 10 subcategories depending on language (English, Cebuano, Mandarin, Tagalog and Cantonese) and gender."}, {"heading": "4.3. Simplified Autoencoder based Domain adaptation (SADA)", "text": "Autoencoder-based domain adaptation (AEDA) was recently proposed and its paper is in the peer-reviewing process for publication. In this study, we simplify AEDA to a simpler autoencoder structure and propose a simplified autoencoder-based domain adaptation (SADA), but it still works almost as well as AEDA. More details on SADA can be found in the next studies."}, {"heading": "4.4. Gender and Language dependent score normalization (GL-Norm)", "text": "We have gender and language information of the unlabeled data set according to the GCLC approach in Section 4.1. Therefore, we divide the snorm parameters into 4 subcategories according to gender and language. Gender and language of the i vector input are also classified according to the GCLC approach and use appropriate parameters to achieve gender and language-specific normalization."}, {"heading": "4.5. Calibration and fusion", "text": "For calibration and fusion, simple linear calibration and linear fusion were performed using the Bosaris toolkit [8]. For calibration, cluster information from loudspeakers from unmarked large data sets was used (see paragraph 3.3.3) to obtain a target and non-target score distribution of evaluation experiments.The mean value of loudspeaker clusters represents loudspeaker i vectors and they can be evaluated with unmarked large i vectors.As we already have loudspeaker labels from loudspeaker clusters, we can obtain a target point and non-target score distribution and they could be used for score calibration in evaluation experiments.The calibration was performed both before and after score normalization."}, {"heading": "5. SUBSYSTEMS FOR MINOR LANUGUAGES", "text": "By applying the components described in sections 3 and 4, we evaluate the development studies with respect to EER, minimal Cprimary and actual Cprimary. Based on the experimental development studies, we confirm that the proposed ILVC and GL standard approach works better under linguistic mismatch conditions than previous work."}, {"heading": "6. SUBSYSTEMS AND ITS FUSION FOR MAJOR LANGUAGE", "text": "We try to use the development dataset as much as possible, because the development record reflects the evaluation, i.e. would contain more valuable information than the training dataset, which language is English. Each subsystem has used most of the competitive techniques from the studies of previous sections to achieve the best result. In contrast to estimating the gender and language marking of the unlabeled dataset for ILVC in sections 5 and Table 2, we use the dataset for enrollment and the test dataset of the secondary language and its labels for ILVC. According to this method, the performance of the subsystems improved dramatically like Table 3. The main reason for this is that we did not process each study independently and use information about the matriculation and test datasets for development studies. We expect that the performance of the evaluation studies would not dramatically improve as development studies do, but still convince that they have beneficial effects on systems for important languages. The submitted system has little difference in the overall dataset normalization of the data evaluation system for both the language evaluation system and the language evaluation system."}, {"heading": "7. CPU EXECUTION TIME", "text": "All tasks were executed on 64bit Linux with 64G RAM and Intel i7 6700 3.4GHz and GTX1080 for GPU. All CPU times are calculated on the basis of a CPU."}, {"heading": "8. REFERENCES", "text": "[1] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P.Ouellet, \"Front-End Factor Analysis for Speaker Verification,\" IEEE Trans. Audio, Speech, Lang. Process., Volume 19, No. 4, pp. 788-798, May 2011. [2] D. Snyder, D. Garcia-Romero, and D. Povey, \"Time delaydeep neural network-based universal background models for speaker recognition,\" 2015 IEEE Work. Autom. Speech Recognit. Understanding, ASRU 2015 - Proc., No. 1232825, pp. 92-97, 2016. [3] F. Richardson, S. Member, D. Reynolds, and N. Dehak, \"Deep Neural Network Approaches to Speaker and Languech Recognition. [Definition and Language Recognition, IEEE Signal Process. Lett., No. 22, S. ARS."}], "references": [{"title": "Front-End Factor Analysis for Speaker Verification", "author": ["N. Dehak", "P.J. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Trans. Audio, Speech, Lang. Process., vol. 19, no. 4, pp. 788\u2013798, May 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Time delay deep neural network-based universal background models for speaker recognition", "author": ["D. Snyder", "D. Garcia-Romero", "D. Povey"], "venue": "2015 IEEE Work. Autom. Speech Recognit. Understanding, ASRU 2015 - Proc., no. 1232825, pp. 92\u201397, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Neural Network Approaches to Speaker and Language Recognition", "author": ["F. Richardson", "S. Member", "D. Reynolds", "N. Dehak"], "venue": "IEEE Signal Process. Lett., vol. 22, no. 10, pp. 1671\u20131675, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Inter dataset variability compensation for speaker recognition", "author": ["H. Aronowitz"], "venue": "IEEE ICASSP, 2014, pp. 4002\u20134006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Analysis of ivector Length Normalization in Speaker Recognition Systems", "author": ["D. Garcia-Romero", "C.Y. Espy-Wilson"], "venue": "Interspeech, 2011, pp. 249\u2013252.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised Domain Adaptation for I-Vector Speaker Recognition", "author": ["D. Garcia-Romero", "A. McCree", "S. Shum", "N. Brummer", "C. Vaquero"], "venue": "Proc. Odyssey 2014 - Speak. Lang. Recognit. Work., no. June, pp. 260\u2013264, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised Speaker Adaptation based on the Cosine Similarity for Text-Independent Speaker Verification", "author": ["S. Shum", "N. Dehak", "R. Dehak", "J.R. Glass"], "venue": "Proc. Odyssey, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "The BOSARIS Toolkit: Theory, Algorithms and Code for Surviving the New DCF", "author": ["N. Br\u00fcmmer", "E. de Villiers"], "venue": "NIST SRE\u201911 Analysis Workshop, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "GMM-UBM (GU) For General i-vector extraction approach [1] by modifying Kaldi\u2019s recipe (sre08/v1).", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "SRE (04~10, part of 12) and Switchboard Dataset were used for training DNN-UBM and total variability matrix [2].", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "Based on Kaldi\u2019s recipe (sre10/v2), Supervised GMMUBM[2] was trained using posterior of TDNN network.", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "BNF features were extracted using DNN which containing bottleneck layer [3].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "Inter Dataset Variability Compensation (IDVC)[4]", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "Whitening Transform and Length Normalization using unlabeled dataset (WTLN)[5]", "startOffset": 75, "endOffset": 78}, {"referenceID": 5, "context": "Interpolated PLDA (SRE04-08) + PLDA (speaker clustering using AHC) (IPLDA) [6]", "startOffset": 75, "endOffset": 78}, {"referenceID": 6, "context": "S-norm [7]", "startOffset": 7, "endOffset": 10}, {"referenceID": 7, "context": "For calibration and fusion, simple linear calibration and linear fusion were done by Bosaris toolkit [8].", "startOffset": 101, "endOffset": 104}], "year": 2017, "abstractText": "Korea University \u2013 Intelligent Signal Processing Lab. (KUISPL) developed speaker recognition system for SRE16 fixed training condition. Data for evaluation trials are collected from outside North America, spoken in Tagalog and Cantonese while training data only is spoken English. Thus, main issue for SRE16 is compensating the discrepancy between different languages. As development dataset which is spoken in Cebuano and Mandarin, we could prepare the evaluation trials through preliminary experiments to compensate the language mismatched condition. Our team developed 4 different approaches to extract i-vectors and applied state-of-the-art techniques as backend. To compensate language mismatch, we investigated and endeavored unique method such as unsupervised language clustering, inter language variability compensation and gender/language dependent score normalization.", "creator": "Microsoft\u00ae Word 2013"}}}