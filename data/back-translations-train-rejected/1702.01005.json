{"id": "1702.01005", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2017", "title": "Intrinsic Grassmann Averages for Online Linear and Robust Subspace Learning", "abstract": "Principal Component Analysis (PCA) is a fundamental method for estimating a linear subspace approximation to high-dimensional data. Many algorithms exist in literature to achieve a statistically robust version of PCA called RPCA. In this paper, we present a geometric framework for computing the principal linear subspaces in both situations that amounts to computing the intrinsic average on the space of all subspaces (the Grassmann manifold). Points on this manifold are defined as the subspaces spanned by $K$-tuples of observations. We show that the intrinsic Grassmann average of these subspaces coincide with the principal components of the observations when they are drawn from a Gaussian distribution. Similar results are also shown to hold for the RPCA. Further, we propose an efficient online algorithm to do subspace averaging which is of linear complexity in terms of number of samples and has a linear convergence rate. When the data has outliers, our proposed online robust subspace averaging algorithm shows significant performance (accuracy and computation time) gain over a recently published RPCA methods with publicly accessible code. We have demonstrated competitive performance of our proposed online subspace algorithm method on one synthetic and two real data sets. Experimental results depicting stability of our proposed method are also presented. Furthermore, on two real outlier corrupted datasets, we present comparison experiments showing lower reconstruction error using our online RPCA algorithm. In terms of reconstruction error and time required, both our algorithms outperform the competition.", "histories": [["v1", "Fri, 3 Feb 2017 13:44:44 GMT  (536kb,D)", "http://arxiv.org/abs/1702.01005v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["rudrasis chakraborty", "s{\\o}ren hauberg", "baba c vemuri"], "accepted": false, "id": "1702.01005"}, "pdf": {"name": "1702.01005.pdf", "metadata": {"source": "CRF", "title": "Intrinsic Grassmann Averages for Online Linear and Robust Subspace Learning", "authors": ["Rudrasis Chakraborty", "S\u00f8ren Hauberg", "Baba C. Vemuri"], "emails": ["baba.vemuri}@gmail.com", "2{sohau}@dtu.dk"], "sections": [{"heading": "1 Introduction", "text": "The Principal Component Analysis (PCA), a central workhorse of machine learning, can also be derived in many ways: Pearson [29] suggested finding the subspace that minimizes the projection error of the observed data; Hotelling [20] instead looked for the subspace in which the projected data exhibits maximum variance; and Tipping & Bishop [34] considered a likely formulation in which the covariance of the normally distributed data is predominantly given by a low-level matrix, all of which leads to the same algorithm. Recently, Hauberg et al. [18] found that the average of all one-dimensional subspaces spanned by normally distributed data coincides with the leading main component. Here, the average is calculated using the Grassmann manifold of one-dimensional subspaces (cf. Sec. 2). This average can be calculated very efficiently, but unfortunately has the free component of a subcomponent 170 X.05V for its subspace."}, {"heading": "1.1 Related Work", "text": "In fact, it is so that it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way and a way in which it is about a way and a way in which it is about a way and a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way and a way in which it is about a way and a way and a way in which it is about a way and a way and a way in which it is about a way and a way and a way in which it is about a way and a way and a way and a way in which it is about a way and a way and a way in which it is about a way and a way and a way in which it is about a way and a way in which it is about a way and a way and a way in which it is about a way and a way in which it is about a way and a way and a way in which it is about a way and a way and a way in which it is about a way and a way and a way and a way in which it is about a way and a way and a way in which it is about a way and a way and a way in which it is about a way and a way and a way and"}, {"heading": "2 An Online Linear Subspace Learning Algorithm", "text": "In this section, we propose an efficient online linear learning algorithm for the subspace to find the main components of a dataset. First, we briefly discuss the geometry of Riemann's manifold K-dimensional linear subspaces in research and development. Then, we present an online algorithm to obtain the first K-main components of the D-dimensional data vectors."}, {"heading": "2.1 The Geometry of Subspaces", "text": "The Grassman Manifold (or the Grassmannian) is defined as the set of allK-dimensional linear subspaces in RD and is referred to as Gr (K, D), where D \u2265 K. A special case of the Grassmannian is when K is = 1, i.e., the space of one-dimensional subspaces of RD known as real projection space (referred to by RPD). A point X-Gr (K, D) can be defined by a base, X, i.e., a series of K-linear independent vectors in RD (the columns of X) extending across X. We say X-Gr (X), if X-Gr (.) is the column tension operator. We have included a brief note on the geometry of the Grassmannians in RD (the columns of X)."}, {"heading": "2.2 The Intrinsic Grassmann Average (IGA)", "text": "We are looking at intrinsic averages1 (IGA) on the Grassmannian. < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p. \"p > p > p > p > p > p > p > p > p > p > p."}, {"heading": "2.3 Principal Components as Grassmann Averages", "text": "According to Hauberg et al. [18] we represent linear dimensionality reduction as an average problem on the Grassmannian. We consider an intrinsic Grassman mean (IGA), i.e. an average using geodetic distance, which allows us to look at K > 1 dimensional sub-spaces. We then propose a linear sub-space supply and show that for the zero mean of the Gaussian data the expected IGA to Gr (Xi, D), i.e., expected K-dimensional sub-spaces, coincide with the first K principle components. Given {xi} Ni = 1, the algorithm used to calculate the IGA, the leading K-dimensional basic sub-space is sketched. 1: The IGA algorithm for calculating PCs input: {xi} Ni = 1, K > 0 Output: {v1, vK}."}, {"heading": "3 A Robust Online Linear Subspace Learning Algorithm", "text": "Let's show {X1, X2, XIGA leads to the robust PCA in the number of data received up to the total number of data received. (K < D be inside a regular geodesic ball of radius < p / 2 \u221a 2 s.t.) Let X1, X2, \u00b7 D be the corresponding orthonormal bases, i.e., Xi spans Xi, for all i. The FMe can be calculated by the following minimization: M + arg min M + 1 d (Xi, M) (8) With a slight misuse of notation, we use notationM * (M) to denote both the FM and the FMe (and their orthonormal basis). The FMe is robust as shown in [14], hence we call our estimator Robust IGA (RoIGA). In the following theorem, we will prove that RoIGA leads to the robust PCA in the limitation of the number of data received."}, {"heading": "4 Experimental Results", "text": "In this section, we present an experimental evaluation of the proposed estimators on both real and synthetic data. Our general experimental results are that the RIGA and RRIGA estimators are more accurate and faster than other linear and robust linear estimators. We believe that the higher accuracy in RIGA and RRIGA is due to the use of the intrinsic geometry of the Grass Man in our geometric formulation. Specifically, finding the complete set of PCs is considered an intrinsic mean problem at the Grass Mannium, which was achieved in both cases with a recursive estimator. From a computational perspective, we attribute the efficiency observed in the experiments in the Online PCA case to the fact that RIGA is an optimization and parameter-free method. In the case of RRIGA, the reasons for accuracy and efficiency are much more complicated. At this point, we will speculate why our geometric formulation leads us to find the spaces directly opposed to a primitive adjustment based on a RIGA."}, {"heading": "4.1 Online Linear Subspace Estimation", "text": "Very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very, very high, very, very high, very, very high, very, very high, very high, very high, very high, very high, very, very high, very high, very, very high, very, very high, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very high, very, very high, very high, very, very high, very, very, very high, very high, very, very, very high, very, very, very high, very, very high, very high, very, very, very, very, very high, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very, very"}, {"heading": "4.2 Robust Subspace Estimation", "text": "In fact, it is a purely intellectual game, which seeks to put people's interests first, not to put them centre stage."}, {"heading": "5 Conclusions", "text": "In this paper, we present a new geometric framework for estimating the complete set of major components from given data. We present two online algorithms, one each for estimating PCA and RPCA. Since they are naturally online, they are naturally scalable to very large sets of data, as shown in the Experimental Results section. The key idea in the geometric framework is to calculate an intrinsic Grassmann average as a substitute for the main linear subspace. We show that when samples are taken from a Gaussian distribution, the intrinsic Grassmann average is expected to correspond to the main breed space. Furthermore, for our online recursive RPCA algorithm, we have proven that the estimated major components are statistically robust. Our algorithms have a linear time complexity and linear convergence rate. Unlike most other online algorithms, there are no incremental or other parameters that will focus on our future work on adapting to the geometric property, which is extremely useful."}, {"heading": "6 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Riemannian Geometry of the Grassmannian", "text": "& & # 223; e & # 223; e & # 246; & # 223; e & # 246; & # 223; e & # 246; & # 223; e & # 246; & # 223; e & # 246; & # 223; e & # 246; e & # 246; e & # 246; e & # 246; e & # 223; e K (K & # 246; & # 223; K & # 246; & # 246; e & # 246; & # 246; & # 246; & # 223;. & # 8222; K & # 8222; K & # 822; 822; 822; 822; 8K # 822; 822; 8K; 822; 822; 8K # 2K # 2K; 2K # 2K # 2K; 2K # 2K; 2K # 2K # 222; 822; 822; 8K # 2K; 2K # 2K; 2K # 2K; 2K # 2K; 2K # 2K # 222; 822; 822"}, {"heading": "6.2 Proof of Theorem 2", "text": "Theorem 2. (Convergence Rate) Let X1,.., XN be the samples taken from a distribution P (X). Then Equation (5) in the main paper has a linear convergence rate. Proof. Let X1,.., XN be the samples taken from a distribution P (X) to Gr (K, D). Letter M is the IGA of {Xi}. Then, using the triangular inequality d (Mk \u2212 1, M) \u2264 d (Mk \u2212 1, Mk) + d (Mk \u2212 1, M) + d (Mk \u2212 1, M) \u2264 1 k (Mk \u2212 1, M) + d (Xk \u2212 1, M) + d (Xk \u2212 1, M) + d (Mk \u2212 sic \u2212 1, M) and f (Mk \u2212 1, Msic \u2212 M), Mk \u2212 and Mk \u2212 M \u2212 since \u2212 M)."}], "references": [{"title": "Handbook of mathematical functions", "author": ["M. Abramowitz", "I.A. Stegun"], "venue": "Applied mathematics series,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1966}, {"title": "Riemannian geometry of grassmann manifolds with a view on algorithmic computation", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": "Acta Applicandae Mathematicae, 80(2):199\u2013220,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Riemannian lp center of mass: Existence, uniqueness, and convexity", "author": ["B. Afsari"], "venue": "Proceedings of the American Mathematical Society, 139(2):655\u2013673,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Online identification and tracking of subspaces from highly incomplete information", "author": ["L. Balzano", "R. Nowak", "B. Recht"], "venue": "Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on, pages 704\u2013711. IEEE,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic gradient descent on riemannian manifolds", "author": ["S. Bonnabel"], "venue": "IEEE Transactions on Automatic Control, 58(9):2217\u20132229,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Online principal components analysis", "author": ["C. Boutsidis", "D. Garber", "Z. Karnin", "E. Liberty"], "venue": "Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 887\u2013901. SIAM,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Convex optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge university press,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Robust principal component analysis", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Online expectation-maximisation", "author": ["O. Capp\u00e9"], "venue": "Mixtures: Estimation and Applications, pages 31\u201353,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Recursive frechet mean computation on the grassmannian and its applications to computer vision", "author": ["R. Chakraborty", "B.C. Vemuri"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 4229\u20134237,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Statistics on Special Manifolds", "author": ["Y. Chikuse"], "venue": "Springer, February", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Robust pca in high-dimension: A deterministic approach", "author": ["J. Feng", "H. Xu", "S. Yan"], "venue": "arXiv preprint arXiv:1206.4628,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Online robust pca via stochastic optimization", "author": ["J. Feng", "H. Xu", "S. Yan"], "venue": "Advances in Neural Information Processing Systems, pages 404\u2013412,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "The geometric median on riemannian manifolds with application to robust atlas estimation", "author": ["P.T. Fletcher", "S. Venkatasubramanian", "S. Joshi"], "venue": "NeuroImage, 45(1):S143\u2013S152,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Les \u00e9l\u00e9ments al\u00e9atoires de nature quelconque dans un espace distanci\u00e9", "author": ["M. Fr\u00e9chet"], "venue": "Annales de l\u2019institut Henri Poincar\u00e9, volume 10, pages 215\u2013310. Presses universitaires de France,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1948}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 23(6):643\u2013660,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Robust pca with compressed data", "author": ["W. Ha", "R.F. Barber"], "venue": "Advances in Neural Information Processing Systems, pages 1936\u20131944,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Scalable robust principal component analysis using grassmann averages", "author": ["S. Hauberg", "A. Feragen", "R. Enficiaud", "M.J. Black"], "venue": "TPAMI,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Incremental gradient on the grassmannian for online foreground and background separation in subsampled video", "author": ["J. He", "L. Balzano", "A. Szlam"], "venue": "CVPR, pages 1568\u20131575,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis of a complex of statistical variables into principal components", "author": ["H. Hotelling"], "venue": "Journal of educational psychology, 24(6):417,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1933}, {"title": "Robust statistics", "author": ["P.J. Huber"], "venue": "Springer,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust dictionary learning with capped l 1-norm", "author": ["W. Jiang", "F. Nie", "H. Huang"], "venue": "Proceedings of the 24th International Conference on Artificial Intelligence, pages 3590\u2013 3596. AAAI Press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Riemannian center of mass and mollifier smoothing", "author": ["H. Karcher"], "venue": "Communications on pure and applied mathematics, 30(5):509\u2013541,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1977}, {"title": "Probability, convexity, and harmonic maps with small image i: uniqueness and fine existence", "author": ["W.S. Kendall"], "venue": "Proceedings of the London Mathematical Society, 3(2):371\u2013406,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "Anomaly detection in crowded scenes", "author": ["V. Mahadevan", "W. Li", "V. Bhalodia", "N. Vasconcelos"], "venue": "CVPR, volume 249, page 250,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "The m-distribution-a general formula of intensity distribution of rapid fading", "author": ["M. Nakagami"], "venue": "Statistical Method of Radio Propagation,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1960}, {"title": "Simplified neuron model as a principal component analyzer", "author": ["E. Oja"], "venue": "Journal of mathematical biology, 15(3):267\u2013273,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1982}, {"title": "On lines and planes of closest fit to system of points in space", "author": ["K. Pearson"], "venue": "Philosophical Magazine, 2(11):559\u2013572,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1901}, {"title": "Intrinsic statistics on riemannian manifolds: Basic tools for geometric measurements", "author": ["X. Pennec"], "venue": "Journal of Mathematical Imaging and Vision, 25(1):127\u2013154,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Dyna: A model of dynamic human shape in motion", "author": ["G. Pons-Moll", "J. Romero", "N. Mahmood", "M.J. Black"], "venue": "SIGGRAPH, 34(4):120:1\u2013120:14, Aug.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "The umbral calculus", "author": ["S. Roman"], "venue": "Springer,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "EM algorithms for pca and spca", "author": ["S. Roweis"], "venue": "Advances in neural information processing systems, pages 626\u2013632,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1998}, {"title": "Probabilistic principal component analysis", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611\u2013622,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1999}, {"title": "Sectional curvatures of grassmann manifolds", "author": ["Y.-C. Wong"], "venue": "Proceedings of the National Academy of Sciences, 60(1):75\u201379,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1968}, {"title": "Online robust principal component analysis for background subtraction: A system evaluation on toyota car data", "author": ["X. Xu"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}], "referenceMentions": [{"referenceID": 27, "context": "Principal component analysis (PCA), a key work-horse of machine learning, can be derived in many ways: Pearson [29] proposed to find the subspace that minimizes the projection error of the observed data; Hotelling [20] instead sought the subspace in which the projected data has maximal variance; and Tipping & Bishop [34] consider a probabilistic formulation where the covariance of normally distributed data is predominantly given by a low-rank matrix.", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "Principal component analysis (PCA), a key work-horse of machine learning, can be derived in many ways: Pearson [29] proposed to find the subspace that minimizes the projection error of the observed data; Hotelling [20] instead sought the subspace in which the projected data has maximal variance; and Tipping & Bishop [34] consider a probabilistic formulation where the covariance of normally distributed data is predominantly given by a low-rank matrix.", "startOffset": 214, "endOffset": 218}, {"referenceID": 32, "context": "Principal component analysis (PCA), a key work-horse of machine learning, can be derived in many ways: Pearson [29] proposed to find the subspace that minimizes the projection error of the observed data; Hotelling [20] instead sought the subspace in which the projected data has maximal variance; and Tipping & Bishop [34] consider a probabilistic formulation where the covariance of normally distributed data is predominantly given by a low-rank matrix.", "startOffset": 318, "endOffset": 322}, {"referenceID": 17, "context": "[18] noted that the average of all one-dimensional subspaces spanned by normally distributed data coincides with the leading principal component.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "There are several existing approaches in literature that tackle the online PCA and the online Robust PCA problems and we discuss some of these approaches here: Oja\u2019s rule [28] is a classic online estimator for the leading principal components of a dataset.", "startOffset": 171, "endOffset": 175}, {"referenceID": 31, "context": "EM-PCA [33] is usually derived for probabilistic PCA, but is easily be adapted to the online setting [9].", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "EM-PCA [33] is usually derived for probabilistic PCA, but is easily be adapted to the online setting [9].", "startOffset": 101, "endOffset": 104}, {"referenceID": 3, "context": "GROUSE and GRASTA [4, 19] are online PCA and matrix completion algorithms.", "startOffset": 18, "endOffset": 25}, {"referenceID": 18, "context": "GROUSE and GRASTA [4, 19] are online PCA and matrix completion algorithms.", "startOffset": 18, "endOffset": 25}, {"referenceID": 6, "context": "This optimization is carried out in an alternating fashion using the well known ADMM [7] for estimating a set of parameters involving the weights, the sparse outlier vector and the dual vector in the augmented Lagrangian framework.", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "Recursive covariance estimation [6] is straight-forward, and the principal components can be extracted via standard eigen-decompositions.", "startOffset": 32, "endOffset": 35}, {"referenceID": 5, "context": "[6] consider", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "In [8], Candes et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 16, "context": "In another recent work, Ha and Barber [17] proposed an online RPCA algorithm when X = (L+ S)C where C is a data compression matrix.", "startOffset": 38, "endOffset": 42}, {"referenceID": 12, "context": "[13] solved RPCA using a stochastic optimization approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Though their algorithm is online, the optimization steps ( Algorithm 1 in [13]) are expensive for high-dimensional data.", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "[12] proposed a deterministic approach to solve RPCA (dubbed DHR-PCA) for high-dimensional data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Finally, we would like to refer the readers to an excellent source of references on RPCA in a recent MS thesis [36].", "startOffset": 111, "endOffset": 115}, {"referenceID": 17, "context": "[18], who recently showed that for a data set drawn from a zero-mean multivariate Gaussian distribution, the average subspace spanned by the data coincides with the leading principal component.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "As the Grassmannian is geodesically complete, one can extend the geodesics on the Grassmannian indefinitely [2, 11].", "startOffset": 108, "endOffset": 115}, {"referenceID": 10, "context": "As the Grassmannian is geodesically complete, one can extend the geodesics on the Grassmannian indefinitely [2, 11].", "startOffset": 108, "endOffset": 115}, {"referenceID": 0, "context": "Given X ,Y \u2208 Gr(K,D), with their respective orthonormal basis X and Y , the unique geodesic \u0393X : [0, 1]\u2192 Gr(K,D) between X and Y is given by: \u0393X (t) = span ( XV\u0302 cos(\u0398t) + \u00db sin(\u0398t) ) (3)", "startOffset": 97, "endOffset": 103}, {"referenceID": 23, "context": "Then, we call this ball \u201cregular\u201d [24] if 2r \u221a \u03ba < \u03c0.", "startOffset": 34, "endOffset": 38}, {"referenceID": 33, "context": "Using the results in [35], we know that, for RP withD \u2265 2, \u03ba = 1, while for generalGr(K,D) with min(K,D) \u2265 2, 0 \u2264 \u03ba \u2264 2.", "startOffset": 21, "endOffset": 25}, {"referenceID": 2, "context": "On Gr(K,D), IGA exists and is unique if the support of P (X ) is within a \u201cregular geodesic ball\u201d of radius < \u03c0/22 [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 28, "context": "The IGA may be computed using a Riemannian steepest descent, but this is computationally expensive and requires selecting a suitable step-size [30].", "startOffset": 143, "endOffset": 147}, {"referenceID": 9, "context": "[10] proposed a simple and efficient inductive (intrinsic) mean estimator:", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "(Weak Consistency [10]) Let X1, .", "startOffset": 18, "endOffset": 22}, {"referenceID": 22, "context": "1These are also known as Fr\u00e9chet means [23, 15].", "startOffset": 39, "endOffset": 47}, {"referenceID": 14, "context": "1These are also known as Fr\u00e9chet means [23, 15].", "startOffset": 39, "endOffset": 47}, {"referenceID": 17, "context": "[18] we pose the linear dimensionality reduction as an averaging problem on the Grassmannian.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "5 [23], which serves as the principal subspace.", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "As arccos is a decreasing function and a bijection on [0, 1], we can write an alternative form of Eq.", "startOffset": 54, "endOffset": 60}, {"referenceID": 13, "context": "The FMe is robust as was shown in [14], hence we call our estimator Robust IGA (RoIGA).", "startOffset": 34, "endOffset": 38}, {"referenceID": 4, "context": "We refer the readers to [5] for the consistency proof of the estimator.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "Since arccos is a decreasing function and is a bijection on [0, 1], we can rewrite Eq.", "startOffset": 60, "endOffset": 66}, {"referenceID": 25, "context": "agami distribution [27].", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "tion [21] of \u03c1 is proportional to \u03c8(m) , \u2202E[ \u221a\u2211K j=1((Si)jj) 2] \u2202m and if we can show that limm\u2192\u221e \u03c8(m) = 0, then we can claim that our objective function in Eq.", "startOffset": 5, "endOffset": 9}, {"referenceID": 20, "context": "10 is robust [21].", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "5) \u0393(m)2 , where \u03c6 is the polygamma function [1] of order 0.", "startOffset": 45, "endOffset": 48}, {"referenceID": 30, "context": "Here, {Bk} are the Bernoulli numbers of the second kind [32].", "startOffset": 56, "endOffset": 60}, {"referenceID": 8, "context": "For EM-PCA we follow the recommendations from Capp\u00e9 [9] and use step-sizes \u03b3t = 1/t with \u03b1-values between 0.", "startOffset": 52, "endOffset": 55}, {"referenceID": 0, "context": "Expressed Variance = \u2211K k=1 \u2211N n=1(x T nv (est) k ) 2 \u2211K k=1 \u2211N n=1(x T nv (true) k ) 2 \u2208 [0, 1].", "startOffset": 90, "endOffset": 96}, {"referenceID": 29, "context": "As an example, we consider a large collection of threedimensional scans of human body shape [31].", "startOffset": 92, "endOffset": 96}, {"referenceID": 24, "context": "We have used UCSD anomaly detection database [26] and Extended YaleB database [16].", "startOffset": 45, "endOffset": 49}, {"referenceID": 15, "context": "We have used UCSD anomaly detection database [26] and Extended YaleB database [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 21, "context": "Due to varying lighting condition, some of the face images are shaded/ dark and appeared as outliers (this experimental setup is similar to the one in [22]).", "startOffset": 151, "endOffset": 155}, {"referenceID": 0, "context": "Given X ,Y \u2208 Gr(K,D), with their respective orthonormal basis X and Y , the unique geodesic \u0393X : [0, 1]\u2192 Gr(K,D) between X and Y is given by: \u0393X (t) = span ( XV\u0302 cos(\u0398t) + \u00db sin(\u0398t) ) (13)", "startOffset": 97, "endOffset": 103}], "year": 2017, "abstractText": "Principal Component Analysis (PCA) is a fundamental method for estimating a linear subspace approximation to high-dimensional data. Many algorithms exist in literature to achieve a statistically robust version of PCA called RPCA. In this paper, we present a geometric framework for computing the principal linear subspaces in both situations that amounts to computing the intrinsic average on the space of all subspaces (the Grassmann manifold). Points on this manifold are defined as the subspaces spanned by K-tuples of observations. We show that the intrinsic Grassmann average of these subspaces coincide with the principal components of the observations when they are drawn from a Gaussian distribution. Similar results are also shown to hold for the RPCA. Further, we propose an efficient online algorithm to do subspace averaging which is of linear complexity in terms of number of samples and has a linear convergence rate. When the data has outliers, our proposed online robust subspace averaging algorithm shows significant performance (accuracy and computation time) gain over a recently published RPCA methods with publicly accessible code. We have demonstrated competitive performance of our proposed online subspace algorithm method on one synthetic and two real data sets. Experimental results depicting stability of our proposed method are also presented. Furthermore, on two real outlier corrupted datasets, we present comparison experiments showing lower reconstruction error using our online RPCA algorithm. In terms of reconstruction error and time required, both our algorithms outperform the competition.", "creator": "LaTeX with hyperref package"}}}