{"id": "1702.01101", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2017", "title": "Multilingual Multi-modal Embeddings for Natural Language Processing", "abstract": "We propose a novel discriminative model that learns embeddings from multilingual and multi-modal data, meaning that our model can take advantage of images and descriptions in multiple languages to improve embedding quality. To that end, we introduce a modification of a pairwise contrastive estimation optimisation function as our training objective. We evaluate our embeddings on an image-sentence ranking (ISR), a semantic textual similarity (STS), and a neural machine translation (NMT) task. We find that the additional multilingual signals lead to improvements on both the ISR and STS tasks, and the discriminative cost can also be used in re-ranking $n$-best lists produced by NMT models, yielding strong improvements.", "histories": [["v1", "Fri, 3 Feb 2017 18:19:47 GMT  (25kb,D)", "http://arxiv.org/abs/1702.01101v1", "4 pages (5 including references), no figures"]], "COMMENTS": "4 pages (5 including references), no figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["iacer calixto", "qun liu", "nick campbell"], "accepted": false, "id": "1702.01101"}, "pdf": {"name": "1702.01101.pdf", "metadata": {"source": "CRF", "title": "Multilingual Multi-modal Embeddings for Natural Language Processing", "authors": ["Iacer Calixto", "Qun Liu"], "emails": ["iacer.calixto@adaptcentre.ie"], "sections": [{"heading": "1 Introduction", "text": "In this work, we expand the idea of training multimodal embedding (Kiros et al., 2014; Socher et al., 2014) and introduce a model that can be trained not only on images and their monolingual descriptions, but also on additional multilingual image descriptions, if available. We believe that multiple descriptions of an image, regardless of its language, are likely to increase the scope and variability of the ideas described in the image, which can lead to a better generalization of the scene semantics depicted. In addition, a similar description expressed in different languages can differ in a subtle but meaningful way.To this end, we introduce a novel training target function that uses noise-contrasting assessments adapted to the case of three or more input sources, i.e. an image and multilingual tendencies (\u00a7 2) that links our objective function NISR with an arbitrary number of languages and multilingual sentences in any number of languages (we use the idea and \u00a7 3 in our experiments)."}, {"heading": "2 Multilingual and multi-modal embeddings", "text": "We have two main components: a textual and a visual component for each sentence we have made our own. - We have the different languages Lk, k, K, K, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S,"}, {"heading": "3 Datasets", "text": "The original Flickr30k dataset contains 30k images and 5 English sentence descriptions for each image (Young et al., 2014). To train the NMT model in Table 2, we use the translated Multi30k, future M30kT, with each of the 30k images in the original Flickr30k translating one of its English descriptions manually into German by a professional translator. Training, validation and test kits contain 29k, 1014 and 1k images, each accompanied by a translated sentence pair in English and German. In all other experiments in \u00a7 5, we use the comparable Multi30k, future M30kC, an extension of the Flickr30k, where 5 German descriptions for each image in the original Flickr30k were collected independently of the English descriptions (Elliott et al., 2016). Training, validation and test kits contain 29k, each accompanied by 5 English and 5 German sentences. Since the M36 sentences for the first validation are not available in the entire 22,000 set, we will use the first 3k for the validation."}, {"heading": "4 Experiments", "text": "All non-recurring matrices are initialized by scanning Gaussian, N (0, 0.01), recurring matrices are random orthogonal and bias vectors, which are all initialized to zero. We apply dropouts (Srivastava et al., 2014) with a probability of 0.5 in both text and image representations, which in turn are mapped to a multimodal embedding space of 2048D. We set the margin \u03b1 = 0.2. Our models are trained with stochastic gradient descent with Adam (Kingma and Ba, 2015) with minibatches of 128 instances."}, {"heading": "5 Results", "text": "In fact, most of us are able to put ourselves at the top, \"he said in an interview with the German Press Agency.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "6 Conclusions", "text": "We propose a new discrimination model that shows both multilingual and multimodal similarities and introduces a modified noisecontrastive estimation function to optimize our model, which shows promising results in three different tasks. Results obtained with the recently published Multi30k dataset show that our model can learn meaningful multimodal embedding, effectively utilizes multilingual signals, and performs consistently better than a comparable monolingual model. In the future, we will train our model in a multilingual setting, with images and descriptions in ten languages. Finally, we would like to explore in more detail how these models can be used in the NMT."}], "references": [{"title": "Semeval-2014 task 10: Multilingual semantic textual similarity", "author": ["Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Rada Mihalcea", "German Rigau", "Janyce Wiebe."], "venue": "Proceedings of the", "citeRegEx": "Agirre et al\\.,? 2014", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations, ICLR 2015. San Diego, California.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Colin Cherry", "George Foster."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Cherry and Foster.,? 2012", "shortCiteRegEx": "Cherry and Foster.", "year": 2012}, {"title": "Learning Phrase Representations using RNN Encoder\u2013 Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "In", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["Koby Crammer", "Yoram Singer."], "venue": "J. Mach. Learn. Res. 3:951\u2013991. https://doi.org/10.1162/jmlr.2003.3.4-5.951.", "citeRegEx": "Crammer and Singer.,? 2003", "shortCiteRegEx": "Crammer and Singer.", "year": 2003}, {"title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation. Baltimore, Maryland, USA, pages 376\u2013380.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Multi30K: Multilingual English-German Image Descriptions", "author": ["Desmond Elliott", "Stella Frank", "Khalil Sima\u2019an", "Lucia Specia"], "venue": "In Proceedings of the 5th Workshop on Vision", "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "International Conference on Learning Representations, ICLR 2015. San Diego, California.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel."], "venue": "CoRR abs/1411.2539. http://arxiv.org/abs/1411.2539.", "citeRegEx": "Kiros et al\\.,? 2014", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman."], "venue": "arXiv preprint arXiv:1409.1556 .", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."], "venue": "In Proceedings of Association for Machine Translation in the Americas. Cambridge, MA, pages", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Karpathy Andrej", "Q Le", "Chris Manning", "Andrew Ng."], "venue": "Transactions of the Association for Computational Linguistics 2014.", "citeRegEx": "Socher et al\\.,? 2014", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15:1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguis-", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "In this work, we expand on the idea of training multi-modal embeddings (Kiros et al., 2014; Socher et al., 2014) and introduce a model that can be trained not only on images and their monolingual descriptions but also on additional multilingual image descriptions when these are available.", "startOffset": 71, "endOffset": 112}, {"referenceID": 13, "context": "In this work, we expand on the idea of training multi-modal embeddings (Kiros et al., 2014; Socher et al., 2014) and introduce a model that can be trained not only on images and their monolingual descriptions but also on additional multilingual image descriptions when these are available.", "startOffset": 71, "endOffset": 112}, {"referenceID": 3, "context": "In the textual component, we have K different languages Lk, k \u2208 K, and for each language we use a recurrent neural network (RNN) with gated recurrent units (GRU) (Cho et al., 2014) as a sentence encoder.", "startOffset": 162, "endOffset": 180}, {"referenceID": 11, "context": "Simonyan and Zisserman (2014) trained deep convolutional neural network (CNN) models", "startOffset": 0, "endOffset": 30}, {"referenceID": 10, "context": "for classifying images into one out of 1000 ImageNet classes (Russakovsky et al., 2015).", "startOffset": 61, "endOffset": 87}, {"referenceID": 11, "context": "use the activations of the penultimate fully connected layer FC7, network configuration E, as our image feature vector (Simonyan and Zisserman, 2014).", "startOffset": 119, "endOffset": 149}, {"referenceID": 8, "context": "If the number of languages K = 1 and \u03b2 = 1, our model computes the Visual Semantic Embedding (VSE) of Kiros et al. (2014).", "startOffset": 102, "endOffset": 122}, {"referenceID": 8, "context": "Table 1: Monolingual baseline (VSE) of Kiros et al. (2014) and our MLMME model on the M30kC test set.", "startOffset": 39, "endOffset": 59}, {"referenceID": 15, "context": "The original Flickr30k data set contains 30k images and 5 English sentence descriptions for each image (Young et al., 2014).", "startOffset": 103, "endOffset": 123}, {"referenceID": 6, "context": "collected for each image in the original Flickr30k independently from the English descriptions (Elliott et al., 2016).", "startOffset": 95, "endOffset": 117}, {"referenceID": 14, "context": "We apply dropout (Srivastava et al., 2014) with a probability of 0.", "startOffset": 17, "endOffset": 42}, {"referenceID": 7, "context": "Our models are trained using stochastic gradient descent with Adam (Kingma and Ba, 2015) with minibatches of 128 instances.", "startOffset": 67, "endOffset": 88}, {"referenceID": 8, "context": "As our main baseline, we retrain Kiros et al. (2014) monolingual models separately on English and German sentences (+images).", "startOffset": 33, "endOffset": 53}, {"referenceID": 8, "context": "Image\u2194Sentence Ranking In Table 1, we show results for the monolingual VSE English and German models of Kiros et al. (2014) and our MLMME models on the M30kC data set and evaluated on images and bilingual sentences.", "startOffset": 104, "endOffset": 124}, {"referenceID": 8, "context": "2) and median rank\u2014mrank in English reduced from 8 to 5 and in German from 11 to 6 in comparison to the best model by Kiros et al. (2014). Nevertheless, when ranking sentences given images, results are unclear.", "startOffset": 118, "endOffset": 138}, {"referenceID": 0, "context": "2014) and 2015 (Agirre et al., 2015). In Table 2, we note that our MLMME model consistently improves on the monolingual baseline of Kiros et al. (2014) in the two in-domain similarity tasks, remaining competitive even compared to the best comparable SemEval model (differences <10%).", "startOffset": 16, "endOffset": 152}, {"referenceID": 1, "context": "of experiments, we train the attention-based model of Bahdanau et al. (2015)1 on the M30kT training set to translate from English into German.", "startOffset": 54, "endOffset": 77}, {"referenceID": 4, "context": "We then train an n-best list re-ranker on the M30kT validation set\u2019s 20best lists with k-best MIRA (Crammer and Singer, 2003; Cherry and Foster, 2012), and use the new distances as additional features to the original MT log-likelihood p(Y |X).", "startOffset": 99, "endOffset": 150}, {"referenceID": 2, "context": "We then train an n-best list re-ranker on the M30kT validation set\u2019s 20best lists with k-best MIRA (Crammer and Singer, 2003; Cherry and Foster, 2012), and use the new distances as additional features to the original MT log-likelihood p(Y |X).", "startOffset": 99, "endOffset": 150}, {"referenceID": 9, "context": "1 TER points (Papineni et al., 2002; Denkowski and Lavie, 2014; Snover et al., 2006).", "startOffset": 13, "endOffset": 84}, {"referenceID": 5, "context": "1 TER points (Papineni et al., 2002; Denkowski and Lavie, 2014; Snover et al., 2006).", "startOffset": 13, "endOffset": 84}, {"referenceID": 12, "context": "1 TER points (Papineni et al., 2002; Denkowski and Lavie, 2014; Snover et al., 2006).", "startOffset": 13, "endOffset": 84}], "year": 2017, "abstractText": "We propose a novel discriminative model that learns embeddings from multilingual and multi-modal data, meaning that our model can take advantage of images and descriptions in multiple languages to improve embedding quality. To that end, we introduce a modification of a pairwise contrastive estimation optimisation function as our training objective. We evaluate our embeddings on an image\u2013sentence ranking (ISR), a semantic textual similarity (STS), and a neural machine translation (NMT) task. We find that the additional multilingual signals lead to improvements on both the ISR and STS tasks, and the discriminative cost can also be used in re-ranking n-best lists produced by NMT models, yielding strong improvements.", "creator": "LaTeX with hyperref package"}}}