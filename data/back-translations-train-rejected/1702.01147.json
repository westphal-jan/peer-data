{"id": "1702.01147", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2017", "title": "Predicting Target Language CCG Supertags Improves Neural Machine Translation", "abstract": "Neural machine translation (NMT) models are able to partially learn syntactic information from sequential lexical information. Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. This work aims to answer two questions: 1) Does explicitly modeling source or target language syntax help NMT? 2) Is tight integration of words and syntax better than multitask training? We introduce syntactic information in the form of CCG supertags either in the source as an extra feature in the embedding, or in the target, by interleaving the target supertags with the word sequence. Our results on WMT data show that explicitly modeling syntax improves machine translation quality for English-German, a high-resource pair, and for English-Romanian, a low-resource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training.", "histories": [["v1", "Fri, 3 Feb 2017 20:31:34 GMT  (141kb,D)", "http://arxiv.org/abs/1702.01147v1", null], ["v2", "Tue, 18 Jul 2017 12:07:45 GMT  (226kb,D)", "http://arxiv.org/abs/1702.01147v2", "Accepted at the Second Conference on Machine Translation (WMT17). This version includes more results regarding target syntax for Romanian-&gt;English and reports fewer results regarding source syntax"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["maria nadejde", "siva reddy", "rico sennrich", "tomasz dwojak", "marcin junczys-dowmunt", "philipp koehn", "alexandra birch"], "accepted": false, "id": "1702.01147"}, "pdf": {"name": "1702.01147.pdf", "metadata": {"source": "CRF", "title": "Syntax-aware Neural Machine Translation Using CCG", "authors": ["Maria N\u0103dejde", "Siva Reddy", "Tomasz Dwojak", "Marcin Junczys-Dowmunt", "Alexandra Birch"], "emails": ["m.nadejde@sms.ed.ac.uk,", "rico.sennrich}@ed.ac.uk", "t.dwojak@amu.edu.pl,", "junczys@amu.edu.pl,", "phi@jhu.edu"], "sections": [{"heading": "1 Introduction", "text": "We have the ability to identify a variety of word pairs that are based on high quality, and some syntax is actually captured by these models. In a detailed analysis, we show that Bentivogli et al., Bentivogli et al., (2016) show that NMT is significantly improved using phrase-based SMT models, just as they can implicitly learn model phenomena underlying high quality word phenomena, and that results can be improved for longer sentences and complex syntactic phenomena, such as prepositional phrases (PP). Another study by Shi et al (2016) shows that the coding layer of NMT can be explicitly improved to include syntactic phrases and word strings."}, {"heading": "2 Related work", "text": "Syntax has helped to capture dependencies in statistical machine translation (SMT) between distant words that influence morphological concordance, sub-categorization, and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007). However, there has been some work in NMT to model source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) implicitly capture the hierarchical aspects of language using revolutionary neural networks. Sennrich and Haddow (2016) generalize the embedding layer of NMT implicitly or explicitly with linguistic features such as dependency relationships and sub-language tags, and we extend their work to include CCG supertags. Other architectures have suggested integrating source-side syntax, such as Erichi et al guet al."}, {"heading": "3 Modeling Syntax in NMT", "text": "It is a lexicalized formalism in which words are assigned with syntactic categories, i.e., supertags that indicate context-sensitive morphosyntactic properties of a word in a sentence. CCG's combinators allow supertags to capture global syntactic constraints locally. Although NMT captures long-term dependencies using long-term memories, short-term memory is cheap and reliable. What (S [wq] / NP) can help make the model more reliant on local information (short-term) and not rely heavily on long-term memories. Consider a decoder that needs to generate the following sentences: 1. What (S [wq] / NP) is the city where we can rely on local strategies? / NP is the Taj Mahal in? 2. WhereS [q] / NP is (S] / NP) is (S [S] / NP] / Taj Mahal."}, {"heading": "4 Experimental Setup and Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data and methods", "text": "We train the neural MT systems on all parallel data available at WMT16 (Bojar et al., 2016) for the German \u2194 English and Romanian \u2194 English language pairs. The English side of the parallel data is provided with lexical tags using EasySRL (Lewis et al., 2015). Some longer sentences cannot be processed by the parser and therefore we remove them from our training and test data. We report the record number for the filtered data sets in Table 1. During the training, we validate our models with BLEU on development sets: newstest2013 for German English and newsdev2016 for Romanian English. We evaluate the systems with BLEU (Papineni et al., 2002) and report on the results on newstestestestest2016 for both language pairs. All neural MT systems are attention-oriented encoder encoder encoder encoder networks (Bahineni et al., 2015) we will use the neural parameters similar to those implemented in Nematol.1"}, {"heading": "4.2 Results", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "4.3 Discussion", "text": "It would be interesting to evaluate the effects of the target syntax for such a language pair. In the future, we plan to use the Hindi CCGBank (Ambati et al., 2016) to conduct experiments in English \u2194 Hin.Although the focus of this paper is not on improving CCG tagging, we can measure that SNMT is accurate in predicting CCG supertags. We compare the CCG sequence predicted by the SNMT models with that of EasySRL and get the following accuracies: 93.2 for Romanian \u2192 English, 95.8 for German \u2192 English with both source and target syntax. This result could be useful when CCG supertagging, as the multilingual question is answered."}, {"heading": "5 Conclusions", "text": "Our results suggest that the idea of an explicit syntax, here in the form of CCG supertags, in the encoder or decoder improves machine translation for both English \u2194 German and English \u2194 Romanian language pairs. Previous work on syntax-conscious NMT mainly modelled the syntax in the encoder, while our results suggest that the syntax modelling in the decoder is also useful. Furthermore, by modelling the syntax in both the encoder and the decoder, we achieve the greatest improvement over the NMT base system, especially for longer sentences and syntactic phenomena such as prepositional attachment and coordination. Finally, our results show that close integration of the syntax in the decoder improves the decoupling of target words and syntax."}, {"heading": "A Appendix", "text": "In the second half of the last decade, when the US and the EU were in crisis, the situation in the US deteriorated considerably, both in the US and in the US, \"he said.\" The US is able to get the crisis under control. \"In the second half of the last decade, as well as in the second half of the last decade, when the global economic crisis in the US reached its peak, the world economy manoeuvred itself into recession.\" The US manoeuvred itself into recession. \"In the second half of the last decade, the world economy catapulted itself into recession.\" In the second half of the last decade, the world economy has strayed into recession, \"in the second half of the last decade,\" the world economy has catapulted into recession. \"In the second half of the decade, the world economy has catapulted itself into recession."}], "references": [{"title": "Hindi CCGbank: CCG Treebank from the Hindi Dependency Treebank", "author": ["Bharat Ram Ambati", "Tejaswini Deoskar", "Mark Steedman."], "venue": "Language Resources and Evaluation.", "citeRegEx": "Ambati et al\\.,? 2016", "shortCiteRegEx": "Ambati et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR)..", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Neural versus phrasebased machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "Ccg supertags in factored statistical machine translation", "author": ["Alexandra Birch", "Miles Osborne", "Philipp Koehn."], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation. Association for Computational Linguistics, Stroudsburg, PA, USA,", "citeRegEx": "Birch et al\\.,? 2007", "shortCiteRegEx": "Birch et al\\.", "year": 2007}, {"title": "Findings of the 2016 conference on machine translation", "author": ["Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."], "venue": "Proceedings of the First Conference on Machine Translation. Association for", "citeRegEx": "Post et al\\.,? 2016", "shortCiteRegEx": "Post et al\\.", "year": 2016}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Comput. Linguist. 33(2):201\u2013228.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statisti-", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "\u00c7a\u011flar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Tree-to-sequence attentional neural machine translation", "author": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of the 54th", "citeRegEx": "Eriguchi et al\\.,? 2016", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2016}, {"title": "What\u2019s in a translation rule", "author": ["Michel Galley", "Mark Hopkins", "Kevin Knight", "Daniel Marcu"], "venue": "In Proceedings of Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics", "citeRegEx": "Galley et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2004}, {"title": "Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang."], "venue": "Proceedings of the IWSLT 2016.", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Seattle, Washington, USA, pages", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Factored translation models", "author": ["Philipp Koehn", "Hieu Hoang."], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. pages 868\u2013876.", "citeRegEx": "Koehn and Hoang.,? 2007", "shortCiteRegEx": "Koehn and Hoang.", "year": 2007}, {"title": "Joint a* ccg parsing and semantic role labelling", "author": ["Mike Lewis", "Luheng He", "Luke Zettlemoyer."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Lewis et al\\.,? 2015", "shortCiteRegEx": "Lewis et al\\.", "year": 2015}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "Proceedings of International Conference on Learning Representations (ICLR 2016).", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Factored Neural Machine Translation Architectures", "author": ["Mercedes Garc\u0131\u0301a Mart\u0131\u0301nez", "Lo\u0131\u0308c Barrault", "Fethi Bougares"], "venue": "In International Workshop on Spoken Language Translation (IWSLT\u201916)", "citeRegEx": "Mart\u0131\u0301nez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mart\u0131\u0301nez et al\\.", "year": 2016}, {"title": "Using dependency order templates to improve generality in translation", "author": ["Arul Menezes", "Chris Quirk."], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation. pages 1\u20138.", "citeRegEx": "Menezes and Quirk.,? 2007", "shortCiteRegEx": "Menezes and Quirk.", "year": 2007}, {"title": "Using factored word representation in neural network language models", "author": ["Jan Niehues", "Thanh-Le Ha", "Eunah Cho", "Alex Waibel."], "venue": "Proceedings of the First Conference on Machine Translation. Berlin, Germany.", "citeRegEx": "Niehues et al\\.,? 2016", "shortCiteRegEx": "Niehues et al\\.", "year": 2016}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Modelling and Optimizing on Syntactic N-Grams for Statistical Machine Translation", "author": ["Rico Sennrich."], "venue": "Transactions of the Association for Computational Linguistics 3:169\u2013182.", "citeRegEx": "Sennrich.,? 2015", "shortCiteRegEx": "Sennrich.", "year": 2015}, {"title": "Linguistic input features improve neural machine translation", "author": ["Rico Sennrich", "Barry Haddow."], "venue": "Proceedings of the First Conference on Machine Translation. Berlin, Germany, pages 83\u201391.", "citeRegEx": "Sennrich and Haddow.,? 2016", "shortCiteRegEx": "Sennrich and Haddow.", "year": 2016}, {"title": "Edinburgh neural machine translation systems for wmt 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics, Berlin, Germany, pages 371\u2013", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Does string-based neural mt learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "Association for Computational Linguistics, Austin, Texas, pages", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "The syntactic process, volume 24", "author": ["Mark Steedman."], "venue": "MIT Press.", "citeRegEx": "Steedman.,? 2000", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems. NIPS\u201914, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Ghkm rule extraction and scope-3 parsing in moses", "author": ["Philip Williams", "Philipp Koehn."], "venue": "Proceedings of the Seventh Workshop on Statistical Machine Translation. pages 388\u2013394.", "citeRegEx": "Williams and Koehn.,? 2012", "shortCiteRegEx": "Williams and Koehn.", "year": 2012}], "referenceMentions": [{"referenceID": 26, "context": "Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al.", "startOffset": 61, "endOffset": 127}, {"referenceID": 7, "context": "Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al.", "startOffset": 61, "endOffset": 127}, {"referenceID": 1, "context": "Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al.", "startOffset": 61, "endOffset": 127}, {"referenceID": 15, "context": "Recent work which incorporates additional linguistic information in NMT models (Luong et al., 2016; Sennrich and Haddow, 2016) show that even", "startOffset": 79, "endOffset": 126}, {"referenceID": 21, "context": "Recent work which incorporates additional linguistic information in NMT models (Luong et al., 2016; Sennrich and Haddow, 2016) show that even", "startOffset": 79, "endOffset": 126}, {"referenceID": 20, "context": "mation in the source, as an extra feature in the embedding layer following the approach of Sennrich and Haddow (2016). We also propose a method for generating syntactic information in the target: tightly coupling words and syntax by interleav-", "startOffset": 91, "endOffset": 118}, {"referenceID": 15, "context": "We compare this to loosely coupling words and syntax using multitask solutions, where the shared parts of the model are trained to produce either a target sequence of words or supertags in a similar fashion to Luong et al. (2016).", "startOffset": 210, "endOffset": 230}, {"referenceID": 25, "context": "We use CCG syntactic categories (Steedman, 2000), also known as supertags, to represent syntax explicitly.", "startOffset": 32, "endOffset": 48}, {"referenceID": 9, "context": "Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007).", "startOffset": 175, "endOffset": 277}, {"referenceID": 17, "context": "Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007).", "startOffset": 175, "endOffset": 277}, {"referenceID": 27, "context": "Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007).", "startOffset": 175, "endOffset": 277}, {"referenceID": 20, "context": "Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007).", "startOffset": 175, "endOffset": 277}, {"referenceID": 5, "context": "Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007).", "startOffset": 175, "endOffset": 277}, {"referenceID": 5, "context": ", 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al.", "startOffset": 75, "endOffset": 211}, {"referenceID": 5, "context": ", 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Sennrich, 2015; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks.", "startOffset": 75, "endOffset": 231}, {"referenceID": 8, "context": "proposed to integrate source-side syntax such as Eriguchi et al. (2016) who use the phrase structure of the source sentence to guide the recurrence and attention model in a tree-to-sequence model.", "startOffset": 49, "endOffset": 72}, {"referenceID": 8, "context": "proposed to integrate source-side syntax such as Eriguchi et al. (2016) who use the phrase structure of the source sentence to guide the recurrence and attention model in a tree-to-sequence model. Luong et al. (2016) co-train a translation model and a syntactic parser which share the encoder.", "startOffset": 49, "endOffset": 217}, {"referenceID": 17, "context": "Niehues et al. (2016) proposed a factored RNN-based language model for re-scoring an nbest list produced by a phrase-based MT system.", "startOffset": 0, "endOffset": 22}, {"referenceID": 16, "context": "In recent work Mart\u0131\u0301nez et al. (2016) implemented an NMT model which first generated lemmas and morphology, and then used these to generate the word form.", "startOffset": 15, "endOffset": 39}, {"referenceID": 3, "context": "Previous work on integrating CCG supertags in factored phrase-based models (Birch et al., 2007) showed promising results.", "startOffset": 75, "endOffset": 95}, {"referenceID": 13, "context": "factored models originally proposed for statistical machine translation (Koehn and Hoang, 2007) suffered from data sparsity and did not consider longer sequences as context.", "startOffset": 72, "endOffset": 95}, {"referenceID": 21, "context": "Source-side syntax When modeling the sourceside syntactic information, we include the CCG supertags as extra features in the NMT encoder using the framework of (Sennrich and Haddow, 2016).", "startOffset": 160, "endOffset": 187}, {"referenceID": 1, "context": "The model of (Bahdanau et al., 2015) is extended by learning a separate embedding for several source-side features such as the word itself or its part-of-speech.", "startOffset": 13, "endOffset": 36}, {"referenceID": 23, "context": "The baseline features are the subword units obtained using byte-pair-encoding (BPE, (Sennrich et al., 2016b)) together with the annotation of the subword structure using IOB format by marking if", "startOffset": 84, "endOffset": 108}, {"referenceID": 15, "context": "In the multitask framework (Luong et al., 2016) the encoder part is shared while the decoder is different for each of the prediction tasks: translation and tagging.", "startOffset": 27, "endOffset": 47}, {"referenceID": 14, "context": "The English side of the parallel data is annotated with CCG lexical tags using EasySRL (Lewis et al., 2015).", "startOffset": 87, "endOffset": 107}, {"referenceID": 19, "context": "We evaluate the systems using BLEU (Papineni et al., 2002) and report results on newstest2016 for both language pairs.", "startOffset": 35, "endOffset": 58}, {"referenceID": 1, "context": "All the neural MT systems are attentional encoder-decoder networks (Bahdanau et al., 2015) as implemented in the Nematus toolkit.", "startOffset": 67, "endOffset": 90}, {"referenceID": 22, "context": "1 We use similar hyper-parameters to those reported by (Sennrich et al., 2016a) with minor modifications:", "startOffset": 55, "endOffset": 79}, {"referenceID": 12, "context": "we used mini-batches of size 60 and Adam optimizer (Kingma and Ba, 2014).", "startOffset": 51, "endOffset": 72}, {"referenceID": 23, "context": "(Sennrich et al., 2016b), resulting in a vocabulary size of 85,000.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "In this section we evaluate whether our syntaxaware NMT model (SNMT) with source-side and target-side CCG supertags improves translation quality as compared to a baseline NMT model (Bahdanau et al., 2015; Sennrich et al., 2016a).", "startOffset": 181, "endOffset": 228}, {"referenceID": 22, "context": "In this section we evaluate whether our syntaxaware NMT model (SNMT) with source-side and target-side CCG supertags improves translation quality as compared to a baseline NMT model (Bahdanau et al., 2015; Sennrich et al., 2016a).", "startOffset": 181, "endOffset": 228}, {"referenceID": 0, "context": "In the future we plan to use the Hindi CCGBank (Ambati et al., 2016) to run experiments for English\u2194Hindi.", "startOffset": 47, "endOffset": 68}], "year": 2017, "abstractText": "Neural machine translation (NMT) models are able to partially learn syntactic information from sequential lexical information. Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. This work aims to answer two questions: 1) Does explicitly modeling source or target language syntax help NMT? 2) Is tight integration of words and syntax better than multitask training? We introduce syntactic information in the form of CCG supertags either in the source as an extra feature in the embedding, or in the target, by interleaving the target supertags with the word sequence. Our results on WMT data show that explicitly modeling syntax improves machine translation quality for English\u2194German, a high-resource pair, and for English\u2194Romanian, a lowresource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training.", "creator": "LaTeX with hyperref package"}}}