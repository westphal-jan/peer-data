{"id": "1702.01293", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2017", "title": "Latent Hinge-Minimax Risk Minimization for Inference from a Small Number of Training Samples", "abstract": "Deep Learning (DL) methods show very good performance when trained on large, balanced data sets. However, many practical problems involve imbalanced data sets, or/and classes with a small number of training samples. The performance of DL methods as well as more traditional classifiers drops significantly in such settings. Most of the existing solutions for imbalanced problems focus on customizing the data for training. A more principled solution is to use mixed Hinge-Minimax risk [19] specifically designed to solve binary problems with imbalanced training sets. Here we propose a Latent Hinge Minimax (LHM) risk and a training algorithm that generalizes this paradigm to an ensemble of hyperplanes that can form arbitrary complex, piecewise linear boundaries. To extract good features, we combine LHM model with CNN via transfer learning. To solve multi-class problem we map pre-trained category-specific LHM classifiers to a multi-class neural network and adjust the weights with very fast tuning. LHM classifier enables the use of unlabeled data in its training and the mapping allows for multi-class inference, resulting in a classifier that performs better than alternatives when trained on a small number of training samples.", "histories": [["v1", "Sat, 4 Feb 2017 14:33:16 GMT  (399kb,D)", "http://arxiv.org/abs/1702.01293v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["dolev raviv", "margarita osadchy"], "accepted": false, "id": "1702.01293"}, "pdf": {"name": "1702.01293.pdf", "metadata": {"source": "CRF", "title": "Latent Hinge-Minimax Risk Minimization for Inference from a Small Number of Training Samples", "authors": ["Dolev Raviv", "Margarita Osadchy"], "emails": ["dolev.raviv@gmail.com", "rita@cs.haifa.ac.il"], "sections": [{"heading": "1. Introduction", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "2. Background", "text": "It has been shown that hinge losses [26, 29, 1, 2, 11] are mathematically attractive when there are only a relatively small number of training samples, so this approach could be used to measure positive class risk within unbalanced problems. Alternatively, the Minimax risk [16, 10] limits the distribution that instance-label examples generate in the world. This approach is mathematically attractive when there are (infinitely) many training examples, since it uses only their statistical properties such as mean and covariance. Consequently, it could be used as a negative class risk. In this work, we derive a mixed risk and an efficient training algorithm for a more general ensemble of hyperplanes. Our approach builds on the mixed risk for the intersection of K hyperplanes [19], briefly summarized in Section 2.1."}, {"heading": "2.1. K-hyperplane Hinge-Minmax Classifier", "text": "s leave wj, j = 1,.., K for K hyperplanes. Let us leave W for a K \u00b7 d matrix with wj as the jster column. Let us leave A-K hyperplane Hinge Minmax classifier (KHHM) for an intersection of positive hemispheres defined by these K hyperplanes. Let us leave X +, {x-X: y = 1}, and X \u2212, {x-X: y = 1) for the hyperplane. (X-X: y = 1) for the hyperplane."}, {"heading": "3. Latent Hinge-Minmax Classifier", "text": "To accommodate classes that form non-convex or disjunctural groups, we propose a new model, the latent hinge-minmax classifier (LHM) and a training scheme that simultaneously discovers the convex components of the positive class and learns the K hyperlane models that separate each convex component from the negative class. We define the LHM classifier as a union of intersections of positive half-spaces. We assume that each intersection consists of K hyperplanes: W i = [wi1,..., w i K] and there are C components in the Union. Let us define WLHM, (W 1,..., WC) the LHM model. Likewise, we can define the LHM classifier asfLHM (x, WLHM) = characters (max i, {1.. C} {min j, wij T x})."}, {"heading": "3.1. Latent Hinge-Minimax Risk", "text": "We extend the hinge minimax risk in Equation 1 to include multiple latent components and a hidden allocation variable. Specifically, we define a latent variable (x) = i, i, i, i, i, 0,.., C} for each sample (x, y). We define the LHM risk function as follows: LD (WLHM) = L M \u00b5, \u03a3 (WLHM) + L H D (WLHM), (2) whereas LM\u00b5 (WLHM) = Pr z (Z) = L (3) = L (WLHM) + L H D (WLHM) is reduced to the simplicity of notation."}, {"heading": "3.2. Empirical Risk", "text": "Each sample with positive labeling encounters a loss only in a single latent component, specified by its latent variable components (x) according to Equation 4. Thus, we define a single positive sample loss as follows: L (W\u0442 (x); x, 1, 1, 1 (x) = 1 m + (x) LMX - (W) + \u03bbLHX + (W); x) where LMX - (W\u0442 (x)) = supz \"Z (\u00b5, \u03a3) Pr (z\" Q i) is constant for all positive examples with the same mapping (mean and covariance are estimated from X \u2212) and LHX + (W); x) = \"(W\u0442 (x); x, 1). Let us leave Xi, {x\" X \"(X\" Q \"Q\") Pr (z \"Q\") +: (x) a subset of X +. The empirical risk of a latent component i aggregates the sample loss in all samples (X)."}, {"heading": "3.3. LHM Training", "text": "The training aims to minimize the empirical risks in Eq."}, {"heading": "4. Mapping LHM Classifier to a Neural Network", "text": "We propose mapping the LHM classifier to a neural network, which enables 1) continuous training of CNN functions and the LHM classifier for unbalanced problems and 2) generalization of the LHM to multiple classes, allowing a lower number of labeled training samples than NN to be used."}, {"heading": "4.1. Binary NN", "text": "A union of the intersections of positive hemispheres can be realized by an NN with three hidden layers; the first completely connected hidden layer has K \u00b7 H type neurons, where K is the number of hyperplanes in a section and H is the number of components; the second hidden layer has H nodes connected only to the neurons that form the corresponding section with hyperplanes; the weights on these connections and the distortions are solid and mixing AND function, namely, all weights of this layer are equal to 1 / K and the distortions are equal to \u2212 1 + 1 / (2K); the last hidden layer has two neurons that are completely connected to the previous layer with the fixed weights and distortions that imitate the OR operation, namely the first neuron has weights equal to 1 / H and the distortions equal to \u2212 1 / (2H); the second output has weights equal to \u2212 1 / H and the distortion of 1 / H is represented by 1 / 1 distortions of 1 / 1 that imitate the OR operation, namely the first neuron has weights equal to 1 / H and the distortions equal to -1 / (2H)."}, {"heading": "4.2. Multi-Class NN", "text": "For a multi-class environment, we suggest training the LHM model for each class based on additional, unlabeled data to estimate the negative class statistics. We then map these models to a multi-class layer NN with the following architecture. The first hidden layer is a fully connected layer with H-K neurons per class, H-K neurons in total, where C is the number of classes. These correspond to H-K-C hyperplanes in the LHM model. For each hidden layer, all hyperplanes are intersected with their corresponding node in the AND layer (as described in Section 4.1). The AND layer consists of H neurons. The next layer is a fully connected layer consisting of C nodes. The weights on the connections to the H components of the corresponding class are initialized with 1, and the weights on the remaining connections are initialized with very small values from a Gaussian distribution."}, {"heading": "5. Experiments", "text": "We start with a vivid example in 2D (Section 5.1) that shows the LHM classifier's ability to detect the hidden components in the positive class and separate each one from the negative class using a K-hyperplane model. Next, we compare the LHM model with alternative ensembles of hyperplanes on the 2007 PASCAL VOC dataset [6] (Section 5.2) and show its advantage over these methods and its robustness over the choice of the number of latent components. In these experiments, we use simple HOG features and a flat architecture. Finally, we show (Section 5.3) that the LHM classifier can be combined with CNN via transfer learning. We address two settings: 1) binary problems with unbalanced sets, 2) multi-class problems with a small number of labeled training samples. In both cases, LHM-based models show significantly better performance than Ns Ns 100. The experiments are carried out on the basis of images from 13-CNN and Cifar]."}, {"heading": "5.1. Synthetic Data", "text": "A simpler alternative to the LHM model is a two-step algorithm that first determines the structure of the target class by applying a type of unsupervised learning (e.g. k-means clustering) and then building a model for each component. Such a simple approach has been used in [8] with a per-cluster trained LDA [9] classifier. Unless the clusters are very small (as in the time-consuming example-based approach [18]), it depends heavily on the results of clustering. If an initial cluster formation is flawed (as in Figure 3, right), LDA (or another convex classifier) cannot separate the resulting components from the background without including many false positives. LHM training finds the underlying structure of the data and the model iterative and improves both (as in Figure 3, left). Furthermore, LHM is relatively robust compared to the initial mapping of the basic M. Figure 4 shows some errors in the corresponding loss of the elelelelial component structure."}, {"heading": "5.2. Ensembles of Hyperplanes", "text": "Next, we compared the LHM classifier with alternative ensembles of linear classifiers on the PASCAL VOC 2007 dataset [6] using a Dalal-Triggs variant of the HOG characteristics [3] with a fixed number of cells. LHM Model: We set the number of hyperplanes in each component to 2 and varied the number of components from 2 to 5. Initial mapping to the components was done using k averages with the Euclidean distance. LDA Union (as a base model): We applied k averages clusters to white characteristics to find the partition. Afterwards, we learned an LDA classifier for each cluster in this partition. We varied the number of clusters from 2 to 5 NN with an architecture that corresponds to LHM: We used the model described in Section 4.1 with K = 2 and H = 2,.., 5, but the weights were randomly initialized with the HM model HK19, which is essentially the only one component in the HM model."}, {"heading": "71.48% 65.17% 67.19% 69.45%", "text": "Similar to [8, 21], we learned the mean and covariance using Bounding Boxes from all classes and used them to represent the negative class in LDA Union, KHHM and LHM Training. We tested all ensemble classifiers in all windows of the test set. Table 1 summarizes the results (((1 \u2212 EER) \u00b7 100)) for all tested ensembles on average across classes and different parameters. It shows that the LHM model outperforms all other classifiers. Figure 5 compares LHM with NN in 20 categories (as one against all binary classifiers) for different numbers of hidden components. The graph shows that LHM outperforms NN regardless of the number of components."}, {"heading": "5.3. Deep Architecture", "text": "Next, we tested the LHM classifier on top of the pre-trained CNN feature extraction in unbalanced binary problems and in multi-class tasks with a small number of labeled examples. We examined the following transfer learning settings. The first setting refers to the best-case scenario, in which the source and the target classification tasks operate on the same set of characteristics, but differ in the classification problem. The second setting refers to the worst-case scenario for the transfer learning process, in which the source and the target classification problems show very little similarity. The \"worst-case scenario\" is very common in practice, as many classification tasks do not have a large, comprehensive training set (such as ImageNet [4] in object recognition) to be used in transfer learning. There is currently no good solution for such problems. We used the CZE F10, Cat FAR, Cat FAR, Cat Flight, Cat Flight, Cat Flight, Cat Flight, Cat Flight 10, Cat Flight, Cat Flight, Cat Flight, Cat Flight, Cat Fly, Cat Fly, Cat Fly, Cat Aircraft, Cat Aircraft, Cat Fly, Cat Fly, Cat Fly, Cat Fly, Cat Fly, Cat Fly, Cat Fly, Cat Cat Fly, Cat Fly, Cat Cat Cat Category 1, Cat Fly, Cat Fly, Cat Cat Fly, Cat Cat Fly, Cat Cat Fly, Cat Cat Category 1."}, {"heading": "5.3.1 Binary Imbalanced Problem", "text": "The \"Best Case\" Transfer Learning: We trained binary classifiers for course pairs from CIFAR-10 with unbalanced training sets, with the negative class comprising all samples from all other classes (40,000 examples) and the positive class comprising a different number of samples (140, 300, 600, 1400, 2000, 5000-all), resulting in imbalances from 1: 256 to 1: 4.LHM model trained with 2 hidden components and 3 hyperplanes per component. Matching NN missed the configuration of the LHM model, but weights were allowed to change during training. Figure 6-left shows the 1-EER (averaged over 5 classification problems) of the LHM classifier and the two NN baseline as a function of the positive sample size. The \"Worst Case\" Transfer Learning: Since the number of samples per class in CIFAR-100 shows significantly smaller than the number of negative samples, this test is based on the robustness of the training sample and the small number of negative ones."}, {"heading": "5.3.2 Multi-Class Problem", "text": "The \"Best Case\" Transfer Learning: We have fine-tuned the LHM binary classifiers trained for 5 pairs of categories to a multi-level NN as described in Section 4.2. We have fine-tuned the weights to a multi-level NN with a very fast workout (only a handful of epochs, while training requires two orders of magnitude more training epochs from scratch). The \"Worst Case\" Transfer Learning: We have upgraded the LHM binary classifiers mapped for the 5 categories of CIFAR-100 (using CIFAR-10 features) to a multi-level NN and adjusted the weights to a small number of epochs. To test the complexity of the transfer learning problem, we also trained a CNN (LeNet model implemented in MatvNet) problem to the relatively small NN."}, {"heading": "6. Training Efficiency", "text": "Another advantage of LHM-NN is its training efficiency. A class-specific LHM model converges in 5-10 iterations, and its training time depends primarily on the number of positive samples and the dimension. Negative samples are used to estimate the mean and covariance of the background. The initial estimate (which includes a large number of samples) can only be done once and used for all classes. As the probability of the negative class within the positive region is evaluated using false positives [19], the number of which decreases very quickly, the estimation time of the mean and covariance during training is negligible. Training of one binary classifier per class is independent of other classes, so their training can be done in parallel. Finally, fine-tuning of the multi-class network after mapping occurs very quickly, as all layers are initialized (using supervised learning)."}, {"heading": "7. Conclusions", "text": "We proposed a novel Hinge Minimax latent binary problem classifier, which detects the hidden components of the positive class and separates them from the negative class through the intersections of positive halves. The main advantage of this classifier is its ability to incorporate unlabeled data into the training, resulting in better robustness against imbalance problems. We demonstrated that class-specific LHM models for multi-class tasks can be mapped to a multi-class NN with a suitable architecture that requires only a few iterations of fine-tuning. Finally, the proposed LHM architecture can be integrated with CNN features through transfer learning. The entire training process is very efficient. Our experiments showed that such classifiers are much more robust than the number of labeled training samples. We plan to incorporate multi-class losses into the HingeMinimax paradigm and design an efficient algorithm to minimize this risk."}], "references": [{"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Introduction to statistical learning theory", "author": ["O. Bousquet", "S. Boucheron", "G. Lugosi"], "venue": "In Advanced lectures on machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "In CVPR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In CVPR09,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R.B. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,CVPR", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Discriminative decorrelation for clustering and classification", "author": ["B. Hariharan", "J. Malik", "D. Ramanan"], "venue": "In Computer Vision - ECCV 2012 - 12th European Conference on Computer", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Tight Bounds for the Expected Risk of Linear Classifiers and PAC-Bayes Finite- Sample Guarantees", "author": ["J. Honorio", "T. Jaakkola"], "venue": "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "author": ["S.M. Kakade", "K. Sridharan", "A. Tewari"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Siamese neural networks for one-shot image recognition", "author": ["G. Koch", "R. Zemel", "R. Salakhutdinov"], "venue": "ICML Deep Learning workshop,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "From N to N+1: multiclass transfer incremental learning", "author": ["I. Kuzborskij", "F. Orabona", "B. Caputo"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "A robust minimax approach to classification", "author": ["G.R. Lanckriet", "L.E. Ghaoui", "C. Bhattacharyya", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "The multiverse loss for robust transfer learning", "author": ["E. Littwin", "L. Wolf"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Ensemble of exemplar-svms for object detection and beyond", "author": ["T. Malisiewicz", "A. Gupta", "A.A. Efros"], "venue": "In IEEE International Conference on Computer Vision,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "K-hyperplane hingeminimax classifier", "author": ["M. Osadchy", "T. Hazan", "D. Keren"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Hybrid classifiers for object classification with a rich background", "author": ["M. Osadchy", "D. Keren", "B. Fadida-Specktor"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Recognition using hybrid classifiers", "author": ["M. Osadchy", "D. Keren", "D. Raviv"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Meta-learning with memory-augmented neural networks", "author": ["A. Santoro", "S. Bartunov", "M. Botvinick", "D. Wierstra", "T.P. Lillicrap"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S.E. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "The nature of statistical learning theory, ser. statistics for engineering and information science", "author": ["V.N. Vapnik"], "venue": "New York: Springer,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "MatConvNet: Convolutional Neural Networks for MATLAB", "author": ["A. Vedaldi", "K. Lenc"], "venue": "In Proc. of the 23rd Annual ACM Conference on Multimedia Conference,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Learning structural svms with latent variables", "author": ["C.-N.J. Yu", "T. Joachims"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Covering number bounds of certain regularized linear function classes", "author": ["T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}], "referenceMentions": [{"referenceID": 18, "context": "A more principled solution is to use mixed Hinge-Minimax risk [19] specifically designed to solve binary problems with imbalanced training sets.", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "Deep Neural Networks have recently shown very impressive performance in large-scale multi-class problems [14, 25, 23, 24].", "startOffset": 105, "endOffset": 121}, {"referenceID": 24, "context": "Deep Neural Networks have recently shown very impressive performance in large-scale multi-class problems [14, 25, 23, 24].", "startOffset": 105, "endOffset": 121}, {"referenceID": 22, "context": "Deep Neural Networks have recently shown very impressive performance in large-scale multi-class problems [14, 25, 23, 24].", "startOffset": 105, "endOffset": 121}, {"referenceID": 23, "context": "Deep Neural Networks have recently shown very impressive performance in large-scale multi-class problems [14, 25, 23, 24].", "startOffset": 105, "endOffset": 121}, {"referenceID": 19, "context": "We follow the paradigm introduced in [20] that combines hinge risk for the smaller class and minimax risk [16, 10] for the larger class to address imbalanced classification problems.", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "We follow the paradigm introduced in [20] that combines hinge risk for the smaller class and minimax risk [16, 10] for the larger class to address imbalanced classification problems.", "startOffset": 106, "endOffset": 114}, {"referenceID": 9, "context": "We follow the paradigm introduced in [20] that combines hinge risk for the smaller class and minimax risk [16, 10] for the larger class to address imbalanced classification problems.", "startOffset": 106, "endOffset": 114}, {"referenceID": 19, "context": "The mixed risk was used to train linear and kernel hybrid classifiers in [20].", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "To address these issues, [19] derived a hinge-minimax risk and an efficient training algorithm for intersection ofK positive halfspaces.", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "The robustness of LHM to imbalanced problems can be explained by the use of the minimax risk [16, 10], that serves as a regularizer in training (since it utilizes the statistics of the entire class, as opposed to learning from small batches of examples).", "startOffset": 93, "endOffset": 101}, {"referenceID": 9, "context": "The robustness of LHM to imbalanced problems can be explained by the use of the minimax risk [16, 10], that serves as a regularizer in training (since it utilizes the statistics of the entire class, as opposed to learning from small batches of examples).", "startOffset": 93, "endOffset": 101}, {"referenceID": 4, "context": ",[5, 7, 17]) to combine a pre-trained CNN (trained on a much larger training set for a related classification problem) for feature extraction, with a classifier for the target problem.", "startOffset": 1, "endOffset": 11}, {"referenceID": 6, "context": ",[5, 7, 17]) to combine a pre-trained CNN (trained on a much larger training set for a related classification problem) for feature extraction, with a classifier for the target problem.", "startOffset": 1, "endOffset": 11}, {"referenceID": 16, "context": ",[5, 7, 17]) to combine a pre-trained CNN (trained on a much larger training set for a related classification problem) for feature extraction, with a classifier for the target problem.", "startOffset": 1, "endOffset": 11}, {"referenceID": 14, "context": "Similarly to [15], which considered the transfer learning for the n+ 1 category from a fully trained n-category classifier, we use only a handful of training samples for tuning it.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "In contrast to [15], we do not restrict the new classifier to belong to the span of the previously learned n classifiers.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "The method proposed here is different from the one-shot learning approach [12, 22], which attempts to find a mapping between target and source examples and apply it to the examples or to the model.", "startOffset": 74, "endOffset": 82}, {"referenceID": 21, "context": "The method proposed here is different from the one-shot learning approach [12, 22], which attempts to find a mapping between target and source examples and apply it to the examples or to the model.", "startOffset": 74, "endOffset": 82}, {"referenceID": 25, "context": "It was shown that hinge loss [26, 29, 1, 2, 11] is computationally appealing when there are fairly small number of training samples, thus it could be used to measure the positive class risk within imbalanced problem settings.", "startOffset": 29, "endOffset": 47}, {"referenceID": 28, "context": "It was shown that hinge loss [26, 29, 1, 2, 11] is computationally appealing when there are fairly small number of training samples, thus it could be used to measure the positive class risk within imbalanced problem settings.", "startOffset": 29, "endOffset": 47}, {"referenceID": 0, "context": "It was shown that hinge loss [26, 29, 1, 2, 11] is computationally appealing when there are fairly small number of training samples, thus it could be used to measure the positive class risk within imbalanced problem settings.", "startOffset": 29, "endOffset": 47}, {"referenceID": 1, "context": "It was shown that hinge loss [26, 29, 1, 2, 11] is computationally appealing when there are fairly small number of training samples, thus it could be used to measure the positive class risk within imbalanced problem settings.", "startOffset": 29, "endOffset": 47}, {"referenceID": 10, "context": "It was shown that hinge loss [26, 29, 1, 2, 11] is computationally appealing when there are fairly small number of training samples, thus it could be used to measure the positive class risk within imbalanced problem settings.", "startOffset": 29, "endOffset": 47}, {"referenceID": 15, "context": "Alternatively, the minimax risk [16, 10] upper bounds the distribution that generates the instances-labels examples in the world.", "startOffset": 32, "endOffset": 40}, {"referenceID": 9, "context": "Alternatively, the minimax risk [16, 10] upper bounds the distribution that generates the instances-labels examples in the world.", "startOffset": 32, "endOffset": 40}, {"referenceID": 18, "context": "Our approach builds upon the mixed risk for the intersection of K-hyperplanes [19] which is briefly summarized in section 2.", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "The KHHM training algorithm in [19] minimizes the empirical risk:", "startOffset": 31, "endOffset": 35}, {"referenceID": 18, "context": "It was shown in [19] that", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "1 is defined as LX+(W ) = \u2211 x\u2208X+ `(W ;x, 1), where `(W ;x, y) = \u2211 j max {0, 1\u2212 yw j x} is the K-hyperplane hinge loss [19].", "startOffset": 118, "endOffset": 122}, {"referenceID": 27, "context": "Similarly to latent SVM [28], the complexity of the optimal assignment of samples to latent components is exponential.", "startOffset": 24, "endOffset": 28}, {"referenceID": 18, "context": "This risk is minimized by the training algorithm proposed in [19].", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "KHHM-train refers to the training of intersection of hyperplanes from [19].", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": ", C using the iterative algorithm from [19] (the convergence of which was shown in [19]).", "startOffset": 39, "endOffset": 43}, {"referenceID": 18, "context": ", C using the iterative algorithm from [19] (the convergence of which was shown in [19]).", "startOffset": 83, "endOffset": 87}, {"referenceID": 5, "context": "Next, we compare LHM model to alternative ensembles of hyperplanes on the PASCAL-VOC 2007 dataset [6] (Section 5.", "startOffset": 98, "endOffset": 101}, {"referenceID": 12, "context": "The experiments are performed on images from cifar-10 and cifar-100 [13] and using LeNet CNN for features extraction.", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "Such a simple approach was employed in [8] with LDA [9] classifier trained per cluster.", "startOffset": 39, "endOffset": 42}, {"referenceID": 8, "context": "Such a simple approach was employed in [8] with LDA [9] classifier trained per cluster.", "startOffset": 52, "endOffset": 55}, {"referenceID": 17, "context": "suming [18]), it relies heavily on the results of the clustering.", "startOffset": 7, "endOffset": 11}, {"referenceID": 5, "context": "Next, we compared the LHM classifier to alternative ensembles of linear classifiers on PASCAL VOC 2007 dataset [6] using Dalal-Triggs variant of the HOG features [3] with a fixed number of cells.", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "Next, we compared the LHM classifier to alternative ensembles of linear classifiers on PASCAL VOC 2007 dataset [6] using Dalal-Triggs variant of the HOG features [3] with a fixed number of cells.", "startOffset": 162, "endOffset": 165}, {"referenceID": 18, "context": "KHHM model [19]: This is essentially an LHM model with a single component, thus it is theoretically inferior to LHM.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "Similarly to [8, 21], we learned the background mean and covariance using bounding boxes from all classes and used them to represent the negative class in LDA union, KHHM, and LHM training.", "startOffset": 13, "endOffset": 20}, {"referenceID": 20, "context": "Similarly to [8, 21], we learned the background mean and covariance using bounding boxes from all classes and used them to represent the negative class in LDA union, KHHM, and LHM training.", "startOffset": 13, "endOffset": 20}, {"referenceID": 3, "context": "The \u201cworst case\u201d scenario is very common in practice, as many classification tasks do not have a large, comprehensive training set (such as ImageNet [4] in object recognition) to be used in transfer learning.", "startOffset": 149, "endOffset": 152}, {"referenceID": 26, "context": "Specifically, we trained the LeNet model implemented in MatConvNet [27] on CIFAR10.", "startOffset": 67, "endOffset": 71}, {"referenceID": 26, "context": "To test the complexity of the transfer learning problem we also trained a CNN (LeNet model implemented in MatConvNet [27]) on the target problem.", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "Since the probability of the negative class is evaluated inside the positive region using false positives [19], the number of which drops very fast, the estimation time of the mean and covariance during the training is negligible.", "startOffset": 106, "endOffset": 110}], "year": 2017, "abstractText": "Deep Learning (DL) methods show very good performance when trained on large, balanced data sets. However, many practical problems involve imbalanced data sets, or/and classes with a small number of training samples. The performance of DL methods as well as more traditional classifiers drops significantly in such settings. Most of the existing solutions for imbalanced problems focus on customizing the data for training. A more principled solution is to use mixed Hinge-Minimax risk [19] specifically designed to solve binary problems with imbalanced training sets. Here we propose a Latent Hinge Minimax (LHM) risk and a training algorithm that generalizes this paradigm to an ensemble of hyperplanes that can form arbitrary complex, piecewise linear boundaries. To extract good features, we combine LHM model with CNN via transfer learning. To solve multi-class problem we map pre-trained categoryspecific LHM classifiers to a multi-class neural network and adjust the weights with very fast tuning. LHM classifier enables the use of unlabeled data in its training and the mapping allows for multi-class inference, resulting in a classifier that performs better than alternatives when trained on a small number of training samples.", "creator": "LaTeX with hyperref package"}}}