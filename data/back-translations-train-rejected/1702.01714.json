{"id": "1702.01714", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2017", "title": "DNN adaptation by automatic quality estimation of ASR hypotheses", "abstract": "In this paper we propose to exploit the automatic Quality Estimation (QE) of ASR hypotheses to perform the unsupervised adaptation of a deep neural network modeling acoustic probabilities. Our hypothesis is that significant improvements can be achieved by: i)automatically transcribing the evaluation data we are currently trying to recognise, and ii) selecting from it a subset of \"good quality\" instances based on the word error rate (WER) scores predicted by a QE component. To validate this hypothesis, we run several experiments on the evaluation data sets released for the CHiME-3 challenge. First, we operate in oracle conditions in which manual transcriptions of the evaluation data are available, thus allowing us to compute the \"true\" sentence WER. In this scenario, we perform the adaptation with variable amounts of data, which are characterised by different levels of quality. Then, we move to realistic conditions in which the manual transcriptions of the evaluation data are not available. In this case, the adaptation is performed on data selected according to the WER scores \"predicted\" by a QE component. Our results indicate that: i) QE predictions allow us to closely approximate the adaptation results obtained in oracle conditions, and ii) the overall ASR performance based on the proposed QE-driven adaptation method is significantly better than the strong, most recent, CHiME-3 baseline.", "histories": [["v1", "Mon, 6 Feb 2017 17:21:39 GMT  (284kb)", "http://arxiv.org/abs/1702.01714v1", "Computer Speech &amp; Language December 2016"]], "COMMENTS": "Computer Speech &amp; Language December 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniele falavigna", "marco matassoni", "shahab jalalvand", "matteo negri", "marco turchi"], "accepted": false, "id": "1702.01714"}, "pdf": {"name": "1702.01714.pdf", "metadata": {"source": "CRF", "title": "DNN Adaptation by Automatic Quality Estimation of ASR Hypotheses", "authors": ["Daniele Falavigna", "Marco Matassoni", "Shahab Jalalvand", "Matteo Negri", "Marco Turchi"], "emails": ["falavi@fbk.eu", "matasso@fbk.eu", "jalalvand@fbk.eu", "negri@fbk.eu", "turchi@fbk.eu"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.01 714v 1 [cs.C L] 6F eb2 01In this thesis, we propose to use the automatic Quality Estimation (QE) of ASR hypotheses to perform the unattended adjustment of a deep neural network modelling acoustic probabilities. Our hypothesis is that significant improvements can be achieved by: i) automatically transcribing the evaluation data we are currently trying to detect and ii) selecting a subset of \"good quality\" instances based on the word error rate (WHO) predicted by a QE component. To confirm this hypothesis, we are conducting several experiments based on the evaluation data sets published for the CHiME-3 challenge. First, we operate in oracle conditions where manual transcriptions of the evaluation data are available, whereby we can calculate the true theorem WHO, and in this scenario we perform the overall results of the variable QE adjustment with the different amounts of the QME-E."}, {"heading": "1. Introduction", "text": "Automatic Voice Recognition (ASR) with microphone arrays is gaining increasing interest in a variety of application scenarios in which different types of application scenarios such as home and office automation, intelligent cars and humanoid robots are compared. In such applications, ASR should be able to operate in environments. Please quote this article as: D. Falavigna et al., DNN adaptation by automatic quality assessment of ASR hypotheses, Computer Speech & Language (2016), matasso @ fbk.eu (Marco Matassoni), jalalvand @ fbk.eu (Shahab Jalvand), negri @ fbk.eu (Matteo Negri), turchi @ fbk.eu."}, {"heading": "2. Related work", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "3. KL-divergence based regularization", "text": "The DNNs taken into account in this work estimate the posterior probability of an output device si associated with an HMM output probability function (PDF) The posterior probability p [si | ot], which is an observation at the time t, is then converted into a PDF with the following formula: p [ot | si] = p [si] p [ot] p [ot] p [si] p [si] p [si] p [si] p [si] p [si] p [ot] p [ot] p [ot] p [ot] p [si] p [ot] p [si] p [si] p [si] p [si] p [si] p [si] p] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p [si] p] p [si p] p] p [si] p] p [si p] p] p [si p] p [si p] p] p [si p] p] p [si p] p] p [si p] p] p [si p] p] p [si p] p] p [si p] p] p [si p] p [p] p] p [si p] p] p [si p] p] p [p] p] p] p] p [si p] p [si p] p [p] p] p] p [si p] p] p [p [p] p] p] p [p p p p p p] p] p p p p] p] p] p [p p p p p p p p p p p p p p p p p] p] p] p p p p] p p p p p p p p] p p p p p p p p p p p p] p p p p p p p p p p p p p p p p p p p p p p p"}, {"heading": "3.1. Soft DNN adaptation", "text": "Experiments in [55] have shown that the optimal value of \u03b1 in Eq.5 depends on the size of the fit data. On the basis of this intuition, we propose to calculate \u03b1 on a sentence basis as a function of the sentence-WER estimates. To this end, we use previous research that we have carried out on the basis of the ASR quality estimate (WER prediction) and word error recognition. In principle, we could simply use the following value as a sentence-dependent regulation coefficient: \u03b1 (k) = WERpredk, 1 \u2264 k \u2264 K, where 0 \u2264 WER predk \u2264 1 is an automatic estimate of the box sentence WHO and K is the total number of fit sets WHO and K is the total number of fit sets. However, note that the value of K is small and WERpredk \u00b2 is weighted the original distribution of p, in Eq.5."}, {"heading": "4. ASR quality estimation", "text": "The simplest approach to rough estimation of transcription quality (without reference transcripts) is to consider sentence confidence values that describe how the system is confident about the quality of its own hypotheses. Sentence confidence values can be calculated by averaging the trust of the words in the best output string. However, such information often reflects a biased perspective influenced by individual ASR decoder characteristics. In fact, confidence values are usually close to the maximum value, shifting the predicted WHO (calculated as 1 \u2212 confidence) to values that are close to zero. In order to obtain more objective and reliable sentence predictions, we proposed ASR quality estimates as a supervised regression method that effectively utilizes a combination of \"glass box\" and \"black box\" characteristics. Glass box characteristics, similar to configuration values, include the information contained in the transcription system's inner functionality."}, {"heading": "5. ASR system", "text": "The architecture of our ASR system is illustrated in Figures 1 and 2. In the former it is based on normalized features of the fMLLR, in the latter on features of the filter bank. The system is essentially based on the KALDI CHiME-3 v2 package (derived from the ASR system described in [20]) with the addition of a second decoding passport that performs an unattended DNN adaptation, as described in Section 3. In our template to CHiME-3 [22], we achieved the best performance in the et05 real evaluation set with a simple delay and sum (DS) beamforming consisting of a uniform weighting of the phased signals of the 5 front microphones. A similar approach, although based on the well-known BeamformIt toolkit [4], is also included in the latest software package implementing the CHiME-3 baseline."}, {"heading": "5.1. Filter-bank features", "text": "The used filter bank consists of 40 log Mel filters. Feature vectors are calculated every 10ms using a hamming window of 25ms length and normalized to mean / variance on a loudspeaker basis. Base DNN is trained using the Karel setups included in the KALDI toolkit [51]. To this end, the 8,738 training expressions were aligned to their transcriptions using the GMM-HMM base models. [5] A context window with 11 frames (5 frames on each side) is used as input to create a 440-dimensional feature 5The initial GMM system uses the KALDI recipe associated with the earlier CDNME challenges. [6, 52] Vector. The DNN has 7 hidden layers, each with 2,048 neurons. The DNN is trained in several stages, including Restricted Boltzmann (RM Machines) sequencing from the initial state of 0.08% to the mini optimization, with the minimum of 0.048%."}, {"heading": "5.2. fMLLR features", "text": "For training, 13 microfrequency receiver coefficients (MFCCs) are calculated every 10 ms using a hamming window of 25 ms length. These characteristics are normalized on speaker-by-speaker basis, spliced by + / - 3 frames next to the central frame, and projected down to 40 dimensions using linear discriminant analysis (LDA) and maximum probability of linear transformation (MLLT). Subsequently, a single loudspeaker-dependent fMLR transformation is estimated and applied for the adaptive formation of three-voice HMDNM GMMs. DNNHMMs hybrid systems are built on the basis of LDA + MLT + fMLLR characteristics and three-voice HMM GMMs. During decoding, the first LDA + MLTT + fMLLR forms of the three-voice HMLM-GMM form are used as auxiliary form."}, {"heading": "5.3. Language models", "text": "The LM used in the experiments is the 3 gram LM supplied with the release of the CHiME-3 v2 package, which uses the Kneser-Ney smoothing method to estimate backoff probabilities. It was trained with about 37 million words. After editing low-frequency words, the vocabulary size is about 5,000 words and the perplexity value (measured using real Dt05 reference transcriptions) is 119.2. Finally, although not shown in the figure, we also perform a final re-correction of the n-best lists created in the second decoding pass with the 5 gram LM and the RNLM contained in the CHiME-3 v2 package."}, {"heading": "6. Experimental setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Speech corpora", "text": "For our experiments, we use the multi-microphone evaluation data collected for the CHiME-3 challenge, which is publicly available. 6 Full details of this data set, the general challenge, and its results can be found in the corresponding review article [5], which also reported the performance of the 26 participating systems.Six different microphones placed on a tablet PC were used to record sentences from the Wall Street Journal (WSJ) corpus uttered by different speakers in four different environments (bus, cafe, pedestrian precinct, and intersection).The training corpus consists of 1,600 \"real\" loud sentences uttered by 4 speakers, and 7,138 \"simulated\" loud sentences uttered by 83 speakers who formed the WSJ-84 training set. Simulated loud sentences are uttered by folding clean signals with impulse responses from the above environments and summarizing the corresponding loudspeaker noises."}, {"heading": "6.2. Experiment definition", "text": "The aforementioned lcihsrc\u00fcehncS rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the r"}, {"heading": "7. Results", "text": "In this section, we present the experimental results obtained in the various environments outlined in Table 3, playing with: i) the different conditions, ii) the type of monitoring, iii) the size of the fit data, and iv) the way in which the fit data is selected. In the analysis and subsequent discussion, our WER values are not compared with those of the best ASR system participating in the CHiME-3 challenge [54], which uses a much more complex architecture for signal preprocessing and system combinations, as well as an extended set of training data. In fact, the implementation of such a state-of-the-art system was outside the scope of this work, the aim of which is to demonstrate the effectiveness of QE-based DNN matching in order to improve the performance of a standard, less complex, but still strong ASR system. Therefore, our comparative term is represented by the HiCME-Baseline-05 reference, leading to 15.4% WETER and 8.4% on DETER."}, {"heading": "7.1. DNN adaptation in cross conditions", "text": "In this section, we analyze the performance achieved under transverse conditions, with both manual and automatic monitoring. First, we use all the records in DT05 to adjust the DNN. Then, we show the performance achieved with automatic monitoring, which is achieved by matching data derived from DT05 after the utterances with the highest VER have been removed."}, {"heading": "7.1.1. Using all the adaptation utterances", "text": "Figure 3a shows the WERs (as functions of the regulation coefficient \u03b1 in Equation 5) achieved on the ET05 rating scale by using fMLLR characteristics in both manual and automatic monitoring. Similarly, the performance achieved with filter bank characteristics is indicated in Figure 3b. The horizontal line in both numbers corresponds to the output power. As can be seen, the use of manual supervision or equivalent monitored adjustment enables an improvement in the output power for both characteristics. In both cases, there is an optimal intermediate value of \u03b1 in the interval [0, 1], which indicates that we should not fully trust either the initial model or the adjustment data. 7 With the best value, we obtain about 1% WER point, which indicates the effectiveness of the interpolation process expressed by equation."}, {"heading": "7.1.2. Selecting adaptation utterances", "text": "In order to verify the possible effects of automatic transcription errors in monitoring, we extracted from the DT05 adaptation set the utterances whose true WHERE, calculated from reference transcriptions, is less than 10%. Subsequently, we adapted the baseline DNN using the hard approach and by varying the value of the regularization coefficient \u03b1. The results are shown in Figures 4a and 4b. For comparison purposes, the two figures also include the same curves of Figures 3a and 3b, which are related to the use of the entire adaptation set. As can be seen, the selection of adaptation manifestations with WHERE < 10% results in curves that approach those obtained under manual supervision, demonstrating the benefits of reducing transcription errors in supervision."}, {"heading": "7.2. DNN adaptation in homogeneous conditions", "text": "In this section, we report and discuss the performance achieved under homogeneous conditions with automatic monitoring. First, we use the entire available set of adjustment expressions. Then, by applying ASR QE to the WER prediction, we experiment with different subsets of data selected according to their estimated quality."}, {"heading": "7.2.1. Using all the adaptation utterances", "text": "The experiments were carried out by carrying out two decoding passes, 8 as explained in section 5. Although the transcriptions resulting from the first pass = \u03b2 provide all the conditions based on the basic DNNs, monitoring for the following adaptation steps. Then, the adapted DNNs are used in the second decoding pass to produce the final transcriptions. The performance results achieved on both development and evaluation sets, with hard and soft adjustment, are shown in Table 4 (in brackets, we show the absolute WER reduction in terms of baseline performance). In the case of soft adjustment, both oracular and automatically predictable results of WERs were tested, with the experiments in Figure 3, we measured performance as a function of the coefficient. We also performed the same series of experiments to evaluate performance also as a function of the coefficient \u03b2 defined in Equation 6. For reasons of compactness, we do not provide the entire set of results and refer only to the peak values of ER."}, {"heading": "7.2.2. Selecting adaptation utterances", "text": "For conciseness, in the next series of experiments, which we have predicted only for fMLLR results, a comparison will be made between the two methods used (according to which the selection of the two methods is likely to be most effective), but the same and more obvious trends have also been observed using the filter bank functions. Figures 5a and 5b report on the performance achieved on DT05 using subsets of adjustment statements of different sizes. The statements of the development group DT05 described in Section 4 were sorted by the WHO values resulting from the first decoding cycle. We extracted from four adjustment sets, each containing the \"best\" 300, 600 and 1,200 statements. The various subgroups, along with their automatic transcriptions, were used to adjust the baseline DNN using the hard approach. The reason for setting thresholds to the size of the compilation of our results is the fact that we are seeking a comparison between the two methods applied."}, {"heading": "7.2.3. LM rescoring", "text": "Table 6 illustrates the results obtained at ET05 with the LM rescoring method published in the updated CHiME-3 recipe. This method rescores the last word grids generated in the second decoding cycle in two successive steps: first with a 5 gram LM, then with a linear combination of a 5 gram LM and an RNNLM. Significant performance gains show the additive effect of LM rescoring over DNN adaptation, allowing us to achieve a significant WHO of 10.9% of ET05.101010. See http: / / spandh.dcs.shef.ac.uk / chime _ challenge / results.html for the official results of the challenge."}, {"heading": "8. Discussion", "text": "The driving forces mentioned are able to assert themselves in the manner in which they are able to behave and in the way in which they are able to behave, in the way in which they are able to behave, in the way in which they are able to behave, in the way in which they are able to behave, in the way in which they are able to behave, in the way in which they are able to behave, in the way in which they are able to behave, in the way in which they are able to behave, in the way in which they are able to behave and in the way in which they are able to behave."}, {"heading": "9. Conclusions", "text": "This year it will be so far that it will be able to the mentionlcihsrcnlrVo rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the rf\u00fc the rfu the rfu the rfu the rf\u00fc the rfu the rfu the rf\u00fc the rfu the rfu"}], "references": [{"title": "Connectionist Speaker Normalization and Adaptation", "author": ["V. Abrash", "H. Franco", "A. Sankar", "M. Cohen"], "venue": "in: Proc. of Interspeech,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Adaptation of Artificial Neural Networks Avoiding Catastrophic Forgetting", "author": ["D. Albesano", "R. Gemello", "P. Laface", "F. Mana", "S. Scanzio"], "venue": "in: Proc. of International Joint Conference on Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin", "author": ["D Amodei"], "venue": "in: Proc. of International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Acoustic Beamforming for Speaker Diarization of Meetings", "author": ["X. Anguera", "C. Wooters", "J. Hernando"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "The third \u2019CHiME\u2019 Speech Separation and Recognition Challenge: Dataset, task and baselines", "author": ["J. Barker", "R. Marxer", "E. Vincent", "S. Watanabe"], "venue": "in: Proc. of IEEE ASRU Workshop,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "The PASCAL CHiME speech separation and recognition challenge", "author": ["J. Barker", "E. Vincent", "N. Ma", "H. Christensen", "P. Green"], "venue": "Computer Speech and Language", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Microphone Arrays: Signal Processing Techniques and Applications", "author": ["M. Brandstein", "D. Ward"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Context Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Trans. on Audio Speech and Language Processing", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Large Vocabulary Decoding and Confidence Estimation Using Word Posterior Probabilities", "author": ["G. Evermann", "P.C. Woodland"], "venue": "in: Proc. of ICASSP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Design and Evaluation of Acoustic and Language Models for Large Scale Telephone Services", "author": ["A. Facco", "D. Falavigna", "R. Gretter", "M. Vigano"], "venue": "Speech Communication", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "A Post-processing System to Yield Reduced Word Error Rates: Recognizer Output Voting Error Reduction (ROVER)", "author": ["J.G. Fiscus"], "venue": "in: Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Maximum Likelihood Linear Transformations for HMM-based Speech Recognition", "author": ["M.J.F. Gales"], "venue": "Computer Speech and Language", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Linear Hidden Transformations for Adaptation of Hybrid ANN/HMM Models", "author": ["R. Gemello", "F. Mana", "S. Scanzio", "P. Laface", "R.D. Mori"], "venue": "Speech Communication", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Extremely Randomized Trees", "author": ["P. Geurts", "D. Ernst", "L. Wehenkel"], "venue": "Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Confidence Scores for Acoustic Model Adaptation", "author": ["C. Gollan", "M. Bacchiani"], "venue": "in: Proc. of ICASSP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Towards End-to-End Speech Recognition with Recurrent Neural Networks", "author": ["A. Graves", "N. Jaitly"], "venue": "in: Proc. of International Conference on Machine", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Active Learning for Automatic Speech Recognition", "author": ["D. Hakkani-Tur", "G. Riccardi", "A. Gorin"], "venue": "in: Proc. of ICASSP,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "Y. Wang"], "venue": "IEEE Signal Processing Magazine", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "The MERL/SRI System for the 3RD CHiME Challenge using Beamforming, Robust Feature Extraction, and Advanced Speech Recognition, in: IEEE workshop on Automatic Speech Recognition and Understanding (ASRU)", "author": ["T. Hori", "Z. Chen", "H. Erdogan", "J.R. Hershey", "J.L. Roux", "V. Mitra", "S. Watanabe"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Feature Space Maximum a Posteriori Linear Regression for Adaptation of Deep Neural Networks", "author": ["Z. Huang", "J. Li", "M. Siniscalchi", "I. Chen", "C. Weng", "C. Lee"], "venue": "in: Proc. of INTERSPEECH,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Boosted Acoustic Model Learning and Hypotheses", "author": ["S. Jalalvand", "D. Falavigna", "M. Matassoni", "P. Svaizer", "M. Omologo"], "venue": "Rescoring on the CHiME-3 Task, in: Proc. of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Driving ROVER With Segment-based ASR Quality Estimation", "author": ["S. Jalalvand", "M. Negri", "D. Falavigna", "M. Turchi"], "venue": "in: Proc. of ACL, Beijing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "I-vector Estimation using Informative Priors for Adaptation of Deep Neural Networks", "author": ["P. Karanasou", "M.J.F. Gales", "P.C. Woodland"], "venue": "in: Proc. of Interspeech,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "A Study of Interspeaker Variability in Speaker Verification", "author": ["P. Kenny", "P. Oullet", "N. Dehak", "V. Gupta", "P. Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Investigating Lightly Supervised Acoustic Model Training", "author": ["L. Lamel", "J. Gauvain", "G. Adda"], "venue": "in: Proc. of ICASSP, Salt Lake City,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "Comparison of Discriminative Input and Output Transformation for Speaker Adaptation in the Hybrid NN/HMM Systems", "author": ["B. Li", "K. Sim"], "venue": "in: Proc. of Interspeech,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Regularized Adaptation of Discriminative Classifiers", "author": ["X. Li", "J. Bilmes"], "venue": "in: Proc. of ICASSP,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Finding Consensus in Speech Recognition: Word Error Minimization and Other Applications of Confusion Networks", "author": ["L. Mangu", "E. Brill", "A. Stolcke"], "venue": "Computer Speech and Language", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}, {"title": "Match without a Referee: Evaluating MT Adequacy without Reference Translations", "author": ["Y. Mehdad", "M. Negri", "M. Federico"], "venue": "in: Proc. of the Machine TranslationWorkshop (WMT2012),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Speaker Adaptive Training of Deep Neural Network Acoustic Models using I-vectors", "author": ["Y. Miao", "H. Zhang", "F. Metze"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing 23,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Acoustic Modeling Using Deep Belief Networks", "author": ["A. Mohamed", "G. Dahl", "G. Hinton"], "venue": "IEEE Trans. on Audio Speech and Language Processing", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Quality Estimation for Automatic Speech Recognition", "author": ["M. Negri", "M. Turchi", "D. Falavigna", "J.G.C. de Souza"], "venue": "in: Proc. of COLING,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Speaker Adaptation for Hybrid HMM-ANN Continuous Speech Recognition System", "author": ["J. Neto", "L. Almeida", "M. Hochberg", "C. Martins", "L. Nunes", "S. Renals", "T. Robinson"], "venue": "in: Proc. of Interspeech,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1995}, {"title": "fMLLR Based Feature-space Speaker Adaptation of DNN Acoustic Models", "author": ["S.H.K. Parthasarathi", "B. Hoffmeister", "S. Matsoukas", "A. Mandal", "N. Strom", "S. Garimella"], "venue": "in: Proc. of Interspeech,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Improved Mllr Speaker Adaptation Using Confidence Measures For Conversational Speech Recognition", "author": ["M. Pitz", "F. Wessel", "H. Ney"], "venue": "in: Proc. Int. Conf Spoken Language Processing, Beijing,China", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2000}, {"title": "Neural Networks for Distant Speech Recognition, in: Proc. of Hands-free Speech Communication and Microphone Arrays (HSCMA) Wokshop, Villers-les-Nancy", "author": ["S. Renals", "P. Swietojanski"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Feature Engineering in Context-dependent Deep Neural Networks for Conversational Speech Transcription", "author": ["F. Seide", "G. Li", "X. Chen", "D. Yu"], "venue": "in: Proc. of IEEE ASRU Workshop,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Conversational Speech Transcriptions Using Context-Dependent Deep Neural Networks", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "in: Proc. of Interspeech,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "An Investigation of Deep Neural Networks for Noise Robust Speech Recognition", "author": ["M. Seltzer", "D. Yu", "Y.Q. Wang"], "venue": "in: Proc. of ICASSP,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Hermitian Polynomial for Speaker Adaptation of Connectionist Spech Recognition Systems", "author": ["S. Siniscalchi", "J. Li", "C. Lee"], "venue": "IEEE Trans. on Audio Speech and Language Processing", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "FBK-UEdin Participation to the WMT13 Quality Estimation", "author": ["J.G.C. de Souza", "C. Buck", "M. Turchi", "M. Negri"], "venue": "Shared Task, in: Proc. of the Eighth Workshop on Statistical Machine", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "FBK-UPV-UEdin participation in the WMT14 Quality Estimation shared-task", "author": ["J.G.C. de Souza", "J. Gonz\u00e1lez-Rubio", "C. Buck", "M. Turchi", "M. Negri"], "venue": "in: Proc. of the Ninth Workshop on Statistical Machine Translation,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Multitask Learning for Adaptive Quality Estimation of Automatically Transcribed Utterances", "author": ["J.G.C. de Souza", "H. Zamani", "M. Negri", "M. Turchi", "D. Falavigna"], "venue": "in: Proc. of NAACL,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "Hybrid Acoustic Models for Distant and Multichannel Large Vocabulary Speech Recognition, in: Proc. of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), Olomuc, Czech Rep", "author": ["P. Swietojanski", "S. Renals"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "Learning Hidden Unit Contributions for Unsupervised Speaker Adaptation of Neural Network Acoustic Models", "author": ["P. Swietojanski", "S. Renals"], "venue": "in: Proc. of IEEE Workshop on Spoken Language Technology, Lake Tahoe,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "Deep Neural Network Features and Semi- Supervised Training for Low Resource Speech Recognition", "author": ["S. Thomas", "M. Seltzer", "K. Church", "H. Hermansky"], "venue": "in: Proc. of ICASSP,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2013}, {"title": "Coping with the Subjectivity of Human Judgements in MT Quality Estimation, in: Proc. of the Eighth Workshop on Statistical Machine Translation, Association for Computational Linguistics, Sofia, Bulgaria", "author": ["M. Turchi", "M. Negri", "M. Federico"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2013}, {"title": "Sequence-discriminative Training of Deep Neural Networks", "author": ["K. Vesely", "A.Ghoshal", "L. Burget", "D. Povey"], "venue": "in: Proc. of Interspeech,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2011}, {"title": "The Second CHiME Speech Separation and Recognition Challenge: Datasets, Tasks and Baselines", "author": ["E. Vincent", "J. Barker", "S. Watanabe", "J. Le Roux", "F. Nesta", "M. Matassoni"], "venue": "in: Proc. of ICASSP,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2013}, {"title": "Adaptation of Context-Dependent Deep Neural Networks for Automatic Speech Recognition", "author": ["K. Yao", "D. Yu", "F. Seide", "H. Su", "L. Deng", "Y. Gong"], "venue": "in: Proc. of SLT,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2012}, {"title": "Advances in speech enhancement and recognition for mobile multi-microphone devices", "author": ["Yoshioka", ".T", "N. Ito", "M. Delcroix", "A. Ogawa", "K. Kinoshita", "M. Fujimoto", "C. Yu", "W. Fabian", "M. Espi", "T. Higuchi", "S. Araki", "T. Nakatani"], "venue": "in: IEEE workshop on Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2015}, {"title": "KL-Divergence Regularized Deep Neural Network Adaptation for Improved Large Vocabulary Speech Recognition", "author": ["D. Yu", "K. Yao", "H. Su", "G. Li", "F. Seide"], "venue": "in: Proc. of ICASSP,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "To cope with the above scenarios, most of the current approaches are based on the implementation of a variety of enhancement techniques such as beamforming, denoising and dereverberation [7].", "startOffset": 187, "endOffset": 190}, {"referenceID": 4, "context": "Participants\u2019 results [5] evidenced the effectiveness of signal enhancement approaches, mostly based on \u201cbeamforming\u201d, combined with the use of hybrid acoustic models based on deep neural networks hidden Markov models (DNN-HMMs) [19, 33, 47, 39].", "startOffset": 22, "endOffset": 25}, {"referenceID": 17, "context": "Participants\u2019 results [5] evidenced the effectiveness of signal enhancement approaches, mostly based on \u201cbeamforming\u201d, combined with the use of hybrid acoustic models based on deep neural networks hidden Markov models (DNN-HMMs) [19, 33, 47, 39].", "startOffset": 229, "endOffset": 245}, {"referenceID": 30, "context": "Participants\u2019 results [5] evidenced the effectiveness of signal enhancement approaches, mostly based on \u201cbeamforming\u201d, combined with the use of hybrid acoustic models based on deep neural networks hidden Markov models (DNN-HMMs) [19, 33, 47, 39].", "startOffset": 229, "endOffset": 245}, {"referenceID": 43, "context": "Participants\u2019 results [5] evidenced the effectiveness of signal enhancement approaches, mostly based on \u201cbeamforming\u201d, combined with the use of hybrid acoustic models based on deep neural networks hidden Markov models (DNN-HMMs) [19, 33, 47, 39].", "startOffset": 229, "endOffset": 245}, {"referenceID": 35, "context": "Participants\u2019 results [5] evidenced the effectiveness of signal enhancement approaches, mostly based on \u201cbeamforming\u201d, combined with the use of hybrid acoustic models based on deep neural networks hidden Markov models (DNN-HMMs) [19, 33, 47, 39].", "startOffset": 229, "endOffset": 245}, {"referenceID": 7, "context": "The effectiveness of acoustic modelling based on context-dependent DNN-HMMs was also demonstrated in several works dealing with applications spanning from mobile voice search [8] to the transcription of broadcast news and YouTube videos [19], conversational (at the telephone or in live scenarios) speech recognition [41] and ASR in noisy environments [42].", "startOffset": 175, "endOffset": 178}, {"referenceID": 17, "context": "The effectiveness of acoustic modelling based on context-dependent DNN-HMMs was also demonstrated in several works dealing with applications spanning from mobile voice search [8] to the transcription of broadcast news and YouTube videos [19], conversational (at the telephone or in live scenarios) speech recognition [41] and ASR in noisy environments [42].", "startOffset": 237, "endOffset": 241}, {"referenceID": 37, "context": "The effectiveness of acoustic modelling based on context-dependent DNN-HMMs was also demonstrated in several works dealing with applications spanning from mobile voice search [8] to the transcription of broadcast news and YouTube videos [19], conversational (at the telephone or in live scenarios) speech recognition [41] and ASR in noisy environments [42].", "startOffset": 317, "endOffset": 321}, {"referenceID": 38, "context": "The effectiveness of acoustic modelling based on context-dependent DNN-HMMs was also demonstrated in several works dealing with applications spanning from mobile voice search [8] to the transcription of broadcast news and YouTube videos [19], conversational (at the telephone or in live scenarios) speech recognition [41] and ASR in noisy environments [42].", "startOffset": 352, "endOffset": 356}, {"referenceID": 20, "context": "In [22], we observed a significant WER reduction on the CHiME-3 real test data by retraining the baseline DNN on the evaluation set itself and using the automatic transcriptions resulting from a first decoding pass to align acoustic observations with DNN outputs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In particular, the full DNN retraining step adopted in [22] is now substituted by a more sophisticated solution, which enhances the adaptation with effective instance weighing and selection criteria.", "startOffset": 55, "endOffset": 59}, {"referenceID": 51, "context": "At its core, our adaptation method is similar to the one described in [55], which adds to the objective function to optimize a regularization term based on the Kullback-Leibler divergence (KLD) between the original (non adapted) and the current DNN output distribution.", "startOffset": 70, "endOffset": 74}, {"referenceID": 31, "context": "In particular, building on the outcomes of previous research on automatic quality estimation for ASR [34, 23], we focus on two alternative solutions.", "startOffset": 101, "endOffset": 109}, {"referenceID": 21, "context": "In particular, building on the outcomes of previous research on automatic quality estimation for ASR [34, 23], we focus on two alternative solutions.", "startOffset": 101, "endOffset": 109}, {"referenceID": 18, "context": "To complete our analysis, the usefulness of the proposed QE-based adaptation method is verified not only with filter-bank features, but also with feature normalization via maximum likelihood linear regression (fMLLR) transformations, which characterize the best performing systems in the CHiME-3 challenge [20, 54] as well as the most recent baseline.", "startOffset": 306, "endOffset": 314}, {"referenceID": 50, "context": "To complete our analysis, the usefulness of the proposed QE-based adaptation method is verified not only with filter-bank features, but also with feature normalization via maximum likelihood linear regression (fMLLR) transformations, which characterize the best performing systems in the CHiME-3 challenge [20, 54] as well as the most recent baseline.", "startOffset": 306, "endOffset": 314}, {"referenceID": 31, "context": "\u2022 A new application of the ASR QE procedure described in [34] to predict the WER of automatic transcription hypotheses;", "startOffset": 57, "endOffset": 61}, {"referenceID": 51, "context": "\u2022 An extension of the KLD regularization approach for unsupervised DNN adaptation [55], which could be easily integrated in the KALDI speech recognition toolkit [38];", "startOffset": 82, "endOffset": 86}, {"referenceID": 5, "context": "Detailed overviews about the CHiME challenges of years 2011 (CHiME-1), 2013 (CHiME2) and 2015 (CHiME-3) can be respectively found in [6, 52, 5].", "startOffset": 133, "endOffset": 143}, {"referenceID": 48, "context": "Detailed overviews about the CHiME challenges of years 2011 (CHiME-1), 2013 (CHiME2) and 2015 (CHiME-3) can be respectively found in [6, 52, 5].", "startOffset": 133, "endOffset": 143}, {"referenceID": 4, "context": "Detailed overviews about the CHiME challenges of years 2011 (CHiME-1), 2013 (CHiME2) and 2015 (CHiME-3) can be respectively found in [6, 52, 5].", "startOffset": 133, "endOffset": 143}, {"referenceID": 20, "context": "Our submission [22] mostly focused on three aspects: i) the automatic selection of the best channel, ii) DNN retraining and iii) rescoring of word lattices with a linear combination of 4-grams LMs and RNNLMs.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "These are mostly based on the estimation of linear transformations of their input, output or hidden units [14, 1, 35, 28, 43].", "startOffset": 106, "endOffset": 125}, {"referenceID": 0, "context": "These are mostly based on the estimation of linear transformations of their input, output or hidden units [14, 1, 35, 28, 43].", "startOffset": 106, "endOffset": 125}, {"referenceID": 32, "context": "These are mostly based on the estimation of linear transformations of their input, output or hidden units [14, 1, 35, 28, 43].", "startOffset": 106, "endOffset": 125}, {"referenceID": 25, "context": "These are mostly based on the estimation of linear transformations of their input, output or hidden units [14, 1, 35, 28, 43].", "startOffset": 106, "endOffset": 125}, {"referenceID": 39, "context": "These are mostly based on the estimation of linear transformations of their input, output or hidden units [14, 1, 35, 28, 43].", "startOffset": 106, "endOffset": 125}, {"referenceID": 36, "context": "Feature discriminative linear regression (fDLR) [40] and output-features discriminative linear regression (oDLR) [53] are other approaches specifically investigated for DNN adaptation.", "startOffset": 48, "endOffset": 52}, {"referenceID": 49, "context": "Feature discriminative linear regression (fDLR) [40] and output-features discriminative linear regression (oDLR) [53] are other approaches specifically investigated for DNN adaptation.", "startOffset": 113, "endOffset": 117}, {"referenceID": 19, "context": "A variant of fDLR is described in [21], which proposes to adapt the DNN parameters within a maximum a posterior (MAP) framework.", "startOffset": 34, "endOffset": 38}, {"referenceID": 26, "context": "This approach demonstrated to be equivalent to L2 norm regularization [29] if the prior distribution of transformation weights is assumed to be Gaussian N (0, I).", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "An excellent review of \u201cconservative training\u201d approaches for artificial neural networks can be found in [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 51, "context": "In [55], the Kullback-Leibler divergence (KLD) between the original unadapted distribution of the DNN outputs and the related distribution estimated on the adaptation set is considered as regularization term.", "startOffset": 3, "endOffset": 7}, {"referenceID": 51, "context": "As reported in [55] this approach, also employed in our work, allowed obtaining significant WER reductions compared to fDLR transformation of the input features on two different tasks: voice search and lecture transcription.", "startOffset": 15, "endOffset": 19}, {"referenceID": 33, "context": "The use of fMLLR features in combination with hybrid DNN-HMMs has been studied in [36].", "startOffset": 82, "endOffset": 86}, {"referenceID": 44, "context": "An approach for unsupervised speaker adaptation of DNNs using fMLLR features is also reported in [48].", "startOffset": 97, "endOffset": 101}, {"referenceID": 11, "context": "In the context of speaker-adaptive training (SAT) via fMLLR [12], recently proposed approaches make use of i-vector [26] as speaker representation to perform acoustic feature normalization.", "startOffset": 60, "endOffset": 64}, {"referenceID": 23, "context": "In the context of speaker-adaptive training (SAT) via fMLLR [12], recently proposed approaches make use of i-vector [26] as speaker representation to perform acoustic feature normalization.", "startOffset": 116, "endOffset": 120}, {"referenceID": 29, "context": "In [32] an adaptation neural network is trained to convert i-vectors to speakerdependent linear shifts which, in turn, are used to generate speaker-normalized features for SAT-DNN training/decoding.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "The work reported in [25] proposes to incorporate prior statistics (derived from gender clustering of training data) into i-vectors estimation, showing significant perfomance improvements when the approach is used for DNN adaptation of a hybrid ASR system.", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "The automatic selection of training data for acoustic modelling in speech recognition has been previously addressed in the context of lightly supervised training [27] and active learning approaches [18, 10].", "startOffset": 162, "endOffset": 166}, {"referenceID": 16, "context": "The automatic selection of training data for acoustic modelling in speech recognition has been previously addressed in the context of lightly supervised training [27] and active learning approaches [18, 10].", "startOffset": 198, "endOffset": 206}, {"referenceID": 9, "context": "The automatic selection of training data for acoustic modelling in speech recognition has been previously addressed in the context of lightly supervised training [27] and active learning approaches [18, 10].", "startOffset": 198, "endOffset": 206}, {"referenceID": 34, "context": "The use of confidence measures for improving MLLR transformations has also been investigated by [37] in a German conversational speech recognition task.", "startOffset": 96, "endOffset": 100}, {"referenceID": 45, "context": "More recently, [49] proposed an automatic sentence selection method based on different types of confidence measures for the semi-supervised training of DNNs in a low-resource setting.", "startOffset": 15, "endOffset": 19}, {"referenceID": 28, "context": "The use of QE as a quality prediction method alternative to confidence estimation is inspired by previous research on QE for machine translation [31, 44, 50, 45].", "startOffset": 145, "endOffset": 161}, {"referenceID": 40, "context": "The use of QE as a quality prediction method alternative to confidence estimation is inspired by previous research on QE for machine translation [31, 44, 50, 45].", "startOffset": 145, "endOffset": 161}, {"referenceID": 46, "context": "The use of QE as a quality prediction method alternative to confidence estimation is inspired by previous research on QE for machine translation [31, 44, 50, 45].", "startOffset": 145, "endOffset": 161}, {"referenceID": 41, "context": "The use of QE as a quality prediction method alternative to confidence estimation is inspired by previous research on QE for machine translation [31, 44, 50, 45].", "startOffset": 145, "endOffset": 161}, {"referenceID": 31, "context": "In the ASR field it has been first proposed in [34].", "startOffset": 47, "endOffset": 51}, {"referenceID": 8, "context": "pass the dependency of confidence estimation on knowledge about the inner workings of the decoder that produces the transcriptions and, in turn, to avoid the risk of biased (often overestimated) quality estimates [9].", "startOffset": 213, "endOffset": 216}, {"referenceID": 31, "context": "In [34] ASR QE is explored as a supervised regression problem in which the WER of an utterance transcription has to be automatically predicted.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "The extensive experiments in different testing conditions discussed in [34] indicate that regression models based on Extremely Randomized Trees (XRT) [15] can achieve competitive performance, being able to outperform strong baselines and to approximate the true WER scores computed against reference transcripts.", "startOffset": 71, "endOffset": 75}, {"referenceID": 13, "context": "The extensive experiments in different testing conditions discussed in [34] indicate that regression models based on Extremely Randomized Trees (XRT) [15] can achieve competitive performance, being able to outperform strong baselines and to approximate the true WER scores computed against reference transcripts.", "startOffset": 150, "endOffset": 154}, {"referenceID": 42, "context": "In [46], our basic approach was refined in order to achieve robustness to large differences between training and test data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "In order to explore the possible applications of ASR QE, in [23] we proposed its use for successfully improving hypothesis combination with ROVER [11].", "startOffset": 60, "endOffset": 64}, {"referenceID": 10, "context": "In order to explore the possible applications of ASR QE, in [23] we proposed its use for successfully improving hypothesis combination with ROVER [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 51, "context": "In [55] the authors propose to use the Kullback-Leibler divergence between the original distribution and the adapted one as regularization term.", "startOffset": 3, "endOffset": 7}, {"referenceID": 51, "context": "As reported in [55], Equation 3 can be rewritten as follows:", "startOffset": 15, "endOffset": 19}, {"referenceID": 51, "context": "Experiments in [55] have shown a dependency of the optimal value of \u03b1 in Equation 5 on the size of the adaptation data.", "startOffset": 15, "endOffset": 19}, {"referenceID": 31, "context": "To obtain more objective and reliable sentence-level WER predictions, in [34] we proposed ASR quality estimation as a supervised regression method that effectively exploits a combination of \u201cglass-box\u201d and \u201cblack-box\u201d features.", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "In this paper, we trained XRT-based models [15] with a combination of 41 ASR (glassbox) and textual (black-box) features.", "startOffset": 43, "endOffset": 47}, {"referenceID": 27, "context": "The ASR features are extracted from the confusion network (CN) [30] derived from the word lattices generated by the ASR decoder (the one employed in this work is based on the KALDI toolkit [38]), while the textual features are the same of [34].", "startOffset": 63, "endOffset": 67}, {"referenceID": 31, "context": "The ASR features are extracted from the confusion network (CN) [30] derived from the word lattices generated by the ASR decoder (the one employed in this work is based on the KALDI toolkit [38]), while the textual features are the same of [34].", "startOffset": 239, "endOffset": 243}, {"referenceID": 18, "context": "The system is mainly based on the KALDI CHiME-3 v2 package (derived from the ASR system described in [20]) with the addition of a second decoding pass that performs unsupervised DNN adaptation as described in Section 3.", "startOffset": 101, "endOffset": 105}, {"referenceID": 20, "context": "In our submission to CHiME-3 [22], we reached the best performance on the evaluation set, et05 real, with a simple delay-and-sum (DS) beamforming consisting in uniform weighting of the rephased signals of the 5 frontal microphones.", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": "A similar approach, although based on the well known BeamformIt toolkit [4], is also included in the recent software package implementing the CHiME-3 baseline.", "startOffset": 72, "endOffset": 75}, {"referenceID": 47, "context": "The baseline DNN is trained using the Karel\u2019s setup [51] included in the KALDI toolkit.", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": "The initial GMM system makes use of the KALDI recipe associated to the earlier CHiME challenges [6, 52].", "startOffset": 96, "endOffset": 103}, {"referenceID": 48, "context": "The initial GMM system makes use of the KALDI recipe associated to the earlier CHiME challenges [6, 52].", "startOffset": 96, "endOffset": 103}, {"referenceID": 4, "context": "Complete details about this data set, the overall challenge and its outcomes can be found in the related overview paper [5], which also reports the performance of the 26 participating systems.", "startOffset": 120, "endOffset": 123}, {"referenceID": 50, "context": "In the analysis and in the subsequent discussion, our WER scores are not compared against those achieved by the best ASR system participating in the CHiME-3 challenge [54], which uses a far more complex architecture for signal pre-processing and cross system combination, as well as an augmented set of training data.", "startOffset": 167, "endOffset": 171}, {"referenceID": 50, "context": "Despite the generality of the proposed approach, integrating our method in a state-of-the-art ASR system like the one described in [54] and quantify the performance gains yielded by QE-based DNN adaptation is left as a possible direction for future work.", "startOffset": 131, "endOffset": 135}, {"referenceID": 0, "context": "In both cases, there is an intermediate optimal value of \u03b1 in the interval [0, 1], indicating that we should not totally trust neither the original model nor the adaptation data.", "startOffset": 75, "endOffset": 81}, {"referenceID": 20, "context": "These experiments were motivated by the significant performance improvement obtained in [22] using \u201cfull\u201d retraining of DNN in a two-pass ASR architecture.", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "Table 6: %WER achieved, in homogeneous conditions on ET05, with automatic data selection and using the baseline LM rescoring passes (see [20]).", "startOffset": 137, "endOffset": 141}, {"referenceID": 51, "context": "This is probably due to the fact that the size of the adaptation sets considered in our experiments is large enough to prevent data overfitting (actually, previous research on KLD regularization [55] demonstrates its effectiveness using only few minutes of adaptation data);", "startOffset": 195, "endOffset": 199}, {"referenceID": 49, "context": "Therefore, in order to assess the effectiveness and the general applicability of the proposed QE-based approach, we also experimented with the output-feature discriminative linear regression (oDLR) transformation, in a way similar to that described in [53].", "startOffset": 252, "endOffset": 256}, {"referenceID": 14, "context": "Such lower results can be explained by the findings reported in [16], in which the authors compared approaches based on MLLR and maximum a posterior probability (MAP) for GMM-HMMs adaptation.", "startOffset": 64, "endOffset": 68}, {"referenceID": 31, "context": "To this aim, building on previous positive results on quality estimation for ASR [34, 46, 23], we used automatic WER prediction as a criterion to isolate subsets of the adaptation data featuring variable quality.", "startOffset": 81, "endOffset": 93}, {"referenceID": 42, "context": "To this aim, building on previous positive results on quality estimation for ASR [34, 46, 23], we used automatic WER prediction as a criterion to isolate subsets of the adaptation data featuring variable quality.", "startOffset": 81, "endOffset": 93}, {"referenceID": 21, "context": "To this aim, building on previous positive results on quality estimation for ASR [34, 46, 23], we used automatic WER prediction as a criterion to isolate subsets of the adaptation data featuring variable quality.", "startOffset": 81, "endOffset": 93}, {"referenceID": 15, "context": "capable of modeling time dependencies among acoustic observations, such as bidirectional recurrent neural networks [17, 3]) having a higher number of parameters to adapt.", "startOffset": 115, "endOffset": 122}, {"referenceID": 2, "context": "capable of modeling time dependencies among acoustic observations, such as bidirectional recurrent neural networks [17, 3]) having a higher number of parameters to adapt.", "startOffset": 115, "endOffset": 122}], "year": 2016, "abstractText": "In this paper we propose to exploit the automatic Quality Estimation (QE) of ASR hypotheses to perform the unsupervised adaptation of a deep neural network modeling acoustic probabilities. Our hypothesis is that significant improvements can be achieved by: i) automatically transcribing the evaluation data we are currently trying to recognise, and ii) selecting from it a subset of \u201cgood quality\u201d instances based on the word error rate (WER) scores predicted by a QE component. To validate this hypothesis, we run several experiments on the evaluation data sets released for the CHiME-3 challenge. First, we operate in oracle conditions in which manual transcriptions of the evaluation data are available, thus allowing us to compute the true sentence WER. In this scenario, we perform the adaptation with variable amounts of data, which are characterised by different levels of quality. Then, we move to realistic conditions in which the manual transcriptions of the evaluation data are not available. In this case, the adaptation is performed on data selected according to the WER scores predicted by a QE component. Our results indicate that: i) QE predictions allow us to closely approximate the adaptation results obtained in oracle conditions, and ii) the overall ASR performance based on the proposed QE-driven adaptation method is significantly better than the strong, most recent, CHiME-3 baseline.", "creator": "gnuplot 4.2 patchlevel 6 "}}}