{"id": "1702.01802", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2017", "title": "Ensemble Distillation for Neural Machine Translation", "abstract": "Knowledge distillation describes a method for training a student network to perform better by learning from a stronger teacher network. In this work, we run experiments with different kinds of teacher net- works to enhance the translation performance of a student Neural Machine Translation (NMT) network. We demonstrate techniques based on an ensemble and a best BLEU teacher network. We also show how to benefit from a teacher network that has the same architecture and dimensions of the student network. Further- more, we introduce a data filtering technique based on the dissimilarity between the forward translation (obtained during knowledge distillation) of a given source sentence and its target reference. We use TER to measure dissimilarity. Finally, we show that an ensemble teacher model can significantly reduce the student model size while still getting performance improvements compared to the baseline student network.", "histories": [["v1", "Mon, 6 Feb 2017 21:49:12 GMT  (15kb)", "http://arxiv.org/abs/1702.01802v1", null], ["v2", "Tue, 8 Aug 2017 01:41:25 GMT  (19kb)", "http://arxiv.org/abs/1702.01802v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["markus freitag", "yaser al-onaizan", "baskaran sankaran"], "accepted": false, "id": "1702.01802"}, "pdf": {"name": "1702.01802.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["freitagm@us.ibm.com", "onaizan@us.ibm.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.01 802v 1 [cs.C L] 6F eb2 017"}, {"heading": "1 Introduction", "text": "Knowledge distillation describes the idea of improving a student network by aligning its predictions with those of a stronger teacher network. There are two possible ways to use Neural Machine Translation (NMT) knowledge distillation: First, the student network can be a model with fewer layers and / or hidden units. The main purpose of this is to reduce the model size of the NMT system without significantly reducing the translation quality. Second, you can make reasonable profits without changing the model architecture by combining different models of the same architecture with an ensemble. By using an ensemble, you also have the disadvantage of a much slower decoding speed. We show that the performance of a teacher consisting of an ensemble of 6 models can be achieved with a student composed of a single model, resulting in significantly faster decoding and a smaller memory imprint. In addition to the ensemble teaching network, we also examine a teacher network that produces multiple sets, such as the one that produces the best translations."}, {"heading": "2 Related Work", "text": "The results of eight test problems show that the loss of performance due to compression is usually negligible. (Ba and Caruana, 2014) show that flat networks can learn the complex functions previously learned through deep networks with knowledge distillation. On TIMIT Phone Recognition and CIFAR-10 Image Recognition tasks, flat networks can be trained that function similarly to deeper revolutionary models. (Hinton et al., 2015) present knowledge distillation for image classification (MNIST) and acoustic modeling. They show that almost all improvements achieved by forming an ensemble of deep neural networks can be distilled into a single neural network of the same size. (Kim and Rush, 2016) use knowledge distillation for NMT to reduce the model size of their neural network."}, {"heading": "3 Knowledge Distillation", "text": "The idea of knowledge distillation is to align the predictions of a student network with those of a teacher network. In this work, we collect the predictions of the teacher network by translating the complete training data with the teacher network. In this way, we create a new reference for the training data that can be used by the student network to simulate the teacher network. The idea is that the forward translation of the teacher network should be more accessible than the original reference. There are two ways to use forward translation: first, we can train the student network only from the original source and the translations; second, we can add the translations as additional training data to the original training data; and this has the side effect of doubling the final training data size of the student network."}, {"heading": "4 Teacher Networks", "text": "\u2022 Ensemble Teacher Model An ensemble of different NMT models can improve the translation performance of an NMT system. The idea is to train several models in parallel and combine their predictions by averaging the probabilities of each model at each step during decoding. \u2022 Best BLEU Teacher Model We use an ensemble of 6 models as a teacher model. All 6 individual systems are trained on the basis of the same parallel data and use the same optimization method. The only difference is the random initialization of the parameters. \u2022 Best BLEU Teacher Model We use a left-to-right beam detector to create new translations that aim to maximize the conditional probability of a given model. It stops the search when it has found a fixed number of hypotheses ending with a sequential ending symbol and selects the translation with the highest probability of logic from the final candidate list. In our distillation approach we produce the forward translation of our EU, as we know the maximum possible translation of all the sentences in the BL1."}, {"heading": "5 Data Filtering", "text": "In machine translation, bilingual pairs of sentences that serve as training data are usually crawled out of the web and contain many non-parallel pairs of sentences. In addition, a source sentence can have several correct translations that differ in word choice and word order. If the training corpus contains noisy pairs of sentences or sentences with several correct translations, the training network is complicated. In our knowledge distillation approach, we translate the complete parallel data with our teacher model. This allows us to evaluate each translation with the original reference. We remove sentences with high TER values (Snover et al., 2006) from our training data. By removing loud or unreachable pairs of sentences, the training algorithm is able to learn a stronger network."}, {"heading": "6 Experiments", "text": "We conduct our experiments using the German \u2192 English translation problem WMT 2016 (Bojar et al., 2016) (3.9 parallel sentences). Instead of words that reduce the vocabulary to 40k subword symbols for source and target (Sennrich et al., 2015), we use subword units that are extracted by byte pair encoding. We use an embedding dimension of 620 and fix the RNN-GRU layers to 1000 cells each. For the training process, we use SGD (Bishop, 1995) to update the model parameters with a mini-stack size of 64. Starting with the 4th epoch, we reduce the learning rate by half of each epoch. The training data is reshuffled after each epoch and we use beam 5 for all translations. All the different setups are executed twice: First, we train the student network randomly, secondly, we continue to build the final parameters."}, {"heading": "7 Results", "text": "By using forward translation, we can improve the model by 1.4 points in both BLEU and TER. Truncating the training data and using only pairs of sentences with a TER score of less than 0.8 results in a similar translation quality, while we reduce the training data by 12%, resulting in faster training. \u2022 Ensemble Teacher Model The results for using an ensemble of 6 models as a teacher model are summarized in Table 2. Using only forward translation, the individual system improves by 1.4 points in BLEU and 1.9 points in TER. Using both the original reference and forward translation, we get an additional 0.3 point improvement in BLEU."}, {"heading": "8 Conclusion", "text": "In this paper, we applied knowledge distillation to different types of teacher networks. First, we show how we can benefit from a teacher network that is of the same architecture as the student network. By combining forward translation and original reference, we get a 1.4 point improvement in BLEU. Using an ensemble model with 6 individual models as a teacher model further improves the translation quality of the student network. We showed how to trim the parallel data based on the TER values obtained with forward translations. Combining an ensemble teacher network and truncating all sentences with a TER value above 0.8 leads us to the best setup that improves the baseline by 2 points in BLEU and 2.2 points in TER. Using a teacher model based on the best set levels of BLEU translations improves the translation quality, but the results are slightly worse compared to the ensemble teacher model. Furthermore, we showed how to reduce the baseline by 2 points in BLEU and 2.2 points in TER. Using a teacher model based on the best set levels of BLEU translations improves the translation quality, but the results are slightly worse compared to the ensemble teacher model."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["Ba", "Caruana2014] Jimmy Ba", "Rich Caruana"], "venue": null, "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "and Y", "author": ["D. Bahdanau", "K. Cho"], "venue": "Bengio.", "citeRegEx": "Bahdanau et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural networks for pattern recognition", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q1995\\E", "shortCiteRegEx": "Bishop.", "year": 1995}, {"title": "Christof Monz", "author": ["Ondrej Bojar", "Rajen Chatterjee", "Christian Federmann", "Yvette Graham", "Barry Haddow", "Matthias Huck", "Antonio Jimeno Yepes", "Philipp Koehn", "Varvara Logacheva"], "venue": "et al.", "citeRegEx": "Bojar et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Rich Caruana", "author": ["Cristian Bucilu\u01ce"], "venue": "and Alexandru Niculescu-Mizil.", "citeRegEx": "Bucilu\u01ce et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Oriol Vinyals", "author": ["Geoffrey Hinton"], "venue": "and Jeff Dean.", "citeRegEx": "Hinton et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence-level knowledge distillation", "author": ["Kim", "Rush2016] Yoon Kim", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1606.07947", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics", "author": ["Lin", "Och2004] Chin-Yew Lin", "Franz Josef Och"], "venue": "In Proceedings of the 42nd Annual Meeting on Association", "citeRegEx": "Lin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2004}, {"title": "Barry Haddow", "author": ["Rico Sennrich"], "venue": "and Alexandra Birch.", "citeRegEx": "Sennrich et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Linnea Micciulla", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz"], "venue": "and John Makhoul.", "citeRegEx": "Snover et al.2006", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [], "year": 2017, "abstractText": "Knowledge distillation describes a method for training a student network to perform better by learning from a stronger teacher network. In this work, we run experiments with different kinds of teacher networks to enhance the translation performance of a student Neural Machine Translation (NMT) network. We demonstrate techniques based on an ensemble and a best BLEU teacher network. We also show how to benefit from a teacher network that has the same architecture and dimensions of the student network. Furthermore, we introduce a data filtering technique based on the dissimilarity between the forward translation (obtained during knowledge distillation) of a given source sentence and its target reference. We use TER to measure dissimilarity. Finally, we show that an ensemble teacher model can significantly reduce the student model size while still getting performance improvements compared to the baseline student", "creator": "LaTeX with hyperref package"}}}