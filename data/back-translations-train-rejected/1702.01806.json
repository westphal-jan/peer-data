{"id": "1702.01806", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2017", "title": "Beam Search Strategies for Neural Machine Translation", "abstract": "The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-to- right while keeping a fixed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the draw- back of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43% for the two language pairs German-English and Chinese-English without losing any translation quality.", "histories": [["v1", "Mon, 6 Feb 2017 22:08:46 GMT  (21kb)", "https://arxiv.org/abs/1702.01806v1", null], ["v2", "Wed, 14 Jun 2017 01:00:18 GMT  (27kb)", "http://arxiv.org/abs/1702.01806v2", "First Workshop on Neural Machine Translation, 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["markus freitag", "yaser al-onaizan"], "accepted": false, "id": "1702.01806"}, "pdf": {"name": "1702.01806.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["freitagm@us.ibm.com", "onaizan@us.ibm.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.01 806v 2 [cs.C L] 14 Jun 2017Translation (NMT) consists in training a large neural network that maximizes translation performance on a given parallel corpus. NMT then uses a simple left-left beam search decoder to generate new translations that roughly maximize the trained conditional probability. The current beam search strategy generates the target sentence word for word from the left beam while maintaining a fixed number of active candidates at each time step. First, this simple search is less adaptable as it also expands candidates whose scores are much worse than the current beams. Second, it does not expand hypotheses when they are not among the best candidates, even if their scores are close to the best. The latter can be avoided by increasing the bar size until no performance improvement can be observed. While you can achieve a better performance, it has the disadvantage of a slower key rate we are focusing on each of the two applicant countries in this English breakdown."}, {"heading": "1 Introduction", "text": "Due to the fact that Neural Machine Translation (NMT) has achieved comparable or even better performance compared to traditional statistical Machine Translation (SMT) models (Jean et al., 2015; Luong et al., 2015) in recent years (Kalchburner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014), attention has shifted towards a more practical translation. One of the challenges is the search strategy to obtain the best translation for a given source sentence. In NMT, new sentences are translated by a simple beam detector that finds a translation that approximately maximizes the conditional likelihood of a trained NMT model. The beam search strategy generates the translation word by word from left to right while exhibiting a fixed number (beam) of active candidates. By increasing the beam size, the translation performance can significantly increase the cost of the partial translation, without partially increasing the speed observed there."}, {"heading": "2 Related Work", "text": "The original beam search for sequences for sequence models was introduced and described by (Graves, 2012; Boulanger-Lewandowski et al., 2013) and by (Sutskever et al., 2014) for neural machine translation. (Hu et al., 2015; Mi et al., 2016) Improved beam search by using a constraint softmax function that took into account only a limited set of words from translation candidates to reduce computational complexity, with the advantage that they normalize only a small group of candidates, thereby improving decryption speed. (Wu et al., 2016) In this work, we only take into account tokens that have local values that are no more than below the best token during their search. Furthermore, the authors prune any sub-hypotheses whose score is smaller than the best final hypothesis (if one has already been established)."}, {"heading": "3 Original Beam Search", "text": "The original bar search strategy finds a translation that approximately maximizes the conditional probability given by a given model. It builds the translation from left to right and receives a fixed number (bars) of translation candidates with the highest log probability. For each end-of-sequence symbol that is selected among the candidates with the highest score, the bar is reduced by one and the translation is saved to a final candidate list. If the bar is zero, it terminates the search and selects the translation with the highest log probability (normalized by the number of target words) from the final candidate list."}, {"heading": "4 Search Strategies", "text": "In this section, we describe the various strategies we have experimented with. In all of our extensions, we first reduce the candidate list to the current bar size and apply this or more of the following circumcision schemes.Relative threshold circumcision. The relatively threshold circumcision method discards those candidates who are far inferior to the best active candidate. In the face of a circumcision threshold rp and an active candidate list C, candidates who are inferior to the best active candidate by a certain threshold are discarded. In the face of a circumcision threshold and an active candidate list C (c) (1) Absolute threshold circumcision. Instead of taking into account the relative difference in results, we simply discard those candidates who are inferior to the best active candidate by a certain threshold. In the face of a circumcision threshold and an active candidate list C, a candidate cand an active candidate list C are discarded if: Score (cand) max c).Score (c).Score (2)."}, {"heading": "5 Experiments", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "6 Conclusion", "text": "The original bar search decoder used in Neural Machine Translation is very simple. It generated translations from left to right while only looking at a fixed number (bars) of candidates from the last time step. By setting the bar size sufficiently large, we ensure that the best translation performance can be achieved, with the disadvantage that many candidates whose results are far from the best are also examined. In this essay, we have introduced several trimming techniques that trim candidates whose results are far from the best. By applying a combination of absolute and relative trimming programs, we speed up the decoder by up to 43% without losing the translation quality. More diversity in the decoders did not improve the translation quality."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "ArXiv e-prints .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Neural networks for pattern recognition", "author": ["Christopher M Bishop."], "venue": "Oxford university press.", "citeRegEx": "Bishop.,? 1995", "shortCiteRegEx": "Bishop.", "year": 1995}, {"title": "Audio chord recognition with recurrent neural networks", "author": ["Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent."], "venue": "ISMIR. Citeseer, pages 335\u2013340.", "citeRegEx": "Boulanger.Lewandowski et al\\.,? 2013", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2013}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1211.3711 .", "citeRegEx": "Graves.,? 2012", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Improved beam search with constrained softmax for nmt", "author": ["Haifeng Wang."], "venue": "Proceedings of MT Summit XV page 297.", "citeRegEx": "Wang.,? 2015", "shortCiteRegEx": "Wang.", "year": 2015}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of ACL. Beijing, China, pages 1\u201310.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Seattle.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of ACL. Beijing, China, pages 11\u201319.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Vocabulary manipulation for neural machine translation", "author": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "arXiv preprint arXiv:1605.03209 .", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909 .", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014,", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A systematic comparison of phrase table pruning techniques", "author": ["Richard Zens", "Daisy Stanton", "Peng Xu."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Zens et al\\.,? 2012", "shortCiteRegEx": "Zens et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "Due to the fact that Neural Machine Translation (NMT) is reaching comparable or even better performance compared to the traditional statistical machine translation (SMT) models (Jean et al., 2015; Luong et al., 2015), it has become very popular in the recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al.", "startOffset": 177, "endOffset": 216}, {"referenceID": 7, "context": "Due to the fact that Neural Machine Translation (NMT) is reaching comparable or even better performance compared to the traditional statistical machine translation (SMT) models (Jean et al., 2015; Luong et al., 2015), it has become very popular in the recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al.", "startOffset": 177, "endOffset": 216}, {"referenceID": 6, "context": ", 2015), it has become very popular in the recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 56, "endOffset": 135}, {"referenceID": 10, "context": ", 2015), it has become very popular in the recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 56, "endOffset": 135}, {"referenceID": 0, "context": ", 2015), it has become very popular in the recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 56, "endOffset": 135}, {"referenceID": 3, "context": "The original beam search for sequence to sequence models has been introduced and described by (Graves, 2012; Boulanger-Lewandowski et al., 2013) and by (Sutskever et al.", "startOffset": 94, "endOffset": 144}, {"referenceID": 2, "context": "The original beam search for sequence to sequence models has been introduced and described by (Graves, 2012; Boulanger-Lewandowski et al., 2013) and by (Sutskever et al.", "startOffset": 94, "endOffset": 144}, {"referenceID": 10, "context": ", 2013) and by (Sutskever et al., 2014) for neural machine translation.", "startOffset": 15, "endOffset": 39}, {"referenceID": 8, "context": "(Hu et al., 2015; Mi et al., 2016) improved the beam search with a constraint softmax function which only considered a limited", "startOffset": 0, "endOffset": 34}, {"referenceID": 11, "context": "phrase table pruning (Zens et al., 2012).", "startOffset": 21, "endOffset": 40}, {"referenceID": 0, "context": "In all our experiments, we use our in-house attention-based NMT implementation which is similar to (Bahdanau et al., 2014).", "startOffset": 99, "endOffset": 122}, {"referenceID": 9, "context": "For German\u2192English, we use sub-word units extracted by byte pair encoding (Sennrich et al., 2015) instead of words which shrinks the vocabulary to 40k sub-word symbols for both source and target.", "startOffset": 74, "endOffset": 97}, {"referenceID": 8, "context": "During translation, we use the alignments (from the attention mechanism) to replace the unknown tokens either with potential targets (obtained from an IBM Model-1 trained on the parallel data) or with the source word itself (if no target was found) (Mi et al., 2016).", "startOffset": 249, "endOffset": 266}, {"referenceID": 1, "context": "For the training procedure, we use SGD (Bishop, 1995) to update model parameters with a mini-batch size of 64.", "startOffset": 39, "endOffset": 53}], "year": 2017, "abstractText": "The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-toright beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-toright while keeping a fixed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the drawback of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43% for the two language pairs German\u2192English and Chinese\u2192English without losing any translation quality.", "creator": "gnuplot 5.0 patchlevel 2"}}}