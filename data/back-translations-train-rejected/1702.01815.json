{"id": "1702.01815", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2017", "title": "Living a discrete life in a continuous world: Reference with distributed representations", "abstract": "Reference is the crucial property of language that allows us to connect linguistic expressions to the world. Modeling it requires handling both continuous and discrete aspects of meaning. Data-driven models excel at the former, but struggle with the latter, and the reverse is true for symbolic models.", "histories": [["v1", "Mon, 6 Feb 2017 22:50:49 GMT  (1068kb,D)", "https://arxiv.org/abs/1702.01815v1", "Under review at ACL 2017. 6 pages, 1 figure"], ["v2", "Mon, 4 Sep 2017 08:44:28 GMT  (2276kb,D)", "http://arxiv.org/abs/1702.01815v2", "Accepted at IWCS 2017. Final version, 9 pages"]], "COMMENTS": "Under review at ACL 2017. 6 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["gemma boleda", "sebastian pad\\'o", "nghia the pham", "marco baroni"], "accepted": false, "id": "1702.01815"}, "pdf": {"name": "1702.01815.pdf", "metadata": {"source": "CRF", "title": "Living a discrete life in a continuous world: Reference in cross-modal entity tracking", "authors": ["Gemma Boleda", "Sebastian Pad\u00f3", "Nghia The Pham", "Marco Baroni"], "emails": ["gemma.boleda@upf.edu,", "pado@ims.uni-stuttgart.de,", "thenghia.pham@unitn.it,", "mbaroni@fb.com"], "sections": [{"heading": null, "text": "This paper presents (a) a concrete reference task for testing both aspects, called cross-modal entity tracking; (b) proposes an architecture of neural networks that uses external memory to build an entity library inspired by the DRS of DRT, with a mechanism to dynamically introduce new references or add information to references already in the library. Our model is promising: it beats traditional neural network architectures in this task, but it is still outperformed by Memory Networks, another model with external memory."}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to abide by the rules they have established with regard to their country."}, {"heading": "2 Cross-modal Entity Tracking: Task and Data", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far, until it is so far."}, {"heading": "3 The DIRE Model", "text": "The core novelty of our model, DIRE (for DIstributed model of REference), is a method for the dynamic construction of an entity library, conceptually inspired in 1) the DRS of DRT, 4 and 2) Joulin and Mikolov1Available at http: / / www.ims.uni-stuttgart.de / research / resources / corpora / dire. 2http: / / imagenet.stanford.edu 3For the sake of simplicity, we use the basic form of verbs and not the participle of the past. 4DRS represent many types of information; as explained above, we focus here on entity-related information. (2015) and Graves et al. (2016), which simulate discrete memory building operations in a differentiated continuous section.5 The model is a feed-forward network enhanced by a dynamic memory (the entity library), as well as mechanisms for interacting with it."}, {"heading": "3.1 Building the DIRE Entity Library", "text": "The first Exposition-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector"}, {"heading": "3.2 Cross-modal Entity Tracking with DIRE", "text": "This year, as never before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country,"}, {"heading": "4 Experiments", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "5 Discussion", "text": "We have reported on working on such a model, DIRE, which unlike memory networks, and emulates formal approaches such as DRT within an end-to-end neural architecture, is designed to make decisions about how to store the information in the input processing time, in a way that supports further reasoning, namely organizing by unit. The results suggest that the merging of complementary aspects of DIRE and MemN could be fruitful. We have also presented a new task, the intermodal tracking of units that test the categorization and individualization capabilities of computational models, and a challenging dataset for the task. Our project covers several areas of active research. Reference is a classic topic in language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 2015)."}], "references": [{"title": "Reference", "author": ["B. Abbott"], "venue": "Oxford, UK: Oxford University Press.", "citeRegEx": "Abbott,? 2010", "shortCiteRegEx": "Abbott", "year": 2010}, {"title": "VQA: Visual Question Answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "Proceedings of ICCV, Santiago de Chile, Chile.", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Show me the cup: Reference with continuous representations", "author": ["M. Baroni", "G. Boleda", "S. Pad\u00f3"], "venue": "Proceedings of CICLing (International Conference on Computational Linguistics and Intelligent Text Processing).", "citeRegEx": "Baroni et al\\.,? 2017", "shortCiteRegEx": "Baroni et al\\.", "year": 2017}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["M. Baroni", "G. Dinu", "G. Kruszewski"], "venue": "context-predicting semantic vectors. In Proceedings of ACL, Baltimore, MD, pp. 238\u2013247.", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Distributional Memory: A general framework for corpus-based semantics", "author": ["M. Baroni", "A. Lenci"], "venue": "Computational Linguistics 36(4), 673\u2013721.", "citeRegEx": "Baroni and Lenci,? 2010", "shortCiteRegEx": "Baroni and Lenci", "year": 2010}, {"title": "Ad hoc categories", "author": ["L.W. Barsalou"], "venue": "Memory & Cognition 11(3), 211\u2013227.", "citeRegEx": "Barsalou,? 1983", "shortCiteRegEx": "Barsalou", "year": 1983}, {"title": "Formal distributional semantics: Introduction to the special issue", "author": ["G. Boleda", "A. Herbelot"], "venue": "Computational Linguistics 42(4), 619\u2013635.", "citeRegEx": "Boleda and Herbelot,? 2016", "shortCiteRegEx": "Boleda and Herbelot", "year": 2016}, {"title": "Wide-coverage semantic analysis with Boxer", "author": ["J. Bos"], "venue": "Proceedings of the 2008 Conference on Semantics in Text Processing, pp. 277\u2013286. Association for Computational Linguistics.", "citeRegEx": "Bos,? 2008", "shortCiteRegEx": "Bos", "year": 2008}, {"title": "Distributional semantics in technicolor", "author": ["E. Bruni", "G. Boleda", "M. Baroni", "N.K. Tran"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, Jeju Island, Korea, pp. 136\u2013145.", "citeRegEx": "Bruni et al\\.,? 2012", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Concreteness ratings for 40 thousand generally known English word lemmas", "author": ["M. Brysbaert", "A.B. Warriner", "V. Kuperman"], "venue": "Behavior Research Methods 46, 904\u2013911.", "citeRegEx": "Brysbaert et al\\.,? 2014", "shortCiteRegEx": "Brysbaert et al\\.", "year": 2014}, {"title": "Learning to interpret natural language navigation instructions from observations", "author": ["D. Chen", "R. Mooney"], "venue": "Proceedings of AAAI, San Francisco, CA, pp. 859\u2013865.", "citeRegEx": "Chen and Mooney,? 2011", "shortCiteRegEx": "Chen and Mooney", "year": 2011}, {"title": "\u00dcber Sinn und Bedeutung", "author": ["G. Frege"], "venue": "Zeitschrift f\u00fcr Philosophie und philosophische Kritik 100, 25\u201350.", "citeRegEx": "Frege,? 1892", "shortCiteRegEx": "Frege", "year": 1892}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["A. Graves", "G. Wayne", "M. Reynolds", "T. Harley", "I. Danihelka", "A. Grabska-Barwi\u0144ska", "S. G\u00f3mez Colmenarejo", "E. Grefenstette", "T. Ramalho", "J. Agapiou", "A.P. Badia", "K. Moritz Hermann", "Y. Zwols", "G. Ostrovski", "A. Cain", "H. King", "C. Summerfield", "P. Blunsom", "K. Kavukcuoglu", "D. Hassabis"], "venue": "Nature 538(7626), 471\u2013476.", "citeRegEx": "Graves et al\\.,? 2016", "shortCiteRegEx": "Graves et al\\.", "year": 2016}, {"title": "Tracking the World State with Recurrent Neural Networks", "author": ["M. Henaff", "J. Weston", "A. Szlam", "A. Bordes", "Y. LeCun"], "venue": "https://arxiv.org/abs/1612.03969.", "citeRegEx": "Henaff et al\\.,? 2016", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Mr Darcy and Mr Toad, gentlemen: distributional names and their kinds", "author": ["A. Herbelot"], "venue": "Proceedings of the 11th International Conference on Computational Semantics, London, UK, pp. 151\u2013161. Association for Computational Linguistics.", "citeRegEx": "Herbelot,? 2015", "shortCiteRegEx": "Herbelot", "year": 2015}, {"title": "Building a shared world: mapping distributional to model-theoretic semantic spaces", "author": ["A. Herbelot", "E.M. Vecchi"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, pp. 22\u201332. Association for Computational Linguistics.", "citeRegEx": "Herbelot and Vecchi,? 2015", "shortCiteRegEx": "Herbelot and Vecchi", "year": 2015}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["A. Joulin", "T. Mikolov"], "venue": "Proceedings of NIPS, Montreal, Canada.", "citeRegEx": "Joulin and Mikolov,? 2015", "shortCiteRegEx": "Joulin and Mikolov", "year": 2015}, {"title": "Entity Representations and Articulated Contexts: An Exploration of the Semantics and Pragmatics of Definite Noun Phrases", "author": ["H. Kamp"], "venue": "Ms. University of Stuttgart.", "citeRegEx": "Kamp,? 2015", "shortCiteRegEx": "Kamp", "year": 2015}, {"title": "From Discourse to Logic: Introduction to Model-theoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory", "author": ["H. Kamp", "U. Reyle"], "venue": "Dordrecht: Kluwer.", "citeRegEx": "Kamp and Reyle,? 1993", "shortCiteRegEx": "Kamp and Reyle", "year": 1993}, {"title": "Computational generation of referring expressions: A survey", "author": ["E. Krahmer", "K. Van Deemter"], "venue": "Computational Linguistics 38(1), 173\u2013218.", "citeRegEx": "Krahmer and Deemter,? 2012", "shortCiteRegEx": "Krahmer and Deemter", "year": 2012}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["A. Lazaridou", "N. Pham", "M. Baroni"], "venue": "Proceedings of NAACL, Denver, CO, pp. 153\u2013163.", "citeRegEx": "Lazaridou et al\\.,? 2015", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "The Big Book of Concepts", "author": ["G. Murphy"], "venue": "Cambridge, MA: MIT Press.", "citeRegEx": "Murphy,? 2002", "shortCiteRegEx": "Murphy", "year": 2002}, {"title": "Anaphora Resolution: Algorithms, Resources, and Applications", "author": ["M. Poesio", "R. Stuckardt", "Y. Versley"], "venue": "Springer. In press.", "citeRegEx": "Poesio et al\\.,? 2017", "shortCiteRegEx": "Poesio et al\\.", "year": 2017}, {"title": "Models of semantic representation with visual attributes", "author": ["C. Silberer", "V. Ferrari", "M. Lapata"], "venue": "Proceedings of ACL, Sofia, Bulgaria, pp. 572\u2013582.", "citeRegEx": "Silberer et al\\.,? 2013", "shortCiteRegEx": "Silberer et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Proceedings of ICLR Conference Track, San Diego, CA. Published online: http://www.iclr.cc/doku.php?id=iclr2015:main.", "citeRegEx": "Simonyan and Zisserman,? 2015", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Tensor product variable binding and the representation of symbolic structures in connectionist networks", "author": ["P. Smolensky"], "venue": "Artificial Intelligence 46, 159\u2013216.", "citeRegEx": "Smolensky,? 1990", "shortCiteRegEx": "Smolensky", "year": 1990}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems 28, pp. 2440\u20132448. Montral, Canada.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Not exactly: In praise of vagueness", "author": ["K. Van Deemter"], "venue": "Oxford University Press.", "citeRegEx": "Deemter,? 2012", "shortCiteRegEx": "Deemter", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "Language combines discrete and continuous facets, as exemplified by the phenomenon of reference (Frege, 1892; Abbott, 2010): When we refer to an object in the world with the noun phrase the mug I bought, we use content words such as mug, which are notoriously fuzzy or vague in their meaning (Van Deemter, 2012; Murphy, 2002) and are best modeled through continuous means (Boleda and Herbelot, 2016).", "startOffset": 96, "endOffset": 123}, {"referenceID": 0, "context": "Language combines discrete and continuous facets, as exemplified by the phenomenon of reference (Frege, 1892; Abbott, 2010): When we refer to an object in the world with the noun phrase the mug I bought, we use content words such as mug, which are notoriously fuzzy or vague in their meaning (Van Deemter, 2012; Murphy, 2002) and are best modeled through continuous means (Boleda and Herbelot, 2016).", "startOffset": 96, "endOffset": 123}, {"referenceID": 21, "context": "Language combines discrete and continuous facets, as exemplified by the phenomenon of reference (Frege, 1892; Abbott, 2010): When we refer to an object in the world with the noun phrase the mug I bought, we use content words such as mug, which are notoriously fuzzy or vague in their meaning (Van Deemter, 2012; Murphy, 2002) and are best modeled through continuous means (Boleda and Herbelot, 2016).", "startOffset": 292, "endOffset": 325}, {"referenceID": 6, "context": "Language combines discrete and continuous facets, as exemplified by the phenomenon of reference (Frege, 1892; Abbott, 2010): When we refer to an object in the world with the noun phrase the mug I bought, we use content words such as mug, which are notoriously fuzzy or vague in their meaning (Van Deemter, 2012; Murphy, 2002) and are best modeled through continuous means (Boleda and Herbelot, 2016).", "startOffset": 372, "endOffset": 399}, {"referenceID": 5, "context": "The ability to categorize, that is, to recognize that different entities are equivalent with regard to some concept of interest (e.g. two mugs, two instances of the \u201cthings to take on a camping trip\u201d category; Barsalou, 1983).", "startOffset": 128, "endOffset": 225}, {"referenceID": 6, "context": "Data-driven, continuous models are very good at categorizing, but not at individuating, and the reverse holds for symbolic models (Boleda and Herbelot, 2016).", "startOffset": 130, "endOffset": 157}, {"referenceID": 18, "context": "Our long-term research goal is to build a continuous computational model of reference that emulates discrete referential mechanisms such as those defined in DRT (Kamp and Reyle, 1993); here we present initial work towards that goal, with two specific contributions.", "startOffset": 161, "endOffset": 183}, {"referenceID": 18, "context": "In DRT terms (Kamp and Reyle, 1993), each entity exposure either introduces a new discourse referent or updates the representation of an old referent with new information.", "startOffset": 13, "endOffset": 35}, {"referenceID": 16, "context": "DIRE uses the concept of external memory from deep learning (Joulin and Mikolov, 2015; Graves et al., 2016) to build an entity library for an exposure sequence that conceptually corresponds to the set of DRT discourse referents, using similarity-based reasoning on distributed representations to decide between aggregating and initializing entity representations.", "startOffset": 60, "endOffset": 107}, {"referenceID": 12, "context": "DIRE uses the concept of external memory from deep learning (Joulin and Mikolov, 2015; Graves et al., 2016) to build an entity library for an exposure sequence that conceptually corresponds to the set of DRT discourse referents, using similarity-based reasoning on distributed representations to decide between aggregating and initializing entity representations.", "startOffset": 60, "endOffset": 107}, {"referenceID": 7, "context": "In contrast to symbolic implementations of DRT (Bos, 2008), which manipulate discourse referents on the basis of manually specified algorithms, DIRE learns to make these decisions directly from observing reference acts using end-to-end training.", "startOffset": 47, "endOffset": 58}, {"referenceID": 1, "context": ", 2017, for a recent survey), but focuses on identifying language-external objects from images rather than mentions of a referent in text; to Visual Question Answering (Antol et al., 2015), but it cannot be solved with visual information alone; and to Referring Expression Generation (Krahmer and Van Deemter, 2012), but involves identification rather than generation.", "startOffset": 168, "endOffset": 188}, {"referenceID": 20, "context": "1 It is assembled on the basis of 2k object categories with 50 ImageNet2 images each, sampled from a larger dataset (Lazaridou et al., 2015).", "startOffset": 116, "endOffset": 140}, {"referenceID": 4, "context": "We build a set of linguistic attributes for each object by first extracting the 500 most associated, and thus plausible, syntactic neighbors for the category according to the DM resource (Baroni and Lenci, 2010).", "startOffset": 187, "endOffset": 211}, {"referenceID": 4, "context": "We build a set of linguistic attributes for each object by first extracting the 500 most associated, and thus plausible, syntactic neighbors for the category according to the DM resource (Baroni and Lenci, 2010). This excludes nonsensical combinations such as repair:dog. We further retain only (relatively) abstract verbs taking the target item as direct object.3 This is because (a) concrete verbs are likely to have strong visual correlates that could conflict with the image (cf. walk dog); and (b) referential expressions routinely successfully mix concrete and abstract cues (e.g., the dog I own). We remove all verbs with a score over 2.5 (on a 1\u20135 scale) in the concreteness norms of Brysbaert et al. (2014). We then construct each sequence as follows.", "startOffset": 188, "endOffset": 716}, {"referenceID": 12, "context": "(2015) and Graves et al. (2016), who simulate discrete memory-building operations in a differentiable continuous setup.", "startOffset": 11, "endOffset": 32}, {"referenceID": 16, "context": "The entity library is updated by \u201csoft insertion\u201d (Joulin and Mikolov, 2015) of the current exposure vector ui into the library.", "startOffset": 50, "endOffset": 76}, {"referenceID": 13, "context": "Concretely, we add the vector to each entity in the library, weighted by the While we developed DIRE, Henaff et al. (2016) proposed a similar architecture; we leave a comparison to future work.", "startOffset": 102, "endOffset": 123}, {"referenceID": 12, "context": "However, we expect those inserted for exposures of old entities (that is, when pold i \u2248 1) to be near zero, and removable from the library along the lines of Graves et al. (2016).", "startOffset": 158, "endOffset": 179}, {"referenceID": 26, "context": "To select the best entity match for the query, we compute a \u201csoft retrieval\u201d operation inspired by Sukhbaatar et al. (2015). To query the entity library, we first map the query (a linguistic referring expression, consisting of one noun and two attributes) to multimodal space.", "startOffset": 99, "endOffset": 124}, {"referenceID": 7, "context": "8 At the same time, it emulates discretelike operations like insertion and retrieval of entity representations, that, in frameworks such as DRT, are performed entirely in symbolic terms, and are manually coded in the DRT system Boxer (Bos, 2008).", "startOffset": 234, "endOffset": 245}, {"referenceID": 24, "context": "Images are represented by 4096-dimensional vectors produced by passing images through the pre-trained VGG 19-layer CNN of Simonyan and Zisserman (2015) (trained on the ILSVRC2012 data), and extracting the corresponding activations on the topmost fully connected layer.", "startOffset": 122, "endOffset": 152}, {"referenceID": 26, "context": "We also implement the related Memory Network model (MemN; Sukhbaatar et al., 2015).", "startOffset": 51, "endOffset": 82}, {"referenceID": 2, "context": "representations are given by 400-dimensional cbow embeddings from Baroni et al. (2014), trained on about 2.", "startOffset": 66, "endOffset": 87}, {"referenceID": 11, "context": "Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al.", "startOffset": 71, "endOffset": 132}, {"referenceID": 0, "context": "Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al.", "startOffset": 71, "endOffset": 132}, {"referenceID": 18, "context": "Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al.", "startOffset": 71, "endOffset": 132}, {"referenceID": 17, "context": "Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al.", "startOffset": 71, "endOffset": 132}, {"referenceID": 25, "context": "Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al.", "startOffset": 263, "endOffset": 306}, {"referenceID": 16, "context": "Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al.", "startOffset": 263, "endOffset": 306}, {"referenceID": 2, "context": "Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al., 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al.", "startOffset": 345, "endOffset": 409}, {"referenceID": 14, "context": "Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al., 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al.", "startOffset": 345, "endOffset": 409}, {"referenceID": 15, "context": "Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al., 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al.", "startOffset": 345, "endOffset": 409}, {"referenceID": 10, "context": ", 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al., 2012; Silberer et al., 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al.", "startOffset": 85, "endOffset": 151}, {"referenceID": 8, "context": ", 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al., 2012; Silberer et al., 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al.", "startOffset": 85, "endOffset": 151}, {"referenceID": 23, "context": ", 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al., 2012; Silberer et al., 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al.", "startOffset": 85, "endOffset": 151}, {"referenceID": 22, "context": ", 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al., 2017) are important subjects in Computational Linguistics.", "startOffset": 47, "endOffset": 99}], "year": 2017, "abstractText": "Reference is a crucial property of language that allows us to connect linguistic expressions to the world. Modeling it requires handling both continuous and discrete aspects of meaning. Data-driven models excel at the former, but struggle with the latter, and the reverse is true for symbolic models. This paper (a) introduces a concrete referential task to test both aspects, called cross-modal entity tracking; (b) proposes a neural network architecture that uses external memory to build an entity library inspired in the DRSs of DRT, with a mechanism to dynamically introduce new referents or add information to referents that are already in the library. Our model shows promise: it beats traditional neural network architectures on the task. However, it is still outperformed by Memory Networks, another model with external memory.", "creator": "LaTeX with hyperref package"}}}