{"id": "1702.01923", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "Comparative Study of CNN and RNN for Natural Language Processing", "abstract": "Deep neural networks (DNN) have revolutionized the field of natural language processing (NLP). Convolutional neural network (CNN) and recurrent neural network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state of the art on many NLP tasks often switches due to the battle between CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.", "histories": [["v1", "Tue, 7 Feb 2017 08:33:35 GMT  (88kb,D)", "http://arxiv.org/abs/1702.01923v1", "7 pages, 11 figures"]], "COMMENTS": "7 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenpeng yin", "katharina kann", "mo yu", "hinrich sch\\\"utze"], "accepted": false, "id": "1702.01923"}, "pdf": {"name": "1702.01923.pdf", "metadata": {"source": "CRF", "title": "Comparative Study of CNN and RNN for Natural Language Processing", "authors": ["Wenpeng Yin", "Katharina Kann", "Mo Yu", "Hinrich Sch\u00fctze"], "emails": ["wenpeng@cis.lmu.de,", "kann@cis.lmu.de,", "yum@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "There are two major DNN architectures: Convolutionary Neural Network (CNN) (LeCun et al., 1998) and Recursive Neural Network (RNN) (Elman, 1990). Gating mechanisms have been developed to mitigate some of the limitations of the basic RNN, leading to two dominant types of RNN: Long-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRU). Generally speaking, CNN's are hierarchical and RNN sequence architectures. How should we choose between them for language processing? Based on the characterization of hierarchical (CNN) and sequential (RNN) sequences."}, {"heading": "2 Related Work", "text": "To our knowledge, there has been no systematic comparison of CNN and RNN across a wide range of NLP tasks. Vu et al. (2016) investigate CNN and basic RNN (i.e., no gating mechanisms) for classifying relationships. They report a higher performance of CNN than RNN and provide evidence that CNN and RNN provide complementary information: While the RNN calculates a weighted combination of all words in a sentence, CNN extracts the most informative programs for the relationship and only takes into account their resulting activations.ar Xiv: 170 2.01 923v 1 [cs.C L] 7F eb2 017Both Wen et al. (2016) and Nobility and Protectors (2017) support CNN via GRU / LSTM for classifying long judgments. In addition, Yin et al. (2016) achieve better performance of attention-based CNN tasks as an attention-based LSTM for selecting answers."}, {"heading": "3 Models", "text": "This section gives a brief introduction to CNN, GRU and LSTM."}, {"heading": "3.1 Convolutional Neural Network (CNN)", "text": "Each entry is represented by a dense d-dimensional vector; therefore, the input layer x is used as a characteristic map of dimensionality d \u00b7 n. Figure 1 (a) shows the input layer as the lower rectangle with several columns. The folding layer is used to represent what is learned from moving W-Grams. In an input sequence with n entries: x1, x2,.., xn, leave the vector ci-Rwd the concatenated embedding of the w entries xi \u2212 w + 1,., xi where w is the filter width and 0 < i < s + w. Embedding for xi, i < 1 or i > n are filled to zero. We then create the representation pi-Rd for the w-gram xi \u2212 w + 1,., xi with the convolution weights W < Rd \u00d7 wd: ph \u00b7 h \u00b7 w \u00b7 b (1 \u00b7 b), where (tanw \u00b7 b = 1)"}, {"heading": "3.2 Gated Recurrent Unit (GRU)", "text": "As shown in Figure 1 (b), GRU models text x as follows: z = \u03c3 (xtU z + ht \u2212 1W z) (2) r = \u03c3 (xtU r + ht \u2212 1W r) (3) st = tanh (xtU s + (ht \u2212 1 \u0445 r) Ws) (4) ht = (1 \u2212 z) \u0445 st + z - ht \u2212 1 (5) xt - Rd represents the symbol in x at position t, ht Rh is the hidden state on t that should encode the history x1, \u00b7, xt. z and r. All U characters Rd \u00b7 h, W Rh \u00b7 h are parameters."}, {"heading": "3.3 Long Short-Time Memory (LSTM)", "text": "LSTM is referred to in Figure 1 (c). It models the word sequence x as follows: it = \u03c3 (xtU i + ht \u2212 1W i + bi) (6) ft = \u03c3 (xtU f + ht \u2212 1W f + bf) (7) ot = \u03c3 (xtU o + ht \u2212 1W o + bo) (8) qt = tanh (xtU q + ht \u2212 1W q + bq) (9) pt = ft; pt \u2212 1 + it; qt (10) ht = ot; tanh (pt) (11) LSTM has three gates: entrance gate it, gate ft forgotten and exit gate ot. All gates are ht \u2212 1 by a sigmoid function over the entire ensemble of input text and previous hidden state. To generate the hidden state at current step t, it first generates a temporary result qt by a tanh nonlinearity over the entire ensemble of input text and previous hidden state \u2212 1, then combines it with a hidden result \u2212 1."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Tasks", "text": "This data collection predicts the mood (positive or negative) of film ratings. We use the given principles of 6920 train, 872 dev and 1821 test sets. As in (Kalchbrenner et al., 2014; Le and Mikolov, 2014), we treat phrases described as subsets of training sets as independent training instances. Measurement: Accuracy of Relation Classification (RC) on SemEval 2010 task 8 (Hendrickx et al., 2009). It consists of sentences manually labeled with 19 relationships (18 directed relationships and others), 8000 sets in train and 2717 in test. Since there is no dev set, we use 1500 training examples as dev, similar to Vu et al. (2016)."}, {"heading": "4.2 Experimental Setup", "text": "To study the coding capability of various basic DNNs fairly, our experiments have the following design. (i) Always train from the ground up, with no additional knowledge, such as no pre-built word embeddings. (ii) Always train with a basic setup without complex tricks such as batch normalization. (iii) Look for optimal hyperparameters for each task and model separately, so that all results are based on optimal hyperparameters. (iv) Examine the basic architecture and usage of each model: CNN consists of a folding layer and a max pooling layer; GRU and LSTM model the input from left to right and always use the last hidden state as the final representation of the input. An exception is POS tagging, we also report bidirectional RNNs, as this can ensure that the representation of each word can encode in the context of both sides, as the CNN documents are maximum set parameters, and CNN Sentence Size: Qdev."}, {"heading": "4.3 Results & Analysis", "text": "In fact, most people who live in the United States live in the United States, not only in the United States, but also in the United States, in Europe, and in other parts of the world. Most of them were born and raised in the United States, but most of them were born and raised in the United States. Most of them were born and raised in the United States. Most of them are born and raised in the United States, where they grew up. Most of them are born and raised in the United States, where they grew up. Most of them are born and raised in the United States. Most of them are born and raised in the United States. Most of them are in the United States, where they grew up. Most of them are in the United States, where they grew up. Most of them are in the United States, but also in the United States, where they grew up. Most of them are in the United States, where they live in the United States. Most of them are in the United States, where they grew up."}, {"heading": "5 Conclusions", "text": "This work compared the three most commonly used DNNs - CNN, GRU, and LSTM - in a representative sample of NLP tasks. We found that RNNs perform well and robustly across a wide range of tasks, except when the task is essentially a keyword recognition task, as in some sentiment detection and question-answer matching settings. In addition, hidden size and batch size can dramatically vary DNN performance, suggesting that optimizing these two parameters is critical to good performance of both CNNs and RNNNs."}], "references": [{"title": "Exploring different dimensions of attention for uncertainty detection", "author": ["Heike Adel", "Hinrich Sch\u00fctze."], "venue": "Proceedings of EACL.", "citeRegEx": "Adel and Sch\u00fctze.,? 2017", "shortCiteRegEx": "Adel and Sch\u00fctze.", "year": 2017}, {"title": "Comparison of neural network architectures for sentiment analysis of russian tweets", "author": ["K. Arkhipenko", "I. Kozlov", "J. Trofimovich", "K. Skorniakov", "A. Gomzin", "D. Turdakov."], "venue": "Proceedings of \u201cDialogue 2016\u201d.", "citeRegEx": "Arkhipenko et al\\.,? 2016", "shortCiteRegEx": "Arkhipenko et al\\.", "year": 2016}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "Proceedings of EMNLP. pages 120\u2013128.", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning."], "venue": "Proceedings of EMNLP. pages 632\u2013642.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation .", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555 .", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Language modeling with gated convolutional networks", "author": ["Yann N Dauphin", "Angela Fan", "Michael Auli", "David Grangier."], "venue": "arXiv preprint arXiv:1612.08083 .", "citeRegEx": "Dauphin et al\\.,? 2016", "shortCiteRegEx": "Dauphin et al\\.", "year": 2016}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive Science 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Traversing knowledge graphs in vector space", "author": ["Kelvin Guu", "John Miller", "Percy Liang."], "venue": "Proceedings of EMNLP. pages 318\u2013327.", "citeRegEx": "Guu et al\\.,? 2015", "shortCiteRegEx": "Guu et al\\.", "year": 2015}, {"title": "Semeval-2010 task 8: Multi-way classification of semantic relations", "author": ["Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2009}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of ICML. pages 2342\u20132350.", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of ACL.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "Proceedings of ICML. pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner."], "venue": "Proceedings of the IEEE 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "Proceedings of NIPS. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Slav Petrov", "Ryan McDonald."], "venue": "Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL). volume 59.", "citeRegEx": "Petrov and McDonald.,? 2012", "shortCiteRegEx": "Petrov and McDonald.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings EMNLP. volume 1631, page", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu."], "venue": "Proceedings of EMNLP. pages 1422\u20131432.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Combining recurrent and convolutional neural networks for relation classification", "author": ["Ngoc Thang Vu", "Heike Adel", "Pankaj Gupta", "Hinrich Sch\u00fctze."], "venue": "Proceedings of NAACL HLT . pages 534\u2013539.", "citeRegEx": "Vu et al\\.,? 2016", "shortCiteRegEx": "Vu et al\\.", "year": 2016}, {"title": "Learning text representation using recurrent convolutional neural network with highway layers", "author": ["Ying Wen", "Weinan Zhang", "Rui Luo", "Jun Wang."], "venue": "SIGIR Workshop on Neural Information Retrieval .", "citeRegEx": "Wen et al\\.,? 2016", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Enhancing freebase question answering using textual evidence", "author": ["Kun Xu", "Yansong Feng", "Siva Reddy", "Songfang Huang", "Dongyan Zhao."], "venue": "CoRR abs/1603.00957.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen-tau Yih", "Christopher Meek."], "venue": "Proceedings of EMNLP. pages 2013\u20132018.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "The value of semantic parse labeling for knowledge base question answering", "author": ["Wen-tau Yih", "Matthew Richardson", "Chris Meek", "MingWei Chang", "Jina Suh."], "venue": "Proceedings of ACL. pages 201\u2013206.", "citeRegEx": "Yih et al\\.,? 2016", "shortCiteRegEx": "Yih et al\\.", "year": 2016}, {"title": "ABCNN: attention-based convolutional neural network for modeling sentence pairs", "author": ["Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou."], "venue": "TACL 4:259\u2013272.", "citeRegEx": "Yin et al\\.,? 2016", "shortCiteRegEx": "Yin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "There are two main DNN architectures: convolutional neural network (CNN) (LeCun et al., 1998) and recurrent neural network (RNN) (Elman, 1990).", "startOffset": 73, "endOffset": 93}, {"referenceID": 7, "context": ", 1998) and recurrent neural network (RNN) (Elman, 1990).", "startOffset": 43, "endOffset": 56}, {"referenceID": 10, "context": "Gating mechanisms have been developed to alleviate some limitations of the basic RNN, resulting in two prevailing RNN types: long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRU) (Cho et al.", "startOffset": 155, "endOffset": 189}, {"referenceID": 4, "context": "Gating mechanisms have been developed to alleviate some limitations of the basic RNN, resulting in two prevailing RNN types: long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRU) (Cho et al., 2014).", "startOffset": 221, "endOffset": 239}, {"referenceID": 18, "context": "For example, RNNs perform well on document-level sentiment classification (Tang et al., 2015); and Dauphin et al.", "startOffset": 74, "endOffset": 93}, {"referenceID": 4, "context": "Gating mechanisms have been developed to alleviate some limitations of the basic RNN, resulting in two prevailing RNN types: long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (GRU) (Cho et al., 2014). Generally speaking, CNNs are hierarchical and RNNs sequential architectures. How should we choose between them for processing language? Based on the characterization \u201chierarchical (CNN) vs. sequential (RNN)\u201d, it is tempting to choose a CNN for classification tasks like sentiment classification since sentiment is usually determined by some key phrases; and to choose RNNs for a sequence modeling task like language modeling as it requires flexible modeling of context dependencies. But current NLP literature does not support such a clear conclusion. For example, RNNs perform well on document-level sentiment classification (Tang et al., 2015); and Dauphin et al. (2016) recently showed that gated CNNs outperform LSTMs on language modeling tasks, even though LSTMs had long been seen as better suited.", "startOffset": 222, "endOffset": 914}, {"referenceID": 19, "context": "Vu et al. (2016) investigate CNN and basic RNN (i.", "startOffset": 0, "endOffset": 17}, {"referenceID": 15, "context": "(2016) compare word2vec (Mikolov et al., 2013), CNN, GRU and LSTM in sentiment analysis of Russian tweets, and find GRU outperforms LSTM and CNN.", "startOffset": 24, "endOffset": 46}, {"referenceID": 14, "context": "Both Wen et al. (2016) and Adel and Sch\u00fctze (2017) support CNN over GRU/LSTM for classification of long sentences.", "startOffset": 5, "endOffset": 23}, {"referenceID": 0, "context": "(2016) and Adel and Sch\u00fctze (2017) support CNN over GRU/LSTM for classification of long sentences.", "startOffset": 11, "endOffset": 35}, {"referenceID": 0, "context": "(2016) and Adel and Sch\u00fctze (2017) support CNN over GRU/LSTM for classification of long sentences. In addition, Yin et al. (2016) achieve better performance of attentionbased CNN than attention-based LSTM for answer selection.", "startOffset": 11, "endOffset": 130}, {"referenceID": 0, "context": "(2016) and Adel and Sch\u00fctze (2017) support CNN over GRU/LSTM for classification of long sentences. In addition, Yin et al. (2016) achieve better performance of attentionbased CNN than attention-based LSTM for answer selection. Dauphin et al. (2016) further argue that a fine-tuned gated CNN can also model longcontext dependency, getting new state-of-the-art in language modeling above all RNN competitors In contrast, Arkhipenko et al.", "startOffset": 11, "endOffset": 249}, {"referenceID": 0, "context": "(2016) and Adel and Sch\u00fctze (2017) support CNN over GRU/LSTM for classification of long sentences. In addition, Yin et al. (2016) achieve better performance of attentionbased CNN than attention-based LSTM for answer selection. Dauphin et al. (2016) further argue that a fine-tuned gated CNN can also model longcontext dependency, getting new state-of-the-art in language modeling above all RNN competitors In contrast, Arkhipenko et al. (2016) compare word2vec (Mikolov et al.", "startOffset": 11, "endOffset": 444}, {"referenceID": 0, "context": "(2016) and Adel and Sch\u00fctze (2017) support CNN over GRU/LSTM for classification of long sentences. In addition, Yin et al. (2016) achieve better performance of attentionbased CNN than attention-based LSTM for answer selection. Dauphin et al. (2016) further argue that a fine-tuned gated CNN can also model longcontext dependency, getting new state-of-the-art in language modeling above all RNN competitors In contrast, Arkhipenko et al. (2016) compare word2vec (Mikolov et al., 2013), CNN, GRU and LSTM in sentiment analysis of Russian tweets, and find GRU outperforms LSTM and CNN. In empirical evaluations, Chung et al. (2014) and Jozefowicz et al.", "startOffset": 11, "endOffset": 629}, {"referenceID": 0, "context": "(2016) and Adel and Sch\u00fctze (2017) support CNN over GRU/LSTM for classification of long sentences. In addition, Yin et al. (2016) achieve better performance of attentionbased CNN than attention-based LSTM for answer selection. Dauphin et al. (2016) further argue that a fine-tuned gated CNN can also model longcontext dependency, getting new state-of-the-art in language modeling above all RNN competitors In contrast, Arkhipenko et al. (2016) compare word2vec (Mikolov et al., 2013), CNN, GRU and LSTM in sentiment analysis of Russian tweets, and find GRU outperforms LSTM and CNN. In empirical evaluations, Chung et al. (2014) and Jozefowicz et al. (2015) found there is no clear winner between GRU and LSTM.", "startOffset": 11, "endOffset": 658}, {"referenceID": 17, "context": "1 Tasks Sentiment Classification (SentiC) on Stanford Sentiment Treebank (SST) (Socher et al., 2013).", "startOffset": 79, "endOffset": 100}, {"referenceID": 12, "context": "As in (Kalchbrenner et al., 2014; Le and Mikolov, 2014), we treat labeled phrases that occur as subparts of training sentences as independent training instances.", "startOffset": 6, "endOffset": 55}, {"referenceID": 13, "context": "As in (Kalchbrenner et al., 2014; Le and Mikolov, 2014), we treat labeled phrases that occur as subparts of training sentences as independent training instances.", "startOffset": 6, "endOffset": 55}, {"referenceID": 9, "context": "Relation Classification (RC) on SemEval 2010 task 8 (Hendrickx et al., 2009).", "startOffset": 52, "endOffset": 76}, {"referenceID": 3, "context": "Textual Entailment (TE) on Stanford Natural Language Inference (SNLI) (Bowman et al., 2015).", "startOffset": 70, "endOffset": 91}, {"referenceID": 22, "context": "Answer Selection (AS) on WikiQA (Yang et al., 2015), an open domain question-answer dataset.", "startOffset": 32, "endOffset": 51}, {"referenceID": 23, "context": "We utilize WebQSP (Yih et al., 2016) dataset to create a large-scale relation detection task, benefitting from the availability of labeled semantic parses of questions.", "startOffset": 18, "endOffset": 36}, {"referenceID": 2, "context": "We use the setup of (Blitzer et al., 2006; Petrov and McDonald, 2012): sections 2-21 are train, section 22 is dev and section 23 is test.", "startOffset": 20, "endOffset": 69}, {"referenceID": 16, "context": "We use the setup of (Blitzer et al., 2006; Petrov and McDonald, 2012): sections 2-21 are train, section 22 is dev and section 23 is test.", "startOffset": 20, "endOffset": 69}, {"referenceID": 6, "context": "Relation Classification (RC) on SemEval 2010 task 8 (Hendrickx et al., 2009). It consists of sentences which have been manually labeled with 19 relations (18 directed relations and Other), 8000 sentences in train and 2717 in test. As there is no dev set, we use 1500 training examples as dev, similar to Vu et al. (2016). Measure: F1.", "startOffset": 53, "endOffset": 321}, {"referenceID": 2, "context": "Textual Entailment (TE) on Stanford Natural Language Inference (SNLI) (Bowman et al., 2015). SNLI contains premise-hypothesis pairs, labeled with a relation (entailment, contradiction, neutral). After removing unlabeled pairs, we end up having 549,367 pairs for train, 9,842 for dev and 9,824 for test. Measure: accuracy. Answer Selection (AS) on WikiQA (Yang et al., 2015), an open domain question-answer dataset. We use the subtask that assumes that there is at least one correct answer for a question. The corresponding dataset consists of 20,360 questioncandidate pairs in train, 1,130 in dev and 2,352 in test where we adopt the standard setup of only considering questions with correct answers in test. The task is to choose the correct answer(s) from some candidates for a question. Measures: MAP and MRR. Question Relation Match (QRM). We utilize WebQSP (Yih et al., 2016) dataset to create a large-scale relation detection task, benefitting from the availability of labeled semantic parses of questions. For each question, we (i) select the topic entity from the parse; (ii) select all the relations/relation chains (length \u2264 2) connecting to the topic entity; and (iii) set the relations/relationchains in the labeled parse as positive and all the others as negative. Following Yih et al. (2016) and Xu et al.", "startOffset": 71, "endOffset": 1306}, {"referenceID": 2, "context": "Textual Entailment (TE) on Stanford Natural Language Inference (SNLI) (Bowman et al., 2015). SNLI contains premise-hypothesis pairs, labeled with a relation (entailment, contradiction, neutral). After removing unlabeled pairs, we end up having 549,367 pairs for train, 9,842 for dev and 9,824 for test. Measure: accuracy. Answer Selection (AS) on WikiQA (Yang et al., 2015), an open domain question-answer dataset. We use the subtask that assumes that there is at least one correct answer for a question. The corresponding dataset consists of 20,360 questioncandidate pairs in train, 1,130 in dev and 2,352 in test where we adopt the standard setup of only considering questions with correct answers in test. The task is to choose the correct answer(s) from some candidates for a question. Measures: MAP and MRR. Question Relation Match (QRM). We utilize WebQSP (Yih et al., 2016) dataset to create a large-scale relation detection task, benefitting from the availability of labeled semantic parses of questions. For each question, we (i) select the topic entity from the parse; (ii) select all the relations/relation chains (length \u2264 2) connecting to the topic entity; and (iii) set the relations/relationchains in the labeled parse as positive and all the others as negative. Following Yih et al. (2016) and Xu et al. (2016), we formulate this task as a sequence matching problem.", "startOffset": 71, "endOffset": 1327}, {"referenceID": 2, "context": "Textual Entailment (TE) on Stanford Natural Language Inference (SNLI) (Bowman et al., 2015). SNLI contains premise-hypothesis pairs, labeled with a relation (entailment, contradiction, neutral). After removing unlabeled pairs, we end up having 549,367 pairs for train, 9,842 for dev and 9,824 for test. Measure: accuracy. Answer Selection (AS) on WikiQA (Yang et al., 2015), an open domain question-answer dataset. We use the subtask that assumes that there is at least one correct answer for a question. The corresponding dataset consists of 20,360 questioncandidate pairs in train, 1,130 in dev and 2,352 in test where we adopt the standard setup of only considering questions with correct answers in test. The task is to choose the correct answer(s) from some candidates for a question. Measures: MAP and MRR. Question Relation Match (QRM). We utilize WebQSP (Yih et al., 2016) dataset to create a large-scale relation detection task, benefitting from the availability of labeled semantic parses of questions. For each question, we (i) select the topic entity from the parse; (ii) select all the relations/relation chains (length \u2264 2) connecting to the topic entity; and (iii) set the relations/relationchains in the labeled parse as positive and all the others as negative. Following Yih et al. (2016) and Xu et al. (2016), we formulate this task as a sequence matching problem. Ranking-loss is used for training. Measure: accuracy. Path Query Answering (PQA) on the path query dataset released by Guu et al. (2015). It contains KB paths like eh, r0, r1, \u00b7 \u00b7 \u00b7 , rt, et, where head entity eh and relation sequence r0, r1, \u00b7 \u00b7 \u00b7 , rt are encoded to predict the tail entity et.", "startOffset": 71, "endOffset": 1520}, {"referenceID": 3, "context": "This can also explain the phenomenon in SemMatch \u2013 GRU/LSTM surpass CNN in TE while CNN dominates in AS, as textual entailment relies on the comprehension of the whole sentence (Bowman et al., 2015), question-answer in AS instead can be effectively identified by key-phrase matching (Yin et al.", "startOffset": 177, "endOffset": 198}, {"referenceID": 24, "context": ", 2015), question-answer in AS instead can be effectively identified by key-phrase matching (Yin et al., 2016).", "startOffset": 92, "endOffset": 110}], "year": 2017, "abstractText": "Deep neural networks (DNNs) have revolutionized the field of natural language processing (NLP). Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting positioninvariant features and RNN at modeling units in sequence. The state-of-the-art on many NLP tasks often switches due to the battle of CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.", "creator": "LaTeX with hyperref package"}}}