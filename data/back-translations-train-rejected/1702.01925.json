{"id": "1702.01925", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "Effects of Stop Words Elimination for Arabic Information Retrieval: A Comparative Study", "abstract": "The effectiveness of three stop words lists for Arabic Information Retrieval---General Stoplist, Corpus-Based Stoplist, Combined Stoplist ---were investigated in this study. Three popular weighting schemes were examined: the inverse document frequency weight, probabilistic weighting, and statistical language modelling. The Idea is to combine the statistical approaches with linguistic approaches to reach an optimal performance, and compare their effect on retrieval. The LDC (Linguistic Data Consortium) Arabic Newswire data set was used with the Lemur Toolkit. The Best Match weighting scheme used in the Okapi retrieval system had the best overall performance of the three weighting algorithms used in the study, stoplists improved retrieval effectiveness especially when used with the BM25 weight. The overall performance of a general stoplist was better than the other two lists.", "histories": [["v1", "Tue, 7 Feb 2017 08:49:58 GMT  (549kb)", "http://arxiv.org/abs/1702.01925v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["ibrahim abu el-khair"], "accepted": false, "id": "1702.01925"}, "pdf": {"name": "1702.01925.pdf", "metadata": {"source": "META", "title": "Effects of Stop Words Elimination for AIR", "authors": ["Ibrahim Abu El-Khair"], "emails": ["iabuelkhair@gmail.com"], "sections": [{"heading": null, "text": "The idea is to combine the statistical approaches with linguistic approaches to achieve optimal performance and compare their impact on data retrieval. LDC's Arabic dataset (Linguistic Data Consortium) was used with the Lemur Toolkit. The best-match weight scheme used in the Okapi retrieval system had the best overall performance of the three weighting algorithms used in the study. Stoplists improved the effectiveness of data retrieval, especially when used with BM25 weight. Overall performance of a general stop list was better than that of the other two lists. Keywords: Arabic Information Retrieval, Stoplists, Lemur Toolkit."}, {"heading": "1. Introduction", "text": "Although most research in the field of information retrieval has focused on the English language, there has been a considerable amount of work and effort recently to develop information retrieval systems for languages other than English. Research and experimentation in the field of information retrieval in Arabic is relatively new and limited compared to research conducted in English, which has long been dominant in the field of information retrieval, despite the fact that Arabic is one of the five languages of the United Nations, the mother tongue of over 256 million people, and, because it is the language of the Koran, it is also the second language for many Muslims and Muslim countries around the world. [4] This study attempts to compare the use and effect of stopwords for Arabic information retrieval. Using the Lemur Toolkit, a language modeling and information retrieval method, the use of foreign words in other languages is stopped (see methodology for more details), multiple weighting, and three stoplists are implemented to determine the influence of three transmitters of information."}, {"heading": "2. Related Studies", "text": "Stopwords are very common words that appear in the text, which have little meaning; they serve only a syntactical function, but do not differ in content. These stopwords have two different effects on the information retrieval process; they can affect the effectiveness of the query because they are very frequent and tend to reduce the effects of frequency differences between less frequent words that affect the weighting process; the removal of the stopwords can also affect the length of the document and thus the evaluation process, as they carry no meaning, which can lead to a large amount of unproductive processing; the removal of words can increase the efficiency of the indexing process by 30 to 50% of the tokens in a large text collection. Stopwords [18] The identification of a stopwords list or a stopwords list that contain such words in order to eliminate them from word processing is essential for an information system. Stoplists can be divided into two categories; independent lists can be created and dependent lists can be created."}, {"heading": "3. Methodology", "text": "This study examines the use of stop words and their effect on Arabic information retrieval. It compares the use of three terminology weighting schemes and three stop lists. These techniques are examined using a large corpus that was not available at TREC before the introduction of Arabic Cross-Language Retrieval in 2001. Evaluation was carried out using the Lemur Toolkit with Arabic language capabilities.The study evaluates these techniques against the usual recall and precision metrics as a basis for comparison. It answers the following question: What effect do the stop lists have on retrieval, i.e. how sensitive is retrieval on the use of stop words; and which of the lists, the general one that is superior to the corpus-based or the combined list of the others? First, the performance of terminology weighting schemes without elimination of stop words was compared, and then combinations of weighting schemes and stop lists were executed."}, {"heading": "3.1. Data Set", "text": "This research used an Arabic test corpus created by the Linguistic Data Consortium in Philadelphia and also used in the recent TREC experiments.The Arabic Newswire A-Corpus was created by David Graff and Kevin Walker of the Linguistic Data Consortium [13] and consists of articles from Agence France Presse (AFP) Arabic Newswire. Source material was provided with TIPSTER-style SGML and transcoded into Unicode (UTF-8).The corpus includes articles from May 13, 1994 to December 20, 2000. Data is contained in 2,337 compressed Arabic text files. There are 209 MBytes of compressed data (869 MBytes uncompressed) with 383,872 documents containing 76 million tokens containing approximately 666,094 unique words."}, {"heading": "3.2. Query Sets and Relevance Judgments", "text": "The query set associated with the LDC corpus was created for TREC 2001 and 2002 [12, 20, 21] and consists of 75 queries developed at the LDC by native Arabic speakers and translated into English and French. Relevance assessments for these queries were based on evaluation pools from various runs at TREC 2001 and 2002, as well as the top 70 documents from each run with an average size of 910 documents for each pool. For TREC 2001, the average number of relevant documents across the 25 queries was 164.9, with five topics containing more than 300 relevant documents and another five containing less than 25 relevant documents. [22] For TREC 2002, the average number of relevant documents across the 50 queries was 118.2, with eight topics containing more than 300 relevant documents and 16 topics containing less than 25 relevant documents."}, {"heading": "3.3. Retrieval Engine", "text": "The Lemur Toolkit for Language Modelling and Information Retrieval was used. Results of the experiments were mapped against the relevance assessments available for the dataset. Standard recall and precision measurements were calculated using the ireval.pl routine in the toolkit. Assessment was based on the use of eleven layers of recall that generated the recall precision matrix. The Lemur Toolkit was chosen for several reasons. It supports the construction of basic text recovery systems using voice modeling methods, as well as traditional methods such as those based on the vector space model and Okapi. It is written on the web as open source software in C and C + + and runs on both UNIX and Windows (NT). It was developed through collaboration between the Computer Science Department at the University of Massachusetts and the School of Computer Science at the University of California."}, {"heading": "3.4. Stoplists", "text": "A general stop list has been created based on the Arabic language structure and features without additions. All possible words or articles that can be considered stop words have been systematically compiled from the various syntactic classes in Arabic to ensure the completeness of the list. Word categories used [1, 2] are: adverbs. Conditional pronouns. Questioning pronouns. Prepositions. Reference names / determinants. Relative pronouns transformers (verbs, letters) that are used. Verbal pronouns. Others. Choosing a word from one of these categories was based on a personal judgment. Not all words under these categories were used, as some of them were not considered stop words. The resulting list consisted of 1377 words opposed to Khoja [8] and Alshehris [3] lists, and two standard English lists listing Okapi and SMART."}, {"heading": "3.5. Experimental Setup", "text": "The data and query set for the experiments were processed as follows: The 383,872 files in the data set were converted from the UTF-8 format to the WindowsCode Page 1256 encoding (CP1256) for Lemur compatibility; the queries were converted from ASMO 708 encoding to the CP1256 encoding; the title and description for each of the 75 queries were extracted from the original query set; several fatal spelling errors in the queries were corrected; the table normalization function in the Light10 encoding in Lemur was implemented for all runs, regardless of the techniques used; the letters were replaced by the letter list."}, {"heading": "3.6. Evaluation", "text": "The performance of each technique was evaluated using the standard standards Recall and Precision, [9, 15]. A total of 27 passes were performed, each pass representing one or more of these techniques. Raw results from the RetEval application in Lemur were processed using the ireval.pl script to ensure recall and precision. The script performs a TREC-like evaluation and the output includes: total number of relevant documents; total number of relevant documents retrieved; average non-interpolated precision; interpolated precision across the eleven retrieval levels; non-interpolated precision at document cut-off levels; uniform (exact) precision."}, {"heading": "3.7. Data Analysis", "text": "The Friedman Two-Way ANOVA test and the Wilcoxon Matched-Paired Signed-Rank test have been used to assess whether measured differences between different methods can be considered statistically significant or not. Hull [7] has examined the validity of various statistical techniques used in comparing call tests and character tests. He notes that there are two non-parametric alternatives to the t test that do not make assumptions about the forms of distributions of the two variables. Wilcoxon Matched-Paired Signed-Rank test and the character test indicate that the character test only looks at the sign of difference and ignores its magnitude. If one method performs better than the other far more frequently than the distributions of the two variables, then this is strong evidence that it is superior. The Wilcoxon test replaces each difference by the rank of its value."}, {"heading": "4. Results and Data Analysis", "text": "This study compared alternative stop word lists and their effect on retrieval efficacy. Six different techniques with a total of 12 different combinations were studied, and the results were compared using the Wilcoxon test (see Table 2) and retrieval and precision curves (Figures 1-4). Friedman's two-way ANOVA test was used to determine whether the differences were statistically significant (see Table 1), the test statistics \u03c72 = 70,471 and the score * = 0,000, which indicates that the differences between techniques as a whole are statistically significant. This was not surprising given the wide variety of techniques used, but the test does not show any individual differences between two techniques. To investigate these differences, the techniques are grouped according to weighting schemes and notation lists and combinations of them. For each group of retrieval techniques, the signature test begins with the Friedman two-way ANOVA test to determine the differences between the post-coxed techniques applied to each of them in order to determine the differences between the post-coxed techniques."}, {"heading": "4.1. Term Weighting", "text": "The results of the evaluation approach show that the three algorithms performed relatively well given the difficulties of Arabic language and the fact that no linguistic adjustment was implemented for them during the call. Although the BM25 and the KL model were known to perform well compared to the TFIDF weight, the P value was set at 0.05 for both Wilcoxon and Friedman tests.The overall performance of the TFIDF weight was better than the performance of both the BM25 and KL model (see Figure 1).The good performance of the TFIDF is due to the way the term frequency portion of the weight in the Lemur toolkit is calculated using the TF function from the BM25 scheme, which significantly improves its performance.In the Friedman test, the P value was 4.2 = 5.946; the P value = 0.051 and the P value shows that the differences between these three techniques were significant compared to the FIxon."}, {"heading": "4.2. Term Weighting and Stoplists", "text": "In this section, we present the results obtained by using the three stoplists created for this study, which were created on the assumption that they would improve query efficiency when using other techniques, and show how sensitive the query is to the use of stopwords, which will essentially affect the term weights used as they have a significant impact on the frequency of the term."}, {"heading": "4.2.1. General Stoplist", "text": "The results obtained when using the general stop list are shown in Figure 2.6 The differences in mean precision were minimal, but the increase in precision achieved by the list was noticeable at a low threshold, especially for the BM25. The Wilcoxon test shows that the differences between the KL _ GS and the basic precision were not statistically significant. There was a minimal improvement for only one query with the KL model. For the BM25 _ GS and TFIDF _ GS there was a significant difference and a change. After combining the GS with BM25, the results changed drastically, increasing from 30 queries to 47 queries, and the same happened for the TFIDF _ GS with 49 queries that preferred the BM25 over the basic function, with the results increasing dramatically, with the FIDF function being particularly sensitive to the FIDF frequency."}, {"heading": "4.2.2. Corpus-Based Stoplist", "text": "Comparing these results with the basic precision, the test statistic for the Friedman test \u03c72 is 12.957 and the P-value at.0055. This indicates that the differences between these runs and the basic precision are significant; the differences in mean precision were minimal; the Wilcoxon test shows that the differences between BM25 _ CBS, TFIDF _ CBS and the basic precision were not statistically significant, although there was an obvious improvement over the BM25. Although there was a slight improvement with the KL model (no more than 2.7%), the corpus-based disposition had a negative impact on the overall performance of the model, as the results of 31 queries were degraded to 25 in favor of the KL model when combined with the disposition."}, {"heading": "4.2.3. Combined Stoplist", "text": "In this figure, the curves show that the results were also very close, as with the general stop list, and that the list improved the precision for BM25 weight at the lower levels of memory. Comparing these results with the base precision, the test statistic for the Friedman test \u03c72 is 13.327 and the P-value 0.004, indicating that the differences between these runs and the base precision are significant; the differences in mean precision were minimal; the Wilcoxon test shows that the differences between the BM25 _ CS, KL _ CS and the base precision were not statistically significant; however, there was an improvement in BM25 weight; there were some differences compared to the general stop list, but the results were almost identical to the general stop list combinations, despite the additions thereto. Looking at the individual stop lists, the precision differences were in favor of the general stop list."}, {"heading": "5. Discussion", "text": "Using the usual recall and precision measurements, the above techniques were compared. Six techniques were separated and combined, generating a total of 24 different indexing approaches. Without additional linguistic processing, the three schemes, TF * IDF weighting, Okapi best-match algorithm, and the Kullback-Leibler divergence model performed well with Arabic, which was not surprising given their previous success with other languages, since they depended only on the corpus and query statistics, and the differences between the three weight schemes were very minimal and statistically not significant. The TF * IDF scheme is the best weight scheme that can be used with the Arabic language when used separately, without removing the corpus and stop words. This contradicts some previous research suggesting that the BM25 algorithm is better when combined with the Arabic term Lemon, which is a percentage of BM.7% difference in BM.7 * The reason for this is a combination of BM.25% of BM.7 is that BM.7 is a common percentage of BM.7"}, {"heading": "KL 0.2264 31 43 1 0.287", "text": "BP: Baseline Precision (TFIDF).The results show how sensitively the BM25 weight and the KL model react to the use of stop words.The use of stop words has had a positive effect on BM25 weight, while the mapping to the KL model negatively affects outcomes.The corpus-based list was the lower than the general list, suggesting that we should rethink the corpus-based list. Unfortunately, there are no clear rules on how to create a list like this and most of the decisions made in creating this list were arbitrary. In general, the overall performance of the general stop list was better than the corpus-based stop list and to some extent better than the combined stop list. The list can be used as a default list for retrieving Arabic, regardless of the nature of the data used. The list is added to the Lemur toolkit, making it available for research and further development in Arabic, as there is no publicly available stop list for the language."}, {"heading": "6. Conclusions and Further Research", "text": "This study examined several query techniques and their potential to improve query effectiveness; the effects of the weighting of terms and stopwords on the Arabic query were studied and compared with the Lemur toolkit; the best query algorithm, BM25, with the combined or generic stoplist was the best functioning feature for the query in the Arabic language; the performance of a generic stoplist or a combined list was relatively limited; the use of one of them is recommended, but the generic stoplist is certainly preferred when dealing with a different corpus.The Kullback Leibler divergence model had problems with stopwords; further investigation with the model could reveal the extent of this problem, especially when dealing with different smoothing algorithms and variable query lengths."}], "references": [{"title": "Arabic for English Speaking Students", "author": ["M. Abdul-Rauf"], "venue": "Al- Saadawi Publications,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Grammatical Application", "author": ["A. Al-Raghi"], "venue": "Dar El- Nahda Al-Arabia Lelteba\u2019ah Wa Al-nashr", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1981}, {"title": "Optimization and Effectiveness of N-Grams Approach for Indexing and Retrieval in Arabic Information Retrieval Systems, Ph", "author": ["A. Alshehri"], "venue": "D Thesis, School of Information Sciences University of Pittsburgh,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Translation Term Weighting and Combining Translation Resources in Cross-Language Retrieval", "author": ["A. Chen", "F. Gey"], "venue": "Tenth Text REtrieval Conference ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "A Stop List for General Text", "author": ["C. Fox"], "venue": "SIGIR Forum, Vol. 24, No. 1-2", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Using Statistical Testing in the Evaluation of Retrieval Experiments", "author": ["D. Hull"], "venue": "Proceedings of the 16th annual international ACM SIGIR conference on Research and Development in Information Retrieval", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "Stemming Arabic Text", "author": ["S. Khoja", "R. Garside"], "venue": "http://www.comp.lancs.ac.uk/computing/users/ khoja/stemmer.ps", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Information Storage and Retrieval", "author": ["R.R. Korfhage"], "venue": "John Wiley", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Improving Stemming for Arabic Information Retrieval: Light Stemming and Cooccurrence Analysis", "author": ["L.S. Larkey", "L. Ballesteros", "M.E. Connell"], "venue": "Proceedings of the 25 Annual International AC SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Arabic Information Retrieval at UMass in TREC-10", "author": ["L.S. Larkey", "M.E. Connell"], "venue": "Tenth Text REtrieval Conference ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Term- Weighting Approaches in Automatic Text Retrieval", "author": ["G. Salton", "C. Buckley"], "venue": "Information Processing & Management, Vol. 24, No. 4", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Introduction to Modern Information Retrieval", "author": ["G. Salton", "M. McGill"], "venue": "McGraw-Hill Book Company", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1983}, {"title": "Report on the TREC- 11 Experiment: Arabic", "author": ["J. Savoy", "Y. Rasolofo"], "venue": "Named Page and Topic Distillation Searches. Eleventh Text REtrieval Conference ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Multimedia Information Retrieval: Content-based Information Retrieval from Large Text and Audio Databases", "author": ["P. Schauble"], "venue": "Kluwer Academic Publishers", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Nonparametric Statistics of the Behavioral Sciences", "author": ["S. Siegel", "N.J. Castellan"], "venue": "2nd edition, McGraw-Hill Book Company", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1988}, {"title": "D", "author": ["E.M. Voorhees"], "venue": "Harman, Overview of TREC 2001.Tenth Text Retrieval Conference, TREC 2001", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 7, "context": "amount of unproductive processing [9].", "startOffset": 34, "endOffset": 37}, {"referenceID": 13, "context": "text collection can represent stopwords [18].", "startOffset": 40, "endOffset": 44}, {"referenceID": 4, "context": "Fox [6] was the first to create an English stoplist to be", "startOffset": 4, "endOffset": 7}, {"referenceID": 6, "context": "The stoplist used in the Lemur Toolkit is the one created by Khoja [8]", "startOffset": 67, "endOffset": 70}, {"referenceID": 9, "context": "This list was used by Larkey and Connell [11] and Larkey, Ballesteros and", "startOffset": 41, "endOffset": 45}, {"referenceID": 8, "context": "Connell [10].", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "Chen and Gey [5] used a list they created by translating an English list and augmenting it with", "startOffset": 13, "endOffset": 16}, {"referenceID": 12, "context": "Savoy and Rasolofo\u2019s stoplist 1 [17] is a domain dependent list which has three problems.", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "9 with five topics having more than 300 relevant documents and another five with fewer than 25 relevant documents [22].", "startOffset": 114, "endOffset": 118}, {"referenceID": 0, "context": "The word categories [1, 2] that were used are: \uf0a7 Adverbs.", "startOffset": 20, "endOffset": 26}, {"referenceID": 1, "context": "The word categories [1, 2] that were used are: \uf0a7 Adverbs.", "startOffset": 20, "endOffset": 26}, {"referenceID": 6, "context": "The list was checked against Khoja [8] and Alshehri\u2019s [3]", "startOffset": 35, "endOffset": 38}, {"referenceID": 2, "context": "The list was checked against Khoja [8] and Alshehri\u2019s [3]", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "The performance of each technique was evaluated using the standard measures of Recall and Precision, [9, 15].", "startOffset": 101, "endOffset": 108}, {"referenceID": 10, "context": "The performance of each technique was evaluated using the standard measures of Recall and Precision, [9, 15].", "startOffset": 101, "endOffset": 108}, {"referenceID": 5, "context": "Hull [7] has examined the validity of different statistical techniques that are used in comparing", "startOffset": 5, "endOffset": 8}, {"referenceID": 14, "context": "in addition to the direction of the differences considered [19].", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": "between pairs increase, significance also increases [16].", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "than their raw values to calculate the statistic [19].", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "BM25 algorithm is better when used with Arabic [17].", "startOffset": 47, "endOffset": 51}], "year": 2017, "abstractText": "The effectiveness of three stop words lists for Arabic Information Retrieval---General Stoplist, CorpusBased Stoplist, Combined Stoplist ---were investigated in this study. Three popular weighting schemes were examined: the inverse document frequency weight, probabilistic weighting, and statistical language modelling. The Idea is to combine the statistical approaches with linguistic approaches to reach an optimal performance, and compare their effect on retrieval. The LDC (Linguistic Data Consortium) Arabic Newswire data set was used with the Lemur Toolkit. The Best Match weighting scheme used in the Okapi retrieval system had the best overall performance of the three weighting algorithms used in the study, stoplists improved retrieval effectiveness especially when used with the BM25 weight. The overall performance of a general stoplist was better than the other two lists.", "creator": "Microsoft\u00ae Word 2016"}}}