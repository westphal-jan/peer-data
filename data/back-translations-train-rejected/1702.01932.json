{"id": "1702.01932", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "A Knowledge-Grounded Neural Conversation Model", "abstract": "Neural network models are capable of generating extremely natural sounding conversational interactions. Nevertheless, these models have yet to demonstrate that they can incorporate content in the form of factual information or entity-grounded opinion that would enable them to serve in more task-oriented conversational applications. This paper presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses without slot filling. We generalize the widely-used Seq2Seq approach by conditioning responses on both conversation history and external \"facts\", allowing the model to be versatile and applicable in an open-domain setting. Our approach yields significant improvements over a competitive Seq2Seq baseline. Human judges found that our outputs are significantly more informative.", "histories": [["v1", "Tue, 7 Feb 2017 09:16:46 GMT  (2637kb,D)", "http://arxiv.org/abs/1702.01932v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marjan ghazvininejad", "chris brockett", "ming-wei chang", "bill dolan", "jianfeng gao", "wen-tau yih", "michel galley"], "accepted": false, "id": "1702.01932"}, "pdf": {"name": "1702.01932.pdf", "metadata": {"source": "CRF", "title": "A Knowledge-Grounded Neural Conversation Model", "authors": ["Marjan Ghazvininejad", "Chris Brockett", "Ming-Wei Chang", "Bill Dolan", "Jianfeng Gao", "Wen-tau Yih", "Michel Galley"], "emails": ["ghazvini@isi.edu,", "mgalley@microsoft.com", "@pizzalibretto"], "sections": [{"heading": null, "text": "Neural network models are capable of generating extremely natural-sounding conversation interactions, but these models have yet to prove that they can incorporate content in the form of fact-based information or business opinions that would enable them to serve in more task-oriented conversation applications. This paper presents a novel, fully data-driven and knowledge-based neural conversation model that aims to produce more satisfactory answers without filling slots. We are generalizing the widespread SEQ2SEQ approach by conditioning responses to both conversation history and external \"facts,\" making the model versatile and applicable in an open domain environment."}, {"heading": "1 Introduction", "text": "So there is a growing need to build systems that can respond seamlessly and appropriately, and the task of generating conversation responses has lately become an active area of research in natural language processing. This work has been done at Microsoft.data-driven fashion without manual encoding, but these fully data-driven systems are lacking in the real world and lack access to external knowledge (textual or structural), making it difficult to respond substantially."}, {"heading": "2 Grounded Response Generation", "text": "In fact, it is the case that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there"}, {"heading": "2.1 Dialog Encoder and Decoder", "text": "The dialog encoder and the response decoder together form a sequence-to-sequence (SEQ2SEQ model (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014) that has been successfully used in building end-to-end conversation systems (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a). Both encoders and decoders are recursive neural network models (RNN): an RNN that encodes a variable-length input string into a fixed-length vector representation, and an RNN that decodes the vector representation into a variable-length output string. This part of our model is almost identical to earlier conversational SEQ2SEQ models, except that we use gated recurrent units (GRU) (Chung et al., 2014) instead of LSTM (high-frequency and word decoding models, but sometimes do not share the encoding densities)."}, {"heading": "2.2 Facts Encoder", "text": "The Fig. 3 Fact Encoder is similar to the Memory Network Model first proposed by (Weston et al., 2014; Sukhbaatar et al., 2015). It is an associative memory for modeling the facts relevant to a particular problem - in our case, an entity mentioned in a conversation - retrieves these facts based on user input and conversation history to generate an answer. Storage network models are widely used in question-answering to draw conclusions based on the facts stored in the memory (Weston et al., 2015).In our adaptation of storage networks, we use an RNN encoder to turn the input sequence (conversation history) into a vector, instead of a bag of word representation used in the original storage network models. This allows us to exploit the interlexic dependencies between the different parts of the input, making this storage network model more SE2Q (SE2Q) Fact-direct."}, {"heading": "3 Datasets", "text": "The approach described above is fairly general and applies to all data sets that allow us to map named entities to free-form texts (e.g. Wikipedia, IMDB, TripAdvisor, etc.) For experimental purposes, we use data sets derived from two popular social media services: Twitter (voice data) and Foursquare (voice data). Note that none of the processing applied to these data sets is specific to an underlying task or domain."}, {"heading": "3.1 Foursquare", "text": "Foursquare tips are customer comments about restaurants and other, usually commercial establishments, much of which describe aspects of the business and give recommendations about what the customer liked (or didn't like). We extracted 1.1 million tips about establishments in North America from the Web. This was achieved by identifying a series of 11 likely \"foodie cities\" and then collecting tip data associated with zip codes near city centers. While we targeted foodie cities, the data set is very general and includes tips for many types of local businesses (restaurants, theaters, museums, shopping, etc.). In the interest of manageability, we ignored establishments associated with fewer than 10 tips, but other experiments with up to 50 tips per venue yielded comparable results."}, {"heading": "3.2 Twitter", "text": "We collected a 23M dataset of 3-turn conversations. This serves as a non-factual background dataset, and its enormous size is key to learning the conversation structure or backbone. Separately, based on the Twitter handles in the Foursquare tip data, we collected about 1 million two-turn conversations containing entities associated with Foursquare. We call this the 1M ground dataset. Specifically, we identify conversation pairs in which the first round contained either a handle of the business name (preceded by the \"@\" symbol) or a hashtag that matched a handle. 3 Because we are interested in conversations between real users (as opposed to customer service representatives), we removed conversations in which the response was generated by a user with a handle found in the Foursquare data."}, {"heading": "3.3 Grounded Conversation Datasets", "text": "The number of contextually relevant tips for some handles can sometimes be enormous, up to 10k. In order to filter them based on relevance to the input, the system uses tf-idf similarity between the input set and all of these tips, retaining 10 tips with the highest score. In addition, the last round was not particularly informative, e.g. if it offers a purely socializing answer (e.g. \"Have fun there\") and not a satisfactory one. Since one of our goals is to evaluate conversation systems based on their ability to produce satisfied answers, we choose a Dev and Testset (4k conversations in total) that is designed to contain answers that are informative and usable. For each handle in our dataset, we have created two evaluation functions: \u2022 Perplexity according to a 1 gram, which we evaluate based on the selected square values."}, {"heading": "4 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Multi-Task Learning", "text": "We use multi-task learning with these tasks: \u2022 FACTS task: We expose the complete model with ({f1,..., fn, S}, R) training examples. \u2022 NOFACTS task: We expose the model with-out fact encoder with (S, R) examples. \u2022 AUTOENCODER task: It is similar to the FACTS task except that we replace the answer with each of the facts, i.e., this model is trained on ({f1,..., fn, S}, fi) examples. There are n times more examples of this task than for the FACTS task task.4The FACTS and NOFACTS tasks are representative of how our model should work, but we found that the AUTOENCODER tasks inject more factual content into the answer. Then, the different variants of our multi-task learning system are as follows: \u2022 SEQ2SEQ: This system is trained on FACTS task."}, {"heading": "4.2 Decoding and Reranking", "text": "We use a beam search decoder similar to (Sutskever et al., 2014) with a beam size of 200 and a maximum response length of 30. Following (Li et al., 2016a) we create N leaderboards with three features: (1) the log probability logP (R | S, F) according to the decoder; (2) the number of words; (3) the log probability logP (S | R) of the given answer. The third feature is added to deal with the problem of generating everyday and generic answers such as \"I don't know,\" which is discussed in detail in (Li et al., 2016a). Our models often don't need the third feature to be effective, but - since our baseline requires it to avoid everyday answers - we include this feature in all systems. This leads to the following reranking score: logP (R | S, F + Wide-range messages (R | S), we fine-tune the overall logistics (R + S) and (S | P) facts."}, {"heading": "4.3 Evaluation Metrics", "text": "Subsequently (Sordoni et al., 2015; Wen et al., 2016; Li et al., 2016a) we use automatic evaluation by BLEU. While (Liu et al., 2016) suggest that BLEU correlates poorly with human judgment at the judgment level, we instead use corpus5This confirms previous findings that accurate automatic evaluation of sentences is indeed difficult, even for Ma-BLEU, which is known to correlate better with human judgment (Przybocki et al., 2008), including reaction generation (Galley et al., 2015)."}, {"heading": "5 Results", "text": "We have calculated perplexity and BLEU improvements. (Papineni et al., 2002b) for each system. These are shown in Tables 1 and 2. We observe that the perplexity of MTASK and MTASK-R models on both the general and grounded data lines is as low as the SEQ2SEQ models specifically trained on general and grounded data. We observe that the injection of more factual content into the response in MTASKF and MTASK-RF has increased the perplexity especially on grounded data. BLEU scores are low, but this is not atypical for conversational systems (e.g., Li et al., 2016a, b). Table 2 shows that the MTASK-R model delivers a significant performance increase, with a BLEU score increase of 96% and 71% compared to the competitive SEQ2SEQ diversity."}, {"heading": "6 Discussion", "text": "Figure 6 shows examples of results generated by the MTASK-RF model. It shows that our model's responses are generally not only adequate, but also more informative and useful.6 Sub-values have been rounded up, affecting both systems equally. For example, the first answer combines \"having a safe flight,\" which is safe and appropriate and as such is typical of existing neural call systems, but also \"nice airport terminal,\" which is based on knowledge of the specific airport. While our model sometimes repurchases fragments of tips in its answers, it often mixes information from various tips and conversations to generate an answer, as in the fifth and final answer in the figure. The fifth answer is mainly influenced by two Foursquare tips7, and the model creates a fusion of the two, a type of text manipulation that would be difficult to fill slots."}, {"heading": "7 Related Work", "text": "This paper expands the data-driven paradigm of conversation generation by injecting knowledge from textual data into models derived from conversation data, a paradigm introduced by Ritter et al. (2011), who initially proposed using statistical models of machine translation to generate conversational responses from social media data. It was further developed by the introduction of neural network7 (\"sit and take a picture of The Simpsons on the 3rd floor\") and (\"Watch the video feed on 6 and Simpsons / Billiards on 3!\") models (Sordoni et al., 2015; Serban et al., 2016; Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016a, b). The introduction of contextual models of (Sordoni et al., 2015) is an important step forward; we build on this by including contexts outside the constellation of constellations."}, {"heading": "8 Conclusions", "text": "We have presented a novel knowledge-based conversation engine that could serve as a core component of a multi-turn recommendation or conversation QA system. It is a large-scale, scalable, fully data-driven neural conversation model that effectively leverages external knowledge without explicit slot filling. It generalizes the SEQ2SEQ approach to neural conversation models by combining conversation and non-conversation data, of course, through multi-task learning. Our simple entity matching approach to building external information based on the conversation context yields a model that is informative, versatile, and applicable in open domain systems."}, {"heading": "Acknowledgments", "text": "We thank Xuetao Yin and Leon Xu, who helped us extract Foursquare data, as well as Kevin Knight, Chris Quirk, Nebojsa Jojic, Lucy Vanderwende, Vighnesh Shiv, Yi Luan, John Wieting, Alan Ritter, Donald Brinkman and Puneet Agrawal for their helpful suggestions and discussions."}], "references": [{"title": "Luke, i am your father: dealing with out-of-domain requests by using movies subtitles", "author": ["David Ameixa", "Luisa Coheur", "Pedro Fialho", "Paulo Quaresma."], "venue": "Intelligent Virtual Agents. Springer, pages 13\u201321.", "citeRegEx": "Ameixa et al\\.,? 2014", "shortCiteRegEx": "Ameixa et al\\.", "year": 2014}, {"title": "Iris: a chat-oriented dialogue system based on the vector space model", "author": ["Rafael E Banchs", "Haizhou Li."], "venue": "Proceedings of the ACL 2012 System Demonstrations. Association for Computational Linguistics, pages 37\u201342.", "citeRegEx": "Banchs and Li.,? 2012", "shortCiteRegEx": "Banchs and Li.", "year": 2012}, {"title": "Learning end-to-end goal-oriented dialog", "author": ["Antoine Bordes", "Jason Weston."], "venue": "CoRR abs/1605.07683.", "citeRegEx": "Bordes and Weston.,? 2016", "shortCiteRegEx": "Bordes and Weston.", "year": 2016}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Learning to learn, Springer, pages 95\u2013133.", "citeRegEx": "Caruana.,? 1998", "shortCiteRegEx": "Caruana.", "year": 1998}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "deltableu: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of", "citeRegEx": "Galley et al\\.,? 2015", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Accurate evaluation of segment-level machine translation metrics", "author": ["Yvette Graham", "Timothy Baldwin", "Nitika Mathur."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Graham et al\\.,? 2015", "shortCiteRegEx": "Graham et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of NAACL-HLT .", "citeRegEx": "Li et al\\.,? 2016a", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of ACL.", "citeRegEx": "Li et al\\.,? 2016b", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian Vlad Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "CoRR", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information retrieval", "author": ["Xiaodong Liu", "Jianfeng Gao", "Xiaodong He", "Li Deng", "Kevin Duh", "Ye-Yi Wang"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "arXiv preprint arXiv:1511.06114 .", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Developing non-goal dialog system based on examples of drama television", "author": ["Lasguido Nio", "Sakriani Sakti", "Graham Neubig", "Tomoki Toda", "Mirna Adriani", "Satoshi Nakamura."], "venue": "Natural Interaction with Robots, Knowbots and Smartphones, Springer,", "citeRegEx": "Nio et al\\.,? 2014", "shortCiteRegEx": "Nio et al\\.", "year": 2014}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1. Association for Computational Linguistics, pages 160\u2013167.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Stochastic language generation for spoken dialogue systems", "author": ["Alice H Oh", "Alexander I Rudnicky."], "venue": "Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems-Volume 3. Association for Computational Linguistics, pages 27\u2013", "citeRegEx": "Oh and Rudnicky.,? 2000", "shortCiteRegEx": "Oh and Rudnicky.", "year": 2000}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational", "citeRegEx": "Papineni et al\\.,? 2002a", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proc. of ACL. pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002b", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Official results of the nist 2008 metrics for machine translation challenge", "author": ["M. Przybocki", "K. Peterson", "S. Bronsart"], "venue": null, "citeRegEx": "Przybocki et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Przybocki et al\\.", "year": 2008}, {"title": "Trainable approaches to surface natural language generation and their application to conversational dialog systems", "author": ["Adwait Ratnaparkhi."], "venue": "Computer Speech & Language 16(3):435\u2013455.", "citeRegEx": "Ratnaparkhi.,? 2002", "shortCiteRegEx": "Ratnaparkhi.", "year": 2002}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 583\u2013 593.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proc. of AAAI.", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A survey of available corpora for building data-driven dialogue systems", "author": ["Iulian Vlad Serban", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau."], "venue": "CoRR abs/1512.05742.", "citeRegEx": "Serban et al\\.,? 2015", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "ACL-IJCNLP. pages 1577\u20131586.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "End-to-end memory networks. In Advances in neural information processing systems", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": null, "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems (NIPS). pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "Proc. of ICML Deep Learning Workshop.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Multi-domain neural network language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina Maria Rojas-Barahona", "Pei-hao Su", "David Vandyke", "Steve J. Young."], "venue": "CoRR abs/1603.01232.", "citeRegEx": "Wen et al\\.,? 2016", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrk\u0161i\u0107", "PeiHao Su", "David Vandyke", "Steve Young."], "venue": "Proc. of EMNLP. Association for Computational", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1502.05698 .", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "arXiv preprint arXiv:1410.3916 .", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "Recent work (Ritter et al., 2011; Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015) has shown that it is possible to train conversational models in an end-to-end and completely", "startOffset": 12, "endOffset": 97}, {"referenceID": 24, "context": "Recent work (Ritter et al., 2011; Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015) has shown that it is possible to train conversational models in an end-to-end and completely", "startOffset": 12, "endOffset": 97}, {"referenceID": 23, "context": "Recent work (Ritter et al., 2011; Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015) has shown that it is possible to train conversational models in an end-to-end and completely", "startOffset": 12, "endOffset": 97}, {"referenceID": 27, "context": "Recent work (Ritter et al., 2011; Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015) has shown that it is possible to train conversational models in an end-to-end and completely", "startOffset": 12, "endOffset": 97}, {"referenceID": 7, "context": "It offers a framework that generalizes the SEQ2SEQ approach (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014) of most previous neural conversation models, as it naturally combines conversational and non-conversational data via multi-task learning (Caruana, 1998; Liu et al.", "startOffset": 60, "endOffset": 118}, {"referenceID": 26, "context": "It offers a framework that generalizes the SEQ2SEQ approach (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014) of most previous neural conversation models, as it naturally combines conversational and non-conversational data via multi-task learning (Caruana, 1998; Liu et al.", "startOffset": 60, "endOffset": 118}, {"referenceID": 3, "context": ", 2014) of most previous neural conversation models, as it naturally combines conversational and non-conversational data via multi-task learning (Caruana, 1998; Liu et al., 2015).", "startOffset": 145, "endOffset": 178}, {"referenceID": 11, "context": ", 2014) of most previous neural conversation models, as it naturally combines conversational and non-conversational data via multi-task learning (Caruana, 1998; Liu et al., 2015).", "startOffset": 145, "endOffset": 178}, {"referenceID": 24, "context": "The key idea of this approach is that it not only conditions responses based on conversation history (Sordoni et al., 2015), but also on external \u201cfacts\u201d that are relevant to the current context (for example, Foursquare entries as in Fig.", "startOffset": 101, "endOffset": 123}, {"referenceID": 22, "context": "While these datasets (Serban et al., 2015) have grown dramatically in size thanks in particular to social media (Ritter et al.", "startOffset": 21, "endOffset": 42}, {"referenceID": 20, "context": ", 2015) have grown dramatically in size thanks in particular to social media (Ritter et al., 2011), this data is still very far from containing discussions of every entry in Wikipedia, Foursquare, Goodreads, or IMDB.", "startOffset": 77, "endOffset": 98}, {"referenceID": 12, "context": "We train our system using multi-task learning (Luong et al., 2015) as a way of combining conversational data that is naturally associated with external data (e.", "startOffset": 46, "endOffset": 66}, {"referenceID": 7, "context": "The dialog encoder and response decoder form together a sequence-to-sequence (SEQ2SEQ model (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014), which has been sucessfully used in building end-to-end conversational systems (Sordoni et al.", "startOffset": 92, "endOffset": 150}, {"referenceID": 26, "context": "The dialog encoder and response decoder form together a sequence-to-sequence (SEQ2SEQ model (Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014), which has been sucessfully used in building end-to-end conversational systems (Sordoni et al.", "startOffset": 92, "endOffset": 150}, {"referenceID": 24, "context": ", 2014), which has been sucessfully used in building end-to-end conversational systems (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a).", "startOffset": 87, "endOffset": 149}, {"referenceID": 27, "context": ", 2014), which has been sucessfully used in building end-to-end conversational systems (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a).", "startOffset": 87, "endOffset": 149}, {"referenceID": 8, "context": ", 2014), which has been sucessfully used in building end-to-end conversational systems (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a).", "startOffset": 87, "endOffset": 149}, {"referenceID": 4, "context": "This part of our model is almost identical to prior conversational SEQ2SEQ models, except that we use gated recurrent units (GRU) (Chung et al., 2014) instead of LSTM (Hochreiter and Schmidhuber, 1997) cells.", "startOffset": 130, "endOffset": 150}, {"referenceID": 7, "context": ", 2014) instead of LSTM (Hochreiter and Schmidhuber, 1997) cells.", "startOffset": 24, "endOffset": 58}, {"referenceID": 31, "context": "3 is similar to the Memory Network model first proposed by (Weston et al., 2014; Sukhbaatar et al., 2015).", "startOffset": 59, "endOffset": 105}, {"referenceID": 25, "context": "3 is similar to the Memory Network model first proposed by (Weston et al., 2014; Sukhbaatar et al., 2015).", "startOffset": 59, "endOffset": 105}, {"referenceID": 30, "context": "Memory network models are widely used in Question Answering to make inferences based on the facts saved in the memory (Weston et al., 2015).", "startOffset": 118, "endOffset": 139}, {"referenceID": 25, "context": "Based on (Sukhbaatar et al., 2015) we have:", "startOffset": 9, "endOffset": 34}, {"referenceID": 31, "context": "The memory network model of (Weston et al., 2014) can be defined as a multi-layer structure.", "startOffset": 28, "endOffset": 49}, {"referenceID": 12, "context": "We use the same learning technique as (Luong et al., 2015) for multi-task learning.", "startOffset": 38, "endOffset": 58}, {"referenceID": 26, "context": "We use a beam-search decoder similar to (Sutskever et al., 2014) with beam size of 200, and maximum response length of 30.", "startOffset": 40, "endOffset": 64}, {"referenceID": 8, "context": "Following (Li et al., 2016a), we generate N -best lists containing three features: (1) the log-likelihood logP (R|S, F ) according to the decoder; (2) word count; (3) the log-likelihood logP (S|R) of the source given the response.", "startOffset": 10, "endOffset": 28}, {"referenceID": 8, "context": "The third feature is added to deal with the issue of generating commonplace and generic responses such as \u201cI don\u2019t know\u201d, which is discussed in details in (Li et al., 2016a).", "startOffset": 155, "endOffset": 173}, {"referenceID": 14, "context": "\u03bb and \u03b3 are free parameters, which we tune on our development N -best lists using MERT (Och, 2003) by optimizing BLEU (Papineni et al.", "startOffset": 87, "endOffset": 98}, {"referenceID": 16, "context": "\u03bb and \u03b3 are free parameters, which we tune on our development N -best lists using MERT (Och, 2003) by optimizing BLEU (Papineni et al., 2002a).", "startOffset": 118, "endOffset": 142}, {"referenceID": 24, "context": "Following (Sordoni et al., 2015; Wen et al., 2016; Li et al., 2016a), we use BLEU automatic evaluation.", "startOffset": 10, "endOffset": 68}, {"referenceID": 28, "context": "Following (Sordoni et al., 2015; Wen et al., 2016; Li et al., 2016a), we use BLEU automatic evaluation.", "startOffset": 10, "endOffset": 68}, {"referenceID": 8, "context": "Following (Sordoni et al., 2015; Wen et al., 2016; Li et al., 2016a), we use BLEU automatic evaluation.", "startOffset": 10, "endOffset": 68}, {"referenceID": 10, "context": "While (Liu et al., 2016) suggest that BLEU correlates poorly with human judgment at the sentence-level,5 we use instead corpus-level", "startOffset": 6, "endOffset": 24}, {"referenceID": 18, "context": "BLEU, which is known to better correlate with human judgments (Przybocki et al., 2008) including for response generation (Galley et al.", "startOffset": 62, "endOffset": 86}, {"referenceID": 5, "context": ", 2008) including for response generation (Galley et al., 2015).", "startOffset": 42, "endOffset": 63}, {"referenceID": 17, "context": "Automatic Evaluation: We computed perplexity and BLEU (Papineni et al., 2002b) for each system.", "startOffset": 54, "endOffset": 78}, {"referenceID": 6, "context": "chine Translation (Graham et al., 2015), as BLEU and related metrics were originally designed as corpus-level metrics.", "startOffset": 18, "endOffset": 39}, {"referenceID": 20, "context": "This paradigm was introduced by Ritter et al. (2011) who first proposed using statistical Machine Translation models to generate conversational responses from social media data.", "startOffset": 32, "endOffset": 53}, {"referenceID": 24, "context": "The introduction of contextual models by (Sordoni et al., 2015) is an important advance; we build on this by incorporating context from outside the conversation.", "startOffset": 41, "endOffset": 63}, {"referenceID": 15, "context": ", (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 2, "endOffset": 104}, {"referenceID": 19, "context": ", (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 2, "endOffset": 104}, {"referenceID": 1, "context": ", (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 2, "endOffset": 104}, {"referenceID": 0, "context": ", (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 2, "endOffset": 104}, {"referenceID": 13, "context": ", (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014).", "startOffset": 2, "endOffset": 104}, {"referenceID": 2, "context": "Relevant to the current work is (Bordes and Weston, 2016), who employ memory networks to handle restaurant reservations, using a small number of keywords to handle entity types in a knowledge base (cuisine type, location, price range, party size, rating, phone number and address).", "startOffset": 32, "endOffset": 57}], "year": 2017, "abstractText": "Neural network models are capable of generating extremely natural sounding conversational interactions. Nevertheless, these models have yet to demonstrate that they can incorporate content in the form of factual information or entity-grounded opinion that would enable them to serve in more task-oriented conversational applications. This paper presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses without slot filling. We generalize the widely-used SEQ2SEQ approach by conditioning responses on both conversation history and external \u201cfacts\u201d, allowing the model to be versatile and applicable in an open-domain setting. Our approach yields significant improvements over a competitive SEQ2SEQ baseline. Human judges found that our outputs are significantly more informative.", "creator": "LaTeX with hyperref package"}}}