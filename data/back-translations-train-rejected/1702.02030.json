{"id": "1702.02030", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "Empirical Risk Minimization for Stochastic Convex Optimization: $O(1/n)$- and $O(1/n^2)$-type of Risk Bounds", "abstract": "Although there exist plentiful theories of empirical risk minimization (ERM) for supervised learning, current theoretical understandings of ERM for a related problem---stochastic convex optimization (SCO), are limited. In this work, we strengthen the realm of ERM for SCO by exploiting smoothness and strong convexity conditions to improve the risk bounds. First, we establish an $\\widetilde{O}(d/n + \\sqrt{F_*/n})$ risk bound when the random function is nonnegative, convex and smooth, and the expected function is Lipschitz continuous, where $d$ is the dimensionality of the problem, $n$ is the number of samples, and $F_*$ is the minimal risk. Thus, when $F_*$ is small we obtain an $\\widetilde{O}(d/n)$ risk bound, which is analogous to the $\\widetilde{O}(1/n)$ optimistic rate of ERM for supervised learning. Second, if the objective function is also $\\lambda$-strongly convex, we prove an $\\widetilde{O}(d/n + \\kappa F_*/n )$ risk bound where $\\kappa$ is the condition number, and improve it to $O(1/[\\lambda n^2] + \\kappa F_*/n)$ when $n=\\widetilde{\\Omega}(\\kappa d)$. As a result, we obtain an $O(\\kappa/n^2)$ risk bound under the condition that $n$ is large and $F_*$ is small, which to the best of our knowledge, is the first $O(1/n^2)$-type of risk bound of ERM. Third, we stress that the above results are established in a unified framework, which allows us to derive new risk bounds under weaker conditions, e.g., without convexity of the random function and Lipschitz continuity of the expected function. Finally, we demonstrate that to achieve an $O(1/[\\lambda n^2] + \\kappa F_*/n)$ risk bound for supervised learning, the $\\widetilde{\\Omega}(\\kappa d)$ requirement on $n$ can be replaced with $\\Omega(\\kappa^2)$, which is dimensionality-independent.", "histories": [["v1", "Tue, 7 Feb 2017 14:14:19 GMT  (25kb)", "http://arxiv.org/abs/1702.02030v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lijun zhang", "tianbao yang", "rong jin"], "accepted": false, "id": "1702.02030"}, "pdf": {"name": "1702.02030.pdf", "metadata": {"source": "CRF", "title": "Empirical Risk Minimization for Stochastic Convex Optimization: O(1/n)- and O(1/n)-type of Risk Bounds", "authors": ["Lijun Zhang", "Tianbao Yang", "Rong Jin"], "emails": ["zhanglj@lamda.nju.edu.cn", "tianbao-yang@uiowa.edu", "rongjin@cse.msu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.02 030v 1 [cs.L G] 7 \u221a F * / n) risk-related, if the random function is not negative, convex and smooth and the expected function is continuous, where d is the dimensionality of the problem, n is the number of samples and F * is the minimum risk. If F * is small, we therefore obtain an O (d / n) risk-related, corresponding to the O (1 / n) optimistic rate of ERM for assisted learning. Secondly, if the objective function is also convex, we prove an O (d / n + \u03baF * / n) risk-related, where it is a condition number, and improve it to O (1 / [\u03bbn2] + \u0445F * / n), if the objective function is strongly convex. Consequently, we obtain an O-risk, which is linked to the condition number, and improve it on O (1 / n), if the introspectICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICY ()))"}, {"heading": "1. Introduction", "text": "In this problem, the goal is to optimize the value of an expected objective function F (\u00b7), which depends on a (possibly unknown) distribution problem. (1) A known special case is the risk minimization problem in parent learning (Vapnik, 1998, 2000), which results in the following formminute h (h).H F (x), y), y)], where H (h). (2), where H (X) 7, R)."}, {"heading": "2. Related Work", "text": "In this section we give a brief introduction to previous work on stochastic optimization."}, {"heading": "2.1 ERM for Stochastic Optimization", "text": "As we have already mentioned, there is very little work dedicated to stochastic optimization ERM."}, {"heading": "2.2 ERM for Supervised Learning", "text": "In the context of supervised learning, the performance of the ERM is closely linked to the uniform convergence of F (\u00b7) to F (\u00b7) with respect to Hypothesis 2. Its excessive risk is associated with minimizing regulated empirical risk (Bartlett).class H (Koltchinskii, 2011). Indeed, uniform convergence is a sufficient condition for learning ability (Shalev-Shwartz and Ben-David, 2014), and in some specific cases, such as binary classification (Bartlett), it is also a necessary condition (Vapnik, 1998).The accuracy of uniform convergence and the quality of the empirical minimizer H. can be limited upwards in relation to the complexity of the hypothesis in classH, including data-independent metrics such as the VC dimension and data-dependent metrics such as the Rademacher complexity."}, {"heading": "2.3 SA for Stochastic Optimization", "text": "Stochastic Approach (SA) solves the stochastic optimization problem by noisy observations of the expected function (Kushner and Yin, 2003). Shortly, we will discuss only best-in-class methods for SCO, and in this case n is the number of stochastic gradients consumed by the algorithm. If the random function f (\u00b7) is not negative and smooth, the stochastic gradient descent (SGD) has a risk limit of O (1 / n) and becomes O (1 / n) if F (Nemirovski and Yudin, 1983). If the random function f (\u00b7) is not negative and smooth, the SGD (with an appropriate step size) has a risk limit of O (1 / n + E). If F (\u00b7) is convex, some variants of SGD (Hazan and Kale, 2011; Rakhal loss (Srebro et al al al al al al al, al) will have an optimal coral loss (4)."}, {"heading": "3. Faster Rates of ERM", "text": "We first present all the assumptions used in our analysis, then present theoretical results under different combinations of them, and finally discuss a specific case of supervised learning."}, {"heading": "3.1 Assumptions", "text": "Domain W is a convex subset of Rd and is delimited by R, i.e., it is delimited by R, i.e., it is delimited by W. (4) Assumption 2 The random function f (\u00b7) is not negative, and L-smooth by W, i.e., it is continuously replaced by G-Lipschitz by W, i.e., it is replaced by G-w (w). (4) Assumption 3 The expected function F (\u00b7) is G-Lipschitz continuously above W, i.e., | F (w) \u2212 F (w)."}, {"heading": "3.2 Risk Bounds for SCO", "text": "(9) Under conditions 1, 2, and 4 (d), with a probability of at least 1 \u2212 2, we have F (w) \u2212 F (w) \u2212 F (p), 2LC (p) n + 8RM log (2 / 8R) n, and 4 (d), with a probability of at least 1 \u2212 2, we have F (p) \u2212 F (w) \u2212 F (p), 2LC (p) n + 8RM log (2 / 8R) n, and 2LF (2 / 8R) n, with a probability of at least 1 \u2212 2, we have F (p) \u2212 F (p) \u2212 2) \u2212 F (w), 2LC (p) n + 8RM log (2 / 8R) n, and 2LF (2 / 8R)."}, {"heading": "3.3 Risk Bounds for Supervised Learning", "text": "If the conditions of theorem 3 or theorem 5 are met, we can apply them directly to establish an O (1 / [2] + \u03baF (n) risk for supervised learning. < < < < < < M (1 / [3] + < W (1 / [4] + \u03baF (n) risk for supervised learning). < M (2) > W (3) is that the lower limit of supervised learning depends on dimensionality d, and therefore cannot be applied to unlimited dimensional cases. < M (3), W (3) and Smola, 2002). In this section, we use the structure of supervised learning to make the dimensionality of theory independent. We focus on the generalized linear form of supervised learning: minimum w (4)."}, {"heading": "4. Analysis", "text": "Here we present the evidence for the most important theorems that have been omitted and are to be found in the appendices."}, {"heading": "4.1 The Key Idea", "text": "As a result of the convexity of F \u00b2 (\u00b7) and the optimal condition of W \u00b2 (Boyd and Vandenberghe, 2004), we have < F \u00b2 (w \u00b2), W \u00b2 > 0, W \u00b2 W (28) Our theoretical analysis is based on the following imbalance: F (w \u00b2) \u2212 F (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 F (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2) \u2212 F (w \u00b2) \u2212 F (w \u00b2) \u2212 F (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W \u00b2 (w \u00b2), W (w \u00b2), W (w \u00b2), W (w \u00b2), W (w \u00b2, W (w \u00b2), W (w \u00b2), W (w \u00b2), W (w \u00b2), W (w), W (w), W (w), W (w), W (w), W \u00b2, W (w, W (w), W (w), W (w, \u00b2, W, W (w), W (w), W, W (w, W, \u00b2, W (w), W (w), W (w, W, \u00b2, W, W (w), W, W (w, W, \u00b2, W (w \u00b2, W (w), W (w), W \u00b2, W (w \u00b2, W (w, W \u00b2, W (w), W (w), W (w \u00b2, W \u00b2, W (w), W (w \u00b2, W (w), W \u00b2, W (w \u00b2, W (w), W (w \u00b2, W (w \u00b2, w,"}, {"heading": "4.2 Proof of Theorem 1", "text": "We have a uniform convergence of F (w), we have a uniform F (w), we have a uniform F (w), we have a uniform F (w), we have a uniform F (w), we have a uniform F (w), we have a uniform F (w), we have a uniform F (w), we have a uniform F (w), we have a uniform F (w), we have a uniform F (w), we have a uniform F (w), we have a uniform F (w), we have a uniform F (w), we have a uniform F (w), we have a uniform F (w), we have a uniform F (w), we have a uniform F (w)."}, {"heading": "4.3 Proof of Lemma 1", "text": "We introduce Lemma 2 by Smale and Zhou (2007).Lemma 3 Let H be a Hilbert space and let us have a random variable with values in H. Suppose you have a random value in H. Suppose you have a random value in H. Suppose you have a random value in H. Suppose you have a random value in H. Let us suppose. Let us accept a random value. Let us accept a random value. Let us accept a random value in H. Let us accept a random value. Let us accept a random value. Let us accept a random value. Let us accept a random value. Let us accept a random value. Let us accept a random value. Let us accept a random value. Let us accept a random value. Let us accept a random value. Let us accept a random value. Let us accept a random amount. Let us accept a random amount. Let us accept a random amount. Let us accept an amount. Let us accept a random amount. Let us accept an amount."}, {"heading": "4.4 Proof of Lemma 2", "text": "In order to apply Lemma 3, we need an upper limit of E [\u0433\u0442 fi (w \u0445) 2]. Since fi (\u00b7) L is smooth and not negative, we have from Lemma 4.1 from Srebro et al. (2010) that we have the definition in (8) that fi (w) 2 \u2264 4Lfi (w) and thusE [\u0441 fi (w) 2] \u2264 4LE [fi (w)] = 4LF. From the definition in (8), we then have the definition of fi (w) \u2264 M. According to Lemma 3, then we have a probability of at least 1 \u2212 GA that we have the definition of F (w) \u2212 and F (w)."}, {"heading": "4.5 Proof of Theorem 3", "text": "The proof follows the same logic as that of Theorem 1 \u2212 41. Assuming 4 (b), (30), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), (4b), 4b, 4b, 4b, 4b (4b), 4b, 4b (4b, 4b, 4b, 4b, 4b, 4b (4b), 4b (4b), 4b (4b), 4b (4b), 4b (4b), 4b), 4b (4b), 4b (4b), 4b), 4b (4b), 4b), 4b (4b), 4b, 4b), 4b (4b), 4b, 4b), 4b, 4b (4b), 4b, 4b), 4b, 4b, 4b (4b), 4b, 4b, 4b, 4b, 4b (4b), 4b, 4b, 4b, 4b, 4b (4b), 4b, 4b, 4b, 4b (4b), 4b, 4b, 4b, 4b, 4b, 4b, 4b (4b), 4b, 4b (4b, 4b, 4b, 4b, 4b (4b), 4b, 4b, 4b, 4b, 4b, 4b, 4b, 4b, 4b (4b, 4b, 4b, 4b, 4b, 4b, 4b, 4"}, {"heading": "4.6 Proof of Theorem 5", "text": "Without Assumption 4 (d), Lemma 1, which is no longer used in the proofs of Theorems 1 and 3, we will use the following version, which is based only on the smooth condition. \u2212 Assuming 2, with a probability of at least 1 \u2212 w, for all W + W (W + W), we will have the following version, which is based only on the smooth condition. \u2212 w \u2212 w (W) \u2212 w (W + W). \u2212 w (W). \u2212 w (W). \u2212 w (W). \u2212 w (W). \u2212 w (W). \u2212 w (W). \u2212 w (W). \u2212 w (W). \u2212 w (W). \u2212 w (W). \u2212 w (W). \u2212 w (W)."}, {"heading": "4.7 Proof of Theorem 8", "text": "We consider two cases. In the first case we assume that we assume two cases. \u2212 w (w) \u2212 w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (n) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (n) w (w) w (w) w (w) m) w (w) w (w) w (n). \u2212 w (n) w (w) (w) (w) w) w (w) w (w) w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w (w) w (w) w (w) w (w (w) w (w) w (w) w (w) w (w (w) w) w (w (w) w (w) w (w) w (w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w) w (w (w) w (w) w (w) w) w (w (w) w (w) w (w) w (w) w) w (w) w (w (w) w (w) w (w) w) w (w (w) w) w (w) w) w (w) w (w (w (w) w) w) w (w"}, {"heading": "5. Conclusions and Future work", "text": "Our theoretical results show that it is possible to achieve O (1 / n) type of risk limits under (i) smoothness and low minimum risk conditions (i.e., theorem 1) or (ii) smoothness and strong convexity (i.e., the first part of theorems 3, 5 and 8).A more exciting result is that if n is large enough, the ERM O (1 / n2) type of risk limits under smoothness, strong convexity and low minimum risk conditions (i.e., the second part of theorems 3, 5 and 8).In connection with SCO, many open problems remain regarding ERM.1. Our current results are limited to the Hilbert or Euclidean space, since smoothness and strong convexity are defined in terms of exvexity."}, {"heading": "Appendix A. Proof of Theorem 7", "text": "This result is actually a by-product of Theorem 5. Since there is no strong convexity, we put \u03bb = 0 in (41) and obtain F (w) \u2212 F (w) \u2264 LC (\u03b5) \u2012 w \u00b2 2n + L \u00b2 w \u00b2 2b \u00b2 2b \u00b2 2b \u00b2 C (\u03b5) n + 2M log (2 / 3) \u00b7 W \u00b2 n + W \u00b2 n (4) \u2264 4R 2LC (\u03b5) n + 4RM log (2 / 3) n + 4R2L \u00b2 C (\u03b5) n + 2R \u00b2 8LF \u00b2 log (2 / 3) n + (4RL + 2RL \u00b2 C) n + (4RL + 2RLC) n +."}, {"heading": "Appendix B. Proof of Lemma 5", "text": "First, we divide the range (1 / n2, 2R) into s = 2 log2 (n) + log2 (2R) + consecutive segments (1, 2,...) so that we divide the case (k = 2k \u2212 1n2) so that we have a fixed value of k. Then we look at the case (k = 2k \u2212 1n2) with a fixed value of k. We have w: \u00b2 w \u2212 w (w) \u2212 w (H (w) \u2212 H (w) \u2212 H (w) \u2212 H (w) \u2212 H (w) \u2212 H (w), w \u2212 H (H) with a fixed value of k."}, {"heading": "Appendix C. Proof of Lemma 7", "text": "To simplify the notation, let us assume that f: An 7 \u2192 R satisfiessup x1,... xn, x \u00b2, x \u00b2, x \u00b2, x \u00b2, x \u00b2, x \u00b2, x \u00b2, x \u00b2, x \u00b2, x."}, {"heading": "Appendix D. Proof of Lemma 8", "text": "Definepi (w) = 1 (a) 2 (a) 2 (a) 2 (a) 2 (a) 2 (a) 2 (a) 2 (a) 3 (b) 3 (b) 4 (b) 4 (b) 4 (b) 4 (b) 4 (b) 4 (b) 4 (c) 4 (c) 4 (c) 4 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5 (c) 5) 5 (c) 5 (c) 5) 5 (c) 5 (c) 5) 5 (c) 5) 5 (c) 5) 5 (c) 5) 5 (c) 5 (c) 5) 5 (c) 5) 5 (c) 5) 5 (c) 5) 5 (c) 5) 5 (c) 5)."}], "references": [{"title": "Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization", "author": ["Alekh Agarwal", "Peter L. Bartlett", "Pradeep Ravikumar", "Martin J. Wainwright"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Agarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)", "author": ["Francis Bach", "Eric Moulines"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bach and Moulines.,? \\Q2013\\E", "shortCiteRegEx": "Bach and Moulines.", "year": 2013}, {"title": "Rademacher and gaussian complexities: risk bounds and structural results", "author": ["Peter L. Bartlett", "Shahar Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2002}, {"title": "Local rademacher complexities", "author": ["Peter L. Bartlett", "Olivier Bousquet", "Shahar Mendelson"], "venue": "The Annals of Statistics,", "citeRegEx": "Bartlett et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2005}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Learning with deep cascades", "author": ["Giulia Desalvo", "Mehryar Mohri", "Umar Syed"], "venue": "In Proceedings of the 26th International Conference on Algorithmic Learning Theory,", "citeRegEx": "Desalvo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Desalvo et al\\.", "year": 2015}, {"title": "Generalization of erm in stochastic convex optimization: The dimension strikes back", "author": ["Vitaly Feldman"], "venue": "ArXiv e-prints,", "citeRegEx": "Feldman.,? \\Q2016\\E", "shortCiteRegEx": "Feldman.", "year": 2016}, {"title": "Average stability is invariant to data preconditioning. implications to exp-concave empirical risk minimization", "author": ["Alon Gonen", "Shai Shalev-Shwartz"], "venue": "ArXiv e-prints,", "citeRegEx": "Gonen and Shalev.Shwartz.,? \\Q2016\\E", "shortCiteRegEx": "Gonen and Shalev.Shwartz.", "year": 2016}, {"title": "Equivalence of models for polynomial learnability", "author": ["David Haussler", "Michael Kearns", "Nick Littlestone", "Manfred K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Haussler et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Haussler et al\\.", "year": 1991}, {"title": "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In Proceedings of the 24th Annual Conference on Learning Theory,", "citeRegEx": "Hazan and Kale.,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2011}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Johnson and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2013}, {"title": "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems", "author": ["Vladimir Koltchinskii"], "venue": null, "citeRegEx": "Koltchinskii.,? \\Q2011\\E", "shortCiteRegEx": "Koltchinskii.", "year": 2011}, {"title": "Fast rates for exp-concave empirical risk minimization", "author": ["Tomer Koren", "Kfir Levy"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Koren and Levy.,? \\Q2015\\E", "shortCiteRegEx": "Koren and Levy.", "year": 2015}, {"title": "Stochastic Approximation and Recursive Algorithms and Applications", "author": ["Harold J. Kushner", "G. George Yin"], "venue": null, "citeRegEx": "Kushner and Yin.,? \\Q2003\\E", "shortCiteRegEx": "Kushner and Yin.", "year": 2003}, {"title": "Probability in Banach Spaces: Isoperimetry and Processes", "author": ["Michel Ledoux", "Michel Talagrand"], "venue": null, "citeRegEx": "Ledoux and Talagrand.,? \\Q1991\\E", "shortCiteRegEx": "Ledoux and Talagrand.", "year": 1991}, {"title": "The importance of convexity in learning with squared loss", "author": ["Wee Sun Lee", "Peter L. Bartlett", "Robert C. Williamson"], "venue": "In Proceedings of the 9th Annual Conference on Computational Learning Theory,", "citeRegEx": "Lee et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1996}, {"title": "Lower and upper bounds on the generalization of stochastic exponentially concave optimization", "author": ["Mehrdad Mahdavi", "Lijun Zhang", "Rong Jin"], "venue": "In Proceedings of the 28th Conference on Learning Theory,", "citeRegEx": "Mahdavi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahdavi et al\\.", "year": 2015}, {"title": "On the method of bounded differences", "author": ["Colin McDiarmid"], "venue": "In Surveys in Combinatorics,", "citeRegEx": "McDiarmid.,? \\Q1989\\E", "shortCiteRegEx": "McDiarmid.", "year": 1989}, {"title": "Fast rates with high probability in exp-concave statistical learning", "author": ["Nishant A. Mehta"], "venue": "ArXiv e-prints,", "citeRegEx": "Mehta.,? \\Q2016\\E", "shortCiteRegEx": "Mehta.", "year": 2016}, {"title": "Generalization error bounds for bayesian mixture algorithms", "author": ["Ron Meir", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Meir and Zhang.,? \\Q2003\\E", "shortCiteRegEx": "Meir and Zhang.", "year": 2003}, {"title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning", "author": ["Eric Moulines", "Francis R. Bach"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Moulines and Bach.,? \\Q2011\\E", "shortCiteRegEx": "Moulines and Bach.", "year": 2011}, {"title": "Problem complexity and method efficiency in optimization", "author": ["A. Nemirovski", "D.B. Yudin"], "venue": null, "citeRegEx": "Nemirovski and Yudin.,? \\Q1983\\E", "shortCiteRegEx": "Nemirovski and Yudin.", "year": 1983}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Introductory lectures on convex optimization: a basic course, volume 87 of Applied optimization", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Nesterov.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov.", "year": 2004}, {"title": "Some extensions of an inequality of vapnik and chervonenkis", "author": ["Dmitriy Panchenko"], "venue": "Electronic Communications in Probability,", "citeRegEx": "Panchenko.,? \\Q2002\\E", "shortCiteRegEx": "Panchenko.", "year": 2002}, {"title": "The volume of convex bodies and Banach space geometry. Cambridge Tracts in Mathematics (No. 94)", "author": ["Gilles Pisier"], "venue": null, "citeRegEx": "Pisier.,? \\Q1989\\E", "shortCiteRegEx": "Pisier.", "year": 1989}, {"title": "One-bit compressed sensing by linear programming", "author": ["Yaniv Plan", "Roman Vershynin"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Plan and Vershynin.,? \\Q2013\\E", "shortCiteRegEx": "Plan and Vershynin.", "year": 2013}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Learning with kernels : support vector machines, regularization, optimization, and beyond", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["Shai Shalev-Shwartz", "Shai Ben-David"], "venue": null, "citeRegEx": "Shalev.Shwartz and Ben.David.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz and Ben.David.", "year": 2014}, {"title": "Stochastic convex optimization", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "In Proceedings of the 22nd Annual Conference on Learning Theory,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2009}, {"title": "Lectures on Stochastic Programming: Modeling and Theory", "author": ["Alexander Shapiro", "Darinka Dentcheva", "Andrzej Ruszczy\u0144ski"], "venue": "SIAM, second edition,", "citeRegEx": "Shapiro et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shapiro et al\\.", "year": 2014}, {"title": "Learning theory estimates via integral operators and their approximations", "author": ["Steve Smale", "Ding-Xuan Zhou"], "venue": "Constructive Approximation,", "citeRegEx": "Smale and Zhou.,? \\Q2007\\E", "shortCiteRegEx": "Smale and Zhou.", "year": 2007}, {"title": "Optimistic rates for learning with a smooth loss", "author": ["Nathan Srebro", "Karthik Sridharan", "Ambuj Tewari"], "venue": "ArXiv e-prints,", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "Fast rates for regularized objectives", "author": ["Karthik Sridharan", "Shai Shalev-shwartz", "Nathan Srebro"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sridharan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sridharan et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 32, "context": "Stochastic optimization occurs in almost all areas of science and engineering, such as machine learning, statistics and operations research (Shapiro et al., 2014).", "startOffset": 140, "endOffset": 162}, {"referenceID": 14, "context": "Two classical approaches for solving stochastic optimization are stochastic approximation (SA) (Kushner and Yin, 2003) and the sample average approximation (SAA), the latter of which is also referred to as empirical risk minimization (ERM) in the machine learning community (Vapnik, 1998).", "startOffset": 95, "endOffset": 118}, {"referenceID": 2, "context": "While both SA and ERM have been extensively studied in recent years (Bartlett and Mendelson, 2002; Bartlett et al., 2005; Koltchinskii, 2011; Nemirovski et al., 2009; Moulines and Bach, 2011), most theoretical guarantees of ERM are restricted to the supervised learning problem in (2).", "startOffset": 68, "endOffset": 191}, {"referenceID": 3, "context": "While both SA and ERM have been extensively studied in recent years (Bartlett and Mendelson, 2002; Bartlett et al., 2005; Koltchinskii, 2011; Nemirovski et al., 2009; Moulines and Bach, 2011), most theoretical guarantees of ERM are restricted to the supervised learning problem in (2).", "startOffset": 68, "endOffset": 191}, {"referenceID": 12, "context": "While both SA and ERM have been extensively studied in recent years (Bartlett and Mendelson, 2002; Bartlett et al., 2005; Koltchinskii, 2011; Nemirovski et al., 2009; Moulines and Bach, 2011), most theoretical guarantees of ERM are restricted to the supervised learning problem in (2).", "startOffset": 68, "endOffset": 191}, {"referenceID": 23, "context": "While both SA and ERM have been extensively studied in recent years (Bartlett and Mendelson, 2002; Bartlett et al., 2005; Koltchinskii, 2011; Nemirovski et al., 2009; Moulines and Bach, 2011), most theoretical guarantees of ERM are restricted to the supervised learning problem in (2).", "startOffset": 68, "endOffset": 191}, {"referenceID": 21, "context": "While both SA and ERM have been extensively studied in recent years (Bartlett and Mendelson, 2002; Bartlett et al., 2005; Koltchinskii, 2011; Nemirovski et al., 2009; Moulines and Bach, 2011), most theoretical guarantees of ERM are restricted to the supervised learning problem in (2).", "startOffset": 68, "endOffset": 191}, {"referenceID": 2, "context": "While both SA and ERM have been extensively studied in recent years (Bartlett and Mendelson, 2002; Bartlett et al., 2005; Koltchinskii, 2011; Nemirovski et al., 2009; Moulines and Bach, 2011), most theoretical guarantees of ERM are restricted to the supervised learning problem in (2). As pointed out in a seminal work of Shalev-Shwartz et al. (2009), the success of ERM for supervised learning cannot be directly extended to stochastic optimization.", "startOffset": 69, "endOffset": 351}, {"referenceID": 2, "context": "While both SA and ERM have been extensively studied in recent years (Bartlett and Mendelson, 2002; Bartlett et al., 2005; Koltchinskii, 2011; Nemirovski et al., 2009; Moulines and Bach, 2011), most theoretical guarantees of ERM are restricted to the supervised learning problem in (2). As pointed out in a seminal work of Shalev-Shwartz et al. (2009), the success of ERM for supervised learning cannot be directly extended to stochastic optimization. Actually, Shalev-Shwartz et al. (2009) have constructed an instance of SCO that is learnable by SA but cannot be solved by ERM.", "startOffset": 69, "endOffset": 490}, {"referenceID": 31, "context": "State-of-the-art risk bounds of ERM include: an \u00d5( \u221a d/n) bound when the random function f(\u00b7) is Lipschitz continuous,1 where d is the dimensionality of w; an O(1/\u03bbn) bound when f(\u00b7) is \u03bb-strongly convex (Shalev-Shwartz et al., 2009); and an \u00d5(d/\u03b7n) bound when f(\u00b7) is \u03b7-exponentially concave (\u03b7-exp-concave) (Mehta, 2016).", "startOffset": 204, "endOffset": 233}, {"referenceID": 19, "context": ", 2009); and an \u00d5(d/\u03b7n) bound when f(\u00b7) is \u03b7-exponentially concave (\u03b7-exp-concave) (Mehta, 2016).", "startOffset": 83, "endOffset": 96}, {"referenceID": 34, "context": "From existing studies of ERM for supervised learning (Srebro et al., 2010), we know that smoothness can be utilized to boost the risk bound.", "startOffset": 53, "endOffset": 74}, {"referenceID": 34, "context": ", F\u2217 = O(d 2/n), we obtain an \u00d5(d/n) risk bound, which is analogous to the \u00d5(1/n) optimistic rate of ERM for supervised learning (Srebro et al., 2010) and also matches a recent lower bound of ERM for SCO (Feldman, 2016, Theorem 3.", "startOffset": 129, "endOffset": 150}, {"referenceID": 13, "context": "Stochastic optimization with exp-concave functions is studied recently (Koren and Levy, 2015),2 and Mehta (2016) proves an \u00d5(d/\u03b7n) bound of ERM that holds with high probability when f(\u00b7) is \u03b7-exp-concave, Lipschitz continuous, and bounded.", "startOffset": 71, "endOffset": 93}, {"referenceID": 19, "context": "A simple way to achieve a high probability bound is to use ERM combined with a generic or specific boosting-the-confidence method (Mehta, 2016; Haussler et al., 1991), but the guarantee is not directly on the empirical minimizer as noted by Shalev-Shwartz et al.", "startOffset": 130, "endOffset": 166}, {"referenceID": 8, "context": "A simple way to achieve a high probability bound is to use ERM combined with a generic or specific boosting-the-confidence method (Mehta, 2016; Haussler et al., 1991), but the guarantee is not directly on the empirical minimizer as noted by Shalev-Shwartz et al.", "startOffset": 130, "endOffset": 166}, {"referenceID": 27, "context": "When W \u2282 Rd is bounded and f(\u00b7) is Lipschitz continuous, Shalev-Shwartz et al. (2009) demonstrate that F\u0302 (w) converges to F (w) uniformly over W with an \u00d5( \u221a d/n) error bound that holds with high probability, implying an \u00d5( \u221a d/n) risk bound of ERM.", "startOffset": 57, "endOffset": 86}, {"referenceID": 11, "context": "Stochastic optimization with exp-concave functions is studied recently (Koren and Levy, 2015),2 and Mehta (2016) proves an \u00d5(d/\u03b7n) bound of ERM that holds with high probability when f(\u00b7) is \u03b7-exp-concave, Lipschitz continuous, and bounded.", "startOffset": 72, "endOffset": 113}, {"referenceID": 6, "context": "Lower bounds of ERM for stochastic optimization is investigated by Feldman (2016), who exhibits (i) a lower bound of \u03a9(d/\u01eb2) sample complexity for uniform convergence that nearly matches the upper bound of Shalev-Shwartz et al.", "startOffset": 67, "endOffset": 82}, {"referenceID": 6, "context": "Lower bounds of ERM for stochastic optimization is investigated by Feldman (2016), who exhibits (i) a lower bound of \u03a9(d/\u01eb2) sample complexity for uniform convergence that nearly matches the upper bound of Shalev-Shwartz et al. (2009); and (ii) a lower bound of \u03a9(d/\u01eb) sample complexity of ERM, which is matched by our \u00d5(d/n + \u221a F\u2217/n) bound when F\u2217 is small.", "startOffset": 67, "endOffset": 235}, {"referenceID": 6, "context": "Lower bounds of ERM for stochastic optimization is investigated by Feldman (2016), who exhibits (i) a lower bound of \u03a9(d/\u01eb2) sample complexity for uniform convergence that nearly matches the upper bound of Shalev-Shwartz et al. (2009); and (ii) a lower bound of \u03a9(d/\u01eb) sample complexity of ERM, which is matched by our \u00d5(d/n + \u221a F\u2217/n) bound when F\u2217 is small. It is worth mentioning the difference among proof techniques in these works. The uniform convergence result of Shalev-Shwartz et al. (2009) leverages the covering number to bound |F\u0302 (w) \u2212 F (w)| for any w \u2208 W.", "startOffset": 67, "endOffset": 499}, {"referenceID": 6, "context": "Lower bounds of ERM for stochastic optimization is investigated by Feldman (2016), who exhibits (i) a lower bound of \u03a9(d/\u01eb2) sample complexity for uniform convergence that nearly matches the upper bound of Shalev-Shwartz et al. (2009); and (ii) a lower bound of \u03a9(d/\u01eb) sample complexity of ERM, which is matched by our \u00d5(d/n + \u221a F\u2217/n) bound when F\u2217 is small. It is worth mentioning the difference among proof techniques in these works. The uniform convergence result of Shalev-Shwartz et al. (2009) leverages the covering number to bound |F\u0302 (w) \u2212 F (w)| for any w \u2208 W. The analysis for strongly convex functions by Shalev-Shwartz et al. (2009) and exp-concave functions by Koren and Levy (2015) utilize the tool of stability, which only produces risk bounds that hold in expectation.", "startOffset": 67, "endOffset": 645}, {"referenceID": 6, "context": "Lower bounds of ERM for stochastic optimization is investigated by Feldman (2016), who exhibits (i) a lower bound of \u03a9(d/\u01eb2) sample complexity for uniform convergence that nearly matches the upper bound of Shalev-Shwartz et al. (2009); and (ii) a lower bound of \u03a9(d/\u01eb) sample complexity of ERM, which is matched by our \u00d5(d/n + \u221a F\u2217/n) bound when F\u2217 is small. It is worth mentioning the difference among proof techniques in these works. The uniform convergence result of Shalev-Shwartz et al. (2009) leverages the covering number to bound |F\u0302 (w) \u2212 F (w)| for any w \u2208 W. The analysis for strongly convex functions by Shalev-Shwartz et al. (2009) and exp-concave functions by Koren and Levy (2015) utilize the tool of stability, which only produces risk bounds that hold in expectation.", "startOffset": 67, "endOffset": 696}, {"referenceID": 6, "context": "Lower bounds of ERM for stochastic optimization is investigated by Feldman (2016), who exhibits (i) a lower bound of \u03a9(d/\u01eb2) sample complexity for uniform convergence that nearly matches the upper bound of Shalev-Shwartz et al. (2009); and (ii) a lower bound of \u03a9(d/\u01eb) sample complexity of ERM, which is matched by our \u00d5(d/n + \u221a F\u2217/n) bound when F\u2217 is small. It is worth mentioning the difference among proof techniques in these works. The uniform convergence result of Shalev-Shwartz et al. (2009) leverages the covering number to bound |F\u0302 (w) \u2212 F (w)| for any w \u2208 W. The analysis for strongly convex functions by Shalev-Shwartz et al. (2009) and exp-concave functions by Koren and Levy (2015) utilize the tool of stability, which only produces risk bounds that hold in expectation. A simple way to achieve a high probability bound is to use ERM combined with a generic or specific boosting-the-confidence method (Mehta, 2016; Haussler et al., 1991), but the guarantee is not directly on the empirical minimizer as noted by Shalev-Shwartz et al. (2009). The convergence of ERM given by Mehta (2016) relies on a central condition or \u201cstochastic mixability\u201d of the exp-concave function.", "startOffset": 67, "endOffset": 1055}, {"referenceID": 6, "context": "Lower bounds of ERM for stochastic optimization is investigated by Feldman (2016), who exhibits (i) a lower bound of \u03a9(d/\u01eb2) sample complexity for uniform convergence that nearly matches the upper bound of Shalev-Shwartz et al. (2009); and (ii) a lower bound of \u03a9(d/\u01eb) sample complexity of ERM, which is matched by our \u00d5(d/n + \u221a F\u2217/n) bound when F\u2217 is small. It is worth mentioning the difference among proof techniques in these works. The uniform convergence result of Shalev-Shwartz et al. (2009) leverages the covering number to bound |F\u0302 (w) \u2212 F (w)| for any w \u2208 W. The analysis for strongly convex functions by Shalev-Shwartz et al. (2009) and exp-concave functions by Koren and Levy (2015) utilize the tool of stability, which only produces risk bounds that hold in expectation. A simple way to achieve a high probability bound is to use ERM combined with a generic or specific boosting-the-confidence method (Mehta, 2016; Haussler et al., 1991), but the guarantee is not directly on the empirical minimizer as noted by Shalev-Shwartz et al. (2009). The convergence of ERM given by Mehta (2016) relies on a central condition or \u201cstochastic mixability\u201d of the exp-concave function.", "startOffset": 67, "endOffset": 1101}, {"referenceID": 12, "context": "class H (Koltchinskii, 2011).", "startOffset": 8, "endOffset": 28}, {"referenceID": 30, "context": "In fact, uniform convergence is a sufficient condition for learnability (Shalev-Shwartz and Ben-David, 2014), and in some special cases such as binary classification, it is also a necessary condition (Vapnik, 1998).", "startOffset": 72, "endOffset": 108}, {"referenceID": 2, "context": ", H contains linear functions with low-norm, implying an O(1/ \u221a n) risk bound (Bartlett and Mendelson, 2002).", "startOffset": 78, "endOffset": 108}, {"referenceID": 16, "context": "There have been intensive efforts to derive rates faster than O(1/ \u221a n) under various conditions (Lee et al., 1996; Panchenko, 2002; Bartlett et al., 2005; Gonen and Shalev-Shwartz, 2016), such as low-noise (Tsybakov, 2004), smoothness (Srebro et al.", "startOffset": 97, "endOffset": 187}, {"referenceID": 25, "context": "There have been intensive efforts to derive rates faster than O(1/ \u221a n) under various conditions (Lee et al., 1996; Panchenko, 2002; Bartlett et al., 2005; Gonen and Shalev-Shwartz, 2016), such as low-noise (Tsybakov, 2004), smoothness (Srebro et al.", "startOffset": 97, "endOffset": 187}, {"referenceID": 3, "context": "There have been intensive efforts to derive rates faster than O(1/ \u221a n) under various conditions (Lee et al., 1996; Panchenko, 2002; Bartlett et al., 2005; Gonen and Shalev-Shwartz, 2016), such as low-noise (Tsybakov, 2004), smoothness (Srebro et al.", "startOffset": 97, "endOffset": 187}, {"referenceID": 7, "context": "There have been intensive efforts to derive rates faster than O(1/ \u221a n) under various conditions (Lee et al., 1996; Panchenko, 2002; Bartlett et al., 2005; Gonen and Shalev-Shwartz, 2016), such as low-noise (Tsybakov, 2004), smoothness (Srebro et al.", "startOffset": 97, "endOffset": 187}, {"referenceID": 34, "context": ", 2005; Gonen and Shalev-Shwartz, 2016), such as low-noise (Tsybakov, 2004), smoothness (Srebro et al., 2010), strong convexity (Sridharan et al.", "startOffset": 88, "endOffset": 109}, {"referenceID": 35, "context": ", 2010), strong convexity (Sridharan et al., 2009), to name a few amongst many.", "startOffset": 26, "endOffset": 50}, {"referenceID": 2, "context": ", H contains linear functions with low-norm, implying an O(1/ \u221a n) risk bound (Bartlett and Mendelson, 2002). There have been intensive efforts to derive rates faster than O(1/ \u221a n) under various conditions (Lee et al., 1996; Panchenko, 2002; Bartlett et al., 2005; Gonen and Shalev-Shwartz, 2016), such as low-noise (Tsybakov, 2004), smoothness (Srebro et al., 2010), strong convexity (Sridharan et al., 2009), to name a few amongst many. Specifically, when the random function f(\u00b7) is nonnegative and smooth, Srebro et al. (2010) have established a risk bound of \u00d5(Rn(H) + Rn(H) \u221a F\u2217), reducing to an \u00d5(1/n) bound if Rn(H) = O(1/ \u221a n) and F\u2217 = O(1/n).", "startOffset": 79, "endOffset": 532}, {"referenceID": 2, "context": ", H contains linear functions with low-norm, implying an O(1/ \u221a n) risk bound (Bartlett and Mendelson, 2002). There have been intensive efforts to derive rates faster than O(1/ \u221a n) under various conditions (Lee et al., 1996; Panchenko, 2002; Bartlett et al., 2005; Gonen and Shalev-Shwartz, 2016), such as low-noise (Tsybakov, 2004), smoothness (Srebro et al., 2010), strong convexity (Sridharan et al., 2009), to name a few amongst many. Specifically, when the random function f(\u00b7) is nonnegative and smooth, Srebro et al. (2010) have established a risk bound of \u00d5(Rn(H) + Rn(H) \u221a F\u2217), reducing to an \u00d5(1/n) bound if Rn(H) = O(1/ \u221a n) and F\u2217 = O(1/n). A generalized linear form of (2) is studied by Sridharan et al. (2009), and a risk bound of O(1/\u03bbn) is proved if the expected function F (\u00b7) is \u03bb-strongly convex.", "startOffset": 79, "endOffset": 725}, {"referenceID": 14, "context": "3 SA for Stochastic Optimization Stochastic approximation (SA) solves the stochastic optimization problem via noisy observations of the expected function (Kushner and Yin, 2003).", "startOffset": 154, "endOffset": 177}, {"referenceID": 22, "context": "For Lipschitz continuous convex functions, stochastic gradient descent (SGD) exhibits the optimal O(1/ \u221a n) risk bound (Nemirovski and Yudin, 1983).", "startOffset": 119, "endOffset": 147}, {"referenceID": 9, "context": "If F (\u00b7) is \u03bb-strongly convex, some variants of SGD (Hazan and Kale, 2011; Rakhlin et al., 2012) achieve an O(1/\u03bbn) rate which is known to be minimax optimal (Agarwal et al.", "startOffset": 52, "endOffset": 96}, {"referenceID": 28, "context": "If F (\u00b7) is \u03bb-strongly convex, some variants of SGD (Hazan and Kale, 2011; Rakhlin et al., 2012) achieve an O(1/\u03bbn) rate which is known to be minimax optimal (Agarwal et al.", "startOffset": 52, "endOffset": 96}, {"referenceID": 0, "context": ", 2012) achieve an O(1/\u03bbn) rate which is known to be minimax optimal (Agarwal et al., 2012).", "startOffset": 69, "endOffset": 91}, {"referenceID": 1, "context": "For the square loss and the logistic loss, an O(1/n) rate is attainable without any strong convexity assumptions (Bach and Moulines, 2013).", "startOffset": 113, "endOffset": 138}, {"referenceID": 10, "context": "When the random function f(\u00b7) is \u03b7-exp-concave, the online Newton step (ONS) is equipped with an \u00d5(d/\u03b7n) risk bound (Hazan et al., 2007; Mahdavi et al., 2015).", "startOffset": 116, "endOffset": 158}, {"referenceID": 17, "context": "When the random function f(\u00b7) is \u03b7-exp-concave, the online Newton step (ONS) is equipped with an \u00d5(d/\u03b7n) risk bound (Hazan et al., 2007; Mahdavi et al., 2015).", "startOffset": 116, "endOffset": 158}, {"referenceID": 31, "context": "In the literature, the most comparable result is the O(1/\u03bbn) risk bound proved by Shalev-Shwartz et al. (2009) but with striking differences highlighted in Table 1.", "startOffset": 82, "endOffset": 111}, {"referenceID": 31, "context": "In the literature, the most comparable result is the O(1/\u03bbn) risk bound proved by Shalev-Shwartz et al. (2009) but with striking differences highlighted in Table 1. Since the risk bound of Shalev-Shwartz et al. (2009) is independent of the dimensionality d, it is natural to ask whether it is possible to prove a dimensionality-independent \u00d5(\u03ba/n) bound that holds with high probability.", "startOffset": 82, "endOffset": 218}, {"referenceID": 11, "context": "Recently, a variance reduction technique named SVRG (Johnson and Zhang, 2013) or EMGD (Zhang et al.", "startOffset": 52, "endOffset": 77}, {"referenceID": 6, "context": "Remark 7 In a recent work, Feldman (2016) shows that SCO without the Lipschitz condition cannot be solved by ERM.", "startOffset": 27, "endOffset": 42}, {"referenceID": 29, "context": ", kernel methods (Sch\u00f6lkopf and Smola, 2002).", "startOffset": 17, "endOffset": 44}, {"referenceID": 35, "context": "Remark 9 The first part of Theorem 8 presents an O(\u03ba/n) risk bound,3 similar to the O(1/\u03bbn) risk bound of Sridharan et al. (2009). The second part is an O(1/[\u03bbn2] + \u03baH\u2217/n) risk bound, and in this case, the lower bound of n is \u03a9(\u03ba2), which is dimensionalityindependent.", "startOffset": 106, "endOffset": 130}, {"referenceID": 4, "context": "1 The Key Idea By the convexity of F\u0302 (\u00b7) and the optimality condition of \u0175 (Boyd and Vandenberghe, 2004), we have \u3008\u2207F\u0302 (\u0175),w \u2212 \u0175\u3009 \u2265 0, \u2200w \u2208 W.", "startOffset": 76, "endOffset": 105}, {"referenceID": 33, "context": "4 Based on the concentration inequality of vectors (Smale and Zhou, 2007), we establish a uniform convergence of \u2207F (w) \u2212 \u2207F (w\u2217) to \u2207F\u0302 (w)\u2212\u2207F\u0302 (w\u2217) over any w \u2208 N (W, \u03b5).", "startOffset": 51, "endOffset": 73}, {"referenceID": 33, "context": "3 Proof of Lemma 1 We introduce Lemma 2 of Smale and Zhou (2007). Lemma 3 Let H be a Hilbert space and let \u03be be a random variable with values in H.", "startOffset": 43, "endOffset": 65}, {"referenceID": 24, "context": "7) of Nesterov (2004), we have \u2016\u2207fi(w)\u2212\u2207fi(w\u2217)\u2016 \u2264 L (fi(w)\u2212 fi(w\u2217)\u2212 \u3008\u2207fi(w\u2217),w \u2212w\u2217\u3009) .", "startOffset": 6, "endOffset": 22}, {"referenceID": 26, "context": "According to a standard volume comparison argument (Pisier, 1989), we have", "startOffset": 51, "endOffset": 65}, {"referenceID": 34, "context": "1 of Srebro et al. (2010), we have", "startOffset": 5, "endOffset": 26}], "year": 2017, "abstractText": "Although there exist plentiful theories of empirical risk minimization (ERM) for supervised learning, current theoretical understandings of ERM for a related problem\u2014stochastic convex optimization (SCO), are limited. In this work, we strengthen the realm of ERM for SCO by exploiting smoothness and strong convexity conditions to improve the risk bounds. First, we establish an \u00d5(d/n+ \u221a F\u2217/n) risk bound when the random function is nonnegative, convex and smooth, and the expected function is Lipschitz continuous, where d is the dimensionality of the problem, n is the number of samples, and F\u2217 is the minimal risk. Thus, when F\u2217 is small we obtain an \u00d5(d/n) risk bound, which is analogous to the \u00d5(1/n) optimistic rate of ERM for supervised learning. Second, if the objective function is also \u03bb-strongly convex, we prove an \u00d5(d/n + \u03baF\u2217/n) risk bound where \u03ba is the condition number, and improve it to O(1/[\u03bbn] + \u03baF\u2217/n) when n = \u03a9\u0303(\u03bad). As a result, we obtain an O(\u03ba/n) risk bound under the condition that n is large and F\u2217 is small, which to the best of our knowledge, is the first O(1/n)-type of risk bound of ERM. Third, we stress that the above results are established in a unified framework, which allows us to derive new risk bounds under weaker conditions, e.g., without convexity of the random function and Lipschitz continuity of the expected function. Finally, we demonstrate that to achieve an O(1/[\u03bbn] + \u03baF\u2217/n) risk bound for supervised learning, the \u03a9\u0303(\u03bad) requirement on n can be replaced with \u03a9(\u03ba), which is dimensionality-independent.", "creator": "LaTeX with hyperref package"}}}