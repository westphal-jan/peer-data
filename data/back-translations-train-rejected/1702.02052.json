{"id": "1702.02052", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "Knowledge Adaptation: Teaching to Adapt", "abstract": "Domain adaptation is crucial in many real-world applications where the distribution of the training data differs from the distribution of the test data. Previous Deep Learning-based approaches to domain adaptation need to be trained jointly on source and target domain data and are therefore unappealing in scenarios where models need to be adapted to a large number of domains or where a domain is evolving, e.g. spam detection where attackers continuously change their tactics.", "histories": [["v1", "Tue, 7 Feb 2017 14:59:45 GMT  (3529kb,D)", "http://arxiv.org/abs/1702.02052v1", "11 pages, 4 figures, 2 tables"]], "COMMENTS": "11 pages, 4 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sebastian ruder", "parsa ghaffari", "john g breslin"], "accepted": false, "id": "1702.02052"}, "pdf": {"name": "1702.02052.pdf", "metadata": {"source": "CRF", "title": "Knowledge Adaptation: Teaching to Adapt", "authors": ["Sebastian Ruder", "Parsa Ghaffari", "John G. Breslin"], "emails": ["sebastian.ruder@insight-centre.org", "john.breslin@insight-centre.org", "sebastian@aylien.com", "parsa@aylien.com"], "sections": [{"heading": null, "text": "To fill this gap, we propose Knowledge Adaptation, an extension of knowledge distillation (Bucilua et al., 2006; Hinton et al., 2015) to the domain adaptation scenario. We show how a student model, using a standard mood benchmark, achieves up-to-date results on unattended domain adaptation from multiple sources by taking into account the domain-specific expertise of several teachers and the similarities between their domains.When learning from a single teacher, the use of domain similarity to measure trustworthiness is insufficient. To this end, we propose a simple metric that correlates well with the accuracy of the teacher in the target area. We show that incorporating this metric of selected examples with high confidence enables the student model to achieve excellence in the single-source scenario."}, {"heading": "1 Introduction", "text": "In many real-world applications, such as sensation classification (Pang and Lee, 2008), such a strategy is impractical (Pang and Lee, 2008), a model based on one domain cannot work well when applied directly to another domain, due to the difference in data distribution between domains. At the same time, labeled data in new domains is rare or non-existent, and manually labeling large amounts of target domain data is expensive. Domain adaptation allows models to reduce domain discrepancy and adapt to new domain data. While fine-tuning is a commonly used method for managed domain adaptation, there is no cheap equivalent in the unattended world as existing deep-learning-based approaches that need to be trained jointly on source and target domain data. This is prohibitive in scenarios with a large number of domain types, such as blog sensitization of realworld categories, or Hamilton common categories."}, {"heading": "2 Related work", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "3 Knowledge Adaptation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Problem definition", "text": "In the following, we describe the domain matching within the framework of knowledge matching: We receive one or more source domains DSi and a target domain DT. For each of the source domains, we receive a teacher model Ti, which was trained using examples XSi = {xSi1, \u00b7 \u00b7, xSin} and their designations {y Si 1, \u00b7 \u00b7, y Si 1} from DSi. In the target domain DT, we only have access to the examples {xT1, \u00b7 \u00b7, xTn} without knowing their designations. Please note that below, for the sake of simplicity, we omit source and target domain indexes when the examples are clear. Our task now is to train a student model S that performs well on invisible examples from the target domain DT."}, {"heading": "3.2 Single teacher-student model", "text": "Our teacher and student models are simple multi-layer perceptrons (MLP). The basic MLP consists of an input layer, one or more intermediate layers and an output layer. Each interlayer \"learns to embed the output of the previous layer x in a latent representation h '= f' (W 'x + b'), where W 'and b' are the weights and prejudices of the\" th layer, \"while f 'is the activation, typically ReLU fl (x) = max {0, x} for hidden layers and Softmax units fl (x) = Softmax (x) = Softmax (x) = ex / \u2211 | i = 1 Exi for the output layer. In the detail source setting, the teacher T has an output Softmax PT = Softmax (zT) for hidden layers and Softmax units fl (x) for Softmax units fl (x) = Softmax labels for the output layer, the T is trained to T (T)."}, {"heading": "3.3 Multiple teacher-student model", "text": "The teacher-student paradigm is, of course, appropriate for the multi-domain scenario. Intuitively, the trust a student should place in a teacher should be proportional to the degree of similarity between the teacher's domain and the student's domain. To this end, we look at three measures of domain similarity, both based on Kullback-Leibler divergence and successfully applied to domain adaptation research: JensenShannon divergence (Remus, 2012) and Renyi divergence (Van Asch and Daelemans, 2010), both based on Kullback-Leibler divergence and calculated taking into account domain termination distributions; and Maximum Mean Discrepancy (Tzeng et al., 2014), which we calculate in terms of the latent representation of the teacher. These measures are calculated between the target domain DT and each quell domain DT (using additional information from our choice and A.T)."}, {"heading": "3.4 Leveraging a single teacher\u2019s knowledge", "text": "In the scenario with a single teacher, it is not helpful to know whether we can trust the teacher in general. Rather, we want a measurement that allows us to determine whether we can trust the teacher for a particular example. To arrive at such a measurement, we review the representations that the teacher learns from the input data: xSCross-entropyTeacher modelyS (a) Teacher model objective reliability Teacher model student model student model (b) Student model, in order to make precise predictions, the teacher model learns to separate the representation of the different starting classes in its hidden representation (we use a single-layer MLP in our experiments as detailed in \u00a7 4.2; in deeper networks, this would be an intermediate layer). Even if the teacher model is trained on the source domain, this separation keeps - albeit with reduced accuracy - in the target group that can be seen."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data set", "text": "The data set consists of 4 different domains: Book (B), DVDs (D), Electronics (E) and Kitchen (K). We follow the conventions of past work and evaluate according to the task of binary classification, where reviews with more than 3 stars are considered positive and ratings with 3 stars or less are considered negative. Each domain contains 1,000 positive, 1,000 negative and about 4,000 blank reviews. To be fair, we use the characteristics pre-processed with tf-idf as input (Blitzer et al., 2006). For single-source customization, we replicate the setup of previous methods and train our teacher models on all 2,000 designated examples, of which we reserve 200 as dev-sets. For domain customization from multiple sources, we follow the conventions of Bollegala in general (2011) and limit our teacher models to 1,000 labeled examples each."}, {"heading": "4.2 Hyperparameters", "text": "Both student and teacher models are single-layer MLPs with 1,000 hidden dimensions. We use a vocabulary size of 10,000, a temperature of 5, a batch size of 10, and Adam (Kingma and Ba, 2015) as an optimizer with a learning rate of 0.001. For each experiment, we give the average of 10 runs."}, {"heading": "4.3 Domain adaptation from multiple sources", "text": "It is easier for students to have confidence if they learn from several teachers than from others."}, {"heading": "4.4 Single-source domain adaptation", "text": "We also evaluate the student's ability to learn from a single teacher. This scenario is more difficult because the student cannot take into account other teachers who provide more relevant predictions; for each target domain, each of the other three domains is used as the source domain, creating 12 pairs of domains. Firstly, on these domains, we evaluate our student-teacher model (TS). For the training of a model that contains highly confidential teacher predictions (TS-MCD), we cross the interpolation parameters in Equation 5 and the number of examples with the highest MCD values. We find that a low level (around 0.2) provides the best results in the domain fitting situation, as the high confidence predictions are helpful to guide the student's learning during training."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed Knowledge Adaptation, an extension of the idea of knowledge distillation to the domain adaptation scenario. This method, unlike dominant domain adaptation methods, is able to perform adaptation without retraining. First, we demonstrated the utility of this paradigm by showing that a student model that takes into account the predictions of multiple teachers and their domain similarities is capable of exceeding the current state of the art for multi-source domain adaptation without supervision using a standard sentiment analysis benchmark. In addition, we introduced a simple metric to measure the trustworthiness of a single teacher and demonstrated how this metric can be used to achieve unattended domain results for 8 out of 12 pairs of domain names for single source adaptation."}, {"heading": "A Appendix", "text": "We use three measures of domain similarity in our experiments: Jensen-Shannon divergence, Renyi divergence, and Maximum Mean Discrepancy (MMD).Jensen-Shannon divergence is a smoothed, symmetrical variant of KL divergence. The divergence between two different probability distributions P and Q can be called: DJS (P | | Q) = 12 [DKL (Q | M)] (6) whereM = 12 (P + Q), i.e. the average distribution of P and Q is KL divergence. (P | Q) = 1 pi pi qi qi qi qi (7) Renyi divergence similarly generalizes KL divergence by assigning different weights to the probability distribution of the source and target domain and is defined as follows:"}], "references": [{"title": "DomainAdversarial Training of Neural Networks", "author": ["Hana Ajakan", "Hugo Larochelle", "Mario Marchand", "Victor Lempitsky."], "venue": "Journal of Machine Learning Research 17:1\u201335. https://doi.org/10.1088/1475-7516/2015/08/013.", "citeRegEx": "Ajakan et al\\.,? 2016", "shortCiteRegEx": "Ajakan et al\\.", "year": 2016}, {"title": "Analysis of representations for domain adaptation", "author": ["Shai Ben-David", "John Blitzer", "Koby Crammer", "Fernando Pereira."], "venue": "Advances in Neural Information Processing Systems 19:137\u2013144.", "citeRegEx": "Ben.David et al\\.,? 2007", "shortCiteRegEx": "Ben.David et al\\.", "year": 2007}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["John Blitzer", "Mark Dredze", "Fernando Pereira."], "venue": "Annual Meeting-Association for Computational Linguistics 45(1):440.", "citeRegEx": "Blitzer et al\\.,? 2007", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Domain Adaptation with Structural Correspondence Learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "EMNLP \u201906 Proceedings", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification", "author": ["Danushka Bollegala", "David Weir", "John Carroll."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational", "citeRegEx": "Bollegala et al\\.,? 2011", "shortCiteRegEx": "Bollegala et al\\.", "year": 2011}, {"title": "Domain Separation Networks", "author": ["Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan."], "venue": "NIPS .", "citeRegEx": "Bousmalis et al\\.,? 2016", "shortCiteRegEx": "Bousmalis et al\\.", "year": 2016}, {"title": "Model compression", "author": ["Cristian Bucilua", "Rich Caruana", "Alexandru Niculescu-Mizil."], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD \u201906 page 535.", "citeRegEx": "Bucilua et al\\.,? 2006", "shortCiteRegEx": "Bucilua et al\\.", "year": 2006}, {"title": "Multi-Source Domain Adaptation and Its Application to Early Detection of Fatigue", "author": ["Rita Chattopadhyay", "Qian Sun", "Jieping Ye", "Sethuraman Panchanathan", "W E I Fan", "I A N Davidson."], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD)", "citeRegEx": "Chattopadhyay et al\\.,? 2012", "shortCiteRegEx": "Chattopadhyay et al\\.", "year": 2012}, {"title": "Marginalized Denoising Autoencoders for Domain Adaptation", "author": ["Minmin Chen", "Zhixiang Xu", "Kilian Q. Weinberger", "Fei Sha."], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML-12) pages 767\u2014-774.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Choosing your informant: weighing familiarity and recent accuracy", "author": ["Kathleen Corriveau", "Paul L Harris."], "venue": "Developmental science 12(3):426\u2013437.", "citeRegEx": "Corriveau and Harris.,? 2009", "shortCiteRegEx": "Corriveau and Harris.", "year": 2009}, {"title": "Frustratingly Easy Domain Adaptation", "author": ["Hal Daum\u00e9 III."], "venue": "Association for Computational Linguistic (ACL)s (June):256\u2013263. https://doi.org/10.1.1.110.2062.", "citeRegEx": "III.,? 2007", "shortCiteRegEx": "III.", "year": 2007}, {"title": "Domain Adaptation from Multiple Sources via Auxiliary Classifiers", "author": ["Lixin Duan", "Ivor W. Tsang", "Dong Xu", "Tat-Seng Chua."], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning.", "citeRegEx": "Duan et al\\.,? 2009", "shortCiteRegEx": "Duan et al\\.", "year": 2009}, {"title": "Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."], "venue": "Proceedings of the 28th International Conference on Machine Learning (1):513\u2013520. http://www.icml-", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Inducing DomainSpecific Sentiment Lexicons from Unlabeled Corpora", "author": ["William L. Hamilton", "Kevin Clark", "Jure Leskovec", "Dan Jurafsky."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Hamilton et al\\.,? 2016", "shortCiteRegEx": "Hamilton et al\\.", "year": 2016}, {"title": "Distilling the Knowledge in a Neural Network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean."], "venue": "arXiv preprint arXiv:1503.02531 pages 1\u20139. https://doi.org/10.1063/1.4931082.", "citeRegEx": "Hinton et al\\.,? 2015", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Continuous manifold based adaptation for evolving visual domains", "author": ["Judy Hoffman", "Trevor Darrell", "Kate Saenko."], "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition pages 867\u2013874.", "citeRegEx": "Hoffman et al\\.,? 2014", "shortCiteRegEx": "Hoffman et al\\.", "year": 2014}, {"title": "Harnessing Deep Neural Networks with Logic Rules", "author": ["Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eduard Hovy", "Eric Xing."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. pages 1\u201318.", "citeRegEx": "Hu et al\\.,? 2016", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Instance Weighting for Domain Adaptation in NLP", "author": ["Jing Jiang", "ChengXiang Zhai."], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (October):264\u2013 271. https://doi.org/10.1145/1273496.1273558.", "citeRegEx": "Jiang and Zhai.,? 2007", "shortCiteRegEx": "Jiang and Zhai.", "year": 2007}, {"title": "Cognitive and Affective Trust in Service Relationships", "author": ["Devon Johnson", "Kent Grayson."], "venue": "Journal of Business research 58(4):500\u2013507. https://doi.org/10.1016/S0148-2963(03)00140-1.", "citeRegEx": "Johnson and Grayson.,? 2005", "shortCiteRegEx": "Johnson and Grayson.", "year": 2005}, {"title": "SequenceLevel Knowledge Distillation", "author": ["Yoon Kim", "Alexander M Rush."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16) .", "citeRegEx": "Kim and Rush.,? 2016", "shortCiteRegEx": "Kim and Rush.", "year": 2016}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba."], "venue": "International Conference on Learning Representations pages 1\u2013", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Learning small-size DNN with output-distribution-based criteria", "author": ["Jinyu Li", "Rui Zhao", "Jui Ting Huang", "Yifan Gong."], "venue": "Proceedings of the Annual Conference of the International Speech Communication Association, INTER-", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Multidomain Adaptation for Sentiment Classication: Using Multiple Classifier Combining Methods", "author": ["Shoushan Li", "Chengqing Zong."], "venue": "International Conference on Natural Language Processing and Knowledge Engineering (NLP-KE\u201908).", "citeRegEx": "Li and Zong.,? 2008", "shortCiteRegEx": "Li and Zong.", "year": 2008}, {"title": "Learning Multiple Tasks with Deep Relationship Networks", "author": ["Mingsheng Long", "Jianmin Wang."], "venue": "Arxiv pages 1\u20139. http://arxiv.org/abs/1506.02117.", "citeRegEx": "Long and Wang.,? 2015", "shortCiteRegEx": "Long and Wang.", "year": 2015}, {"title": "Unifying distillation and privileged information", "author": ["David Lopez-Paz", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "Vladimir Vapnik."], "venue": "ICLR http://arxiv.org/abs/1511.03643.", "citeRegEx": "Lopez.Paz et al\\.,? 2016", "shortCiteRegEx": "Lopez.Paz et al\\.", "year": 2016}, {"title": "Domain Adaptation with Multiple Sources", "author": ["Yishay Mansour."], "venue": "NIPS .", "citeRegEx": "Mansour.,? 2009", "shortCiteRegEx": "Mansour.", "year": 2009}, {"title": "Evaluation of Adaptive Mixture of Competing Experts", "author": ["Steven J. Nowlan", "Geoffrey E. Hinton."], "venue": "NIPS.", "citeRegEx": "Nowlan and Hinton.,? 1990", "shortCiteRegEx": "Nowlan and Hinton.", "year": 1990}, {"title": "Cross-Domain Sentiment Classification via Spectral Feature Alignment", "author": ["Sinno Jialin Pan", "Xiaochuan Ni", "Jian-tao Sun", "Qiang Yang", "Zheng Chen."], "venue": "Proceedings of the 19th International Conference on World Wide Web. pages 751\u2013760.", "citeRegEx": "Pan et al\\.,? 2010", "shortCiteRegEx": "Pan et al\\.", "year": 2010}, {"title": "Opinion Mining and Sentiment Analysis", "author": ["Bo Pang", "Lillian Lee."], "venue": "Foundations and trends in information retrieval 2(1-2):1\u2013135. https://doi.org/10.1561/1500000001.", "citeRegEx": "Pang and Lee.,? 2008", "shortCiteRegEx": "Pang and Lee.", "year": 2008}, {"title": "Domain adaptation using Domain Similarity- and Domain Complexity-based Instance Selection for Cross-Domain Sentiment Analysis", "author": ["Robert Remus."], "venue": "IEEE ICDM SENTIRE-2012.", "citeRegEx": "Remus.,? 2012", "shortCiteRegEx": "Remus.", "year": 2012}, {"title": "Fitnets: Hints for Thin Deep Nets", "author": ["Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio."], "venue": "ICLR pages 1\u201313. http://arxiv.org/pdf/1412.6550.pdf.", "citeRegEx": "Romero et al\\.,? 2015", "shortCiteRegEx": "Romero et al\\.", "year": 2015}, {"title": "Deep Domain Confusion: Maximizing for Domain Invariance", "author": ["Eric Tzeng", "Judy Hoffman", "Ning Zhang", "Kate Saenko", "Trevor Darrell."], "venue": "CoRR https://arxiv.org/pdf/1412.3474.pdf.", "citeRegEx": "Tzeng et al\\.,? 2014", "shortCiteRegEx": "Tzeng et al\\.", "year": 2014}, {"title": "Using Domain Similarity for Performance Estimation", "author": ["Vincent Van Asch", "Walter Daelemans."], "venue": "Computational Linguistics (July):31\u201336. http://eprints.pascalnetwork.org/archive/00007014/.", "citeRegEx": "Asch and Daelemans.,? 2010", "shortCiteRegEx": "Asch and Daelemans.", "year": 2010}, {"title": "Sentiment Domain Adaptation with Multiple Sources", "author": ["Fangzhao Wu", "Yongfeng Huang."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016) pages 301\u2013310.", "citeRegEx": "Wu and Huang.,? 2016", "shortCiteRegEx": "Wu and Huang.", "year": 2016}, {"title": "Transfer Learning for Multiple-Domain Sentiment Analysis - Identifying Domain Dependent/Independent Word Polarity", "author": ["Yasuhisa Yoshida", "Tsutomu Hirao", "Tomoharu Iwata", "Masaaki Nagata", "Yuji Matsumoto."], "venue": "Proceedings of the Twenty-Fifth", "citeRegEx": "Yoshida et al\\.,? 2011", "shortCiteRegEx": "Yoshida et al\\.", "year": 2011}, {"title": "KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition", "author": ["Dong Yu", "Kaisheng Yao", "Hang Su", "Gang Li", "Frank Seide."], "venue": "ICASSP, IEEE International Conference on Acoustics, Speech and", "citeRegEx": "Yu et al\\.,? 2013", "shortCiteRegEx": "Yu et al\\.", "year": 2013}, {"title": "Linking Heterogeneous Input Features with Pivots for Domain Adaptation", "author": ["Guangyou Zhou", "Tingting He", "Wensheng Wu", "Xiaohua Tony Hu."], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015)", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "BiTransferring Deep Neural Networks for Domain Adaptation", "author": ["Guangyou Zhou", "Zhiwen Xie", "Jimmy Xiangji Huang", "Tingting He."], "venue": "ACL pages 322\u2013332. https://www.aclweb.org/anthology/P/P16/P16-", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "Supervised Representation Learning: Transfer Learning with Deep Autoencoders", "author": ["Fuzhen Zhuang", "Xiaohu Cheng", "Ping Luo", "Sinno Jialin Pan", "Qing He."], "venue": "IJCAI International Joint Conference on Artificial Intelligence pages 4119\u20134125.", "citeRegEx": "Zhuang et al\\.,? 2015", "shortCiteRegEx": "Zhuang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "To fill this gap, we propose Knowledge Adaptation, an extension of Knowledge Distillation (Bucilua et al., 2006; Hinton et al., 2015) to the domain adaptation sce-", "startOffset": 90, "endOffset": 133}, {"referenceID": 14, "context": "To fill this gap, we propose Knowledge Adaptation, an extension of Knowledge Distillation (Bucilua et al., 2006; Hinton et al., 2015) to the domain adaptation sce-", "startOffset": 90, "endOffset": 133}, {"referenceID": 28, "context": "In many real-world applications such as sentiment classification (Pang and Lee, 2008), a model trained on one domain may not work well when directly applied to another domain due to the difference in the data distribution between the domains.", "startOffset": 65, "endOffset": 85}, {"referenceID": 13, "context": "world review categories, blog types, or communities (Hamilton et al., 2016).", "startOffset": 52, "endOffset": 75}, {"referenceID": 15, "context": "classification where the scene changes over time (Hoffman et al., 2014), or a conversational agent for a user with a rapidly evolving style, such as a child or second language learner.", "startOffset": 49, "endOffset": 71}, {"referenceID": 6, "context": "This objective aligns organically with the idea of Knowledge Distillation (Bucilua et al., 2006; Hinton et al., 2015), which we extend as Knowledge Adaptation to the domain adaptation scenario.", "startOffset": 74, "endOffset": 117}, {"referenceID": 14, "context": "This objective aligns organically with the idea of Knowledge Distillation (Bucilua et al., 2006; Hinton et al., 2015), which we extend as Knowledge Adaptation to the domain adaptation scenario.", "startOffset": 74, "endOffset": 117}, {"referenceID": 9, "context": "real-world teacher-student and adviser-advisee relationships: Children learn early on to trust familiar advisers but to moderate that trust depending on the adviser\u2019s recent history of accuracy or inaccuracy (Corriveau and Harris, 2009), while adults may surround themselves with advisers, e.", "startOffset": 208, "endOffset": 236}, {"referenceID": 18, "context": "to make a financial investment and gradually learn whose expertise to trust (Johnson and Grayson, 2005).", "startOffset": 76, "endOffset": 103}, {"referenceID": 21, "context": "and (Li et al., 2014) for adapting an acoustic model and the Adaptive Mixture of Experts model (Nowlan and Hinton, 1990), which also learns which expert to trust for a given example.", "startOffset": 4, "endOffset": 21}, {"referenceID": 26, "context": ", 2014) for adapting an acoustic model and the Adaptive Mixture of Experts model (Nowlan and Hinton, 1990), which also learns which expert to trust for a given example.", "startOffset": 81, "endOffset": 106}, {"referenceID": 6, "context": "Bucilua et al. (2006) first proposed a method to compress the knowledge of a source model, which was later improved by Hinton et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "Bucilua et al. (2006) first proposed a method to compress the knowledge of a source model, which was later improved by Hinton et al. (2015). Romero et al.", "startOffset": 0, "endOffset": 140}, {"referenceID": 6, "context": "Bucilua et al. (2006) first proposed a method to compress the knowledge of a source model, which was later improved by Hinton et al. (2015). Romero et al. (2015) showed how this method can be adapted to train deep and thin models, while Kim and Rush (2016) apply the technique to sequence-level models.", "startOffset": 0, "endOffset": 162}, {"referenceID": 6, "context": "Bucilua et al. (2006) first proposed a method to compress the knowledge of a source model, which was later improved by Hinton et al. (2015). Romero et al. (2015) showed how this method can be adapted to train deep and thin models, while Kim and Rush (2016) apply the technique to sequence-level models.", "startOffset": 0, "endOffset": 257}, {"referenceID": 6, "context": "Bucilua et al. (2006) first proposed a method to compress the knowledge of a source model, which was later improved by Hinton et al. (2015). Romero et al. (2015) showed how this method can be adapted to train deep and thin models, while Kim and Rush (2016) apply the technique to sequence-level models. In addition, Hu et al. (2016) use it to constrain a student model with logic rules.", "startOffset": 0, "endOffset": 333}, {"referenceID": 6, "context": "Bucilua et al. (2006) first proposed a method to compress the knowledge of a source model, which was later improved by Hinton et al. (2015). Romero et al. (2015) showed how this method can be adapted to train deep and thin models, while Kim and Rush (2016) apply the technique to sequence-level models. In addition, Hu et al. (2016) use it to constrain a student model with logic rules. Our goal differs from the previous methods due to the difference in data distributions between source and target data, which necessitates to learn from the teacher\u2019s knowledge only insofar as it is useful for the target domain. Similar in spirit to Knowledge Distillation is the KLdivergence based objective by (2013) Yu et al.", "startOffset": 0, "endOffset": 705}, {"referenceID": 2, "context": "Domain adaptation has a long history of research: Blitzer et al. (2006) proposed a structural correspondence learning algorithm.", "startOffset": 50, "endOffset": 72}, {"referenceID": 2, "context": "Domain adaptation has a long history of research: Blitzer et al. (2006) proposed a structural correspondence learning algorithm. Daum\u00e9 III (2007) introduced a kernel function that maps source and target domain data to a space that encourages in-domain similarity, while Pan et al.", "startOffset": 50, "endOffset": 146}, {"referenceID": 2, "context": "Domain adaptation has a long history of research: Blitzer et al. (2006) proposed a structural correspondence learning algorithm. Daum\u00e9 III (2007) introduced a kernel function that maps source and target domain data to a space that encourages in-domain similarity, while Pan et al. (2010) proposed a spectral feature alignment algorithm to align domain-specific words into meaningful clusters, while Long and Wang (2015) use multi-task learning to avoid neg-", "startOffset": 50, "endOffset": 288}, {"referenceID": 2, "context": "Domain adaptation has a long history of research: Blitzer et al. (2006) proposed a structural correspondence learning algorithm. Daum\u00e9 III (2007) introduced a kernel function that maps source and target domain data to a space that encourages in-domain similarity, while Pan et al. (2010) proposed a spectral feature alignment algorithm to align domain-specific words into meaningful clusters, while Long and Wang (2015) use multi-task learning to avoid neg-", "startOffset": 50, "endOffset": 420}, {"referenceID": 12, "context": "Deep learning-based approaches to domain adaptation are more recent and have focused mainly on learning domain-invariant representations: Glorot et al. (2011) first employed stacked Denois-", "startOffset": 138, "endOffset": 159}, {"referenceID": 8, "context": "Chen et al. (2012) in turn extended SDA to marginalized SDA by addressing SDA\u2019s high computational cost and lack of scalability to high-dimensional features, while Zhuang", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "(Ajakan et al., 2016) added a Gradient Reversal Layer that hinders the model\u2019s ability to discriminate between domains.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "(Ajakan et al., 2016) added a Gradient Reversal Layer that hinders the model\u2019s ability to discriminate between domains. Finally, Zhou et al. (2016) transferred the source examples to the target domain and vice versa using BiTransferring Deep Neural Networks, while Bousmalis et al.", "startOffset": 1, "endOffset": 148}, {"referenceID": 0, "context": "(Ajakan et al., 2016) added a Gradient Reversal Layer that hinders the model\u2019s ability to discriminate between domains. Finally, Zhou et al. (2016) transferred the source examples to the target domain and vice versa using BiTransferring Deep Neural Networks, while Bousmalis et al. (2016) propose Domain Separation Networks.", "startOffset": 1, "endOffset": 289}, {"referenceID": 7, "context": "(2009) proposed a method to learn a least-squares SVM classifer by leveraging source classifiers, while (Chattopadhyay et al., 2012) assign pseudolabels to the target data.", "startOffset": 104, "endOffset": 132}, {"referenceID": 23, "context": "For domain adaptation from multiple sources, Mansour (2009) proposed a distribution weighted hypothesis with theoretical guarantees.", "startOffset": 45, "endOffset": 60}, {"referenceID": 10, "context": "Duan et al. (2009) proposed a method to learn a least-squares SVM classifer by leveraging source classifiers, while (Chattopadhyay et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "(2009) proposed a method to learn a least-squares SVM classifer by leveraging source classifiers, while (Chattopadhyay et al., 2012) assign pseudolabels to the target data. Finally, Wu and Huang (2016) exploit general sentiment knowledge and word-level sentiment polarity relations for multisource domain adaptation.", "startOffset": 105, "endOffset": 202}, {"referenceID": 14, "context": "In the context of knowledge distillation (Hinton et al., 2015), the student S is trained so that its output PS is similar to the teacher\u2019s output PT and to the true labels.", "startOffset": 41, "endOffset": 62}, {"referenceID": 24, "context": "Thus the student S is trained solely to mimic the teacher\u2019s softened output with the following loss, which is similar to treating source input modalities as privileged information (Lopez-Paz et al., 2016):", "startOffset": 180, "endOffset": 204}, {"referenceID": 29, "context": "used in domain adaptation research: JensenShannon divergence (Remus, 2012) and Renyi divergence (Van Asch and Daelemans, 2010), which are both based on Kullback-Leibler divergence and are computed with regard to the domains\u2019 term distributions; and Maximum Mean Discrepancy", "startOffset": 61, "endOffset": 74}, {"referenceID": 31, "context": "(Tzeng et al., 2014), which we compute with respect to the teacher\u2019s latent representation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 17, "context": "This can be seen as a representation-based variant of instance adaptation (Jiang and Zhai, 2007), which uses MCD as a measure of confidence as it correlates better with teacher accuracy", "startOffset": 74, "endOffset": 96}, {"referenceID": 3, "context": "For fairness of comparison, we use the raw bagof-words unigram/bigram features pre-processed with tf-idf as input (Blitzer et al., 2006).", "startOffset": 114, "endOffset": 136}, {"referenceID": 2, "context": "We use the Amazon product reviews sentiment analysis dataset of Blitzer et al. (2006), a common benchmark for domain adaptation.", "startOffset": 64, "endOffset": 86}, {"referenceID": 4, "context": "For domain adaptation from multiple sources, we follow the conventions of Bollegala et al. (2011) and limit the total number of training examples for all teachers to 1,600, i.", "startOffset": 74, "endOffset": 98}, {"referenceID": 5, "context": "set similar to (Bousmalis et al., 2016).", "startOffset": 15, "endOffset": 39}, {"referenceID": 20, "context": "a vocabulary size of 10,000, a temperature of 5, a batch size of 10, and Adam (Kingma and Ba, 2015) as optimizer with a learning rate of 0.", "startOffset": 78, "endOffset": 99}, {"referenceID": 3, "context": "against the following methods: domain adaptation with structural correspondence learning (SCL) (Blitzer et al., 2006); domain adaptation based on spectral feature alignment (SFA) (Pan et al.", "startOffset": 95, "endOffset": 117}, {"referenceID": 27, "context": ", 2006); domain adaptation based on spectral feature alignment (SFA) (Pan et al., 2010); adaptations of SCL and SFA via major-", "startOffset": 69, "endOffset": 87}, {"referenceID": 4, "context": "ity voting to the multi-source scenario (SCL-com and SFA-com); cross-domain sentiment classification by constructing a sentiment-sensitive thesaurus (SST) (Bollegala et al., 2011); multipledomain sentiment analysis by identifying domain", "startOffset": 155, "endOffset": 179}, {"referenceID": 34, "context": "dependent/independent word polarity (IDDIWP) (Yoshida et al., 2011); three general-purpose multiple source domain adaptation methods (DWHC, (Mansour, 2009)), (DAM, (Duan et al.", "startOffset": 45, "endOffset": 67}, {"referenceID": 25, "context": ", 2011); three general-purpose multiple source domain adaptation methods (DWHC, (Mansour, 2009)), (DAM, (Duan et al.", "startOffset": 80, "endOffset": 95}, {"referenceID": 11, "context": ", 2011); three general-purpose multiple source domain adaptation methods (DWHC, (Mansour, 2009)), (DAM, (Duan et al., 2009)), (CP-MDA, (Chattopadhyay et al.", "startOffset": 104, "endOffset": 123}, {"referenceID": 7, "context": ", 2009)), (CP-MDA, (Chattopadhyay et al., 2012)); crossdomain sentiment classification by transferring sentiment along a sentiment graph with hinge loss and logistic loss respectively (SDAMS-SVM and SDAMS-Log) (Wu and Huang, 2016).", "startOffset": 19, "endOffset": 47}, {"referenceID": 33, "context": ", 2012)); crossdomain sentiment classification by transferring sentiment along a sentiment graph with hinge loss and logistic loss respectively (SDAMS-SVM and SDAMS-Log) (Wu and Huang, 2016).", "startOffset": 170, "endOffset": 190}, {"referenceID": 7, "context": ", 2009)), (CP-MDA, (Chattopadhyay et al., 2012)); crossdomain sentiment classification by transferring sentiment along a sentiment graph with hinge loss and logistic loss respectively (SDAMS-SVM and SDAMS-Log) (Wu and Huang, 2016). Numbers are taken from Wu and Huang (2016).", "startOffset": 20, "endOffset": 275}, {"referenceID": 25, "context": "The student model outperforms comparison methods that rely on source model predictions by combining (Mansour, 2009) or predicting (Duan et al.", "startOffset": 100, "endOffset": 115}, {"referenceID": 11, "context": "The student model outperforms comparison methods that rely on source model predictions by combining (Mansour, 2009) or predicting (Duan et al., 2009) them.", "startOffset": 130, "endOffset": 149}, {"referenceID": 3, "context": "Book DVD Electronics Kitchen SCL (Blitzer et al., 2006) 0.", "startOffset": 33, "endOffset": 55}, {"referenceID": 27, "context": "8207 SFA (Pan et al., 2010) 0.", "startOffset": 9, "endOffset": 27}, {"referenceID": 4, "context": "8258 SST (Bollegala et al., 2011) 0.", "startOffset": 9, "endOffset": 33}, {"referenceID": 34, "context": "8518 IDDIWP (Yoshida et al., 2011) 0.", "startOffset": 12, "endOffset": 34}, {"referenceID": 25, "context": "8383 DWHC (Mansour, 2009) 0.", "startOffset": 10, "endOffset": 25}, {"referenceID": 11, "context": "8478 DAM (Duan et al., 2009) 0.", "startOffset": 9, "endOffset": 28}, {"referenceID": 7, "context": "8419 CP-MDA (Chattopadhyay et al., 2012) 0.", "startOffset": 12, "endOffset": 40}, {"referenceID": 33, "context": "8465 SDAMS-SVM (Wu and Huang, 2016) 0.", "startOffset": 15, "endOffset": 35}, {"referenceID": 33, "context": "8578 SDAMS-Log (Wu and Huang, 2016) 0.", "startOffset": 15, "endOffset": 35}, {"referenceID": 33, "context": "Both models improve over existing approaches to domain adaptation from multiple sources and outperform approaches that rely on sentiment analysisspecific information (Wu and Huang, 2016) in all", "startOffset": 166, "endOffset": 186}, {"referenceID": 27, "context": ", 2006) and SFA (Pan et al., 2010), as well as against multi-label consensus training (MCT), which combines base classifiers trained with SCL (Li and Zong, 2008) and against an approach that links heterogeneous input features with points via non-negative matrix factorization (PJNMF) (Zhou et al.", "startOffset": 16, "endOffset": 34}, {"referenceID": 22, "context": ", 2010), as well as against multi-label consensus training (MCT), which combines base classifiers trained with SCL (Li and Zong, 2008) and against an approach that links heterogeneous input features with points via non-negative matrix factorization (PJNMF) (Zhou et al.", "startOffset": 115, "endOffset": 134}, {"referenceID": 36, "context": ", 2010), as well as against multi-label consensus training (MCT), which combines base classifiers trained with SCL (Li and Zong, 2008) and against an approach that links heterogeneous input features with points via non-negative matrix factorization (PJNMF) (Zhou et al., 2015).", "startOffset": 257, "endOffset": 276}, {"referenceID": 12, "context": "We additionally compare against the following deep learning-based approaches: stacked denoising auto-encoders (SDA) (Glorot et al., 2011); marginalized SDA (mSDA) (Chen et al.", "startOffset": 116, "endOffset": 137}, {"referenceID": 8, "context": ", 2011); marginalized SDA (mSDA) (Chen et al., 2012); transfer learning with deep auto-encoders (TLDA) (Zhuang et al.", "startOffset": 33, "endOffset": 52}, {"referenceID": 38, "context": ", 2012); transfer learning with deep auto-encoders (TLDA) (Zhuang et al., 2015); and bi-transferring deep neural networks (BTDNN) (Zhou et al.", "startOffset": 58, "endOffset": 79}, {"referenceID": 37, "context": ", 2015); and bi-transferring deep neural networks (BTDNN) (Zhou et al., 2016).", "startOffset": 58, "endOffset": 77}], "year": 2017, "abstractText": "Domain adaptation is crucial in many real-world applications where the distribution of the training data differs from the distribution of the test data. Previous Deep Learning-based approaches to domain adaptation need to be trained jointly on source and target domain data and are therefore unappealing in scenarios where models need to be adapted to a large number of domains or where a domain is evolving, e.g. spam detection where attackers continuously change their tactics. To fill this gap, we propose Knowledge Adaptation, an extension of Knowledge Distillation (Bucilua et al., 2006; Hinton et al., 2015) to the domain adaptation scenario. We show how a student model achieves state-of-the-art results on unsupervised domain adaptation from multiple sources on a standard sentiment analysis benchmark by taking into account the domain-specific expertise of multiple teachers and the similarities between their domains. When learning from a single teacher, using domain similarity to gauge trustworthiness is inadequate. To this end, we propose a simple metric that correlates well with the teacher\u2019s accuracy in the target domain. We demonstrate that incorporating high-confidence examples selected by this metric enables the student model to achieve state-of-the-art performance in the single-source scenario.", "creator": "LaTeX with hyperref package"}}}