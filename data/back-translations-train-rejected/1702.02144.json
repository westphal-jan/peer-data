{"id": "1702.02144", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "Rapid parametric density estimation", "abstract": "Parametric density estimation, for example as Gaussian distribution, is the base of the field of statistics. Machine learning requires inexpensive estimation of much more complex densities, and the basic approach is relatively costly maximum likelihood estimation (MLE). There will be discussed inexpensive density estimators, for example literally fitting polynomial to the sample, which coefficients are calculated from just averaging monomials over the sample (estimators of moments). Another discussed basic application is fitting distortion to some standard distribution like Gaussian. The estimated parameters are approaching the optimal values with error dropping like $1/\\sqrt{n}$, where $n$ is the sample size.", "histories": [["v1", "Tue, 7 Feb 2017 16:55:37 GMT  (234kb,D)", "https://arxiv.org/abs/1702.02144v1", "6 pages, 2 figures"], ["v2", "Mon, 20 Feb 2017 14:29:27 GMT  (382kb,D)", "http://arxiv.org/abs/1702.02144v2", "8 pages, 4 figures"]], "COMMENTS": "6 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jarek duda"], "accepted": false, "id": "1702.02144"}, "pdf": {"name": "1702.02144.pdf", "metadata": {"source": "CRF", "title": "Rapid parametric density estimation", "authors": ["Jarek Duda"], "emails": ["dudajar@gmail.com"], "sections": [{"heading": null, "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "II. GENERAL CASE", "text": "Suppose we have a sample of n points in D-dimensional space: S = {xi} i = 1 > n, where xi = {xi1,.., xiD} n, while assuming some probability distribution, we will try to estimate its probability density function (PDFs) as a function of some selected parameters (PDFs), where each point also has a weight class of W (x), which is W = 1 for probability estimation, but can generally represent a mass of an object, or can even be negative for separation into two classes, or even a complex number or vector for simultaneous classification into several classes."}, {"heading": "III. DENSITY ESTIMATION WITH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A LINEAR COMBINATION", "text": "The most convenient application of the discussed method is the density estimation with a linear combination of some functional families, for example polynomials or sine and cosine. In this and the following section we assume that some functions fi: RD \u2192 R (not necessarily negative in the whole domain) can actually be an infinitely complete orthogonal base, for example for polynomials and Fourier bases in a finite closed interval."}, {"heading": "A. Basic formula", "text": "In the case of the linear combination, we obtain a surprisingly simple formula: ai = [fi] = fj, and the necessary condition (6) becomes simple: \u2211 iai < fi, fj > = 1n \u2211 x \u0445 S w (x) W (x) fj (x) (8) for j = 1,..., m. Denotation (< fi, fj >) as an example of the m \u00b7 m matrix of scalar products, the optimal coefficients become: aT = (< fi, fj >) \u2212 1 \u00b7 ([f1],., [fm] T (9), where [f]: = 1n x x x x x clustering x x \u00b2 S (x) W (x) f (10) as an estimate of the expected value of f. The summary of orthonormal functional set: < fi, fj > = ij and w = W = 1 standard weights, we obtain a surprisingly simple formula: ai = [fi] x \u00b2 x \u00b2 x."}, {"heading": "B. Asymptotic behavior (n\u2192\u221e)", "text": "Suppose W = w = 1 weights, orthonormal function family (< fi, fj > = \u03b4ij) and that the real density can actually be written as \u03c1 = \u2211 i aifi (not necessarily finite). To estimate ai, the formula (12) says to use ai \u2248 [fi]: average value of fi over the sample obtained. With the number of points going into infinity, it approaches the expected value of fi for this probability distribution: [fi] n \u2192 \u043c \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 < fi \u2012 dx = fi = aiAs necessary, [fi] approaches the exact value of ai from the assumed probability distribution. From the Central Limit Theorem, the error of the i-th coefficient results from approximately a normal width distribution, which is the standard deviation from fi (assuming it is finite) divided by \u221a n: [fi] \u2212 ai \u0445 N (0.1 n)."}, {"heading": "C. Nondegenerate kernel corrections", "text": "For a general kernel k (non-regative, integrating to 1), orthonormal base and D = 1 case, we would get an estimate analogously: ai = 1n% x x x (y \u2212 x) fi (y) dy \u2248 1 n \u00b2 x x \u00b2 S (fi (x) + 1 2 f \u00b2 i (x) \u0445 h2k (h) dh)), where we used second order Taylor expansion (fi (x + h) \u2248 fi (x) + h \u00b2 f \u00b2 i (x) and assumed symmetry of the kernel (k (\u2212 h) = k (h)), using first order correction (fi (x + h) \u2248 fi (x) + h \u00b2 i (x) + h \u00b2 i (x) + 12h \u00b2 f \u00b2 i (x) and assumed symmetry of the kernel (k (h (h) = k (h))), where the first order correction (fi) + h \u00b2 i (x) + vf \u2032 i \u00b2 i (x) and the kernel symmetry (h) is \u2212 native."}, {"heading": "D. Normalization of density", "text": "At least theoretically, the probable density functions should integrate to 1. For polynomials and Fourier series, it is forced by construction: All but the zero order fi will integrate to 0, so a normalization of density can be achieved by simply specifying the coefficient of zero order. There are also situations, especially in machine learning, where the need for density integration exactly to 1 is not decisive. Let's briefly consider situations where density normalization is required but cannot be forced directly by construction. A trivial solution is simply to perform an additional final normalization step: Divide the obtained density by a (x) dx.However, more accurate values should be achieved by applying the normalization condition of density as a constraint, while the (2) minimization, which can be done with the 6 Lagrange multiplier method. DenotingFi: = Density (x) (16) is achieved by the normalization condition."}, {"heading": "IV. POLYNOMIALS, FOURIER AND GAUSSIAN", "text": "Three natural examples of adaptation to a linear combination are briefly discussed, assuming the weight w = 1. As discussed, the choice of this family as orthogonal allows an independent assessment of the parameters. The first example considered are (legendary) polynomials, the second is the Fourier series, the last are (hermite) polynomials multiplied by e \u2212 x 2 / 2."}, {"heading": "A. Fitting polynomial in a finite region", "text": "A natural first application is the parameterizing density function as a polynomial. The zero order function should always be constant, but must also be integrated to a finite value. Therefore, we must limit \u2212 \u2212 to a finite region here (of volume v < \u221e), preferably a region such as [\u2212 1, 1] in Fig. 1, or a product of areas (hyperrectangle) in a higher dimension. We will only use points within this region to approximate the density within this region with a polynomial - however, this region should be selected as a relatively small region containing the behavior of interest. All integrals and sums here are within this finite region, for example by setting w = 0 outwards."}, {"heading": "B. Fourier series", "text": "The orthonormal basis for the [\u2212 1, 1] range is formed by the functions sin (j\u03c0x) and cos (j\u03c0x), which do not affect the normalization. Here, we need (2m) D coefficients for i to m in D dimensions. Make sure that sinusoidal concepts disappear at the boundaries of the interval (hypercube) - only with them can we reduce artifacts at the boundaries. If we have to work on a general spherical base, we can obtain a two-dimensional orthography here."}, {"heading": "C. Global fitting combination of vanishing functions", "text": "To overcome it, we can, for example, apply a combination of vanishing functions that fall to zero as they move away from the central point, for example in e-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x the way in which the convenience of applying the (12) formula is applied, preferring that this set of functions is orthogonal in nature. A well-known example of such an orthogononal function family are hermites polynomial multiplied by e-x2 / 2 / 2 the convenience of using i-th hermites polynomial, the following set of functions orthonorororororororally for & ltltltltltltlt; f > = g: \"R f (x) dx scalar product (weight w = 1): fi.\""}, {"heading": "V. SOME FURTHER POSSIBILITIES", "text": "The approach discussed is very general, here are some enhancements, optimizations for specific problems. A number of possibilities is the use of a linear combination of different function groups, for example Gram-Schmidt orthonormalization. While we focused on the disturbance of Gaussian distributions with hermite polynomials, in some situations it might be worthwhile to shift space into (hyper-) rectangles instead. While we focused on global adaptation in the region under consideration, there were also local distributions, for example as a linear combination of vanishing functions such as waves. Or, let's divide space into (hyper-) rectangles and flatten polynomias in each of them and flatten the boundaries with a weighted mean."}, {"heading": "VI. CONCLUSIONS", "text": "A very general and powerful but still-looking basic approach has been presented and discussed, which may already be known but seems to be overlooked in the standard literature. Its basic cases discussed may be the literal adaptation of a polynomial or Fourier series to the sample (in a finite region) or the distortion by a standard probability distribution such as Gauss. There is a wide range of possible applications, starting with modelling the local density for data of different origin, testing PRNG, finding the real signal in a noise analogy to ICA, reconstructing the density of this distortion, or for various cluster problems. This article introduces only this topic and discusses very basic possibilities, with only a few of many water points extending this general approach of adaptation to the Dirac delta boundary of smoothing the sample with a nucleus."}], "references": [{"title": "Remarks on some nonparametric estimates of a density function", "author": ["M. Rosenblatt"], "venue": "The Annals of Mathematical Statistics, vol. 27, no. 3, pp. 832\u2013837, 1956.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1956}, {"title": "On estimation of a probability density function and mode", "author": ["E. Parzen"], "venue": "The annals of mathematical statistics, pp. 1065\u20131076, 1962.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1962}, {"title": "On the probable errors of frequency-constants (contd.)", "author": ["F.Y. Edgeworth"], "venue": "Journal of the Royal Statistical Society, vol. 71, no. 4, pp. 651\u2013678, 1908.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1908}, {"title": "The large-sample distribution of the likelihood ratio for testing composite hypotheses", "author": ["S.S. Wilks"], "venue": "The Annals of Mathematical Statistics, vol. 9, no. 1, pp. 60\u201362, 1938.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1938}, {"title": "Society, The Problem of Moments, ser. Mathematical Surveys and Monographs", "author": ["J. Shohat", "J. Tamarkin"], "venue": "American Mathematical Society,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1943}, {"title": "Independent component analysis: algorithms and applications", "author": ["A. Hyv\u00e4rinen", "E. Oja"], "venue": "Neural networks, vol. 13, no. 4, pp. 411\u2013430, 2000.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Normalized rotation shape descriptors and lossy compression of molecular shape", "author": ["J. Duda"], "venue": "arXiv preprint arXiv:1509.09211, 2015. [Online]. Available: https://arxiv.org/pdf/1509.09211", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In machine learning there are popular kernel density estimators (KDE) ([1], [2]), which smoothen the sample by convolving it with a kernel: a nonnegative function integrating to 1, for instance a Gaussian distribution.", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "In machine learning there are popular kernel density estimators (KDE) ([1], [2]), which smoothen the sample by convolving it with a kernel: a nonnegative function integrating to 1, for instance a Gaussian distribution.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "The standard approach to estimate such parameters is the maximum likelihood estimation (MLE) ([3], [4]), which finds Figure 1.", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "The standard approach to estimate such parameters is the maximum likelihood estimation (MLE) ([3], [4]), which finds Figure 1.", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "Taking D length sequences of its values as points from [0, 1] hypercube, ideally they should come from \u03c1 = 1 uniform density.", "startOffset": 55, "endOffset": 61}, {"referenceID": 4, "context": "The standard way is to calculate higher (central) moments, however, it is a difficult problem to translate them back into the actual probability density - so called \u201dthe problem of moments\u201d [5].", "startOffset": 190, "endOffset": 193}, {"referenceID": 5, "context": "The most widely used methods for understanding distortion from Gaussian distribution, as noise in which we would often like to find the real signal, is probably the large family of independent component analysis (ICA) [6] methods.", "startOffset": 218, "endOffset": 221}, {"referenceID": 6, "context": "For example, the original motivation for this approach was fitting a low order polynomial to a distribution of atomic masses or electron negativity along the longest axis of a chemical molecule [7].", "startOffset": 194, "endOffset": 197}, {"referenceID": 6, "context": "Or the sampled points may contain some weights, basing on which we might want to parameterize the density profile, for example of a chemical molecule [7].", "startOffset": 150, "endOffset": 153}], "year": 2017, "abstractText": "Parametric density estimation, for example as Gaussian distribution, is the base of the field of statistics. Machine learning requires inexpensive estimation of much more complex densities, and the basic approach is relatively costly maximum likelihood estimation (MLE). There will be discussed inexpensive density estimation, for example literally fitting a polynomial (or Fourier series) to the sample, which coefficients are calculated by just averaging monomials (or sine/cosine) over the sample. Another discussed basic application is fitting distortion to some standard distribution like Gaussian analogously to ICA, but additionally allowing to reconstruct the disturbed density. Finally, by using weighted average, it can be also applied for estimation of non-probabilistic densities, like modelling mass distribution, or for various clustering problems by using negative (or complex) weights: fitting a function which sign (or argument) determines clusters. The estimated parameters are approaching the optimal values with error dropping like 1/ \u221a n, where n is the sample size.", "creator": "LaTeX with hyperref package"}}}