{"id": "1702.02170", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks", "abstract": "Maybe the single most important goal of representation learning is making subsequent learning faster. Surprisingly, this fact is not well reflected in the way embeddings are evaluated. In addition, recent practice in word embeddings points towards importance of learning specialized representations. We argue that focus of word representation evaluation should reflect those trends and shift towards evaluating what useful information is easily accessible. Specifically, we propose that evaluation should focus on data efficiency and simple supervised tasks, where the amount of available data is varied and scores of a supervised model are reported for each subset (as commonly done in transfer learning).", "histories": [["v1", "Tue, 7 Feb 2017 19:21:50 GMT  (2049kb,D)", "http://arxiv.org/abs/1702.02170v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["stanis{\\l}aw jastrzebski", "damian le\\'sniak", "wojciech marian czarnecki"], "accepted": false, "id": "1702.02170"}, "pdf": {"name": "1702.02170.pdf", "metadata": {"source": "CRF", "title": "How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks", "authors": ["Stanis\u0142aw Jastrzebski", "Damian Le\u015bniak", "Wojciech Marian Czarnecki"], "emails": ["stanislaw.jastrzebski@uj.edu.pl"], "sections": [{"heading": null, "text": "Astonishingly, this fact is not well reflected in the way embedding is evaluated. Furthermore, recent practice of word embedding highlights the importance of learning specialized representations. We argue that the emphasis of word embedding evaluation should reflect these trends and shift toward evaluating useful information that is easily accessible. Specifically, we suggest that evaluation should focus on data efficiency and simple monitored tasks, where the amount of available data is varied and evaluations of a monitored model are reported for each subset (as is usually the case with transfer learning).To illustrate the importance of such analysis, a comprehensive evaluation of selected word embedding is presented, which provides a more complete picture and provides new insights into performance characteristics, such as information about word similarity or analogy, which tend not to be encoded linearly in the embedding space, which the cosine-based, unguarded evaluation methods are available to all evaluators in question and analysis."}, {"heading": "1 Introduction", "text": "Word embedding remains a standard practice in modern NLP systems, in both flat and deep architectures, which are directly used as conditions of need (Goldberg, 2015).By encoding information about words in a relatively simple algebraic structure [Arora et al., 2016] they can allow a quick transfer to the task of interest.The importance of word representation learning has led to the development of several algorithms, but the lack of principles impedes the evaluation of the field, which motivates the development of principled ways to evaluate word representation.Word embedding is not only difficult to evaluate, but also a challenge for education.Recent Practice shows that one often has to adjust algorithms, corpus and hyperparameters towards the target task. [Lai et al., 2016, Sharp et al.], which holds the promise of broad applicability of unmonitored pretraining.Word evaluation methods can be divided into two evaluation methods."}, {"heading": "2 Proposal", "text": "In fact, it is as if most people are able to understand themselves and to understand how they have behaved. (...) It is not as if they are able to understand themselves. (...) It is not as if they are able to understand themselves. (...) It is as if they were able to change the world. (...) It is as if they were able to change the world. (...) It is as if they were able to change the world. (...) It is as if they were able to stay in the world of the world of the world. (...) It is as if they were able to change the world. (...) It is as if they are in the world. \"(...) It is as if they are in the world. (...) It is as if they are in the world. (...) It is as if they are in the world. (...) It is as if they are in the world. (...) It is as if they are in the world."}, {"heading": "3 Experiments", "text": "In this section, we define specific indicators and tasks and, by way of example, perform the evaluation of several pre-trained embeddings in the proposed setting. Specifically, we try to answer several questions empirically, all of which are aimed at an experimental validation of the three main points of the proposal: \u2022 Do supervised versions of WA and WS benchmarks provide additional insights? \u2022 How stable is embedding under changed data set size? \u2022 Are there embeddings that benefit from non-linear models? \u2022 The first question helps to understand how useful simple supervised tasks are in connection with data efficiency; the second question shows that the embedding of embeddings under transfer learning evaluation changes. The last question examines whether embedding encodes information in a \"non-linear\" way; while one of the main objectives of imaging learning is the untangling of variation factors, commonly learned settings are intertwined and condensed, which raises interesting questions such as how to extract patterns."}, {"heading": "3.1 Datasets and models", "text": "In fact, most of them are able to survive themselves if they do not see themselves able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think they will be able to change the world. \""}, {"heading": "3.2 Embeddings", "text": "Our goal was to cover embedding representatives coming from both flat and deeper architectures. Deep embedding is more difficult to train, so we decided to include pre-formed and publicly available vectors.7 The setup includes the following \"flat\" pre-formed embedding: GloVe (100 and 300 dimensions) [Pennington et al., 2014], Hellinger PCA (HPCA) [Lebret and Collobert, 2014], PDC (100 and 300 dimensions) and HDC (300 dimensions) [Sun et al., 2015], In addition, the following \"deep\" embedding is evaluated: Neural Translation Machine (NMT, 5ccos (~ v1, ~ v2) = 1 + cos (~ v1, ~ v2) 2 6We also tried RankSVM [Lee and Lin, 2014], but it did not work better than other models while it was very computer intensive."}, {"heading": "3.3 Results", "text": "For each data set that we randomly select and evaluate to increase the size of training data sets, approximate generalization errors are detected after seeing increasing amounts of data. Splits are repeated a total of 6 times to reduce noise. Thus, for each task, 6 learning curves are assigned, with a score for each subset of data (see Fig. 2).Rankings of embedding in each point are calculated using a greedy sequential procedure in which we assign embedding the same rank if their results (each point on the curve is represented by 6 points) are not significantly different from those tested with pastoral ANOVA tests. All results are available with online8.3.4 learning processes."}, {"heading": "3.5 Rank stability under changing dataset size", "text": "The second question was how stable the orders are under the growing data set. To this end, we measured the rank at the beginning (30% of the data) and at the end of the training. The mean absolute value of the change in the ranking is approximately (averaged across all categories) 0.6 with a standard deviation of 0.2. This means that an embedding usually has a changed rank after the training, which explains the usefulness of measuring the data efficiency for the embedding tested. Interestingly, none of the embedding is consistently more data efficient than others. On top of that, the standard deviation of both embedding at the end and beginning is about 2.5, which further reinforces the results from [Schnabel and Labutov, 2015] that embedding has different characteristics of data efficiency for different tasks, i.e. none of the embedding is consistently more efficient than others. Moreover, the standard deviation of both embedding at the end and beginning is about 2.5, which depends on the results from Schnabel [Einmaubel, 2015 and Labutov]."}, {"heading": "3.6 Linear vs non\u2013linear models", "text": "Our last question was how stable the arrangement is under a modified model type. Specifically, are there embedding systems that are specifically suitable for use with linear models? Some embedding systems are indeed, see Fig. 1. This is an important empirical fact for users who are motivated to include such embedding systems in experiments. In particular, it clearly shows that typically used embedding systems do not answer the question \"Is there any information about task X in embedding Y,\" but only \"information about task X stored in embedding Y can be easily separated by a static (or linear) classifier.\" An illustrative example is the difference in performance between two pre-trained GloVe embedding systems of different dimensions (100 and 300). It has already been shown that low-dimensional GloVe embedding systems in syntactical tasks are better [Lai et al.], but our evaluation shows a more complicated codicular image that could promote a codifier."}, {"heading": "3.7 Discussion", "text": "Researchers can ask more precise questions, such as \"is it worth adjusting syntax-specific embedding, even if the size of the monitored data sets is large?\" (to which answer our experiments give a positive answer) or \"is the HASINSTANCE relationship in space encoded?\" (to which answer some embedding also responds positively). Unfortunately, there is already a high volatility of final embedding that is ordered when using standard evaluations, and our proposed scheme sometimes makes it even more difficult to decide which embedding is optimal. This suggests that purely uncontrolled pre-training on a large scale may not be suitable for NLP applications. Most importantly, the evaluation should be more targeted, either to a specific scope or to specific characteristics of representation. One of the arguments presented for including verified models for testing information content is the algebraic interpretation of word embedding [Arora, 2016]."}, {"heading": "4 Conclusions", "text": "Using experiments as an example, the proposed evaluation shows differences between embedding along commonly overlooked dimensions: data efficiency, non-linearity of the downstream model, and simple monitored tasks (including the restoration of higher order relationships between words).Interesting new conclusions can be reached, including differences between GloVe embedding of different sizes or performance of nonlinear models on comparative scales. Furthermore, the results confirm the conclusions from other published studies that there are no generally good embedding and that these may not be attainable or pose a well-posed problem. In designing the evaluation, great care should be taken and emphasis should be laid on. If, for example, the main objective of word embedding is to be useful during transfer, suggested data efficiency metrics should be included."}, {"heading": "A Theoretical analysis", "text": "First, let's try to answer the question of what the information about the task is and how it can be measured in light of some data representations. For simplicity's sake, let's assume that the task is a binary classification, but the same reasoning applies to multiclass, multilabel, regression, etc. However, a very natural machine learning perspective is to define information that is stored as the Bayes risk of an optimal model trained to perform that task. Obviously, the raw representation already has some non-negative Bayes risks that cannot be reduced during each embedding. In fact, it is fairly easy to show that almost every embedding preserves all the information contained in the source representation. Observation 1. Any injective embedding preserves all the information about the task. Proof. Let's assume that Bayes gives an optimal classifier (which we call the Bayes risk RX) in the input space we call the x (beyond what we call our input x)."}, {"heading": "B Regression neural network for word analogy task", "text": "We assume that all embedded words have a Euclidean norm that corresponds to one - this guarantees that the scalar product of two embedded words is also their cosmic similarity. The word analogy task is defined as follows: Given (embedded) words v1, v2, and v3, predict the word v4 that fulfills the analogy \"v1 is related to v2 because v3 is related to v4.\" Our v4 estimator is defined as: \u2022 W1, W2, and W3 - D matrices initialized with identities, \u2022 b - D-dimensional vectors initialized with zeros, and the cost is defined as: & ltj < vj4, v > ADD matrices initialized with a miniature."}], "references": [{"title": "Expanding subjective lexicons for social media mining with embedding subspaces", "author": ["Silvio Amir", "Ram\u00f3n Fern\u00e1ndez Astudillo", "Wang Ling", "Paula C. Carvalho", "M\u00e1rio J. Silva"], "venue": "CoRR, abs/1701.00145,", "citeRegEx": "Amir et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Amir et al\\.", "year": 2017}, {"title": "Linear algebraic structure of word senses, with applications to polysemy", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": "arXiv preprint arXiv:1601.03764,", "citeRegEx": "Arora et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "Improving reliability of word similarity evaluation by redesigning annotation task and performance", "author": ["Oded Avraham", "Yoav Goldberg"], "venue": "measure. CoRR,", "citeRegEx": "Avraham and Goldberg.,? \\Q2016\\E", "shortCiteRegEx": "Avraham and Goldberg.", "year": 2016}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam Khanh Tran", "Marco Baroni"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Intrinsic evaluation of word vectors fails to predict extrinsic performance", "author": ["Billy Chiu", "Anna Korhonen", "Sampo Pyysalo"], "venue": "ACL 2016,", "citeRegEx": "Chiu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chiu et al\\.", "year": 2016}, {"title": "Problems With Evaluation of Word Embeddings Using Word Similarity Tasks", "author": ["M. Faruqui", "Y. Tsvetkov", "P. Rastogi", "C. Dyer"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Nondistributional word vector representations", "author": ["Manaal Faruqui", "Chris Dyer"], "venue": "CoRR, abs/1506.05230,", "citeRegEx": "Faruqui and Dyer.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2015}, {"title": "Non-distributional word vector representations", "author": ["Manaal Faruqui", "Chris Dyer"], "venue": "In Proceedings of ACL,", "citeRegEx": "Faruqui and Dyer.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2015}, {"title": "Wordrep: A benchmark for research on learning word representations", "author": ["Bin Gao", "Jiang Bian", "Tie-Yan Liu"], "venue": "CoRR, abs/1407.1640,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "Proceedings of the Twenty-eight International Conference on Machine Learning,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "A Primer on Neural Network Models for Natural Language", "author": ["Yoav Goldberg"], "venue": "URL http://arxiv.org/abs/1510", "citeRegEx": "Goldberg.,? \\Q2015\\E", "shortCiteRegEx": "Goldberg.", "year": 2015}, {"title": "Embedding word similarity with neural machine", "author": ["Felix Hill", "Kyunghyun Cho", "S\u00e9bastien Jean", "Coline Devin", "Yoshua Bengio"], "venue": "translation. CoRR,", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Simlex999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "Computational Linguistics,", "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "What\u2019s in an embedding? analyzing word embeddings through multilingual evaluation", "author": ["Arne K\u00f6hn"], "venue": null, "citeRegEx": "K\u00f6hn.,? \\Q2015\\E", "shortCiteRegEx": "K\u00f6hn.", "year": 2015}, {"title": "What\u2019s in an embedding? analyzing word embeddings through multilingual evaluation", "author": ["Arne K\u00f6hn"], "venue": null, "citeRegEx": "K\u00f6hn.,? \\Q2015\\E", "shortCiteRegEx": "K\u00f6hn.", "year": 2015}, {"title": "How to generate a good word embedding", "author": ["Siwei Lai", "Kang Liu", "Shizhu He", "Jun Zhao"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "Lai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2016}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Kazuya Kawakami", "Sandeep Subramanian", "Chris Dyer"], "venue": "In In proceedings of NAACL-HLT (NAACL 2016).,", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Word embeddings through hellinger pca", "author": ["R\u00e9mi Lebret", "Ronan Collobert"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Lebret and Collobert.,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert.", "year": 2014}, {"title": "the sum of its parts\": Joint learning of word and phrase representations with autoencoders", "author": ["R\u00e9mi Lebret", "Ronan Collobert"], "venue": "CoRR, abs/1506.05703,", "citeRegEx": "Lebret and Collobert.,? \\Q2015\\E", "shortCiteRegEx": "Lebret and Collobert.", "year": 2015}, {"title": "Large-scale linear ranksvm", "author": ["Ching-Pei Lee", "Chih-Jen Lin"], "venue": "Neural computation,", "citeRegEx": "Lee and Lin.,? \\Q2014\\E", "shortCiteRegEx": "Lee and Lin.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Wang Ling", "Chris Dyer", "Alan Black", "Isabel Trancoso"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Evaluating word embeddings using a representative suite of practical tasks", "author": ["Neha Nayak", "Gabor Angeli", "Christopher D. Manning"], "venue": "In RepEval Workshop,", "citeRegEx": "Nayak et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nayak et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Association for Computational Linguistics", "author": ["October"], "venue": "URL http://www.aclweb.org/ anthology/D14-1162.", "citeRegEx": "October,? 2014", "shortCiteRegEx": "October", "year": 2014}, {"title": "Word embedding calculus in meaningful ultradense subspaces. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016", "author": ["Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Rothe and Sch\u00fctze.,? \\Q2016\\E", "shortCiteRegEx": "Rothe and Sch\u00fctze.", "year": 2016}, {"title": "Sparsifying word representations for deep unordered sentence modeling", "author": ["Prasanna Sattigeri", "Jayaraman J Thiagarajan"], "venue": "ACL 2016,", "citeRegEx": "Sattigeri and Thiagarajan.,? \\Q2016\\E", "shortCiteRegEx": "Sattigeri and Thiagarajan.", "year": 2016}, {"title": "Labutov. Evaluation methods for unsupervised word embeddings. In EMNLP, pages 298\u2013307", "author": ["Tobias Schnabel", "Igor"], "venue": "The Association for Computational Linguistics,", "citeRegEx": "Schnabel and Igor,? \\Q2015\\E", "shortCiteRegEx": "Schnabel and Igor", "year": 2015}, {"title": "Creating causal embeddings for question answering with minimal supervision", "author": ["Rebecca Sharp", "Mihai Surdeanu", "Peter Jansen", "Peter Clark", "Michael Hammond"], "venue": "CoRR, abs/1609.08097,", "citeRegEx": "Sharp et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sharp et al\\.", "year": 2016}, {"title": "Creating causal embeddings for question answering with minimal supervision", "author": ["Rebecca Sharp", "Mihai Surdeanu", "Peter Jansen", "Peter Clark", "Michael Hammond"], "venue": "CoRR, abs/1609.08097,", "citeRegEx": "Sharp et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sharp et al\\.", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Yulia Tsvetkov", "Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Tsvetkov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2015}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Yulia Tsvetkov", "Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Tsvetkov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Ye Zhang", "Byron Wallace"], "venue": "arXiv preprint arXiv:1510.03820,", "citeRegEx": "Zhang and Wallace.,? \\Q2015\\E", "shortCiteRegEx": "Zhang and Wallace.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Using word embeddings remains a standard practice in modern NLP systems, both in shallow and deep architectures [Goldberg, 2015].", "startOffset": 112, "endOffset": 128}, {"referenceID": 1, "context": "By encoding information about words in a relatively simple algebraic structure [Arora et al., 2016] they enable fast transfer to the task of interest1.", "startOffset": 79, "endOffset": 99}, {"referenceID": 22, "context": "For instance, it has been shown [Baroni and Dinu, 2014] that neural-based word embeddings perform consistently better then count-based models and later, using WS and WA tasks, it was argued otherwise [Levy et al., 2015].", "startOffset": 200, "endOffset": 219}, {"referenceID": 6, "context": "critiques both benchmarks have received in the literature [Faruqui et al., 2016], in authors\u2019 opinion mostly due to their purely unsupervised nature.", "startOffset": 58, "endOffset": 80}, {"referenceID": 13, "context": "\u2022 Similarity: SimLex999 [Hill et al., 2015],", "startOffset": 24, "endOffset": 43}, {"referenceID": 4, "context": "MEN [Bruni et al., 2014], WordSimilarity353 [Finkelstein et al.", "startOffset": 4, "endOffset": 24}, {"referenceID": 24, "context": ", 2001] and Rare Words [Luong et al., 2013].", "startOffset": 23, "endOffset": 43}, {"referenceID": 9, "context": "\u2022 Analogy: 4 categories from WordRep [Gao et al., 2014]4.", "startOffset": 37, "endOffset": 55}, {"referenceID": 33, "context": "\u2022 Sentence: Stanford Sentiment Treebank [Socher et al., 2013] and News20 (3 binary datasets) [Tsvetkov et al.", "startOffset": 40, "endOffset": 61}, {"referenceID": 22, "context": "\u2022 Analogy: 3COSADD, 3COSMUL [Levy et al., 2015] and regression neural network (performing regression on the 4th word given", "startOffset": 28, "endOffset": 47}, {"referenceID": 26, "context": "Setup includes following \u201cshallow\u201d pretrained embeddings: GloVe (100 and 300 dimensions) [Pennington et al., 2014], Hellinger PCA (HPCA) [Lebret and Collobert, 2014], PDC (100 and 300 dimensions) and HDC (300 dimensions) [Sun et al.", "startOffset": 89, "endOffset": 114}, {"referenceID": 19, "context": ", 2014], Hellinger PCA (HPCA) [Lebret and Collobert, 2014], PDC (100 and 300 dimensions) and HDC (300 dimensions) [Sun et al.", "startOffset": 30, "endOffset": 58}, {"referenceID": 21, "context": "2 We also tried RankSVM [Lee and Lin, 2014], but it did not perform better than other models, while being very computationally intensive.", "startOffset": 24, "endOffset": 43}, {"referenceID": 12, "context": "activations of the deep model are extracted as word embeddings) [Hill et al., 2014], morphological embeddings (morph, which can learn morphological differences between words directly) [Luong et al.", "startOffset": 64, "endOffset": 83}, {"referenceID": 24, "context": ", 2014], morphological embeddings (morph, which can learn morphological differences between words directly) [Luong et al., 2013] and HPCA variant trained using", "startOffset": 108, "endOffset": 128}, {"referenceID": 20, "context": "autoencoder architecture [Lebret and Collobert, 2015].", "startOffset": 25, "endOffset": 53}, {"referenceID": 23, "context": "pora [Ling et al., 2015] (used commonly in syntax demanding tasks, like tagging).", "startOffset": 5, "endOffset": 24}, {"referenceID": 9, "context": "Low constant model scores are similar to numbers reported in [Gao et al., 2014].", "startOffset": 61, "endOffset": 79}, {"referenceID": 9, "context": "thors [Gao et al., 2014] reported low accuracies (often even below 5%) on most analogy questions and we were able to improve absolute score upon the tested subset on average by absolute 11% (see Tab.", "startOffset": 6, "endOffset": 24}, {"referenceID": 12, "context": "It was claimed in [Hill et al., 2014] that NMT embeddings are better at encoding true similarity between words, but SVR on Glove embeddings performs better after training on the whole dataset (i.", "startOffset": 18, "endOffset": 37}, {"referenceID": 1, "context": "One of the presented arguments for including supervised models for testing information content is algebraic interpretation of word embeddings [Arora et al., 2016].", "startOffset": 142, "endOffset": 162}, {"referenceID": 9, "context": "from WordRep benchmark [Gao et al., 2014]).", "startOffset": 23, "endOffset": 41}], "year": 2017, "abstractText": "Maybe the single most important goal of representation learning is making subsequent learning faster. Surprisingly, this fact is not well reflected in the way embeddings are evaluated. In addition, recent practice in word embeddings points towards importance of learning specialized representations. We argue that focus of word representation evaluation should reflect those trends and shift towards evaluating what useful information is easily accessible. Specifically, we propose that evaluation should focus on data efficiency and simple supervised tasks, where the amount of available data is varied and scores of a supervised model are reported for each subset (as commonly done in transfer learning). In order to illustrate significance of such analysis, a comprehensive evaluation of selected word embeddings is presented. Proposed approach yields a more complete picture and brings new insight into performance characteristics, for instance information about word similarity or analogy tends to be non\u2013linearly encoded in the embedding space, which questions the cosine\u2013based, unsupervised, evaluation methods. All results and analysis scripts are available online.", "creator": "LaTeX with hyperref package"}}}