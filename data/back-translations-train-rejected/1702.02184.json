{"id": "1702.02184", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "Transfer from Multiple Linear Predictive State Representations (PSR)", "abstract": "In this paper, we tackle the problem of transferring policy from multiple partially observable source environments to a partially observable target environment modeled as predictive state representation. This is an entirely new approach with no previous work, other than the case of transfer in fully observable domains. We develop algorithms to successfully achieve policy transfer when we have the model of both the source and target tasks and discuss in detail their performance and shortcomings. These algorithms could be a starting point for the field of transfer learning in partial observability.", "histories": [["v1", "Tue, 7 Feb 2017 20:14:30 GMT  (54kb,D)", "http://arxiv.org/abs/1702.02184v1", "8 pages, 3 algorithms, 3 figures"]], "COMMENTS": "8 pages, 3 algorithms, 3 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sri ramana sekharan", "ramkumar natarajan", "siddharthan rajasekaran"], "accepted": false, "id": "1702.02184"}, "pdf": {"name": "1702.02184.pdf", "metadata": {"source": "CRF", "title": "Transfer from Multiple Linear Predictive State Representations (PSR)", "authors": ["Sri Ramana Sekharan", "Ramkumar Natarajan", "Siddharthan Rajasekaran"], "emails": ["ssekharan@wpi.edu", "rnatarajan@wpi.edu", "sperundurairajas@wpi.edu"], "sections": [{"heading": null, "text": "In this paper, we address the problem of transferring policy from multiple partially observable source environments to a partially observable target environment modelled as forward-looking state representation. This is a completely new approach with no prior work, except for transfer in fully observable areas. We develop algorithms to successfully achieve policy transfer when we have the model of both source and target tasks, and discuss in detail their performance and shortcomings. These algorithms could be a starting point for the field of partial observability transfer learning."}, {"heading": "1 INTRODUCTION", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "2 RELATED WORK", "text": "There are a variety of different approaches available in the transfer learning literature for different environmental conditions, but all address only fully observable areas. [8] One of the approaches that is most relevant to our goal is transfer learning using bisimulation metrics. [3] The bisimulation metrics available in the literature specify the extent of similarity between two states in a Markov decision-making process. This metric limits the difference in value functions between the two states, but so far no such metrics have been developed for partially observable areas. The closest approach available in the literature is the development of bisimulation equivalence, which states whether two states of belief are similar in a partially observable Markov decision process [3]. It does not provide information on the extent of similar learning processes."}, {"heading": "3 PROBLEM STATEMENT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 PREDICTIVE STATE REPRESENTATION", "text": "A controlled dynamic, partially observable discrete system with a finite series of actions A can emit a finite number of observations. (An agent in the system performs an action at any point in the system and perceives an observation at any point. A story h at any point in the system is defined as an action observation sequence that has taken place up to that point. (h = a1o1a2o2.... atot.) A test is defined as a future observation sequence that will occur in the future t = at + 1ot + 1at + 2ot + 2... at + kot + k. The prediction for the test t given by P (t | h) is defined as the probability that the test will occur given the current history of the agent. (A, A, Q, Mao, Mao, Mao, mao, P (Q | quot) > where, O = {oi} is a nuclear observation."}, {"heading": "3.2 INITIAL PROBLEM FORMULATION", "text": "We have a set of source tasks S = {Si} and a target task T. These tasks are modeled as predictive state representation, defined by the tuples < O, A, QSi, MSiao, mSiao, RSi > and < O, A, QT, MTao, mTao, RT >. We assume that both the source and target tasks have the same discrete observation and action tasks, and we have access to the complete model of both tasks. More formally, for each state in target bT, we must find a suitable state in source task S by using the source function as successor task S (bS)."}, {"heading": "4 OUR APPROACH", "text": "First we learn the model of the source and target task. Then we find the optimal Q function for each of the source tasks through approximate Q-Learning. Then we develop and test two of our approaches for transferring to the target task."}, {"heading": "4.1 LEARNING THE MODEL", "text": "We use the Analytical Discovery Learning (ADL) [4] algorithm to build the model of the PSR. Since we have access to the POMDP model of the environment, we can calculate any entry in the D matrix. First, we list all entries that correspond to the single-level tests and histories. We find the single-level core tests and histories by taking the linear independent columns and rows. Then, we compute the entries associated with the single-level extensions to core tests and histories. This procedure repeats itself to the rank of the core matrix on two consecutive iterations. Then, the linear independent rows and columns are taken as the final core test / history. The points associated with each test can be repeated using the formula mt = P (Q | H) \u2212 1P (t | H) \u2212 1P (t | H), with P (Q | H) being the core matrix {> history, the corank of the algorithm L-Q (Q) during the update (Q) algorithm (Q)."}, {"heading": "4.2 ONLINE LEARNING USING APPROXIMATE Q-LEARNING", "text": "Since the PSR state, which contains the predictive vectors, is a continuous and high-dimensional planning, Q-Learning must be performed in a continuous space. James et al. [5] used Cerebellar Model Articulation Controllers (CMAC) [1] as an approximate value to implement Q-Learning. This is the planning phase in which the Q values of the state and the action space are approached online. CMAC is a class of sparsely coded memory that has r overlapped and offset tilts, each of which has the number of edges corresponding to the length of the query variables. Each edge of the tiling encompasses the entire space of the corresponding state variables and is quantified into different levels based on its sparseness and length of the prediction vector. Each combination of state vector components and the action value activates exactly one part of each tile, which is known as the total network value, each of which is randomly matched to one of the tiles."}, {"heading": "4.3 TRANSFER LEARNING IN PREDICTIVE STATE REPRESENTATION", "text": "Definition: We define the projection of a test t on a task K after history h as \u03c7K (t, h), the probability of occurrence of t on task K after history h\u03c7K (t, h) = PK (t | h) = mtPK (QK | h). If we have a series of tests T = {ti}, with slight notation misuse, we represent the projection vector as\u03c7K (T, h) = {\u03c7K (ti, h)} We use interchangeably the terms \"projection of a test on a task\" and \"projection of a task on a test.\" Both mean the same thing."}, {"heading": "4.3.1 Core Test Projection Algorithm", "text": "We define bS | Tp as the projection of the source core test on the target. It is the probability of occurrence of the source core test on the target at any given time. Mathematically, bS | Tp (h) = \u03c7T (QS, h) = \u03c7T (qSi, h) We can do this using the following expressionbS | Tp = PT (QS | h) = MTQSb Twhere MTQs is the matrix that relates the probability of occurrence of source core tests QS in the target task T, the columns of which are equal to Eq.2. We use this projection matrix MTQs for each iteration to select an action. We project that the magnitude of the similarity is proportional to the probability of occurrence of source core tests QS."}, {"heading": "4.3.2 Validating Test Projection Algorithm", "text": "Instead of projecting the target on the core test of the source, we now project both the source and the target on a common set of validation tests. We then find the distance between their projections and draw a conclusion on their similarity. Let's remember that the probability of a test occurring on the basis of current belief is P (t | h) = P (Q | h) mt. Let's leave the validation test Vt = {tv1, tv2, tvm}. Therefore, the projection of a task T on these tests is given by PT (Vt | h) = MTVtPT (Q | h). The value of the mkt tests and each task k is calculated using Eq.2. Let's leave the projection of task S (Vt, h) on the validation of task S on the validation tests Vt. We define the distance between source (S) and target (T) as a product of validation tests."}, {"heading": "5 EXPERIMENTS AND RESULTS", "text": "This year it has come to the point that it will be able to put itself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We are able to put ourselves at the top, \"he said."}, {"heading": "6 FUTURE WORK AND DISCUSSIONS", "text": "With the ADL algorithm, we could only model simple Pocman environments with small observatories, and the same transfer algorithm should be tested using models for complex environments. TPSR [7] provides a good framework for modeling complex environments, and we assume that the cost of learning the model is zero or that the model is given to us. Our next step would be to improve our algorithm for online learning, where the model of the target task is not previously known. Interleaved Learning and Planning [6] provides a method to learn and plan simultaneously using PSRs. An important research facility is the inclusion of the transfer algorithm in this framework. We used hand-coded validation tests for SET-1 and SET-2. We are currently exploring ways to automatically find a series of tests to validate the source tasks. A simple example to demonstrate the limitations of our approach would be an optimal step for the North-East approach, where the source site is. \""}, {"heading": "7 CONCLUSION", "text": "This is the first work for transfer in partially observable environments. We are developing a basic framework for the problem. We are demonstrating in simple experiments the successful transfer of policy to solve a partially observable labyrinth. Our method was able to find similarities (quantitatively) between tasks and was able to transfer policy from the most similar source task. Our method is suitable if there is an achievable configuration in one of the source tasks that is similar to the target task and has the same optimal action (as in the target task). If all sources have no such configuration or no common optimal action, the algorithm requires an action map between the sources and the target."}, {"heading": "Acknowledgements", "text": "We thank Prof. Dmitry Berenson for his constant motivation and constructive feedback throughout the project. We also thank Prof. Balaraman Ravindran, Mr. Prasanna Parthasarathi and Mr. Janarthanan Rajendran from IIT Madras."}], "references": [{"title": "A theory of cerebellar function", "author": ["J.S. Albus"], "venue": "Mathematical Biosciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1971}, {"title": "Closing the learning-planning loop with predictive state representations", "author": ["B. Boots", "S.M. Siddiqi", "G.J. Gordon"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "On planning, prediction and knowledge transfer in fully and partially observable markov decision processes", "author": ["P.S. Castro"], "venue": "McGill University, PhD thesis,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Learning and discovery of predictive state representations in dynamical systems with reset", "author": ["M.R. James", "S. Singh"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Planning with predictive state representations", "author": ["M.R. James", "S. Singh"], "venue": "IEEE Proceedings", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Goaldirected online learning of predictive models", "author": ["S. Ong", "Y. Grinberg", "J. Pineau"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Learning low dimensional predictive representations", "author": ["M. Rosencrantz", "G. Gordon", "S. Thrun"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "An introduction to inter-task transfer for reinforcement learning", "author": ["M.E. Taylor", "P. Stone"], "venue": "AI Magazine,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Learning from delayed rewards", "author": ["C.J.C.H. Watkins"], "venue": "PhD thesis,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1989}, {"title": "Modeling dynamical systems with structured predictive state representations", "author": ["B.D. Wolfe"], "venue": "PhD thesis, University of Michigan,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Predictive representations of state", "author": ["M.L. Littman", "R.S. Sutton", "S. Singh"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}], "referenceMentions": [{"referenceID": 7, "context": "This has a huge advantage over POMDPs in that, not only we do not need to mention the number of states beforehand, but also some systems which cannot be modeled as a POMDP can be modeled as a PSR [8] [10].", "startOffset": 196, "endOffset": 199}, {"referenceID": 9, "context": "This has a huge advantage over POMDPs in that, not only we do not need to mention the number of states beforehand, but also some systems which cannot be modeled as a POMDP can be modeled as a PSR [8] [10].", "startOffset": 200, "endOffset": 204}, {"referenceID": 7, "context": "[8] presents a good survey of the available transfer learning literature.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "One of the approaches that is most relevant to our goal is transfer learning using bisimulation metrics [3].", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "The closest approach available in literature is the development of bisimulation equivalence, which states whether two belief states in a Partially Observable Markov Decision Process are similar [3].", "startOffset": 194, "endOffset": 197}, {"referenceID": 3, "context": "[4] formulated the first discovery algorithm for linear PSRs with reset.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "This method was exploited by [6] [2] to integrate learning and planning in predictive representations.", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "This method was exploited by [6] [2] to integrate learning and planning in predictive representations.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "[4] have also made progress in the problem of planning using a learned model in which they have suggested value iteration and approximate Q-Learning methods.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "In our approach we use approximate Q-Learning using Cerebellar Model Articulation Controller (CMAC) [1] [5].", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "In our approach we use approximate Q-Learning using Cerebellar Model Articulation Controller (CMAC) [1] [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 9, "context": "at+kot+k, can be calculated from the PSR model using the expression[10],", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "To do planning in PSR, we incorporate a discrete set of rewards R = {ri} along with the observation [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "We use Analytical Discovery Learning (ADL) algorithm [4] to build the model of the PSR.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "[5] used Cerebellar Model Articulation Controller (CMAC) [1] as a function approximator to implement Q-learning [9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[5] used Cerebellar Model Articulation Controller (CMAC) [1] as a function approximator to implement Q-learning [9].", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "[5] used Cerebellar Model Articulation Controller (CMAC) [1] as a function approximator to implement Q-learning [9].", "startOffset": 112, "endOffset": 115}, {"referenceID": 6, "context": "TPSR [7] provides a good framework for modeling complex environments.", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "Interleaved learning and planning [6] provides a method to simultaneously learn and", "startOffset": 34, "endOffset": 37}], "year": 2017, "abstractText": "In this paper we tackle the problem of transferring policy from multiple partially observable source environments to a partially observable target environment modeled as predictive state representation. This is an entirely new approach with no previous work, other than the case of transfer in fully observable domains. We develop algorithms to successfully achieve policy transfer when we have the model of both the source and target tasks and discuss in detail their performance and shortcomings. These algorithms could be a starting point for the field of transfer learning in partial observability.", "creator": "LaTeX with hyperref package"}}}