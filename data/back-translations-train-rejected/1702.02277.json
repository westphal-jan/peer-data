{"id": "1702.02277", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "A Historical Review of Forty Years of Research on CMAC", "abstract": "The Cerebellar Model Articulation Controller (CMAC) is an influential brain-inspired computing model in many relevant fields. Since its inception in the 1970s, the model has been intensively studied and many variants of the prototype, such as Kernel-CMAC, Self-Organizing Map CMAC, and Linguistic CMAC, have been proposed. This review article focus on how the CMAC model is gradually developed and refined to meet the demand of fast, adaptive, and robust control. Two perspective, CMAC as a neural network and CMAC as a table look-up technique are presented. Three aspects of the model: the architecture, learning algorithms and applications are discussed. In the end, some potential future research directions on this model are suggested.", "histories": [["v1", "Wed, 8 Feb 2017 04:27:11 GMT  (988kb,D)", "http://arxiv.org/abs/1702.02277v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["frank z xing"], "accepted": false, "id": "1702.02277"}, "pdf": {"name": "1702.02277.pdf", "metadata": {"source": "CRF", "title": "A Historical Review of Forty Years of Research on CMAC", "authors": ["Frank Z. Xing"], "emails": ["zxing001@e.ntu.edu.sg"], "sections": [{"heading": null, "text": "This year it is more than ever before."}, {"heading": "II. ARCHITECTURE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Basic Architectures", "text": "Before the CMAC was proposed in the 1970s, the anatomy and physiology of the cerebellum was studied for a long time. It is generally accepted that many different types of nerve cells are involved in cerebellar functioning. Fig. 2 shows the computer model proposed by Mauk and Donegan. A simpler and more feasible model is shown in Fig. 3.Based on the computer model of the cerebellum (Fig. 3), the primary CMAC structure is designed. While it is obvious that the high-dimensional proximity of association rules cannot be captured in this primary form, because the nodes are arranged in a one-dimensional array."}, {"heading": "B. Modified Architectures", "text": "This year it is more than ever before in the history of the city, where it is so far that it is a place, where it is a place, where it is a place."}, {"heading": "III. LEARNING ALGORITHM", "text": "The original form of learning proposed by Albus is based on the reverse propagation of errors. The rapid convergence of this algorithm is mathematically proven by successful research. Specifically, the convergence rate is determined only by the size of the susceptible fields for association cells [37]. Some studies [25] suggest that it would be useful to distinguish between target training and error training, although they share the same mathematical form. In the weight updating rule, wi (k + 1) = wi (k) + \u03b1 error count (A) xik is the epoch time, \u03b1 [0,1] is the learning rate, xi is a state indicator of activation. The learning process is theoretically faster with a larger \u03b1, while there may be overruns. If the difference between output and the desired value \u0445 y = y \u2212 y \u00b2 can well define the error, target training and error training will be equivalent. In certain cases, Error = 1 2c (second W2c) is the initial statistic based on many popular."}, {"heading": "A. Adjusting value of weights", "text": "It can be achieved by using two CMAC components, one as the main controller, another as the controller or compensated controller. Another way to achieve this adjustment is by imposing a resistant term for the weight update rule: wi (k + 1) = wi (k) + 0 c\u03b1i Error count (A) xiIn the above rule, it is 0 and c constants that indicate the average activation times of the memory units activated by training. For both fixed learning rate and adaptive learning rate, the adjustments begin with a randomized set of parameters. Experiments indicate that the training samples are within the local generalization span, perfect linear interpolation cannot be achieved."}, {"heading": "B. Adjusting number of weights", "text": "The adjustment of the number of weights is mainly realized through the introduction of multi-resolution and dynamic quantization techniques. As Section I explained, CMAC was initially used for the real-time control system. Consequently, the structural design fits well with the hardware implementation. Many CMAC variants, such as LCMAC and FCMAC, inherit the memory unit in a grid-based manner. Inputs are generally constructed on grids with the same space. This property adds local constraints to the value of neighboring units. If we look at this problem from a functional approximation perspective, it is also quite intuitive that the local complex form requires a larger number of lower-order elements to approach it. Of course, multi-resolution grating techniques [19] were proposed in the 1990s. With our prior knowledge, some metrics can be used to determine the resolution, for example, the variation of output in certain memory areas that can be shaped."}, {"heading": "IV. APPLICATION", "text": "Although CMAC was originally proposed for manipulation control, it has proven itself in recent decades in the areas of robotic arm control, fractionation control, piezoelectric actuator control [22], mechanics, signal processing, intelligent driver warning system, fuel injection, automation of channel control, printer color reproduction, cash demand at the ATM [31] and many other areas [7] due to the fast learning ability and stable performance of CMAC models. In addition, engineering-oriented software and toolkit also help promote the use of CAMC. In the following part, the application of CMAC is elaborated on two emerging technical areas."}, {"heading": "A. Financial Engineering", "text": "On the practical side of financial engineering, various instruments have been used to model the price and risk of bonds, shares, options, futures and other derivatives. In most cases, historical data can be broken down into chunks of selected time span to allow for a supervised learning process. In 2008, Teddy et al. [29] proposed an improved CMAC variant (PSECMAC) to model the price of the American call option for foreign exchange trading. In the article, three variables are taken into account: difference between strike price and current price, maturity and price volatility. Therefore, the price function is formulated as follows: C0 = f (S0 \u2212 X, T, \u03c330) The article identifies PSECMAC as the most powerful model among several CMAC-like systems. It is also reported that most CMAC models achieve better results than the use of the RMSE Black Scholes model."}, {"heading": "B. Adaptive Control", "text": "?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????)????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "V. DISCUSSIONS", "text": "Referring to Section IV, it has been shown that CMAC is effective in many classic control problems and has been applied to emerging technical problems. However, due to the lack of a fundamental breakthrough in the last decade, this model appears to have encountered a bottleneck. Nowadays, the issues discussed are mainly focused on trivial modifications of the memory structure and learning algorithm. The framework of error multiplication or minimization of loss function remains unchanged. To my knowledge, the limitations of current CMAC models can be attributed to their excessive simplification of cerebellar structure. Therefore, the next generation cerebellar model may adopt new neuroscientific insights, for example, associative memory cells may assume different roles instead of being treated identically. In fact, anatomical models typically exhibit several types of elementary cerebellar processing units. Meanwhile, the theory of spike timing dependent plasticity (STDP) suggests that the learning process may be ordered during the firing of neuroscience [6]."}], "references": [{"title": "A Theory of Cerebellar Function", "author": ["J.S. Albus"], "venue": "Mathematical Biosciences, pp. 25-61, 1971.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1971}, {"title": "A New Approach to Manipulator Control: the Cerebellar Model Articulation Controller (CMAC)", "author": ["J.S. Albus"], "venue": "Trans. ASME, Series G. Journal of Dynamic Systems, Measurement and Control, 97, pp. 220- 233, 1975.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1975}, {"title": "Mechanisms of Planning and Problem Solving in the Brain", "author": ["J.S. Albus"], "venue": "Mathematical Biosciences, 45, pp. 247-293, 1979.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1979}, {"title": "Design Improvements in Associative Memories for Cerebellar Model Articulation Controllers", "author": ["P.C.E. An", "W.T. Miller", "P.C. Parks"], "venue": "Proceedings of ICANN, pp. 1207-10, 1991.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1991}, {"title": "Improved MCMAC with momentum, neighborhood, and averaged trapezoidal output", "author": ["K.K. Ang", "C. Quek"], "venue": "IEEE Transactions on Systems. Man and Cybernetics: PartB, 30(3), pp. 491-590, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Spike TimingDependent Plasticity: A Hebbian Learning Rule", "author": ["N. Caporale", "Y. Dan"], "venue": "Annual Review of Neuroscience, 31(1), pp. 25-46, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "CMAC Neural Network based Neural Computation and Neuro-control", "author": ["P. Duan", "H. Shao"], "venue": "Information and Control (in Chinese), 28(3), 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "A Cascade of Linguistic CMAC Neural Networks for Decision Making", "author": ["H. He", "Z. Zhu", "A. Tiwari", "A. Mills"], "venue": "International Joint Conference on Neural Networks (IJCNN), 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Kernel CMAC with improved capability", "author": ["G. Horv\u00e1th", "T. Szab\u00f3"], "venue": "IEEE Transactions on Systems. Man and Cybernetics: PartB, 37(1), pp. 124\u201338, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Cascade-CMAC neural network applications on the color scanner to printer calibration", "author": ["K.-L. Huang", "S.-C. Hsieh", "H.-C. Fu"], "venue": "International Conference on Neural Networks, 1997.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Theory and development of higher-order CMAC neural networks", "author": ["S.H. Lane", "D.A. Handelman", "J.J. Gelfand"], "venue": "IEEE Control Systems, April, pp. 23\u201330, 1992.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1992}, {"title": "Adaptive steering control using fuzzy CMAC for electric seatless unicycles", "author": ["Y.-Y. Li", "C.-C. Tsai", "F.-C. Tai", "H.-S. Yap"], "venue": "IEEE International Conference on Control & Automation, pp. 556\u2013561, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "On a new CMAC control scheme, and its comparisons with the PID controllers", "author": ["C.C. Lin", "F.C. Chen"], "venue": "Proceedings of the American Control Conference, pp. 769\u2013774, 2001.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Adaptive CMAC-based supervisory control for uncertain nonlinear systems", "author": ["C.-M. Lin", "Y.-F. Peng"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B, 34(2), pp. 1248\u201360, 2004.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Self-organizing adaptive wavelet CMAC backstepping control system design for nonlinear chaotic systems", "author": ["C.-M. Lin", "H.-Y. Li"], "venue": "Nonlinear Analysis: Real World Applications, 14(1), pp. 206\u2013223, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "CMAC Study with Adaptive Quantization", "author": ["H.-C. Lu", "M.-F. Yeh", "J.-C. Chang"], "venue": "IEEE Intl. Conf. on Systems, Man, and Cybernetics, Taipei, pp. 2596\u20132601, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Using RBFs in a CMAC to prevent parameter drift in adaptive control", "author": ["C.J.B. Macnab"], "venue": "Neurocomputing, pp. 45\u201352, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "A Model of Pavlovian Eyelid Conditioning Based on The Synaptic Organization of Cerebellum", "author": ["M.D. Mauk", "N.H. Donegan"], "venue": "Learn. Mem., 3, pp. 130-158, 1997.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "On the training of a multi-resolution CMAC neural network", "author": ["A. Menozzi", "M.-Y. Chow"], "venue": "Proceedings of the IEEE International Symposium on Industrial Electronics, 1997.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "CMAC neural networks structures", "author": ["K. Mohajeri", "G. Pishehvar", "M. Seifi"], "venue": "IEEE International Symposium on Computational Intelligence in Robotics and Automation, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Fuzzy CMAC structures", "author": ["K. Mohajeri", "M. Zakizadeh", "B. Moaveni", "M. Teshnehlab"], "venue": "IEEE International Conference on Fuzzy Systems, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive CMAC model reference control system for linear piezoelectric ceramic motor", "author": ["Y.-F. Peng", "R.-J. Wai", "C.-M. Lin"], "venue": "IEEE International Symposium on Computational Intelligence in Robotics and Automation, 2003.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms", "author": ["F. Rosenblatt"], "venue": "Spartan Books, Washington DC, 1961.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1961}, {"title": "Learning Internal Representations by Error Propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323(9), 1986.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1986}, {"title": "Intelligent Motion Control with an Artificial Cerebellum", "author": ["R.L. Smith"], "venue": "PhD Thesis of The University of Auckland, New Zealand, Chapter 3, 1998.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "Credit assigned CMAC and its application to online learning robust controllers", "author": ["S.-F. Su", "T. Tao", "T.-H. Hung"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 33(2), 2003.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding", "author": ["R.S. Sutton"], "venue": "Advances in Neural Information Processing Systems, pp. 1038\u20131044, MIT Press, 1996.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Hierarchically Clustered Adaptive Quantization CMAC and Its Learning Convergence", "author": ["S.D. Teddy", "E.M.-K. Lai", "C. Quek"], "venue": "IEEE TRANSACTIONS ON NEURAL NETWORKS, 18(6), 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "A cerebellar associative memory approach to option pricing and arbitrage trading", "author": ["S.D. Teddy", "C. Quek", "E.M.-K. Lai"], "venue": "Neurocomputing, 19(4), 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "PSECMAC: A Novel Self- Organizing Multiresolution Associative Memory Architecture", "author": ["S.D. Teddy", "C. Quek", "E.M.-K. Lai"], "venue": "IEEE Trans. Neural Networks, 19(4), pp. 689\u2013712, 2008.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "Forecasting ATM cash demands using a local learning model of cerebellar associative memory network", "author": ["S.D. Teddy", "S.K. Ng"], "venue": "International Journal of Forecasting, 27(3), pp. 760\u2013776, 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "A hierarchical CMAC architecture for context dependent function approximation", "author": ["C.-K. Tham"], "venue": "IEEE International Conference on Neural Networks, 1996.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "KCMAC-BYY: Kernel CMAC using Bayesian YingYang learning", "author": ["K. Tian", "B. Guo", "G. Liu", "I. Mitchell", "D. Cheng", "W. Zhao"], "venue": "Neurocomputing, 101, pp. 24-31, 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "The Cerebellum: An Incomplete Multilayer Perceptron", "author": ["H. Voicu"], "venue": "Neurocomputing, 72, pp. 592-599, 2008.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Hash-coding in CMAC Neural Networks", "author": ["Z.-Q. Wang", "J.L. Shiano", "M. Ginsberg"], "venue": "IEEE International Conference on Neural Networks, Washington, pp. 1698-1703, 1996.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1996}, {"title": "CMAC Learning is Governed by a Single Parameter", "author": ["Y.-F."], "venue": "IEEE International Conference on Neural Networks, San Francisco, pp. 1439-43, 1993.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1993}, {"title": "Predicting Evolving Chaotic Time Series with Fuzzy Neural Networks", "author": ["F.Z. Xing", "E. Cambria", "X. Zou"], "venue": "International Joint Conference on Neural Networks (IJCNN), 2017.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2017}, {"title": "FIE-FCMAC: A novel fuzzy cerebellum model articulation controller (FCMAC) using fuzzy interpolation and extrapolation technique", "author": ["W.J. Zhou", "D.L. Maskell", "C. Quek"], "venue": "International Joint Conference on Neural Networks (IJCNN), 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Albus in 1975 [2].", "startOffset": 14, "endOffset": 17}, {"referenceID": 22, "context": "Parallel at this time in the history, the concept of perceptron [23] had already been popular, whereas effective learning schemes to tune perceptrons [24] were not on the stage yet.", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "Parallel at this time in the history, the concept of perceptron [23] had already been popular, whereas effective learning schemes to tune perceptrons [24] were not on the stage yet.", "startOffset": 150, "endOffset": 154}, {"referenceID": 0, "context": "Consequently, although the name of CMAC appears bio-inspired enough, and the theory that the cerebellum is analogous to a perceptron has been proposed earlier [1], CMAC was emphasized to be understood as a table referring technique that can adaptive to real-time control system.", "startOffset": 159, "endOffset": 162}, {"referenceID": 2, "context": "Nevertheless, the underlying bioscience mechanism was addressed again in 1979 by Albus [3], which always gives the CMAC model two different ways of interpretation.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "tions are represented in the weighted look-up table, rather than by solution of analytic equations or by analog [2].", "startOffset": 112, "endOffset": 115}, {"referenceID": 33, "context": "For instance, recent biological evidence from eyelid conditioning experiments suggests that the cerebellum is capable of computing exclusive disjunction [34].", "startOffset": 153, "endOffset": 157}, {"referenceID": 24, "context": "Despite the advantage aforementioned, CMAC has the following disadvantages as well [25]:", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "in 2009 [20], this article is inventive for its chronological perspective.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "2 shows the computational model proposed by Mauk and Donegan [18].", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "A more simple and implementable model proposed by Albus [3] is exhibited in Fig.", "startOffset": 56, "endOffset": 59}, {"referenceID": 0, "context": "Albus\u2019 model [1] of cerebellum resembles a forward feed perceptron.", "startOffset": 13, "endOffset": 16}, {"referenceID": 24, "context": "5 illustrates the difference at a conceptual level [25].", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "Literature [25] introduced a hardware implementation which uses selected binary bits of indexes as the hashing code.", "startOffset": 11, "endOffset": 15}, {"referenceID": 34, "context": "[35], claims that due", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Therefore, many other attempts have been made, such as neighbor sequential, CMAC with general basis functions, and adaptive coding [7], which is a similar idea to hashing in the sense of weight space", "startOffset": 131, "endOffset": 134}, {"referenceID": 9, "context": "Cascade CMAC architecture was firstly proposed in 1997 for the purpose of printer calibration [10] (Fig.", "startOffset": 94, "endOffset": 98}, {"referenceID": 31, "context": "Then the architecture can be reckoned as a Hierarchical CMAC (H-CMAC), which is described by Tham [32] in 1996.", "startOffset": 98, "endOffset": 102}, {"referenceID": 31, "context": "Hierarchical CMAC architecture, adapted from [32]", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "[11] descried high order CMAC as CMAC with binary neuron output replaced by spline functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "A cascade of LCMAC series was further developed in 2015 [8].", "startOffset": 56, "endOffset": 59}, {"referenceID": 20, "context": "Parametric-FCMAC (PFCMAC) [21].", "startOffset": 26, "endOffset": 30}, {"referenceID": 37, "context": "In 2015, Zhou and Quek proposed FIE-FCMAC [39], which adds fuzzy interpolation and extrapolation to a rigid CMAC structure.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "It is reported that using RBFs can prevent parameter drift and accelerate synchronization speed to the changing system [17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "For this type of combination, beside RBF, Wavelet Neural Network (WNN), fuzzy rule neuron and recurrent mechanism [15] or a mingle of them can also", "startOffset": 114, "endOffset": 118}, {"referenceID": 36, "context": "Previous works, such as [38], have provided evidence that these features are effective for modeling complicated dynamic systems.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "Back-forward signal acceptor or conjugated CMACs are often used to accelerate this process [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "Similarly, CMAC structures that modifies the storage optimizing methods, for example quantization [16] and multiresolution [19], will only result in architectural difference from a hardware implementation sense.", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "Similarly, CMAC structures that modifies the storage optimizing methods, for example quantization [16] and multiresolution [19], will only result in architectural difference from a hardware implementation sense.", "startOffset": 123, "endOffset": 127}, {"referenceID": 35, "context": "Specifically, the convergence rate is only governed by the size of receptive fields for association cells [37].", "startOffset": 106, "endOffset": 110}, {"referenceID": 24, "context": "Some study [25] suggests that it would be useful to distinguish between target training and error training, despite they share the same mathematical form.", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "k is the epoch times, \u03b1 \u2208 [0,1] is the learning rate, xi is a state indicator of activation.", "startOffset": 26, "endOffset": 31}, {"referenceID": 13, "context": "This improvement is called adaptive learning rate [14].", "startOffset": 50, "endOffset": 54}, {"referenceID": 24, "context": "Experiments suggest that for sparse data, even if the training samples are within the local generalization range, perfect linear interpolation may not be achieved [25].", "startOffset": 163, "endOffset": 167}, {"referenceID": 25, "context": "In 2003, research [26] pointed out that equally assign the error to each weight is not a meticulous method.", "startOffset": 18, "endOffset": 22}, {"referenceID": 4, "context": "In 2000, Ang and Quek [5] proposed learning with momentum, neighborhood, and averaged fuzzy output for both CMAC and MCMAC.", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "The neighborhood learning rule proposed by Ang and Quek [5] serves the same purpose as weight smoothing technique.", "startOffset": 56, "endOffset": 59}, {"referenceID": 8, "context": "In 2007, Kernel CMAC (KCMAC) was proposed [9], [11] to reduce the CMAC dimension which usually", "startOffset": 42, "endOffset": 45}, {"referenceID": 10, "context": "In 2007, Kernel CMAC (KCMAC) was proposed [9], [11] to reduce the CMAC dimension which usually", "startOffset": 47, "endOffset": 51}, {"referenceID": 32, "context": "Though other learning method can be employed as well, for instance, using Bayesian Ying-Yang (BYY) learning as proposed in 2013 by Tian et al [33].", "startOffset": 142, "endOffset": 146}, {"referenceID": 18, "context": "Naturally, multi-resolution lattice techniques [19] were proposed in the 1990s.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "To the best of my knowledge, the idea of adaptive quantization was initially proposed in 2006 [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 27, "context": "The termination condition is that, within all intervals, the difference of output values should not exceed an experimental constant C [28].", "startOffset": 134, "endOffset": 138}, {"referenceID": 29, "context": "The adaptive quantization technique is later developed to the Pseudo Self Evolving CMAC (PSECMAC) model [30], which further introduced neighborhood activation mechanism.", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "Though CMAC was firstly proposed for manipulator control, during the past decades it has been proved effective in robotic arm control, fractionator control, piezoelectric actuator control [22], mechanic system, signal processing, intelligent driver warning system, fuel injection, canal control automation, printer color reproduction, ATM cash demand [31], and many other fields [7].", "startOffset": 188, "endOffset": 192}, {"referenceID": 30, "context": "Though CMAC was firstly proposed for manipulator control, during the past decades it has been proved effective in robotic arm control, fractionator control, piezoelectric actuator control [22], mechanic system, signal processing, intelligent driver warning system, fuel injection, canal control automation, printer color reproduction, ATM cash demand [31], and many other fields [7].", "startOffset": 351, "endOffset": 355}, {"referenceID": 6, "context": "Though CMAC was firstly proposed for manipulator control, during the past decades it has been proved effective in robotic arm control, fractionator control, piezoelectric actuator control [22], mechanic system, signal processing, intelligent driver warning system, fuel injection, canal control automation, printer color reproduction, ATM cash demand [31], and many other fields [7].", "startOffset": 379, "endOffset": 382}, {"referenceID": 28, "context": "et al [29] proposed an improved CMAC variant (PSECMAC) to model the price of currency exchange American call option.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "Recently, studies have been carried on adaptive control of disabled wheelchair and seatless unicycle [12].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "Li et al [12] proposed a TSK type FCMAC to synthesis the equations of adaptive steering control.", "startOffset": 9, "endOffset": 13}], "year": 2017, "abstractText": "The Cerebellar Model Articulation Controller (CMAC) is an influential brain-inspired computing model in many relevant fields. Since its inception in the 1970s, the model has been intensively studied and many variants of the prototype, such as KCMAC, MCMAC, and LCMAC, have been proposed. This review article focus on how the CMAC model is gradually developed and refined to meet the demand of fast, adaptive, and robust control. Two perspective, CMAC as a neural network and CMAC as a table look-up technique are presented. Three aspects of the model: the architecture, learning algorithms and applications are discussed. In the end, some potential future research directions on this model are suggested.", "creator": "LaTeX with hyperref package"}}}