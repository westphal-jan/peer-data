{"id": "1702.02287", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Name Disambiguation in Anonymized Graphs using Network Embedding", "abstract": "In real-world, our DNA is unique but many people share same names. This phenomenon often causes erroneous aggregation of documents of multiple persons who are namesake of one another. Such mistakes deteriorate the performance of document retrieval, web search, and more seriously, cause improper attribution of credit or blame in digital forensic. To resolve this issue, the name entity disambiguation task is designed which aims to partition the documents associated with a name reference such that each partition contains documents pertaining to a unique real-life person. Existing solutions to this task substantially rely on feature engineering, such as biographical feature extraction, or construction of auxiliary features from Wikipedia. However, for many scenarios, such features may be costly to obtain or unavailable due to the risk of privacy violation. In this work, we propose a novel name disambiguation method. Our proposed method is non-intrusive of privacy because instead of using attributes pertaining to a real-life person, our method leverages only relational data in the form of anonymized graphs. In the aspect of methodological novelty, the proposed method uses a representation learning strategy to embed each document in a low dimensional vector space where name disambiguation can be solved by a hierarchical agglomerative clustering algorithm. Our experimental results demonstrate that the proposed method is significantly better than the existing name entity disambiguation methods working in a similar setting.", "histories": [["v1", "Wed, 8 Feb 2017 04:54:09 GMT  (94kb)", "http://arxiv.org/abs/1702.02287v1", null], ["v2", "Thu, 4 May 2017 00:40:44 GMT  (0kb,I)", "http://arxiv.org/abs/1702.02287v2", "Made critical mistake in the experimental section"], ["v3", "Tue, 8 Aug 2017 14:29:03 GMT  (487kb)", "http://arxiv.org/abs/1702.02287v3", "The 26th ACM International Conference on Information and Knowledge Management (CIKM 2017) research track full paper"], ["v4", "Sat, 9 Sep 2017 23:05:04 GMT  (486kb)", "http://arxiv.org/abs/1702.02287v4", "The 26th ACM International Conference on Information and Knowledge Management (CIKM 2017) research track full paper"]], "reviews": [], "SUBJECTS": "cs.SI cs.CL cs.IR", "authors": ["baichuan zhang", "mohammad al hasan"], "accepted": false, "id": "1702.02287"}, "pdf": {"name": "1702.02287.pdf", "metadata": {"source": "META", "title": "Name Entity Disambiguation in Anonymized Graphs using Link Analysis: A Network Embedding based Solution", "authors": ["Baichuan Zhang", "Mohammad Al Hasan"], "emails": ["zhan1910@purdue.edu", "alhasan@cs.iupui.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.02 287v 1 [cs.S I] 8 February 201 123 7In the real world, our DNA is unique, but many people share the same names. This phenomenon often leads to incorrect aggregation of documents by multiple persons who are namesakes of each other. Such errors worsen the performance of retrieval of documents, web search and more seriously, causing an improper allocation of credits or blame in digital forensics. To solve this problem, the task of name sensibility disambiguation is designed, which aims to distribute the documents associated with a name reference in such a way that each partition contains documents that relate to a unique real person. Existing solutions to this task rely essentially on feature engineering, such as biographical features extraction or the construction of auxiliary features from Wikipedia. However, such features can be costly to obtain or unavailable due to the risk of privacy infringement."}, {"heading": "1 INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2 RELATED WORK", "text": "There is a large number of work on the decoding of names [3, 8, 30]. In terms of privacy, existing work has been classified as supervised [1, 8], unsupervised [3, 9], and probabilistic relationship models [22, 23, 29]. However, in the supervised environment, the disamination of names is proposed through the use of Naive Bayes and SVM. In these works, a certain real being can be considered as a class, and the goal is to classify each data set into one of the classes. For the unsupervised disamination of names, the data sets are divided into several clusters, with the goal of maintaining a certain real identity."}, {"heading": "3 PROBLEM FORMULATION", "text": "This year, it has reached the point where it will be able to leave the country in which it is able to leave it, and it is able to leave the country in order to save it."}, {"heading": "4 METHOD", "text": "In this section, we discuss our proposed network embedding model for disambiguating name units. Our goal is to embed the local neighborhood structures captured by the three networks (see Definitions 3.1 3.2 3.3) in the k-dimensional document, which contains a matrix with strong disambiguation capability of the name."}, {"heading": "4.1 Model Formulation", "text": "The main intuition of our network embedding model is that nodes with a similar local neighborhood structure should have a similar vector representation in the embedding space. (For example, the embedding vector for di should be a good predictor of the embedding vector of its adjacent vertex dj.) This goal can be achieved by maximizing the following objective function: OBJdd = max D, i, D, J, J, J, J, J, D, D, OddlogP (dj, di), where D, IRN, K, P (dj, di) are the conditional probabilities of vertex dj generated by vertex di in Gdd and can be defined by the following softmax function: P, dj, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D."}, {"heading": "4.2 Model Optimization", "text": "To speed up the learning speed, we must adopt the negative sampling method approach in Word2Vec [17], where the negative sampling method is defined by the following objective function: logus (dTj di) + T-1) + T-1-2-1-4-4-4-4-4-4-4-4-4-4-4-4-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-"}, {"heading": "4.3 Pseudo-code and Complexity Analysis", "text": "The whole process consists of two phases: the network embedding for the presentation of documents and the embedding of entities by clustering. Specifically, we first prepare the training instances in line 1-2. Line 3 initializes the author and the document by randomly selecting elements from the Gaussian distribution with the standard deviation of 0 mean and 0.1. Then we train our proposed network embedding model and update A and D using the training samples based on the ASGD optimization in line 4-6. Then we get the obtained document in which the matrix D and L are embedded. (Line 7, we perform HAC to partition N documents into Da in L."}, {"heading": "5 EXPERIMENTS AND RESULTS", "text": "We are conducting several experiments to test the performance of our proposed network embedding method for solving the entity disambiguation task in a privacy-preserving environment with only linked data. We are also comparing our method with various other methods to demonstrate its superiority over these methods."}, {"heading": "5.1 Datasets", "text": "A key challenge in evaluating the entity disambiguation task is the lack of availability of marked records from different fields of application. In recent years, the bibliographic repository sites Arnetminer 2 and CiteSeerX 3 have published several ambiguous references to author names along with the respective basic truths (paper list of each real person) that we use for assessment. From each of these two sources, we use 10 highly ambiguous (with a larger number of different authors for a given name) name references and show the performance of our method in these name references. The statistics of name references in Arnetminer and CiteSeerX records are in Table 1 and Table 2, respectively. In these tables, we show for each name reference the number of documents and the number of unique authors associated with that name reference. It is important to understand that the entity disambiguity model is based on a name reference, not on a source data such as:"}, {"heading": "5.2 Competing Methods", "text": "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "5.3 Experimental Setting", "text": "For each of the 20 name references, we perform name checks using our proposed method and each of the competing methods to show that our proposed method is better than the competing methods. For metric evaluation, we use macro F1 measurement [27], which is the average of measurement by F1 in each class. The range of measurement by macro F1 is between 0 and 1, and a higher value indicates better disambiguation performance. In addition to comparing with competing methods, we also conduct experiments to show that our method is robust against variation of user-defined parameters (in particular the embedding dimension and number of clusters) across a wide range of parameter values. Experiments are also conducted to show the convergence of the learning model while performing the embedding phase of the document. Finally, we show a comparison of 2D visualization of document clusters as achieved by using different embedding methods."}, {"heading": "5.4 Comparison among Various Name Entity Disambiguation Methods", "text": "In both tables, the rows correspond to the name references and columns (2 to 12) used for various methods; the first group includes the baseline methods that we have designed, such as the random predictor (margin) and the methods that use the author list for clustering; the second group includes different types of methods that embed methods; and the third group includes methods based on graphs; the cell values are associated with the use of Macro F1 Score for document dislocation; and the last column shows the general improvement of our proposed method compared to the best methods."}, {"heading": "5.5 Parameter Sensitivity of Embedding Dimension", "text": "We are also conducting experiments to show how the embedding dimension k affects the embedding performance of our proposed method and other competing embedding methods. In all methods, we vary the number of embedding dimensions k as 10 and 30. The embedding results among all 20 name references from both sources are in Tables 5, 6, 7 and 8. As we can see in both sets of data, our proposed method exceeds all competing methods in most name references. On the other hand, we note that with most name references, the embedding performance improves significantly as the embedding dimension increases, due to the fact that if the embedding dimension increases from 10% to 0.30%, the display capability of the embedding vectors is insufficient and we may lose information. However, with a few name references such as \"A Kumar\" in CiteSeerX dataset, where the embedding dimension increases from 10% to 0.30%, the embedding size may be larger than the suggested version of 1."}, {"heading": "5.6 Performance Comparison over the Number of Clusters", "text": "One of the potential problems in finding names is determining the number of real people L under a particular name reference, because in real life L is generally a priori unknown. Therefore, a method should be preferred whose performance exceeds a range of L values. For this comparison, after learning the presentation of the document, we use different L values as input into the HAC to disambiguate name units and record the Macro F1 value over different L values for the competing methods. In our experiment, we compare the Macro F1 value of our method with two other best-functioning methods over several names, but for reasons of space, we show this result only for one name (\"Lei Wang\" in Arnetminer) using bar charts in Figure 2. In this figure, we compare the performance differences between our method and two other best-functioning methods (GF and LINE), since we vary L as {40, 45, 50, 55, 60}. Note that the actual number of authors suggested in this table is always better than the number of authors shown in Lei 1, since the improvement in Wang is 0.48. \""}, {"heading": "5.7 Convergence Analysis", "text": "Figure 3 shows the convergence analysis of our method under two name parameters, with \"Lei Wang\" coming from Arnetminer and \"J Martin\" from CiteSeerX. For each iteration, we select 5% | Edd | samples to update the model parameters. We can observe that our method converges within about 150 iterations of both name parameters and achieves promising convergence results. However, as shown in Eq.10, the objective function in our proposed embedding model is not convex, making a global optimal solution using ASGD-based optimization techniques quite a challenging task. A possible remedy could be to reduce the learning rate \u03b1 in ASGD if the number of iterations increases. Another strategy is to try out multiple runs with different seed initializations. Similar convergence patterns are also observed with other name references."}, {"heading": "5.8 Document Visualization", "text": "Finally, we provide an illustrative document visualization result under the name \"Rakesh Kumar\" to compare the quality of different network embedding approaches. As shown in Table 1, it contains 82 documents associated with 5 different real-life authors. In Figure 4, each point represents a document and its corresponding color reflects the ground-truth cluster. Furthermore, for all embedding approaches, we set the embedding dimension to 20. As we can see, the quality of PTE visualization is worst because documents belonging to the same real-life author are not tightly clustered. The visualization results of DeepWalk and Node2Vec are better than PTE. However, there is still a significant number of loud points between two dominant clusters shown in blue and grass-green colors. This is because both methods use a random, path AC-based approach to well reflect the neighbors \"ease of embedding due to very high noise levels."}, {"heading": "6 CONCLUSION", "text": "Finally, in this paper, we propose a network-based solution to the problem of disambiguation of name units. As the proposed solution only uses relational data, it is particularly useful for disambiguating name units in the anonymised network, where node attributes are not available due to privacy concerns. Our experimental results on several data sets show that the proposed method significantly exceeds the existing state of the art in terms of disambiguating names in a similar setup."}], "references": [{"title": "Using Encyclopedic Knowledge for Named Entity Disambiguation", "author": ["Razvan Bunescu", "Marius Pasca"], "venue": "In European Chapter of the Association for Comp. Linguistics", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "GraRep: Learning Graph Representations with Global Structural Information", "author": ["Shaosheng Cao", "Wei Lu", "Qiongkai Xu"], "venue": "In CIKM", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Author Disambiguation by Hierarchical Agglomerative Clustering with Adaptive Stopping Criterion", "author": ["Lei Cen", "Eduard C. Dragut", "Luo Si", "Mourad Ouzzani"], "venue": "In SIGIR", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Efficient Techniques for Document Sanitization", "author": ["Venkatesan T. Chakaravarthy", "Himanshu Gupta", "Prasan Roy", "Mukesh K. Mohania"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Heterogeneous Network Embedding via Deep Architectures", "author": ["Shiyu Chang", "Wei Han", "Jiliang Tang", "Guo-Jun Qi", "Charu C. Aggarwal", "Thomas S. Huang"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Multi-centrality graph spectral decompositions and their application to cyber intrusion detection", "author": ["P.Y. Chen", "S. Choudhury", "A.O. Hero"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Node2Vec: Scalable Feature Learning for Networks", "author": ["Aditya Grover", "Jure Leskovec"], "venue": "In SIGKDD", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Two Supervised Learning Approaches for Name Disambiguation in Author Citations", "author": ["Hui Han", "Lee Giles", "Hongyuan Zha", "Cheng Li", "Kostas Tsioutsiouliklis"], "venue": "In Joint Conf. on Digital Libraries", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Name Disambiguation in Author Citations Using a K-way Spectral Clustering Method", "author": ["Hui Han", "Hongyuan Zha", "C. Lee Giles"], "venue": "In ACM Joint Conf. on Digital Libraries", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Collective Entity Linking in Web Text: A Graph-based Method", "author": ["Xianpei Han", "Le Sun", "Jun Zhao"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Named Entity Disambiguation by Leveraging Wikipedia Semantic Knowledge", "author": ["Xianpei Han", "Jun Zhao"], "venue": "In CIKM", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Entity Disambiguation in Anonymized Graphs Using Graph Kernels", "author": ["Linus Hermansson", "Tommi Kerola", "Fredrik Johansson", "Vinay Jethava", "Devdatt Dubhashi"], "venue": "In CIKM", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Robust Disambiguation of Named Entities in Text", "author": ["Johannes Hoffart", "Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Symmetric Non-negative Matrix Factorization for Graph Clustering", "author": ["Da Kuang", "Haesun Park", "Chris H.Q. Ding"], "venue": "In SDM", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Algorithms for Nonnegative Matrix Factorization", "author": ["Daniel D. Lee", "H. Sebastian Seung"], "venue": "In NIPS", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Unsupervised Large Graph Embedding", "author": ["Feiping Nie", "Wei Zhu", "Xuelong Li"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Deep- Walk: Online Learning of Social Representations", "author": ["Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena"], "venue": "In SIGKDD", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T. Roweis", "Lawrence K. Saul"], "venue": "SCIENCE", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Structure Preserving Embedding", "author": ["Blake Shaw", "Tony Jebara"], "venue": "In ICML\u201909", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Efficient Topic-based Unsupervised Name Disambiguation", "author": ["Yang Song", "Jian Huang", "Isaac G. Councill", "Jia Li", "C. Lee Giles"], "venue": "JCDL", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "A Unified Probabilistic Framework for Name Disambiguation in Digital Library", "author": ["Jie Tang", "Alvis C.M. Fong", "Bo Wang", "Jing Zhang"], "venue": "IEEE TKDE", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "PTE: Predictive Text Embedding Through Large-scale Heterogeneous Text Networks", "author": ["Jian Tang", "Meng Qu", "Qiaozhu Mei"], "venue": "In SIGKDD", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "LINE: Large-scale Information Network Embedding", "author": ["Jian Tang", "Meng Qu", "Mingzhe Wang", "Ming Zhang", "Jun Yan", "Qiaozhu Mei"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Visualizing High- Dimensional Data Using t-SNE", "author": ["van der Maaten", "G.E. Hinton"], "venue": "JMLR", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Data Mining and Analysis: Fundamental Concepts and Algorithms", "author": ["Mohammed J. Zaki", "Wagner Meira Jr."], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "A Constraint-based Probabilistic Framework for Name Disambiguation", "author": ["Duo Zhang", "Jie Tang", "Juanzi Li", "Kehong Wang"], "venue": "In CIKM", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Robust and Collective Entity Disambiguation Through Semantic Embeddings", "author": ["Stefan Zwicklbauer", "Christin Seifert", "Michael Granitzer"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Name entity disambiguation [3, 8, 28, 29] is an important problem, which has numerous applications in information re-", "startOffset": 27, "endOffset": 41}, {"referenceID": 7, "context": "Name entity disambiguation [3, 8, 28, 29] is an important problem, which has numerous applications in information re-", "startOffset": 27, "endOffset": 41}, {"referenceID": 25, "context": "Name entity disambiguation [3, 8, 28, 29] is an important problem, which has numerous applications in information re-", "startOffset": 27, "endOffset": 41}, {"referenceID": 0, "context": "However, the majority of existing solutions [1, 3, 10, 13, 30] for this task use biographical features such as name, address, institutional affiliation, email address, and homepage.", "startOffset": 44, "endOffset": 62}, {"referenceID": 2, "context": "However, the majority of existing solutions [1, 3, 10, 13, 30] for this task use biographical features such as name, address, institutional affiliation, email address, and homepage.", "startOffset": 44, "endOffset": 62}, {"referenceID": 9, "context": "However, the majority of existing solutions [1, 3, 10, 13, 30] for this task use biographical features such as name, address, institutional affiliation, email address, and homepage.", "startOffset": 44, "endOffset": 62}, {"referenceID": 12, "context": "However, the majority of existing solutions [1, 3, 10, 13, 30] for this task use biographical features such as name, address, institutional affiliation, email address, and homepage.", "startOffset": 44, "endOffset": 62}, {"referenceID": 26, "context": "However, the majority of existing solutions [1, 3, 10, 13, 30] for this task use biographical features such as name, address, institutional affiliation, email address, and homepage.", "startOffset": 44, "endOffset": 62}, {"referenceID": 10, "context": "Also, contextual features such as collaborator, community affiliation, and external data source such as Wikipedia are used in some works [11, 13].", "startOffset": 137, "endOffset": 145}, {"referenceID": 12, "context": "Also, contextual features such as collaborator, community affiliation, and external data source such as Wikipedia are used in some works [11, 13].", "startOffset": 137, "endOffset": 145}, {"referenceID": 9, "context": "For such a privacy-preserving scenario many existing techniques [10, 13, 23], which compute document similarity using biographical attributes are not applicable.", "startOffset": 64, "endOffset": 76}, {"referenceID": 12, "context": "For such a privacy-preserving scenario many existing techniques [10, 13, 23], which compute document similarity using biographical attributes are not applicable.", "startOffset": 64, "endOffset": 76}, {"referenceID": 20, "context": "For such a privacy-preserving scenario many existing techniques [10, 13, 23], which compute document similarity using biographical attributes are not applicable.", "startOffset": 64, "endOffset": 76}, {"referenceID": 24, "context": "learning model embeds the document collection into a set of disambiguation-aware vectors in their latent space, such that a traditional hierarchical clustering [27] of the vectors generates excellent name entity disambiguation performance.", "startOffset": 160, "endOffset": 164}, {"referenceID": 2, "context": "There exist a large number of works on name entity disambiguation [3, 8, 30].", "startOffset": 66, "endOffset": 76}, {"referenceID": 7, "context": "There exist a large number of works on name entity disambiguation [3, 8, 30].", "startOffset": 66, "endOffset": 76}, {"referenceID": 26, "context": "There exist a large number of works on name entity disambiguation [3, 8, 30].", "startOffset": 66, "endOffset": 76}, {"referenceID": 0, "context": "In terms of methodologies, existing works have considered supervised [1, 8], unsupervised [3, 9], and probabilistic relational models [22, 23, 29].", "startOffset": 69, "endOffset": 75}, {"referenceID": 7, "context": "In terms of methodologies, existing works have considered supervised [1, 8], unsupervised [3, 9], and probabilistic relational models [22, 23, 29].", "startOffset": 69, "endOffset": 75}, {"referenceID": 2, "context": "In terms of methodologies, existing works have considered supervised [1, 8], unsupervised [3, 9], and probabilistic relational models [22, 23, 29].", "startOffset": 90, "endOffset": 96}, {"referenceID": 8, "context": "In terms of methodologies, existing works have considered supervised [1, 8], unsupervised [3, 9], and probabilistic relational models [22, 23, 29].", "startOffset": 90, "endOffset": 96}, {"referenceID": 19, "context": "In terms of methodologies, existing works have considered supervised [1, 8], unsupervised [3, 9], and probabilistic relational models [22, 23, 29].", "startOffset": 134, "endOffset": 146}, {"referenceID": 20, "context": "In terms of methodologies, existing works have considered supervised [1, 8], unsupervised [3, 9], and probabilistic relational models [22, 23, 29].", "startOffset": 134, "endOffset": 146}, {"referenceID": 25, "context": "In terms of methodologies, existing works have considered supervised [1, 8], unsupervised [3, 9], and probabilistic relational models [22, 23, 29].", "startOffset": 134, "endOffset": 146}, {"referenceID": 7, "context": "[8] proposed supervised name disambiguation methodologies by utilizing Naive Bayes and SVM.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] used K-way spectral clustering for name disambiguation in bibliographical data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "For instance, [23] proposesd to use Markov Random Fields to address name entity disambiguation challenge in a unified probabilistic framework.", "startOffset": 14, "endOffset": 18}, {"referenceID": 11, "context": "To address this issue, a few works [12, 16, 28] have considered name disambiguation using anonymized graphs without leveraging the node attributes.", "startOffset": 35, "endOffset": 47}, {"referenceID": 11, "context": "For example, authors in [12] characterized the similarity between two nodes based on their local neighborhood structures using graph kernels and solved the name dis-", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "However, the major drawback of the proposed method in [12] is that it can only detect entities that should be disambiguated, but fails to further partition the documents into their corresponding homogeneous groups.", "startOffset": 54, "endOffset": 58}, {"referenceID": 1, "context": "Our proposed solution utilizes a network representation learning based approach [2, 5, 7, 18, 19, 24, 25]\u2014 a rather recent development in machine learning.", "startOffset": 80, "endOffset": 105}, {"referenceID": 4, "context": "Our proposed solution utilizes a network representation learning based approach [2, 5, 7, 18, 19, 24, 25]\u2014 a rather recent development in machine learning.", "startOffset": 80, "endOffset": 105}, {"referenceID": 6, "context": "Our proposed solution utilizes a network representation learning based approach [2, 5, 7, 18, 19, 24, 25]\u2014 a rather recent development in machine learning.", "startOffset": 80, "endOffset": 105}, {"referenceID": 15, "context": "Our proposed solution utilizes a network representation learning based approach [2, 5, 7, 18, 19, 24, 25]\u2014 a rather recent development in machine learning.", "startOffset": 80, "endOffset": 105}, {"referenceID": 16, "context": "Our proposed solution utilizes a network representation learning based approach [2, 5, 7, 18, 19, 24, 25]\u2014 a rather recent development in machine learning.", "startOffset": 80, "endOffset": 105}, {"referenceID": 21, "context": "Our proposed solution utilizes a network representation learning based approach [2, 5, 7, 18, 19, 24, 25]\u2014 a rather recent development in machine learning.", "startOffset": 80, "endOffset": 105}, {"referenceID": 22, "context": "Our proposed solution utilizes a network representation learning based approach [2, 5, 7, 18, 19, 24, 25]\u2014 a rather recent development in machine learning.", "startOffset": 80, "endOffset": 105}, {"referenceID": 18, "context": "Different from traditional graph embedding methods, such as Structure Preserving Embedding (SPE) [21], Local Linear Embedding (LLE) [20], and Laplacian Eigenmaps [6], the recently proposed network embedding methods, such as DeepWalk [19], LINE [25], PTE [24], and Node2Vec [7], are more scalable and have shown better performance in node classification and link prediction tasks.", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "Different from traditional graph embedding methods, such as Structure Preserving Embedding (SPE) [21], Local Linear Embedding (LLE) [20], and Laplacian Eigenmaps [6], the recently proposed network embedding methods, such as DeepWalk [19], LINE [25], PTE [24], and Node2Vec [7], are more scalable and have shown better performance in node classification and link prediction tasks.", "startOffset": 132, "endOffset": 136}, {"referenceID": 5, "context": "Different from traditional graph embedding methods, such as Structure Preserving Embedding (SPE) [21], Local Linear Embedding (LLE) [20], and Laplacian Eigenmaps [6], the recently proposed network embedding methods, such as DeepWalk [19], LINE [25], PTE [24], and Node2Vec [7], are more scalable and have shown better performance in node classification and link prediction tasks.", "startOffset": 162, "endOffset": 165}, {"referenceID": 16, "context": "Different from traditional graph embedding methods, such as Structure Preserving Embedding (SPE) [21], Local Linear Embedding (LLE) [20], and Laplacian Eigenmaps [6], the recently proposed network embedding methods, such as DeepWalk [19], LINE [25], PTE [24], and Node2Vec [7], are more scalable and have shown better performance in node classification and link prediction tasks.", "startOffset": 233, "endOffset": 237}, {"referenceID": 22, "context": "Different from traditional graph embedding methods, such as Structure Preserving Embedding (SPE) [21], Local Linear Embedding (LLE) [20], and Laplacian Eigenmaps [6], the recently proposed network embedding methods, such as DeepWalk [19], LINE [25], PTE [24], and Node2Vec [7], are more scalable and have shown better performance in node classification and link prediction tasks.", "startOffset": 244, "endOffset": 248}, {"referenceID": 21, "context": "Different from traditional graph embedding methods, such as Structure Preserving Embedding (SPE) [21], Local Linear Embedding (LLE) [20], and Laplacian Eigenmaps [6], the recently proposed network embedding methods, such as DeepWalk [19], LINE [25], PTE [24], and Node2Vec [7], are more scalable and have shown better performance in node classification and link prediction tasks.", "startOffset": 254, "endOffset": 258}, {"referenceID": 6, "context": "Different from traditional graph embedding methods, such as Structure Preserving Embedding (SPE) [21], Local Linear Embedding (LLE) [20], and Laplacian Eigenmaps [6], the recently proposed network embedding methods, such as DeepWalk [19], LINE [25], PTE [24], and Node2Vec [7], are more scalable and have shown better performance in node classification and link prediction tasks.", "startOffset": 273, "endOffset": 276}, {"referenceID": 3, "context": "However, we refrained from using word co-occurrence due to the privacy concern as sometimes a list of a set of unique words can reveal the identity of a person [4].", "startOffset": 160, "endOffset": 163}, {"referenceID": 22, "context": "The sampling strategy is based on the edge sampling [25], which samples an edge for model update with the probability proportional to the edge weight in its corresponding network.", "startOffset": 52, "endOffset": 56}, {"referenceID": 24, "context": "For the name disambiguation, the computational cost of hierarchical clustering is O(NlogN) [27].", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "(3) AuthorList-NNMF: We perform Non-Negative Matrix Factorization (NNMF) [15] on the generated author-list features the same way described above.", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "(4) Graph Factorization (GF) [14]: In this method, we first represent the linked document network Gdd as an affinity matrix, and then utilize matrix factorization technique", "startOffset": 29, "endOffset": 33}, {"referenceID": 16, "context": "(5) DeepWalk [19]: DeepWalk is an approach recently proposed for network embedding, which is only applicable for network with binary edges.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "(6) LINE [25]: Given Gdd, LINE aims to learn the document embedding that preserves both the first-order and second-order proximities .", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "(7) Node2Vec [7]: Similar to DeepWalk, Node2Vec designs a biased random walk procedure for document embedding.", "startOffset": 13, "endOffset": 16}, {"referenceID": 21, "context": "(8) PTE [24]: Predictive Text Embedding (PTE) framework aims to capture the relations of word-word, word-document, and word-label.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "(9) Graph Kernel [12]: In this work, size-3 graphlets (GL3) and size-4 graphlets (GL4) are used to build graph kernels, which measure documents\u2019 similarity in Gdd.", "startOffset": 17, "endOffset": 21}, {"referenceID": 24, "context": "For evaluation metric, we use Macro-F1 measure [27], which is the average of F1 measure of each class.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 85, "endOffset": 88}, {"referenceID": 21, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 85, "endOffset": 88}, {"referenceID": 21, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "Name Our Method Rand AuthorList AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] GL3 [12] GL4 [12] Improv.", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 45, "endOffset": 49}, {"referenceID": 22, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 69, "endOffset": 72}, {"referenceID": 21, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 45, "endOffset": 49}, {"referenceID": 22, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 69, "endOffset": 72}, {"referenceID": 21, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 45, "endOffset": 49}, {"referenceID": 22, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 69, "endOffset": 72}, {"referenceID": 21, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 45, "endOffset": 49}, {"referenceID": 22, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 69, "endOffset": 72}, {"referenceID": 21, "context": "Name Our Method AuthorList- GF [14] DeepWalk [19] LINE [25] Node2Vec [7] PTE [24] Reference NNMF", "startOffset": 77, "endOffset": 81}, {"referenceID": 23, "context": "Figure 4: Document visualization using our proposed embedding model on name reference \u201cRakesh Kumar\u201d, visualized with the t-SNE [26] package.", "startOffset": 128, "endOffset": 132}], "year": 2017, "abstractText": "In real-world, our DNA is unique but many people share same names. This phenomenon often causes erroneous aggregation of documents of multiple persons who are namesake of one another. Such mistakes deteriorate the performance of document retrieval, web search, and more seriously, cause improper attribution of credit or blame in digital forensic. To resolve this issue, the name entity disambiguation task is designed which aims to partition the documents associated with a name reference such that each partition contains documents pertaining to a unique real-life person. Existing solutions to this task substantially rely on feature engineering, such as biographical feature extraction, or construction of auxiliary features from Wikipedia. However, for many scenarios, such features may be costly to obtain or unavailable due to the risk of privacy violation. In this work, we propose a novel name disambiguation method. Our proposed method is non-intrusive of privacy because instead of using attributes pertaining to a real-life person, our method leverages only relational data in the form of anonymized graphs. In the aspect of methodological novelty, the proposed method uses a representation learning strategy to embed each document in a low dimensional vector space where name disambiguation can be solved by a hierarchical agglomerative clustering algorithm. Our experimental results demonstrate that the proposed method is significantly better than the existing name entity disambiguation methods working in a similar setting.", "creator": "LaTeX with hyperref package"}}}