{"id": "1702.02302", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Autonomous Braking System via Deep Reinforcement Learning", "abstract": "In this paper, we propose a new autonomous braking system based on deep reinforcement learning. The proposed autonomous braking system automatically decides whether to apply the brake at each time step when confronting the risk of collision using the information on the obstacle obtained by the sensors. The problem of designing brake control is formulated as searching for the optimal policy in Markov decision process (MDP) model where the state is given by the relative position of the obstacle and the vehicle's speed, and the action space is defined as whether brake is stepped or not. The policy used for brake control is learned through computer simulations using the deep reinforcement learning method called deep Q-network (DQN). In order to derive desirable braking policy, we propose the reward function which balances the damage imposed to the obstacle in case of accident and the reward achieved when the vehicle runs out of risk as soon as possible. DQN is trained for the scenario where a vehicle is encountered with a pedestrian crossing the urban road. Experiments show that the control agent exhibits desirable control behavior and avoids collision without any mistake in various uncertain environments.", "histories": [["v1", "Wed, 8 Feb 2017 06:51:33 GMT  (800kb)", "http://arxiv.org/abs/1702.02302v1", null], ["v2", "Mon, 24 Apr 2017 12:43:36 GMT  (745kb)", "http://arxiv.org/abs/1702.02302v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hyunmin chae", "chang mook kang", "byeoungdo kim", "jaekyum kim", "chung choo chung", "jun won choi"], "accepted": false, "id": "1702.02302"}, "pdf": {"name": "1702.02302.pdf", "metadata": {"source": "CRF", "title": "Autonomous Braking System via Deep Reinforcement Learning", "authors": ["Hyunmin Chae", "Chang Mook Kang", "ByeoungDo Kim", "Jaekyum Kim", "Jun Won Choi"], "emails": ["hmchae@spo.hanyang.ac.kr,", "kcm0728@hanyang.ac.kr,", "jkkim}@spo.hanyang.ac.kr)", "cchung@hanyang.ac.kr)", "junwchoi@hanyang.ac.kr)"], "sections": [{"heading": null, "text": "In fact, in recent years, the number of those born in the United States has more than tripled in the last ten years, many times over, \"he told the Deutsche Presse-Agentur in an interview.\" I don't think we will be able to put the world in order, \"he told the Deutsche Presse-Agentur.\" I don't think we will be able to put the world in order, \"he said,\" but I think we will be able to put the world in order. \""}, {"heading": "II. SYSTEM DESCRIPTION", "text": "In this section, we describe the overall structure of the autonomous braking system. First, we define the possible scenarios for autonomous braking and explain the detailed functioning of the proposed system."}, {"heading": "A. Scenarios", "text": "One of the factors that hinder safe driving in autonomous driving is the risk of accidents caused by nearby objects, such as pedestrians. Many accidents could happen if the vehicle does not recognise a pedestrian or does not stop in front of him when a pedestrian crosses the road. Therefore, to avoid accidents, we need an intelligent autonomous braking system that recognises the threat that can potentially lead to accidents and applies appropriate braking measures to stop the vehicle in front of the obstacle. However, there are various levels of uncertainty that make it difficult to design the autonomous braking system, such as the initial speed of the vehicle, the position of the pedestrian, the speed of the pedestrian, the crossing of the direction of the pedestrian, the measurement of the pedestrian, the condition of the road Even if a pedestrian is accurately detected, it is difficult to know when it may become a threat to the vehicle, so the braking strategy should change for different situations."}, {"heading": "B. Autonomous Braking System", "text": "The detailed functioning of the proposed autonomous braking system is shown in Fig. 4. The vehicle moves at velocity and in position (vehposx, vehposy).Once a pedestrian is detected, the autonomous braking system receives the relative position of the pedestrian, i.e. (pedposx \u2212 vehposx, pedposy \u2212 vehposy) from the sensors where (pedposx, pedposy) the location of the pedestrian is. Considering the velocity of the vehicle and the relative position (pedposx \u2212 vehposx, pedposy \u2212 vehposy), the vehicle decides whether it brakes at each time step. The interval between the successive time steps is specified by \u0445T. Considering only two braking operations; no braking ointments and no braking. We can include other braking operations with more refined steps, e.g. strong, medium, weak braking operations, which are not taken into account in this work."}, {"heading": "III. DEEP REINFORCEMENT LEARNING FOR AUTONOMOUS BRAKING SYSTEM", "text": "In this section, we present the details of the proposed DRL-based autonomous braking system. First, we present the structure of the DQN and explain the architecture and reward function with which the DQN is trained."}, {"heading": "A. Structure of DRL", "text": "Our system follows the basic RL structure. The agent performs an action in the given state St under the policy \u03c0. The agent receives the state as feedback from the environment and gets the reward rt for the action taken. The state feedback that the agent takes from the sensors consists of the speed of the vehicle VVEH and the relative position to the pedestrian (pedposx \u2212 vehposx, pedposy \u2212 vehposy) for the past n time steps. Possible action that the agent can choose is either slowing down or maintaining the current speed ointment. Within this relationship between the agent and the environment, the goal of our proposed autonomous braking system is to maximize the accumulated reward called \"value function\" that will be obtained in the future within an episode. With the help of computer simulations, the agent learns from the interaction with the environment episode by episode. An episode begins when a pedestrian is detected. Note that the starting position of the pedestrian and the initial speed of the vehicle."}, {"heading": "B. Deep Q-Network", "text": "Q-Learning is one of the popular RL methods that efficiently searches for the optimal policy [13]. Essentially, the Q-value function q\u03c0 (s, a) is defined, with the Q-value function being the expected sum of future rewards, indicating how good the action a is under the agent s policy. The contribution to the Q-value function decays exponentially with the discount factor g and action a. The Q-value function is the expected sum of future rewards, indicating how good the action a is under the policy of the agent p. (2) The contribution to the Q-value function decays exponentially with the discounting factor g for rewards with distant future states. For the given Q-value function, the greedy policy is achieved when St-value is reached. (s) = argmax a qp-value (s, a) qp-function. (2) It can be shown that the following policy should be considered (2):"}, {"heading": "C. Reward Function", "text": "Unlike video games, the reward should be adequately defined by a system designer in the autonomous braking system. As already mentioned, the reward function determines the behavior of the brake control. Therefore, in order to ensure the reliability of the brake control, it is crucial to use the correctly defined reward function. In our model, there is a conflict between two important objectives of brake control; 1) collision should be avoided no matter what and 2) the vehicle should get out of the risky situation quickly. If it is unbalanced, the agent becomes either too conservative or reckless. Therefore, we should use the reward function that balances two contradictory objectives. Taking this into account, we suggest the following reward function functionrt = \u03b1vt \u2212 \u03b2vt1 (St = Bump) + 0 (St = Pass) (8) \u03b1, \u03b2 > 0where vt is the velocity of the vehicle at the time step t and 1 (x = y has) the statement inside is true and else."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we evaluate the performance of the proposed autonomous braking system using computer simulations."}, {"heading": "A. Simulation Setup", "text": "In simulations, we used the commercial software PreScan, which models the vehicle dynamics in real time [16]. We created the environment to train the DQN by simulating the random behavior of the pedestrian. Suppose that the location information obtained from the sensors is given to the agent. We added slight measurement noises to the relative position of the pedestrian. In each episode, the starting position of the vehicle is set to (0, 0) with equal probability. The initial speed of the vehicle is evenly distributed between vvehmin = 5 m / s and vvehmax = 10 m / s. First, the pedestrian is either on the far side or near the vehicle with equal probability. The pedestrian crosses the road as soon as the vehicle passes the pedestrian crossing. The pedestrian crossing is also evenly distributed on the pedestrian crossing (between 20 m and 30 m)."}, {"heading": "B. Training of DQN", "text": "We trained the DQN on 100,000 episodes. The network used in the DQN consists of five fully connected layers, each with 40, 30, 15, 5 and 2 nodes. We used the RMSProp [15] algorithm to minimize loss with the learning rate \u00b5 = 0.0005 and set the batch size to 32. \u2022 State buffer size: n = 5 \u2022 Network architecture: five fully connected layers \u2022 Non-linear function: leaky ReLU [14] \u2022 Number of nodes for each layer: [15 (input layer), 40.30, 15, 5, 2 (output layer)] \u2022 RMSProp optimizer with learning rate 0.0005 \u2022 Playback memory size: 20,000 \u2022 Batch size: 32 \u2022 Reward function: \u03b1 = 1, \u03b2 = 80, \u03b3 = 20Fig. 5 shows the plot of the total cumulative award (i.e. value function) achieved during the training."}, {"heading": "C. Test Results", "text": "The simulation parameters vary according to the distribution given in section IVA. We evaluate the performance by counting the number of episodes that each end with the state of bump, stop and pass. Table I provides the results for each test scenario. We observe that the agent successfully avoids a collision in all cases in scenarios 1 and 2 (i.e. crossing scenarios). In scenarios 3 and 4 (i.e. standing scenarios) the agent passes the pedestrian only without unnecessary stopovers. Fig. 6 shows the drawing of the actual position of the stopped vehicle and that of the pedestrian in 1,000 attempts in scenario 1. We see that in most cases the vehicle stops about 10 meters in front of the pedestrian. This seems to be reasonably safe considering the safety radius of l = 6 m. Note that this distance can be adjusted by changing the reward parameters and that the proposed braking system has overall reliable results."}, {"heading": "V. CONCLUSIONS", "text": "The proposed system learns an intelligent way of braking control from the experience of the simulated environment. We designed the autonomous braking systems using the DQN method with carefully designed reward functions. We demonstrate through computer simulations that the proposed autonomous braking system exhibits desirable and consistent braking control behavior for various scenarios in which the behavior of the pedestrian is unsafe. A limitation of the proposed system is the inability to train the agent in the real environment, as it learns from trials and mistakes. However, this problem will be solved by developing a more precise and realistic simulator. It would be interesting to find a reward function that can lead to a human-like braking effect through inverse amplification techniques. This topic is left to future research."}], "references": [{"title": "Survey of pedestrian detection for advanced driver assistance systems", "author": ["D. Geronimo", "A.M. Lopez", "A.D. Sappa", "T. Graf"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 32, no. 7, pp. 1239\u20131258, July 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Research advances in intelligent collision avodiance and adaptive cruise control,", "author": ["A. Vahidi", "A. Eskandarian"], "venue": "IEEE Trans. Intelligent Transportation Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Evaluation of automotive forward collision worning and collision avoidance algorithms,", "author": ["K. Lee", "H. Peng"], "venue": "Vehicle System Dynamics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Pedestrian collision avodiance systems: a survey of computer vision based recent studies,", "author": ["T. Gandhi", "M.M. Trivedi"], "venue": "IEEE Trans. Intelligent Transportation Systems Conference,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "An empricial evaluatio of deep learning on highway driving,", "author": ["B. Huval", "T. Wang", "S. Tandon", "J. Kiske", "W. Song", "J. Pazhayampallil", "P M. Andrilukam", "Rajpurkar", "T. Migimatsu", "R. Cheng-Yue", "F. Mujica", "A. Coates", "A.Y. Ng"], "venue": "arXiv preprint,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Deep convolutional neural networks for pedestrian detection,", "author": ["D. Tome", "F. Monti", "L. Baroffio", "L. Bondi", "M. Tagliasacchi", "S. Tubaro"], "venue": "Signal Processing: Image Communication,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Neural network based lane change trajectory prediction in autonomous vehicles,", "author": ["R.S. Tomar", "S. Verma"], "venue": "Transactions on computational science XIII,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "DeepDriving: learning affordance for direct perception in autonomous driving,", "author": ["C. Chen", "A. Seff", "A. Kornhauser", "J. Xiao"], "venue": "Proceedings of the IEEE International Conference on Computer Vision(ICCV),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Evolving deep unsupervised convolutional networks for vision-based reinforcement learning,", "author": ["J. Koutnik", "J. Schmidhuber", "F. Gomez"], "venue": "Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Cooperative adaptive cruise control: a reinforcement learning approch,", "author": ["C. Desjardins", "B. Chaib-draa"], "venue": "IEEE Trans. Intelligent Transportation Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Reinforcement learning: an introduction,", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Rectified linear units improve restricted boltzmann machines,", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th Internation Conference on Machine Learning(ICML),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Lecture 6.5-rmsprop: Devide the gradient by a running average of its recent magnitude,", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural network for machine learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Recently, several safety systems including collision avoidance, pedestrian detection, and front collision warning (FCW) have been proposed to enhance the safety of the autonomous vehicle [1]\u2013[4].", "startOffset": 187, "endOffset": 190}, {"referenceID": 3, "context": "Recently, several safety systems including collision avoidance, pedestrian detection, and front collision warning (FCW) have been proposed to enhance the safety of the autonomous vehicle [1]\u2013[4].", "startOffset": 191, "endOffset": 194}, {"referenceID": 4, "context": "The DNN has been applied to autonomous driving from camera-based perception [5]\u2013[7] to end-to-end approach which learns mapping from the sensing to the control [8], [9].", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "The DNN has been applied to autonomous driving from camera-based perception [5]\u2013[7] to end-to-end approach which learns mapping from the sensing to the control [8], [9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": "The DNN has been applied to autonomous driving from camera-based perception [5]\u2013[7] to end-to-end approach which learns mapping from the sensing to the control [8], [9].", "startOffset": 165, "endOffset": 168}, {"referenceID": 8, "context": "Recently, the DRL is applied to design control systems for autonomous driving in [11], [12].", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "Recently, the DRL is applied to design control systems for autonomous driving in [11], [12].", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "Q-learning is one of the popular RL methods which searches for the optimal policy in an efficient way [13].", "startOffset": 102, "endOffset": 106}, {"referenceID": 10, "context": "One can show that for the policy provided in (2), the following Bellman equation should hold [13];", "startOffset": 93, "endOffset": 97}, {"referenceID": 12, "context": "We used the RMSProp [15] algorithm to minimize the loss with learning rate \u03bc = 0.", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "\u2022 State buffer size: n = 5 \u2022 Network architecture: five fully-connected layers \u2022 Nonlinear function: leaky ReLU [14] \u2022 Number of nodes for each layers : [15(Input layer), 40, 30, 15, 5, 2(Output layer)] \u2022 RMSProp optimizer with learning rate 0.", "startOffset": 112, "endOffset": 116}], "year": 2017, "abstractText": "In this paper, we propose a new autonomous braking system based on deep reinforcement learning. The proposed autonomous braking system automatically decides whether to apply the brake at each time step when confronting the risk of collision using the information on the obstacle obtained by the sensors. The problem of designing brake control is formulated as searching for the optimal policy in Markov decision process (MDP) model where the state is given by the relative position of the obstacle and the vehicle\u2019s speed, and the action space is defined as whether brake is stepped or not. The policy used for brake control is learned through computer simulations using the deep reinforcement learning method called deep Q-network (DQN). In order to derive desirable braking policy, we propose the reward function which balances the damage imposed to the obstacle in case of accident and the reward achieved when the vehicle runs out of risk as soon as possible. DQN is trained for the scenario where a vehicle is encountered with a pedestrian crossing the urban road. Experiments show that the control agent exhibits desirable control behavior and avoids collision without any mistake in various uncertain environments.", "creator": "LaTeX with hyperref package"}}}