{"id": "1702.02426", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Data Selection Strategies for Multi-Domain Sentiment Analysis", "abstract": "Domain adaptation is important in sentiment analysis as sentiment-indicating words vary between domains. Recently, multi-domain adaptation has become more pervasive, but existing approaches train on all available source domains including dissimilar ones. However, the selection of appropriate training data is as important as the choice of algorithm. We undertake -- to our knowledge for the first time -- an extensive study of domain similarity metrics in the context of sentiment analysis and propose novel representations, metrics, and a new scope for data selection. We evaluate the proposed methods on two large-scale multi-domain adaptation settings on tweets and reviews and demonstrate that they consistently outperform strong random and balanced baselines, while our proposed selection strategy outperforms instance-level selection and yields the best score on a large reviews corpus.", "histories": [["v1", "Wed, 8 Feb 2017 13:49:59 GMT  (3701kb,D)", "http://arxiv.org/abs/1702.02426v1", "10 pages, 2 figures, 4 tables"]], "COMMENTS": "10 pages, 2 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sebastian ruder", "parsa ghaffari", "john g breslin"], "accepted": false, "id": "1702.02426"}, "pdf": {"name": "1702.02426.pdf", "metadata": {"source": "CRF", "title": "Data Selection Strategies for Multi-Domain Sentiment Analysis", "authors": ["Sebastian Ruder", "Parsa Ghaffari", "John G. Breslin"], "emails": ["sebastian.ruder@insight-centre.org", "john.breslin@insight-centre.org", "sebastian@aylien.com", "parsa@aylien.com"], "sections": [{"heading": "1 Introduction", "text": "When two domains are similar, such as electronics and kitchen appliances, the adaptation is successful; in contrast, the transfer is less productive for different domains, such as books and electronic ratings (Blitzer et al., 2007).Consequently, recent research considers the more realistic setting of multi-domain customization, where multiple source domains are available and the goal is to maximize performance on the target domain. However, such approaches will be made available at a later stage. Selection of samples from different domains that are not helpful for predicting in the actual target domain. To mitigate this, existing approaches (Zhou et al., 2016) will measure domain similarity to weigh predictions of separate source domain models. However, Ruder et al al al al al al al al al al al al al al al al al (2017) show that forming a model on all source data is generally more effective.Even within a domain, such as Jiang et, significantly depends on the adaptation."}, {"heading": "2 Related work", "text": "Domain Customization. Domain Customization has a long history of research: Blitzer et al. (2006) proposed a structural matching learning algorithm. Daum\u00e9 III (2007) introduced a core function that maps source and target domain data into a space that promotes domain similarity, while Pan et al. (2010) proposed a spectral alignment algorithm to align domain-specific words into meaningful clusters. Glorot et al. (2011) looked at stacked denoising encoders to extract meaningful representations that Chen et al. (2012) expanded to a marginalized version to address their high computing costs. Zhuang et al. (2015) suggested using deep auto-encoders for transfer learning, while Zhou et al. (2016) transferred the source examples to target domain selection and vice versa."}, {"heading": "3 Data selection strategies", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Representations", "text": "The relative frequency of distributions of terms in the vocabulary has been successfully used to measure similarities in relation to a target domain (Plank and van Noord, 2011; Wu and Huang, 2016). The underlying assumption is that similar domains have more terms in common than unequal domains. DomainD's term distribution is a vector t-R | V | where ti is the probability of the i-th word in the vocabulary V appearing in D. Term distributions, but only covers superficial occurrence statistics that probably cannot express a more nuanced spectrum of domain similarity. Word embeddings have been used to capture finer-grained terms of similarity between both words (Mikolov et al, 2013) and sentences (Wieting et al, 2016)."}, {"heading": "3.2 Domain similarity metrics", "text": "Jensen-Shannon divergence is one of the most widely used measures of domain similarity (Remus, 2012) and has been shown to exceed other similarity metrics (Plank and van Noord, 2011).JensenShannon divergence is a smoothed, symmetrical variant of KL divergence. Jensen-Shannon divergence between two different probability distributions P and Q can be written as DJS (P | | Q) = 1 2 [DKL (P | M) + DKL (Q | M)] divergence between two target distributions P and Q."}, {"heading": "3.3 Data selection level", "text": "Finally, domain similarity can be measured at different levels, as can be seen in Figure 1: at the level of the entire domain (1a); at the level of a single training example (1b); or at a level that mediates between the two extremes and introduces diversity in data selection (1c).Domain level. Often, as in the case of product reviews, domains are clearly delimited and documents in different domains are clearly distinguishable from each other. We can therefore adopt these human-assigned labels and calculate the similarity of each source domain in relation to the target domain. We then pitch n training examples only from the most similar source domain. In other scenarios, such as on the Web, there is no clear distinction between different domains (Ruder et al., 2016). In this case, the similarity in relation to the target domain is too great."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets and tasks", "text": "Since the challenges for data selection methods vary according to the type of domains to which they are applied, we evaluate our approaches algorithm 1 Instance Subset select 1: procedure SUBSETSELECT (s, n, m, X) 2: numiter \u2190 n / s 3: trainset \u2190 [] 4: for i in range (numiter) do 5: randsubsets \u2190 sample m subsets ofsize s from X 6: simscores \u2190 compute similarityscore for randsubsets 7: sortedsimscores \u2190 sort simscores 8: bestsubset \u2190 get subset with highestsimilarity score 9: trainset.append m subsets of size s from X 6: examples.remove (bestsubset) 11: return trainseton two large multi-domain sentimentanalyse datasqets, which seek to replicate this challenges.Tweets + reviews of theressimilarity score 9: trainset.append (bestsubset)."}, {"heading": "4.2 Training details", "text": "We remove stopwords and use a vocabulary of the 10,000 most common words in all areas. In line with previous work, we use the raw bags of words pre-processed with tf-idf unigram / bigram functions as input into a linear SVM classifier (Blitzer et al., 2006). We use GloVe vectors (Pennington et al., 2014), which we have prepared for our word embedding on 42B tokens of the Common Crawl corpus7. For auto-encoder representations, we use a denosing auto-encoder with a hidden layer of 1000 dimensions and a masking noise with a masking probability p = 0.8, which we use for 50 epochs with the Adam Optimizer / Redatasolance (Kingma and Ba, 2015)."}, {"heading": "4.3 Comparison methods", "text": "Baselines. We compare using two baselines: a) We sample training examples from all source domains (margins); b) We sample the same number of stratified examples from all source domains (all) Because the review corpus contains different product rating domains, we also include a human-marked baseline (H) in which the model is trained from the domain most closely determined by 5 human annotators using randomly selected data. We have provided them with the names of the rating categories and review samples and commissioned them to classify the three most similar domains for each domain, with consensus being selected as the most similar domain. Our methods. For each representation, we use - for space reasons - the similarity metric most commonly used in connection with it, i.e. we select the term distribution and Jensen-Shannon divergence, word embeddings and cosmic similarities, as well as any combination of cosmic similarities."}, {"heading": "4.4 Results", "text": "For all data sets we give accuracy values and the average of 10 runs. We measure the statistical significance on the basis of the Student's T-test. We deliver the results in Tables 3 and 4."}, {"heading": "4.5 Discussion", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city, in which it is a country, in which it is a country."}, {"heading": "4.6 Guidelines", "text": "Finally, we propose guidelines for using data selection for mood analysis in the wild that may be helpful for NLP practitioners: \u2022 Do not use term distribution-based domain selection with Jensen-Shannon divergence as a simple baseline. \u2022 Do not use subset level selection instead of instance selection, especially for term distributions. \u2022 Do not use proxy A spacing instead of the default similarity metric. \u2022 Do not use pre-formed word embedding for data selection on noisy data. \u2022 Do not use automatic encoder representations for data selection when your record is small. \u2022 Do not use automatic encoder representations on large datasets."}, {"heading": "5 Conclusion", "text": "In this paper, for the first time as far as we know, we have extensively examined domain similarity metrics in the context of sentiment analysis. We have proposed several representations and metrics that have not previously been used for data selection. We have introduced a novel data selection strategy that uses subsets of examples and consistently outperforms instinct selection. Finally, we have evaluated the proposed methods on two large-scale multi-domain adaptation settings in tweets and reviews, showing that our proposed metrics and representations outperform strong random and balanced baselines, while our new selection strategy, based on encoder representations, provides the best value in the review corpus."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Domain adaptation is important in sentiment analysis as sentiment-indicating words vary between domains. Recently, multi-domain adaptation has become more pervasive, but existing approaches train on all available source domains including dissimilar ones. However, the selection of appropriate training data is as important as the choice of algorithm. We undertake \u2013 to our knowledge for the first time \u2013 an extensive study of domain similarity metrics in the context of sentiment analysis and propose novel representations, metrics, and a new scope for data selection. We evaluate the proposed methods on two largescale multi-domain adaptation settings on tweets and reviews and demonstrate that they consistently outperform strong random and balanced baselines, while our proposed selection strategy outperforms instance-level selection and yields the best score on a large reviews corpus. All experiments are available at url_redacted1", "creator": "LaTeX with hyperref package"}}}