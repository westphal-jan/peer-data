{"id": "1702.02453", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Preparing for the Unknown: Learning a Universal Policy with Online System Identification", "abstract": "We present a new method of learning control policies that successfully operate under unknown dynamic models. We create such policies by leveraging a large number of training examples that are generated using a physical simulator. Our system is made of two components: a Universal Policy (UP) and a function for Online System Identification (OSI). We describe our control policy as universal because it is trained over a wide array of dynamic models. These variations in the dynamic model may include differences in mass and inertia of the robots' components, variable friction coefficients, or unknown mass of an object to be manipulated. By training the Universal Policy with this variation, the control policy is prepared for a wider array of possible conditions when executed in an unknown environment. The second part of our system uses the recent state and action history of the system to predict the dynamics model parameters mu. The value of mu from the Online System Identification is then provided as input to the control policy (along with the system state). Together, UP-OSI is a robust control policy that can be used across a wide range of dynamic models, and that is also responsive to sudden changes in the environment. We have evaluated the performance of this system on a variety of tasks, including the problem of cart-pole swing-up, the double inverted pendulum, locomotion of a hopper, and block-throwing of a manipulator. UP-OSI is effective at these tasks across a wide range of dynamic models. Moreover, when tested with dynamic models outside of the training range, UP-OSI outperforms the Universal Policy alone, even when UP is given the actual value of the model dynamics. In addition to the benefits of creating more robust controllers, UP-OSI also holds out promise of narrowing the Reality Gap between simulated and real physical systems.", "histories": [["v1", "Wed, 8 Feb 2017 14:53:45 GMT  (1914kb,D)", "http://arxiv.org/abs/1702.02453v1", null], ["v2", "Wed, 22 Feb 2017 17:08:31 GMT  (3314kb,D)", "http://arxiv.org/abs/1702.02453v2", null], ["v3", "Mon, 15 May 2017 13:37:58 GMT  (3301kb,D)", "http://arxiv.org/abs/1702.02453v3", "Accepted as a conference paper at RSS 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.RO cs.SY", "authors": ["wenhao yu", "jie tan", "c karen liu", "greg turk"], "accepted": false, "id": "1702.02453"}, "pdf": {"name": "1702.02453.pdf", "metadata": {"source": "CRF", "title": "Preparing for the Unknown: Learning a Universal Policy with Online System Identification", "authors": ["Wenhao Yu", "C. Karen Liu", "Greg Turk"], "emails": ["wyu68@gatech.edu,", "karenliu@cc.gatech.edu,", "turk@cc.gatech.edu"], "sections": [{"heading": null, "text": "There are two general approaches to controlling physical phenomena for real operations, which may be a reasonable control system adopted by researchers and practitioners in computer animation. However, the success of simulation of highly dynamic movements in computer animation is not fully transferable to the robot world. However, the discrepancy between what can be achieved in simulation and what is referred to in the real world as \"reality gap\" limitations is referred to in the real world as \"reality gap.\" The absence of uncertainty and latency in the sensors and actuators and other unmodelled factors causing a long list of possible factors leading to the reality gap, such as simplified dynamic models of dynamic models, inaccurate model parameter parameters, approximate hardware limitations, the lack of uncertainty and latency in the sensors, and other unmodelled factors leading to transferring the certainty of the transferability of simulation to the accuracy of the world, the ability to transferring the accuracy of the knowledge into the accuracy of the simulation,"}, {"heading": "II. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Deep Reinforcement Learning", "text": "In recent years, researchers have been using deep reinforcement learning to train highly dynamic motor skills in simulated environments with high-dimensional states and action spaces [23, 29, 28, 18, 20, 10]. For example, Schulman et al. demonstrated learning whole-body humanoid walking and standing with mere feedback from the reward function using Trust Region Policy Optimization (TRPO) [28] and Generalized Advantage Estimation (GAE) [29]. Lillicrap et al. [18] extended their work of Deep Q-Learning [19] and Deterministic Policy Gradient (DPG) [30] to learn robotic motor skills such as hopping, reaching and 2D walking directly from pixel input. These methods usually require a large amount of interaction time between the agent and the environment, which presents a significant challenge for both the robot and the experimenter when using them directly to perform a robotic task."}, {"heading": "B. Transfer Learning in Reinforcement Learning", "text": "In practice, the identification of the system is often linked to the control of the real world in order to minimize the number of real experiments required. [3] Some widely used models, such as the linear processes [8], and the differentiated equations have proved effective."}, {"heading": "C. Learning Policy in Unknown Environment", "text": "Our work also relates to the learning of a control policy in an environment with unknown parameters, i.e. partially observable Markov decision-making process (POMDP). An example of such a work is the idea of event learning proposed by Szita et al. [32]. They showed that their method, in combination with a controller for static and dynamic states (SDS), works better in a dynamically changing environment than standard methods such as SARSA. In contrast to their work, we explicitly incorporate the model parameters as input into the control policy. Using this additional information as input, the control policy has the potential to achieve better performance for a wider range of model parameters. Based on the method of the Deterministic Policy Gradient [30], Heess et al. [12] proposed a method that represents a control policy as a recurrent network. They demonstrated control strategies that identify task-related signals and store them as they are represented in the observable data sequences, rather than the target position of the robot or the mass control."}, {"heading": "III. METHODS", "text": "Our algorithm consists of two components, a universal policy (UP) and an online system identification model (OSI), shown in Figure 1. First, we formulate an amplification problem to learn a universal policy (UP), \u03c0: (x, \u00b5) 7 \u2192 u, for a space of dynamic models, xt + 1 = f\u00b5 (xt, ut), parameterized by the dynamic model parameters \u00b5. Unlike a conventional control policy, which maps a state vector x to a control vector u, UP takes both the state and the dynamic model parameters (i.e. \u00b5) as input and outputs a control vector. Second, we formulate a verified learning problem to explore an online system identification model (OSI)."}, {"heading": "A. Learning Universal Policy", "text": "Our goal is to learn a control policy that can be generalized to a parametrized space of dynamic models. Many existing methods [7, 31] apply an ensemble approach by learning individual control policies and consolidating them into a regression model. Our initial experiment with the ensemble approach showed that for many dynamic tasks, a small change in the model parameter sometimes requires a drastically different control policy to succeed in the task at hand. Adapting a regression model to this uneven landscape of strategies often leads to poor generalizability. In this thesis, we found that it is possible to train a large neural network directly to represent a universal control policy (x, \u00b5), for a space of dynamic models parametrized by \u00b5. With a powerful policy optimization algorithm and sufficient data, the universal policy can achieve high rewards throughout the space \u00b5, with a political performance comparable to that of the optimized state of the governance package we have trained."}, {"heading": "B. Learning Online System Identification Model", "text": "Even with the ability to perform control under different dynamic models, UP can only succeed if precise model parameters are specified, and this information is typically not readily available. We suggest to learn an online system identification model (OSI) because it (xt \u2212 h: t, ut \u2212 h: t \u2212 1) 7 \u2192 \u00b5, which continuously identifies the correct model parameters \u00b5 for UP if there is a short recent history of states and actions. The training process can be formulated as a supervised learning problem where input is a course of action and output is the model parameters under which the input rollout is generated."}, {"heading": "IV. EVALUATION", "text": "We evaluate UP-OSI based on four dynamic motor control problems. In each example, the control policy does not know the true model parameters in advance and relies on OSI to identify the parameters during execution. We vary different model parameters, such as mass, inertia, friction coefficient or task-related parameters, to show that UP-OSI can successfully perform all motor skills under unknown dynamic models. We compare the performance of UP-OSI against the performance of the condition, UP-true, which uses the true model parameters. UPtrue's performance can be considered an informal upper limit for UP-OSI. Furthermore, we show that UP-OSI can also function well when the model parameters of the test environment are outside the training area. This result is particularly interesting when we compare with UP-true."}, {"heading": "A. Double inverted pendulum with unknown center of mass", "text": "We start with a classic problem of motor control: balancing a double reverse pendulum. We define the reward function as, r (x) = \u2212 k1 (\u03c31 + \u03c32) 2 \u2212 k2 | pcart | + 10, where \u03c31 and \u03c32 are angles of the two poles from the upright configuration, pcart is the position of the basket and k1 and k2 are the corresponding weights of the two terms. We normalize the angles to be in [0, \u03c0] and use k1 = 10.0, k2 = 1.0 in our experiment. The length of the two poles is both 0.5. We finish the simulation if | pcart \u2265 5 | or (\u03c31 + \u03c32) have a value of 0.5\u03c02. The unknown model parameter for this problem is the center of gravity of the mass of the lower pole, which has an unknown distance from the physical axis (\u00b5, 0.2\u00b5), from the geometric center. To ensure that the control policies would have to apply different strategies to balance the problem."}, {"heading": "B. Manipulator with unknown object mass", "text": "In this example, we train a robotic arm to grab a block and throw it at a certain height, but not beyond it. Initially, the arm points down and the block is in the air near the gripper of the arm. Similar motor skills can be observed when serving a tennis ball. Condition x includes the joint position q, the joint speed q of the robotic arm and the position of the block block. The reward function is defined as: r (x, u) = \u2212 k1rh \u2212 k2 | u | 2 \u2212 k3 | q \u00b2 | 2 + 35rh = {htarget \u2212 hblock if htarget \u2264 htarget 0, otherwise, where k1 = 10, k2 = 1e \u2212 5, k3 = 1e \u2212 3, htarget = 2m and hblock is the height of the block. We finish the rollout if the box drops below \u2212 0.2 m or if the block is higher than 0.8 m away. By giving reward beyond the target we give the height of the arm to encourage the block to throw into the block at a high speed."}, {"heading": "C. Hopper with unknown friction coefficient", "text": "In this example, we show that our method can be used to identify the coefficient of friction at the contact point online. The task is to move the monopod robot in 2D, the hopper, forward as fast as possible without falling. The reward is defined as r (x, u) = k1x2 (k2), where k1 = 1, k2 = 0.002 is achieved for our experiments. Unknown model parameters are the coefficient of friction with the range \u00b5 [0.3, 1.0]. We record the maximum distance that the hopper travels before the termination criteria are met (the hopper falls or the maximum length of the rollout is reached) instead of the reward value to better illustrate the performance of the hopper. Input to UP includes the common position of the hopper q, the joint velocity q and the coefficient of friction between foot and floor."}, {"heading": "D. Cart-pole swing-up with unknown pole length and unknown attached mass", "text": "In order to solve the classic cart pole swing-up problem, the control policy must learn not only how to balance the pole, but also how to swing it upwards from a straight position. Our experiment makes two modifications to increase the difficulty of the problem. First, we limit the force used by the shopping basket to be within [\u2212 40N, 40N]. As such, the controller must swing the pole back and forth before it rises upwards. We also add an additional mass to the tip of the pole to imitate the weight lifting task (Figure 5 (a)). We use a variant of the reward function suggested by [15]: r\u03c3 = w\u03c32 + v log (\u03c32 + a), with the pole representing the angle of the pole. The first term promotes quick learning of swing-up motion and the second term promotes quick learning of balance. In our experiment, we put a double equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium to a, in our experiment, we put the equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium equilibrium a, a, a, a, a, a, a, a, a, in our experiment, in the equilibrium equili"}, {"heading": "E. Generalization to varying model parameter", "text": "We run the trained UP-OSI for hopper on a track with different coefficients of friction to test its generalisability. We create a track with coefficient of friction \u00b5konst = 0.9 anywhere except in the region between 20m and 30m. We then vary the coefficient of friction \u00b5vary in that region and plot the performance of the controller in terms of \u00b5vary. Figure 4 (d) shows the performance of UP-OSI and UP-true. Note that UP-true is always given the coefficient of friction as if it had a perfect contact friction sensor at the foot, with UP-OSI having to identify this information based on the recent history of movement. Results show that UP-OSI can achieve a comparable and sometimes better performance than UP-true. We also test the performance of UP with fixed input \u00b5 = 0.9, i.e. the hopper is aware of the change in the coefficient of friction, which is not apparent in the blue success shown in the graph 4."}, {"heading": "F. Generalization beyond training range", "text": "Another way to assess the generalisability of the policy is to test it using model parameters not seen during the training phase. We couple these two unknown parameters linearly, so that at a pollen length of 0.8m the attached mass is 1.0kg and at a pollen length of 1.4m the attached mass is 1.9kg. This is 100% above the original training range ([0.2m, 0.8m] and [0.1kg, 1.0kg]). We test both UP-true and UP-OSI with this extended range, which is shown in Figure 5 (d).The result shows that UPOSI can work for a wide range of unknown pollen and attached mass using only position information as input. Interestingly, UP-OSI may perform significantly better than UP-true in this range."}, {"heading": "V. DISCUSSION", "text": "Although we have shown that OSI can work well for model parameters in R4 (the basket pole example), a more rigorous analysis is needed to assess the sample efficiency of UP-OSI for high-dimensional model parameters. However, a theoretical upper limit and convergence conditions are not established in this work. Furthermore, our current implementation assumes that both the policy and the dynamic model are deterministic, but UPOSI can easily be extended to a stochastic formulation, which focuses primarily on identifying model parameters that have nothing to do with uncertainty."}, {"heading": "VI. CONCLUSION", "text": "A key aspect of these strategies is that they are robust under a variety of dynamic models and actually determine the dynamic model parameters on-the-fly. This approach was developed with the aim of developing control guidelines for real-world tasks by performing a comprehensive offline training with physical simulations and in-depth reinforcement learning. Although we have yet to apply such strategies to real-world robots, their performance offers several promising clues. First, the UPOSI control guidelines provide almost the same performance as the UP-real baseline, while they have to derive the unknown dynamic models alone. Second, UP-OSI can perform better than Universal Politics alone when the dynamic model is outside the training area. Finally, such control guidelines can be adapted to changing environments such as moving across a surface with different friction at each step of the time by constantly evaluating the dynamic model."}], "references": [{"title": "Exploration and Apprenticeship Learning in Reinforcement Learning", "author": ["Pieter Abbeel", "Andrew Y. Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Using Inaccurate Models in Reinforcement Learning", "author": ["Pieter Abbeel", "Morgan Quigley", "Andrew Y. Ng"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Nonlinear System Identification Using Coevolution of Models and Tests", "author": ["Josh C. Bongard", "Hod Lipson"], "venue": "IEEE Trans. Evolutionary Computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model", "author": ["Paul Christiano", "Zain Shah", "Igor Mordatch", "Jonas Schneider", "Trevor Blackwell", "Joshua Tobin", "Pieter Abbeel", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1610.03518,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Learning parameterized skills", "author": ["Bruno Da Silva", "George Konidaris", "Andrew Barto"], "venue": "arXiv preprint arXiv:1206.6398,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "PILCO: A model-based and data-efficient approach to policy search", "author": ["Marc Deisenroth", "Carl E Rasmussen"], "venue": "In Proceedings of the 28th International Conference on machine learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "System identification without lennart ljung: what would have been different", "author": ["Michel Gevers"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Q-Prop: Sample- Efficient Policy Gradient with An Off-Policy Critic", "author": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E Turner", "Sergey Levine"], "venue": "arXiv preprint arXiv:1611.02247,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Reducing Hardware Experiments for Model Learning and Policy Optimization", "author": ["Sehoon Ha", "Katsu Yamane"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Memory-based control with recurrent neural networks", "author": ["Nicolas Heess", "Jonathan J Hunt", "Timothy P Lillicrap", "David Silver"], "venue": "arXiv preprint arXiv:1512.04455,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "3D Simulation for Robot Arm Control with Deep Q-Learning", "author": ["Stephen James", "Edward Johns"], "venue": "arXiv preprint arXiv:1609.03759,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Crossing the reality gap in evolutionary robotics by promoting transferable controllers", "author": ["Sylvain Koos", "Jean-Baptiste Mouret", "St\u00e9phane Doncieux"], "venue": "In Genetic and Evolutionary Computation Conference. ACM,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Learning contact-rich manipulation skills with guided policy search", "author": ["Sergey Levine", "Nolan Wagener", "Pieter Abbeel"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "End-to-End Training of Deep Visuomotor Policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection", "author": ["Sergey Levine", "Peter Pastor", "Alex Krizhevsky", "Deirdre Quillen"], "venue": "CoRR, abs/1603.02199,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Ensemble-CIO: Full-body dynamic motion planning that transfers to physical humanoids", "author": ["Igor Mordatch", "Kendall Lowrey", "Emanuel Todorov"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Evolutionary Robotics: The Biology, Intelligence, and Technology", "author": ["S. Nolfi", "D. Floreano"], "venue": "MIT Press (Cambridge,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2000}, {"title": "Terrain-Adaptive Locomotion Skills Using Deep Reinforcement Learning", "author": ["Xue Bin Peng", "Glen Berseth", "Michiel van de Panne"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Supersizing selfsupervision: Learning to grasp from 50k tries and 700 robot hours", "author": ["Lerrel Pinto", "Abhinav Gupta"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Deep learning helicopter dynamics models", "author": ["Ali Punjani", "Pieter Abbeel"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Agnostic system identification for model-based reinforcement learning", "author": ["Stephane Ross", "J. Andrew Bagnell"], "venue": "In In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["Andrei A Rusu", "Matej Vecerik", "Thomas Roth\u00f6rl", "Nicolas Heess", "Razvan Pascanu", "Raia Hadsell"], "venue": "arXiv preprint arXiv:1610.04286,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel"], "venue": "CoRR, abs/1502.05477,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Deterministic Policy Gradient Algorithms", "author": ["David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin A. Riedmiller"], "venue": "In ICML,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Learning compact parameterized skills with a single regression", "author": ["Freek Stulp", "Gennaro Raiola", "Antoine Hoarau", "Serena Ivaldi", "Olivier Sigaud"], "venue": "In Humanoid Robots (Humanoids),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "\u03b5- MDPs: Learning in varying environments", "author": ["Istv\u00e1n Szita", "B\u00e1lint Tak\u00e1cs", "Andr\u00e1s L\u00f6rincz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2002}, {"title": "Back-to-Reality: Crossing the Reality Gap in Evolutionary Robotics", "author": ["Juan Cristobal Zagal", "Javier Ruiz-del-Solar", "Paul Vallejos"], "venue": "IAV", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}], "referenceMentions": [{"referenceID": 19, "context": "The discrepancy between what can be achieved in simulation and that in real world is referred as the \u201cReality Gap\u201d in the Evolutionary Robotics community [22, 14].", "startOffset": 154, "endOffset": 162}, {"referenceID": 11, "context": "The discrepancy between what can be achieved in simulation and that in real world is referred as the \u201cReality Gap\u201d in the Evolutionary Robotics community [22, 14].", "startOffset": 154, "endOffset": 162}, {"referenceID": 20, "context": "In recent years, researchers have used deep reinforcement learning to train highly dynamic motor skills in simulated environments that have high-dimensional state and action spaces [23, 29, 28, 18, 20, 10].", "startOffset": 181, "endOffset": 205}, {"referenceID": 26, "context": "In recent years, researchers have used deep reinforcement learning to train highly dynamic motor skills in simulated environments that have high-dimensional state and action spaces [23, 29, 28, 18, 20, 10].", "startOffset": 181, "endOffset": 205}, {"referenceID": 25, "context": "In recent years, researchers have used deep reinforcement learning to train highly dynamic motor skills in simulated environments that have high-dimensional state and action spaces [23, 29, 28, 18, 20, 10].", "startOffset": 181, "endOffset": 205}, {"referenceID": 15, "context": "In recent years, researchers have used deep reinforcement learning to train highly dynamic motor skills in simulated environments that have high-dimensional state and action spaces [23, 29, 28, 18, 20, 10].", "startOffset": 181, "endOffset": 205}, {"referenceID": 17, "context": "In recent years, researchers have used deep reinforcement learning to train highly dynamic motor skills in simulated environments that have high-dimensional state and action spaces [23, 29, 28, 18, 20, 10].", "startOffset": 181, "endOffset": 205}, {"referenceID": 7, "context": "In recent years, researchers have used deep reinforcement learning to train highly dynamic motor skills in simulated environments that have high-dimensional state and action spaces [23, 29, 28, 18, 20, 10].", "startOffset": 181, "endOffset": 205}, {"referenceID": 25, "context": "using Trust Region Policy Optimization (TRPO) [28] and Generalized Advantage Estimation (GAE) [29].", "startOffset": 46, "endOffset": 50}, {"referenceID": 26, "context": "using Trust Region Policy Optimization (TRPO) [28] and Generalized Advantage Estimation (GAE) [29].", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "[18] extended their work of Deep Q-Learning [19] and Deterministic Policy Gradient (DPG) [30] to learn robotic motor skills such as hopping, reaching and 2D walking directly from pixel input.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] extended their work of Deep Q-Learning [19] and Deterministic Policy Gradient (DPG) [30] to learn robotic motor skills such as hopping, reaching and 2D walking directly from pixel input.", "startOffset": 44, "endOffset": 48}, {"referenceID": 27, "context": "[18] extended their work of Deep Q-Learning [19] and Deterministic Policy Gradient (DPG) [30] to learn robotic motor skills such as hopping, reaching and 2D walking directly from pixel input.", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "robots [16, 24, 17].", "startOffset": 7, "endOffset": 19}, {"referenceID": 21, "context": "robots [16, 24, 17].", "startOffset": 7, "endOffset": 19}, {"referenceID": 14, "context": "robots [16, 24, 17].", "startOffset": 7, "endOffset": 19}, {"referenceID": 14, "context": "While the results are impressive, these methods usually require extensive amount of experimental data [17, 24] or relatively restrictive settings [16].", "startOffset": 102, "endOffset": 110}, {"referenceID": 21, "context": "While the results are impressive, these methods usually require extensive amount of experimental data [17, 24] or relatively restrictive settings [16].", "startOffset": 102, "endOffset": 110}, {"referenceID": 13, "context": "While the results are impressive, these methods usually require extensive amount of experimental data [17, 24] or relatively restrictive settings [16].", "startOffset": 146, "endOffset": 150}, {"referenceID": 0, "context": "In practice, system identification is often interleaved with control policy optimization to minimize the number of required realworld experiments [3, 9, 5].", "startOffset": 146, "endOffset": 155}, {"referenceID": 6, "context": "In practice, system identification is often interleaved with control policy optimization to minimize the number of required realworld experiments [3, 9, 5].", "startOffset": 146, "endOffset": 155}, {"referenceID": 2, "context": "In practice, system identification is often interleaved with control policy optimization to minimize the number of required realworld experiments [3, 9, 5].", "startOffset": 146, "endOffset": 155}, {"referenceID": 0, "context": "Some widely-used models, such as linear models [3], Gaussian processes [8, 11], and differential equations [33, 4], have proven effective for continuous dynamics and control tasks with relatively low action space.", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": "Some widely-used models, such as linear models [3], Gaussian processes [8, 11], and differential equations [33, 4], have proven effective for continuous dynamics and control tasks with relatively low action space.", "startOffset": 71, "endOffset": 78}, {"referenceID": 8, "context": "Some widely-used models, such as linear models [3], Gaussian processes [8, 11], and differential equations [33, 4], have proven effective for continuous dynamics and control tasks with relatively low action space.", "startOffset": 71, "endOffset": 78}, {"referenceID": 30, "context": "Some widely-used models, such as linear models [3], Gaussian processes [8, 11], and differential equations [33, 4], have proven effective for continuous dynamics and control tasks with relatively low action space.", "startOffset": 107, "endOffset": 114}, {"referenceID": 1, "context": "Some widely-used models, such as linear models [3], Gaussian processes [8, 11], and differential equations [33, 4], have proven effective for continuous dynamics and control tasks with relatively low action space.", "startOffset": 107, "endOffset": 114}, {"referenceID": 0, "context": "For example, Abbeel and Ng [3] used a time-variant linear function to model the dynamics of a helicopter from the realworld data while learning a control policy to perform inverted autonomous helicopter flight.", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "Deisenroth and Rasmussen [8] trained a Gaussian process model from real-world data to analytically calculate the control policy gradient, significantly reducing the number of samples compared to sampling-based policy gradient estimation.", "startOffset": 25, "endOffset": 28}, {"referenceID": 23, "context": "Ross and Bagnell [26] provided a proof that such iterative processes can converge to an optimal policy, given an accurate dynamic model learning method and a good policy search algorithm.", "startOffset": 17, "endOffset": 21}, {"referenceID": 22, "context": "More recently, deep neural networks have been applied to learn both forward dynamics [25] and inverse dynamics [6] from the real-world data, which can potentially model more complicated dynamics.", "startOffset": 85, "endOffset": 89}, {"referenceID": 3, "context": "More recently, deep neural networks have been applied to learn both forward dynamics [25] and inverse dynamics [6] from the real-world data, which can potentially model more complicated dynamics.", "startOffset": 111, "endOffset": 114}, {"referenceID": 10, "context": "James and Johns [13] demonstrated a simulation-", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "[27] used progressive networks to efficiently learn a manipulation task on a Jaco arm from a policy that was trained in simulation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] who optimized the motion trajectory for an ensemble of dynamic models perturbed from the assumed one.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Based on the Deterministic Policy Gradient method [30], Heess et al.", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "[12] proposed a method that represents a control policy as a recurrent neural network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Many existing methods [7, 31] employ an ensemble approach by learning a discrete set of control policies and consolidating them into one regression model.", "startOffset": 22, "endOffset": 29}, {"referenceID": 28, "context": "Many existing methods [7, 31] employ an ensemble approach by learning a discrete set of control policies and consolidating them into one regression model.", "startOffset": 22, "endOffset": 29}, {"referenceID": 25, "context": "We use the Trust Region Policy Optimization (TRPO) method [28] and show that by simply augmenting the input state with the model parameters \u03bc, TRPO can successfully", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "We use a variant of the reward function suggested by [15]: r\u03c3 = w\u03c3 2 + v log(\u03c3 + a), where \u03c3 is the angle of the pole.", "startOffset": 53, "endOffset": 57}, {"referenceID": 9, "context": "To closely compare with the cart-pole examples in [12], where they control an inverted pendulum with varying pole length using a RNN with the whole history trajectory as input, we train OSI to estimate the velocity of the system, instead of directly giving the true velocity to the policy as part of the state.", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "Figure 5(b) and (c) show that UP-OSI can achieve high reward for a range of unknown pole lengths, similar to the inverted pendulum result shown in [12], but UP-OSI only requires three time steps of history as input.", "startOffset": 147, "endOffset": 151}, {"referenceID": 9, "context": "Indeed, previous work [12] has shown that a recurrent network can learn to control a dynamic system with unknown model parameters.", "startOffset": 22, "endOffset": 26}], "year": 2010, "abstractText": "We present a new method of learning control policies that successfully operate under unknown dynamic models. We create such policies by leveraging a large number of training examples that are generated using a physical simulator. Our system is made of two components: a Universal Policy (UP) and a function for Online System Identification (OSI). We describe our control policy as universal because it is trained over a wide array of dynamic models. These variations in the dynamic model may include differences in mass and inertia of the robots components, variable friction coefficients, or unknown mass of an object to be manipulated. By training the Universal Policy with this variation, the control policy is prepared for a wider array of possible conditions when executed in an unknown environment. The second part of our system uses the recent state and action history of the system to predict the dynamics model parameters \u03bc. The value of \u03bc from the Online System Identification is then provided as input to the control policy (along with the system state). Together, UP-OSI is a robust control policy that can be used across a wide range of dynamic models, and that is also responsive to sudden changes in the environment. We have evaluated the performance of this system on a variety of tasks, including the problem of cart-pole swing-up, the double inverted pendulum, locomotion of a hopper, and block-throwing of a manipulator. UP-OSI is effective at these tasks across a wide range of dynamic models. Moreover, when tested with dynamic models outside of the training range, UP-OSI outperforms the Universal Policy alone, even when UP is given the actual value of the model dynamics. In addition to the benefits of creating more robust controllers, UP-OSI also holds out promise of narrowing the Reality Gap between simulated and real physical systems.", "creator": "LaTeX with hyperref package"}}}