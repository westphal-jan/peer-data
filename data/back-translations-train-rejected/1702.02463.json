{"id": "1702.02463", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Video Frame Synthesis using Deep Voxel Flow", "abstract": "We address the problem of synthesizing new video frames in an existing video, either in-between existing frames (interpolation), or subsequent to them (extrapolation). This problem is challenging because video appearance and motion can be highly complex. Traditional optical-flow-based solutions often fail where flow estimation is challenging, while newer neural-network-based methods that hallucinate pixel values directly often produce blurry results. We combine the advantages of these two methods by training a deep network that learns to synthesize video frames by flowing pixel values from existing ones, which we call deep voxel flow. Our method requires no human supervision, and any video can be used as training data by dropping, and then learning to predict, existing frames. The technique is efficient, and can be applied at any video resolution. We demonstrate that our method produces results that both quantitatively and qualitatively improve upon the state-of-the-art.", "histories": [["v1", "Wed, 8 Feb 2017 15:20:14 GMT  (2024kb,D)", "http://arxiv.org/abs/1702.02463v1", "Project page:this https URL"], ["v2", "Sat, 5 Aug 2017 04:43:44 GMT  (2074kb,D)", "http://arxiv.org/abs/1702.02463v2", "To appear in ICCV 2017 as an oral paper. More details at the project page:this https URL"]], "COMMENTS": "Project page:this https URL", "reviews": [], "SUBJECTS": "cs.CV cs.GR cs.LG", "authors": ["ziwei liu", "raymond a yeh", "xiaoou tang", "yiming liu", "aseem agarwala"], "accepted": false, "id": "1702.02463"}, "pdf": {"name": "1702.02463.pdf", "metadata": {"source": "CRF", "title": "Video Frame Synthesis using Deep Voxel Flow", "authors": ["Ziwei Liu", "Raymond Yeh", "Xiaoou Tang", "Yiming Liu", "Aseem Agarwala"], "emails": ["lz013@ie.cuhk.edu.hk", "xtang@ie.cuhk.edu.hk", "yeh17@illinois.edu", "yimingl@google.com", "aseemaa@google.com"], "sections": [{"heading": "1. Introduction", "text": "In fact, most people who are able to survive on their own are still alive even if they are not able to flourish."}, {"heading": "2. Related Work", "text": "In this sense, the quality of flow-based interpolations is completely independent of the accuracy of the flow, which is often called into question by large and fast movements. Mahajan et al. [18] investigate a variation of the optical flow that calculates the paths in the source images and copies of pixel gradients along the interpolated images, followed by a Poisson reconstruction. Meyer et al. [20] deal with a phase-based approach to interpolation, but the method is limited to smaller movements. Convolutionary neural networks are used to make current and dramatic improvements in image and video recognition."}, {"heading": "3. Our Approach", "text": "The question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of how to deal with the question of"}, {"heading": "3.1. Learning", "text": "For our DVF training, we use the l1-reconstruction loss with spatial and temporal coherence regularizations to reduce visual artefacts. (Total variation (TV) regularizations are used here to force coherence. Since these regularization mechanisms are imposed on the output of the network, they can easily be included in the backpropagation scheme. (3D propagation scheme) Our general target function, which we minimize, is: L = 1 N-0-0-0-0-0-1-1-1-1-1-1-1-1-1-1-1-1-1-0-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-"}, {"heading": "3.2. Multi-scale Flow Fusion", "text": "As explained in Section 3.1, the gradients of the reconstruction error are achieved only by looking at the core support region for each output voxel, which makes it difficult to find large movements that are outside the core. Therefore, we propose a multi-scale deep voxel flow (multi-scale DVF) to better encode both large and small movements. In particular, we have a series of revolutionary encoder decoders HN, HN \u2212 1, \u00b7 \u00b7 \u00b7 \u00b7, H0 that work on video images from coarse scale sN to fine scale s0. Typically, in our experiments we use s2 = 64 \u00b7 64, s1 = 128 \u00b7 128 and s0 = 256 \u00b7 256 \u00b7 256. In each scale k, the subnetwork Hk 3D predicts voxel flow Fk at this resolution. Intuitively, large movements will have a relatively small offset vector Fk in coarse scale sN."}, {"heading": "3.3. Multi-step Prediction", "text": "Our framework can, of course, be extended to multi-level predictions in interpolation or extrapolation. For example, the goal is to predict the next D-frames based on the current L-frames. In this case, target Y becomes a 3D volume (Y-RH-W-D) rather than a 2D frame (Y-RH-W). Similar to Eqn. 4, any output voxel Y-X (x, y, t) can be achieved by performing trilinear interpolation on the input video X according to its projected virtual voxel. We have observed that spatial-time coherence in our output volume is maintained because windings over time layers allow local correlations to be maintained. As multi-level predictions are more difficult, we reduce the learning rate to 0.0005 to maintain stability in training."}, {"heading": "4. Experiments", "text": "In the past, we have always said that we will be able to solve the world's problems, \"he told the Deutsche Presse-Agentur in an interview with\" Welt am Sonntag. \"\" We have not managed to get a grip on the problem, \"he told the Deutsche Presse-Agentur in Berlin.\" We have not managed to get a grip on the problem, but we have managed to solve it. \""}, {"heading": "4.1. Effectiveness of Multi-scale Voxel Flow", "text": "In this section, we will demonstrate the benefits of multi-scale voxel flow (multi-scale VF); we will also examine the results separately along two axes: appearance and motion. To model appearance, we will identify texture regions based on local boundaries. For motion modeling, we will identify (large) motion regions based on the flow maps provided in [23]. Figure 5 (a-b) compares the PSNR performance of the UCF-101 test set with and without multi-scale voxel flow. The multi-scale architecture also allows DVF to handle large movements, as shown in Figure 5 (b). Large movements become small after downsampling, and these motion estimates are merged with higher-resolution estimates at the last layer of our network."}, {"heading": "4.2. Generalization to View Synthesis", "text": "In this section, we show that DVF can be easily generalized to view the synthesis even without retraining. We apply the model trained on UCF-101 directly to the task of view synthesis. The KITTI odometry dataset [9] is used here for evaluation and follows [36]. Table 1 (right) lists the performance comparisons of various methods. Surprisingly, our approach without fine tuning already performs better by 0.164 and 0.135, respectively [28] and [36]. We find that fine tuning the KIITI training set could further reduce the reconstruction error. Note that the KITTI dataset has large camera movements that differ greatly from our original training data. (UCF-101 focuses mainly on human actions.) This observation implies that the voxel flow has good generalization capability and can be used as a universal frame / view synthesizer."}, {"heading": "4.3. Frame Synthesis as Self-Supervision", "text": "In addition to advances in the quality of video interpolation / extrapolation, we show that video frame synthesis can serve as a self-monitoring task for viewing learning, applying the internal representation learned by DVF to unattended sequence estimation and pre-training in action detection. Remember, the 3D voxel flow can be projected into a 2D motion field illustrated in Fig. 2 (e). We quantify the flow stimulation of DVF by comparing the projected 2D motion field with the optical flow field of truth. KITTI Flow 2012 data set [9] is used as a test set. Table 2 (left) shows the average endpoint error (EPE) across all labeled pixels. After fine tuning, the unattended flow generated by DVF maps the traditional methods [3] and performs comparable to some of the monitored deep models [5]."}, {"heading": "4.4. Applications", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "5. Discussion", "text": "In this paper, we propose an end-to-end deep network, deep voxel flow (DVF), for video image synthesis. Our method is capable of copying pixels from existing video images rather than hallucinating them from scratch. On the other hand, our method can be trained on any video in an unattended manner. Our experiments show that this approach improves both the optical flow and the newer CNN techniques for interpolating and extrapolating video. In the future, it may be useful to combine flow layers with pure synthesis layers to better predict pixels that cannot be copied from other video images. Also, the way we extend our method to multiframe prediction is relatively simple; there are a number of interesting alternatives, such as using the desired time step (e.g. t =.25 for the first of three interpolated frames) as input into the network."}], "references": [{"title": "et al", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "A database and evaluation methodology for optical flow", "author": ["S. Baker", "D. Scharstein", "J. Lewis", "S. Roth", "M.J. Black", "R. Szeliski"], "venue": "IJCV, 92(1):1\u201331", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Large displacement optical flow: descriptor matching in variational motion estimation", "author": ["T. Brox", "J. Malik"], "venue": "PAMI, 33(3):500\u2013513", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "P", "author": ["A. Dosovitskiy", "P. Fischer", "E. Ilg", "P. H\u00e4usser", "C. Hazirbas", "V. Golkov"], "venue": "v.d. Smagt, D. Cremers, and T. Brox. Flownet: Learning optical flow with convolutional networks. In ICCV", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "P", "author": ["A. Dosovitskiy", "P. Fischery", "E. Ilg", "C. Hazirbas", "V. Golkov"], "venue": "van der Smagt, D. Cremers, T. Brox, et al. Flownet: Learning optical flow with convolutional networks. In ICCV, pages 2758\u20132766", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning for physical interaction through video prediction", "author": ["C. Finn", "I. Goodfellow", "S. Levine"], "venue": "arXiv preprint arXiv:1605.07157", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepstereo: Learning to predict new views from the world\u2019s imagery", "author": ["J. Flynn", "I. Neulander", "J. Philbin", "N. Snavely"], "venue": "CVPR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepwarp: Photorealistic image resynthesis for gaze manipulation", "author": ["Y. Ganin", "D. Kononenko", "D. Sungatullina", "V. Lempitsky"], "venue": "arXiv preprint arXiv:1607.07215", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Are we ready for autonomous driving? the kitti vision benchmark suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "CVPR, pages 3354\u20133361", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "THUMOS challenge: Action recognition with a large number of classes", "author": ["A. Gorban", "H. Idrees", "Y.-G. Jiang", "A. Roshan Zamir", "I. Laptev", "M. Shah", "R. Sukthankar"], "venue": "http: //www.thumos.info/", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman"], "venue": "Spatial transformer networks. In NIPS, pages 2017\u20132025", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "CVPR, pages 1725\u20131732", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1097\u20131105", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning image matching by simply watching video", "author": ["G. Long", "L. Kneip", "J.M. Alvarez", "H. Li", "X. Zhang", "Q. Yu"], "venue": "ECCV, pages 434\u2013450", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Moving gradients: A path-based method for plausible image interpolation", "author": ["D. Mahajan", "F.-C. Huang", "W. Matusik", "R. Ramamoorthi", "P. Belhumeur"], "venue": "TOG, 28(3):42:1\u201342:11", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["M. Mathieu", "C. Couprie", "Y. LeCun"], "venue": "arXiv preprint arXiv:1511.05440", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Phase-based frame interpolation for video", "author": ["S. Meyer", "O. Wang", "H. Zimmer", "M. Grosse", "A. Sorkine- Hornung"], "venue": "CVPR, pages 1410\u20131418", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Shuffle and learn: unsupervised learning using temporal order verification", "author": ["I. Misra", "C.L. Zitnick", "M. Hebert"], "venue": "ECCV, pages 527\u2013544", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Video (language) modeling: a baseline for generative models of natural videos", "author": ["M. Ranzato", "A. Szlam", "J. Bruna", "M. Mathieu", "R. Collobert", "S. Chopra"], "venue": "arXiv preprint arXiv:1412.6604", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Epicflow: Edge-preserving interpolation of correspondences for optical flow", "author": ["J. Revaud", "P. Weinzaepfel", "Z. Harchaoui", "C. Schmid"], "venue": "CVPR, pages 1164\u20131172", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "NIPS, pages 568\u2013576", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "UCF101: A dataset of 101 human actions classes from videos in the wild", "author": ["K. Soomro", "A. Roshan Zamir", "M. Shah"], "venue": "CRCV-TR-12-01", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1502.04681", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Prediction error as a quality metric for motion and stereo", "author": ["R. Szeliski"], "venue": "ICCV", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "Singleview to multi-view: Reconstructing unseen views with a convolutional network", "author": ["M. Tatarchenko", "A. Dosovitskiy", "T. Brox"], "venue": "arXiv preprint arXiv:1511.06702", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating Videos with Scene Dynamics", "author": ["C. Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": "In NIPS. 2016", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "An uncertain future: Forecasting from static images using variational autoencoders", "author": ["J. Walker", "C. Doersch", "A. Gupta", "M. Hebert"], "venue": "ECCV, pages 835\u2013851", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "ICCV, pages 2794\u20132802", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "TIP, 13(4):600\u2013612", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks", "author": ["J. Xie", "R.B. Girshick", "A. Farhadi"], "venue": "ECCV, pages 842\u2013857", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks", "author": ["T. Xue", "J. Wu", "K.L. Bouman", "W.T. Freeman"], "venue": "arXiv preprint arXiv:1607.02586", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness", "author": ["J.J. Yu", "A.W. Harley", "K.G. Derpanis"], "venue": "arXiv preprint arXiv:1608.05842", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "View synthesis by appearance flow", "author": ["T. Zhou", "S. Tulsiani", "W. Sun", "J. Malik", "A.A. Efros"], "venue": "arXiv preprint arXiv:1605.03557", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "A new approach [22, 19, 26] uses generative convolutional neural networks (CNNs) to directly hallucinate RGB pixel values of synthesized video frames.", "startOffset": 15, "endOffset": 27}, {"referenceID": 18, "context": "A new approach [22, 19, 26] uses generative convolutional neural networks (CNNs) to directly hallucinate RGB pixel values of synthesized video frames.", "startOffset": 15, "endOffset": 27}, {"referenceID": 25, "context": "A new approach [22, 19, 26] uses generative convolutional neural networks (CNNs) to directly hallucinate RGB pixel values of synthesized video frames.", "startOffset": 15, "endOffset": 27}, {"referenceID": 26, "context": "Video interpolation is commonly used for video retiming, novel-view rendering, and motion-based video compression [27].", "startOffset": 114, "endOffset": 118}, {"referenceID": 1, "context": "Optical flow is the most common approach to video interpolation, and frame prediction is often used to evaluate optical flow accuracy [2].", "startOffset": 134, "endOffset": 137}, {"referenceID": 17, "context": "[18] explore a variation on optical flow that computes paths in the source images and copies pixel gradients along them to the interpolated images, followed by a Poisson reconstruction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] employ a Eulerian, phase-based approach to interpolation, but the method is limited to smaller motions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Convolutional neural networks have been used to make recent and dramatic improvements in image and video recognition [16].", "startOffset": 117, "endOffset": 121}, {"referenceID": 3, "context": "They can also be used to predict optical flow [4], which suggests that CNNs can understand temporal motion.", "startOffset": 46, "endOffset": 49}, {"referenceID": 16, "context": "A related unsupervised approach [17] uses a CNN to predict optical flow by synthesizing interpolated frames, and then inverting the CNN.", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "There are a number of papers that use CNNs to directly generate images [10] and videos [29, 34].", "startOffset": 71, "endOffset": 75}, {"referenceID": 28, "context": "There are a number of papers that use CNNs to directly generate images [10] and videos [29, 34].", "startOffset": 87, "endOffset": 95}, {"referenceID": 33, "context": "There are a number of papers that use CNNs to directly generate images [10] and videos [29, 34].", "startOffset": 87, "endOffset": 95}, {"referenceID": 6, "context": "Generative CNNs can also be used to generate new views of a scene from existing photos taken at nearby viewpoints [7, 33].", "startOffset": 114, "endOffset": 121}, {"referenceID": 32, "context": "Generative CNNs can also be used to generate new views of a scene from existing photos taken at nearby viewpoints [7, 33].", "startOffset": 114, "endOffset": 121}, {"referenceID": 12, "context": "Our technical approach is inspired by recent techniques for including differentiable motion layers in CNNs [13].", "startOffset": 107, "endOffset": 111}, {"referenceID": 35, "context": "Optical flow layers have also been used to render novel views of objects [36] and change eye gaze direction while videoconferencing [8].", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "Optical flow layers have also been used to render novel views of objects [36] and change eye gaze direction while videoconferencing [8].", "startOffset": 132, "endOffset": 135}, {"referenceID": 25, "context": "LSTMs have been used to extrapolate video [26], but the results can be blurry.", "startOffset": 42, "endOffset": 46}, {"referenceID": 18, "context": "[19] reduce blurriness by using adversarial training [10] and unique loss functions, but the results still contain artifacts (we compare our results against this method).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[19] reduce blurriness by using adversarial training [10] and unique loss functions, but the results still contain artifacts (we compare our results against this method).", "startOffset": 53, "endOffset": 57}, {"referenceID": 5, "context": "[6] use LSTMs and differentiable motion models to better sample the multimodal distribution of video future predictions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": ", pre-alignment or lighting adjustment) is needed for the input videos, which is a necessary component for most existing systems [30, 34].", "startOffset": 129, "endOffset": 137}, {"referenceID": 33, "context": ", pre-alignment or lighting adjustment) is needed for the input videos, which is a necessary component for most existing systems [30, 34].", "startOffset": 129, "endOffset": 137}, {"referenceID": 0, "context": "i,j,k\u2208[0,1] WX(V) (4) W = (1\u2212 (Lx \u2212 bLxc))(1\u2212 (Ly \u2212 bLyc))(1\u2212\u2206t) W = (Lx \u2212 bLxc)(1\u2212 (Ly \u2212 bLyc))(1\u2212\u2206t) .", "startOffset": 6, "endOffset": 11}, {"referenceID": 14, "context": "Learning the network is achieved via ADAM solver [15] with learning rate of 0.", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "Batch normalization [12] is adopted for faster convergence.", "startOffset": 20, "endOffset": 24}, {"referenceID": 12, "context": "Similar to [13], the partial derivative of the synthesized voxel color \u0176(x, y) w.", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "i,j,k\u2208[0,1] EX(V) (7)", "startOffset": 6, "endOffset": 11}, {"referenceID": 24, "context": "We trained Deep Voxel Flow (DVF) on videos from the UCF101 dataset [25].", "startOffset": 67, "endOffset": 71}, {"referenceID": 18, "context": "Following [19] and [30], both UCF101 [25] and THUMOS-15 [11] test sets are used as benchmarks.", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "Following [19] and [30], both UCF101 [25] and THUMOS-15 [11] test sets are used as benchmarks.", "startOffset": 19, "endOffset": 23}, {"referenceID": 24, "context": "Following [19] and [30], both UCF101 [25] and THUMOS-15 [11] test sets are used as benchmarks.", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "Following [19] and [30], both UCF101 [25] and THUMOS-15 [11] test sets are used as benchmarks.", "startOffset": 56, "endOffset": 60}, {"referenceID": 31, "context": "We use both PSNR and SSIM [32] to evaluate the image quality of video frame synthesis; higher values of PSNR and SSIM indicate better results.", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "We compare our approach against several methods, including the state-of-the-art optical flow technique EpicFlow [23].", "startOffset": 112, "endOffset": 116}, {"referenceID": 1, "context": "To synthesize the in-between images given the computed flow fields we apply the interpolation algorithm used in the Middlebury interpolation benchmark [2].", "startOffset": 151, "endOffset": 154}, {"referenceID": 18, "context": "For the CNN-based methods, we compare DVF to BeyondMSE [19], which achieved the bestperforming results in video prediction.", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "BeyondMSE [19] 28.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "87 EpicFlow [23] 30.", "startOffset": 12, "endOffset": 16}, {"referenceID": 27, "context": "Views [28] 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 35, "context": "Flow [36] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 18, "context": "We therefore use their best-performing loss (ADV+GDL), and replace the backbone networks in BeyondMSE [19] with ours and train using the same data and protocol as in DVF.", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "Furthermore, we note that our Figure 2 depicts the same video as Figure 6 in [19]; our result is much more realistic.", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "For motion modeling, we identify (large) motion regions according to the flow maps provided by [23].", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": "The KITTI odometry dataset [9] is used here for evaluation, following [36].", "startOffset": 27, "endOffset": 30}, {"referenceID": 35, "context": "The KITTI odometry dataset [9] is used here for evaluation, following [36].", "startOffset": 70, "endOffset": 74}, {"referenceID": 27, "context": "Surprisingly, without fine-tuning, our approach already outperforms [28] and [36] by 0.", "startOffset": 68, "endOffset": 72}, {"referenceID": 35, "context": "Surprisingly, without fine-tuning, our approach already outperforms [28] and [36] by 0.", "startOffset": 77, "endOffset": 81}, {"referenceID": 2, "context": "LD Flow [3] 12.", "startOffset": 8, "endOffset": 11}, {"referenceID": 34, "context": "Basics [35] 9.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "9 FlowNet [5] 9.", "startOffset": 10, "endOffset": 13}, {"referenceID": 22, "context": "1 EpicFlow [23] 3.", "startOffset": 11, "endOffset": 15}, {"referenceID": 30, "context": "Video [31] 43.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "8 Shuffle&Learn [21] 50.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "2 ImageNet [14] 63.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "The KITTI flow 2012 dataset [9] is used as a test set.", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "After fine-tuning, the unsupervised flow generated by DVF surpasses traditional methods [3] and performs comparably to some of the supervised deep models [5].", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "After fine-tuning, the unsupervised flow generated by DVF surpasses traditional methods [3] and performs comparably to some of the supervised deep models [5].", "startOffset": 154, "endOffset": 157}, {"referenceID": 24, "context": "The model is fine-tuned and tested with an action recognition loss on the UCF-101 dataset (split-1) [25].", "startOffset": 100, "endOffset": 104}, {"referenceID": 30, "context": "As demonstrated in Table 2 (right), our approach outperforms random initialization by a large margin and also shows superior performance to other representation learning alternatives [31].", "startOffset": 183, "endOffset": 187}, {"referenceID": 23, "context": "To synthesize frames using voxel flow, DVF has to encode both appearance and motion information, which implicitly mimics a two-stream CNN [24].", "startOffset": 138, "endOffset": 142}, {"referenceID": 22, "context": "Thus, we choose EpicFlow [23] to serve as a strong baseline.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "Our model is implemented using TensorFlow [1].", "startOffset": 42, "endOffset": 45}], "year": 2017, "abstractText": "We address the problem of synthesizing new video frames in an existing video, either in-between existing frames (interpolation), or subsequent to them (extrapolation). This problem is challenging because video appearance and motion can be highly complex. Traditional optical-flow-based solutions often fail where flow estimation is challenging, while newer neural-network-based methods that hallucinate pixel values directly often produce blurry results. We combine the advantages of these two methods by training a deep network that learns to synthesize video frames by flowing pixel values from existing ones, which we call deep voxel flow. Our method requires no human supervision, and any video can be used as training data by dropping, and then learning to predict, existing frames. The technique is efficient, and can be applied at any video resolution. We demonstrate that our method produces results that both quantitatively and qualitatively improve upon the state-ofthe-art.", "creator": "LaTeX with hyperref package"}}}