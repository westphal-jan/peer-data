{"id": "1702.02676", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2017", "title": "Energy Saving Additive Neural Network", "abstract": "In recent years, machine learning techniques based on neural networks for mobile computing become increasingly popular. Classical multi-layer neural networks require matrix multiplications at each stage. Multiplication operation is not an energy efficient operation and consequently it drains the battery of the mobile device. In this paper, we propose a new energy efficient neural network with the universal approximation property over space of Lebesgue integrable functions. This network, called, additive neural network, is very suitable for mobile computing. The neural structure is based on a novel vector product definition, called ef-operator, that permits a multiplier-free implementation. In ef-operation, the \"product\" of two real numbers is defined as the sum of their absolute values, with the sign determined by the sign of the product of the numbers. This \"product\" is used to construct a vector product in $R^N$. The vector product induces the $l_1$ norm. The proposed additive neural network successfully solves the XOR problem. The experiments on MNIST dataset show that the classification performances of the proposed additive neural networks are very similar to the corresponding multi-layer perceptron and convolutional neural networks (LeNet).", "histories": [["v1", "Thu, 9 Feb 2017 02:02:27 GMT  (768kb,D)", "http://arxiv.org/abs/1702.02676v1", "8 pages (double column), 2 figures, 1 table"]], "COMMENTS": "8 pages (double column), 2 figures, 1 table", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["arman afrasiyabi", "ozan yildiz", "baris nasir", "fatos t yarman vural", "a enis cetin"], "accepted": false, "id": "1702.02676"}, "pdf": {"name": "1702.02676.pdf", "metadata": {"source": "CRF", "title": "Energy Saving Additive Neural Network", "authors": ["Arman Afrasiyabi", "Ozan Yildiz", "Baris Nasir", "Fatos T. Yarman", "Enis Cetin"], "emails": ["vural}@ceng.metu.edu.tr", "cetin@bilkent.edu.tr"], "sections": [{"heading": null, "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "II. RELATED WORK", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "III. A NEW ENERGY EFFICIENT OPERATOR", "text": "Let x and y be two vectors in Rd. We define a new operator, called ef operator, as the vector product of x and y as follows; x y: = d \u2211 i = 1character (xi \u00b7 yi) (| xi | + | yi |), (1), which can also be represented as follows; x y: = d \u2211 i = 1character (xi) yi + character (yi) xi, (2), where x = [x1,.., xd] T, y = [y1,..., yd] T-Rd. The new vector product operation does not require any multiplications. The operation (xi \u00b7 yi) (| + | yi |) uses the character of the ordinary multiplication, but it sets the sum of the absolute values of xi and ji. ef operator, can be implemented without any multiplications."}, {"heading": "IV. ADDITIVE NEURAL NETWORK WITH EF-OPERATOR", "text": "We propose a modification of the representation of a neuron in a classical neural network by replacing the vector product of input and weight with the l1 product defined in efoperation. This modification can be applied to a wide range of artificial neural networks, including multi-layer perceptrons (MLP), recursive neural networks (RNN), and convolutionary neural networks (CNN). A neuron in a classical neural network is represented by the following activation function: f (xW + b), (5) where W-Rd \u00b7 M, b-RM are weights and distortions, and x-Rd is the input vector. A neuron in the proposed additive neural network is represented by the activation function, in which we modify the affine transform using the ef operator, as follows: f (a (x) + neuron function, whereby the neuron function is simply defined by the division."}, {"heading": "A. Training the Additive Neural Network", "text": "The standard back propagation algorithm is applicable to the proposed additive neural network with small approximations. Back propagation algorithm calculates derivatives in relation to current values of parameters of a differentiable function in order to update its parameters. Derivatives are calculated iteratively, using previously calculated derivatives from upper layers based on the chain rule. Therefore, the only difference in additive neural network training is the calculation of the derivatives of the argument (a (x W) + b), the activation function in relation to parameters W, a, b and the input, x, as indicated below, is the calculation of the derivatives of the argument, (a (x W) + b), the activation function in relation to parameters W, a, b and the input, x, x, as indicated below, the only difference in additive neural network training is the calculation of the derivatives of the argument, a (W) (x)."}, {"heading": "B. Existence and Convergence of the Solution in Additive Neural Network", "text": "In this section we show that the proposed additional neural network has the universal approximation property of [24] b (1) b (1) x (1) x (1) x (1) x (2) x (2) x (2) x (2) x (4) x (4) x (2) x (2) x (4) x (3) x (3) x (3) x (4) x (4) x (4) x (4) x (4) x (4) x (4) x (4) x (4) x) x (4) x (4) x (4) x (4) x (4) x x x) x (4) x x x (4) x x (4) x (4) x (4) x (4) x x (4) x (4) x (x) x (4) x (4) x (4) x x (4) x x (4) x x x (4) x x x x x x x (4) x x x x x) x x x x x (4) x x x (4) x x x x x (4) x x x x x (4) x x x x (4) x x x (4) x x x (4) x x x (4) x x x x (4) x (4) x x (4) x (4) x (4) x (4) x (4) x (4) x (4) x (4) x (4 x (4) x (4) x (4 (4) x (4) x (4) x (4 (4) x (4) x (4) x (4 (4) x (x (4) x (4) x (4) x (x (4) x (4 (4) x (4) x (x (4) x (4) x (4) x (x (4) x (4) x (x (4) x (4) x (4) x (x (4) x (x (4) x (4) x (4) x (4) x (4) x (x (4) x (4) x (x (4) x (4) x (x (4) x (x (4) x ("}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "In fact, it is in such a way that we are able to put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into a world, in which we put ourselves into a world, in which we put ourselves into a world, in which we put ourselves into a world, in which we put ourselves into a world, in which we put ourselves into a world, in which we put ourselves into a world, in which we put ourselves into a world, in which we put ourselves into a world, in which we put ourselves into a world, in which we put ourselves into a world, in which we in which we put ourselves into a world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves in which we put ourselves into another world, in which we put ourselves into another world, in which we put ourselves in which we put ourselves into another world, in which we put ourselves in which we put ourselves in which we put ourselves into a world, in which we put ourselves in a world, in which we put ourselves into a world, in which we put ourselves in a world, in which we put ourselves in a world, in which we in which we put ourselves in which we put ourselves in a world, in which we put ourselves in a world, in which we put ourselves in which we put ourselves in a world, in a world, in which we in which we put ourselves in which we put into a world, in which we put ourselves in which we put ourselves in a world, in which we put into a world, in a world, in which we, in which we put in which we put"}, {"heading": "VI. CONCLUSION", "text": "In this study, we propose an energy-efficient additive architecture of neural networks. At the core of this architecture is the lasso-standards-based ef-operator, which eliminates energy consumption multiplications in conventional architecture. We have investigated the universal approximation property of the proposed architecture across the space of integrable functions of Lebesgue and tested it in real-world problems. We have shown that ef-operator can successfully solve the non-linear XOR problem. Furthermore, we have observed that our proposed network can be used with an accuracy of 0.39% and 0.69% in the Multilayer Perceptron (MLP) or conventional neural network to classify the MNIST dataset. As a future work, we plan to test the proposed architecture in the state-of-the-art deep neural networks."}, {"heading": "ACKNOWLEDGMENT", "text": "A. Enis Cetin's work was partly funded by a grant from Qualcomm."}], "references": [{"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436444", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 22782324", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks, in Advances in neural information processing", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,2014,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonlinear adaptive flight control using neural networks", "author": ["A.J. Calise", "R.T. Rysdyk"], "venue": "IEEE control systems, vol. 18, no. 6, pp. 1425", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "D", "author": ["A. Giusti", "J. Guzzi"], "venue": "C. Cires an, F.-L. He, J. P. Rodr guez, F. Fontana, M. Faessler, C. Forster, J. Schmidhuber, G. Di Caro et al., A machine learning approach to visual perception of forest trails for mobile robots, IEEE Robotics and Automation Letters, vol. 1, no. 2, pp. 661667", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Image description using a multiplier-less operator", "author": ["H. Tuna", "I. Onaran", "A.E. Cetin"], "venue": "IEEE Signal Processing Letters, vol. 16, no. 9, pp. 751753", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "A multiplication-free framework for signal processing and applications in biomedical image analysis", "author": ["A. Suhre", "F. Keskin", "T. Ersahin", "R. Cetin-Atalay", "R. Ansari", "A.E. Cetin"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Co-difference based object tracking algorithm for infrared videos", "author": ["H.S. Demir", "A.E. Cetin"], "venue": "2016 IEEE International Conference on Image Processing (ICIP). IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Multiplication-free neural networks, in 2015", "author": ["C.E. Akbas", "A. Bozkurt", "A.E. Cetin", "R. Cetin-Atalay", "A. Uner"], "venue": "23th Signal Processing and Communications Applications Conference (SIU). IEEE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "D", "author": ["S.K. Esser", "P.A. Merolla", "J.V. Arthur", "A.S. Cassidy", "R. Appuswamy", "A. Andreopoulos", "D.J. Berg", "J.L. McKinstry", "T. Melano"], "venue": "R. Barch et al., Convolutional networks for fast, energy-efficient neuromorphic computing, arXiv preprint arXiv:1603.08270", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Spinnaker: A 1-w 18-core system-on-chip for massively-parallel neural network simulation", "author": ["E. Painkras", "L.A. Plana", "J. Garside", "S. Temple", "F. Galluppi", "C. Patterson", "D.R. Lester", "A.D. Brown", "S.B. Furber"], "venue": "IEEE Journal of Solid-State Circuits, vol. 48, no. 8, pp. 19431953", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "A", "author": ["T. Pfeil"], "venue": "Gr ubl, S. Jeltsch, E. M uller, P. M uller, M. A. Petrovici, M. Schmuker, D. Br uderle, J. Schemmel, and K. Meier, Six networks on a universal neuromorphic computing substrate, arXiv preprint arXiv:1210.7083", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "An event-based neural network architecture with an asynchronous programmable synaptic memory", "author": ["S. Moradi", "G. Indiveri"], "venue": "IEEE transactions on biomedical circuits and systems, vol. 8, no. 1, pp. 98107", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A 65k-neuron 73-mevents/s 22-pj/event asynchronous micro-pipelined integrate-andfire array transceiver", "author": ["J. Park", "S. Ha", "T. Yu", "E. Neftci", "G. Cauwenberghs"], "venue": "2014 IEEE Biomedical Circuits and System Conference (BioCAS) Proceedings. IEEE", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiplierless artificial neurons exploiting error resiliency for energy-efficient neural computing", "author": ["S.S. Sarwar", "S. Venkataramani", "A. Raghunathan", "K. Roy"], "venue": "2016 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Accurate and efficient hyperbolic tangent activation function on fpga using the dct interpolation filter", "author": ["A.M. Abdelsalam", "J. Langlois", "F. Cheriet"], "venue": "arXiv preprint arXiv:1609.07750", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "arXiv preprint arXiv:1603.05279", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "The Fourier transform and its applications", "author": ["R.N. Bracewell"], "venue": "3rd ed., ser. McGraw-Hill series in electrical and computer engineering; Circuits and systems. McGraw Hill", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of control, signals and systems, vol. 2, no. 4, pp. 303314", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1989}, {"title": "Pattern Recognition and Machine Learning, Vol. I", "author": ["C.M. Bishop", "Ed"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "D", "author": ["H.G.E. Rumelhart"], "venue": "E. and R. J. Williams, Learning representations byback-propagating errors, Nature, pp. 323, 533536", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1986}, {"title": "D", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg"], "venue": "Man e, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi egas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, TensorFlow: Large-scale machine learning on heterogeneous systems", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Artificial Neural Networks (ANN) have been shown to solve many real world problems, such as, computer vision, natural language processing, recommendation systems and many other fields [1].", "startOffset": 184, "endOffset": 187}, {"referenceID": 1, "context": "Convolutional Neural Network (CNN) architectures achieve human performance in many computer vision problems including image classification tasks [2], [3], [4], [5], [6].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "Convolutional Neural Network (CNN) architectures achieve human performance in many computer vision problems including image classification tasks [2], [3], [4], [5], [6].", "startOffset": 150, "endOffset": 153}, {"referenceID": 3, "context": "Convolutional Neural Network (CNN) architectures achieve human performance in many computer vision problems including image classification tasks [2], [3], [4], [5], [6].", "startOffset": 155, "endOffset": 158}, {"referenceID": 4, "context": "Convolutional Neural Network (CNN) architectures achieve human performance in many computer vision problems including image classification tasks [2], [3], [4], [5], [6].", "startOffset": 160, "endOffset": 163}, {"referenceID": 5, "context": "Convolutional Neural Network (CNN) architectures achieve human performance in many computer vision problems including image classification tasks [2], [3], [4], [5], [6].", "startOffset": 165, "endOffset": 168}, {"referenceID": 6, "context": "ANNs are already being used in drones and unmanned aerial vehicles for flight control, path estimation [7], obstacle avoidance and human recognition like abilities [8] (DJI Phantom 4).", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "ANNs are already being used in drones and unmanned aerial vehicles for flight control, path estimation [7], obstacle avoidance and human recognition like abilities [8] (DJI Phantom 4).", "startOffset": 164, "endOffset": 167}, {"referenceID": 8, "context": "According to the [9], the multiplication operation is the most energy consuming operation.", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "An addition consumes relatively lower energy compared to a regular multiplication as shown in [9] in most processors.", "startOffset": 94, "endOffset": 97}, {"referenceID": 9, "context": "We first introduced the l1 norm based vector product for some image processing applications in 2009 [10], [11], [12], [13].", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "We first introduced the l1 norm based vector product for some image processing applications in 2009 [10], [11], [12], [13].", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "We first introduced the l1 norm based vector product for some image processing applications in 2009 [10], [11], [12], [13].", "startOffset": 118, "endOffset": 122}, {"referenceID": 12, "context": "We also proposed the multiplication free neural network structure in 2015 [14].", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "neuromorphic devices [15], [16], [17], [18], [19].", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "neuromorphic devices [15], [16], [17], [18], [19].", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "neuromorphic devices [15], [16], [17], [18], [19].", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "neuromorphic devices [15], [16], [17], [18], [19].", "startOffset": 39, "endOffset": 43}, {"referenceID": 17, "context": "neuromorphic devices [15], [16], [17], [18], [19].", "startOffset": 45, "endOffset": 49}, {"referenceID": 18, "context": "used the error resiliency property of neural networks and proposed an approximation to multiplication operation on artificial neurons for energy-efficient neural computing [20].", "startOffset": 172, "endOffset": 176}, {"referenceID": 8, "context": "proposed a model that reduces both computational cost and storage by feature learning [9].", "startOffset": 86, "endOffset": 89}, {"referenceID": 19, "context": "approximate the tangent activation function using the Discrete Cosine Transform Interpolation Filter (DCTIF) to run the neural networks on FPGA boards efficiently [21].", "startOffset": 163, "endOffset": 167}, {"referenceID": 20, "context": "The first method, Binary-Weight-Networks, approximates all the weight values to binary values [22].", "startOffset": 94, "endOffset": 98}, {"referenceID": 21, "context": "The above derivatives can be easily calculated using the following equation suggested by [23]:", "startOffset": 89, "endOffset": 93}, {"referenceID": 22, "context": "In this section, first, we show that the proposed additive neural network satisfies the universal approximation property of [24], over the space of Lebesgue integrable functions.", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "\u2022 Hidden layer 2, a2 = [1], b2 = [b],", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "a3 = [ 1 1 ]T ,", "startOffset": 5, "endOffset": 12}, {"referenceID": 0, "context": "a3 = [ 1 1 ]T ,", "startOffset": 5, "endOffset": 12}, {"referenceID": 1, "context": "W3 = [ 2 1 ] .", "startOffset": 5, "endOffset": 12}, {"referenceID": 0, "context": "W3 = [ 2 1 ] .", "startOffset": 5, "endOffset": 12}, {"referenceID": 0, "context": "\u2022 Hidden layer 4, a4 = [1], b4 = [0],", "startOffset": 23, "endOffset": 26}, {"referenceID": 22, "context": "This can be shown by the universal approximation theorem for bounded measurable sigmoidal functions [24].", "startOffset": 100, "endOffset": 104}, {"referenceID": 23, "context": "Multi-layer perceptron (MLP) [25] is used to measure the ability of the proposed additive neural network, in machine learning problems.", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "While some studies such as [3] have shown that ReLU outperform the others in most of the cases, we also examined sigmoid and Tanh in the following experiments.", "startOffset": 27, "endOffset": 30}, {"referenceID": 24, "context": "The aim of MLP is to find the optimal values for parameters W and b using backpropagation [26] and optimization algorithms such as stochastic gradient descent (SGD).", "startOffset": 90, "endOffset": 94}, {"referenceID": 25, "context": "In order to implement the network, Tensorflow [27], a python library for numeric computation, is used.", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "In the second experiment, we classified the digits of MNIST dataset of [2] which consists of handwritten examples to examine our proposed additive neural network in multiclass classification problem.", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "[3], because we simply aim to show that our proposed ef-operator gives the learning ability to the deep MLP.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "With addition to MLP, we have used the proposed efoperator to learn the parameters of LeNet-5 [2] to classifying MNIST dataset.", "startOffset": 94, "endOffset": 97}], "year": 2017, "abstractText": "In recent years, machine learning techniques based on neural networks for mobile computing become increasingly popular. Classical multi-layer neural networks require matrix multiplications at each stage. Multiplication operation is not an energy efficient operation and consequently it drains the battery of the mobile device. In this paper, we propose a new energy efficient neural network with the universal approximation property over space of Lebesgue integrable functions. This network, called, additive neural network, is very suitable for mobile computing. The neural structure is based on a novel vector product definition, called ef-operator, that permits a multiplierfree implementation. In ef-operation, the \u201dproduct\u201d of two real numbers is defined as the sum of their absolute values, with the sign determined by the sign of the product of the numbers. This \u201dproduct\u201d is used to construct a vector product in R . The vector product induces the l1 norm. The proposed additive neural network successfully solves the XOR problem. The experiments on MNIST dataset show that the classification performances of the proposed additive neural networks are very similar to the corresponding multi-layer perceptron and convolutional neural networks (LeNet).", "creator": "LaTeX with hyperref package"}}}