{"id": "1702.02738", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2017", "title": "Joint Discovery of Object States and Manipulation Actions", "abstract": "Many activities involve object manipulations aiming to modify object state. Examples of common state changes include full/empty bottle, open/closed door, and attached/detached car wheel. In this work, we seek to automatically discover the states of objects and the associated manipulating actions. Given a set of videos for a particular task, we propose a joint model that learns to identify object states and to localize state-modifying actions. Our model is formulated as a discriminative clustering cost. We assume a consistent temporal order for the changes in object states and manipulating actions, and learn the model without additional supervision. Our method is validated on a new dataset of videos depicting real-life object manipulations. We demonstrate the successful discovery of seven manipulating actions and corresponding object states. Moreover, we emphasize our joint formulation and show the improvement of object state discovery by action recognition and vice versa.", "histories": [["v1", "Thu, 9 Feb 2017 08:04:33 GMT  (2090kb,D)", "https://arxiv.org/abs/1702.02738v1", "12 pages"], ["v2", "Mon, 10 Apr 2017 08:23:00 GMT  (1965kb,D)", "http://arxiv.org/abs/1702.02738v2", "14 pages"], ["v3", "Mon, 28 Aug 2017 08:04:18 GMT  (3190kb,D)", "http://arxiv.org/abs/1702.02738v3", "Appears in: International Conference on Computer Vision 2017 (ICCV 2017). 15 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["jean-baptiste alayrac", "josev sivic", "ivan laptev", "simon lacoste-julien"], "accepted": false, "id": "1702.02738"}, "pdf": {"name": "1702.02738.pdf", "metadata": {"source": "CRF", "title": "Joint Discovery of Object States and Manipulation Actions", "authors": ["Jean-Baptiste Alayrac", "Josef Sivic", "Ivan Laptev", "Simon Lacoste-Julien"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Many of our activities involve changes in object states. We need to open a book to read it, cut bread before we eat it, and light candles before we take out a birthday cake. Transitions from object states are often coupled with certain manipulation measures (open, cut, enlightened). In addition, the success of an action is often characterized by achieving the desired state of an object (whipped cream, ironed shirt) and avoiding other states (branded shirt). Object states and manipulation actions are therefore expected to become a key component of future systems, such as portable automatic assistants or home robots that help people in their daily tasks. Human visual systems can easily distinguish different states of objects, such as open / closed bottle or empty coffee mug. Automatic recognition of object states and we expect to adopt automatic recognition of object states and the arrangement of ENL'S."}, {"heading": "2. Related work", "text": "In fact, it is the case that most people who are able to identify themselves are able to identify themselves, and this also applies to the way in which they are able to identify themselves, in the way in which they move. In the way in which they move, they are not able to identify themselves, in the way in which they move. In the way in which they move, they are also able to identify themselves. In the way in which they move, they are not able to identify themselves."}, {"heading": "3. Modeling manipulated objects", "text": "We also assume that we obtain an a priori model of the corresponding object in the form of a pre-trained object and identify its states over time. We do this by collectively locating the appearances of an object (such as an \"oyster\") that appear in all clips in two classes and corresponding to the two different states (such as \"closed\" and \"open\"), while simultaneously locating a consistent \"opening\" action that consistently reconciles the two states. \"Formally, we formulate the problem as minimizing a common cost function that links the prediction in time."}, {"heading": "3.1. Discovering object states", "text": "\"We assume that the first target we have set ourselves is a different type of object detector based on existing data systems such as ImageNet.\" We assume that each clip n is accompanied by a series of mn tracklets1 of interest to the object. We formalize the task of locating the states of objects as a discriminatory clustering problem, where the goal is to find an mapping matrix Yn. (Yn) mk = 1 indicates that the m-th tracklet represents the object in the state of Yn. We also allow a complete set of Yn to encode that no state has been mapped to the corresponding tracklet."}, {"heading": "3.2. Action localization", "text": "Our action model corresponds to that of [6], which is applied to only one action. Specifically, the goal is to find an assignment matrix Zn, {0, 1} Tn for each clip n, with 2We combine all variables Yn into a matrix Y.Znt = 1. The costs we minimize for this problem are similar to those of the object: f (Z) = min Wv Rdv12T Z \u2212 XvWv 2F 1 encodes that no action is detected at the interval t. The costs we minimize for this problem are similar to those of the object: f (Z) = min Wv Rdv12T Z \u2212 XvWv 2F 2 Wv 2 regulator, (3) where Wv is the action classifier, it is a regularization model and Xv is a matrix of visual features."}, {"heading": "3.3. Linking actions and object states", "text": "Actions in our model are directly related to changes in the object states. Therefore, we want to force consistency between the two problems. To this end, we are designing a novel common cost function based on the action video caption Zn and the assignment of the state Yn for each clip. We want to impose a restriction that the action takes place between the two different object states. In other words, we want to punish the fact that condition 1 is detected after the action or the fact that condition 2 is triggered before the action occurs. Common cost definition we propose the following common symmetrical cost function for each clip: Zn (Yn) = Yn (Yn) [Yn) \u2212 TZn] + Yn (Yn) + Yn (Zn) + Yn) + Yn (Zn)"}, {"heading": "4. Optimization", "text": "Optimizing problem (1) presents us with several challenges to face. First, we propose loosening the intervention restrictions and distortion function (Section 4.1). Second, we optimize this relaxation with the help of Frank-Wolfe with a new dynamic program that is able to handle our tracklet restrictions (Section 4.2). Finally, we introduce a new rounding technique to obtain an integral candidate solution to our problem (Section 4.3)."}, {"heading": "4.1. Relaxation", "text": "Problem (1) is generally NP-hard [30] due to its specific integer constraints. Inspired by the approach of [5], which has been successful in approaching combinatorial optimization problems, we propose to use the narrowest convex loosening of the feasible subset of binary matrices by taking their convex hull. Since our variables can now include values in [0, 1], we must also propose a consistent extension for the various cost functions to handle fraction values as input. For the cost functions f and g, we can take their expression directly on the relaxed set, since they are already expressed as (convex) square functions. Similarly, for the joint cost function d in (4) we use its natural two-dimensional relaxation: d (Zn, Yn) = Mn \u00b2 i = 1 Tn \u00b2 t = 1 (Yn) i1Znt [tni + Ytnt] function d in (4), however, it is not related to (itni) \u2212 in time."}, {"heading": "4.2. Joint optimization using Frank-Wolfe", "text": "In dealing with a limited optimization problem, for which it is easy to solve linear programs but difficult to project onto the realizable amount, the Frank Wolfe algorithm is an excellent choice [22, 28]. It is precisely the case for our relaxed problem, where the linear program can be efficiently solved via the convex body of the realizable integer matrices using dynamic programming. Furthermore, [27] has recently shown that the Frank Wolfe algorithm can be converted by line search to a stationary point for non-convex targets at a rate of O (1 / \u221a k). We use this algorithm for the joint optimization of (1). Since the goal is square, we can perform exact line search analytically, which speeds up convergence in practice. Finally, to obtain a good initialization for both variables Z and Y, we optimize first without cong (Z) and Y (Y)."}, {"heading": "4.3. Joint rounding method", "text": "Once we have obtained a candidate solution for the relaxed problem, we must round it down to an integer solution to make predictions. Previous work [2, 6] has observed that using the learned W classifier for rounding yielded better results than other possible alternatives. We extend this approach to our joint lineup by proposing the following new rounding method. We optimize problem (1), but specify the values of W in the discriminatory cluster costs. Specifically, we minimize the following cost function by using the integer points Z-Z and Y: 12T-Z \u2212 XvW; v-2F + 12M-Y \u2212 XsW; s-2F + d (Z, Y), (7) where W-v and W-s are the classification weights that are achieved at the end of the relaxed optimization. Since y2 = y, if y is binary, (7) in fact a linear target over the binary matrix Yn for Zn is set. Therefore, we can optimize per year (7) by solving each program."}, {"heading": "5. Experiments", "text": "In this section, we will first describe our data set, the object tracking pipeline, and the feature representation for object tracklets and videos (Section 5.1). We will consider two experimental arrangements. In the first weakly monitored setup (Section 5.2), we will apply our method to a series of video clips that we know contain the action of interest but do not know its exact timing. In the second, more sophisticated \"wild\" setup (Section 5.3), the input amount of weakly monitored clips is obtained by automatically processing the text associated with the videos, and may therefore contain erroneous clips that do not contain relevant manipulation actions. The data and code are available online [1]."}, {"heading": "5.1. Dataset and features", "text": "In fact, most of them are able to survive themselves if they don't put themselves in a position to survive themselves; most of them are able to survive themselves; most of them are able to survive themselves; most of them are able to survive themselves; and most of them are able to survive themselves; and most of them are able to survive themselves; most of them are not able to survive themselves; most of them are able to survive themselves; most of them are able to survive themselves; and most of them are able to survive themselves; and most of them are not able to survive themselves."}, {"heading": "5.2. Weakly supervised object state discovery", "text": "We first apply our method in a weakly monitored setup, where we provide for each action a series of video clips in which we know that the action takes place somewhere in the clip, but we do not provide the exact time localization. Each clip may contain other actions that affect other objects or actions that do not affect any object at all (e.g. walking / jumping). The input clips are about 20 years long and are achieved by taking about \u00b1 10 seconds of each annotated manipulation action. Rating: average precision. For all variants of our method, we use the rounded solution that has reached the smallest target during optimization. We evaluate these predictions with an accuracy score averaged over all videos. A temporary action localization is called correct if it falls within the basic truth interval. Likewise, a government prediction is correct if it corresponds to the basic truth level."}, {"heading": "5.3. Object state discovery in the wild", "text": "Considering that action clips were made with automatic text-based retrievals, we use our method next. Clip retrievals by text. Instructional videos [2, 31, 36] usually come with a narrative provided by the narrator, who describes the sequence of actions performed. In this experiment, we keep only such narrated tutorial videos from our record, resulting in a total of 140 videos, which are on average three minutes long. We extract the narration in the form of subtitles associated with the video. These subtitles were downloaded directly from YouTube and were obtained either by Youtube's Automatic Speech Recognition (ASR) or by users. We use the resulting text to retrieve clip candidates that change the state of an object."}, {"heading": "6. Conclusion and future work", "text": "More generally, our work provides evidence that actions should be modeled in the broader context of goals and effects. Finally, our work opens up the possibility of Internet-based learning of actions from narrated video sequences. Recognition This research was partially supported by a Google Research Award, ERC grants Activia (No. 307574) and LEAP (No. 336845), the CIFAR Learning in Machines & Brains program, and ESIF, OP Research, Development and Education Project IMPACT No. CZ.02.1.01 / 0.0 / 15 003 / 000000468."}, {"heading": "A. SVM training for clip retrieval", "text": "In Section 5.3, we proposed an automatic method for retrieving video clips with manipulated objects, using narrations that accompany instructional videos. Narrations in the form of text are first automatically obtained with Automatic Speech Recognition (ASR) 6 and then processed as detailed below. Language records. For each manipulation action, we first find relevant positive and negative sentences on instruction websites such as Wikihow. On average, we receive about 12 positive and 50 negative sentences per action. Language functions. Standard methods for analyzing text typically fail in the absence of punctuation. To process ASR output that does not require punctuation, we suggest using the following simple but robust text rendering. We represent each 10-word window of narration with a TF-IDF vector based on unigrams and bi-programs."}, {"heading": "B. Dataset of manipulated objects", "text": "Table 4 provides statistics for the data set introduced in Section 5.1 of the main article. For each object class, we provide associated action classes and the number of video clips for each action. We also provide the list of states and the number of object tracklets with state comments. In total, we have about 20,000 commented tracks that we use for quantitative assessment of state detection."}, {"heading": "C. Dynamic program for the tracklets", "text": "This year it is more than ever before."}, {"heading": "D. Joint cost rounding method", "text": "Remember that we propose a convex relaxation approach in order to obtain a candidate solution to the main problem (1). Therefore, we must subsequently round the relaxed solution in order to obtain a valid integer solution. We propose here a new rounding that is adapted to our common problem. We call this rounding the rounding of common costs (see section 4 of the main paper). This rounding is inspired by [6, 2]. You observe that the use of the learned W classifier gives them better solutions, both in terms of objective value and performance. We propose to use their natural expansion for our common model. We first fix the classifiers for actions Wa and for states Ws to their relaxed solution (W-a, W-s) and find for each clip n the pair (Zn, Yn) that minimizes the common costs (7)."}, {"heading": "E. Supervised baselines for Action Localization", "text": "In order to compare the numbers with our experiment, we used a leave-one-out technique. For each action, we train a binary classifier with SVM on all but one videos. Similar to our setting, we then select the top scoring time interval of the test video left alone. We repeat this process for all videos and report on the metric used in our work. For the baseline (1), we use the same functions we use in the main work. For the baseline (2), we complete our functions with all channels of improved dense trajectories (IDT) [43]. Detailed results are given in Table 5.Algorithm 1 Cost rounding for videos."}, {"heading": "F. Failure cases", "text": "We observed two main types of errors shown in Figure 6: the first occurs when a false positive object detection consistently fulfills the hypothesis of our model in multiple videos (the top two lines in Figure 6); the second typical error mode is based on ambiguous labels (bottom line in Figure 6), underscoring the difficulty of commenting on the truth of the soil for long actions such as \"drinking coffee.\""}], "references": [{"title": "Unsupervised learning from narrated instruction videos", "author": ["J.-B. Alayrac", "P. Bojanowski", "N. Agrawal", "I. Laptev", "J. Sivic", "S. Lacoste Julien"], "venue": "CVPR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "DIFFRAC: A discriminative and flexible framework for clustering", "author": ["F. Bach", "Z. Harchaoui"], "venue": "NIPS,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Finding actors and actions in movies", "author": ["P. Bojanowski", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid", "J. Sivic"], "venue": "ICCV,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Weakly supervised action labeling in videos under ordering constraints", "author": ["P. Bojanowski", "R. Lajugie", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid", "J. Sivic"], "venue": "ECCV,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Weakly-supervised alignment of video with text", "author": ["P. Bojanowski", "R. Lajugie", "E. Grave", "F. Bach", "I. Laptev", "J. Ponce", "C. Schmid"], "venue": "ICCV,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Detecting changes in real-world objects: The relationship between visual long-term memory and change blindness", "author": ["T.F. Brady", "T. Konkle", "A. Oliva", "G.A. Alvarez"], "venue": "Communicative and Integrative Biology,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, pages 273\u2013297,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "You-do, i-learn: Discovering task relevant objects and their modes of interaction from multi-user egocentric video", "author": ["D. Damen", "T. Leelasawassuk", "O. Haines", "A. Calway", "W. Mayol-Cuevas"], "venue": "BMVA,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning person-object interactions for action recognition in still images", "author": ["V. Delaitre", "J. Sivic", "I. Laptev"], "venue": "NIPS,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "CVPR,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Discovering localized attributes for fine-grained recognition", "author": ["K. Duan", "D. Parikh", "D. Crandall", "K. Grauman"], "venue": "CVPR, pages 3474\u20133481,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I.E. Lim", "D. Hoiem", "D. Forsyth"], "venue": "CVPR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Modeling actions through state changes", "author": ["A. Fathi", "J.M. Rehg"], "venue": "CVPR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Modeling video evolution for action recognition", "author": ["B. Fernando", "E. Gavves", "M.J. Oramas", "A. Ghodrati", "T. Tuytelaars"], "venue": "CVPR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "ICCV,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Observing humanobject interactions: Using spatial and functional compatibility for recognition", "author": ["A. Gupta", "A. Kembhavi", "L.S. Davis"], "venue": "PAMI, 31(10):1775\u20131789,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Connectionist temporal modeling for weakly supervised action labeling", "author": ["D.-A. Huang", "L. Fei-Fei", "J.C. Niebles"], "venue": "ECCV,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Discovering states and transformations in image collections", "author": ["P. Isola", "J.J. Lim", "E.H. Adelson"], "venue": "CVPR,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Revisiting Frank-Wolfe: Projection-free sparse convex optimization", "author": ["M. Jaggi"], "venue": "ICML,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Representing videos using mid-level discriminative patches", "author": ["A. Jain", "A. Gupta", "M. Rodriguez", "L. Davis"], "venue": "CVPR, pages 2571\u20132578,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient image and video co-localization with Frank-Wolfe algorithm", "author": ["A. Joulin", "K. Tang", "L. Fei-Fei"], "venue": "ECCV,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual objectaction recognition: Inferring object affordances from human demonstration", "author": ["H. Kjellstr\u00f6m", "J. Romero", "D. Kragi\u0107"], "venue": "CVIU, 115(1):81\u201390,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Geodesic object proposals", "author": ["P. Krhenbhl", "V. Koltun"], "venue": "ECCV,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Convergence rate of Frank-Wolfe for nonconvex objectives", "author": ["S. Lacoste-Julien"], "venue": "arXiv:1607.00345,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "On the global linear convergence of Frank-Wolfe optimization variants", "author": ["S. Lacoste-Julien", "M. Jaggi"], "venue": "NIPS,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning realistic human actions from movies", "author": ["I. Laptev", "M. Marszalek", "C. Schmid", "B. Rozenfeld"], "venue": "CVPR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "A survey for the quadratic assignment problem", "author": ["E.M. Loiola", "N.M.M. de Abreu", "P.O. Boaventura-Netto", "P. Hahn", "T. Querido"], "venue": "EJOR,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "What\u2019s cookin\u2019? Interpreting cooking videos using text, speech and vision", "author": ["J. Malmaud", "J. Huang", "V. Rathod", "N. Johnston", "A. Rabinovich", "K. Murphy"], "venue": "NAACL,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Relative attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "ICCV,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "The SUN attribute database: Beyond categories for deeper scene understanding", "author": ["G. Patterson", "C. Xu", "H. Su", "J. Hays"], "venue": "IJCV,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Detecting activities of daily living in first-person camera views", "author": ["H. Pirsiavash", "D. Ramanan"], "venue": "CVPR,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "NIPS,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised semantic parsing of video collections", "author": ["O. Sener", "A. Zamir", "S. Savarese", "A. Saxena"], "venue": "ICCV,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Hollywood in homes: Crowdsourcing data collection for activity understanding", "author": ["G.A. Sigurdsson", "G. Varol", "X. Wang", "A. Farhadi", "I. Laptev", "A. Gupta"], "venue": "ECCV,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "NIPS,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised discovery of mid-level discriminative patches", "author": ["S. Singh", "A. Gupta", "A.A. Efros"], "venue": "ECCV,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Co-localization in real-world images", "author": ["K. Tang", "A. Joulin", "L.-J. Li", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning spatiotemporal features with 3D convolutional networks", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "ICCV,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Action recognition with improved trajectories", "author": ["H. Wang", "C. Schmid"], "venue": "ICCV,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Actions \u0303 transformations", "author": ["X. Wang", "A. Farhadi", "A. Gupta"], "venue": "CVPR,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Maximum margin clustering", "author": ["L. Xu", "J. Neufeld", "B. Larson", "D. Schuurmans"], "venue": "NIPS,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2004}, {"title": "Human action recognition by learning bases of action attributes and parts", "author": ["B. Yao", "X. Jiang", "A. Khosla", "A.L. Lin", "L. Guibas", "L. Fei- Fei"], "venue": "ICCV,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 5, "context": "Human visual system can easily distinguish different states of objects, such as open/closed bottle or full/empty coffee cup [7].", "startOffset": 124, "endOffset": 127}, {"referenceID": 18, "context": "Despite much work on object recognition and localization, recognition of object states has received only limited attention in computer vision [21].", "startOffset": 142, "endOffset": 146}, {"referenceID": 10, "context": "One solution to recognizing object states would be to manually annotate states for different objects, and treat the problem as a supervised fine-grained object classification task [13, 14].", "startOffset": 180, "endOffset": 188}, {"referenceID": 11, "context": "One solution to recognizing object states would be to manually annotate states for different objects, and treat the problem as a supervised fine-grained object classification task [13, 14].", "startOffset": 180, "endOffset": 188}, {"referenceID": 1, "context": "We formulate our problem by adopting a discriminative clustering loss [3] and a joint consistency cost between states and actions.", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "Modeling co-occurrences of objects and actions have shown benefits for recognizing actions in [10, 18, 25, 34, 46].", "startOffset": 94, "endOffset": 114}, {"referenceID": 15, "context": "Modeling co-occurrences of objects and actions have shown benefits for recognizing actions in [10, 18, 25, 34, 46].", "startOffset": 94, "endOffset": 114}, {"referenceID": 22, "context": "Modeling co-occurrences of objects and actions have shown benefits for recognizing actions in [10, 18, 25, 34, 46].", "startOffset": 94, "endOffset": 114}, {"referenceID": 31, "context": "Modeling co-occurrences of objects and actions have shown benefits for recognizing actions in [10, 18, 25, 34, 46].", "startOffset": 94, "endOffset": 114}, {"referenceID": 43, "context": "Modeling co-occurrences of objects and actions have shown benefits for recognizing actions in [10, 18, 25, 34, 46].", "startOffset": 94, "endOffset": 114}, {"referenceID": 0, "context": "in instructional videos [2, 31, 36] or while performing daily activities [37].", "startOffset": 24, "endOffset": 35}, {"referenceID": 28, "context": "in instructional videos [2, 31, 36] or while performing daily activities [37].", "startOffset": 24, "endOffset": 35}, {"referenceID": 33, "context": "in instructional videos [2, 31, 36] or while performing daily activities [37].", "startOffset": 24, "endOffset": 35}, {"referenceID": 34, "context": "in instructional videos [2, 31, 36] or while performing daily activities [37].", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "Prior work has addressed recognition of object attributes [14, 32, 33], which can be seen as different object states in some cases.", "startOffset": 58, "endOffset": 70}, {"referenceID": 29, "context": "Prior work has addressed recognition of object attributes [14, 32, 33], which can be seen as different object states in some cases.", "startOffset": 58, "endOffset": 70}, {"referenceID": 30, "context": "Prior work has addressed recognition of object attributes [14, 32, 33], which can be seen as different object states in some cases.", "startOffset": 58, "endOffset": 70}, {"referenceID": 18, "context": "[21] discover object states and transformations between them by analyzing large collections of still images downloaded from the Internet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "In [9], the authors use consistent manipulations to discover task relevant objects.", "startOffset": 3, "endOffset": 6}, {"referenceID": 26, "context": "Most of the prior work on action recognition has focused on designing features to describe time intervals of a video using motion and appearance [29, 38, 42, 43].", "startOffset": 145, "endOffset": 161}, {"referenceID": 35, "context": "Most of the prior work on action recognition has focused on designing features to describe time intervals of a video using motion and appearance [29, 38, 42, 43].", "startOffset": 145, "endOffset": 161}, {"referenceID": 39, "context": "Most of the prior work on action recognition has focused on designing features to describe time intervals of a video using motion and appearance [29, 38, 42, 43].", "startOffset": 145, "endOffset": 161}, {"referenceID": 40, "context": "Most of the prior work on action recognition has focused on designing features to describe time intervals of a video using motion and appearance [29, 38, 42, 43].", "startOffset": 145, "endOffset": 161}, {"referenceID": 12, "context": "This observation has been used to design action models in [15, 16, 44].", "startOffset": 58, "endOffset": 70}, {"referenceID": 13, "context": "This observation has been used to design action models in [15, 16, 44].", "startOffset": 58, "endOffset": 70}, {"referenceID": 41, "context": "This observation has been used to design action models in [15, 16, 44].", "startOffset": 58, "endOffset": 70}, {"referenceID": 41, "context": "In [44], for example, the authors propose to learn an embedding in which a given action acts as a transformation of features of the video.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "[15] who represent actions in egocentric videos by changes of appearance of objects (also called object states), however, their method requires manually annotated precise temporal localization of actions in training videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Our model builds on unsupervised discriminative clustering methods [3, 40, 45] that group data samples according to a simultaneously learned classifier.", "startOffset": 67, "endOffset": 78}, {"referenceID": 37, "context": "Our model builds on unsupervised discriminative clustering methods [3, 40, 45] that group data samples according to a simultaneously learned classifier.", "startOffset": 67, "endOffset": 78}, {"referenceID": 42, "context": "Our model builds on unsupervised discriminative clustering methods [3, 40, 45] that group data samples according to a simultaneously learned classifier.", "startOffset": 67, "endOffset": 78}, {"referenceID": 2, "context": "Such methods can incorporate (weak) supervision that helps to steer the clustering towards a preferred solution [4, 12, 20, 23, 41].", "startOffset": 112, "endOffset": 131}, {"referenceID": 17, "context": "Such methods can incorporate (weak) supervision that helps to steer the clustering towards a preferred solution [4, 12, 20, 23, 41].", "startOffset": 112, "endOffset": 131}, {"referenceID": 20, "context": "Such methods can incorporate (weak) supervision that helps to steer the clustering towards a preferred solution [4, 12, 20, 23, 41].", "startOffset": 112, "endOffset": 131}, {"referenceID": 38, "context": "Such methods can incorporate (weak) supervision that helps to steer the clustering towards a preferred solution [4, 12, 20, 23, 41].", "startOffset": 112, "endOffset": 131}, {"referenceID": 1, "context": "In particular, we build on the discriminative clustering approach of [3] that has been shown to perform well in a variety of computer vision problems [4].", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "In particular, we build on the discriminative clustering approach of [3] that has been shown to perform well in a variety of computer vision problems [4].", "startOffset": 150, "endOffset": 153}, {"referenceID": 21, "context": "Part of our object state model is related to [24], while our action model is related to [6].", "startOffset": 45, "endOffset": 49}, {"referenceID": 4, "context": "Part of our object state model is related to [24], while our action model is related to [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 14, "context": "ject detector [17].", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "We obtain candidate object detections using standard object detectors pre-trained on large scale existing datasets such as ImageNet [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 1, "context": "In detail, we minimize the following discriminative clustering cost [3]:2", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "The minimization in Ws actually leads to a convex quadratic cost function in Y (see [3]).", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "We call this last constraint the \u201cat least one\u201d constraint in contrast to forcing \u201cexactly one\u201d ordered prediction as previously proposed in a discriminative clustering approach on video for action localization [6].", "startOffset": 211, "endOffset": 214}, {"referenceID": 4, "context": "Our action model is equivalent to the one of [6] applied to only one action.", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "We constrain our model to predict exactly one time interval for an action per clip, an approach for actions that was shown to be beneficial in a weakly supervised setting [6] (referred to as \u201caction saliency\u201d constraint).", "startOffset": 171, "endOffset": 174}, {"referenceID": 27, "context": "Problem (1) is NP-hard in general [30] due to its specific integer constraints.", "startOffset": 34, "endOffset": 38}, {"referenceID": 3, "context": "Inspired by the approach of [5] that was successful to approximate combinatorial optimization problems, we propose to use the tightest convex relaxation of the feasible subset of binary matrices by taking its convex hull.", "startOffset": 28, "endOffset": 31}, {"referenceID": 19, "context": "When dealing with a constrained optimization problem for which it is easy to solve linear programs but difficult to project on the feasible set, the Frank-Wolfe algorithm is an excellent choice [22, 28].", "startOffset": 194, "endOffset": 202}, {"referenceID": 25, "context": "When dealing with a constrained optimization problem for which it is easy to solve linear programs but difficult to project on the feasible set, the Frank-Wolfe algorithm is an excellent choice [22, 28].", "startOffset": 194, "endOffset": 202}, {"referenceID": 24, "context": "Moreover, [27] recently showed that the Frank-Wolfe algorithm with line-search converges to a stationary point for non-convex objectives at a rate of", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "Previous work has explored \u201cexact one\u201d ordering constraints for time localization problems [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 0, "context": "Previous works [2, 6] have observed that using the learned W \u2217 classifier for rounding gave better results than other possible alternatives.", "startOffset": 15, "endOffset": 21}, {"referenceID": 4, "context": "Previous works [2, 6] have observed that using the learned W \u2217 classifier for rounding gave better results than other possible alternatives.", "startOffset": 15, "endOffset": 21}, {"referenceID": 0, "context": "sources: the instructional video dataset introduced in [2], the Charades dataset from [37], and some additional videos downloaded from YouTube.", "startOffset": 55, "endOffset": 58}, {"referenceID": 34, "context": "sources: the instructional video dataset introduced in [2], the Charades dataset from [37], and some additional videos downloaded from YouTube.", "startOffset": 86, "endOffset": 90}, {"referenceID": 14, "context": "In order to obtain detectors for the five objects, we finetune the FastRCNN network [17] with training data from ImageNet [11].", "startOffset": 84, "endOffset": 88}, {"referenceID": 9, "context": "In order to obtain detectors for the five objects, we finetune the FastRCNN network [17] with training data from ImageNet [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 32, "context": "In our set-up with only moderate amount of training data, we observed that class-agnostic object proposals combined with FastRCNN performed better than FasterRCNN [35].", "startOffset": 163, "endOffset": 167}, {"referenceID": 23, "context": "In detail, we use geodesic object proposals [26] and set a relatively low object detection threshold (0.", "startOffset": 44, "endOffset": 48}, {"referenceID": 2, "context": "We track objects using a generic KLT tracker from [4].", "startOffset": 50, "endOffset": 53}, {"referenceID": 32, "context": "The CNN feature is extracted with a ROI pooling [35] of ResNet50 [19].", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "The CNN feature is extracted with a ROI pooling [35] of ResNet50 [19].", "startOffset": 65, "endOffset": 69}, {"referenceID": 0, "context": "the approach of [2, 5, 6], each video is divided into chunks of 10 frames that are represented by a motion and appearance descriptor averaged over 30 frames.", "startOffset": 16, "endOffset": 25}, {"referenceID": 3, "context": "the approach of [2, 5, 6], each video is divided into chunks of 10 frames that are represented by a motion and appearance descriptor averaged over 30 frames.", "startOffset": 16, "endOffset": 25}, {"referenceID": 4, "context": "the approach of [2, 5, 6], each video is divided into chunks of 10 frames that are represented by a motion and appearance descriptor averaged over 30 frames.", "startOffset": 16, "endOffset": 25}, {"referenceID": 40, "context": "For the motion we use a 2,000 dimensional bag-of-word representation of histogram of local optical flow (HOF) obtained from Improved Dense Trajectories [43].", "startOffset": 152, "endOffset": 156}, {"referenceID": 0, "context": "Following [2], we add an appearance vector that is obtained from a 1,000 dimensional bagof-word vector of conv5 features from VGG16 [39].", "startOffset": 10, "endOffset": 13}, {"referenceID": 36, "context": "Following [2], we add an appearance vector that is obtained from a 1,000 dimensional bagof-word vector of conv5 features from VGG16 [39].", "startOffset": 132, "endOffset": 136}, {"referenceID": 4, "context": "22 (ii) [6] 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": "20 (iii) [6] + object cues 0.", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "While the saliency approach (taking only the most confident detection per video) was useful for action modeling in [6], it is less suitable for our setup where multiple tracklets can be in the same state.", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "Baseline (ii) is the method introduced in [6] used here with only one action.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "To address this issue, we also evaluate baseline (iii), which complements [6] with the additional constraint that the action prediction has to be within the first and the last frame where the object of interest is detected, improving the overall performance above chance.", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "Instructional videos [2, 31, 36] usually come with a narration provided by the speaker describing the performed sequence of actions.", "startOffset": 21, "endOffset": 32}, {"referenceID": 28, "context": "Instructional videos [2, 31, 36] usually come with a narration provided by the speaker describing the performed sequence of actions.", "startOffset": 21, "endOffset": 32}, {"referenceID": 33, "context": "Instructional videos [2, 31, 36] usually come with a narration provided by the speaker describing the performed sequence of actions.", "startOffset": 21, "endOffset": 32}, {"referenceID": 6, "context": "We then train a linear SVM classifier [8] on bigram text features.", "startOffset": 38, "endOffset": 41}], "year": 2017, "abstractText": "Many human activities involve object manipulations aiming to modify the object state. Examples of common state changes include full/empty bottle, open/closed door, and attached/detached car wheel. In this work, we seek to automatically discover the states of objects and the associated manipulation actions. Given a set of videos for a particular task, we propose a joint model that learns to identify object states and to localize state-modifying actions. Our model is formulated as a discriminative clustering cost with constraints. We assume a consistent temporal order for the changes in object states and manipulation actions, and introduce new optimization techniques to learn model parameters without additional supervision. We demonstrate successful discovery of seven manipulation actions and corresponding object states on a new dataset of videos depicting real-life object manipulations. We show that our joint formulation results in an improvement of object state discovery by action recognition and vice versa.", "creator": "LaTeX with hyperref package"}}}