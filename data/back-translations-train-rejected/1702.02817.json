{"id": "1702.02817", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2017", "title": "Graph Based Relational Features for Collective Classification", "abstract": "Statistical Relational Learning (SRL) methods have shown that classification accuracy can be improved by integrating relations between samples. Techniques such as iterative classification or relaxation labeling achieve this by propagating information between related samples during the inference process. When only a few samples are labeled and connections between samples are sparse, collective inference methods have shown large improvements over standard feature-based ML methods. However, in contrast to feature based ML, collective inference methods require complex inference procedures and often depend on the strong assumption of label consistency among related samples. In this paper, we introduce new relational features for standard ML methods by extracting information from direct and indirect relations. We show empirically on three standard benchmark datasets that our relational features yield results comparable to collective inference methods. Finally we show that our proposal outperforms these methods when additional information is available.", "histories": [["v1", "Thu, 9 Feb 2017 12:58:23 GMT  (765kb,D)", "http://arxiv.org/abs/1702.02817v1", "Pacific-Asia Conference on Knowledge Discovery and Data Mining"]], "COMMENTS": "Pacific-Asia Conference on Knowledge Discovery and Data Mining", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.LG", "authors": ["immanuel bayer", "uwe nagel", "steffen rendle"], "accepted": false, "id": "1702.02817"}, "pdf": {"name": "1702.02817.pdf", "metadata": {"source": "CRF", "title": "Graph Based Relational Features for Collective Classification", "authors": ["Immanuel Bayer", "Uwe Nagel", "Steffen Rendle"], "emails": ["immanuel.bayer@uni-konstanz.de", "uwe.nagel@uni-konstanz.de", "steffen.rendle@uni-konstanz.de"], "sections": [{"heading": "1 Introduction", "text": "These relationships are helpful for tasks such as classifying scientific articles, where patterns such as \"linked samples have similar labels\" are highly predictable. Feature-based ML methods, on the other hand, often assume that samples are independently, identically distributed (iid).This approach is well established, allows efficient parameter estimations and simplified predictions, but ignores the relational information available in SRL Settings. Recently, a number of methods have been proposed [11, 13, 18, 31] which significantly improve on classical methods by using common inferences. Exact joint inference has high runtime complexity which often required approximated joint inference techniques are new difficulties as the need for specialized implementations that are expensive to run and difficult to tune [30]."}, {"heading": "2 Problem Setting", "text": "We start by giving the necessary definitions with the traditional setting of the samples D = {(xi, yi)} Ni = 1, where yi is the class name and xi is a feature vector describing the sample i. We assume that the class name yi is known for the first u-samples (i,..., u}) and the class name is unknown for the samples with index i > u. Relationships between the samples are defined by weighted symmetrical adjacence matrices Rk, Rn \u00b7 n and all the relational information is called R = {Rk} Kk = 1. While in the general case each of the k relations could be complete, i.e. there is some similarity between each sample pair, we explicitly consider the case of sparse, unweighted relationships where only a minority of node pairs are connected by an edge. We start with a statistical argument to motivate the representation of relational information in a way that is compatible with the id assumption."}, {"heading": "2.1 Preserving IID", "text": "Many machine learning algorithms are based on the maximum probability principle to determine the optimal value for model parameters underlying a dataset D (c.f. [17]). A very common assumption in many ML approaches is that the samples in dataset D are independent and identically distributed (iid). This assumption simplifies the probability top (D | \u03b8) iid. A very common assumption is that the samples in dataset D are independent and identically distributed (iid), especially for each example pair (xi, yi) and (xj, yj) conditional independence no holdp ((xi, yi) relational characteristics p (xj, yj). One of the central arguments of relational learning is that examples are not iid, especially for each example pair (xi, yi) and (xj) conditional independence no relational characteristics."}, {"heading": "2.2 Graph Based Relational Features", "text": "It is not as if it is a pure problem, but a pure problem that it could be. (It is not as if it is a pure problem.) It is not as if it is a pure problem. (It is not as if it is a pure problem.) It is not as if it is a categorical variable that is true when samples are connected and false. (It is as if this information is very local and gives limited information about the position of i in Rk, we extend it to include indirect neighbors at different distances to i.) Distancer refers to the number of edges dk (i, j) to a shortest path that i and j in Rk."}, {"heading": "3 Related Work", "text": "We build on two main categories of related work: the first, in Section 3.1, uses characteristics derived from the network structure to improve iid-based conclusions; the second, in Section 3.2, considers collective classification as a common consequence problem, while at the same time deriving the class designation for each instance; and the challenges that relate specifically to problems with a few marked data points have received particular attention [4, 5, 14, 29] and helped us understand the meaning of indirect relationships."}, {"heading": "3.1 Relational Features", "text": "Relational characteristics can be combined with collective conclusions or used directly with standard ML methods, as we argue in Section 2.1. Models such as Relational1 We used the implementation with standard parameters available at http: / / glaros.dtc.umn.edu / gkhome / views / metis. Probabilistic Trees [20], Relational Bayes Classifier [21] and the Link Based Classifier (LBC, [11]) focus primarily on aggregating attributes of associated samples. Others use lines from the (weighted) adjacence matrix as the basis for constructing characteristics [1, 24, 25]."}, {"heading": "3.2 Collective Inference", "text": "Complete relational models such as Markov Logic Networks (MLN, [28]) or the Probabilistic Relational Model [31] can be used for CC [3]. We refer to Sen et al. [30] for a comprehensive overview of collective CC algorithms based on inference. Their strength in high-label autocorrelation settings and the problem of error propagation have been investigated [7, 33] and improved inference schemes proposed [15]."}, {"heading": "4 Evaluation", "text": "In our experiments we examine the following three questions: 1. Are classical ML methods with relational characteristics competitive with MLN approaches and collective classification approaches; 2. What are the main components that make relational characteristics effective; 3. Does the combination of relational and attribute information improve resentment?"}, {"heading": "4.1 Experimental Setup", "text": "Datasets We use three standard benchmark SRL datasets. The Cora and CiteSeer scientific paper collections were used in different versions, we chose versions 3 in [30] and the IMDb datasets 4. Both, Cora and CiteSeer contain text properties in the form of word bags (sheets) representations. We give some statistics of these datasets in Table 1. Benchmark models As baseline models, we use the well-established relational learning methods wvRN [12, 13], nLB [11] and MLN [3]. For MLN, we chose Relaxation Labeling [2] as the collective inference method for wvRN and nLB, as it has been shown to outperform Gibbs sampling and iterative classification on our datasets [14]. For HasWord (p, + w) = > Topic dataset (p, + t) and2 Our code is not available at https: / i.D.D.D.D: / i.D: http / / i.D.D.."}, {"heading": "4.2 Comparing Relational Features to SRL", "text": "In the first experiment, we compare wvRN and nLB with two logistic regression models that use only our relational characteristics; the first relational characteristic model (rwr) is based on a random path; the second model uses both neighborhood and aggregated characteristics (NCP) with distances of 1,2,3. Distances greater than three are excluded because almost any node can be reached from any other node over three edges, so further distances do not provide additional information. Figure 4 illustrates two problems with SRL models: (i) nLB performs poorly when labels are sparse; and (ii) wvRN is sensitive to violations of built-in characteristics - i.e. when the consistency of labeling among neighbors is not given, as in the IMDb dataschema http: / http: / / www.imdata.org /]."}, {"heading": "4.3 Engineering Relational Features", "text": "Two questions will be answered: (i) How important are indirect relationships? (ii) Which of the proposed relational characteristics lead to the best results? All the relational characteristics we are looking at can include information about indirect neighbors. Each method has parameters that adjust the locality of the resulting characteristics. We will first examine the effects of the inclusion of indirect neighbors (Figure 5) and the meaning of undescribed neighbors (Figure 6) for relational neighbors. The influence of the restart parameter r (compare Eq. 2) on the rwr characteristics can be seen in Figure 7, and an informative subset of results for different clusters is in Figure 8. The results suggest that the inclusion of indirect neighbors in the relational characteristics is beneficial regardless of whether they are used directly or for aggregation. Figure 6 shows that undescribed neighbors contribute significantly to the overall performance."}, {"heading": "4.4 Combining Relational and Local Information", "text": "In the following, we examine the effect of adding local attributes. Figure 9 shows results with properties of the neighborhood count (NCC) of distances 1,2,3. Interestingly, the word bag model performs better in Citeseer than only in network models, but worse in Cora. The combination of relational and local attributes, on the other hand, improves the results in both cases. The figure also shows that our properties exceed MLN in both datasets. In summary, our experiments suggest that the combination of relational attributes and attributes is advantageous even in a simple model like logistic regression."}, {"heading": "4.5 Discussion", "text": "Our experiments suggest that relational feature-based models can be easily compared with specialized relational learners, even in network-based and sparse label settings, and this has been verified using three standardized SRL benchmark datasets and three modern SRL comparison methods. Including indirect neighbors has proven to be extremely important, especially in sparse label settings. We have also shown that combining relational characteristics and local attributes is both simple and has the potential to significantly improve over pure feature and network models.Note that our relational characteristics can result in very high-dimensional representations. However, such feature spaces are common in recommendation systems, click-rate predictions, and web searches, where regulated logistic regression has proven to be very effective [6,10]. In addition, we use a standard implementation of logistic regression with billions of samples, which can therefore be trained."}, {"heading": "5 Conclusion", "text": "We have shown that dependencies between samples can be exploited using Relational Feature Engineering. Our method allows us to combine Relational Information from different sources with attributes associated with individual samples. We tested this using standard SRL benchmark data sets and showed that our features are competitive with specialized relational learning models even for network data. Moreover, our features can exceed them when additional information is available. Note that, unlike SRL methods, our proposal achieves these results without collective inference. While we limited our experiments to logistic regression as a predictive model, the proposed features could be used as input for other feature-based learning algorithms such as SVM, neural networks, or random forests."}], "references": [{"title": "The relational vector-space model and industry classification", "author": ["A. Bernstein", "S. Clearwater", "F. Provost"], "venue": "IJCAI workshop. Volume 266.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Enhanced hypertext categorization using hyperlinks", "author": ["S. Chakrabarti", "B. Dom", "P. Indyk"], "venue": "ACM SIGMOD Record 27(2)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Investigating markov logic networks for collective classification", "author": ["R. Crane", "L. McDowell"], "venue": "ICAART (1).", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Leveraging label-independent features for classification in sparsely labeled networks: An empirical study", "author": ["B. Gallagher", "T. Eliassi-Rad"], "venue": "In Giles, L., Smith, M., Yen, J., Zhang, H., eds.: ASONAM. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Using ghost edges for classification in sparsely labeled networks", "author": ["B. Gallagher", "H. Tong", "T. Eliassi-Rad", "C. Faloutsos"], "venue": "KDD.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Web-scale bayesian clickthrough rate prediction for sponsored search advertising in microsoft\u2019s bing search engine", "author": ["T. Graepel", "J.Q. Candela", "T. Borchert", "R. Herbrich"], "venue": "ICML.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Why collective inference improves relational classification", "author": ["D. Jensen", "J. Neville", "B. Gallagher"], "venue": "KDD.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "A fast and high quality multilevel scheme for partitioning irregular graphs", "author": ["G. Karypis", "V. Kumar"], "venue": "SIAM Journal on Scientific Computing 20(1)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Stacked graphical models for efficient inference in markov random fields", "author": ["Z. Kou", "W.W. Cohen"], "venue": "SDM, SIAM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Large-scale sparse logistic regression", "author": ["J. Liu", "J. Chen", "J. Ye"], "venue": "KDD, ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Link-based classification", "author": ["Q. Lu", "L. Getoor"], "venue": "ICML. Volume 3.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Improving learning in networked data by combining explicit and mined links", "author": ["S.A. Macskassy"], "venue": "AAAI. Volume 22.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "A simple relational classifier", "author": ["S.A. Macskassy", "F. Provost"], "venue": "KDD-Workshop.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Classification in networked data: A toolkit and a univariate case study", "author": ["S.A. Macskassy", "F. Provost"], "venue": "JMLR 8", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Cautious collective classification", "author": ["L.K. McDowell", "K.M. Gupta", "D.W. Aha"], "venue": "JMLR 10", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Parallel boosting with momentum", "author": ["I. Mukherjee", "K. Canini", "R. Frongillo", "Y. Singer"], "venue": "ECML PKDD. Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Machine learning: A probabilistic perspective", "author": ["K.P. Murphy"], "venue": "The MIT Press", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Iterative classification in relational data", "author": ["J. Neville", "D. Jensen"], "venue": "Proc. AAAI2000 Workshop on Learning Statistical Models from Relational Data.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Collective classification with relational dependency networks", "author": ["J. Neville", "D. Jensen"], "venue": "UAI.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning relational probability trees", "author": ["J. Neville", "D. Jensen", "L. Friedland", "M. Hay"], "venue": "KDD.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Simple estimators for relational bayesian classifiers", "author": ["J. Neville", "D. Jensen", "B. Gallagher"], "venue": "ICDM.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Automatic multimedia crossmodal correlation discovery", "author": ["J.Y. Pan", "H.J. Yang", "C. Faloutsos", "P. Duygulu"], "venue": "KDD, ACM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "JMLR 12", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Distribution-based aggregation for relational learning with identifier attributes", "author": ["C. Perlich", "F. Provost"], "venue": "Machine Learning 62(1)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Aggregation-based feature invention and relational concept classes", "author": ["C. Perlich", "F. Provost"], "venue": "KDD, ACM", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Relational ensemble classification", "author": ["C. Preisach", "L. Schmidt-Thieme"], "venue": "ICDM.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Scaling factorization machines to relational data", "author": ["S. Rendle"], "venue": "VLDB. Volume 6., VLDB Endowment", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine Learning 62", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Handling missing values when applying classification models", "author": ["M. Saar-Tsechansky", "F. Provost"], "venue": "JMLR", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Collective classification in network data", "author": ["P. Sen", "G.M. Namata", "M. Bilgic", "L. Getoor", "B. Gallagher", "T. Eliassi-Rad"], "venue": "AI Magazine 29(3)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "Probabilistic classification and clustering in relational data", "author": ["B. Taskar", "E. Segal", "D. Koller"], "venue": "IJCAI. Volume 17.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Fast random walk with restart and its applications", "author": ["H. Tong", "C. Faloutsos", "J.Y. Pan"], "venue": "ICDM.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Understanding propagation error and its effect on collective classification", "author": ["R. Xiang", "J. Neville"], "venue": "ICDM, IEEE", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 10, "context": "Recently, a number of methods have been proposed [11, 13, 18, 31] that significantly improves over classical methods by using joint inference.", "startOffset": 49, "endOffset": 65}, {"referenceID": 12, "context": "Recently, a number of methods have been proposed [11, 13, 18, 31] that significantly improves over classical methods by using joint inference.", "startOffset": 49, "endOffset": 65}, {"referenceID": 17, "context": "Recently, a number of methods have been proposed [11, 13, 18, 31] that significantly improves over classical methods by using joint inference.", "startOffset": 49, "endOffset": 65}, {"referenceID": 30, "context": "Recently, a number of methods have been proposed [11, 13, 18, 31] that significantly improves over classical methods by using joint inference.", "startOffset": 49, "endOffset": 65}, {"referenceID": 29, "context": "Exact joint inference has high runtime complexity which often requires approximate solutions [30].", "startOffset": 93, "endOffset": 97}, {"referenceID": 29, "context": "These approximated joint inference techniques introduce new difficulties such as the need for specialized implementations that are expensive to run and difficult to tune [30].", "startOffset": 170, "endOffset": 174}, {"referenceID": 6, "context": "Note that on these datasets, collective classification are considered the best performing methods in the current literature [7, 14].", "startOffset": 124, "endOffset": 131}, {"referenceID": 13, "context": "Note that on these datasets, collective classification are considered the best performing methods in the current literature [7, 14].", "startOffset": 124, "endOffset": 131}, {"referenceID": 16, "context": "[17]) \u03b8\u2217 := arg max \u03b8 p(\u03b8|D) = arg max \u03b8 p(D|\u03b8) where l(\u03b8) := p(D|\u03b8) is called the likelihood.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Neighbor Ids Encoding the direct neighbors of a sample i in relation Rk can be achieved by treating each sample as a categorical variable which is true when samples are connected and false otherwise [1,24,25].", "startOffset": 199, "endOffset": 208}, {"referenceID": 23, "context": "Neighbor Ids Encoding the direct neighbors of a sample i in relation Rk can be achieved by treating each sample as a categorical variable which is true when samples are connected and false otherwise [1,24,25].", "startOffset": 199, "endOffset": 208}, {"referenceID": 24, "context": "Neighbor Ids Encoding the direct neighbors of a sample i in relation Rk can be achieved by treating each sample as a categorical variable which is true when samples are connected and false otherwise [1,24,25].", "startOffset": 199, "endOffset": 208}, {"referenceID": 19, "context": "Aggregated Neighbor Attributes Relational position can also be described by individual features of direct and indirect neighbors [20, 21].", "startOffset": 129, "endOffset": 137}, {"referenceID": 20, "context": "Aggregated Neighbor Attributes Relational position can also be described by individual features of direct and indirect neighbors [20, 21].", "startOffset": 129, "endOffset": 137}, {"referenceID": 21, "context": "They have been proposed as a similarity measure in the context of auto-captioning of images [22].", "startOffset": 92, "endOffset": 96}, {"referenceID": 31, "context": "P can be determined as the solution of a linear system or approximated efficiently [32], leaving r as free parameter.", "startOffset": 83, "endOffset": 87}, {"referenceID": 7, "context": "The clusters can be calculated with negligible runtime using the METIS clustering framework [8].", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "The challenges specific to problems with few labeled data points have received special attention [4, 5, 14, 29] and helped us to understand the importance of indirect relations.", "startOffset": 97, "endOffset": 111}, {"referenceID": 4, "context": "The challenges specific to problems with few labeled data points have received special attention [4, 5, 14, 29] and helped us to understand the importance of indirect relations.", "startOffset": 97, "endOffset": 111}, {"referenceID": 13, "context": "The challenges specific to problems with few labeled data points have received special attention [4, 5, 14, 29] and helped us to understand the importance of indirect relations.", "startOffset": 97, "endOffset": 111}, {"referenceID": 28, "context": "The challenges specific to problems with few labeled data points have received special attention [4, 5, 14, 29] and helped us to understand the importance of indirect relations.", "startOffset": 97, "endOffset": 111}, {"referenceID": 19, "context": "Probabilistic Trees [20], Relational Bayes Classifier [21] and the Link Based Classifier (LBC, [11]) concentrate primarily on the aggregation of attributes of connected samples.", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "Probabilistic Trees [20], Relational Bayes Classifier [21] and the Link Based Classifier (LBC, [11]) concentrate primarily on the aggregation of attributes of connected samples.", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "Probabilistic Trees [20], Relational Bayes Classifier [21] and the Link Based Classifier (LBC, [11]) concentrate primarily on the aggregation of attributes of connected samples.", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "Others use rows from the (weighted) adjacency matrix as basis for feature construction [1, 24, 25].", "startOffset": 87, "endOffset": 98}, {"referenceID": 23, "context": "Others use rows from the (weighted) adjacency matrix as basis for feature construction [1, 24, 25].", "startOffset": 87, "endOffset": 98}, {"referenceID": 24, "context": "Others use rows from the (weighted) adjacency matrix as basis for feature construction [1, 24, 25].", "startOffset": 87, "endOffset": 98}, {"referenceID": 25, "context": "We were especially inspired from the suggestion to extend the neighborhood of samples with few neighbors with distance-two neighborhoods [26] or ghost edges [5].", "startOffset": 137, "endOffset": 141}, {"referenceID": 4, "context": "We were especially inspired from the suggestion to extend the neighborhood of samples with few neighbors with distance-two neighborhoods [26] or ghost edges [5].", "startOffset": 157, "endOffset": 160}, {"referenceID": 27, "context": "Full relational models such as Markov Logic Networks (MLN, [28]) or the Probabilistic Relational Model [31] can be used for CC [3].", "startOffset": 59, "endOffset": 63}, {"referenceID": 30, "context": "Full relational models such as Markov Logic Networks (MLN, [28]) or the Probabilistic Relational Model [31] can be used for CC [3].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "Full relational models such as Markov Logic Networks (MLN, [28]) or the Probabilistic Relational Model [31] can be used for CC [3].", "startOffset": 127, "endOffset": 130}, {"referenceID": 29, "context": "[30] for a comprehensive overview of collective inference based CC algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Their strength in high label autocorrelation settings and the problem of error propagation has been examined [7, 33] and improved inference schemes have been proposed [15].", "startOffset": 109, "endOffset": 116}, {"referenceID": 32, "context": "Their strength in high label autocorrelation settings and the problem of error propagation has been examined [7, 33] and improved inference schemes have been proposed [15].", "startOffset": 109, "endOffset": 116}, {"referenceID": 14, "context": "Their strength in high label autocorrelation settings and the problem of error propagation has been examined [7, 33] and improved inference schemes have been proposed [15].", "startOffset": 167, "endOffset": 171}, {"referenceID": 8, "context": "Recently, stacking of non relational model has been introduced [9] as a fast approximation of Relational Dependency Networks [19].", "startOffset": 63, "endOffset": 66}, {"referenceID": 18, "context": "Recently, stacking of non relational model has been introduced [9] as a fast approximation of Relational Dependency Networks [19].", "startOffset": 125, "endOffset": 129}, {"referenceID": 29, "context": "The Cora and CiteSeer scientific paper collections have been used in different versions, we chose the versions presented in [30] and the IMDb dataset.", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "Benchmark Models As baseline models we use the well established relational learning methods wvRN [12, 13], nLB [11] and MLN [3].", "startOffset": 97, "endOffset": 105}, {"referenceID": 12, "context": "Benchmark Models As baseline models we use the well established relational learning methods wvRN [12, 13], nLB [11] and MLN [3].", "startOffset": 97, "endOffset": 105}, {"referenceID": 10, "context": "Benchmark Models As baseline models we use the well established relational learning methods wvRN [12, 13], nLB [11] and MLN [3].", "startOffset": 111, "endOffset": 115}, {"referenceID": 2, "context": "Benchmark Models As baseline models we use the well established relational learning methods wvRN [12, 13], nLB [11] and MLN [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "We chose relaxation labeling [2] as the collective inference method for wvRN and nLB as it has been shown to outperform Gibbs sampling and iterative classification on our datasets [14].", "startOffset": 29, "endOffset": 32}, {"referenceID": 13, "context": "We chose relaxation labeling [2] as the collective inference method for wvRN and nLB as it has been shown to outperform Gibbs sampling and iterative classification on our datasets [14].", "startOffset": 180, "endOffset": 184}, {"referenceID": 2, "context": "Topic(p,t)^Link(p,p\u2019)=>Topic(p\u2019,t) together with discriminative weight learning and MC-SAT inference as recommended for Cora and Citeseer in a previous study [3].", "startOffset": 158, "endOffset": 161}, {"referenceID": 13, "context": "Measures & Protocol We follow [14] and remove samples that are not connected to any other sample (singletons) in all experiments.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "We used the netkit-srl framework [14] in version (1.", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "The graph clusterings were obtained using METIS [8] version 5.", "startOffset": 48, "endOffset": 51}, {"referenceID": 22, "context": "Relational features are learned with an L2 penalized logistic regression model included in scikit-learn [23] version 0.", "startOffset": 104, "endOffset": 108}, {"referenceID": 13, "context": "8 This has been attributed to a lack of training data [14].", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "Such feature spaces are, however, common in recommender systems, click-through rate prediction and websearch where regularized logistic regression has been shown to be very effective [6,10].", "startOffset": 183, "endOffset": 189}, {"referenceID": 9, "context": "Such feature spaces are, however, common in recommender systems, click-through rate prediction and websearch where regularized logistic regression has been shown to be very effective [6,10].", "startOffset": 183, "endOffset": 189}, {"referenceID": 15, "context": "In addition, we use a standard implementation of logistic regression and can consequently employ scalable versions that can be trained with billions of samples of high dimensions [16, 27].", "startOffset": 179, "endOffset": 187}, {"referenceID": 26, "context": "In addition, we use a standard implementation of logistic regression and can consequently employ scalable versions that can be trained with billions of samples of high dimensions [16, 27].", "startOffset": 179, "endOffset": 187}], "year": 2017, "abstractText": "Statistical Relational Learning (SRL) methods have shown that classification accuracy can be improved by integrating relations between samples. Techniques such as iterative classification or relaxation labeling achieve this by propagating information between related samples during the inference process. When only a few samples are labeled and connections between samples are sparse, collective inference methods have shown large improvements over standard feature-based ML methods. However, in contrast to feature based ML, collective inference methods require complex inference procedures and often depend on the strong assumption of label consistency among related samples. In this paper, we introduce new relational features for standard ML methods by extracting information from direct and indirect relations. We show empirically on three standard benchmark datasets that our relational features yield results comparable to collective inference methods. Finally we show that our proposal outperforms these methods when additional information is available.", "creator": "LaTeX with hyperref package"}}}