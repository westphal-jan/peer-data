{"id": "1702.02897", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2017", "title": "Online and Offline Domain Adaptation for Reducing BCI Calibration Effort", "abstract": "Many real-world brain-computer interface (BCI) applications rely on single-trial classification of event-related potentials (ERPs) in EEG signals. However, because different subjects have different neural responses to even the same stimulus, it is very difficult to build a generic ERP classifier whose parameters fit all subjects. The classifier needs to be calibrated for each individual subject, using some labeled subject-specific data. This paper proposes both online and offline weighted adaptation regularization (wAR) algorithms to reduce this calibration effort, i.e., to minimize the amount of labeled subject-specific EEG data required in BCI calibration, and hence to increase the utility of the BCI system. We demonstrate using a visually-evoked potential oddball task and three different EEG headsets that both online and offline wAR algorithms significantly outperform several other algorithms. Moreover, through source domain selection, we can reduce their computational cost by about 50%, making them more suitable for real-time applications.", "histories": [["v1", "Thu, 9 Feb 2017 17:03:12 GMT  (807kb)", "http://arxiv.org/abs/1702.02897v1", "in press"]], "COMMENTS": "in press", "reviews": [], "SUBJECTS": "cs.LG cs.HC", "authors": ["dongrui wu"], "accepted": false, "id": "1702.02897"}, "pdf": {"name": "1702.02897.pdf", "metadata": {"source": "CRF", "title": "Online and Offline Domain Adaptation for Reducing BCI Calibration Effort", "authors": ["Dongrui Wu"], "emails": ["drwu09@gmail.com"], "sections": [{"heading": null, "text": "It is one of the largest hubs in the world, where it is a question of obtaining more than one million euros."}, {"heading": "II. WEIGHTED ADAPTATION REGULARIZATION WITH SOURCE DOMAIN SELECTION (WARSDS)", "text": "This section describes the offline wARSDS algorithm [51], [53] which originates in adaptation regulation - the algorithm of regulated smallest squares (ARRLS) in [25]. We have made some significant improvements to ARRLS to better handle classification balances and multiple source domains, and also to use marked samples in the target domain. wARSDS first uses source domain selection (SDS) to select the closest source domains for a specific target domain, then uses weighted adaptation regulation (WAR) for each selected source domain to form individual classifiers, and finally performs a model fusion. For simplicity, we will only consider the 2-class classification."}, {"heading": "A. wAR: Problem Definition", "text": "A domain [25], [32] D in TL usually consists of a multidimensional attribute space X and a marginal probability distribution P (x), i.e., D = {X, P (x)}, where the two domains Ds and Dt differ if Xs 6 = Xt, and / or Ps (x) 6 = Pt (x).A task [25], [32] T in TL consists of a tag space Y and a conditional probability distribution Q (y | x). Two tasks Ts and Tt differ if Ys 6 = Yt, or Qs (y | x) 6 = Qt (y | x).Given a source domain Ds with n-designated samples, {x1, y1), EG (xn, yn)}, and a target domain (xn), the target domain X (xn + 1, yn + 1), is a domain."}, {"heading": "B. wAR: The Learning Framework", "text": "It is not that we focus only on the first two prerequisites by taking on similar VEP tasks [so Ps (y) and Pt (y) are intrinsically close. Our future research will consider the more general case that Ps (y) and Pt (y) are different."}, {"heading": "C. wAR: Loss Functions Minimization", "text": "Lety = [y1,..., yn + ml + mu] T (7), where {y1,..., yn} are known names in the source domain, are {yn + 1,..., yn + ml} names known in the target domain, and {yn + ml + 1,..., yn + ml + mu} are pseudo-names for the unlabeled target domain samples, i.e. names estimated on the basis of available sample information both in the source domain and in the target domain. Let us define the square loss in this paper: (f (xi), yi) = (yi \u2212 f (xi)) 2 (9) \u2212 \u2212 \u2212 in the first two terms in (2), we have used the square loss in this paper: (n + ml + mu) \u00b7 (yi), yi) = (yi \u2212 f (i) (i) (i) (i) (i) (i \u2212 ml (i) (i) (i \u2212 ml (i), i (i \u2212 ml) (i \u2212 ml) (i) (i (i \u2212 ml), i (i) (i (i) (i \u2212 ml) (i (i \u2212 ml), i (i) (i (i) (i (i \u2212 ml), i (i) (i (i) (i (i \u2212 ml), i (i (i), i (i (i), i (i), i (i (\u2212 ml), i (i), i (i (i), i (i), i (\u2212 ml), i (i (i), i (i), i (i), i (i (i), i (\u2212 ml), i (i (i), i (i), i (i (i), i (i (i), i (\u2212 ml), i (i (i), i (i), i (i), i (i), i (i (i), i (i (i), i (i), i (\u2212 ml, i), i (i), i (i (i), i (i (i), i (i), i (i), i (\u2212 ml, i (i), i (i), i"}, {"heading": "D. wAR: Structural Risk Minimization", "text": "As in [25], we define structural risk as the square norm of f in HK, i.e., for f-2K = n + ml + mu-i = 1n + ml + mu-j = 1\u03b1i\u03b1jK (xi, xj) = \u03b1 TK\u03b1 (11)"}, {"heading": "E. wAR: Marginal Probability Distribution Adaptation", "text": "As in [25], we calculate Df, K (Ps, Pt) based on the projected maximum mean discrepancy (MMD) between the source and target domains: Df, K (Ps, Pt) = [1nn \u2211 i = 1f (xi) \u2212 1ml + mun + ml + mu \u2211 i = n + 1f (xi)] 2 = \u03b1TKM0K\u03b1 (12), where M0-R (n + ml + mu) \u00d7 (n + ml + mu) is the MMD matrix: (M0) ij = 1 n2, 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 n (ml + mu) 2, n + 1 \u2264 i \u2264 n (ml + mu), n + 1 \u2264 j \u2264 n + ml + mu, n + 1 \u2264 n + ml + mu \u2212 1n (ml + mu), otherwise (13)"}, {"heading": "F. wAR: Conditional Probability Distribution Adaptation", "text": "As in [25], we first calculate pseudo-labels for the blank target domain samples and construct the label vector y in (7). These pseudo-labels can be calculated using the classifier built into the previous iteration, if wAR is used iteratively or estimated using another classifier, e.g. a support vector machine (SVM) [45]. We then calculate the projected MMD for each class. Let Ds, c = {xi | xi-Ds, i = c, i = 1,..., n} leave the set of samples in class c of the source domain, c \u2212 xxi \u2212 yj c = {xj | xj-Dt = c, j = n + 1,..., n + ml + mu} the set of samples in class c of the target domain, nc (\u03b1) the number of elements in class c, c \u2212 xxi \u2212 yj c = c, then c \u2212 c = the distance between class 1D = 1b = 1D, 1D = 1D = 1b, 1D = 1D."}, {"heading": "G. wAR: The Closed-Form Solution", "text": "By replacing (10), (11), (12) and (15) with (2), we must set the derivative of the objective function of HK (yT \u2212 \u03b1TK) E (y \u2212 K\u03b1) + \u03c3\u03b1TK\u03b1 + \u03bb\u03b1TK (M0 + M) K\u03b1 (18) to 0, which results in the following closed solution for \u03b1: \u03b1 = [(E + \u03bbM0 + \u03bbM) K + \u03c3I] \u2212 1Ey (19)"}, {"heading": "H. Source Domain Selection (SDS)", "text": "If there are multiple source domains, it is very time consuming to perform wAR for each source domain and then aggregate the results. Additionally, aggregating results from source domains that are outliers or very different from the target domain can also affect classification performance. Therefore, we are introducing an approach to source domain selection [53] that selects the closest source domains to reduce computing costs and also (potentially) improve classification performance.Suppose there are Z different source domains. For the zth source domain, we first calculate mz, c (c = 1, 2), the middle feature vector of each class. Then we calculate mt mt mt mt mt, c, the middle feature vector of each target domain class, using the ml known labels and the pseudo-labels, calculating the distance between the two domains as clusters: d mt mt mt, c (z \u00b2, c = 2 kz), we are only (20 kz)."}, {"heading": "I. The Complete wARSDS Algorithm", "text": "The pseudo code for the full WARSDS algorithm is represented in Algorithm 1. It first uses SDS to select the closest source domains, then performs WAR separately for each of them to create individual classifiers, and finally aggregates them based on a weighted average, with weights representing the corresponding training accuracy."}, {"heading": "J. Discussions", "text": "As mentioned at the beginning of this section, the formulation and derivation of wAR are similar to the ARRLS algorithm in [25]; however, there are several key differences: 1) wAR assumes that a subject or oracle is available to label a small number of samples in the target domain, while ARRLS assumes that all samples in the target domain are not labeled. As a result, wAR can be iterative, and the classifier can be updated each time new samples are available in the target domain. 2) wAR explicitly addresses the problem of class imbalance in both source and target domains by introducing class-dependent weights in samples. As we will show in Section IV, this makes a huge difference in the balanced classification accuracy of the class imbalance, which is intrinsic in ERP-based BCI systems. 3) RLS also includes multiple regulations, but we could not find it contained in our documentation [3]."}, {"heading": "III. ONLINE WEIGHTED ADAPTATION REGULARIZATION WITH SOURCE DOMAIN SELECTION (OWARSDS)", "text": "This section introduces the OwARSDS algorithm [52], which extends the offline wARSDS algorithm to online BCI calibration. OwARSDS first uses SDS to select the closest source domains, then performs an online weighted fit regulation (OwAR) for each selected source domain to create individual classifiers, and finally aggregates them."}, {"heading": "A. OwAR: The Learning Framework", "text": "With the notations presented in the previous section, the learning framework of OwAR can still be formulated as follows: (2). However, since there are no unmarked target domain samples in the online calibration, the core matrix has co-dimensionality (n + ml) \u00b7 (n + ml) instead of (n + ml + mu) \u00b7 (n + ml + mu) in the offline calibration. As a result, the solution of (2) allows for another expression: fo (x) = n + ml \u2211 i = 1\u03b1oiK o (xi, x) = (\u03b1 o) TKo (Xo, x) (21) where Xo = [x1,..., xn + ml] T (22) and \u03b1o = [\u03b11,..., \u03b1n + ml] T coefficients to be calculated. It has been shown [52] that the closed solution for the affois: \u03b1 o = [(Eo + Moq0 + Mogo 0 + 23) \u2212 n is introduced below."}, {"heading": "B. OwAR: Loss Functions Minimization", "text": "Definitely o = [y1,..., yn + ml] T (24), where {y1,..., yn} are known in the source domain, and {yn + 1,..., yn + ml} are known in the target domain. Also, define Eo-R (n + ml) \u00d7 (n + ml) as a diagonal matrix with Eoii = {ws, i, 1 \u2264 i \u2264 n wtwt, i, n + 1 \u2264 i \u2264 n + ml (25). After deriving in (10) we then have n \u2211 i = 1ws, i (f o (xi), yi) + wtn + ml \u0445 i = n + 1wt, i (f o (xi), yi) = [(yo) T \u2212 (\u03b1o) TKo] Eo (yo \u2212 Koo) (26)"}, {"heading": "C. OwAR: Structural Risk Minimization", "text": "Here, too, we define structural risk as the quadratic norm of fo in HK, i.e. that fo-2K = (\u03b1 o) TKo\u03b1o (27)"}, {"heading": "D. OwAR: Marginal Probability Distribution Adaptation", "text": "We calculate Dfo, Ko (Ps, Pt) using the projected MMD between source and target = j = j = samples: Dfo, Ko (Ps, Pt) = [1nn \u2211 i = 1fo (xi) \u2212 1mln + ml \u2211 i = n + 1fo (xi)] 2 = (\u03b1o) TKoMo0K o o o o (28), where Mo0 + 1 \u00b2 R (n + ml) \u00d7 (n + ml) is the MMD matrix: (Mo0) ij = 1 n2, 1 \u2264 n, 1 \u2264 n1 \u2264 n1 \u2264 n, ml, n \u2264 n ml \u2212 ml, otherwise (29).The offline wARSDS algorithms [51], [53].Input: Z source domains where the zth (z = 1, Z) domain has patterns."}, {"heading": "E. OwAR: Conditional Probability Distribution Adaptation", "text": "In order to minimize the discrepancy between the conditional probability distributions in the source and target domains, we must first calculate the pseudo-labels for the unmarked pattern data of the target domain. In the online calibration, this step is skipped because there are no unmarked pattern data of the target domain. After deriving (15), we still have: Dfo, Ko (Qs, Qt) = (\u03b1 o) TKoMoKo\u03b1o (30), where Mo is still calculated with (16), but only with the patterns of the n source domain and ml pattern data of the target domain."}, {"heading": "F. Source Domain Selection (SDS)", "text": "The SDS procedure in OwARSDS is almost identical to that in wARSDS. The only difference is that the latter also uses the unlabeled samples of the target domain mu in mt, c in (20), while the former uses only the ml-labeled samples of the target domain, because there are no unlabeled samples of the target domain in the online calibration."}, {"heading": "G. The Complete OwARSDS Algorithm", "text": "The pseudo code for the full OwARSDS algorithm is described in Algorithm 2. It first uses SDS to select the closest source domains, then performs OwAR for each of them separately to create individual classifiers, and finally aggregates them based on a weighted average, with weights being the appropriate training accuracy. Note that OwARSDS WARSDS is very similar, the main difference being that there are no unlabeled target domain patterns available for use in OwARSDS."}, {"heading": "IV. THE VEP ODDBALL EXPERIMENT", "text": "This section describes the structure of the VEP Oddball experiment, which is used in the following three sections to evaluate the performance of different algorithms."}, {"heading": "A. Experiment Setup", "text": "A two-voice VEP transverse head task was used [38], in which participants sat in a sound-attenuated recording chamber and were presented with image stimuli at 0.5 Hz (one image every two seconds).The images (152 x 375 pixels) presented for 150 ms in the center of a 24-inch Dell P2410 monitor at a distance of approximately 70 cm were either an enemy fighter [target; an example is shown in Figure 1 (a))) or a U.S. soldier [non-target; an example is shown in Figure 1 (b)].Subjects were instructed to maintain their fixation on the center of the image and identify each image as a target or non-target as quickly and accurately as possible at the press of a single2 button.A total of 270 images were presented to each subject, including 34 targets. The experiments were approved by the U.S. Army Research Laboratory (ARL) Institutional Review."}, {"heading": "B. Preprocessing and Feature Extraction", "text": "The preprocessing and feature extraction method for all three headsets was the same, except that we used all channels for ABM and Emotiv headsets, but for the BioSemi headset we used only 21 channels (Cz, Fz, P1, P3, P5, P7, PO7, PO3, O1, Oz, POz, Pz, P2, P4, P6, P8, P10, PO8, PO4, O2) mainly in the parietal and occipital ranges, as in [48].EEGLAB [12] was used for EEG signal preprocessing and feature extraction. For each headset, we first captured the EEG signals to [1, 50] Hz, then calculated them to 64 Hz, carried out average reference values, and then adjusted the stimulus to the [0, 0.7] second interval."}, {"heading": "C. Performance Measure", "text": "Let m + and m \u2212 be the true number of epochs from the target or non-target class. Let m + and m \u00b2 \u2212 be the number of epochs correctly classified by an algorithm as the target or non-target class. Then, we calculate + = m + +, a \u2212 = m \u2212 m \u2212 where a + is the classification accuracy in the target class and a \u2212 the classification accuracy in the non-target class. The following Balanced Classification Accuracy (BCA) was then used as a performance measure in this essay: BCA = a + + a \u2212 2. (31)"}, {"heading": "V. PERFORMANCE EVALUATION OF OFFLINE CALIBRATION ALGORITHMS", "text": "This selection represents a performance comparison of WARSDS with several other offline calibration algorithms."}, {"heading": "A. Calibration Scenario", "text": "Although we know the labels of all EEG epochs for all 14 subjects in the VEP experiment, we simulated a realistic offline calibration scenario: We had labeled EEG epochs from 13 subjects and also all epochs from the 14th subject, but initially none of them were labeled. Our goal was to label epochs from the 14th subject iteratively and build a classifier so that his / her remaining epochs can be reliably classified. The flow chart for the simulated offline calibration scenario is shown in Figure 2 (a). Assuming the 14th subject has labeled sequence epochs in the VEP experiment, we want to label p epochs in each iteration, starting from zero. We generate a random number m0 [1, m] which represents the starting position in the VEP sequence epoch."}, {"heading": "B. Algorithms", "text": "We compared the performance of wARSDS with six other algorithms [53]: 1) BL1, a basic approach in which we assume we know the labels of all samples of the new subject, and use 5x cross-validation and SVM to find the highest BCA. This represents an upper limit of the BCA that we can obtain by using only the data of the new subject. 2) BL2, which is a simple iterative procedure: in each iteration, we randomly select five unlabeled samples of the new subject to label them, and then form an SVM classifier by 5x cross-validation. We iterate until the maximum number of iterations is reached. 3) TL, which is the TL algorithm introduced in [48]. It simply combines the labeled samples of the new subject with samples from each existing subject and forms an SVM classifier. The final cross-weighted is a more appropriate mean of all the respective classifications."}, {"heading": "C. Experimental Results", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "D. Parameter Sensitivity Analysis", "text": "To save space, we only show the BCA results for the BioSemi headset. Similar results were obtained from the other two headsets. The average BCAs of WAR and WARSDS for different \u03c3 (\u03bbP and \u03bbQ were set to 10) are shown in Fig. 5 (a), and for the other two headsets, the BCAs of wAR and wARSDS are always identical because they are conceptually close to each other. Consider Fig. 5 (b) that both wAR and wARSDS achieved good BCAs for \u03c3 [0.0001, 1], and from Fig. 5 (b) that both wAR and wARSDS achieved good BCAs for \u03bbP [10, 100]. Furthermore, this indicative value increases when the effects of the BCAs are described as minimitive."}, {"heading": "VI. PERFORMANCE EVALUATION OF ONLINE CALIBRATION ALGORITHMS", "text": "This selection compares the performance of OwARSDS with several other online calibration algorithms."}, {"heading": "A. Online Calibration Scenario", "text": "Although we knew the labels of all EEG epochs for all 14 subjects, we simulated a realistic online calibration scenario: We had labeled EEG epochs from 13 subjects, but initially no epochs from the 14th subject; we created labeled epochs from the 14th subject iteratively and sequentially on-the-fly, which were used to train a classifier to label the remaining epochs of this subject. The flow chart for the simulated online calibration scenario is shown in Figure 2 (b). Compared to the offline calibration scenario in Figure 2 (a), the main difference is that the offline calibration has access to all m unlabeled samples from the 14th subject, but the online calibration does not. Specifically, we assume that each subject has the epoch and epoch labels in the 14th epoch sequence."}, {"heading": "B. Online Calibration Algorithms", "text": "We compared the performance of OwARSDS with five other algorithms: 1) BL1 in section V-B. 2) BL2 in section V-B, using different PCA characteristics. 3) TL in section V-B, using different PCA characteristics. 4) TLSDS, which is the above-mentioned TL algorithm with SDS. 5) OwAR, using all existing subjects instead of performing SDS. Again, weighted libSVM [8] with RBF kernel was used as classifier in BL1, BL2, TL and TLSDS. We chose wt = 2, \u03c3 = 0.1 and \u03bb = 10. Note that the online algorithms still used PCA characteristics, but they were calculated differently than those in the offline calibration. In the offline calibration, we had access to the ml-marked samples plus the mu-unmarked samples, so that each basmu and all PCM samples were pre-calculated."}, {"heading": "C. Experimental Results", "text": "The BCAs of the six algorithms, averaged over the 30 runs and over the 14 subjects, are shown in Figure 6 for various EEG headsets. Observations in Section V-C for offline calibration still hold here, except that ARRLS was not included in the online calibration. In particular, OwAR and OwARSDS performed much better than BL2, TL and TLSDS. However, instead of using 13 auxiliary subjects in OwAR, OwARSDS were used on average only 6.51 subjects for BioSemi, and 7.09 subjects for Emotiv, corresponding to 49.77% and 45.46% for the cost savings in OwAR. Friedman's test showed significant differences in performance between the five algorithms (excluding BL1, which are non-iterative)."}, {"heading": "VII. COMPARISON OF OFFLINE AND ONLINE ALGORITHMS", "text": "This section compares the performance of wARSDS and OwARSDS (wAR and OwAR).intuitively, we expect the performance of offline calibration algorithms to be better than their online counterparts, because: 1) offline calibration uses all epochs to calculate PCA bases, while online calibration allows the use of more information. In other words, offline calibration is more representative, and 2) offline calibration uses the epochs in optimization, whereas online calibration does not work, so the calibration assumes more information."}, {"heading": "VIII. CONCLUSIONS", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "ACKNOWLEDGEMENT", "text": "The author thanks Scott Kerick, Jean Vettel, Anthony Ries and David W. Hairston of the U.S. Army Research Laboratory (ARL) for designing the experiment and collecting the data, and Brent J. Lance and Vernon J. Lawhern of the ARL for helpful discussions. Sponsored by the U.S. Army Research Laboratory, the research was conducted under cooperation numbers W911NF-10-2-0022 and W911NF-10-D-0002 / TO 0023. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing, explicitly or implicitly, the official policy of the U.S. Army Research Laboratory or the U.S. Government."}], "references": [{"title": "Multitask learning for brain-computer interfaces", "author": ["M. Alamgir", "M. Grosse-Wentrup", "Y. Altun"], "venue": "Proc. 13th Int\u2019l Conf. on Artificial Intelligence and Statistics (AISTATS), Sardinia, Italy, May 2010, pp. 17\u201324.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiclass braincomputer interface classification by Riemannian geometry", "author": ["A. Barachant", "S. Bonnet", "M. Congedo", "C. Jutten"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 59, no. 4, pp. 920\u2013928, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 2399\u20132434, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Controlling the false discovery rate: A practical and powerful approach to multiple testing", "author": ["Y. Benjamini", "Y. Hochberg"], "venue": "Journal of the Royal Statistical Society, Series B (Methodological), vol. 57, pp. 289\u2013 300, 1995.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Brain activity-based image classification from rapid serial visual presentation", "author": ["N. Bigdely-Shamlo", "A. Vankov", "R. Ramirez", "S. Makeig"], "venue": "IEEE Trans. on Neural Systems and Rehabilitation Engineering, vol. 16, no. 5, pp. 432\u2013441, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Amplitude differences of evoked alpha and gamma oscillations in two different age groups", "author": ["D. Bottger", "C.S. Herrmann", "D.Y. von Cramon"], "venue": "International Journal of Psychophysiology, vol. 45, pp. 245\u2013251, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Visual evoked potentials: Phenotypic and genotypic variability", "author": ["K.B. Bulayeva", "T.A. Pavlova", "G.G. Guseynov"], "venue": "Behavior Genetics, vol. 23, no. 5, pp. 443\u2013447, 1993.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Trans. on Intelligent Systems and Technology, vol. 2, no. 3, pp. 27:1\u201327:27, 2011, software available at http://www.csie.ntu.edu.tw/$\\sim$cjlin/libsvm.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "A two-stage model for multiple target detection in rapid serial visual presentation", "author": ["M.M. Chun", "M.C. Potter"], "venue": "Journal of Experimental Psychology: Human Perception and Performance, vol. 21, no. 1, pp. 109\u2013127, 1995.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Where and when the anterior cingulate cortex modulates attentional response: Combined fMRI and ERP evidence", "author": ["S. Crottaz-Herbette", "V. Menon"], "venue": "Journal of Cognitive Neuroscience, vol. 18, no. 5, pp. 766\u2013780, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis", "author": ["A. Delorme", "S. Makeig"], "venue": "Journal of Neuroscience Methods, vol. 134, pp. 9\u201321, 2004.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Multiple comparisons among means", "author": ["O. Dunn"], "venue": "Journal of the American Statistical Association, vol. 56, pp. 62\u201364, 1961.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1961}, {"title": "Multiple comparisons using rank sums", "author": ["O. Dunn"], "venue": "Technometrics, vol. 6, pp. 214\u2013252, 1964.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1964}, {"title": "A comparison of alternative tests of significance for the problem of m rankings", "author": ["M. Friedman"], "venue": "The Annals of Mathematical Statistics, vol. 11, no. 1, pp. 86\u201392, 1940.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1940}, {"title": "Beamforming in noninvasive brain computer interfaces", "author": ["M. Grosse-Wentrup", "C. Liefhold", "K. Gramann", "M. Buss"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 56, no. 4, pp. 1209\u20131219, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "What is odd in the oddball task? Prefrontal cortex is activated by dynamic changes in response strategy", "author": ["S.A. Huettel", "G. McCarthy"], "venue": "Neuropsychologia, vol. 42, pp. 379\u2013386, 2004.  13", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "The class imbalance problem: A systematic study", "author": ["N. Japkowicz", "S. Stephen"], "venue": "Intelligent Data Analysis, vol. 6, no. 5, pp. 429\u2013449, 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Transfer learning in brain-computer interfaces", "author": ["V. Jayaram", "M. Alamgir", "Y. Altun", "B. Scholkopf", "M. Grosse- Wentrup"], "venue": "IEEE Computational Intelligence Magazine, vol. 11, no. 1, pp. 20\u201331, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "A P300 BCI for the masses: Prior information enables instant unsupervised spelling", "author": ["P.-J. Kindermans", "H. Verschore", "D. Verstraeten", "B. Schrauwen"], "venue": "Proc. Neural Information Processing Systems (NIPS), Lake Tahoe, NV, December 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Aging effect in pattern, motion and cognitive visual evoked potentials", "author": ["M. Kuba", "J. Kremlacek", "J. Langrova", "Z. Kubova", "J. Szanyi", "F. Vt"], "venue": "Vision Research, vol. 62, no. 1, pp. 9\u201316, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient labeling of EEG signal artifacts using active learning", "author": ["V.J. Lawhern", "D.J. Slayback", "D. Wu", "B.J. Lance"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Hong Kong, October 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint feature re-extraction and classification using an iterative semi-supervised support vector machine algorithm", "author": ["Y. Li", "C. Guan"], "venue": "Machine Learning, vol. 71, no. 1, pp. 33\u201353, 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Active class selection", "author": ["R. Lomasky", "C.E. Brodley", "M. Aernecke", "D. Walt", "M. Friedl"], "venue": "Proc. 18th European Conference on Machine Learning, Warsaw, Poland, September 2007, pp. 640\u2013647.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptation regularization: A general framework for transfer learning", "author": ["M. Long", "J. Wang", "G. Ding", "S.J. Pan", "P.S. Yu"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 26, no. 5, pp. 1076\u20131089, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Class imbalance problem in data mining: Review", "author": ["R. Longadge", "S.S. Dongre", "L. Malik"], "venue": "Int\u2019l J. of Computer Science and Network, vol. 2, no. 1, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning from other subjects helps reducing brain-computer interface calibration time", "author": ["F. Lotte", "C. Guan"], "venue": "Proc. IEEE Int\u2019l. Conf. on Acoustics Speech and Signal Processing (ICASSP), Dallas, TX, March 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Regularizing common spatial patterns to improve BCI designs: Unified theory and new algorithms", "author": ["F. Lotte", "C. Guan"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 58, no. 2, pp. 355\u2013362, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Signal processing approaches to minimize or suppress calibration time in oscillatory activity-based brain-computer interfaces", "author": ["F. Lotte"], "venue": "Proc. of the IEEE, vol. 103, no. 6, pp. 871\u2013890, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Evolving signal processing for brain-computer interfaces", "author": ["S. Makeig", "C. Kothe", "T. Mullen", "N. Bigdely-Shamlo", "Z. Zhang", "K. Kreutz-Delgado"], "venue": "Proc. of the IEEE, vol. 100, no. 3, pp. 1567\u20131584, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Improved neural signal classification in a rapid serial visual presentation task using active learning", "author": ["A. Marathe", "V. Lawhern", "D. Wu", "D. Slayback", "B. Lance"], "venue": "IEEE Trans. on Neural Systems and Rehabilitation Engineering, vol. 24, no. 3, pp. 333\u2013343, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345\u20131359, 2010.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Spatiotemporal linear decoding of brain state", "author": ["L. Parra", "C. Christoforou", "A. Gerson", "M. Dyrholm", "A. Luo", "M. Wagner", "M. Philiastides", "P. Sajda"], "venue": "IEEE Signal Processing Magazine, vol. 25, no. 1, pp. 107\u2013115, 2008.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Affective Computing", "author": ["R. Picard"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1997}, {"title": "Closing the loop in cortically-coupled computer vision: a brain-computer interface for searching image databases", "author": ["E.A. Pohlmeyer", "J. Wang", "D.C. Jangraw", "B. Lou", "S.-F. Chang", "P. Sajda"], "venue": "Journal of Neural Engineering, vol. 8, no. 3, 2011.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Short-term conceptual memory for pictures", "author": ["M.C. Potter"], "venue": "Journal of Experimental Psychology: Human Learning and Memory, vol. 2, no. 5, pp. 509\u2013522, 1976.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1976}, {"title": "Machine learning from imbalanced data sets 101", "author": ["F. Provost"], "venue": "Tech. Rep. AAAI Technical Report WS-00-05, 2000.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "A comparison of electroencephalography signals acquired from conventional and mobile systems", "author": ["A.J. Ries", "J. Touryan", "J. Vettel", "K. McDowell", "W.D. Hairston"], "venue": "Journal of Neuroscience and Neuroengineering, vol. 3, no. 1, pp. 10\u201320, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "In a blink of an eye and a switch of a transistor: Cortically coupled computer vision", "author": ["P. Sajda", "E. Pohlmeyer", "J. Wang", "L. Parra", "C. Christoforou", "J. Dmochowski", "B. Hanna", "C. Bahlmann", "M. Singh", "S.-F. Chang"], "venue": "Proc. of the IEEE, vol. 98, no. 3, pp. 462\u2013478, 2010.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Transferring subspaces between subjects in brain-computer interfacing", "author": ["W. Samek", "F. Meinecke", "K.-R. Muller"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 60, no. 8, pp. 2289\u20132298, 2013.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning with kernels: support vector machines, regularization, optimization, and beyond", "author": ["B. Scholkopf", "A.J. Smola"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2001}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "University of Wisconsin\u2013 Madison, Computer Sciences Technical Report 1648, 2009.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "Code of federal regulations protection of human subjects", "author": ["US Department of Defense Office of the Secretary of Defense"], "venue": "Government Printing Office, no. 32 CFR 19, 1999.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1999}, {"title": "Use of volunteers as subjects of research", "author": ["US Department of the Army"], "venue": "Government Printing Office, no. AR 70-25, 1990.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1990}, {"title": "Statistical Learning Theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1998}, {"title": "A review on transfer learning for brain-computer interface classification", "author": ["P. Wang", "J. Lu", "B. Zhang", "Z. Tang"], "venue": "Prof. 5th Int\u2019l Conf. on Information Science and Technology (IC1ST), Changsha, China, April 2015.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Online driver\u2019s drowsiness estimation using domain adaptation with model fusion", "author": ["D. Wu", "C.-H. Chuang", "C.-T. Lin"], "venue": "Proc. Int\u2019l Conf. on Affective Computing and Intelligent Interaction, Xi\u2019an, China, September 2015.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Active transfer learning for reducing calibration data in single-trial classification of visually-evoked potentials", "author": ["D. Wu", "B.J. Lance", "V.J. Lawhern"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man, and Cybernetics, San Diego, CA, October 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Collaborative filtering for braincomputer interaction using transfer learning and active class selection", "author": ["D. Wu", "B.J. Lance", "T.D. Parsons"], "venue": "PLoS ONE, 2013.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "Offline EEG-based driver drowsiness estimation using enhanced batch-mode active learning (EBMAL) for regression", "author": ["D. Wu", "V.J. Lawhern", "S. Gordon", "B.J. Lance", "C.-T. Lin"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Budapest, Hungary, October 2016.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Switching EEG headsets made easy: Reducing offline calibration effort using active weighted adaptation regularization", "author": ["D. Wu", "V.J. Lawhern", "W.D. Hairston", "B.J. Lance"], "venue": "IEEE Trans. on Neural Systems and Rehabilitation Engineering, 2016, in press.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}, {"title": "Reducing BCI calibration effort in RSVP tasks using online weighted adaptation regularization with source domain selection", "author": ["D. Wu", "V.J. Lawhern", "B.J. Lance"], "venue": "Proc. Int\u2019l Conf. on Affective Computing and Intelligent Interaction, Xi\u2019an, China, September 2015.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing offline BCI calibration effort using weighted adaptation regularization with source domain selection", "author": ["D. Wu", "V.J. Lawhern", "B.J. Lance"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Hong Kong, October 2015.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards passive brain-computer interfaces: applying brain-computer interface technology to human-machine systems in general", "author": ["T.O. Zander", "C. Kothe"], "venue": "Journal of Neural Engineering, vol. 8, no. 2, 2011.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 4, "context": "Many real-world brain-computer interface (BCI) applications rely on single-trial classification of event-related potentials (ERPs) in EEG signals [5], [39].", "startOffset": 146, "endOffset": 149}, {"referenceID": 37, "context": "Many real-world brain-computer interface (BCI) applications rely on single-trial classification of event-related potentials (ERPs) in EEG signals [5], [39].", "startOffset": 151, "endOffset": 155}, {"referenceID": 8, "context": "210 Hz) [10], [36], and the subject needs to detect some target images in them.", "startOffset": 8, "endOffset": 12}, {"referenceID": 34, "context": "210 Hz) [10], [36], and the subject needs to detect some target images in them.", "startOffset": 14, "endOffset": 18}, {"referenceID": 33, "context": "The P300 ERPs can be detected by a BCI system [35], and the corresponding images are then triaged for further inspection.", "startOffset": 46, "endOffset": 50}, {"referenceID": 31, "context": "Research [33], [39], [54] has shown that these BCI systems enable the subject to detect targets in large aerial photographs faster and more accurately than traditional standard searches.", "startOffset": 9, "endOffset": 13}, {"referenceID": 37, "context": "Research [33], [39], [54] has shown that these BCI systems enable the subject to detect targets in large aerial photographs faster and more accurately than traditional standard searches.", "startOffset": 15, "endOffset": 19}, {"referenceID": 52, "context": "Research [33], [39], [54] has shown that these BCI systems enable the subject to detect targets in large aerial photographs faster and more accurately than traditional standard searches.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "Unfortunately, because different subjects have different neural responses to even the same visual stimulus [6], [7], [21], it is very difficult, if not impossible, to build a generic ERP classifier whose parameters fit all subjects.", "startOffset": 107, "endOffset": 110}, {"referenceID": 6, "context": "Unfortunately, because different subjects have different neural responses to even the same visual stimulus [6], [7], [21], it is very difficult, if not impossible, to build a generic ERP classifier whose parameters fit all subjects.", "startOffset": 112, "endOffset": 115}, {"referenceID": 19, "context": "Unfortunately, because different subjects have different neural responses to even the same visual stimulus [6], [7], [21], it is very difficult, if not impossible, to build a generic ERP classifier whose parameters fit all subjects.", "startOffset": 117, "endOffset": 121}, {"referenceID": 40, "context": "Additionally, in offline calibration we can query any epoch in the pool for the label (an optimal query strategy can hence be designed by using machine learning methods such as active learning [42], [48]), but in online calibration usually the sequence of the epochs is pre-determined and the subject has no control over which epoch he/she will see next.", "startOffset": 193, "endOffset": 197}, {"referenceID": 46, "context": "Additionally, in offline calibration we can query any epoch in the pool for the label (an optimal query strategy can hence be designed by using machine learning methods such as active learning [42], [48]), but in online calibration usually the sequence of the epochs is pre-determined and the subject has no control over which epoch he/she will see next.", "startOffset": 199, "endOffset": 203}, {"referenceID": 27, "context": "Many signal processing and machine learning approaches have been proposed to reduce the BCI calibration effort [29], [30].", "startOffset": 111, "endOffset": 115}, {"referenceID": 28, "context": "Many signal processing and machine learning approaches have been proposed to reduce the BCI calibration effort [29], [30].", "startOffset": 117, "endOffset": 121}, {"referenceID": 27, "context": "They may be grouped into five categories [29]:", "startOffset": 41, "endOffset": 45}, {"referenceID": 39, "context": "1) Regularization, which is a very effective machine learning approach for constructing robust models [41], especially when the training data size is small.", "startOffset": 102, "endOffset": 106}, {"referenceID": 25, "context": "A popular regularization approach in BCI calibration is shrinkage [27], which gives a regularized estimate of the covariance matrices.", "startOffset": 66, "endOffset": 70}, {"referenceID": 17, "context": "2) Transfer/multi-task learning, which uses relevant data from other subjects to help the current subject [19], [46].", "startOffset": 106, "endOffset": 110}, {"referenceID": 44, "context": "2) Transfer/multi-task learning, which uses relevant data from other subjects to help the current subject [19], [46].", "startOffset": 112, "endOffset": 116}, {"referenceID": 30, "context": "The transfer learning (TL) [32] based approaches are particularly popular [1], [19], [20], [40], [46], [48],", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "The transfer learning (TL) [32] based approaches are particularly popular [1], [19], [20], [40], [46], [48],", "startOffset": 74, "endOffset": 77}, {"referenceID": 17, "context": "The transfer learning (TL) [32] based approaches are particularly popular [1], [19], [20], [40], [46], [48],", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "The transfer learning (TL) [32] based approaches are particularly popular [1], [19], [20], [40], [46], [48],", "startOffset": 85, "endOffset": 89}, {"referenceID": 38, "context": "The transfer learning (TL) [32] based approaches are particularly popular [1], [19], [20], [40], [46], [48],", "startOffset": 91, "endOffset": 95}, {"referenceID": 44, "context": "The transfer learning (TL) [32] based approaches are particularly popular [1], [19], [20], [40], [46], [48],", "startOffset": 97, "endOffset": 101}, {"referenceID": 46, "context": "The transfer learning (TL) [32] based approaches are particularly popular [1], [19], [20], [40], [46], [48],", "startOffset": 103, "endOffset": 107}, {"referenceID": 47, "context": "[49], [52], [53], because in many BCI applications we can easily find legacy data from the same subject in the same task or similar tasks, or legacy data from different subjects in the same task or similar tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[49], [52], [53], because in many BCI applications we can easily find legacy data from the same subject in the same task or similar tasks, or legacy data from different subjects in the same task or similar tasks.", "startOffset": 6, "endOffset": 10}, {"referenceID": 51, "context": "[49], [52], [53], because in many BCI applications we can easily find legacy data from the same subject in the same task or similar tasks, or legacy data from different subjects in the same task or similar tasks.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "3) Adaptive learning, which refines the machine learning model as new (labeled or unlabeled) subject-specific data are available [23], [52], [53].", "startOffset": 129, "endOffset": 133}, {"referenceID": 50, "context": "3) Adaptive learning, which refines the machine learning model as new (labeled or unlabeled) subject-specific data are available [23], [52], [53].", "startOffset": 135, "endOffset": 139}, {"referenceID": 51, "context": "3) Adaptive learning, which refines the machine learning model as new (labeled or unlabeled) subject-specific data are available [23], [52], [53].", "startOffset": 141, "endOffset": 145}, {"referenceID": 20, "context": "4) Active learning, which optimally selects the most informative unlabeled samples to label [22], [31], [48], [50].", "startOffset": 92, "endOffset": 96}, {"referenceID": 29, "context": "4) Active learning, which optimally selects the most informative unlabeled samples to label [22], [31], [48], [50].", "startOffset": 98, "endOffset": 102}, {"referenceID": 46, "context": "4) Active learning, which optimally selects the most informative unlabeled samples to label [22], [31], [48], [50].", "startOffset": 104, "endOffset": 108}, {"referenceID": 48, "context": "4) Active learning, which optimally selects the most informative unlabeled samples to label [22], [31], [48], [50].", "startOffset": 110, "endOffset": 114}, {"referenceID": 40, "context": "There are many criteria to determine which unlabeled samples are the most informative [42].", "startOffset": 86, "endOffset": 90}, {"referenceID": 22, "context": "However, a closely related technique, active class selection [24], can be used for online BCI calibration [49].", "startOffset": 61, "endOffset": 65}, {"referenceID": 47, "context": "However, a closely related technique, active class selection [24], can be used for online BCI calibration [49].", "startOffset": 106, "endOffset": 110}, {"referenceID": 26, "context": "For example, prior information on which EEG channels are the most likely to be useful was used in [28] as a regularizer to optimize spatial filters, and beamforming has been used in [16] to find relevant features from prior regions of interest to reduce the calibration data requirement.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "For example, prior information on which EEG channels are the most likely to be useful was used in [28] as a regularizer to optimize spatial filters, and beamforming has been used in [16] to find relevant features from prior regions of interest to reduce the calibration data requirement.", "startOffset": 182, "endOffset": 186}, {"referenceID": 26, "context": "An optimal spatial filters was designed in [28] for efficient subjectto-subject transfer by combining regularization and a prior information on which channels are the most likely to be useful.", "startOffset": 43, "endOffset": 47}, {"referenceID": 47, "context": "A collaborative filtering approach was developed in [49], which combined TL and active class selection to minimize the online calibration effort.", "startOffset": 52, "endOffset": 56}, {"referenceID": 46, "context": "An active TL approach was proposed in [48] for offline calibration, which combined TL and active learning to minimize the offline calibration effort.", "startOffset": 38, "endOffset": 42}, {"referenceID": 49, "context": "An active weighted adaptation regularization approach, which combines active learning, TL, regularization and semi-supervised learning, was proposed in [51] to facilitate the switching between different EEG headsets.", "startOffset": 152, "endOffset": 156}, {"referenceID": 45, "context": "For example, a domain adaptation with model fusion approach, which combines regularization and TL, was developed in [47] to estimate driver\u2019s drowsiness online continuously.", "startOffset": 116, "endOffset": 120}, {"referenceID": 50, "context": "This paper presents a comprehensive overview and comparison of the offline and online weighted adaptation regularization with source domain selection (wARSDS) algorithms, which we proposed recently [52], [53].", "startOffset": 198, "endOffset": 202}, {"referenceID": 51, "context": "This paper presents a comprehensive overview and comparison of the offline and online weighted adaptation regularization with source domain selection (wARSDS) algorithms, which we proposed recently [52], [53].", "startOffset": 204, "endOffset": 208}, {"referenceID": 51, "context": "The offline wARSDS algorithm, which combined TL, regularization, and semisupervised learning, was first developed in [53] for offline single-trial classification of ERPs in a visually-evoked potential (VEP) oddball task.", "startOffset": 117, "endOffset": 121}, {"referenceID": 50, "context": "It was later extended to online calibration in a RSVP task [52], which still includes TL and regularization but not semi-supervised learning because unlabeled samples are not available in online calibration.", "startOffset": 59, "endOffset": 63}, {"referenceID": 49, "context": "This section describes the offline wARSDS algorithm [51], [53], which originates from the adaptation regularization \u2013 regularized least squares (ARRLS) algorithm in [25].", "startOffset": 52, "endOffset": 56}, {"referenceID": 51, "context": "This section describes the offline wARSDS algorithm [51], [53], which originates from the adaptation regularization \u2013 regularized least squares (ARRLS) algorithm in [25].", "startOffset": 58, "endOffset": 62}, {"referenceID": 23, "context": "This section describes the offline wARSDS algorithm [51], [53], which originates from the adaptation regularization \u2013 regularized least squares (ARRLS) algorithm in [25].", "startOffset": 165, "endOffset": 169}, {"referenceID": 23, "context": "A domain [25], [32] D in TL consists of a multi-dimensional feature space X and a marginal probability distribution P (x), i.", "startOffset": 9, "endOffset": 13}, {"referenceID": 30, "context": "A domain [25], [32] D in TL consists of a multi-dimensional feature space X and a marginal probability distribution P (x), i.", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": "A task [25], [32] T in TL consists of a label space Y and a conditional probability distribution Q(y|x).", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": "A task [25], [32] T in TL consists of a label space Y and a conditional probability distribution Q(y|x).", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": ", Ps(x) 6= Pt(x) and Qs(y|x) 6= Qt(y|x), because the two subjects usually have different neural responses to the same visual stimulus [6], [7], [21].", "startOffset": 134, "endOffset": 137}, {"referenceID": 6, "context": ", Ps(x) 6= Pt(x) and Qs(y|x) 6= Qt(y|x), because the two subjects usually have different neural responses to the same visual stimulus [6], [7], [21].", "startOffset": 139, "endOffset": 142}, {"referenceID": 19, "context": ", Ps(x) 6= Pt(x) and Qs(y|x) 6= Qt(y|x), because the two subjects usually have different neural responses to the same visual stimulus [6], [7], [21].", "startOffset": 144, "endOffset": 148}, {"referenceID": 49, "context": "As in [51], [53], the learning framework of wAR is formulated as:", "startOffset": 6, "endOffset": 10}, {"referenceID": 51, "context": "As in [51], [53], the learning framework of wAR is formulated as:", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "This is very important, because class imbalance is intrinsic to many applications [18], particularly BCI applications.", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "Of course, there are many other approaches for handling class imbalance [18], [26], [37].", "startOffset": 72, "endOffset": 76}, {"referenceID": 24, "context": "Of course, there are many other approaches for handling class imbalance [18], [26], [37].", "startOffset": 78, "endOffset": 82}, {"referenceID": 35, "context": "Of course, there are many other approaches for handling class imbalance [18], [26], [37].", "startOffset": 84, "endOffset": 88}, {"referenceID": 2, "context": "According to the Representer Theorem [3], [25], the solution of (2) can be expressed as:", "startOffset": 37, "endOffset": 40}, {"referenceID": 23, "context": "According to the Representer Theorem [3], [25], the solution of (2) can be expressed as:", "startOffset": 42, "endOffset": 46}, {"referenceID": 23, "context": "As in [25], we define the structural risk as the squared norm of f in HK , i.", "startOffset": 6, "endOffset": 10}, {"referenceID": 23, "context": "As in [25], we compute Df,K(Ps, Pt) using the projected maximum mean discrepancy (MMD) between the source and target domains:", "startOffset": 6, "endOffset": 10}, {"referenceID": 23, "context": "As in [25], we first compute pseudo labels for the unlabeled target domain samples and construct the label vector y in (7).", "startOffset": 6, "endOffset": 10}, {"referenceID": 43, "context": ", a support vector machine (SVM) [45].", "startOffset": 33, "endOffset": 37}, {"referenceID": 51, "context": "So, we introduce a source domain selection approach [53], which selects the closest source domains to reduce the computational cost, and also to (potentially) improve the classification performance.", "startOffset": 52, "endOffset": 56}, {"referenceID": 23, "context": "As mentioned at the beginning of this section, the formulation and derivation of wAR closely resemble the ARRLS algorithm in [25]; however, there are several major differences:", "startOffset": 125, "endOffset": 129}, {"referenceID": 2, "context": "3) ARRLS also includes manifold regularization [3].", "startOffset": 47, "endOffset": 50}, {"referenceID": 50, "context": "This section introduces the OwARSDS algorithm [52], which extends the offline wARSDS algorithm to online BCI calibration.", "startOffset": 46, "endOffset": 50}, {"referenceID": 50, "context": "It has been shown [52] that the closed-form solution for \u03b1 is:", "startOffset": 18, "endOffset": 22}, {"referenceID": 49, "context": "The offline wARSDS algorithm [51], [53].", "startOffset": 29, "endOffset": 33}, {"referenceID": 51, "context": "The offline wARSDS algorithm [51], [53].", "startOffset": 35, "endOffset": 39}, {"referenceID": 50, "context": "The online OwARSDS algorithm [52].", "startOffset": 29, "endOffset": 33}, {"referenceID": 36, "context": "A two-stimulus VEP oddball task was used [38].", "startOffset": 41, "endOffset": 45}, {"referenceID": 41, "context": "The voluntary, fully informed consent of the persons used in this research was obtained as required by federal and Army regulations [43], [44].", "startOffset": 132, "endOffset": 136}, {"referenceID": 42, "context": "The voluntary, fully informed consent of the persons used in this research was obtained as required by federal and Army regulations [43], [44].", "startOffset": 138, "endOffset": 142}, {"referenceID": 46, "context": "The preprocessing and feature extraction method for all three headsets was the same, except that for ABM and Emotiv headsets we used all the channels, but for the BioSemi headset we only used 21 channels (Cz, Fz, P1, P3, P5, P7, P9, PO7, PO3, O1, Oz, POz, Pz, P2, P4, P6, P8, P10, PO8, PO4, O2) mainly in the parietal and occipital areas, as in [48].", "startOffset": 345, "endOffset": 349}, {"referenceID": 10, "context": "EEGLAB [12] was used for EEG signal preprocessing and feature extraction.", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "For each headset, we first band-passed the EEG signals to [1, 50] Hz, then downsampled them to 64 Hz, performed average reference, and next epoched them to the [0, 0.", "startOffset": 58, "endOffset": 65}, {"referenceID": 48, "context": "For each headset, we first band-passed the EEG signals to [1, 50] Hz, then downsampled them to 64 Hz, performed average reference, and next epoched them to the [0, 0.", "startOffset": 58, "endOffset": 65}, {"referenceID": 9, "context": "Similar experimental settings have been used in [11], [17].", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "Similar experimental settings have been used in [11], [17].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "We then normalized each feature dimension separately to [0, 1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "from the 14th subject; Set ; Generate a random number 0 [1, ] m m \u2208", "startOffset": 56, "endOffset": 61}, {"referenceID": 0, "context": "from the 14th subject; Set ; Generate a random number 0 [1, ] m m \u2208", "startOffset": 56, "endOffset": 61}, {"referenceID": 51, "context": "Algorithms We compared the performance of wARSDS with six other algorithms [53]: 1) BL1, a baseline approach in which we assume we know labels of all samples from the new subject, and use 5fold cross-validation and SVM to find the highest BCA.", "startOffset": 75, "endOffset": 79}, {"referenceID": 46, "context": "3) TL, which is the TL algorithm introduced in [48].", "startOffset": 47, "endOffset": 51}, {"referenceID": 23, "context": "5) ARRLS, which was proposed in [25] (manifold regularization was removed), and is also the wAR algorithm introduced in Algorithm 1, by setting wt = ws,i = wt,i = 1.", "startOffset": 32, "endOffset": 36}, {"referenceID": 7, "context": "Weighted libSVM [8] with RBF kernel was used as the classifier in BL1, BL2, TL and TLSDS.", "startOffset": 16, "endOffset": 19}, {"referenceID": 23, "context": "1, and \u03bb = 10, following the practice in [25], [51], [53].", "startOffset": 41, "endOffset": 45}, {"referenceID": 49, "context": "1, and \u03bb = 10, following the practice in [25], [51], [53].", "startOffset": 47, "endOffset": 51}, {"referenceID": 51, "context": "1, and \u03bb = 10, following the practice in [25], [51], [53].", "startOffset": 53, "endOffset": 57}, {"referenceID": 49, "context": "As in [51], [53], we also performed comprehensive statistical tests to check if the performance differences among the six algorithms (BL1 was not included because it is not iterative) were statistically significant.", "startOffset": 6, "endOffset": 10}, {"referenceID": 51, "context": "As in [51], [53], we also performed comprehensive statistical tests to check if the performance differences among the six algorithms (BL1 was not included because it is not iterative) were statistically significant.", "startOffset": 12, "endOffset": 16}, {"referenceID": 29, "context": "We used the area-underperformance-curve (AUPC) [31], [51], [53] to assess overall performance differences among these algorithms.", "startOffset": 47, "endOffset": 51}, {"referenceID": 49, "context": "We used the area-underperformance-curve (AUPC) [31], [51], [53] to assess overall performance differences among these algorithms.", "startOffset": 53, "endOffset": 57}, {"referenceID": 51, "context": "We used the area-underperformance-curve (AUPC) [31], [51], [53] to assess overall performance differences among these algorithms.", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "The AUPC is the area under the curve of the BCAs obtained at each of the 30 runs, and is normalized to [0, 1].", "startOffset": 103, "endOffset": 109}, {"referenceID": 13, "context": "As a result, we used Friedman\u2019s test [15], a two-way non-parametric ANOVA where column effects are tested for significant differences after adjusting for possible row effects.", "startOffset": 37, "endOffset": 41}, {"referenceID": 46, "context": "5Note that this does not conflict with the observation in [48], which said TL was better than BL2.", "startOffset": 58, "endOffset": 62}, {"referenceID": 46, "context": "This is because different datasets were used in the two studies: [48] downsampled the non-target class to balance the two classes before testing the performances of different algorithms, whereas classimbalance was preserved in this paper.", "startOffset": 65, "endOffset": 69}, {"referenceID": 46, "context": "Moreover, [48] only considered the BioSemi headset, and showed that TL outperformed BL2, but the performance difference between TL and BL2 decreased as ml increases.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "Then, non-parametric multiple comparison tests using Dunn\u2019s procedure [13], [14] was used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the false discovery rate method [4].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "Then, non-parametric multiple comparison tests using Dunn\u2019s procedure [13], [14] was used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the false discovery rate method [4].", "startOffset": 76, "endOffset": 80}, {"referenceID": 3, "context": "Then, non-parametric multiple comparison tests using Dunn\u2019s procedure [13], [14] was used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the false discovery rate method [4].", "startOffset": 247, "endOffset": 250}, {"referenceID": 8, "context": "5(b) that both wAR and wARSDS achieved good BCAs for \u03bbP \u2208 [10, 100] and \u03bbQ \u2208 [10, 100].", "startOffset": 58, "endOffset": 67}, {"referenceID": 8, "context": "5(b) that both wAR and wARSDS achieved good BCAs for \u03bbP \u2208 [10, 100] and \u03bbQ \u2208 [10, 100].", "startOffset": 77, "endOffset": 86}, {"referenceID": 7, "context": "Again, weighted libSVM [8] with RBF kernel was used as the classifier in BL1, BL2, TL and TLSDS.", "startOffset": 23, "endOffset": 26}, {"referenceID": 49, "context": "In such applications, wAR can be used to make use of the data obtained from a previous EEG headset to facilitate the calibration for the new headset, as introduced in [51].", "startOffset": 167, "endOffset": 171}, {"referenceID": 32, "context": "), are also frequently used in affective computing [34], \u201ccomputing that relates to, arises from, or deliberately influences emotion or other affective phenomena.", "startOffset": 51, "endOffset": 55}, {"referenceID": 1, "context": ", the information geometry [2].", "startOffset": 27, "endOffset": 30}], "year": 2017, "abstractText": "Many real-world brain-computer interface (BCI) applications rely on single-trial classification of event-related potentials (ERPs) in EEG signals. However, because different subjects have different neural responses to even the same stimulus, it is very difficult to build a generic ERP classifier whose parameters fit all subjects. The classifier needs to be calibrated for each individual subject, using some labeled subject-specific data. This paper proposes both online and offline weighted adaptation regularization (wAR) algorithms to reduce this calibration effort, i.e., to minimize the amount of labeled subjectspecific EEG data required in BCI calibration, and hence to increase the utility of the BCI system. We demonstrate using a visually-evoked potential oddball task and three different EEG headsets that both online and offline wAR algorithms significantly outperform several other algorithms. Moreover, through source domain selection, we can reduce their computational cost by about 50%, making them more suitable for real-time applications.", "creator": "LaTeX with hyperref package"}}}