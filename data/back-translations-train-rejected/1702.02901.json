{"id": "1702.02901", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2017", "title": "Driver Drowsiness Estimation from EEG Signals Using Online Weighted Adaptation Regularization for Regression (OwARR)", "abstract": "One big challenge that hinders the transition of brain-computer interfaces (BCIs) from laboratory settings to real-life applications is the availability of high-performance and robust learning algorithms that can effectively handle individual differences, i.e., algorithms that can be applied to a new subject with zero or very little subject-specific calibration data. Transfer learning and domain adaptation have been extensively used for this purpose. However, most previous works focused on classification problems. This paper considers an important regression problem in BCI, namely, online driver drowsiness estimation from EEG signals. By integrating fuzzy sets with domain adaptation, we propose a novel online weighted adaptation regularization for regression (OwARR) algorithm to reduce the amount of subject-specific calibration data, and also a source domain selection (SDS) approach to save about half of the computational cost of OwARR. Using a simulated driving dataset with 15 subjects, we show that OwARR and OwARR-SDS can achieve significantly smaller estimation errors than several other approaches. We also provide comprehensive analyses on the robustness of OwARR and OwARR-SDS.", "histories": [["v1", "Thu, 9 Feb 2017 17:14:15 GMT  (1269kb)", "http://arxiv.org/abs/1702.02901v1", "in press"]], "COMMENTS": "in press", "reviews": [], "SUBJECTS": "cs.LG cs.HC", "authors": ["dongrui wu", "vernon j lawhern", "stephen gordon", "brent j lance", "chin-teng lin"], "accepted": false, "id": "1702.02901"}, "pdf": {"name": "1702.02901.pdf", "metadata": {"source": "CRF", "title": "Driver Drowsiness Estimation from EEG Signals Using Online Weighted Adaptation Regularization for Regression (OwARR)", "authors": ["Dongrui Wu"], "emails": ["drwu09@gmail.com,", "vernon.j.lawhern.civ@mail.mil,", "sgordon@dcscorp.com,", "brent.j.lance.civ@mail.mil,", "Chin-Teng.Lin@uts.edu.au"], "sections": [{"heading": null, "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "II. ONLINE WEIGHTED ADAPTATION REGULARIZATION FOR REGRESSION (OWARR)", "text": "In [60] we have defined two types of calibration in the BCI: 1) offline calibration, in which a pool of unlabeled EEG epochs is obtained a priori and a subject or oracle is queried to label some of these epochs, which are then used to train a model to label the remaining epochs in the pool. 2) online calibration, in which some labeled EEG epochs are obtained on the fly to train a model for future (invisible) EEG epochs. The big difference between them is that for offline calibration, the unlabeled EEG epochs can be used to help design the model (e.g. semi-supervised learning), while in online calibration there are no unlabeled EEG epochs."}, {"heading": "A. Problem Definition", "text": "A domain [23], [34] D in TL consists of a d-dimensional attribute space X = x and a marginal probability distribution P (x), i.e., D = {X, P (x)}, where x-X. Two domains Dz and Dt differ when X z 6 = X t and / or P z (x) 6 = P t (x).A task [23], [34] T in TL consists of an output space Y and a conditional probability distribution Q (y | x). Two tasks T z and T t differ when Yz 6 = Yt or Qz (y | x) 6 = Qt (y | x).Given the zth source domain Dz with nz samples (xzi, yzi), i = 1, nz, and a target domain (nz, and a target domain z)."}, {"heading": "B. The Learning Framework", "text": "Because (x) = Q (y | x) = P (x, y) P (x) = Q (x | y) P (y) P (x), (1) to use the data in the zth source domain in the target domain, we must minimize the distance between the marginal and conditional probability distributions in the two domains by ensuring that 1 P z (x) is close to P t (x) and Qz (x | y) is also close to Qt (x | y). Suppose that both the output and each dimension of the input vector have an average value. Then, the regression function can be written as asf (x) = \u03b1Tx (2), where \u03b1 is the regression parameter vector to be found. The learning frame of OwARR is then formulated as f = argmin fn \u2211 i = 1 (xi) + wt \u00b2."}, {"heading": "C. Sum of Squared Error Minimization", "text": "LetX = [x1,..., xn + m] T (4) y = [y1,..., yn + m] T (5), where the first n xi and yi are the column input vectors and corresponding outputs in the source domain, the next m1Strictly speaking, we should also make sure that P z (y) is close to P t (y). In this paper, we assume that P z (y) and P t (y) are close to each other. Our future research will consider the general case that P z (y) and P t (y) are different. 3 xi and yi are the column input vectors and corresponding outputs in the target domain, and T is the matrix conversion operation.Define E-R (n + m) \u00d7 (n + m) as diagonal matrix with Eii = {1, 1 \u2264 i wt, n \u2212 m \u2212 Then the first two terms in (3) can be written as large domains."}, {"heading": "D. Marginal Probability Distribution Adaptation", "text": "As in [23], [36], [60], [61] we calculate d (P z, P t) on the basis of the maximum mean discrepancy (MMD): d (P z, P t) = [1nn \u2211 i = 1f (xi) \u2212 1mn + m \u2211 i = n + 1f (xi)] 2 = \u03b1TXMPX\u03b1 (9), where MP-R (n + m) \u00b7 (n + m) is the MMD matrix: (MP) ij = 1 n2, 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 n1 m2, n + 1 \u2264 i \u2264 n + m, n + 1 \u2264 j \u2264 n + m \u2212 1 nm, otherwise (10)"}, {"heading": "E. Conditional Probability Distribution Adaptation", "text": "In [23], [61] a classification problem is considered, and it is easier to adjust the conditional probability distribution. In this section, we first briefly introduce the technique used there, and then describe in detail how we can perform a conditional probability distribution in regression. [33], [33] Conditional Probability Distribution Distribution Adaptation for Classification: Let Dzc = {xi], widely used in EEG functionality extraction [5], [15], [24] and pattern recognition [33], [16], [FS2], [16], [xi], [xi], [xi], [xi], [xi], [xi], [XY], [S], [S,] and [S], [S,] and [S], [S,]."}, {"heading": "F. Maximize the Approximate Sample Pearson Correlation Coefficient", "text": "The exemplary Pearson correlation coefficient r (y, f (x)) is defined as [48]: r (y, f (x)) = y TX\u03b1 (y, f (x)) = \u03b1TX\u03b1yTy (18) Note that r2 (y, f (x)) is on the denominator, so it is very difficult to find a closed solution to maximize it. Note, however, that r2 (y, f (x))) increases with increasing TXTyyTX\u03b1 and decreases with increasing TXTX\u03b1. So, instead of directly maximizing the property r2 (y, f (x), in this essay we try to maximize the following function: r \u04412 (y, f (x) = TXTyyT\u03b1 (T) and the property (XY) TY (T) increases (T)."}, {"heading": "G. The Closed-Form Solution", "text": "By replacing (7), (9), (14) and (19) with (3), we can rewrite it as \u03b1 = argmin \u03b1 (yT \u2212 \u03b1TXT) E (y \u2212 X\u03b1) + \u03bb\u03b1TXT (MP + MQ) X\u03b1 + \u03b3 \u03b1 TXT (I \u2212 yyT) X\u03b1 yTy (20). Setting the above derivative of the objective function to 0 leads to \u03b1 = [XT (E + \u03bbMP + \u03bbMQ + \u03b3 I \u2212 yyT yTy) X] \u2212 1XTEy (21)"}, {"heading": "H. The Complete OwARR Algorithm", "text": "The pseudo-code for the complete OwARR algorithm is described in algorithm 1. First, we run OwARR for each source domain separately and then construct the final regression model as a weighted average of these base models, the weight being the opposite of the training accuracy of the corresponding base model. Then, the final regression model is applied to future blank data. Algorithm 1: The OwARR algorithm. Input: Z source domains, where the zth (z = 1,..., Z) domain has nz samples {xzi, yzi} i = 1,..., nz; m target domain samples, {xtj, ytj} j = 1,..., m; Parameter \u03bb, and \u03c3 in (8). Output: The OwARR regression model for z = 1, 2,..., Z-doConstruct X in (4), query (5), source value (E), and (MP) in the source domain (1)."}, {"heading": "III. OWARR-SDS", "text": "An SDS method for online classification problems was proposed in [60]. In this paper, it is extended to regression problems by using the fuzzy classes shown in Fig. 1. The primary goal of SDS is to reduce the calculation costs of OwARR, because if there is a large number of source domains, we first run mzc (c = 1, 2, 3), the middle vector of each fuzzy class, and then aggregate the base models. Then we also compile mtc, the5 mean vector of each fuzzy class in the target domain. For the m source domain, we first compute mzc (c = 1, 2, 3), the middle vector of each fuzzy class in the m domain. The distance between the two domains is: d (z, t) that we have the distance between the two domains."}, {"heading": "IV. EXPERIMENTS AND DISCUSSIONS", "text": "Experimental results to estimate driver fatigue using EEG signals are presented in this section to demonstrate the performance of OwARR and OwARR-SDS."}, {"heading": "A. Experiment Setup", "text": "16 healthy subjects with normal or corrected vision were recruited to participate in an experiment with sustained attention [3], [4] which consisted of a real vehicle mounted on a motion platform with 6 degrees of freedom, embedded in a 360 degree virtual reality scene. Each participant read and signed an informative consent form before starting the experiment. Each experiment lasted about 60-90 minutes and was conducted in the afternoon when the circadian rhythm of drowsiness reached its peak. To induce drowsiness while driving, the virtual reality scenes simulated monotonous driving at a set speed of 100 km / h on a straight and empty highway. During the experiment, every 5-10 seconds were randomly performed and participants were instructed to steer the vehicle to compensate for these disturbances as quickly as possible."}, {"heading": "B. Evaluation Process and Performance Measures", "text": "The complete procedure for applying OwARR for Driver Fatigue Estimation is presented in Algorithm 3. Compared with Algorithm 1 on OwARR for a generic application, here we also include detailed EEG data pre-processing and feature extraction steps. Note that although the characteristics in each regression model have the same steps, their parameters (channels removed, main components used, areas used in normalization) may be different, so we need to record them for each subject so that the characteristics in each regression model can be corrected. From the experiments we already know, the fatigue indices for all other 1200 epochs. To evaluate the performance of different algorithms, we used up to 100 epochs in a randomly selected block for calibration, and the remaining 1100 epochs for testing. Each time five epochs were acquired, we compress the test performance to show the same performance as the models are repeated."}, {"heading": "C. Preprocessing and Feature Extraction", "text": "The ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the re"}, {"heading": "D. Algorithms", "text": "We compared the performance of OwARR and OwARRSDS with three other algorithms introduced in [54]: 1) Baseline 1 (BL1), which combines data from all 14 existing subjects, builds a ridge regression model [10] and applies it to the new subject, that is, BL1 attempts to build a subject-independent regression model and completely ignores data from the new subject. 2) Baseline 2 (BL2), which builds a ridge regression model using only subject-specific calibration samples from the new subject, that is, BL2 completely ignores data from existing subjects. 3) DAMF, which builds 14 ridge regression models by combining data from each auxiliary with data from the new subject and then using a weighted average to obtain the final regression model. 3) DAMF, which builds 14 ridge regression models, is used by combining data from each auxiliary with data from the new subject and then using a weighted average to obtain the final regression model."}, {"heading": "E. Regression Performance Comparison", "text": "The average RMSEs and CCs for the five algorithms across the 15 subjects are shown in Fig. 4, and the RMSEs and CCs for the individual subjects are shown in Fig. 5. Note that DAMF, OwARR, and OwARR-SDS had very similar CCs, and all of them were better than the CCs for BL1 and BL2. Also: 0 20 40 60 80 100 m, number of subject-specific subject samples 0.20.250.4R S E (a) 0 40 60 80 100 m, number of subject-specific sample 00.10.40.6C CBLS CBL1 BL2 DAMF-SDS (b) Summary, the three DA-based approaches generally had better performance than BL1, which did not use subject-specific samples00er samples00ers. 10.40.50.6C BLF OwARR-SDS S2 BLDAMOwR-SARDS were SARDS-SARDS."}, {"heading": "F. Computational Cost", "text": "This year, the number of unemployed people slipping into unemployment has doubled, and the number of unemployed people slipping into unemployment has doubled."}, {"heading": "G. Robustness to Noises", "text": "It is also important to examine the robustness of the five algorithms for noise. According to [68], there are two types of noise: class noise, which is noise on the model outputs, and attribute noise, which is noise on the model inputs. In this subsection, we focus on the attribute noise. As in [68], for each model entry we randomly replaced q% (q = 0, 10,..., 50) of all periods of the new subject with a uniform noise between its minimum and maximum values. After this was done for both the training and the test data, we trained the five algorithms on the corrupt training data and then tested their performance on the corrupt test data. The RMSEs for three different m (the number of subject-specific samples) averaged over 15 subjects, 10 with five runs per subject, OwOwEs as noise are shown that the noise level of the ARS in general was lower than all the ARS noise levels."}, {"heading": "H. Parameter Sensitivity Analysis", "text": "The OwARR algorithm has three adjustable parameters: \u03c3, which determines the weight wt for the target domain samples; \u03bb, which is a regulation parameter that minimizes the distances between the marginal and conditional probability distributions in the source and target domains; and \u03b3, which maximizes the approximate Pearson correlation coefficient between the true and estimated results. It is interesting to investigate whether all of them are necessary. To this end, we have constructed three modified versions of the OwARR algorithms by setting \u03c3, \u03bb, and \u03b3 to zero, and comparing their average RMSEs with those of the original OwARR. The results are shown in Figure 10. Note that the original OwARR had better performance than all three modified versions, suggesting that all three parameters in OwARR contributed to superior performance."}, {"heading": "I. Effectiveness of the Ensemble Fusion Strategy", "text": "Algorithm 1 (Algorithm 2) clearly indicates that the final step of OwARR (OwARR-SDS) uses ensemble learning: the basic DA models are aggregated on the basis of a weighted average to obtain the final regression model, and the weight is inversely proportional to the RMSE training of the corresponding Base DA model. In this subsection, we examine whether this merger strategy is effective; the performance of the 14 Base DA models and the final aggregation model for a typical subject are shown in Figure 12. Note that the aggregated model is better than most Base DA models and also comes close to the best Base DA model (which is unknown in practice), suggesting that the merger strategy is effective."}, {"heading": "V. CONCLUSIONS AND FUTURE RESEARCH", "text": "In fact, most of them are only a very small group, able to move around without being able to achieve their objectives."}, {"heading": "ACKNOWLEDGEMENT", "text": "The views and conclusions contained in this document are those of the authors and should not be interpreted to represent the official guidelines either expressed or implied by the research laboratory of the U.S. Army or the U.S. Government. This work was also partially implemented by the Australian Research Council (ARC) under Discovery Grant DP150101645. APPENDIX FUZZY SETS (FSS) FS theory was first introduced by Zadeh [67] in 1965 and has been used successfully in many areas, including modeling and control [49], data mining [35], [62], [66], time series prediction [14], [46], decision making [30], [37], etc.A FS X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X, FSS-X-X-X-X-X-X-X-X-X-X, FSS-X-X-X-X-X-X, FSS-X-X-X-X, FSS-X-X-X-X-X-X-X-X, or-X-X-X-X-X-X-X-X, or-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X, or-X-X-X, or-X-X-X-X-X-X, or-X-X-X-X-X-"}], "references": [{"title": "Improving session-tosession transfer performance of motor imagery-based BCI using adaptive extreme learning machine", "author": ["A. Bamdadian", "C. Guan", "K.K. Ang", "J. Xu"], "venue": "Proc. 35th Annual Int\u2019l Conf. of the IEEE Engineering in Medicine and Biology Society (EMBC), Osaka, Japan, July 2013, pp. 2188\u20132191.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Controlling the false discovery rate: A practical and powerful approach to multiple testing", "author": ["Y. Benjamini", "Y. Hochberg"], "venue": "Journal of the Royal Statistical Society, Series B (Methodological), vol. 57, pp. 289\u2013 300, 1995.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Kinesthesia in a sustained-attention driving task", "author": ["C.-H. Chuang", "L.-W. Ko", "T.-P. Jung", "C.-T. Lin"], "venue": "Neuroimage, vol. 91, pp. 187\u2013202, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Co-modulatory spectral changes in independent brain processes are correlated with task performance", "author": ["S.-W. Chuang", "L.-W. Ko", "Y.-P. Lin", "R.-S. Huang", "T.-P. Jung", "C.-T. Lin"], "venue": "Neuroimage, vol. 62, pp. 1469\u20131477, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Faster self-organizing fuzzy neural network training and a hyperparameter analysis for a braincomputer interface", "author": ["D. Coyle", "G. Prasad", "T. McGinnity"], "venue": "IEEE Trans. on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 39, no. 6, pp. 1458\u20131471, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis", "author": ["A. Delorme", "S. Makeig"], "venue": "Journal of Neuroscience Methods, vol. 134, pp. 9\u201321, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Multisubject learning for common spatial patterns in motor-imagery BCI", "author": ["D. Devlaminck", "B. Wyns", "M. Grosse-Wentrup", "G. Otte", "P. Santens"], "venue": "Computational intelligence and neuroscience, vol. 20, no. 8, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple comparisons among means", "author": ["O. Dunn"], "venue": "Journal of the American Statistical Association, vol. 56, pp. 62\u201364, 1961.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1961}, {"title": "Multiple comparisons using rank sums", "author": ["O. Dunn"], "venue": "Technometrics, vol. 6, pp. 214\u2013252, 1964.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1964}, {"title": "Unsupervised fuzzy cmeans clustering for motor imagery EEG recognition", "author": ["W.-Y. Hsu", "C.-Y. Lin", "W.-F. Kuo", "M. Liou", "Y.-N. Sun", "A.C. hsin Tsai", "H.-J. Hsu", "P.-H. Chen", "I.-R. Chen"], "venue": "Int\u2019l Journal of Innovative Computing, Information and Control, vol. 7, no. 8, pp. 4965\u2013 4976, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Transfer learning in brain-computer interfaces", "author": ["V. Jayaram", "M. Alamgir", "Y. Altun", "B. Scholkopf", "M. Grosse- Wentrup"], "venue": "IEEE Computational Intelligence Magazine, vol. 11, no. 1, pp. 20\u201331, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Composite common spatial pattern for subject-to-subject transfer", "author": ["H. Kang", "Y. Nam", "S. Choi"], "venue": "Signal Processing Letters, vol. 16, no. 8, pp. 683\u2013686, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "DENFIS: Dynamic evolving neural-fuzzy inference system and its application for time-series prediction", "author": ["N.K. Kasabov", "Q. Song"], "venue": "IEEE Trans. on Fuzzy Systems, vol. 10, no. 2, pp. 144\u2013154, 2002.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Driver drowsiness classification using fuzzy wavelet-packet-based feature-extraction algorithm", "author": ["R. Khushaba", "S. Kodagoda", "S. Lal", "G. Dissanayake"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 58, no. 1, pp. 121\u2013131, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Fuzzy Sets and Fuzzy Logic: Theory and Applications", "author": ["G.J. Klir", "B. Yuan"], "venue": "Upper Saddle River, NJ: Prentice-Hall,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Quantile Regression", "author": ["R. Koenker"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Brain-computer interface technologies in the coming decades", "author": ["B.J. Lance", "S.E. Kerick", "A.J. Ries", "K.S. Oie", "K. McDowell"], "venue": "Proc. of the IEEE, vol. 100, no. 3, pp. 1585\u20131599, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Application of covariate shift adaptation techniques in brain-computer interfaces", "author": ["Y. Li", "H. Kambara", "Y. Koike", "M. Sugiyama"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 57, no. 6, pp. 1318\u20131324, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "A framework of adaptive brain computer interfaces", "author": ["Y. Li", "Y. Koike", "M. Sugiyama"], "venue": "Proc. 2nd IEEE Int\u2019l Conf. on Biomedical Engineering and Informatics (BMEI), Tianjin, China, October 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Biosensor technologies for augmented brain-computer interfaces in the next decades", "author": ["L.-D. Liao", "C.-T. Lin", "K. McDowell", "A. Wickenden", "K. Gramann", "T.-P. Jung", "L.-W. Ko", "J.-Y. Chang"], "venue": "Proc. of the IEEE, vol. 100, no. 2, pp. 1553\u20131566, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive EEG-based alertness estimation system by using ICA-based fuzzy neural networks", "author": ["C.-T. Lin", "L.-W. Ko", "I.-F. Chung", "T.-Y. Huang", "Y.-C. Chen", "T.-P. Jung", "S.-F. Liang"], "venue": "IEEE Trans. on Circuits and Systems-I, vol. 53, no. 11, pp. 2469\u20132476, 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Adaptation regularization: A general framework for transfer learning", "author": ["M. Long", "J. Wang", "G. Ding", "S.J. Pan", "P.S. Yu"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 26, no. 5, pp. 1076\u20131089, 2014.  13", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "FuRIA: An inverse solution based feature extraction algorithm using fuzzy set theory for brain-computer interfaces", "author": ["F. Lotte", "A. Lecuyer", "B. Arnaldi"], "venue": "IEEE Trans. on Signal Processing, vol. 57, no. 8, pp. 3253\u2013 3263, 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Signal processing approaches to minimize or suppress calibration time in oscillatory activity-based brain-computer interfaces", "author": ["F. Lotte"], "venue": "Proc. of the IEEE, vol. 103, no. 6, pp. 871\u2013890, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Lapses in alertness: Coherence of fluctuations in performance and EEG spectrum", "author": ["S. Makeig", "M. Inlow"], "venue": "Electroencephalography and Clinical Neurophysiology, vol. 86, pp. 23\u201335, 1993.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1993}, {"title": "Tonic, phasic and transient EEG correlates of auditory awareness in drowsiness", "author": ["S. Makeig", "T.P. Jung"], "venue": "Cognitive Brain Research, vol. 4, pp. 12\u201325, 1996.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}, {"title": "Evolving signal processing for brain-computer interfaces", "author": ["S. Makeig", "C. Kothe", "T. Mullen", "N. Bigdely-Shamlo", "Z. Zhang", "K. Kreutz-Delgado"], "venue": "Proc. of the IEEE, vol. 100, no. Special Centennial Issue, pp. 1567\u20131584, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Improved neural signal classification in a rapid serial visual presentation task using active learning", "author": ["A. Marathe", "V. Lawhern", "D. Wu", "D. Slayback", "B. Lance"], "venue": "IEEE Trans. on Neural Systems and Rehabilitation Engineering, vol. 24, no. 3, pp. 333\u2013343, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Uncertain Rule-Based Fuzzy Logic Systems: Introduction and New Directions", "author": ["J.M. Mendel"], "venue": "Upper Saddle River, NJ: Prentice-Hall,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2001}, {"title": "Perceptual Computing: Aiding People in Making Subjective Judgments", "author": ["J.M. Mendel", "D. Wu"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "A survey of affective brain computer interfaces: principles, state-of-the-art, and challenges", "author": ["C. Muhl", "B. Allison", "A. Nijholt", "G. Chanel"], "venue": "Brain-Computer Interfaces, vol. 1, no. 2, pp. 66\u201384, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Evolutionary fuzzy ARTMAP for autoregressive model order selection and classification of EEG signals", "author": ["R. Palaniappan", "P. Raveendran", "S. Nishida", "N. Saiwaki"], "venue": "IEEE Int\u2019l Conf. on Systems, Man, and Cybernetics, vol. 5, Nashville, TN, October 2000, pp. 3682\u20133686.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2000}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345\u20131359, 2010.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Fuzzy set technology in knowledge discovery", "author": ["W. Pedrycz"], "venue": "Fuzzy Sets and Systems, vol. 98, pp. 279\u2013290, 1998.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1998}, {"title": "Large margin transductive transfer learning", "author": ["B. Quanz", "J. Huan"], "venue": "Proc. 18th ACM Conf. on Information and Knowledge Management (CIKM), Hong Kong, November 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Fuzzy-set social science", "author": ["C.C. Ragin"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2000}, {"title": "Fatigue, sleepiness and reduced alertness as risk factors in driving", "author": ["F. Sagberg", "P. Jackson", "H.-P. Kruger", "A. Muzer", "A. Williams"], "venue": "Institute of Transport Economics, Oslo, Tech. Rep. TOI Report 739/2004, 2004.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2004}, {"title": "Transferring subspaces between subjects in brain-computer interfacing", "author": ["W. Samek", "F. Meinecke", "K.-R. Muller"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 60, no. 8, pp. 2289\u20132298, 2013.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "University of Wisconsin\u2013 Madison, Computer Sciences Technical Report 1648, 2009.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Principal component based covariate shift adaption to reduce non-stationarity in a MEG-based brain-computer interface", "author": ["M. Spuler", "W. Rosenstiel", "M. Bogdan"], "venue": "EURASIP Journal on Advances in Signal Processing, vol. 2012, no. 1, pp. 1\u20137, 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamical ensemble learning with model-friendly classifiers for domain adaptation", "author": ["W. Tu", "S. Sun"], "venue": "Proc. 21st Int\u2019l Conf. on Pattern Recognition (ICPR), Tsukuba, Japan, November 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "A subject transfer framework for EEG classification", "author": ["W. Tu", "S. Sun"], "venue": "Neurocomputing, vol. 82, pp. 109\u2013116, 2012.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "Brain-computer interfaces: Beyond medical applications", "author": ["J. van Erp", "F. Lotte", "M. Tangermann"], "venue": "Computer, vol. 45, no. 4, pp. 26\u201334, 2012.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Fuzzy time series approach for disruption prediction in Tokamak reactors", "author": ["M. Versaci", "F.C. Morabito"], "venue": "IEEE Trans. on Magnetics, vol. 39, no. 3, pp. 1503\u20131506, 2003.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2003}, {"title": "Toward unsupervised adaptation of LDA for brain-computer interfaces", "author": ["C. Vidaurre", "M. Kawanabe", "P.V. Bunau", "B. Blankertz", "K. Muller"], "venue": "IEEE Trans. on Biomedical Engineering, vol. 58, no. 3, pp. 587\u2013597, 2011.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Probability & Statistics for Engineers and Scientists, 8th ed", "author": ["R.W. Walpole", "R.H. Myers", "A. Myers", "K. Ye"], "venue": "Upper Saddle River, NJ: Prentice-Hall,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2007}, {"title": "A Course in Fuzzy Systems and Control", "author": ["L.-X. Wang"], "venue": "Upper Saddle River, NJ: Prentice Hall,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1997}, {"title": "A review on transfer learning for brain-computer interface classification", "author": ["P. Wang", "J. Lu", "B. Zhang", "Z. Tang"], "venue": "Prof. 5th Int\u2019l Conf. on Information Science and Technology (IC1ST), Changsha, China, April 2015.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2015}, {"title": "Selective transfer learning for EEG-based drowsiness detection", "author": ["C.-S. Wei", "Y.-P. Lin", "Y.-T. Wang", "T.-P. Jung", "N. Bigdely-Shamlo", "C.- T. Lin"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Hong Kong, October 2015.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2015}, {"title": "The use of fast Fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms", "author": ["P. Welch"], "venue": "IEEE Trans. on Audio Electroacoustics, vol. 15, pp. 70\u2013 73, 1967.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1967}, {"title": "Online driver\u2019s drowsiness estimation using domain adaptation with model fusion", "author": ["D. Wu", "C.-H. Chuang", "C.-T. Lin"], "venue": "Proc. Int\u2019l Conf. on Affective Computing and Intelligent Interaction, Xi\u2019an, China, September 2015.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Active transfer learning for reducing calibration data in single-trial classification of visually-evoked potentials", "author": ["D. Wu", "B.J. Lance", "V.J. Lawhern"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man, and Cybernetics, San Diego, CA, October 2014.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "Collaborative filtering for braincomputer interaction using transfer learning and active class selection", "author": ["D. Wu", "B.J. Lance", "T.D. Parsons"], "venue": "PLoS ONE, 2013.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "Offline EEG-based driver drowsiness estimation using enhanced batch-mode active learning (EBMAL) for regression", "author": ["D. Wu", "V.J. Lawhern", "S. Gordon", "B.J. Lance", "C.-T. Lin"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Budapest, Hungary, October 2016.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectral meta-learner for regression (SMLR) model aggregation: Towards calibrationless brain-computer interface (BCI)", "author": ["D. Wu", "V.J. Lawhern", "S. Gordon", "B.J. Lance", "C.-T. Lin"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Budapest, Hungary, October 2016.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "Switching EEG headsets made easy: Reducing offline calibration effort using active weighted adaptation regularization", "author": ["D. Wu", "V.J. Lawhern", "W.D. Hairston", "B.J. Lance"], "venue": "IEEE Trans. on Neural Systems and Rehabilitation Engineering, 2016, in press.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2016}, {"title": "Reducing BCI calibration effort in RSVP tasks using online weighted adaptation regularization with source domain selection", "author": ["D. Wu", "V.J. Lawhern", "B.J. Lance"], "venue": "Proc. Int\u2019l Conf. on Affective Computing and Intelligent Interaction, Xi\u2019an, China, September 2015.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2015}, {"title": "Reducing offline BCI calibration effort using weighted adaptation regularization with source domain selection", "author": ["D. Wu", "V.J. Lawhern", "B.J. Lance"], "venue": "Proc. IEEE Int\u2019l Conf. on Systems, Man and Cybernetics, Hong Kong, October 2015.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2015}, {"title": "Linguistic summarization using IF-THEN rules and interval type-2 fuzzy sets", "author": ["D. Wu", "J.M. Mendel"], "venue": "IEEE Trans. on Fuzzy Systems, vol. 19, no. 1, pp. 136\u2013151, 2011.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2011}, {"title": "Inductive transfer learning for handling individual differences in affective computing", "author": ["D. Wu", "T.D. Parsons"], "venue": "Proc. 4th Int\u2019l Conf. on Affective Computing and Intelligent Interaction, vol. 2, Memphis, TN, October 2011, pp. 142\u2013151.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech emotion estimation in 3D space", "author": ["D. Wu", "T.D. Parsons", "E. Mower", "S.S. Narayanan"], "venue": "Proc. IEEE Int\u2019l Conf. on Multimedia & Expo (ICME), Singapore, July 2010, pp. 737\u2013742.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2010}, {"title": "Genetic learning and performance evaluation of type-2 fuzzy logic controllers", "author": ["D. Wu", "W.W. Tan"], "venue": "Engineering Applications of Artificial Intelligence, vol. 19, no. 8, pp. 829\u2013841, 2006.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2006}, {"title": "Database discovery using fuzzy sets", "author": ["R. Yager"], "venue": "International Journal of Intelligent Systems, vol. 11, pp. 691\u2013712, 1996.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1996}, {"title": "Fuzzy sets", "author": ["L.A. Zadeh"], "venue": "Information and Control, vol. 8, pp. 338\u2013353, 1965.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 1965}, {"title": "Class noise vs. attribute noise: A quantitative study of their impacts", "author": ["X. Zhu", "X. Wu"], "venue": "Artificial Intelligence Review, vol. 22, pp. 177\u2013210, 2004.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 16, "context": "Brain computer interfaces (BCIs) [18], [28], [32], [45], [53] have attracted rapidly increasing research interest in the last decade, thanks to recent advances in neurosciences, wearable/mobile biosensors, and analytics.", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "Brain computer interfaces (BCIs) [18], [28], [32], [45], [53] have attracted rapidly increasing research interest in the last decade, thanks to recent advances in neurosciences, wearable/mobile biosensors, and analytics.", "startOffset": 39, "endOffset": 43}, {"referenceID": 30, "context": "Brain computer interfaces (BCIs) [18], [28], [32], [45], [53] have attracted rapidly increasing research interest in the last decade, thanks to recent advances in neurosciences, wearable/mobile biosensors, and analytics.", "startOffset": 45, "endOffset": 49}, {"referenceID": 42, "context": "Brain computer interfaces (BCIs) [18], [28], [32], [45], [53] have attracted rapidly increasing research interest in the last decade, thanks to recent advances in neurosciences, wearable/mobile biosensors, and analytics.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [21], and the availability of highperformance and robust algorithms for signal analysis and interpretation that can effectively handle individual differences and non-stationarity [12], [25], [28], [50].", "startOffset": 179, "endOffset": 183}, {"referenceID": 10, "context": "However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [21], and the availability of highperformance and robust algorithms for signal analysis and interpretation that can effectively handle individual differences and non-stationarity [12], [25], [28], [50].", "startOffset": 358, "endOffset": 362}, {"referenceID": 23, "context": "However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [21], and the availability of highperformance and robust algorithms for signal analysis and interpretation that can effectively handle individual differences and non-stationarity [12], [25], [28], [50].", "startOffset": 364, "endOffset": 368}, {"referenceID": 26, "context": "However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [21], and the availability of highperformance and robust algorithms for signal analysis and interpretation that can effectively handle individual differences and non-stationarity [12], [25], [28], [50].", "startOffset": 370, "endOffset": 374}, {"referenceID": 47, "context": "However, there are still many challenges in their transition from laboratory settings to real-life applications, including the reliability and convenience of the sensing hardware [21], and the availability of highperformance and robust algorithms for signal analysis and interpretation that can effectively handle individual differences and non-stationarity [12], [25], [28], [50].", "startOffset": 376, "endOffset": 380}, {"referenceID": 32, "context": "Transfer learning (TL) [34], which improves learning in a new task by leveraging data or knowledge from other relevant tasks, represents a promising solution to the above challenge.", "startOffset": 23, "endOffset": 27}, {"referenceID": 47, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 59, "endOffset": 63}, {"referenceID": 6, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 111, "endOffset": 114}, {"referenceID": 11, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 116, "endOffset": 120}, {"referenceID": 37, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 122, "endOffset": 126}, {"referenceID": 39, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 128, "endOffset": 132}, {"referenceID": 17, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 219, "endOffset": 223}, {"referenceID": 18, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 225, "endOffset": 229}, {"referenceID": 52, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 231, "endOffset": 235}, {"referenceID": 59, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 237, "endOffset": 241}, {"referenceID": 0, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 407, "endOffset": 410}, {"referenceID": 39, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 412, "endOffset": 416}, {"referenceID": 44, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 418, "endOffset": 422}, {"referenceID": 40, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 442, "endOffset": 446}, {"referenceID": 41, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 448, "endOffset": 452}, {"referenceID": 50, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 477, "endOffset": 481}, {"referenceID": 56, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 483, "endOffset": 487}, {"referenceID": 57, "context": "Many TL approaches have been proposed for BCI applications [50], including: 1) feature representation transfer [7], [13], [39], [41], which encodes the knowledge across different tasks as features; 2) instance transfer [19], [20], [56], [63], which uses certain parts of the data from other tasks to help the learning for the current task; and, 3) classifier transfer, which includes domain adaptation (DA) [1], [41], [47], ensemble learning [42], [43], and their combinations [54], [60], [61].", "startOffset": 489, "endOffset": 493}, {"referenceID": 36, "context": "This is a very important problem because drowsy driving is among the most important causes of road crashes, following only to alcohol, speeding, and inattention [38].", "startOffset": 161, "endOffset": 165}, {"referenceID": 48, "context": "However, to our best knowledge, there have been only two works [51], [54] on TL for drowsiness estimation.", "startOffset": 63, "endOffset": 67}, {"referenceID": 50, "context": "However, to our best knowledge, there have been only two works [51], [54] on TL for drowsiness estimation.", "startOffset": 69, "endOffset": 73}, {"referenceID": 48, "context": "[51] showed that selective TL, which selectively turns TL on or off based the level of session generalizability, can achieve better estimation performance than approaches that always turn TL on or off.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[54] proposed a domain adaptation with model fusion (DAMF) approach for drowsiness estimation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "In this paper, by making use of fuzzy sets (FSs) [67], we extend our earlier work on online weighted adaptation", "startOffset": 49, "endOffset": 53}, {"referenceID": 56, "context": "regularization [60] from classification to regression to estimate driver drowsiness online from EEG signals.", "startOffset": 15, "endOffset": 19}, {"referenceID": 56, "context": "ONLINE WEIGHTED ADAPTATION REGULARIZATION FOR REGRESSION (OWARR) In [60] we have defined two types of calibration in BCI: 1) Offline calibration, in which a pool of unlabeled EEG epochs have been obtained a priori, and a subject or an oracle is queried to label some of these epochs, which are then used to train a model to label the remaining epochs in the pool.", "startOffset": 68, "endOffset": 72}, {"referenceID": 56, "context": "This section introduces the OwARR algorithm, which extends the online weighted adaptation regularization algorithm [60] from classification to regression, by making use of FSs.", "startOffset": 115, "endOffset": 119}, {"referenceID": 21, "context": "Problem Definition A domain [23], [34] D in TL consists of a d-dimensional feature space X and a marginal probability distribution P (x), i.", "startOffset": 28, "endOffset": 32}, {"referenceID": 32, "context": "Problem Definition A domain [23], [34] D in TL consists of a d-dimensional feature space X and a marginal probability distribution P (x), i.", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "A task [23], [34] T in TL consists of an output space Y and a conditional probability distribution Q(y|x).", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "A task [23], [34] T in TL consists of an output space Y and a conditional probability distribution Q(y|x).", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "Marginal Probability Distribution Adaptation As in [23], [36], [60], [61], we compute d(P , P ) using the maximum mean discrepancy (MMD):", "startOffset": 51, "endOffset": 55}, {"referenceID": 34, "context": "Marginal Probability Distribution Adaptation As in [23], [36], [60], [61], we compute d(P , P ) using the maximum mean discrepancy (MMD):", "startOffset": 57, "endOffset": 61}, {"referenceID": 56, "context": "Marginal Probability Distribution Adaptation As in [23], [36], [60], [61], we compute d(P , P ) using the maximum mean discrepancy (MMD):", "startOffset": 63, "endOffset": 67}, {"referenceID": 57, "context": "Marginal Probability Distribution Adaptation As in [23], [36], [60], [61], we compute d(P , P ) using the maximum mean discrepancy (MMD):", "startOffset": 69, "endOffset": 73}, {"referenceID": 21, "context": "In [23], [60], [61] a classification problem is considered, and it is more straightforward to perform conditional probability distribution adaptation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 56, "context": "In [23], [60], [61] a classification problem is considered, and it is more straightforward to perform conditional probability distribution adaptation.", "startOffset": 9, "endOffset": 13}, {"referenceID": 57, "context": "In [23], [60], [61] a classification problem is considered, and it is more straightforward to perform conditional probability distribution adaptation.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 220, "endOffset": 224}, {"referenceID": 63, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 226, "endOffset": 230}, {"referenceID": 4, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 286, "endOffset": 289}, {"referenceID": 13, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 291, "endOffset": 295}, {"referenceID": 22, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 297, "endOffset": 301}, {"referenceID": 9, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 326, "endOffset": 330}, {"referenceID": 20, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 332, "endOffset": 336}, {"referenceID": 31, "context": "In this subsection we first briefly introduce the technique used there, and then describe in detail how we can perform conditional probability distribution adaptation in regression in a similar way, with the help of FSs [16], [67], which have been widely used in EEG feature extraction [5], [15], [24] and pattern recognition [11], [22], [33].", "startOffset": 338, "endOffset": 342}, {"referenceID": 21, "context": "Then, the distance between the conditional probability distributions in source and target domains is computed as the sum of the Euclidian distances between the class means in the two domains [23], [60], [61], i.", "startOffset": 191, "endOffset": 195}, {"referenceID": 56, "context": "Then, the distance between the conditional probability distributions in source and target domains is computed as the sum of the Euclidian distances between the class means in the two domains [23], [60], [61], i.", "startOffset": 197, "endOffset": 201}, {"referenceID": 57, "context": "Then, the distance between the conditional probability distributions in source and target domains is computed as the sum of the Euclidian distances between the class means in the two domains [23], [60], [61], i.", "startOffset": 203, "endOffset": 207}, {"referenceID": 15, "context": ",n+m, define three FSs, Small, 2There is a popular regression analysis method called quantile regression [17] in statistics and econometrics, which estimates either the conditional median or other quantiles of the response variable.", "startOffset": 105, "endOffset": 109}, {"referenceID": 28, "context": ", we could use Gaussian FSs instead of triangular FSs, use p 10 , p 50 and p 90 instead of p 5 , p 50 and p 95 , use other than three FSs in each domain, or use type-2 FSs [30] instead of type-1.", "startOffset": 172, "endOffset": 176}, {"referenceID": 45, "context": "The sample Pearson correlation coefficient r(y, f(x)) is defined as [48]:", "startOffset": 68, "endOffset": 72}, {"referenceID": 56, "context": "A SDS procedure for online classification problems has been proposed in [60].", "startOffset": 72, "endOffset": 76}, {"referenceID": 50, "context": "We reused the experiment setup and data in [54].", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "16 healthy subjects with normal or corrected to normal vision were recruited to participate in a sustained-attention driving experiment [3], [4], which consisted of a real vehicle mounted on a motion platform with 6 degrees of freedom immersed in a 360degree virtual-reality scene.", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": "16 healthy subjects with normal or corrected to normal vision were recruited to participate in a sustained-attention driving experiment [3], [4], which consisted of a real vehicle mounted on a motion platform with 6 degrees of freedom immersed in a 360degree virtual-reality scene.", "startOffset": 141, "endOffset": 144}, {"referenceID": 47, "context": "Set t = 0, and start the calibration; while t < tmax do if t \u2265 30 then Compute the drowsiness index y in (5); // Pre-processing Extract 30s EEG data in [t\u2212 30, t]; Band-pass filter the EEG signals to [0, 50] Hz; Down-sample to 250Hz; Re-reference to averaged earlobes; Compute the PSD in the [4, 7.", "startOffset": 200, "endOffset": 207}, {"referenceID": 0, "context": ", Z do // Feature extraction Concatenate each channel of the powers of the z subject with the corresponding powers of the new subject; Remove channels which have at least one power larger than a certain threshold; Normalize the powers of each remaining channel to mean 0 and std 1; Put all the powers in a matrix, whose rows represent different EEG channels; Extract a few leading principal components of the power matrix that account for 95% of the variance; Find the corresponding scores of the leading principal components; Normalize each dimension of the scores to [0, 1]; Collect the scores for each epoch as features; Record the parameters of the feature extraction method (channels removed, principal components used, ranges used in normalization) as FE; // OwARR training Construct X in (4), y in (5), E in (6), MP in (10), and MQ in (15); Compute \u03b1 by (21) and record it as \u03b1; Use \u03b1 to estimate the outputs for all known samples and record the root mean squared error as a; Assign the z regression model a weight w = 1/a; end Return The OwARR regression model f(x) = \u2211 Z", "startOffset": 569, "endOffset": 575}, {"referenceID": 48, "context": "We defined a function [51], [54] to map the response time \u03c4 to a drowsiness index y \u2208 [0, 1]:", "startOffset": 22, "endOffset": 26}, {"referenceID": 50, "context": "We defined a function [51], [54] to map the response time \u03c4 to a drowsiness index y \u2208 [0, 1]:", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "We defined a function [51], [54] to map the response time \u03c4 to a drowsiness index y \u2208 [0, 1]:", "startOffset": 86, "endOffset": 92}, {"referenceID": 50, "context": "\u03c40 = 1 was used in this paper, as in [54].", "startOffset": 37, "endOffset": 41}, {"referenceID": 24, "context": "This does not reduce the sensitivity of the drowsiness index because the cycle lengths of drowsiness fluctuations are longer than 4 minutes [26].", "startOffset": 140, "endOffset": 144}, {"referenceID": 5, "context": "We used EEGLAB [6] for EEG signal preprocessing.", "startOffset": 15, "endOffset": 18}, {"referenceID": 49, "context": "5 Hz) for each channel using Welch\u2019s method [52], as research [27] has shown that theta band spectrum is a strong indicator of drowsiness.", "startOffset": 44, "endOffset": 48}, {"referenceID": 25, "context": "5 Hz) for each channel using Welch\u2019s method [52], as research [27] has shown that theta band spectrum is a strong indicator of drowsiness.", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "The projections of the theta band powers onto these principal components were then normalized to [0, 1] and used as our features.", "startOffset": 97, "endOffset": 103}, {"referenceID": 50, "context": "Algorithms We compared the performances of OwARR and OwARRSDS with three other algorithms introduced in [54]: 1) Baseline 1 (BL1), which combines data from all 14 existing subjects, builds a ridge regression model [10], and applies it to the new subject.", "startOffset": 104, "endOffset": 108}, {"referenceID": 50, "context": "01 was used in the above three algorithms, as in [54].", "startOffset": 49, "endOffset": 53}, {"referenceID": 7, "context": "Then, non-parametric multiple comparison tests using Dunn\u2019s procedure [8], [9] were used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the False Discovery Rate method [2].", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "Then, non-parametric multiple comparison tests using Dunn\u2019s procedure [8], [9] were used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the False Discovery Rate method [2].", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "Then, non-parametric multiple comparison tests using Dunn\u2019s procedure [8], [9] were used to determine if the difference between any pair of algorithms was statistically significant, with a p-value correction using the False Discovery Rate method [2].", "startOffset": 246, "endOffset": 249}, {"referenceID": 4, "context": "for m \u2208 [5, 75].", "startOffset": 8, "endOffset": 15}, {"referenceID": 64, "context": "According to [68], there are two types of noises: class noise, which is the noise on the model outputs, and attribute noise, which is the noise on the model inputs.", "startOffset": 13, "endOffset": 17}, {"referenceID": 64, "context": "As in [68], for each model input, we randomly replaced q% (q = 0, 10, .", "startOffset": 6, "endOffset": 10}, {"referenceID": 64, "context": ", noise correction [68], before applying OwARR and OwARR-SDS.", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "4], to \u03bb in the range of [1, 20], and to \u03b3 in the range of [0.", "startOffset": 25, "endOffset": 32}, {"referenceID": 18, "context": "4], to \u03bb in the range of [1, 20], and to \u03b3 in the range of [0.", "startOffset": 25, "endOffset": 32}, {"referenceID": 55, "context": "adaptation regularization (wAR) algorithm [59], [61] for offline BCI classification problems, an online weighted adaptation regularization (OwAR) algorithm [60] for online BCI classification problems, and a SDS approach [60], [61] to reduce the computational cost of wAR and OwAR.", "startOffset": 42, "endOffset": 46}, {"referenceID": 57, "context": "adaptation regularization (wAR) algorithm [59], [61] for offline BCI classification problems, an online weighted adaptation regularization (OwAR) algorithm [60] for online BCI classification problems, and a SDS approach [60], [61] to reduce the computational cost of wAR and OwAR.", "startOffset": 48, "endOffset": 52}, {"referenceID": 56, "context": "adaptation regularization (wAR) algorithm [59], [61] for offline BCI classification problems, an online weighted adaptation regularization (OwAR) algorithm [60] for online BCI classification problems, and a SDS approach [60], [61] to reduce the computational cost of wAR and OwAR.", "startOffset": 156, "endOffset": 160}, {"referenceID": 56, "context": "adaptation regularization (wAR) algorithm [59], [61] for offline BCI classification problems, an online weighted adaptation regularization (OwAR) algorithm [60] for online BCI classification problems, and a SDS approach [60], [61] to reduce the computational cost of wAR and OwAR.", "startOffset": 220, "endOffset": 224}, {"referenceID": 57, "context": "adaptation regularization (wAR) algorithm [59], [61] for offline BCI classification problems, an online weighted adaptation regularization (OwAR) algorithm [60] for online BCI classification problems, and a SDS approach [60], [61] to reduce the computational cost of wAR and OwAR.", "startOffset": 226, "endOffset": 230}, {"referenceID": 56, "context": "Meanwhile, we have also extended the SDS algorithm for classification in [60] to regression problems, and verified that OwARR-SDS can achieve similar performance to OwARR, but save about half of the computation time.", "startOffset": 73, "endOffset": 77}, {"referenceID": 48, "context": "This indicates that they still have room for improvement: we could develop a mechanism to switch between BL1 and OwARR (OwARRSDS) so that a more appropriate method is chosen according to the characteristics of the new subject, similar to the idea of selective TL [51].", "startOffset": 263, "endOffset": 267}, {"referenceID": 57, "context": "Second, we will extend OwARR and OwARRSDS to offline calibration, where the goal is to automatically label some initially unlabeled subject-specific samples with a small number of queries [61].", "startOffset": 188, "endOffset": 192}, {"referenceID": 54, "context": "In online calibration it is not easy to estimate the testing RMSEs because we do not know what samples will be encountered in the future; however, in offline calibration we can better estimate the testing performances of the base learners using a spectral meta-learner approach [58], and hence a better model fusion strategy could be developed.", "startOffset": 278, "endOffset": 282}, {"referenceID": 27, "context": "Fourth, similar to offline classification problems [29], [55], [59], in offline regression problems we can also integrate DA with active learning [40], [57] to further reduce the offline calibration effort.", "startOffset": 51, "endOffset": 55}, {"referenceID": 51, "context": "Fourth, similar to offline classification problems [29], [55], [59], in offline regression problems we can also integrate DA with active learning [40], [57] to further reduce the offline calibration effort.", "startOffset": 57, "endOffset": 61}, {"referenceID": 55, "context": "Fourth, similar to offline classification problems [29], [55], [59], in offline regression problems we can also integrate DA with active learning [40], [57] to further reduce the offline calibration effort.", "startOffset": 63, "endOffset": 67}, {"referenceID": 38, "context": "Fourth, similar to offline classification problems [29], [55], [59], in offline regression problems we can also integrate DA with active learning [40], [57] to further reduce the offline calibration effort.", "startOffset": 146, "endOffset": 150}, {"referenceID": 53, "context": "Fourth, similar to offline classification problems [29], [55], [59], in offline regression problems we can also integrate DA with active learning [40], [57] to further reduce the offline calibration effort.", "startOffset": 152, "endOffset": 156}, {"referenceID": 60, "context": "speech signals [64] in affective computing.", "startOffset": 15, "endOffset": 19}, {"referenceID": 63, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 40, "endOffset": 44}, {"referenceID": 46, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 130, "endOffset": 134}, {"referenceID": 61, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 136, "endOffset": 140}, {"referenceID": 33, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 154, "endOffset": 158}, {"referenceID": 58, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 160, "endOffset": 164}, {"referenceID": 62, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 166, "endOffset": 170}, {"referenceID": 12, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 195, "endOffset": 199}, {"referenceID": 43, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 201, "endOffset": 205}, {"referenceID": 28, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 223, "endOffset": 227}, {"referenceID": 29, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 229, "endOffset": 233}, {"referenceID": 35, "context": "FS theory was first introduced by Zadeh [67] in 1965 and has been successfully used in many areas, including modeling and control [49], [65], data mining [35], [62], [66], time-series prediction [14], [46], decision making [30], [31], [37], etc.", "startOffset": 235, "endOffset": 239}, {"referenceID": 0, "context": "A FS X is comprised of a universe of discourse DX of real numbers together with a membership function (MF) \u03bc X : DX \u2192 [0, 1], i.", "startOffset": 118, "endOffset": 124}], "year": 2017, "abstractText": "One big challenge that hinders the transition of brain-computer interfaces (BCIs) from laboratory settings to real-life applications is the availability of high-performance and robust learning algorithms that can effectively handle individual differences, i.e., algorithms that can be applied to a new subject with zero or very little subject-specific calibration data. Transfer learning and domain adaptation have been extensively used for this purpose. However, most previous works focused on classification problems. This paper considers an important regression problem in BCI, namely, online driver drowsiness estimation from EEG signals. By integrating fuzzy sets with domain adaptation, we propose a novel online weighted adaptation regularization for regression (OwARR) algorithm to reduce the amount of subject-specific calibration data, and also a source domain selection (SDS) approach to save about half of the computational cost of OwARR. Using a simulated driving dataset with 15 subjects, we show that OwARR and OwARR-SDS can achieve significantly smaller estimation errors than several other approaches. We also provide comprehensive analyses on the robustness of OwARR and OwARR-SDS.", "creator": "LaTeX with hyperref package"}}}