{"id": "1702.03006", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2017", "title": "Multi-step Off-policy Learning Without Importance Sampling Ratios", "abstract": "To estimate the value functions of policies from exploratory data, most model-free off-policy algorithms rely on importance sampling, where the use of importance sampling ratios often leads to estimates with severe variance. It is thus desirable to learn off-policy without using the ratios. However, such an algorithm does not exist for multi-step learning with function approximation. In this paper, we introduce the first such algorithm based on temporal-difference (TD) learning updates. We show that an explicit use of importance sampling ratios can be eliminated by varying the amount of bootstrapping in TD updates in an action-dependent manner. Our new algorithm achieves stability using a two-timescale gradient-based TD update. A prior algorithm based on lookup table representation called Tree Backup can also be retrieved using action-dependent bootstrapping, becoming a special case of our algorithm. In two challenging off-policy tasks, we demonstrate that our algorithm is stable, effectively avoids the large variance issue, and can perform substantially better than its state-of-the-art counterpart.", "histories": [["v1", "Thu, 9 Feb 2017 22:36:25 GMT  (436kb,D)", "http://arxiv.org/abs/1702.03006v1", "24 pages, 4 figures"]], "COMMENTS": "24 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ashique rupam mahmood", "huizhen yu", "richard s sutton"], "accepted": false, "id": "1702.03006"}, "pdf": {"name": "1702.03006.pdf", "metadata": {"source": "CRF", "title": "Multi-step Off-policy Learning Without Importance Sampling Ratios", "authors": ["A. Rupam Mahmood", "Huizhen Yu", "Richard S. Sutton"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "In recent years, the number of out-of-school predictions has also proven to be important for model learning, options learning (Sutton et al. 1999), the ability to evaluate a policy without actually pursuing it can be invaluable (White, Modayil & Sutton 2012), and knowledge representation (White 2015) The number of computationally scalable algorithms is also important for model learning, options learning (Sutton et al.), scalable lifelong learning (White, Modayil & Sutton al.)."}, {"heading": "2. Problem formulation and notations", "text": "In this section, we formulate the problem of multi-stage solution with parametric functional approximation and establish notations. Consider an agent in a dynamic environment with a finite state space S and room for manoeuvre A. At any time t = 0, 1,.. if the current state is S and the agent takes action, then the next state St + 1 s \u00b2 S is with a probability p (s), and the agent receives a random reward Rt + 1 with mean r (s, a) and finite variance on the transition to the state. A randomized stationary policy \u03c0 specifies the probability with which he takes action. Of our interest is a predetermined policy approach called target policy, and the performance of the agent when it follows. Specifically, our interest in this paper assumes that the action value function of the endp, defined as the expected sum of discounted rewards for each initial state action (s, a)."}, {"heading": "3. The advantage of multi-step learning", "text": "On one side of the spectrum, we have single-level methods that fully exploit all future rewards for performing updates. Many multi-level learning algorithms integrate this full spectrum and can smoothly vary between single-level and Monte Carlo updates using the bootstrapping parameter. Here, 1 \u2212 \u03bb determines the degree to which bootstrapping is used in the algorithm. With \u03bb = 0, these algorithms achieve single-level TD updates while effectively achieving Monte Carlo updates. Unlike single-level learning, multi-level learning is generally regarded as learning without being used in TD > 0.Multi-level learning has a twofold effect on the efficiency of estimation."}, {"heading": "4. Multi-step off-policy learning with importance sampling ratios", "text": "In order to set the stage for our work, we describe in this section the canonical multi-step learning progress = > Quantity learning progress = > Quantity learning progress with meaning sampling rates, and how rates introduce variations in offline updates for off-policy difference (TD) updates. A TD update is generally constructed on the basis of stochastic alignment methods, with the objective of the update based on returns. (Sutton & Barto 1998, Seijen et al. 2016) An offline update of the offline TD update for off-policy action value estimates can be defined as useful and develop the basis for deriving practical and accounting efficient algorithms (Sutton & Barto 1998, Seijen et al. 2016). An offline update of the TD for off-policy value estimation based on multi-step returns can be defined as 0."}, {"heading": "5. Avoiding importance sampling ratios", "text": "In this section, we present the idea of action-dependent bootstrapping effects of significance and general meaning to define the impact on the meaning of sampling ratios in offshore politics. To do this, we first introduce an action-dependent bootstrapping parameter used by Sutton and Singh (1994) and Sutton et al. (2014) for state-value estimation, and by Maei and Sutton (2010) for action-value estimation. In this work, the degree of bootstrapping effects is allowed from one state to another. (0, 1), however, has not been used as a tool to reduce the variability of the estimate. (s, a) can be used algorithmically to absorb the effects of sampling variance of the parameters."}, {"heading": "6. The ABQ(\u03b6) algorithm with gradient correction and scalable updates", "text": "In this section, we will develop a mathematically scalable and stable algorithm that matches the solution of ABQ = > Q = > Q = >. Previous updates (15) cannot be calculated in a scalable way because forward-looking updates require an increasing amount of calculation as time progresses. In addition, off-policy algorithms with bootstrapping and function approximation may be unstable (Sutton & Barto 1998) unless the mechanisms for ensuring stability are integrated. Our goal is to develop a stable update that complies with ABQ solutions while sampling them without explicit use of meaning ratios.First, we produce the equivalent backward view to (15) so that the updates can be implemented without explicitly applying application.2 Since we derive these updates from Appendix A.2, these updates will be applied by the application.wt ="}, {"heading": "7. Experimental results", "text": "We empirically assess ABQ on an order of magnitude that can be as large as 9 simulated Q. We empirically assess ABQ at a time when three policy assessment tasks: the two-state task of Section 5, an off-policy assessment of Section 5, an off-policy assessment of Section 5, an off-policy assessment of Section 5, an off-policy assessment of Section 5, an appropriateness of Section 4 that can produce correct estimates with less deviation compared to GQ (May 2011), the state-of-the-art significance of sampling-based algorithms for action value estimation with functional approximation. We validate the stability of ABQ in the final task in which off-policy algorithms with no guarantee of stability are used (e.g., Off-Policy Q) tends to diverge. Although the MDP is involved in the two-state task, the algorithms may be difficult to relate to this task as they may be large in importance during the task."}, {"heading": "8. Action-dependent bootstrapping as a framework for off-policy", "text": "However, it is not the only action-based bootstrapping scheme that can be better understood. Here, we show how the retrace algorithms of Munos et al. (2016) can be understood using other action-based bootstrapping schemes. (2016) The different schemes not only allow us to derive new algorithms, they can also be used to better understand some existing algorithms. Here, we show how the retrace algorithms of Munos et al. (2016) can be understood in relation to a particular way of setting the action-based bootstrapping parameters."}, {"heading": "9. Related works", "text": "A closely related algorithm is Tree Backup by Precup et al. (2000). This algorithm can also be produced as a special case of ABQ (\u0430) if we remove the gradient correction, always consider the feature vectors as the default basis and always set to a constant instead of setting them in an action-dependent manner. In practical terms, the tree backup algorithm fails to achieve the TD (1) solution, whereas ABQ achieves it with the tree backup algorithm. Our work is not a trivial generalization of this earlier work. The tree backup algorithm was developed using a different intuition based on backup diagrams and was only introduced in the case of the lookup table."}, {"heading": "10. Discussion and conclusions", "text": "The key to this algorithm is to vary the scope of bootstrapping in an action-dependent manner, rather than keeping it constant or just varying it with states. Part of this action-dependent bootstrapping factor mitigates the importance of sampling ratios, while the rest of the factor is spent on achieving multi-level bootstrapping. The resulting effect is that the problem of large variance with sampling ratios is easily eliminated without forgoing multi-level learning, making ABQ (BA) more practical for practical use. Action-dependent bootstrapping provides a revealing and well-founded framework for deriving off-policy algorithms with reduced variance. The same idea can be applied to government estimates, in which case ratios cannot be completely eliminated; however, a reduction in variance can be expected."}], "references": [{"title": "Policy evaluation with temporal differences: a survey and comparison", "author": ["C. Dann", "G. Neumann", "J. Peters"], "venue": "Journal of Machine Learning Research, 15 :809\u2013883.", "citeRegEx": "Dann et al\\.,? 2014", "shortCiteRegEx": "Dann et al\\.", "year": 2014}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L.C. Baird"], "venue": "Proceedings of the 12th International Conference on Machine Learning, pp. 30\u201337.", "citeRegEx": "Baird,? 1995", "shortCiteRegEx": "Baird", "year": 1995}, {"title": "Doubly robust policy evaluation and learning", "author": ["M. Dud\u0301\u0131k", "J. Langford", "L. Li"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Dud\u0301\u0131k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0301\u0131k et al\\.", "year": 2011}, {"title": "Off-policy learning with eligibility traces: A survey", "author": ["M. Geist", "B. Scherrer"], "venue": "Journal of Machine Learning Research, 15 :289\u2013333.", "citeRegEx": "Geist and Scherrer,? 2014", "shortCiteRegEx": "Geist and Scherrer", "year": 2014}, {"title": "Generalized emphatic temporal difference learning: bias-variance analysis", "author": ["A. Hallak", "A. Tamar", "R. Munos", "S. Mannor"], "venue": "arXiv preprint arXiv:1509.05172.", "citeRegEx": "Hallak et al\\.,? 2015a", "shortCiteRegEx": "Hallak et al\\.", "year": 2015}, {"title": "Off-policy model-based learning under unknown factored dynamics", "author": ["A. Hallak", "F. Schnitzler", "T. Mann", "S. Mannor"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, pp. 711\u2013719.", "citeRegEx": "Hallak et al\\.,? 2015b", "shortCiteRegEx": "Hallak et al\\.", "year": 2015}, {"title": "Monte Carlo methods, Methuen & co", "author": ["Hammersley", "D.C.J.M. Handscomb"], "venue": "Ltd., London, pp. 40,", "citeRegEx": "Hammersley and Handscomb,? 1964", "shortCiteRegEx": "Hammersley and Handscomb", "year": 1964}, {"title": "Q (\u03bb) with off-policy corrections", "author": ["A. Harutyunyan", "M.G. Bellemare", "T. Stepleton", "R. Munos"], "venue": "arXiv preprint arXiv:1602.04951.", "citeRegEx": "Harutyunyan et al\\.,? 2016", "shortCiteRegEx": "Harutyunyan et al\\.", "year": 2016}, {"title": "Two timescale stochastic approximation with controlled Markov noise and off-policy temporal difference learning.arXiv preprint arXiv:1503", "author": ["P. Karmakar", "S. Bhatnagar"], "venue": null, "citeRegEx": "Karmakar and Bhatnagar,? \\Q2015\\E", "shortCiteRegEx": "Karmakar and Bhatnagar", "year": 2015}, {"title": "Doubly robust off-policy evaluation for reinforcement learning", "author": ["N. Jiang", "L. Li"], "venue": "arXiv preprint arXiv:1511.03722.", "citeRegEx": "Jiang and Li,? 2015", "shortCiteRegEx": "Jiang and Li", "year": 2015}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT Press, 2009.", "citeRegEx": "Koller and Friedman,? 2009", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Toward minimax off-policy value estimation", "author": ["L. Li", "R. Munos", "C. Szepesvari"], "venue": "Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, pp. 608\u2013616.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Monte Carlo strategies in scientific computing", "author": ["J.S. Liu"], "venue": "Berlin, Springer-Verlag. 16", "citeRegEx": "Liu,? 2001", "shortCiteRegEx": "Liu", "year": 2001}, {"title": "GQ(\u03bb): A general gradient algorithm for temporaldifference prediction learning with eligibility traces", "author": ["H.R. Maei", "R.S. Sutton"], "venue": "Proceedings of the Third Conference on Artificial General Intelligence, pp. 91\u201396. Atlantis Press.", "citeRegEx": "Maei and Sutton,? 2010", "shortCiteRegEx": "Maei and Sutton", "year": 2010}, {"title": "Gradient Temporal-Difference Learning Algorithms", "author": ["H.R. Maei"], "venue": "PhD thesis, University of Alberta.", "citeRegEx": "Maei,? 2011", "shortCiteRegEx": "Maei", "year": 2011}, {"title": "Weighted importance sampling for off-policy learning with linear function approximation", "author": ["A.R. Mahmood", "H. van Hasselt", "R.S. Sutton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mahmood et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2014}, {"title": "Off-policy learning based on weighted importance sampling with linear computational complexity", "author": ["A.R. Mahmood", "R.S. Sutton"], "venue": "Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence.", "citeRegEx": "Mahmood and Sutton,? 2015", "shortCiteRegEx": "Mahmood and Sutton", "year": 2015}, {"title": "Emphatic temporal-difference learning", "author": ["A.R. Mahmood", "H. Yu", "M. White", "R.S. Sutton"], "venue": "European Workshop on Reinforcement Learning 12,arXiv preprint ArXiv:1507. 01569.", "citeRegEx": "Mahmood et al\\.,? 2015", "shortCiteRegEx": "Mahmood et al\\.", "year": 2015}, {"title": "Adaptive lambda least-squares temporal difference learning", "author": ["T.A. Mann", "H. Penedones", "S. Mannor", "T. Hester"], "venue": "arXiv preprint arXiv:1612.09465.", "citeRegEx": "Mann et al\\.,? 2016", "shortCiteRegEx": "Mann et al\\.", "year": 2016}, {"title": "Off-policy Evaluation in Markov Decision Processes, PhD thesis, McGill University", "author": ["C. Paduraru"], "venue": null, "citeRegEx": "Paduraru,? \\Q2013\\E", "shortCiteRegEx": "Paduraru", "year": 2013}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": "Proceedings of the 17th International Conference on Machine Learning, pp. 759\u2013766. Morgan Kaufmann.", "citeRegEx": "Precup et al\\.,? 2000", "shortCiteRegEx": "Precup et al\\.", "year": 2000}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["D. Precup", "R.S. Sutton", "S. Dasgupta"], "venue": "Proceedings of the 18th International Conference on Machine Learning.", "citeRegEx": "Precup et al\\.,? 2001", "shortCiteRegEx": "Precup et al\\.", "year": 2001}, {"title": "Safe and efficient offpolicy reinforcement learning", "author": ["R Munos", "T Stepleton", "A Harutyunyan", "M.G. Bellemare"], "venue": "In Proceedings of Neural Information Processing Systems", "citeRegEx": "Munos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munos et al\\.", "year": 2016}, {"title": "Simulation and the Monte Carlo Method", "author": ["R.Y. Rubinstein"], "venue": null, "citeRegEx": "Rubinstein,? \\Q1981\\E", "shortCiteRegEx": "Rubinstein", "year": 1981}, {"title": "On bias and step size in temporal-difference learning", "author": ["R.S. Sutton", "S.P. Singh"], "venue": "Proceedings of the Eighth Yale Workshop on Adaptive and Learning Systems, pp. 91-96.", "citeRegEx": "Sutton and Singh,? 1994", "shortCiteRegEx": "Sutton and Singh", "year": 1994}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial intelligence, 112(1), 181\u2013211.", "citeRegEx": "Sutton et al\\.,? 1999", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "A new Q(\u03bb) with interim forward view and Monte Carlo equivalence", "author": ["R.S. Sutton", "A.R. Mahmood", "D. Precup", "H. van Hasselt"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Sutton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2014}, {"title": "An emphatic approach to the problem of off-policy temporal-difference learning", "author": ["R.S. Sutton", "A.R. Mahmood", "M. White"], "venue": "Journal of Machine Learning Research 17, (73):1\u2013", "citeRegEx": "Sutton et al\\.,? 2016", "shortCiteRegEx": "Sutton et al\\.", "year": 2016}, {"title": "Safe Reinforcement Learning", "author": ["P.S. Thomas"], "venue": "PhD thesis, University of Massachusetts Amherst.", "citeRegEx": "Thomas,? 2015", "shortCiteRegEx": "Thomas", "year": 2015}, {"title": "Data-efficient off-policy policy evaluation for reinforcement learning", "author": ["P.S. Thomas", "E. Brunskill"], "venue": "arXiv preprint arXiv:1604.00923.", "citeRegEx": "Thomas and Brunskill,? 2016", "shortCiteRegEx": "Thomas and Brunskill", "year": 2016}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "IEEE Transactions on Automatic Control, 42(5), 674\u2013690.", "citeRegEx": "Tsitsiklis and Roy,? 1997", "shortCiteRegEx": "Tsitsiklis and Roy", "year": 1997}, {"title": "Insights in Reinforcement Learning: formal analysis and empirical evaluation of temporal-difference learning algorithms", "author": ["H. van Hasselt"], "venue": "PhD thesis,", "citeRegEx": "Hasselt,? \\Q2011\\E", "shortCiteRegEx": "Hasselt", "year": 2011}, {"title": "Off-policy TD(\u03bb) with a true online equivalence", "author": ["H. van Hasselt", "A.R. Mahmood", "R.S. Sutton"], "venue": "In Proceedings of the 30th Conference on Uncertainty", "citeRegEx": "Hasselt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2014}, {"title": "True online TD(\u03bb)", "author": ["H. van Seijen", "R.S. Sutton"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Seijen and Sutton,? \\Q2014\\E", "shortCiteRegEx": "Seijen and Sutton", "year": 2014}, {"title": "Scaling life-long off-policy learning", "author": ["A. White", "J. Modayil", "R.S. Sutton"], "venue": "Proceedings of the Second Joint IEEE International Conference on Development and Learning and on Epigenetic Robotics, San Diego, USA.", "citeRegEx": "White et al\\.,? 2012", "shortCiteRegEx": "White et al\\.", "year": 2012}, {"title": "Developing a Predictive Approach to Knowledge", "author": ["A. White"], "venue": "PhD thesis, University of Alberta.", "citeRegEx": "White,? 2015", "shortCiteRegEx": "White", "year": 2015}, {"title": "Investigating practical, linear temporal difference learning", "author": ["A. White", "M. White"], "venue": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, 494\u2013502.", "citeRegEx": "White and White,? 2016a", "shortCiteRegEx": "White and White", "year": 2016}, {"title": "A greedy approach to adapting the trace parameter for temporal difference learning", "author": ["M. White", "A. White"], "venue": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, 557\u2013565.", "citeRegEx": "White and White,? 2016b", "shortCiteRegEx": "White and White", "year": 2016}, {"title": "Least squares temporal difference methods: An analysis under general conditions", "author": ["H. Yu"], "venue": "SIAM Journal on Control and Optimization, 50 (6), 3310\u20133343.", "citeRegEx": "Yu,? 2012", "shortCiteRegEx": "Yu", "year": 2012}, {"title": "On convergence of emphatic temporal-difference learning", "author": ["H. Yu"], "venue": "arXiv preprint arXiv:1506.02582; a shorter version appeared in The 28th Annual Conference on Learning Theory (COLT) 2015.", "citeRegEx": "Yu,? 2015", "shortCiteRegEx": "Yu", "year": 2015}, {"title": "Weak convergence properties of constrained emphatic temporal-difference learning with constant and slowly diminishing stepsize", "author": ["H. Yu"], "venue": "Journal of Machine Learning Research 17 (220):1\u201358. 18", "citeRegEx": "Yu,? 2016", "shortCiteRegEx": "Yu", "year": 2016}], "referenceMentions": [{"referenceID": 26, "context": "Learning a large number of off-policy predictions is also considered important for model learning, options learning (Sutton et al. 1999), scalable life-long learning (White, Modayil & Sutton 2012), and knowledge representation (White 2015).", "startOffset": 116, "endOffset": 136}, {"referenceID": 36, "context": "1999), scalable life-long learning (White, Modayil & Sutton 2012), and knowledge representation (White 2015).", "startOffset": 96, "endOffset": 108}, {"referenceID": 20, "context": "A prior algorithm, Tree Backup (Precup et al. 2000), can be retrieved as a special case of our algorithm.", "startOffset": 31, "endOffset": 51}, {"referenceID": 22, "context": "Furthermore, we show that another off-policy algorithm, Retrace (Munos et al. 2016), can also be derived and extended to the case of function approximation with stability using the action-dependent bootstrapping technique.", "startOffset": 64, "endOffset": 83}, {"referenceID": 22, "context": "A closely related idea is state-dependent bootstrapping used by Sutton and Singh (1994) and Sutton et al.", "startOffset": 64, "endOffset": 88}, {"referenceID": 22, "context": "A closely related idea is state-dependent bootstrapping used by Sutton and Singh (1994) and Sutton et al. (2014) for state-value estimation, and by Maei and Sutton (2010) for action-value estimation.", "startOffset": 64, "endOffset": 113}, {"referenceID": 13, "context": "(2014) for state-value estimation, and by Maei and Sutton (2010) for action-value estimation.", "startOffset": 42, "endOffset": 65}, {"referenceID": 26, "context": "To illustrate that ABQ(\u03b6) solutions can retain much of the multi-step benefits, we used a two-state off-policy task similar to the off-policy task by Sutton et al. (2016). In this task, there were two states each with two actions, left and right, leading to one of the two states deterministically.", "startOffset": 150, "endOffset": 171}, {"referenceID": 14, "context": "We take the approach proposed by Maei (2011) to develop a stable gradient-based TD algorithm.", "startOffset": 33, "endOffset": 45}, {"referenceID": 14, "context": "In the first two tasks we investigated whether ABQ(\u03b6) can produce correct estimates with less variance compared to GQ(\u03bb) (Maei 2011), the state-of-the-art importance sampling based algorithm for action-value estimation with function approximation.", "startOffset": 121, "endOffset": 132}, {"referenceID": 1, "context": "In the 7-star Baird\u2019s counterexample, adopted from White (2015), step sizes were set as \u03b1 = 0.", "startOffset": 14, "endOffset": 64}, {"referenceID": 22, "context": "Here, we show how the Retrace algorithm by Munos et al. (2016) can be understood in terms of a particular way of setting the action-dependent bootstrapping parameter.", "startOffset": 43, "endOffset": 63}, {"referenceID": 19, "context": "Related works A closely related algorithm is Tree Backup by Precup et al. (2000). This algorithm can also be produced as a special case of ABQ(\u03b6), if we remove gradient correction, consider the feature vectors always to be the standard basis, and \u03bd\u03b6 to be always set to a constant, instead of setting it in an action-dependent manner.", "startOffset": 60, "endOffset": 81}, {"referenceID": 19, "context": "Related works A closely related algorithm is Tree Backup by Precup et al. (2000). This algorithm can also be produced as a special case of ABQ(\u03b6), if we remove gradient correction, consider the feature vectors always to be the standard basis, and \u03bd\u03b6 to be always set to a constant, instead of setting it in an action-dependent manner. In the on-policy case, the Tree Backup algorithm fails to achieve the TD(1) solution, whereas ABQ achieves it with \u03b6 = 1. Our work is not a trivial generalization of this prior work. The Tree Backup algorithm was developed using a different intuition based on backup diagrams and was introduced only for the lookup table case. Not only does ABQ(\u03b6) extend the Tree Backup algorithm, but the idea of action-dependent bootstrapping also played a crucial role in deriving the ABQ(\u03b6) algorithm with gradient correction in a principled way. Another related algorithm is Retrace, which we have already shown to be a special case of the AB-Trace algorithm. The main differences are that Retrace was introduced and analyzed in the case of tabular representation, and thus Retrace is neither stable nor shown to achieve multi-step solutions in the case of function approximation. Yet another class of related algorithms are where the bootstrapping parameter is adapted based on past data, for example, the works by White and White (2016b) and Mann et al.", "startOffset": 60, "endOffset": 1364}, {"referenceID": 18, "context": "Yet another class of related algorithms are where the bootstrapping parameter is adapted based on past data, for example, the works by White and White (2016b) and Mann et al. (2016). Beside adapting on a state-action-pair basis, the main difference between those algorithms and the ones introduced here under the action-dependent bootstrapping framework is that our algorithms adapt the bootstrapping parameter using only the knowledge of policy probabilities for the current state-action pair whereas the algorithms in the other class involve a separate learning procedure for adapting \u03bb and hence are more complex.", "startOffset": 163, "endOffset": 182}, {"referenceID": 20, "context": "According to an alternative explanation based on backup diagrams (Precup et al. 2000), ABQ updates may be seen as devoid of importance sampling ratios, while the action-dependent bootstrapping framework can now provide us a clearer mechanistic view of how updates without importance sampling ratios can perform off-policy learning.", "startOffset": 65, "endOffset": 85}, {"referenceID": 8, "context": "The convergence of this algorithm can be analyzed similarly to the work by Karmakar and Bhatnagar (2015) for diminishing step sizes and Yu (2015) for constant step sizes, by using properties of the eligibility traces and stochastic approximation theory.", "startOffset": 75, "endOffset": 105}, {"referenceID": 8, "context": "The convergence of this algorithm can be analyzed similarly to the work by Karmakar and Bhatnagar (2015) for diminishing step sizes and Yu (2015) for constant step sizes, by using properties of the eligibility traces and stochastic approximation theory.", "startOffset": 75, "endOffset": 146}], "year": 2017, "abstractText": "To estimate the value functions of policies from exploratory data, most model-free offpolicy algorithms rely on importance sampling, where the use of importance sampling ratios often leads to estimates with severe variance. It is thus desirable to learn off-policy without using the ratios. However, such an algorithm does not exist for multi-step learning with function approximation. In this paper, we introduce the first such algorithm based on temporal-difference (TD) learning updates. We show that an explicit use of importance sampling ratios can be eliminated by varying the amount of bootstrapping in TD updates in an action-dependent manner. Our new algorithm achieves stability using a two-timescale gradient-based TD update. A prior algorithm based on lookup table representation called Tree Backup can also be retrieved using action-dependent bootstrapping, becoming a special case of our algorithm. In two challenging off-policy tasks, we demonstrate that our algorithm is stable, effectively avoids the large variance issue, and can perform substantially better than its state-of-the-art counterpart.", "creator": "LaTeX with hyperref package"}}}