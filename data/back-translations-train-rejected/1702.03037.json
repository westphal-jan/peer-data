{"id": "1702.03037", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2017", "title": "Multi-agent Reinforcement Learning in Sequential Social Dilemmas", "abstract": "Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.", "histories": [["v1", "Fri, 10 Feb 2017 01:48:40 GMT  (770kb,D)", "http://arxiv.org/abs/1702.03037v1", "10 pages, 7 figures"]], "COMMENTS": "10 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.MA cs.AI cs.GT cs.LG", "authors": ["joel z leibo", "vinicius zambaldi", "marc lanctot", "janusz marecki", "thore graepel"], "accepted": false, "id": "1702.03037"}, "pdf": {"name": "1702.03037.pdf", "metadata": {"source": "CRF", "title": "Multi-agent Reinforcement Learning in Sequential Social Dilemmas", "authors": ["Joel Z. Leibo", "Vinicius Zambaldi", "Marc Lanctot", "Janusz Marecki", "Thore Graepel"], "emails": ["jzl@google.com", "vzambaldi@google.com", "lanctot@google.com", "tartel@google.com", "thore@google.com"], "sections": [{"heading": null, "text": "CCS concepts \u2022 computer methods \u2192 multi-agent reinforcement learning; agents / discrete models; stochastic games; keywords social dilemmas, cooperation, Markov games, agent-based social simulation, non-cooperative games"}, {"heading": "1. INTRODUCTION", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2. DEFINITIONS AND NOTATION", "text": "We model sequential social dilemmas as general sum-Markov games (simultaneous movement) in which each actor has only a partial observation of his local environment. Agents must learn appropriate policies while coexisting with each other. A policy is considered to be the implementation of cooperation or apostate based on the characteristics of the insights it produces. A Markov game is an SSD if and only if it contains outcomes from cooperation and renegade strategies that satisfy the same inequalities used in defining MGSDs (equations 1-4), a definition more formally elaborated in Sections 2.1 and 2.2 below."}, {"heading": "2.1 Markov Games", "text": "A two-dimensional Markov game M is defined by a series of states S and an observation function O: S \u00b7 {1, 2} \u2192 Rd, which indicates the d-dimensional view of each player, along with two groups of actions permitted from each state A1 and A2, one for each player, a transition function T: S \u00b7 A1 \u00b7 A2 \u2192 \u2206 (S), where \u2206 (S) denotes the number of discrete probability distributions above S, and a reward function for each player: ri: S \u00b7 A1 \u00b7 A2 \u2192 R for player i. Let us leave Oi = {oi | s \u00b2 S, oi = O (s, i)} the observation room of the player i. To choose measures, each player uses the yardsticks of Oi: Oi \u2192 (Ai).For temporary discounting factors (0, 1] we can freeze the long-term payment of V ~ \u03c0i (s0) to player i = freeze."}, {"heading": "2.2 Definition of Sequential Social Dilemma", "text": "This definition is based on a formalization of empirical game theory analyses [20, 21]. We define the results (R, P, S, T): = (R (s0), P (s0), S (s0), T (s0))), which are induced by the initial state s0, and two strategies \u03c0C, \u03c0D, based on their long-term expected payout (5) and the definitions (6) - (9. We refer to the game matrix with R, P, S, T organized as in Fig. 1-left. as an empirical payout matrix following the terminology of [21]. Definition: A sequential social dilemma is a tuple (M, \u0421C, \u0445D), in which \u0421C and \u0421C are set of strategies, of which it is said that cooperation and deviation are implemented in each case. M is a Markov game with state space S. Let the empirical social dilemma, in which CSS, CSS, CSS, CSS, CSS, CSS, CSS, CSS and CSS, CSS, CSS, CSS, CSS, CSS, CSS, CSS, CSS, and CSS, CSS, CSS, that cooperation and deviation are implemented in each case."}, {"heading": "3. LEARNING ALGORITHMS", "text": "Most of the earlier work on the search for strategies for Markov games is based on the prescriptive view of multi-agent learning [26]: that is, an attempt is made to answer \"what should each agent do?\" Several algorithms and analyses have been developed for the two-player zero-sum case [22, 27, 28, 29, 30]. The general sum case is much more difficult [31], and algorithms either have strong assumptions or must either pursue several potential balances per agent [32, 33], model other actors in order to simplify the problem [34], or they must find an acyclic strategy consisting of several strategies obtained by multiple state space sweeps [35]. Researchers have also examined the emergence of multi-agent coordination within the decentralized, partially observable MDP framework [36, 37, 38]. However, this approach is based on knowledge of the underlying Markov model, an unrealistic assumption of social dilemmativeness."}, {"heading": "3.1 Deep Multiagent Reinforcement Learning", "text": "Modern methods of deep amplification take the perspective of an agent who must learn to maximize his cumulative long-term reward through trial-and-error interactions with his environment. [43, 44] In the multi-agent environment, the i-th agent stores a function Qi: Oi \u00b7 Ai \u2192 R represented by a deep Q network (DQN). See [25] for details in the individual case of the agent. In our case, the true state s is observed differently by each actor, da oi = O (s, i). For the consistency of notation, however, we use an abbreviation: Qi (s, a) = Qi (s, i)."}, {"heading": "4. SIMULATION METHODS", "text": "Both games examined here were implemented in a 2D game engine. The state st and the collective action of all players ~ a determine the state in the next time step st + 1. Observations O (s, i) and R3 \u00d7 16 \u00d7 21 (RGB) of the actual state st depended on the current position and orientation of the player. The observation window extended over 15 grid squares before and 10 grid squares from side to side (see fig. 3B). Actions a) were agent centered: step forward, step backward, step left, step right, rotation left, use of bars and standstill. Each player appears in his own local view blue, light blue in the view of his teammates and red in the view of the opponent. Each episode lasted 1,000 steps. Default neural networks had two hidden layers of 32 units interspersed with reflected linear layers projected onto the output layer, each having a unit for each action."}, {"heading": "5. RESULTS", "text": "In this section, we describe three experiments: one for each game (Gathering and Wolfpack) and a third experiment, which examines parameters that influence the formation of cooperation or waste."}, {"heading": "5.1 Experiment 1: Gathering", "text": "The aim of the collecting game is to collect apples, which are represented by green pixels (see Figure 3A). If a Player Collecte1A group has a low number of game prizes, the cost of using game plans (reflected in each game plan) is considered high. (The only potential motivation for tagging is competition via the Apples. Refer to the Gathering gameplay video games is one of the aggressive strategies in this game - i.e., which involves frequent attempts to remove them from the game. Such a policy is motivated by the ability to take all the apples that reach for the other player's disposal."}, {"heading": "5.2 Experiment 2: Wolfpack", "text": "The Wolfpack game requires two players (wolves) to hunt a third player (the loot). If one of the two wolves touches the loot, all wolves within the catch radius (see fig. 3B) will receive a reward. However, if the two wolves capture the loot together, they will be better able to protect the carcass from the scavengers and thus receive a higher reward. A lone wolf that preys on the loot will offer a reward to scavengers and a capture involving both wolves is worth rteam. If we adopt the Wolfpack game video4 for demonstration purposes, the wolves will learn to catch the loot during the training and capture wolves that work during the training."}, {"heading": "5.3 Experiment 3: Agent parameters influencing the emergence of defection", "text": "This year it is more than ever before."}, {"heading": "6. DISCUSSION", "text": "Dre eeisrteeGsrsrteeeeeteerteerrrrrrrrrrf\u00fc ide eeirlrrteeoiiiiiietcnlhsrlteeoiuiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiuiuirrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "Acknowledgments", "text": "The authors thank Chrisantha Fernando, Toby Ord and Peter Sunehag for the fruitful discussions leading up to this work and Charles Beattie, Denis Teplyashin and Stig Petersen for their support in software development."}], "references": [{"title": "Prisoner\u2019s dilemma\u2013recollections and observations. In Game Theory as a Theory of a Conflict Resolution, pages 17\u201334", "author": ["Anatol Rapoport"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1974}, {"title": "The psychology of social dilemmas: A review", "author": ["Paul AM Van Lange", "Jeff Joireman", "Craig D Parks", "Eric Van Dijk"], "venue": "Organizational Behavior and Human Decision Processes,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Learning dynamics in social dilemmas", "author": ["Michael W Macy", "Andreas Flache"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "The evolution of reciprocal altruism", "author": ["Robert L. Trivers"], "venue": "Quarterly Review of Biology,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1971}, {"title": "The Evolution of Cooperation", "author": ["Robert Axelrod"], "venue": "Basic Books,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1984}, {"title": "Tit for tat in heterogeneous populations", "author": ["Martin A Nowak", "Karl Sigmund"], "venue": "Nature, 355(6357):250\u2013253,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "A strategy of win-stay, lose-shift that outperforms tit-for-tat in the prisoner\u2019s dilemma", "author": ["Martin Nowak", "Karl Sigmund"], "venue": "game. Nature,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "Evolution of indirect reciprocity by image scoring", "author": ["Martin A Nowak", "Karl Sigmund"], "venue": "Nature, 393(6685):573\u2013577,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "An evolutionary approach to norms", "author": ["Robert Axelrod"], "venue": "American political science review,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1986}, {"title": "Cooperation emergence under resource-constrained peer punishment", "author": ["Samhar Mahmoud", "Simon Miles", "Michael Luck"], "venue": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Multiagent reinforcement learning in the iterated prisoner\u2019s", "author": ["T.W. Sandholm", "R.H. Crites"], "venue": "dilemma. Biosystems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1996}, {"title": "Learning to cooperate in multi-agent social dilemmas", "author": ["Enrique Munoz de Cote", "Alessandro Lazaric", "Marcello Restelli"], "venue": "In Proceedings of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Classes of multiagent Q-learning dynamics with greedy exploration", "author": ["M. Wunder", "M. Littman", "M. Babes"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Empirically evaluating multiagent learning", "author": ["Erik Zawadzki", "Asher Lipson", "Kevin Leyton-Brown"], "venue": "algorithms. CoRR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Evolutionary dynamics of multi-agent learning: A survey", "author": ["Daan Bloembergen", "Karl Tuyls", "Daniel Hennes", "Michael Kaisers"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Evolutionary games and spatial chaos", "author": ["Martin A Nowak", "Robert M May"], "venue": "Nature, 359(6398):826\u2013829,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1992}, {"title": "Emotional multiagent reinforcement learning in spatial social dilemmas", "author": ["Chao Yu", "Minjie Zhang", "Fenghui Ren", "Guozhen Tan"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "A simple rule for the evolution of cooperation on graphs and social", "author": ["Hisashi Ohtsuki", "Christoph Hauert", "Erez Lieberman", "Martin A Nowak"], "venue": "networks. Nature,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "A new route to the evolution of cooperation", "author": ["Francisco C Santos", "Jorge M Pacheco"], "venue": "Journal of Evolutionary Biology,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Analyzing complex strategic interactions in multi-agent systems", "author": ["William E Walsh", "Rajarshi Das", "Gerald Tesauro", "Jeffrey O Kephart"], "venue": "Workshop on Game-Theoretic and Decision-Theoretic Agents,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Methods for empirical game-theoretic analysis (extended abstract)", "author": ["Michael Wellman"], "venue": "In Proceedings of the National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["M.L. Littman"], "venue": "In Proceedings of the 11th International Conference on Machine Learning (ICML),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "Game theory and multiagent reinforcement learning", "author": ["Ann Now\u00e9", "Peter Vrancx", "Yann-Micha\u00ebl De Hauwere"], "venue": "Reinforcement Learning: State-of-the-Art,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Coordinate to cooperate or compete: abstract goals  and joint intentions in social interaction", "author": ["Max Kleiman-Weiner", "M K Ho", "J L Austerweil", "Michael L Littman", "Josh B Tenenbaum"], "venue": "In Proceedings of the 38th Annual Conference of the Cognitive Science Society,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "If multi-agent learning is the answer, what is the question", "author": ["Y. Shoham", "R. Powers", "T. Grenager"], "venue": "Artificial Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Value function approximation in zero-sum Markov games", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "In Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Approximate dynamic programming for two-player zero-sum Markov games", "author": ["J. P\u00e9rolat", "B. Scherrer", "B. Piot", "O. Pietquin"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Softened approximate policy iteration for Markov games", "author": ["J. P\u00e9rolat", "B. Piot", "M. Geist", "B. Scherrer", "O. Pietquin"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Algorithms for computing strategies in two-player simultaneous move games", "author": ["Branislav Bo\u0161ansk\u00fd", "Viliam Lis\u00fd", "Marc Lanctot", "J\u01d0\u0155\u0131 \u010cerm\u00e1k", "Mark H.M. Winands"], "venue": "Artificial Intelligence,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Cyclic equilibria in Markov games", "author": ["M. Zinkevich", "A. Greenwald", "M. Littman"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Multiagent reinforcement learning: Theoretical framework and an algorithm", "author": ["J. Hu", "M.P. Wellman"], "venue": "In Proceedings of the 15th International Conference on Machine Learning (ICML),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Correlated-Q learning", "author": ["A. Greenwald", "K. Hall"], "venue": "In Proceedings of the 20th International Conference on Machine Learning (ICML),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2003}, {"title": "Friend-or-foe Q-learning in general-sum games", "author": ["Michael Littman"], "venue": "In Proceedings of the Eighteenth International Conference on Machine Learning,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2001}, {"title": "On the use of non-stationary strategies for solving two-player zero-sum Markov games", "author": ["J. P\u00e9rolat", "B. Piot", "B. Scherrer", "O. Pietquin"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "A framework for sequential planning in multi-agent settings", "author": ["Piotr J Gmytrasiewicz", "Prashant Doshi"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Exploiting coordination locales in distributed POMDPs via social model shaping", "author": ["Pradeep Varakantham", "Jun-young Kwak", "Matthew E Taylor", "Janusz Marecki", "Paul Scerri", "Milind Tambe"], "venue": "In Proceedings of the 19th International Conference on Automated Planning and Scheduling,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Solving transition independent decentralized Markov decision processes", "author": ["Raphen Becker", "Shlomo Zilberstein", "Victor Lesser", "Claudia V Goldman"], "venue": "Journal of  Artificial Intelligence Research,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}, {"title": "The world of independent learners is not Markovian", "author": ["Guillaume J. Laurent", "La\u00ebtitia Matignon", "N. Le Fort-Piat"], "venue": "Int. J. Know.-Based Intell. Eng. Syst.,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis"], "venue": "search. Nature,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "A neural substrate of prediction and reward", "author": ["W. Schultz", "P. Dayan", "P.R. Montague"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1997}, {"title": "Reinforcement learning in the brain", "author": ["Y. Niv"], "venue": "The Journal of Mathematical Psychology,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2009}, {"title": "Introduction to Reinforcement Learning", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1998}, {"title": "Reinforcement learning improves behaviour from evaluative", "author": ["Michael L Littman"], "venue": "feedback. Nature,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Batch reinforcement learning", "author": ["Sascha Lange", "Thomas Gabel", "Martin Riedmiller"], "venue": "In Reinforcement learning,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Time, uncertainty, and individual differences in decisions to cooperate in resource dilemmas", "author": ["Katherine V Kortenkamp", "Colleen F Moore"], "venue": "Personality and Social Psychology Bulletin,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}, {"title": "High and low trusters\u2019 responses to fear in a payoff matrix", "author": ["Craig D Parks", "Lorne G Hulbert"], "venue": "Journal of Conflict Resolution,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1995}, {"title": "When happiness makes us selfish, but sadness makes us fair: Affective influences on interpersonal strategies in the dictator game", "author": ["Hui Bing Tan", "Joseph P Forgas"], "venue": "Journal of Experimental Social Psychology,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "How other-regarding preferences can promote cooperation in non-zero-sum grid games", "author": ["Joseph L. Austerweil", "Stephen Brawner", "Amy Greenwald", "Elizabeth Hilliard", "Mark Ho", "Michael L. Littman", "James MacGlashan", "Carl Trimbach"], "venue": "In Proceedings of the AAAI Symposium on Challenges and Opportunities in Multiagent Learning for the Real World,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control", "author": ["Nathaniel D Daw", "Yael Niv", "Peter Dayan"], "venue": "Nature neuroscience,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2005}, {"title": "Micromotives and macrobehavior", "author": ["Thomas C. Schelling"], "venue": "WW Norton & Company,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1978}], "referenceMentions": [{"referenceID": 0, "context": "Social dilemmas expose tensions between collective and individual rationality [1].", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "However, the lure of free riding and other such parasitic strategies implies a tragedy of the commons that threatens the stability of any cooperative venture [2].", "startOffset": 158, "endOffset": 161}, {"referenceID": 2, "context": "A matrix game is a social dilemma when its four payoffs satisfy the following social dilemma inequalities (this formulation from [3]):", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 21, "endOffset": 33}, {"referenceID": 4, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 21, "endOffset": 33}, {"referenceID": 5, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 21, "endOffset": 33}, {"referenceID": 6, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 21, "endOffset": 33}, {"referenceID": 7, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 56, "endOffset": 59}, {"referenceID": 8, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 78, "endOffset": 85}, {"referenceID": 9, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 78, "endOffset": 85}, {"referenceID": 2, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 126, "endOffset": 129}, {"referenceID": 10, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 165, "endOffset": 185}, {"referenceID": 11, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 165, "endOffset": 185}, {"referenceID": 12, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 165, "endOffset": 185}, {"referenceID": 13, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 165, "endOffset": 185}, {"referenceID": 14, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 165, "endOffset": 185}, {"referenceID": 15, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 205, "endOffset": 209}, {"referenceID": 16, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 220, "endOffset": 224}, {"referenceID": 17, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 253, "endOffset": 261}, {"referenceID": 18, "context": ", direct reciprocity [4, 5, 6, 7], indirect reciprocity [8], norm enforcement [9, 10], simple reinforcement learning variants [3], multiagent reinforcement learning [11, 12, 13, 14, 15], spatial structure [16], emotions [17], and social network effects [18, 19].", "startOffset": 253, "endOffset": 261}, {"referenceID": 19, "context": "To demonstrate the importance of capturing sequential structure in social dilemma modeling, we present empirical game-theoretic analyses [20, 21] of SSDs to identify the empirical payoff matrices summarizing the outcomes that would arise if cooperate and defect policies were selected as one-shot decisions.", "startOffset": 137, "endOffset": 145}, {"referenceID": 20, "context": "To demonstrate the importance of capturing sequential structure in social dilemma modeling, we present empirical game-theoretic analyses [20, 21] of SSDs to identify the empirical payoff matrices summarizing the outcomes that would arise if cooperate and defect policies were selected as one-shot decisions.", "startOffset": 137, "endOffset": 145}, {"referenceID": 21, "context": "g [22, 23, 24]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 22, "context": "g [22, 23, 24]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 23, "context": "g [22, 23, 24]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 24, "context": "g [25]) may be applied to this problem of finding equilibria of SSDs.", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "For temporal discount factor \u03b3 \u2208 [0, 1] we can define the long-term payoff V ~\u03c0 i (s0) to player i when the joint policy", "startOffset": 33, "endOffset": 39}, {"referenceID": 19, "context": "This definition is based on a formalization of empirical game-theoretic analysis [20, 21].", "startOffset": 81, "endOffset": 89}, {"referenceID": 20, "context": "This definition is based on a formalization of empirical game-theoretic analysis [20, 21].", "startOffset": 81, "endOffset": 89}, {"referenceID": 20, "context": "as an empirical payoff matrix following the terminology of [21].", "startOffset": 59, "endOffset": 63}, {"referenceID": 25, "context": "Most previous work on finding policies for Markov games takes the prescriptive view of multiagent learning [26]: that is, it attempts to answer \u201cwhat should each agent do?\u201d Several algorithms and analyses have been developed for the two-player zero-sum case [22, 27, 28, 29, 30].", "startOffset": 107, "endOffset": 111}, {"referenceID": 21, "context": "Most previous work on finding policies for Markov games takes the prescriptive view of multiagent learning [26]: that is, it attempts to answer \u201cwhat should each agent do?\u201d Several algorithms and analyses have been developed for the two-player zero-sum case [22, 27, 28, 29, 30].", "startOffset": 258, "endOffset": 278}, {"referenceID": 26, "context": "Most previous work on finding policies for Markov games takes the prescriptive view of multiagent learning [26]: that is, it attempts to answer \u201cwhat should each agent do?\u201d Several algorithms and analyses have been developed for the two-player zero-sum case [22, 27, 28, 29, 30].", "startOffset": 258, "endOffset": 278}, {"referenceID": 27, "context": "Most previous work on finding policies for Markov games takes the prescriptive view of multiagent learning [26]: that is, it attempts to answer \u201cwhat should each agent do?\u201d Several algorithms and analyses have been developed for the two-player zero-sum case [22, 27, 28, 29, 30].", "startOffset": 258, "endOffset": 278}, {"referenceID": 28, "context": "Most previous work on finding policies for Markov games takes the prescriptive view of multiagent learning [26]: that is, it attempts to answer \u201cwhat should each agent do?\u201d Several algorithms and analyses have been developed for the two-player zero-sum case [22, 27, 28, 29, 30].", "startOffset": 258, "endOffset": 278}, {"referenceID": 29, "context": "Most previous work on finding policies for Markov games takes the prescriptive view of multiagent learning [26]: that is, it attempts to answer \u201cwhat should each agent do?\u201d Several algorithms and analyses have been developed for the two-player zero-sum case [22, 27, 28, 29, 30].", "startOffset": 258, "endOffset": 278}, {"referenceID": 30, "context": "The generalsum case is significantly more challenging [31], and algorithms either have strong assumptions or need to either track several different potential equilibria per agent [32, 33], model other players to simplify the problem [34], or must find a Figure 3: Left: Gathering.", "startOffset": 54, "endOffset": 58}, {"referenceID": 31, "context": "The generalsum case is significantly more challenging [31], and algorithms either have strong assumptions or need to either track several different potential equilibria per agent [32, 33], model other players to simplify the problem [34], or must find a Figure 3: Left: Gathering.", "startOffset": 179, "endOffset": 187}, {"referenceID": 32, "context": "The generalsum case is significantly more challenging [31], and algorithms either have strong assumptions or need to either track several different potential equilibria per agent [32, 33], model other players to simplify the problem [34], or must find a Figure 3: Left: Gathering.", "startOffset": 179, "endOffset": 187}, {"referenceID": 33, "context": "The generalsum case is significantly more challenging [31], and algorithms either have strong assumptions or need to either track several different potential equilibria per agent [32, 33], model other players to simplify the problem [34], or must find a Figure 3: Left: Gathering.", "startOffset": 233, "endOffset": 237}, {"referenceID": 34, "context": "cyclic strategy composed of several policies obtained through multiple state space sweeps [35].", "startOffset": 90, "endOffset": 94}, {"referenceID": 35, "context": "Researchers have also studied the emergence of multi-agent coordination in the decentralized, partially observable MDP framework [36, 37, 38].", "startOffset": 129, "endOffset": 141}, {"referenceID": 36, "context": "Researchers have also studied the emergence of multi-agent coordination in the decentralized, partially observable MDP framework [36, 37, 38].", "startOffset": 129, "endOffset": 141}, {"referenceID": 37, "context": "Researchers have also studied the emergence of multi-agent coordination in the decentralized, partially observable MDP framework [36, 37, 38].", "startOffset": 129, "endOffset": 141}, {"referenceID": 12, "context": ", [13, 15], rather than on designing new learning algorithms.", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": ", [13, 15], rather than on designing new learning algorithms.", "startOffset": 2, "endOffset": 10}, {"referenceID": 38, "context": "It is well-known that the resulting \u201clocal decision process\u201d could be non-Markovian from each agent\u2019s perspective [39].", "startOffset": 114, "endOffset": 118}, {"referenceID": 24, "context": "We use deep reinforcement learning as the basis for each agent in part because of its recent success with solving complex problems [25, 40].", "startOffset": 131, "endOffset": 139}, {"referenceID": 39, "context": "We use deep reinforcement learning as the basis for each agent in part because of its recent success with solving complex problems [25, 40].", "startOffset": 131, "endOffset": 139}, {"referenceID": 40, "context": "Also, temporal difference predictions have been observed in the brain [41] and this class of reinforcement learning algorithm is seen as a candidate theory of animal habit-learning [42].", "startOffset": 70, "endOffset": 74}, {"referenceID": 41, "context": "Also, temporal difference predictions have been observed in the brain [41] and this class of reinforcement learning algorithm is seen as a candidate theory of animal habit-learning [42].", "startOffset": 181, "endOffset": 185}, {"referenceID": 42, "context": "Modern deep reinforcement learning methods take the perspective of an agent that must learn to maximize its cumulative long-term reward through trial-and-error interactions with its environment [43, 44].", "startOffset": 194, "endOffset": 202}, {"referenceID": 43, "context": "Modern deep reinforcement learning methods take the perspective of an agent that must learn to maximize its cumulative long-term reward through trial-and-error interactions with its environment [43, 44].", "startOffset": 194, "endOffset": 202}, {"referenceID": 24, "context": "See [25] for details in the single agent case.", "startOffset": 4, "endOffset": 8}, {"referenceID": 44, "context": "This is a\u201cgrowing batch\u201dapproach to reinforcement learning in the sense of [45].", "startOffset": 75, "endOffset": 79}, {"referenceID": 24, "context": "The network representing the function Q is trained through gradient descent on the mean squared Bellman residual with the expectation taken over transitions uniformly sampled from the batch (see [25]).", "startOffset": 195, "endOffset": 199}, {"referenceID": 23, "context": "In principle, this restriction could be dropped through the use of planning-based reinforcement learning methods like those of [24].", "startOffset": 127, "endOffset": 131}, {"referenceID": 24, "context": "[25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "In particular, Social Psychology has advanced various hypotheses concerning psychological variables that may influence cooperation and give rise to the observed individual differences in human cooperative behavior in laboratory-based social dilemmas [2].", "startOffset": 250, "endOffset": 253}, {"referenceID": 45, "context": "These factors include consideration-of-futureconsequences [46], trust [47], affect (interestingly, it is negative emotions that turn out to promote cooperation [48]), and a personality variable called social value orientation characterized by other-regarding-preferences.", "startOffset": 58, "endOffset": 62}, {"referenceID": 46, "context": "These factors include consideration-of-futureconsequences [46], trust [47], affect (interestingly, it is negative emotions that turn out to promote cooperation [48]), and a personality variable called social value orientation characterized by other-regarding-preferences.", "startOffset": 70, "endOffset": 74}, {"referenceID": 47, "context": "These factors include consideration-of-futureconsequences [46], trust [47], affect (interestingly, it is negative emotions that turn out to promote cooperation [48]), and a personality variable called social value orientation characterized by other-regarding-preferences.", "startOffset": 160, "endOffset": 164}, {"referenceID": 48, "context": "The latter has been studied in a similar Markov game social dilemma setup to our SSD setting by [49].", "startOffset": 96, "endOffset": 100}, {"referenceID": 49, "context": "Recall also that DQN is in the class of reinforcement learning algorithms that is generally considered to be the leading candidate theory of animal habit-learning [50, 42].", "startOffset": 163, "endOffset": 171}, {"referenceID": 41, "context": "Recall also that DQN is in the class of reinforcement learning algorithms that is generally considered to be the leading candidate theory of animal habit-learning [50, 42].", "startOffset": 163, "endOffset": 171}, {"referenceID": 23, "context": ", [24].", "startOffset": 2, "endOffset": 6}, {"referenceID": 50, "context": "Notice that several of the examples in Schelling\u2019s seminal book Micromotives and Macrobehavior [51] can be seen as temporally extended social dilemmas for which policies have been learned over the course of repeated interaction, including the famous opening example of lecture hall seating behavior.", "startOffset": 95, "endOffset": 99}], "year": 2017, "abstractText": "Matrix games like Prisoner\u2019s Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Qnetwork, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.", "creator": "LaTeX with hyperref package"}}}