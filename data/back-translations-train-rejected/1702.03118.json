{"id": "1702.03118", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2017", "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning", "abstract": "In recent years, neural networks have enjoyed a renaissance as function approximators in reinforcement learning. Two decades after Teasauro's TD-Gammon achieved near top-level human performance in backgammon, the deep reinforcement learning algorithm DQN (combining Q-learning with a deep neural network, experience replay, and a separate target network) achieved human-level performance in many Atari 2600 games. The purpose of this study is twofold. First, based on the expected energy restricted Boltzmann machine (EE-RBM), we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear (SiL) unit and its derivative function (SiLd1). The activation of the SiL unit is computed by the sigmoid function multiplied by its input, which is equal to the contribution to the output from one hidden unit in an EE-RBM. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection can be competitive with DQN, without the need for a separate target network. We validate our proposed approach by, first, achieving new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10x10 board, using TD($\\lambda$) learning and shallow SiLd1 network agents, and, then, outperforming DQN in the Atari 2600 domain by using a deep Sarsa($\\lambda$) agent with SiL and SiLd1 hidden units.", "histories": [["v1", "Fri, 10 Feb 2017 10:04:30 GMT  (499kb)", "https://arxiv.org/abs/1702.03118v1", "18 pages, 21 figures"], ["v2", "Thu, 23 Feb 2017 07:40:05 GMT  (501kb)", "http://arxiv.org/abs/1702.03118v2", "17 pages, 21 figures; added reference in section 3.1"], ["v3", "Thu, 2 Nov 2017 02:48:38 GMT  (555kb)", "http://arxiv.org/abs/1702.03118v3", "18 pages, 22 figures; added deep RL results for SZ-Tetris"]], "COMMENTS": "18 pages, 21 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["stefan elfwing", "eiji uchibe", "kenji doya"], "accepted": false, "id": "1702.03118"}, "pdf": {"name": "1702.03118.pdf", "metadata": {"source": "CRF", "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning", "authors": ["Stefan Elfwing", "Eiji Uchibe", "Kenji Doya"], "emails": ["elfwing@atr.jp", "uchibe@atr.jp", "doya@oist.jp"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.03 118v 3 [cs.L G] 2"}, {"heading": "1 Introduction", "text": "We have enjoyed a renaissance in recent years as functioning approximators in the enhancement of learning (Sutton and Barto, 1998).The DQN algorithm (Mnih et al., 2015), which combines Q-Learning with a deep neural network, has reached human performance in many Atari 2600 games. Since the development of the DQN algorithm, there have been several proposed improvements, both in terms of DQN specific and in terms of deep enhancement of learning in general. Van Hasselt et al. (2015) proposed double DQN to reduce overestimation of action values in DQN and Schaul et al. (2016) he developed a framework for more efficient repetition through prioritizing experiences of important state transitions. Wang et al. (2016) he proposed the dueling network for more efficient learning of action value function by separately assessing the function of state value function and the benefits of individual measures."}, {"heading": "2 Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 TD(\u03bb) and Sarsa(\u03bb)", "text": "In this study we use two amplification learning algorithms: TD (\u03bb) (Sutton, 1988) and Sarsa (\u03bb) (Rummery and Niranjan, 1994; Sutton, 1996). TD (\u03bb) learns an estimate of the state value function, V \u03c0, and Sarsa (\u03bb) learns an estimate of the action value function, Q \u03c0, while the agent follows the policy \u03c0. If the approximate value functions, Vt \u2248 V \u03c0 and Qt \u2248 Q \u03c0, are calculated by the parameter Vt (st + 1) \u2212 Vt (st) (2) for the gradient drop learning update of the parameters, then the gradient update of the parameters is calculated by VT + 1 = \u03b8t + \u03b1\u03b4tet, (1) where the TD error, \u03b4t = rt + \u03b3t, Vt (st + 1) \u2212 Vt (st) for the state rate, Vt (st) for the determination time, VT (VT) for the determination time, VT) and VT (VT) for the determination time."}, {"heading": "2.2 Sigmoid-weighted Linear Units", "text": "In our earlier work (Elfwing et al., 2016) we proposed the EE-RBM as a function approximation in amplification learning. (U) In the case of state value-oriented learning, since a state vector s, an EE-RBM approximates the state value function V by the negative expected energy of an RBM (Smolensky, 1986; Freund und Haussler, 1992; Hinton, 2002) Network: V (s) \u2212 kzk\u03c3 (zk) + percussion ibisi, (6) zk = approximately minimum energy of an RBM (Smolensky, 1986; Sidney, 2002) Network: V (s) is the input to hidden unit k, kzk is the sigmoid function, bi is the bias weight for input device si, wik is the weight that connects the state si and the hidden unit k, and bk is the bias weight for hidden unit k."}, {"heading": "2.3 Action selection", "text": "In all experiments, we use a Softmax action selection with a Boltzmann distribution. (13) In the model-based TD algorithm, we select an action in state s that leads to the next state s, with a probability defined as\u03c0 (a | s) = exp (V (s, a) / \u03c4). (14) Here, f (s, a) returns the next state s' according to the state transition dynamics. (15) Here is the temperature that regulates the trade-off between exploration and exploitation. We used hyperbolic annealing of the temperature and the temperature was decreased after each episode i."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 SZ-Tetris", "text": "In fact, it is so that it is a way in which people are able to determine for themselves what they want and what they don't want. (...) In fact, it is so that people are able to decide. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they don't want to do it. (...) It is as if they do it. (...) It is so. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (). (...). (). (It is. (...). (). (). (It is. (...). (). (). (It is. (...). (). (It is. (...). (). (). (It is. (.). (). (It is. (.). (.). (It is. (.). (.). (It is. (.).). (.). (It is. (.). (.). (It is. (.). (.). (.). (. (.). (.). (It is.). (It is. (.). (. (.). (.). (.).). (It is. (. (. (It is. (.).). (. (It is.). (. (.).). (It is. (. (. (.). (It is.). (.). (It is. (. (.).). (It is.). (It is. (. (. (.).).). (. (It is. (. (.). (.).). (It is. (). (.). (It is. (.).). (. (.). (. (.). (It is.). (.). (.). (). ("}, {"heading": "3.2 10\u00d710 Tetris", "text": "The result of the dSiLU network agents in stochastic SZ tetris is impressive, but we cannot compare the result with the methods that have achieved the highest performance levels in standard tetris, because these methods were not applied to stochastic SZTetris. Furthermore, it is not possible to apply our method to Tetris with a standard board size of 20, due to the immeasurably long learning time. The current state of the art for a single run of an algorithm, achieved by the CBMPI algorithm (Gabillon et al., 2013; Scherrer et al.), is an average score of 51 million solved lines that are applied to Tetris, there are results for a smaller, Tetris board, and in this case, the learning time for our method is long, but not in MP3 hours."}, {"heading": "3.3 Atari 2600 games", "text": "In order to further evaluate the use of value-based reinforcement measures with eligibility tracks and Softmax action selection in high-dimensional states, we applied the results of three episodes in the Atari 2600 domain with the Arcade Learning Environment (Bellemare et al., 2013) based on the results for the deep networks in SZ-Tetris, we used SiLU-dSiLU networks with SiLU units in the revolutionary layers and dSiLU units in the fully networked layer in the fully connected layer. To limit the number of games and prevent a biased selection of games, we selected the 12 games played by DQN (Mnih et al) that started with the letters \"A\" and \"B\": Alien, Amidar, Assault, Asterix, Atlantis, Bank Heist, Battle Zone, Beam Bowling and \"We have used the letters\" A \"and\" B. \""}, {"heading": "4 Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Value estimation", "text": "First, we examine the ability of TD (\u03bb) and Sarsa (\u03bb) to estimate precisely estimated discounted returns. (Rt = T \u2212 t \u2211 k = 0\u03b3krt + k.Here T is the length of an episode. The reason for this is that van Hasselt et al. (2015) showed that the dual DQN algorithm improved the performance of DQN in Atari 2600 games by reducing the overestimation of the action values. It is known (Thrun and Schwartz, 1993; van Hasselt, 2010) that Q-learning-based algorithms, such as DQN, can overestimate the action values due to the maximum operator used in the calculation of the learning targets. TD (\u03bb) and Sarsa (\u03bb) do not use the maximum operator to calculate the learning targets and they should therefore not be affected by this problems.Figure 6 shows that for episodes average (or expected) the length of episodes best Sident network discounts TSZ estimates."}, {"heading": "4.2 Action selection", "text": "Secondly, we investigate the importance of Softmax action selection in the games where our proposed agents performed particularly well. Almost all deep reinforcements learn algo rithms used in the Atari 2600 domain (one exception is the asynchronous advantage-actor-critic method A3C, which used Softmax output units for the actor (Mnih et al., 2016). One disadvantage of the \u03b5-greedy action selection is that it selects all actions with equal probability during exploration, which can lead to poor learning results in tasks where the worst actions have very bad consequences. This is clearly the case in both Tetris games and in the Asterix and Asteroids games. In each state in Tetris, many and often most actions cause holes in exploration that are difficult to solve (especially in SZ-Tetris). In the Asterix game, random exploration actions can be carried out by Asteroids, if they kill Asteroids with 0oids."}, {"heading": "5 Conclusions", "text": "In this study, we proposed SiLU and dSiLU as activation functions for neural network. the best agent, the dSiLU network agent, achieved new state-of-the-art results for both stochastic SZ-Tetris and 10 \u00d7 10 Tetris. In the Atari 2600 domain, a deep Sarsa (\u03bb) agent normalized DQN and double DQN with SiLUs in the Constitutional Layers and dSiLUs in the fully interconnected Hidden Layer. An additional purpose of this study was to demonstrate that a more traditional approach of using on-policy learning with authorization tracks and Softmax selection (i.e. essentially a \"textbook version\" of an amplification learning agent, but with non-linear neurons) QN function architectures is also used in QN models (i.e. a combined approach for selective Dynparous, and Traparous learning)."}, {"heading": "Acknowledgments", "text": "This work was supported by the project commissioned by the New Energy and Industrial Technology Development Organization (NEDO), JSPS KAKENHI grant 16K12504 and Okinawa Institute of Science and Technology Graduate University Research support to KD."}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Temporal differences based policy iteration and applications in neuro-dynamic programming", "author": ["D.P. Bertsekas", "S. Ioffe"], "venue": "Technical Report LIDS-P-2349,", "citeRegEx": "Bertsekas and Ioffe,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas and Ioffe", "year": 1996}, {"title": "How to lose at Tetris", "author": ["H. Burgiel"], "venue": "Mathematical Gazette,", "citeRegEx": "Burgiel,? \\Q1997\\E", "shortCiteRegEx": "Burgiel", "year": 1997}, {"title": "Expected energy-based restricted boltzmann machine for classification", "author": ["S. Elfwing", "E. Uchibe", "K. Doya"], "venue": "Neural Networks,", "citeRegEx": "Elfwing et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Elfwing et al\\.", "year": 2015}, {"title": "From free energy to expected energy", "author": ["S. Elfwing", "E. Uchibe", "K. Doya"], "venue": null, "citeRegEx": "Elfwing et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elfwing et al\\.", "year": 2016}, {"title": "Neural network ensembles in reinforcement learning", "author": ["Online", "F. Schwenker"], "venue": null, "citeRegEx": "Online et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Online et al\\.", "year": 2013}, {"title": "Unsupervised learning of distributions on binary vectors", "author": ["Y. Freund", "D. Haussler"], "venue": "Neural Processing Letters,", "citeRegEx": "Freund and Haussler,? \\Q1992\\E", "shortCiteRegEx": "Freund and Haussler", "year": 1992}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["D. Silver"], "venue": null, "citeRegEx": "Silver,? \\Q2015\\E", "shortCiteRegEx": "Silver", "year": 2015}, {"title": "On-line Q-learning using connectionist systems", "author": ["G.A. Rummery", "M. Niranjan"], "venue": "Technical Report CUED/F-INFENG/TR 166,", "citeRegEx": "Rummery and Niranjan,? \\Q1994\\E", "shortCiteRegEx": "Rummery and Niranjan", "year": 1994}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Schaul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "Approximate modified policy iteration and its application to the game of tetris", "author": ["B. Scherrer", "M. Ghavamzadeh", "V. Gabillon", "B. Lesner", "M. Geist"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Scherrer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Scherrer et al\\.", "year": 2015}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume", "citeRegEx": "Smolensky,? \\Q1986\\E", "shortCiteRegEx": "Smolensky", "year": 1986}, {"title": "Learning to predict by the method of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton,? \\Q1988\\E", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["R.S. Sutton"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Sutton,? \\Q1996\\E", "shortCiteRegEx": "Sutton", "year": 1996}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "SZ-Tetris as a benchmark for studying key problems of reinforcement learning", "author": ["I. Szita", "C. Szepesv\u00e1ri"], "venue": "In ICML 2010 workshop on machine learning and games", "citeRegEx": "Szita and Szepesv\u00e1ri,? \\Q2010\\E", "shortCiteRegEx": "Szita and Szepesv\u00e1ri", "year": 2010}, {"title": "Td-gammon, a self-teaching backgammon program, achieves masterlevel play", "author": ["G. Tesauro"], "venue": "Neural Computation,", "citeRegEx": "Tesauro,? \\Q1994\\E", "shortCiteRegEx": "Tesauro", "year": 1994}, {"title": "Improvements on learning tetris with cross entropy", "author": ["C. Thiery", "B. Scherrer"], "venue": "International Computer Games Association Journal,", "citeRegEx": "Thiery and Scherrer,? \\Q2009\\E", "shortCiteRegEx": "Thiery and Scherrer", "year": 2009}, {"title": "Issues in using function approximation for reinforcement learning", "author": ["S. Thrun", "A. Schwartz"], "venue": "In Proceedings of the 1993 Connectionist Models Summer School,", "citeRegEx": "Thrun and Schwartz,? \\Q1993\\E", "shortCiteRegEx": "Thrun and Schwartz", "year": 1993}, {"title": "Double q-learning", "author": ["H. van Hasselt"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Hasselt,? \\Q2010\\E", "shortCiteRegEx": "Hasselt", "year": 2010}, {"title": "Deep reinforcement learning with double q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": null, "citeRegEx": "Hasselt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "T. Schaul", "M. Hessel", "H. van Hasselt", "M. Lanctot", "N. de Freitas"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML2016),", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "Neural networks have enjoyed a renaissance as function approximators in reinforcement learning (Sutton and Barto, 1998) in recent years.", "startOffset": 95, "endOffset": 119}, {"referenceID": 11, "context": "Neural networks have enjoyed a renaissance as function approximators in reinforcement learning (Sutton and Barto, 1998) in recent years. The DQN algorithm (Mnih et al., 2015), which combines Q-learning with a deep neural network, experience replay, and a separate target network, achieved human-level performance in many Atari 2600 games. Since the development of the DQN algorithm, there have been several proposed improvements, both to DQN specifically and deep reinforcement learning in general. Van Hasselt et al. (2015) proposed double DQN to reduce overestimation of the action values in DQN and Schaul et al.", "startOffset": 96, "endOffset": 525}, {"referenceID": 9, "context": "(2015) proposed double DQN to reduce overestimation of the action values in DQN and Schaul et al. (2016) developed a framework for more efficient replay by prioritizing experiences of more important state transitions.", "startOffset": 84, "endOffset": 105}, {"referenceID": 9, "context": "(2015) proposed double DQN to reduce overestimation of the action values in DQN and Schaul et al. (2016) developed a framework for more efficient replay by prioritizing experiences of more important state transitions. Wang et al. (2016) proposed the dueling network architecture", "startOffset": 84, "endOffset": 237}, {"referenceID": 12, "context": "Using a neural network function approximator and TD(\u03bb) learning (Sutton, 1988), TD-Gammon reached near top-level human performance in backgammon, which to this day remains one of the most impressive applications of reinforcement learning.", "startOffset": 64, "endOffset": 78}, {"referenceID": 3, "context": "First, motivated by the high performance of the expected energy restricted Boltzmann machine (EE-RBM) in our earlier studies (Elfwing et al., 2015, 2016), we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU). The activation of the SiLU is computed by the sigmoid function multiplied by its input and it looks like a continuous and \u201cundershooting\u201d version of the linear rectifier unit (ReLU) (Hahnloser et al., 2000). The activation of the dSiLU looks like steeper and \u201covershooting\u201d version of the sigmoid function. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection with simple annealing can be competitive with DQN, without the need for a separate target network. Our approach is something of a throwback to the approach used by Tesauro (1994) to develop TD-Gammon more than two decades ago.", "startOffset": 126, "endOffset": 989}, {"referenceID": 12, "context": "In this study, we use two reinforcement learning algorithms: TD(\u03bb) (Sutton, 1988) and Sarsa(\u03bb) (Rummery and Niranjan, 1994; Sutton, 1996).", "startOffset": 67, "endOffset": 81}, {"referenceID": 8, "context": "In this study, we use two reinforcement learning algorithms: TD(\u03bb) (Sutton, 1988) and Sarsa(\u03bb) (Rummery and Niranjan, 1994; Sutton, 1996).", "startOffset": 95, "endOffset": 137}, {"referenceID": 13, "context": "In this study, we use two reinforcement learning algorithms: TD(\u03bb) (Sutton, 1988) and Sarsa(\u03bb) (Rummery and Niranjan, 1994; Sutton, 1996).", "startOffset": 95, "endOffset": 137}, {"referenceID": 4, "context": "In our earlier work (Elfwing et al., 2016), we proposed the EE-RBM as a function approximator in reinforcement learning.", "startOffset": 20, "endOffset": 42}, {"referenceID": 11, "context": "In the case of state-value based learning, given a state vector s, an EE-RBM approximates the state-value function V by the negative expected energy of an RBM (Smolensky, 1986; Freund and Haussler, 1992; Hinton, 2002) network:", "startOffset": 159, "endOffset": 217}, {"referenceID": 6, "context": "In the case of state-value based learning, given a state vector s, an EE-RBM approximates the state-value function V by the negative expected energy of an RBM (Smolensky, 1986; Freund and Haussler, 1992; Hinton, 2002) network:", "startOffset": 159, "endOffset": 217}, {"referenceID": 3, "context": "In this study, motivated by the high performance of the EE-RBM in both the classification (Elfwing et al., 2015) and the reinforcement learning (Elfwing et al.", "startOffset": 90, "endOffset": 112}, {"referenceID": 4, "context": ", 2015) and the reinforcement learning (Elfwing et al., 2016) domains, we propose the SiLU as an activation function for neural network function approximation", "startOffset": 39, "endOffset": 61}, {"referenceID": 3, "context": "An attractive feature of the SiLU is that it has a self-stabilizing property, which we demonstrated experimentally in Elfwing et al. (2015). The global minimum, where the derivative is zero, functions as a \u201csoft floor\u201d on the weights that serves as an implicit regularizer that inhibits the learning of weights of large magnitudes.", "startOffset": 118, "endOffset": 140}, {"referenceID": 2, "context": "Szita and Szepesv\u00e1ri (2010) proposed stochastic SZ-Tetris (Burgiel, 1997) as a benchmark for reinforcement learning that preserves the core challenges of standard Tetris but allows faster evaluation of different strategies due to shorter episodes by removing easier tetrominoes.", "startOffset": 58, "endOffset": 73}, {"referenceID": 2, "context": "For an alternating sequence of S-shaped and Z-shaped tetrominoes, the upper bound on the episode length in SZ-Tetris is 69 600 fallen pieces (Burgiel, 1997) (corresponding to a score of 27 840 points), but the maximum episode length is probably much shorter, maybe", "startOffset": 141, "endOffset": 156}, {"referenceID": 15, "context": "a few thousands (Szita and Szepesv\u00e1ri, 2010).", "startOffset": 16, "endOffset": 44}, {"referenceID": 15, "context": "In stochastic SZ-Tetris, the reported scores for a wide variety of reinforcement learning algorithms are either approximately zero (Szita and Szepesv\u00e1ri, 2010) or in the single digits .", "startOffset": 131, "endOffset": 159}, {"referenceID": 4, "context": "We achieved an average score of about 200 points using three different neural network function approximators: an EE-RBM, a free energy RBM, and a standard neural network with sigmoid hidden units (Elfwing et al., 2016).", "startOffset": 196, "endOffset": 218}, {"referenceID": 4, "context": "We used the same experimental setup as used in our earlier work (Elfwing et al., 2016).", "startOffset": 64, "endOffset": 86}, {"referenceID": 12, "context": "a few thousands (Szita and Szepesv\u00e1ri, 2010). That means that to evaluate a good strategy SZ-Tetris requires at least five orders of magnitude less computation than standard Tetris. The standard learning approach for Tetris has been to use a model-based setting and define the evaluation function or state-value function as the linear combination of hand-coded features. Value-based reinforcement learning algorithms have a lousy track record using this approach. In regular Tetris, their reported performance levels are many magnitudes lower than black-box methods such as the cross-entropy (CE) method and evolutionary approaches. In stochastic SZ-Tetris, the reported scores for a wide variety of reinforcement learning algorithms are either approximately zero (Szita and Szepesv\u00e1ri, 2010) or in the single digits . Value-based reinforcement learning has had better success in stochastic SZ-Tetris when using non-linear neural network based function approximators. Fau\u00dfer and Schwenker (2013) achieved a score of about 130 points using a shallow neural network function approximator with sigmoid hidden units.", "startOffset": 17, "endOffset": 996}, {"referenceID": 2, "context": "We achieved an average score of about 200 points using three different neural network function approximators: an EE-RBM, a free energy RBM, and a standard neural network with sigmoid hidden units (Elfwing et al., 2016). Jaskowski et al. (2015) achieved the current state-of-the-art results using systematic n-tuple networks as function approximators: average scores of 220 and 218 points achieved by the evolutionary VD-CMA-ES method and TD-learning, respectively, and the best mean score in a single run of 295 points achieved by TD-learning.", "startOffset": 197, "endOffset": 244}, {"referenceID": 1, "context": "The features were similar to the original 21 features proposed by Bertsekas and Ioffe (1996), except for not including the maximum column height and using the differences in column heights instead of the absolute differences.", "startOffset": 66, "endOffset": 93}, {"referenceID": 10, "context": "The current state-of-the-art for a single run of an algorithm, achieved by the CBMPI algorithm (Gabillon et al., 2013; Scherrer et al., 2015), is a mean score of 51 million cleared lines.", "startOffset": 95, "endOffset": 141}, {"referenceID": 17, "context": "The CE method has achieved its best score by combining the Bertsekas features, the Dellacherie features (Fahey, 2003), and three original features (Thiery and Scherrer, 2009).", "startOffset": 147, "endOffset": 174}, {"referenceID": 0, "context": "To further evaluate the use of value-based on-policy reinforcement learning with eligibility traces and softmax action selection in high-dimensional state space domains, as well as the use of SiLU and dSiLU units, we applied Sarsa(\u03bb) with a deep convolution neural network function approximator in the Atari 2600 domain using the Arcade Learning Environment (Bellemare et al., 2013).", "startOffset": 358, "endOffset": 382}, {"referenceID": 0, "context": "To further evaluate the use of value-based on-policy reinforcement learning with eligibility traces and softmax action selection in high-dimensional state space domains, as well as the use of SiLU and dSiLU units, we applied Sarsa(\u03bb) with a deep convolution neural network function approximator in the Atari 2600 domain using the Arcade Learning Environment (Bellemare et al., 2013). Based on the results for the deep networks in SZ-Tetris, we used SiLU-dSiLU networks with SiLU units in the convolutional layers and dSiLU units in the fully-connected layer. To limit the number of games and prevent a biased selection of the games, we selected the 12 games played by DQN (Mnih et al., 2015) that started with the letters \u2019A\u2019 and \u2019B\u2019: Alien, Amidar, Assault, Asterix, Asteroids, Atlantis, Bank Heist, Battle Zone, Beam Rider, Bowling, Boxing, and Breakout. We used a similar experimental setup as Mnih et al. (2015). We pre-processed the raw 210\u00d7160 Atari 2600 RGB frames by extracting the luminance channel, taking the maximum pixel values over consecutive frames to prevent flickering, and then downsampling the grayscale images to 105\u00d780.", "startOffset": 359, "endOffset": 916}, {"referenceID": 0, "context": "To further evaluate the use of value-based on-policy reinforcement learning with eligibility traces and softmax action selection in high-dimensional state space domains, as well as the use of SiLU and dSiLU units, we applied Sarsa(\u03bb) with a deep convolution neural network function approximator in the Atari 2600 domain using the Arcade Learning Environment (Bellemare et al., 2013). Based on the results for the deep networks in SZ-Tetris, we used SiLU-dSiLU networks with SiLU units in the convolutional layers and dSiLU units in the fully-connected layer. To limit the number of games and prevent a biased selection of the games, we selected the 12 games played by DQN (Mnih et al., 2015) that started with the letters \u2019A\u2019 and \u2019B\u2019: Alien, Amidar, Assault, Asterix, Asteroids, Atlantis, Bank Heist, Battle Zone, Beam Rider, Bowling, Boxing, and Breakout. We used a similar experimental setup as Mnih et al. (2015). We pre-processed the raw 210\u00d7160 Atari 2600 RGB frames by extracting the luminance channel, taking the maximum pixel values over consecutive frames to prevent flickering, and then downsampling the grayscale images to 105\u00d780. For computational reasons, we used a smaller network architecture. Instead of three convolutional layers, we used two with half the number of filters, each followed by a max-pooling layer. The input to the network was a 105\u00d780\u00d72 image consisting of the current and the fourth previous pre-processed frame. As we used frame skipping where actions were selected every fourth frame and repeated for the next four frames, we only needed to apply pre-processing to every fourth frame. The first convolutional layer had 16 filters of size 8\u00d78 with a stride of 4. The second convolutional layer had 32 filters of size 4\u00d74 with a stride of 2. The max-pooling layers had pooling windows of size 3\u00d73 with a stride of 2. The convolutional layers were followed by a fully-connected hidden layer with 512 dSiLU units and a fully-connected linear output layer with 4 to 18 output (or action-value) units, depending on the number of valid actions in the considered game. We selected meta-parameters by a preliminary search in the Alien, Amidar and Assault games and used the same values for all 12 games: \u03b1: 0.001, \u03b3: 0.99, \u03bb: 0.8, \u03c40: 0.5, and \u03c4k: 0.0005. As in Mnih et al. (2015), we clipped the rewards to be between \u22121 and +1, but we did not clip the values of the TD-errors.", "startOffset": 359, "endOffset": 2309}, {"referenceID": 18, "context": "It is known (Thrun and Schwartz, 1993; van Hasselt, 2010) that Q-learning based algorithms, such as DQN, can overestimate action values due to the max operator, which is used in the computation of the learning targets.", "startOffset": 12, "endOffset": 57}, {"referenceID": 18, "context": "The reason for doing this is that van Hasselt et al. (2015) showed that the double DQN algorithm improved the performance of DQN in Atari 2600 games by reducing the overestimation of the action values.", "startOffset": 38, "endOffset": 60}, {"referenceID": 21, "context": ", using, as DQN, a separate target network, but also by using more recent advances such as the dueling architecture (Wang et al., 2016) for more accurate estimates of the action values and asynchronous learning by multiple agents in parallel (Mnih et al.", "startOffset": 116, "endOffset": 135}], "year": 2017, "abstractText": "In recent years, neural networks have enjoyed a renaissance as function approximators in reinforcement learning. Two decades after Tesauro\u2019s TD-Gammon achieved near toplevel human performance in backgammon, the deep reinforcement learning algorithm DQN achieved human-level performance in many Atari 2600 games. The purpose of this study is twofold. First, we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU). The activation of the SiLU is computed by the sigmoid function multiplied by its input. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection with simple annealing can be competitive with DQN, without the need for a separate target network. We validate our proposed approach by, first, achieving new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10\u00d710 board, using TD(\u03bb) learning and shallow dSiLU network agents, and, then, by outperforming DQN in the Atari 2600 domain by using a deep Sarsa(\u03bb) agent with SiLU and dSiLU hidden units.", "creator": "LaTeX with hyperref package"}}}