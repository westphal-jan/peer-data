{"id": "1702.03192", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2017", "title": "Supervised Learning Based Algorithm Selection for Deep Neural Networks", "abstract": "Fully connected network has been widely used in deep learning, and its computation efficiency is highly benefited from the matrix multiplication algorithm with cuBLAS on GPU. However, We found that, there exist some drawbacks of cuBLAS in calculating matrix $\\textbf{A}$ multiplies the transpose of matrix $\\textbf{B}$ (i.e., NT operation). To reduce the impact of NT operation by cuBLAS, we exploit the out-of-place transpose of matrix $\\textbf{B}$ to avoid using NT operation, and then we apply our method to Caffe, which is a popular deep learning tool. Our contribution is two-fold. First, we propose a naive method (TNN) and model-based method (MTNN) to increase the performance in calculating $\\textbf{A}\\times \\textbf{B}^T$, and it achieves about 4.7 times performance enhancement in our tested cases on GTX1080 card. Second, we integrate MTNN method into Caffe to enhance the efficiency in training fully connected networks, which achieves about 70% speedup compared to the original Caffe in our configured fully connected networks on GTX1080 card.", "histories": [["v1", "Fri, 10 Feb 2017 14:51:42 GMT  (535kb,D)", "http://arxiv.org/abs/1702.03192v1", "5 pages"], ["v2", "Fri, 17 Mar 2017 02:06:06 GMT  (1750kb,D)", "http://arxiv.org/abs/1702.03192v2", "In review for a conference paper of ICPP2017"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.DC cs.LG", "authors": ["shaohuai shi", "pengfei xu", "xiaowen chu"], "accepted": false, "id": "1702.03192"}, "pdf": {"name": "1702.03192.pdf", "metadata": {"source": "CRF", "title": "Improving the Performance of Fully Connected Neural Networks by Out-of-Place Matrix Transpose", "authors": ["Shaohuai Shi", "Pengfei Xu", "Xiaowen Chu"], "emails": ["chxw}@comp.hkbu.edu.hk"], "sections": [{"heading": null, "text": "Index Terms - Deep Neural Networks; Linear Algebra; Matrix Multiplication; Transpose; GPU;"}, {"heading": "1. Introduction", "text": "The general form of matrix multiplication for two matrices A and B, which are both rows of large matrices, can be represented as follows: C = A \u00d7 B (1), where a different form of matrix multiplication and another form of matrix multiplication is available in row i and column j of a matrix matrix M, we can represent as follows: C = K \u00b2, B \u00b2, B \u00b2, B \u00b2, B \u00b2, B \u00b2, B \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M \u00b2, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M"}, {"heading": "2. Motivation", "text": "For deep neural networks, in particular the fully connected networks [5], matrix matrix multiplication is one of the main operations for training networks. After learning about fully connected networks in [6], we measure the performance of GEMM in NNN and NT operations with a series of matrix configurations on the latest generation of the NVIDIA GPU card (i.e., GTX 1080 with Pascal architecture) using CUDA 8.0. Table 1 shows our hardware configuration for the experiments. The benchmark result is shown in Fig. 1. It can be noted that the performance of NN operation is in most cases much better than the NT operation. For example, in the case of C = A \u00d7 BT, where A-R4096 \u00d7 53504 and B-R2048 \u00d7 53504 the performance of NN operation is up to four times better than the performance of NN operation, which on both GT80 cards is the simpler to show the differences between the two NN profiles, hence there are some major optimizations of the NN."}, {"heading": "3. Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Naive Method", "text": "To calculate Eq.3, we replace the single-stage NT operation with the two-stage TNN operation (i.e., transpose B and do NN operation).This method requires additional GPU memory to store BT to perform misplaced matrix transpose, and the size of the memory required is matrix B. The pseudo-code of the TNN method is shown in algorithm 1. Since TNN operation requires the additional transpose on GPU, the time used by transpose operation (transpose (n, k) should not be greater than the differential between NT (TNT (m, n, k) and NN (TNN, k).In other words, to guarantee TTNN (m, n, k), the time used by transpose operation (TNT (n, k) is less than TNT (k))."}, {"heading": "3.2. Model-based Method", "text": "Based on the analysis of the performance differences between NT and TNN, we are designing a model-based optimized method (MTNN) to calculate Equation 3. Figure 1 shows that the performance of NT is extremely close or even better than that of NN operation. The TNN method could not achieve better performance than NT, so we should not simply replace NT operation with TNN operation in the real world. We propose a model to decide which one we choose in order to achieve better performance between NT and TNN through the size of the matrices. Our method is simple and consists of two steps. First, we collect historical data by testing the results of NT and TNN. Second, the method of machine learning is used to train a decision model."}, {"heading": "3.2.1. Data Collection", "text": "The NT operation of cuBLAS is the API: \"cublasSgemm\" with the second and third parameters \"cublasSgeam\" and then \"cublasSgemm\" with the two second and third parameters \"CUBLAS OP N,\" while the TNN operation performs the misplaced transposition. \"We select a series of matrices with the quantities S = {2i | i = 7, 8,..., 16}. In other words, for all m, n and k (m \u00b2 S, n \u00b2 S, k \u00b2 S) that have 1000 combinations, we test the performance of NT and TNN in the calculation of the equation 3. Let us leave PNT (m, n, k) and PTNN (m, n, k) with two matrices A > B \u00d7 K (PNK) each, with the difference between the data sets D (PNK) and N (R91, PNK) and PTNN (N, n, K) with two matrices A and B (K) each."}, {"heading": "3.2.2. Decision Model", "text": "Given two matrix quantities (i.e. m, n, k) and two methods (i.e. NT and TNN) to calculate Equation 3, we should select those with higher power. In formal, given input variables: < m, n, k > there is a decision function: f, where (m, n, k) If f (m, n, k) = {\u2212 1, PNT (m, n, k) < PTNN (m, n, k) + 1, PNT (m, n, k) \u2265 PTNN (m, n, k) = \u2212 1, then we choose TNN, otherwise we choose NT. Our goal is to learn the decision function: f with the collected data generalized as a classification problem. The training samples are generalized by X = {< log2 (m), log2 (k) \u2212 > m, bsk, bsk}, and the problem, in this case, is called a classification problem."}, {"heading": "3.3. Integration with Caffe", "text": "To speed up deep learning training using the MTNN method, the decision model is integrated into Caffe, and fully connected networks, whose main operations are NT and NN, are tested on the built-in Caffe. If an operation of Equation 3 needs to be calculated, the predictor can choose a faster version to perform the operation, rather than calling NT directly from cuBLAS."}, {"heading": "4. Results", "text": "First we present the results of the matrix multiplication, which compares the performance between the original NT method using cuBLAS and the TNN method, and then the results of the model-based method are used for comparison, and then the performance of our method is shown in Caffe."}, {"heading": "4.1. Results of TNN Method", "text": "The results of the original NT method and the TNN method are shown in Fig. 2. It should be noted that in some cases, the NT method is still better than the TNN method, especially if the value of the K method is low (e.g. there are up to half of the cases where the NT method is better than the TNN method if K is 128). Among the cases tested, the maximum acceleration of the TNN method is up to 4.7 times better than the NT method. In cases with better results of the NT method, the performance of the NT is 1.67 times better than that of the TNN method at maximum. On average, the TNN method is 1.6 times better than the NT method."}, {"heading": "4.2. Results of MTNN", "text": "The collected data is divided into an average of ten parts, and the 10x cross validation method is used to verify the model. Thus, among the 10 parts, each part is used as a test set and the others as a training set. The average precision of the 10x cross validation method is 95.1%, which means that the decision model can perform the calculation of Equation 3 in 95.1% cases fast enough. After the 10x cross validation, we use all samples as a training set to generate the final decision model. The result compared to the original method is in Fig. 3. Compared to Fig. 2, most red rectangles have been replaced by blue hyphens. The statistical result is shown in Table 2 and Table 3. From Table. 2, it is noted that the ratio of cases where NT is better than the average NN decreases from 0.262 to 0.02 of TNN. In other words, our acceleration method can improve the performance in 98% of the cases where NT is better than the average NN from T002 of N00N to less than the N00N of 0.000N of T00N and N002 of N00N of N00N."}, {"heading": "4.3. FCN Results in Caffe", "text": "To test the performance of built-in Caffe, AlexNet [2] selects two fully connected layers with 4096 neurons per layer. In addition, a smaller FCN network is tested to make the comparison, and the size of the minibatches varies from 128 to 2048. Comparing the performance of the FCN model, which runs between three versions of Caffe, e.g. Original Caffe (CaffeNT), Caffe with TNN method (CaffeTNN) and Caffe with MTNN method (CaffeMTNN), is shown in Figure 4. By integrating our method into Caffe, the performance of optimized Caffe can be accelerated by up to 70% compared to the original Caffe on the GTX1080 card. Compared to CaffeTNN, CaffeMTNN has a slightly better performance, which is driven by the decision model."}, {"heading": "5. Conclusion and Future Work", "text": "Our method of multiplying matrix A and the transposition of matrix B is much better than that of cuBLAS API. The initial results achieve about 4.7 times the acceleration compared to the tousing of cuBLAS directly on the GTX1080 card. In addition, the method is applied to Caffe, and the optimized Caffe performs about 70% acceleration on the GTX1080 card. The transposition algorithm we use is an out-of-place method that leads to double the memory of a matrix and cannot run normally if there is not enough memory available. Therefore, we plan to exploit the transposition algorithm of the matrix locally and find a good trade-off between memory overhead and throughput."}], "references": [{"title": "Lenet-5, convolutional neural networks", "author": ["Y. LeCun"], "venue": "URL: http://yann. lecun. com/exdb/lenet, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "cublas \u2014 nvidia", "author": ["NVIDIA"], "venue": "https: / /developer.nvidia.com/cublas, 2016, accessed: 2016-09-6.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia, 2014, pp. 675\u2013678.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Theory of the backpropagation neural network", "author": ["R. Hecht-Nielsen"], "venue": "Neural Networks, 1989. IJCNN., International Joint Conference on. IEEE, 1989, pp. 593\u2013605.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1989}, {"title": "Benchmarking state-of-the-art deep learning software tools", "author": ["S. Shi", "Q. Wang", "P. Xu", "X. Chu"], "venue": "arXiv preprint arXiv:1608.07249, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Least squares support vector machine classifiers", "author": ["J.A. Suykens", "J. Vandewalle"], "venue": "Neural processing letters, vol. 9, no. 3, pp. 293\u2013300, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1\u201327:27, 2011, software available at http://www.csie. ntu.edu.tw/\u223ccjlin/libsvm.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "It is common to see fully connected network has been widely applied in deep learning [1][2].", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "It is common to see fully connected network has been widely applied in deep learning [1][2].", "startOffset": 88, "endOffset": 91}, {"referenceID": 2, "context": "70GHz, with single float matrix multiplication on NVIDIA K40M card [3].", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "Lastly, we exploit our solution to a real-world application Caffe [4] which is a popular deep learning framework and uses cuBLAS to accelerate its operations of matrix-matrix multiplication1.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "On deep neural networks, especially the fully connected networks [5], matrix-matrix multiplication is one of the main operations to do the training of networks.", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "According to findings on fully connected networks in [6], we benchmark the performance of GEMM in NN and NT operations with a set of matrix configurations on the newest generation of NVIDIA GPU card (i.", "startOffset": 53, "endOffset": 56}, {"referenceID": 6, "context": "The SVM algorithm [7] is applied to solve this decision model.", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "We use libsvm [8] with the kernel of radial basis function.", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "To test the performance of integrated Caffe, we choose two fully connected layers, which has 4096 neurons each layer, in AlexNet [2].", "startOffset": 129, "endOffset": 132}], "year": 2017, "abstractText": "Fully connected network has been widely used in deep learning, and its computation efficiency is highly benefited from the matrix multiplication algorithm with cuBLAS on GPU. However, We found that, there exist some drawbacks of cuBLAS in calculating matrix A multiplies the transpose of matrix B (i.e., NT operation). To reduce the impact of NT operation by cuBLAS, we exploit the out-of-place transpose of matrix B to avoid using NT operation, and then we apply our method to Caffe, which is a popular deep learning tool. Our contribution is two-fold. First, we propose a naive method (TNN) and model-based method (MTNN) to increase the performance in calculating A \u00d7 B , and it achieves about 4.7 times performance enhancement in our tested cases on GTX1080 card. Second, we integrate MTNN method into Caffe to enhance the efficiency in training fully connected networks, which achieves about 70% speedup compared to the original Caffe in our configured fully connected networks on GTX1080 card.", "creator": "LaTeX with hyperref package"}}}