{"id": "1702.03342", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2017", "title": "Learning Concept Embeddings for Efficient Bag-of-Concepts Densification", "abstract": "Explicit concept space models have proven efficacy for text representation in many natural language and text mining applications. The idea is to embed textual structures into a semantic space of concepts which captures the main topics of these structures. That so called bag-of-concepts representation suffers from data sparsity causing low similarity scores between similar texts due to low concept overlap. In this paper we propose two neural embedding models in order to learn continuous concept vectors. Once learned, we propose an efficient vector aggregation method to generate fully dense bag-of-concepts representations. Empirical results on a benchmark dataset for measuring entity semantic relatedness show superior performance over other concept embedding models. In addition, by utilizing our efficient aggregation method, we demonstrate the effectiveness of the densified vector representation over the typical sparse representations for dataless classification where we can achieve at least same or better accuracy with much less dimensions.", "histories": [["v1", "Fri, 10 Feb 2017 22:44:59 GMT  (344kb)", "http://arxiv.org/abs/1702.03342v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["walid shalaby", "wlodek zadrozny"], "accepted": false, "id": "1702.03342"}, "pdf": {"name": "1702.03342.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["wshalaby@uncc.edu", "wzadrozn@uncc.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.03 342v 1 [cs.C L] February 10, 2017Proven efficacy for text representation in many natural language and text mining applications. The idea is to embed text structures in a semantic terminology space that covers the main themes of these structures. So-called term representation suffers from a data sparseness that causes low similarity values between similar texts due to low conceptual overlaps. In this paper, we propose two neural embedding models to learn continuous concept vectors. Once learned, we propose an efficient vector aggregation method to produce fully dense conceptual representations. Empirical results on a benchmark dataset for measuring semantic relationships show superior performance over other concept embedding models. Furthermore, by using our efficient aggregation method, we demonstrate the effectiveness of the condensed vector representation over the typical sparse representations for at least the same localization where we can achieve accuracy."}, {"heading": "1 Introduction", "text": "As a rule, these models use textual corpora and / or knowledge bases (KBs) to acquire global knowledge, which is then used to generate a vector representation for the given text in semantic space. Therefore, the goal is to place semantically similar structures close together in this semantic space. On the other hand, different structures should be far apart from each other. Explicit concept space models are motivated by the idea that cognitive tasks such as learning and arguing at a high level are supported by the knowledge we gain from concepts (Song et al, 2015). Such models use concept vectors (a.k.a bag-of-concepts-concepts (BOC)) as the underlying semantic representation of a given text through a process called conceptualization."}, {"heading": "2 Related Work", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live."}, {"heading": "3 Concept Embeddings for BOC Densification", "text": "A main objective of embedding learning concepts is to overcome the inherent problem of data spareness in connection with the BOC representation. Here, we try to learn continuous concept vectors by building on the Skip-gram embedding model (Mikolov et al., 2013b). In the conventional Skipgram model, contexts are created by pushing a context window of predefined size over sentences of a given text corpus. The vector representation of a target word is learned with the aim of maximizing the ability to predict surrounding words of this target word. Formally, a context corpus of V words w1, w2,... wV is used. The Skip gram model aims to maximize the average log probability of a target word: 1VV V \u2211 i = 1 \u0445- s \u2264 j s, j = 0log p of V words w1, w2,... wV. The Skip gram model aims to maximize the prediction of surrounding words of this target word: W W W = W W W W W W = W W W W W W W = W W W W W W W W = W W W W W W W W W = W W W W W W = W W W W W W W W W W = W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W W"}, {"heading": "3.1 Concept Raw Context Model (CRC)", "text": "In this model, we jointly learn the embedding of both words and concepts. Firstly, all concept mentions are identified in the given corpus. Secondly, contexts for words and concepts are generated from other surrounding words as well as from other surrounding concepts. After generating all the contexts, we use the skip grammar model to jointly learn words and concept embedding. Formally, we consider a training corpus of V-words w1, w2,..., wV. We then practice a skip model that aims to identify words and concept mentions and thus to maximize a sequence of T-signs t1, t2,... tT, where T < V (as multiword terms are counted as one symbol). Next, we train a skip model that aims to maximize words and concept mentions of the concept: 1Tti, i = 1, j, s, j, s, j, j, s, j, j, j, j, j =, 6, p, (where)."}, {"heading": "3.2 Concept-Concept Context Model (3C)", "text": "Inspired by the distribution hypothesis (Harris, 1954), in this model we start from the hypothesis that \"similar concepts tend to appear in similar conceptual contexts.\" To test this hypothesis, we learn concept embedding by building a skip-gram model on contexts generated exclusively from concept mentions. As in the CRC model, we start by identifying all concept mentions in the given corpus. Then, contexts are generated only from surrounding concepts. Formally, in the face of a training corpus of V-words w1, w2,..., wV. We iterate over the corpus by identifying concept mentions, thus creating a sequence of C-concept walls c1, c2,... cC, where C < V. Then we train the skip-gram model aimed at maximizing these concepts, CC-j-weptionnt: CC-1ci-eptionnt."}, {"heading": "3.3 Training", "text": "We use a recent Wikipedia dump from August 20161 with about 7 million articles. We extract articles plaintext by discarding images and tables. We also discard references and external links (if any). We trim both articles that are not below the main namespace, and also trim all redirect pages. Finally, our corpus contained about 5 million articles in total. We edit each article by replacing all references to other Wikipedia articles with the corresponding page IDs. If one of the references is a title of a redirect page, we use the page ID of the original page to ensure that all concept mentions are normalized. Following Mikolov et al. (2013b) we use negative sampling to replace the Softmax function by replacing each protocol (wO | wI) in the Softmax function (Equation 4) and the model wS = 1Ews."}, {"heading": "3.4 BOC Densification", "text": "As we mentioned in the corresponding worksection, the current mechanisms of BOC compression are inefficient, since their complexity is least quadratic in terms of the number of non-zero elements in the BOC vector. Here, we propose a simple and efficient vector aggregation method to obtain complete 1http: / / dumps.wikimedia.org / enwiki / compressed BOC vector in linear time.Our mechanism works by performing a weighted average of the embedding vectors of all concepts in the given BOC. This process scales linearly with the number of non-zero dimensions in the BOC vector vector. In addition, it creates a completely dense vector representing the semantics of the original concepts and taking into account their weights. Formally, we consider a sparse BOC vector s = {(c1, w1), where the vector density of the vector vector vector vector vector class vector vector vector vector vector vector class vector vector-dimensions in the class vector-vector-vector-vector-vector-class-vector-vector-vector-vector-vector-vector-class-vector-vector-vector-vector-vector-vector-class-vector-vector-vector-vector-class-vector-vector-class-vector-vector-vector-vector-class-vector-vector-vector-vector-vector-vector-class-vector-vector-vector-class-vector-vector-vector-vector-vector-vector-vector-class-vector-vector-vector-vector-vector-vector-vector-class-vector-vector-vector-vector-vector-vector-vector-class-vector-vector-vector-vector-vector-class-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-"}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Entity Semantic Relatedness", "text": "We evaluate the \"goodness\" of our concept embedding to measure the semantic relationship between entities as an intrinsic assessment. Entity relations have recently been used to model the coherence of entities in many so-called entity disambiguation systems."}, {"heading": "4.1.1 Dataset", "text": "We use a benchmark data set created by Ceccarelli et al. (2013) from CoNLL 2003 data. As in previous studies (Yamada et al., 2016; Huang et al., 2015), we model the measurement of the relationship between entities as a ranking problem. We use the test split of the data set to create 3,314 queries. Each query has a query unit and \u0445 91 response units that are designated as related or unrelated. Quality is measured by the ability of the system to place related entities over unrelated entities."}, {"heading": "4.1.2 Compared Systems", "text": "We compare our models with two previous methods: 1. Yamada et al. (2016), which used the skip grammar model to learn how to embed words and entities together; 2. Witten and Milne (2008), who proposed Wikipedia Link-based Measure (WLM) as a simple mechanism to model the semantic relationship between Wikipedia concepts; and the authors used Wikipedia link structure on the assumption that related concepts would have similar in-depth linkages."}, {"heading": "4.1.3 Results", "text": "Common measures used to evaluate ranking quality are Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (nDCG). Table 1 shows the performance of our CRCs and 3C models compared to previous models. As we can see, the 3C model performs poorly in this task compared to previous models. On the other hand, our CRCs model outperforms all other methods by 2-4% in terms of nDCG and 3% in terms of MAP."}, {"heading": "4.2 Dataless Classification", "text": "Chang et al. (2008) proposed a data-less classification as a learning protocol to perform text categorization without the need for marked data to train a classifier. Given only label names and few descriptive keywords for each label, classification is effortlessly performed by mapping each label into a BOC representation using the ESA. Similarly, each instance of data is mapped into the same semantic BOC space and assigned to the most similar label by means of a correct similarity measurement like Cosinus. Formally, for a set of n labels L = {l1,..., ln}, a text document d, a BOC mapping model f and a similarity function Sim, then d is mapped to the ith label li, so that li = arg maxi Sim (f (li), f (d)))))). We evaluate the effectiveness of our concept, which models are compared on the classification task as efficacy empempiricity by incorporating BOC's insic evaluation of the results."}, {"heading": "4.2.1 Dataset", "text": "We use the 20 newsgroups dataset (20NG) (Lang, 1995), which is commonly used for benchmarking text classification algorithms; the dataset contains 20 categories, each containing 1000 news items; we obtained the BOC representations using Song and Roth's ESA (2014), which used a Wikipedia index of pages with more than 100 words and 5 + outgoing links to create 500-dimensional ESA mappings for both the categories and news items of the 20NG. We designed two types of classification tasks: 1) fine-grained classification with closely related classes such as hockey vs. baseball, cars vs. motorcycles and Guns vs. Mideast vs. Misc; and 2) coarse-grained classification with top-level categories such as sports vs. politics and sports vs. religion. The top-level categories are created by combining instances of fine-grained categories as shown in Table 2."}, {"heading": "4.2.2 Compared Systems", "text": "We compare our models with three previous methods: 1. ESA, which calculates cosmic similarity using the sparse BOC vectors; 2. WEmax & WEhung, proposed by Song and Roth (2015) for BOC compaction, using embedding from Word2Vec. As the authors reported, we set the minimum similarity threshold to 0.85. WEmax finds the best match for each concept, while WEhung uses the Hungarian algorithm to find the best concept alignment on a one-to-one basis. Both mechanisms exhibit polynomic time complexity."}, {"heading": "4.2.3 Results", "text": "In fact, ESA reaches its peak performance with a few hundred dimensions of the sparse BOC vector. Interestingly, the CRC model improves by only 14 percent in terms of cars and motorcycles, and in terms of using 70 concepts in Guns vs Mideast in the three areas."}, {"heading": "5 Conclusion", "text": "In this thesis, we proposed two models for embedding learning concepts based on the Skip-gram model. In addition, we proposed an efficient and effective mechanism for BOC compaction that exceeded the previous proposed compaction schemes in the data-less classification. In contrast to these previous compaction mechanisms, our method scales linearly with the number of BOC dimensions. Furthermore, we demonstrated from the results how this efficient mechanism enables the generation of high-quality dense BOC vectors from a few concepts, thereby reducing the need to obtain hundreds of concepts in the creation of the concept vector."}], "references": [{"title": "Learning relatedness measures for entity linking", "author": ["Diego Ceccarelli", "Claudio Lucchese", "Salvatore Orlando", "Raffaele Perego", "Salvatore Trani."], "venue": "Proceedings of the 22nd ACM international conference on Information & Knowledge Management.", "citeRegEx": "Ceccarelli et al\\.,? 2013", "shortCiteRegEx": "Ceccarelli et al\\.", "year": 2013}, {"title": "Importance of semantic representation: Dataless classification", "author": ["Ming-Wei Chang", "Lev-Arie Ratinov", "Dan Roth", "Vivek Srikumar."], "venue": "AAAI. volume 2, pages 830\u2013835.", "citeRegEx": "Chang et al\\.,? 2008", "shortCiteRegEx": "Chang et al\\.", "year": 2008}, {"title": "Concept-based information retrieval using explicit semantic analysis", "author": ["Ofer Egozi", "Shaul Markovitch", "Evgeniy Gabrilovich."], "venue": "ACM Transactions on Information Systems (TOIS) 29(2):8.", "citeRegEx": "Egozi et al\\.,? 2011", "shortCiteRegEx": "Egozi et al\\.", "year": 2011}, {"title": "Computing semantic relatedness using wikipediabased explicit semantic analysis", "author": ["Evgeniy Gabrilovich", "Shaul Markovitch."], "venue": "IJCAI. volume 7, pages 1606\u20131611.", "citeRegEx": "Gabrilovich and Markovitch.,? 2007", "shortCiteRegEx": "Gabrilovich and Markovitch.", "year": 2007}, {"title": "Distributional structure", "author": ["Zellig S Harris."], "venue": "Word .", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Entity hierarchy embedding", "author": ["Zhiting Hu", "Poyao Huang", "Yuntian Deng", "Yingkai Gao", "Eric P Xing."], "venue": "Proceedings of The 53rd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Hu et al\\.,? 2015", "shortCiteRegEx": "Hu et al\\.", "year": 2015}, {"title": "Leveraging deep neural networks and knowledge graphs for entity disambiguation", "author": ["Hongzhao Huang", "Larry Heck", "Heng Ji."], "venue": "arXiv preprint arXiv:1504.07678 .", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Context-dependent conceptualization", "author": ["Dongwoo Kim", "Haixun Wang", "Alice H Oh."], "venue": "IJCAI. pages 2330\u20132336.", "citeRegEx": "Kim et al\\.,? 2013", "shortCiteRegEx": "Kim et al\\.", "year": 2013}, {"title": "Newsweeder: Learning to filter netnews", "author": ["Ken Lang."], "venue": "Proceedings of the 12th international conference on machine learning. pages 331\u2013339.", "citeRegEx": "Lang.,? 1995", "shortCiteRegEx": "Lang.", "year": 1995}, {"title": "Joint embedding of hierarchical categories and entities for concept categorization and dataless classification", "author": ["Yuezhang Li", "Ronghuo Zheng", "Tian Tian", "Zhiting Hu", "Rahul Iyer", "Katia Sycara."], "venue": "arXiv preprint arXiv:1607.07956 .", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Combinatorial optimization: algorithms and complexity", "author": ["Christos H Papadimitriou", "Kenneth Steiglitz."], "venue": "Courier Corporation.", "citeRegEx": "Papadimitriou and Steiglitz.,? 1982", "shortCiteRegEx": "Papadimitriou and Steiglitz.", "year": 1982}, {"title": "On dataless hier", "author": ["Yangqiu Song", "Dan Roth"], "venue": null, "citeRegEx": "Song and Roth.,? \\Q2014\\E", "shortCiteRegEx": "Song and Roth.", "year": 2014}, {"title": "Joint learning of the em", "author": ["Yoshiyasu Takefuji"], "venue": null, "citeRegEx": "Takefuji.,? \\Q2016\\E", "shortCiteRegEx": "Takefuji.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "For example, measuring semantic similarity/relatedness (Gabrilovich and Markovitch, 2007; Kim et al., 2013; Shalaby and Zadrozny, 2015), dataless classification (Chang et al.", "startOffset": 55, "endOffset": 135}, {"referenceID": 7, "context": "For example, measuring semantic similarity/relatedness (Gabrilovich and Markovitch, 2007; Kim et al., 2013; Shalaby and Zadrozny, 2015), dataless classification (Chang et al.", "startOffset": 55, "endOffset": 135}, {"referenceID": 1, "context": ", 2013; Shalaby and Zadrozny, 2015), dataless classification (Chang et al., 2008; Song and Roth, 2014, 2015; Li et al., 2016), short text clustering (Song et al.", "startOffset": 61, "endOffset": 125}, {"referenceID": 9, "context": ", 2013; Shalaby and Zadrozny, 2015), dataless classification (Chang et al., 2008; Song and Roth, 2014, 2015; Li et al., 2016), short text clustering (Song et al.", "startOffset": 61, "endOffset": 125}, {"referenceID": 2, "context": ", 2015), search and relevancy ranking (Egozi et al., 2011), event detection and coreference resolution (Peng et al.", "startOffset": 38, "endOffset": 58}, {"referenceID": 3, "context": "For example, Gabrilovich and Markovitch (2007) proposed Explicit Semantic Analysis (ESA) which uses Wikipedia articles as concepts and the TF-IDF score of the terms in these article as the association score.", "startOffset": 13, "endOffset": 47}, {"referenceID": 11, "context": "In this paper we propose two neural embedding models in order to learn continuous concept vectors based on the skip-gram model (Mikolov et al., 2013b).", "startOffset": 127, "endOffset": 150}, {"referenceID": 10, "context": "Song and Roth (2015) proposed using the popular Word2Vec model (Mikolov et al., 2013a) to obtain the embeddings of each concept by averaging the vectors of the concept\u2019s individual words.", "startOffset": 63, "endOffset": 86}, {"referenceID": 11, "context": "Song and Roth (2015) proposed using the popular Word2Vec model (Mikolov et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "(2016) who all utilize the skip-gram model (Mikolov et al., 2013b), but differ in how they define the context of the target concept.", "startOffset": 43, "endOffset": 66}, {"referenceID": 5, "context": "Such concept embedding models were proposed by Hu et al. (2015); Li et al.", "startOffset": 47, "endOffset": 64}, {"referenceID": 5, "context": "Such concept embedding models were proposed by Hu et al. (2015); Li et al. (2016); Yamada et al.", "startOffset": 47, "endOffset": 82}, {"referenceID": 5, "context": "Such concept embedding models were proposed by Hu et al. (2015); Li et al. (2016); Yamada et al. (2016) who all utilize the skip-gram model (Mikolov et al.", "startOffset": 47, "endOffset": 104}, {"referenceID": 5, "context": "(2016) extended the embedding model proposed by Hu et al. (2015) by jointly learning concept and category embeddings from contexts defined by all other concepts in the target concept\u2019s article as well as its category hierarchy in Wikipedia.", "startOffset": 48, "endOffset": 65}, {"referenceID": 8, "context": "Unlike Li et al. (2016) and Hu et al.", "startOffset": 7, "endOffset": 24}, {"referenceID": 5, "context": "(2016) and Hu et al. (2015) who learn concept embeddings only, our CRC model (described in Section 3), maps both words and concepts into the same semantic space.", "startOffset": 11, "endOffset": 28}, {"referenceID": 5, "context": "(2016) and Hu et al. (2015) who learn concept embeddings only, our CRC model (described in Section 3), maps both words and concepts into the same semantic space. Therefore we can easily measure word-word, word-concept, and concept-concept semantic similarities. In addition, compared to Yamada et al. (2016) model, we utilize contexts generated from both surrounding words and concepts.", "startOffset": 11, "endOffset": 308}, {"referenceID": 5, "context": "(2016) and Hu et al. (2015) who learn concept embeddings only, our CRC model (described in Section 3), maps both words and concepts into the same semantic space. Therefore we can easily measure word-word, word-concept, and concept-concept semantic similarities. In addition, compared to Yamada et al. (2016) model, we utilize contexts generated from both surrounding words and concepts. Therefore, we can better capture local contextual information of each target word/concept. Moreover, our proposed models are computationally less costly than Hu et al. (2015) and Yamada et al.", "startOffset": 11, "endOffset": 562}, {"referenceID": 5, "context": "(2016) and Hu et al. (2015) who learn concept embeddings only, our CRC model (described in Section 3), maps both words and concepts into the same semantic space. Therefore we can easily measure word-word, word-concept, and concept-concept semantic similarities. In addition, compared to Yamada et al. (2016) model, we utilize contexts generated from both surrounding words and concepts. Therefore, we can better capture local contextual information of each target word/concept. Moreover, our proposed models are computationally less costly than Hu et al. (2015) and Yamada et al. (2016) models as they require few hours rather than days to train on similar computing resources.", "startOffset": 11, "endOffset": 587}, {"referenceID": 5, "context": "(2016) and Hu et al. (2015) who learn concept embeddings only, our CRC model (described in Section 3), maps both words and concepts into the same semantic space. Therefore we can easily measure word-word, word-concept, and concept-concept semantic similarities. In addition, compared to Yamada et al. (2016) model, we utilize contexts generated from both surrounding words and concepts. Therefore, we can better capture local contextual information of each target word/concept. Moreover, our proposed models are computationally less costly than Hu et al. (2015) and Yamada et al. (2016) models as they require few hours rather than days to train on similar computing resources. BOC Densification: distributed concept vectors have been used by BOC densification mechanisms to overcome the BOC sparsity problem. Song and Roth (2015) proposed three different mechanisms for aligning the concepts at different indices given a sparse BOC pair (u,v) in order to increase their similarity score.", "startOffset": 11, "endOffset": 831}, {"referenceID": 12, "context": "It utilizes the Hungarian method in order to find an optimal alignment on a one-to-one basis (Papadimitriou and Steiglitz, 1982).", "startOffset": 93, "endOffset": 128}, {"referenceID": 9, "context": "This mechanism performed the best on dataless classification and was also utilized by Li et al. (2016). However, the Hungarian method is a combinatorial optimization algorithm whose complexity is polynomial.", "startOffset": 86, "endOffset": 103}, {"referenceID": 11, "context": "Here we try to learn continuous concept vectors by building upon the skip-gram embedding model (Mikolov et al., 2013b).", "startOffset": 95, "endOffset": 118}, {"referenceID": 10, "context": "Mikolov et al. (2013b) proposed hierarchical softmax and negative sampling as efficient alternatives to approximate the softmax function which becomes computationally intractable whenW becomes huge.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "Inspired by the distributional hypothesis (Harris, 1954), we, in this model, hypothesize that \u201dsimilar concepts tend to appear in similar conceptual contexts\u201d.", "startOffset": 42, "endOffset": 56}, {"referenceID": 8, "context": "This model is different from Li et al. (2016) and Hu et al.", "startOffset": 29, "endOffset": 46}, {"referenceID": 5, "context": "(2016) and Hu et al. (2015) as they define the context of a target concept by all the other concepts which appear in the concept\u2019s corresponding article.", "startOffset": 11, "endOffset": 28}, {"referenceID": 10, "context": "Following Mikolov et al. (2013b), we utilize negative sampling to approximate the softmax function by replacing every log p(wO|wI) term in the softmax function (equation 4) with:", "startOffset": 10, "endOffset": 33}, {"referenceID": 6, "context": "WLM (Huang et al., 2015) 0.", "startOffset": 4, "endOffset": 24}, {"referenceID": 8, "context": "We use the 20 Newsgroups dataset (20NG) (Lang, 1995) which is commonly used for benchmarking text classification algorithms.", "startOffset": 40, "endOffset": 52}, {"referenceID": 8, "context": "We use the 20 Newsgroups dataset (20NG) (Lang, 1995) which is commonly used for benchmarking text classification algorithms. The dataset contains 20 categories each has \u223c1000 news posts. We obtained the BOC representations using ESA from Song and Roth (2014) who utilized a Wikipedia index containing pages with 100+ words and 5+ outgoing links to create ESA mappings of 500 dimensions for both the categories and news posts of the 20NG.", "startOffset": 41, "endOffset": 259}, {"referenceID": 13, "context": "WEmax &WEhung which were proposed by Song and Roth (2015) for BOC densification using embeddings obtained from Word2Vec.", "startOffset": 37, "endOffset": 58}], "year": 2017, "abstractText": "Explicit concept space models have proven efficacy for text representation in many natural language and text mining applications. The idea is to embed textual structures into a semantic space of concepts which captures the main topics of these structures. That so called bag-of-concepts representation suffers from data sparsity causing low similarity scores between similar texts due to low concept overlap. In this paper we propose two neural embedding models in order to learn continuous concept vectors. Once learned, we propose an efficient vector aggregation method to generate fully dense bag-of-concepts representations. Empirical results on a benchmark dataset for measuring entity semantic relatedness show superior performance over other concept embedding models. In addition, by utilizing our efficient aggregation method, we demonstrate the effectiveness of the densified vector representation over the typical sparse representations for dataless classification where we can achieve at least same or better accuracy with much less dimensions.", "creator": "LaTeX with hyperref package"}}}