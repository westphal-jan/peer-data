{"id": "1702.03380", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2017", "title": "Training Deep Neural Networks via Optimization Over Graphs", "abstract": "In this work, we propose to train a deep neural network by distributed optimization over a graph. Two nonlinear functions are considered: the rectified linear unit (ReLU) and a linear unit with both lower and upper cutoffs (DCutLU). The problem reformulation over a graph is realized by explicitly representing ReLU or DCutLU using a set of slack variables. We then apply the alternating direction method of multipliers (ADMM) to update the weights of the network layerwise by solving subproblems of the reformulated problem. Empirical results suggest that by proper parameter selection, the ADMM- based method converges considerably faster than gradient descent method.", "histories": [["v1", "Sat, 11 Feb 2017 04:02:40 GMT  (889kb)", "https://arxiv.org/abs/1702.03380v1", "5 pages"], ["v2", "Sat, 17 Jun 2017 11:18:48 GMT  (973kb)", "http://arxiv.org/abs/1702.03380v2", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["guoqiang zhang", "w bastiaan kleijn"], "accepted": false, "id": "1702.03380"}, "pdf": {"name": "1702.03380.pdf", "metadata": {"source": "CRF", "title": "Training Deep Neural Networks via Optimization Over Graphs", "authors": ["Guoqiang Zhang", "Bastiaan Kleijn"], "emails": ["guoqiang.zhang@uts.edu.au", "aan.kleijn@ecs.vuw.ac.nz"], "sections": [{"heading": null, "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "II. ON TRAINING A DEEP NEURAL NETWORK", "text": "Suppose we have a sequence of m-training samples, represented by an input matrix D-Rm \u00b7 nin and an output matrix O-Rm \u00b7 nout, in which the q'th row vectors of D and O form an input / output pair. In view of (D, O), we consider the formation of a deep neural network with the weights {(Wi, bi) | i = 1,. \u2212 l of the N layers, in which for each i, Wi-R ni \u2212 1 \u00b7 ni is a weight matrix and bi-R 1 \u00b7 ni is a bias vector. To match the network with the training samples, we leave n0 = nin and nN = nout. The goal is to find the correct weights {(Wi, bi)}} so that the 2 network maps the input matrix and the output O as accurately as possible. Now, let us define the operation of the individual layers Vi \u2212 element and the n \u2212 element, and the N \u2212 not i (Wi-i i), i i i and i \u2212 i."}, {"heading": "III. PROBLEM REFORMULATION ONTO A GRAPH", "text": "First, we present the nonlinear function (1) by introducing a series of flat variables (1). Specifically, (1) can be rewritten asXi = Vi \u2212 1Wi + ebi (4) Xi + Yi = max (Vi \u2212 1Wi + ebi) Xi + ebi (4) Xi + Yi = max (Vi \u2212 1Wi + ebi, l) (5) Xi + Zi = min (Xi + Yi, u), (6) where for each i (1) Xi + ebi (4) Xi + Yi = max (Vi \u2212 1), we introduced three flat matrices Xi, Yi and Zi to characterize the effect of the upper and lower cutoffs of the function on u and l. Next, we argue that the min and max operators can be expressed in (5) - (6) Yi with respect to the constraints on (Xi, Yi, Zi)."}, {"heading": "IV. DISTRIBUTED OPTIMIZATION OVER A GRAPH", "text": "We find that (11) - (13) is a nonconvex optimization due to the nonlinear equality limitations (12). We solve (11) - (13) iteratively with ADMM by solving convex partial problems. It is worth noting that ADMM has already been successfully used to solve non-negative matrix factorization (NMF) [21], which is not convex."}, {"heading": "A. Augmented Lagrangian function", "text": "To apply ADMM, we introduce a Lagrange multiplier \u0441i for the ith equality constraint in (12). We build an advanced 3Lagrange function asL {\u03c1i} ({Xi, Yi, Zi, bi, Wi, \u0432i}, XN | {\u0432 l i, \u0432u i}) = fN (XN; O) + N [i = 1gi (Wi, bi) + N \u2212 1 [i], (14) where for each i = 1,., N, pi, \u0432i (i) + N = 1pi, \u03c1i ((Xi \u2212 1, Yi \u2212 1, Zi \u2212 1), Xi, (Wi, bi), \u0394i), (14), where for each i = 1,.., pi, \u0432i (\u00b7 \u00b7) a positive (\u00b7 \u00b7 \u00b7 \u00b7 \u00b7) definition for Aspi (\u00b7 \u00b7 \u00b7 \u00b7), \u0432i (\u00b7 \u00b7 \u00b7), \u0432i (\u00b7 \u00b7), \u0432i (\u00b7 \u00b7), \u0432i (\u00b7 Z \u2212 1, Z \u2212 1, \u2212 Z, \u2212 0.0, \u2212 Z \u2212 0.\u2212 0.\u2212)."}, {"heading": "B. Blockwise parameter updating using ADMM", "text": "We are now considering the optimization of the Lagrangian function L {i}. We follow a similar updating procedure as the SGD- and the Adam method [10]. That is, we initialize with each iteration all variables and index sets of L {i} by introducing D to the network through the forward calculation. We then update all variables of L {i} block by backward calculation. Unlike with SGD, which calculates the gradient directly, the variables of L {i} are updated by solving minor optimization problems. Let us assume that we update the variables of the layer i + 1 and would like to update the new layer (Xi, Yi, Zi), (Wi, bi) and the new layer i i of layer i. We simplify L {i} initially by removing irrelevant components of L {i}, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i"}, {"heading": "C. Handling of the indicator function", "text": "The function fi (\u00b7) in (16) is composed of a series of indicator functions that make it difficult to calculate (X-newi, Y-new i, Z-new i) in Table I. To facilitate the calculation, we introduce the auxiliary variables (Xci, Y-c, Z-i) to replace (Xi, Yi, Zi) in fi (Xi, Yi, Zi-ward i) with the constraints Xci = X-c i, Y-c i = Yi and Z-i = Zi. We then reapply ADMM to handle the three equality constraints. To do this, we add a new advanced complication role asLi, \u03b2i (Xi, Yi, Zi-section i, Y-y-section i), (X c-c, Y-world, Y-world, Y-world, Z-world, Z-world (X-world)."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "In the simulation, we looked at the handwritten recognition problem by using MNIST with the standard division of the examined data (60,000 samples) and tests (10,000 samples), building a DNN of three layers (N = 3), in which the first and second hidden layer consists of 500 and 600 neurons, respectively; the initial function was chosen as the sum of the individual cross-entropy functions ([22]); the gi function (Wi, bi) was chosen as 0.1 2 (Wi, bi); Adam batch size was specified as 3000; the total training dataset consisted of 20 minibatches.We point out that the cross-entropy function makes it difficult to analytically compile newN in Table I. Updating the above variables at each iteration, we approximate each crossentropy function by a square estimate, where the square coefficient is set to 0.05 and the two method are referred to."}, {"heading": "VI. CONCLUSIONS", "text": "We have proposed a new algorithm for forming a DNN by performing optimizations using a factor graph. The decisive step is to explicitly represent the ReLUs or DCutLUs through a series of slip variables, which allows calculation at the level of the level of the level of the neurons, instead of as in [14]. Experimental results indicate that the new algorithm is less sensitive to overmatching than two references. A future direction of research is to automatically adjust the learning rates {\u03c1i} and {\u03b2i} of the new algorithm, which will probably lead to a good convergence rate for various learning problems.5"}], "references": [{"title": "Deep Learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, pp. 436\u2013444, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "arXiv preprint arXiv:1311.2901v3, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding Neural Networks Through Deep Visualization", "author": ["J. Yosinski", "J. Clune", "A. Nguyen", "T. Fuchs", "H. Lipson"], "venue": "arXiv preprint arXiv:1506.06579v1, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic Modeling Using Deep Belief Networks", "author": ["A.-R. Mohamed", "G.E. Dahl", "G. Hinton"], "venue": "IEEE Trans. Audio Speech Lang. Process, pp. 14\u201322, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Context-Dependent Pre-Trained Deep Neural Networks for Large Vocabulary Speech Recognition", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Trans. Audio Speech Lang. Process, vol. 20, pp. 33\u201342, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Nips, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep Face: Closing the Gap to Human-Level Performance in Face Verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "Proc. Conference on Computer Vision and Pattern Recognition, 2014, pp. 1701\u20131708.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech Recognition with Deep Recurrent Neural Networks", "author": ["A. Graves", "A.-R. Mohamed", "G. Hinton"], "venue": "Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2013, pp. 6645\u20136649.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "Q. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems 27, 2014, pp. 3104\u20133112.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for Stochastic Optimization", "author": ["D.P. Kingma", "J.L. Ba"], "venue": "arXiv preprint arXiv:1412.6980v9, 2017.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed Optimization of Deeply Nested Systems", "author": ["M. Carreira-Perpinan", "W. Wang"], "venue": "arXiv:1212.5921 [cs.LG], 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Training Nueral Networks Without Gradients: A Scalable ADMM Approach", "author": ["G. Taylor", "R. Burmeister", "Z. Xu", "B. Singh", "A. Patel", "T. Goldstein"], "venue": "Proc. IEEE Int. Conf. Machine Learning, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Introduction to Dual Decomposition for Inference", "author": ["D. Sontag", "A. Globerson", "T. Jaakkola"], "venue": "Optimization for Machine Learning. MIT Press, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "An Alternating Direction Method for Dual MAP LP Relaxation", "author": ["O. Meshi", "A. Globerson"], "venue": "ECML, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep Sparse Rectifier Neural Networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proceedings of the 14th International Con- ference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 315\u2013323.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M. Wainwright", "M. Jordan"], "venue": "Foundations and Trends in Machine Learning, vol. 1(1-2), pp. 1\u2013305, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Nonnegative Matrix Factorization Using ADMM: Algorithm and Convergence Analysis", "author": ["D. Hajinezhad", "T.-H. Chang", "X. Wang", "Q. Shi", "M. Hong"], "venue": "Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016, pp. 4742\u20134746.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In the last decade, research on deep learning has made remarkable progress both in theoretical understanding and in practical applications (see [1] for an overview).", "startOffset": 144, "endOffset": 147}, {"referenceID": 1, "context": "Each layer is composed of a set of simple nonlinear processing units (referred to as neurons), which aims to transform the input into progressively more abstract representations [2], [3].", "startOffset": 178, "endOffset": 181}, {"referenceID": 2, "context": "Each layer is composed of a set of simple nonlinear processing units (referred to as neurons), which aims to transform the input into progressively more abstract representations [2], [3].", "startOffset": 183, "endOffset": 186}, {"referenceID": 3, "context": "For instance, feedforward neural networks have been successfully applied in speech recognition [4], [5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "For instance, feedforward neural networks have been successfully applied in speech recognition [4], [5].", "startOffset": 100, "endOffset": 103}, {"referenceID": 5, "context": "Convolutional neural networks (CNNs) are popular in computer vision [6], [7].", "startOffset": 68, "endOffset": 71}, {"referenceID": 6, "context": "Convolutional neural networks (CNNs) are popular in computer vision [6], [7].", "startOffset": 73, "endOffset": 76}, {"referenceID": 7, "context": "Recurrent neural networks (RNNs) have proven to be effective for mapping sequential inputs and outputs [8], [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "Recurrent neural networks (RNNs) have proven to be effective for mapping sequential inputs and outputs [8], [9].", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "The traditional supervised learning approach treats a neural network as a large complex model [1] rather than decomposing it as a combination of many small nonlinear models.", "startOffset": 94, "endOffset": 97}, {"referenceID": 9, "context": "nz been proposed to use the gradient information smartly for either fast convergence or automatic parameter adjustment, such as Adam [10], AdaGrad [11] and RMSprop [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "nz been proposed to use the gradient information smartly for either fast convergence or automatic parameter adjustment, such as Adam [10], AdaGrad [11] and RMSprop [12].", "startOffset": 147, "endOffset": 151}, {"referenceID": 11, "context": "In [13], the authors firstly proposed to decouple the nested structure of DNNs by introducing a set of auxiliary variables and a set of equality constraints.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "The work of [14] avoids the gradient computation of [13] by using the alternating direction method of multipliers (ADMM) [15].", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "The work of [14] avoids the gradient computation of [13] by using the alternating direction method of multipliers (ADMM) [15].", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "The work of [14] avoids the gradient computation of [13] by using the alternating direction method of multipliers (ADMM) [15].", "startOffset": 121, "endOffset": 125}, {"referenceID": 12, "context": "However, [14] needs to perform a computation at each and every neuron to be able to characterize its nonlinear operation.", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "The Bregman iteration is used in [14] to produce stable algorithmic convergence.", "startOffset": 33, "endOffset": 37}, {"referenceID": 14, "context": "In this paper, we propose to train a deep neural network by reformulating the problem as an optimization over a factor graph G = (V , C) [16], [17].", "startOffset": 137, "endOffset": 141}, {"referenceID": 15, "context": "In this paper, we propose to train a deep neural network by reformulating the problem as an optimization over a factor graph G = (V , C) [16], [17].", "startOffset": 143, "endOffset": 147}, {"referenceID": 16, "context": "Our graphic formulation is able to handle rectified linear units (ReLUs) (see [18], [19]) and linear units with both upper and lower cutoffs (DCutLUs) at the layer-level.", "startOffset": 78, "endOffset": 82}, {"referenceID": 17, "context": "Our graphic formulation is able to handle rectified linear units (ReLUs) (see [18], [19]) and linear units with both upper and lower cutoffs (DCutLUs) at the layer-level.", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "Differently from [14] which has to perform computations at the neuron-level, our proposed method is able to perform computations at the layer-level like the SGD and Adam.", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "The popular forms for the nonlinear function hi are sigmoid, tanh and ReLU [1].", "startOffset": 75, "endOffset": 78}, {"referenceID": 17, "context": "It is found in [19] that ReLU leads to fast convergence using SGD as compared to sigmoid and tanh.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "(11)-(13) define a problem over a factor graph G = (V , C) (see [17], [16], [20]), where every node r \u2208 V carries a (convex) component function of (11) and every factor c \u2208 C carries an (nonlinear) equality constraint of (12) (see Fig.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "(11)-(13) define a problem over a factor graph G = (V , C) (see [17], [16], [20]), where every node r \u2208 V carries a (convex) component function of (11) and every factor c \u2208 C carries an (nonlinear) equality constraint of (12) (see Fig.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "(11)-(13) define a problem over a factor graph G = (V , C) (see [17], [16], [20]), where every node r \u2208 V carries a (convex) component function of (11) and every factor c \u2208 C carries an (nonlinear) equality constraint of (12) (see Fig.", "startOffset": 76, "endOffset": 80}, {"referenceID": 19, "context": "It is worth noting that ADMM has already been successfully applied for solving nonnegative matrix factorization (NMF) [21], which is nonconvex.", "startOffset": 118, "endOffset": 122}, {"referenceID": 9, "context": "We follow a similar updating procedure as the SGD and Adam methods [10].", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "In the first experiment, we tested ADMM, SGD and Adam [10] using only the ReLUs.", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "1) Comparison with the state-of-the-art: In addition to ADMM, we also evaluated SGD and Adam [10], where Adam represents the state-of-the-art training method.", "startOffset": 93, "endOffset": 97}, {"referenceID": 9, "context": "Adam was implemented by following [10] directly.", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "The key step is to explicitly represent the ReLUs or DCutLUs by a set of slack variables, which enables layer-level computation rather than neuron-level computation as in [14].", "startOffset": 171, "endOffset": 175}], "year": 2017, "abstractText": "In this work, we propose to train a deep neural network by distributed optimization over a graph. Two nonlinear functions are considered: the rectified linear unit (ReLU) and a linear unit with both lower and upper cutoffs (DCutLU). The problem reformulation over a graph is realized by explicitly representing ReLU or DCutLU using a set of slack variables. We then apply the alternating direction method of multipliers (ADMM) to update the weights of the network layerwise by solving subproblems of the reformulated problem. Empirical results suggest that the ADMM-based method is less sensitive to overfitting than the stochastic gradient descent (SGD) and Adam methods.", "creator": "LaTeX with hyperref package"}}}