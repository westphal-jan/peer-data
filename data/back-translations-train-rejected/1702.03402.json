{"id": "1702.03402", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2017", "title": "Parallel Long Short-Term Memory for Multi-stream Classification", "abstract": "Recently, machine learning methods have provided a broad spectrum of original and efficient algorithms based on Deep Neural Networks (DNN) to automatically predict an outcome with respect to a sequence of inputs. Recurrent hidden cells allow these DNN-based models to manage long-term dependencies such as Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM). Nevertheless, these RNNs process a single input stream in one (LSTM) or two (Bidirectional LSTM) directions. But most of the information available nowadays is from multistreams or multimedia documents, and require RNNs to process these information synchronously during the training. This paper presents an original LSTM-based architecture, named Parallel LSTM (PLSTM), that carries out multiple parallel synchronized input sequences in order to predict a common output. The proposed PLSTM method could be used for parallel sequence classification purposes. The PLSTM approach is evaluated on an automatic telecast genre sequences classification task and compared with different state-of-the-art architectures. Results show that the proposed PLSTM method outperforms the baseline n-gram models as well as the state-of-the-art LSTM approach.", "histories": [["v1", "Sat, 11 Feb 2017 09:50:40 GMT  (99kb)", "http://arxiv.org/abs/1702.03402v1", "2016 IEEE Workshop on Spoken Language Technology"]], "COMMENTS": "2016 IEEE Workshop on Spoken Language Technology", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["mohamed bouaziz", "mohamed morchid", "richard dufour", "georges linar\\`es", "renato de mori"], "accepted": false, "id": "1702.03402"}, "pdf": {"name": "1702.03402.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Mohamed Bouaziz", "Mohamed Morchid", "Richard Dufour", "Georges Linar\u00e8s", "Renato De Mori"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 2.03 402v 1 [cs.L G] 11 Feb 20Recently, machine learning methods have provided a wide range of original and efficient algorithms based on Deep Neural Networks (DNN) to automatically predict a result in terms of a sequence of inputs. However, recurring hidden cells enable these DNN-based models to manage long-term dependencies such as Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM). Nevertheless, these RNs process a single input stream in one (LSTM) or two (bi-directional LSTM) directions, but most of the information available today comes from multistreams or multimedia documents and requires RNNNs to process this information synchronously during training. This paper presents an original LSTM-based architecture called Parallel LSTM (PLSTM) that uses multiple PLNNNs to perform parallel input sequencing for STM classification of the STM."}, {"heading": "1. INTRODUCTION", "text": "Recently, automated sequence classification has become a pervasive problem, having met with a great deal of research interest [1, 2, 3] due to the need to structure knowledge as a set of dependent localized information alongside new computing capabilities in order to efficiently process large amounts of data. Recent methods used to structure these sequences include a set of high-level representations that are well adapted to automated sequence classification based on Deep Neural Networks (CNN). [4] or recursive neural networks (RNN) [5].RNN architectures such as Long Short-Term Memory (LSTM) and bi-directional LSTM (BLSTM) have particular attention in various domains and tasks such as set [8] or consecutive images [9]."}, {"heading": "2. RECURRENT NEURAL NETWORKS", "text": "This section presents the latest concepts of two recurrent neural networks: LSTM and BLSTM."}, {"heading": "2.1. Long Short-Term Memory (LSTM)", "text": "Long Short-Term Memory (LSTM) [6] networks are a special case of Recurrent Neural Networks (RNNs) [5]. The goal of this architecture is to generate an internal cell state of the network that allows the RNN to display a dynamic temporal behavior. This internal state allows the RNN to process arbitrary sequences of input such as word strings [8] for speech modeling, time series [1].The RNN takes a sequence x = (x1, x2,., xT) as input and computes the hidden sequence h = (h1, h2,., hT) and the output vector y = (y1, y2,., yT) by iterating t = 1 to T: Tor = 1 and T: Tor = H (Wxhxt + Whhxt \u2212 1)."}, {"heading": "2.2. Bidirectional Long Short-Term Memory (BLSTM)", "text": "LSTM networks use only the previous context to predict the next segment for a given sequence. Bi-directional RNN (BRNN) [16], shown in Figure 2, can process both directions with two separate hidden layers (one for each direction), resulting in the same output layer that is passed through the two hidden layers. Therefore, the BRNN calculates both forward hidden sequence \u2212 \u2192 h and backward hidden sequence \u2190 \u2212 h as well as the output vector y by iterating \u2212 \u2192 h from t = 1 to T, and \u2190 \u2212 h from t = T to 1: \u2212 \u2192 h t = H (Wx \u2212 h xt + W \u2212 \u2192 h \u2212 h \u2212 h) (8)."}, {"heading": "3. PARALLEL LONG SHORT-TERM MEMORY (PLSTM)", "text": "The BRNN neural architecture uses the same sequence x as input for forward and backward directions, which is useful for information from a single stream. The paper proposes an original neural network, called Parallel RNN (PRNN), and is shown in Figure 3, which benefits from the BRNN structure in a multistream context. By replacing the PRNN cells with LSTM cells, the proposed parallel sequence (PLSTM) is achieved. The original PLSTM architecture corresponds to the PRNN description by replacing the H function with the LSTM composite function. PLSTM differs from the classic BLSTM sequence in that there is no common sequence, but different input vectors through a dedicated input vector xn structure. Moreover, BLSTM only uses 2 hidden layers."}, {"heading": "4. EXPERIMENTAL PROTOCOL", "text": "The classification of multistream sequences is carried out using the proposed PLSTM architecture (2 and 4 parallel sequences) and the classic LSTM network as part of an automatic task for genre labelling of TV shows. Two n-gram-based models (baseline) are also taken into account for a fair comparison. In the next sections, the data set, the classification of genre sequences and the settings for neural networks are described."}, {"heading": "4.1. Multichannel EPG dataset", "text": "The Electronic Program Guide (EPG) is extracted from 4 French TV channels (M6, TF1, France 5 and TV5 Monde) for 3 years from January 2013 to December 2015. In our experiments, the M6 channel is used as the output stream. Data from 2013 and 2014 are merged and divided into training (70%) and validation data sets (30%), using a layered shuffle split [18] to obtain the same percentage of samples from each class in the output of both folds, while the 2015 data set is kept for testing purposes. To ensure a clean testing environment, labels (i.e. genres) missing in at least one of the three folds have been removed, enabling us to obtain equivalent data sets in terms of the label vocabulary. Table 1 shows the generic distribution for M6, the chosen output channel."}, {"heading": "4.2. Genre Prediction Experiments", "text": "The size of the genre sequence (s) varies from 1 to 4. Then, three input configurations are used: Mono-channel input: Only M6 history sequences for a basic experiment with n-gram (using a statistical language model from the SRILM toolkit [19]) and a simple LSTM model. Bi-channel input: Both M6 and TF1 channel history are used as input for P2LSTM (PLSTM with two parallel streams as BLSTM with forward directions and separate inputs). The aim of this experiment is to expand the context information using a similar and competing channel, both being generalist channels. Multi-channel input: The history of each of the 4 streams (i.e. channels) is used as input for 4n-gram and P4STM 4 (experiments in parallel)."}, {"heading": "4.3. Neural Networks Setup", "text": "The classic LSTM and the proposed P2LSTM and P4LSTM consist of three layers: input layer x with a size of 1 to 4, a hidden layer h with a size of 80 for all LSTM-based models, and an output layer y with a size corresponding to the number of possible TV genres (11). The Keras library [20], which is based on Theano [21] for fast tensor manipulation and CUDAbased GPU acceleration, was used to train neural networks on a Nvidia GeForce GTX TITAN X GPU card. Training times, detailed in Table 2 for all models, match the sequence size of all models. In fact, even the most time-consuming configuration, namely P4LSTM with 4 elements of history, takes no more than 25 minutes."}, {"heading": "5. RESULTS AND DISCUSSION", "text": "Table 3 shows the overall results in terms of the standard formula 1 metric in terms of the results of the genre prediction tasks using each method and for different stream sequence sizes from 1 to 4."}, {"heading": "5.1. N-gram based models", "text": "The multi-channel 4n gram model exceeds the simple ngram sequence for each of the different 4 genre sequence configurations except for the 3 genre history. The 4n gram method achieves about 60% of the F score with 1 genre sequences compared to almost 57% for mono-channel n grammar with its best history configuration. The observed results confirm the interest in using multiple streams to predict the genre of the next show for a particular channel."}, {"heading": "5.2. LSTM and PLSTM", "text": "Table 3 shows that mono-channel LSTM performance gradually moves closer and closer to the multi-channel n-gram models as the size of the sequences increases and overtakes them with an F1 value of 58% when using 4-dimensional sequences. Therefore, LSTM-based models require longer sequences to learn long-term dependencies than the ngram-based methods. P4LSTM achieves the best result with an F1 value of almost 66% when using a 4-dimensional sequence. To analyze these results, the error rates (ER) are also presented in Table 4. Overall F1 values differ from those associated with ER. Thus, the P4LSTM error rate in its best configuration reaches about 21.5% when using a 4-dimensional sequence, which corresponds to a correct rate of 78.5% compared to an F1 measurement of just 66%. This is because the F1 meter, in its 1.14 configuration, might not be suitable for the task of assigning the different labels between the 614 and 614 labels."}, {"heading": "5.3. Discussion", "text": "Confusion matrices of 4n-gram and P4LSTM experiments with sequences of 4 TV shows are shown in Tables 5 and 6, respectively, to point out the advantages of the proposed PLSTM model. It is worth noting that most of the missed cases in all systems are erroneously referred to as one of the two most common classes, weather and fiction, as well as the Other Magazine genre (some examples are in green cells). False positives are more common in The Other Magazine than in News, the relatively more common class. The reason for this is that news is a well-defined genre that mostly occurs at the same time every day, unlike the Other Magazine genre, which includes various TV shows broadcast at multiple and irregular times of the day. Teleshopping shows are often broadcast at almost the same time in the morning as cartoons and dramatically affect the performance of the 4n-gram model, which usually occurs in this context at the same time as the Other Magazine Genre, which includes different TV shows broadcast at multiple times of the day and irregular times of the day."}, {"heading": "6. CONCLUSION", "text": "The paper proposes an original neural network architecture based on long short-term memory (LSTM) for the automatic classification of multistream sequences called PLSTM. PLSTM is evaluated as part of a task to predict telecommunications genres and the observed results show that the proposed PLSTM is efficient when the size of the sequences is large enough, with a gain of more than 10 error points compared to the classical ngram model and about 7 and 4 points respectively compared to LSTM and P2LSTM. Future work will apply this promising multistream architecture of neural networks to spoken language understanding tasks such as topic extraction, keyword spotting and part-of-speech tagging."}, {"heading": "7. REFERENCES", "text": "[1] F. A. Gers, D. Eck, and J. Schmidhuber, \"Applyinglstm to time series calculable through time-window approaches,\" in Artificial Neural NetworksICANN 2001. Springer, 2001, pp. 669-676. [2] A. Severyn and A. Moschitti, \"Twitter sentiment anal-ysis with deep convolutional neural networks,\" in Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Retrieval. ACM, 2015, pp. 959-962. [3] M. Huang, Y. Cao, and C. Dong, Modeling rich con-texts for sentiment classification with lstm, CoRR, vol. abs / 1605.01478, 2016. [4] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \"Gradient-based learning applied to document recognition,\" Proceedings of the IEE, vol."}], "references": [{"title": "Applying lstm to time series predictable through time-window approaches", "author": ["F.A. Gers", "D. Eck", "J. Schmidhuber"], "venue": "Artificial Neural NetworksICANN 2001. Springer, 2001, pp. 669\u2013676.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Twitter sentiment analysis with deep convolutional neural networks", "author": ["A. Severyn", "A. Moschitti"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2015, pp. 959\u2013962.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling rich contexts for sentiment classification with lstm", "author": ["M. Huang", "Y. Cao", "C. Dong"], "venue": "CoRR, vol. abs/1605.01478, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1990}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u2013 1780, 1997.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, vol. 18, no. 5, pp. 602\u2013610, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Lstm neural networks for language modeling.", "author": ["M. Sundermeyer", "R. Schl\u00fcter", "H. Ney"], "venue": "in INTERSPEECH,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3156\u20133164.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Bidirectional lstm networks for improved phoneme classification and recognition", "author": ["A. Graves", "S. Fern\u00e1ndez", "J. Schmidhuber"], "venue": "Artificial Neural Networks: Formal Models and Their Applications\u2013ICANN 2005. Springer, 2005, pp. 799\u2013804.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "An application of recurrent neural networks to discriminative keyword spotting", "author": ["S. Fern\u00e1ndez", "A. Graves", "J. Schmidhuber"], "venue": "Artificial Neural Networks\u2013 ICANN 2007. Springer, 2007, pp. 220\u2013229.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Abandoning emotion classes-towards continuous emotion recognition with modelling of long-range dependencies.", "author": ["M. W\u00f6llmer", "F. Eyben", "S. Reiter", "B. Schuller", "C. Cox", "E. Douglas-Cowie", "R. Cowie"], "venue": "in INTER- SPEECH,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Connectionist speech recognition: a hybrid approach", "author": ["H.A. Bourlard", "N. Morgan"], "venue": "Springer Science & Business Media,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Markovian models for sequential data", "author": ["Y. Bengio"], "venue": "Neural computing surveys, vol. 2, no. 1049, pp. 129\u2013 162, 1999.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["F.A. Gers", "N.N. Schraudolph", "J. Schmidhuber"], "venue": "The Journal of Machine Learning Research, vol. 3, pp. 115\u2013143, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "Signal Processing, IEEE Transactions on, vol. 45, no. 11, pp. 2673\u20132681, 1997.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "On supervised learning from sequential data with applications for speech recognition", "author": ["M. Schuster"], "venue": "Daktaro disertacija, Nara Institute of Science and Technology, 1999.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Srilm-an extensible language modeling toolkit.", "author": ["A. Stolcke"], "venue": "in INTERSPEECH,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Recently, automatic sequence classification became an ubiquitous problem, having then encountered a high research interest [1, 2, 3].", "startOffset": 123, "endOffset": 132}, {"referenceID": 1, "context": "Recently, automatic sequence classification became an ubiquitous problem, having then encountered a high research interest [1, 2, 3].", "startOffset": 123, "endOffset": 132}, {"referenceID": 2, "context": "Recently, automatic sequence classification became an ubiquitous problem, having then encountered a high research interest [1, 2, 3].", "startOffset": 123, "endOffset": 132}, {"referenceID": 3, "context": "Among the recent methods employed to structure these sequences, the machine learning domain provides a set of high-level representations well adapted to automatic sequence classification based on Deep Neural Networks (DNN) such as Convolutional Neural Networks (CNN) [4] or Recurrent Neural Networks (RNN) [5].", "startOffset": 267, "endOffset": 270}, {"referenceID": 4, "context": "Among the recent methods employed to structure these sequences, the machine learning domain provides a set of high-level representations well adapted to automatic sequence classification based on Deep Neural Networks (DNN) such as Convolutional Neural Networks (CNN) [4] or Recurrent Neural Networks (RNN) [5].", "startOffset": 306, "endOffset": 309}, {"referenceID": 5, "context": "RNN architectures such as Long Short-Term Memory (LSTM) [6] and Bidirectional LSTM (BLSTM) [7] have gained a particular attention in different domains and tasks including sentence [8] or successive images [9] processing.", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "RNN architectures such as Long Short-Term Memory (LSTM) [6] and Bidirectional LSTM (BLSTM) [7] have gained a particular attention in different domains and tasks including sentence [8] or successive images [9] processing.", "startOffset": 91, "endOffset": 94}, {"referenceID": 7, "context": "RNN architectures such as Long Short-Term Memory (LSTM) [6] and Bidirectional LSTM (BLSTM) [7] have gained a particular attention in different domains and tasks including sentence [8] or successive images [9] processing.", "startOffset": 180, "endOffset": 183}, {"referenceID": 8, "context": "RNN architectures such as Long Short-Term Memory (LSTM) [6] and Bidirectional LSTM (BLSTM) [7] have gained a particular attention in different domains and tasks including sentence [8] or successive images [9] processing.", "startOffset": 205, "endOffset": 208}, {"referenceID": 9, "context": "In speech recognition [10, 11, 12], these LSTM models exploit the contextual information whenever speech production or perception is influenced by emotion, strong accents, or background noise.", "startOffset": 22, "endOffset": 34}, {"referenceID": 10, "context": "In speech recognition [10, 11, 12], these LSTM models exploit the contextual information whenever speech production or perception is influenced by emotion, strong accents, or background noise.", "startOffset": 22, "endOffset": 34}, {"referenceID": 11, "context": "In speech recognition [10, 11, 12], these LSTM models exploit the contextual information whenever speech production or perception is influenced by emotion, strong accents, or background noise.", "startOffset": 22, "endOffset": 34}, {"referenceID": 12, "context": "The most effective use of RNNs for sequence classification is to combine the RNNs with Hidden Markov Models (HMMs) in a hybrid approach [13, 14].", "startOffset": 136, "endOffset": 144}, {"referenceID": 13, "context": "The most effective use of RNNs for sequence classification is to combine the RNNs with Hidden Markov Models (HMMs) in a hybrid approach [13, 14].", "startOffset": 136, "endOffset": 144}, {"referenceID": 5, "context": "Long Short-Term Memory (LSTM) [6] networks are a special case of Recurrent Neural Networks (RNNs) [5].", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "Long Short-Term Memory (LSTM) [6] networks are a special case of Recurrent Neural Networks (RNNs) [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "This internal state allows the RNN to process arbitrary sequences of inputs such as sequences of words [8] for language modeling, time series [1].", "startOffset": 103, "endOffset": 106}, {"referenceID": 0, "context": "This internal state allows the RNN to process arbitrary sequences of inputs such as sequences of words [8] for language modeling, time series [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 5, "context": "[6] shows that LSTM networks outperformRNNs for finding long range context and dependencies.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "The LSTM composite functionH forming the LSTM cell with peephole connections [15] is presented in Figure 1 and defined as:", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "Bidirectional RNN (BRNN) [16], presented in Figure 2, can process both directions with two separate hidden layers (one for each direction).", "startOffset": 25, "endOffset": 29}, {"referenceID": 6, "context": "By replacing the BRNN cells with LSTM cells, the Bidirectionnal LSTM (BLSTM) [7] is obtained.", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "[7] introduces the BLSTM with Back Propagation Trough Time (BPTT) algorithm [17] for training.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[7] introduces the BLSTM with Back Propagation Trough Time (BPTT) algorithm [17] for training.", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "Data from 2013 and 2014 are merged and split into the training (70%) and validation (30%) datasets using a stratified shuffle split [18] in order to preserve the same percentage of samples of each class in the output of both folds, while the 2015 dataset is kept for testing.", "startOffset": 132, "endOffset": 136}, {"referenceID": 18, "context": "Mono-channel input: only M6 history sequences for a baseline n-gram experiment (with a statistical language model from the SRILM toolkit [19]) and a straightforward LSTM model.", "startOffset": 137, "endOffset": 141}, {"referenceID": 19, "context": "The Keras library [20], based on Theano [21] for fast tensor manipulation and CUDAbased GPU acceleration, has been employed to train neural networks on an Nvidia GeForce GTX TITAN X GPU card.", "startOffset": 18, "endOffset": 22}, {"referenceID": 20, "context": "The Keras library [20], based on Theano [21] for fast tensor manipulation and CUDAbased GPU acceleration, has been employed to train neural networks on an Nvidia GeForce GTX TITAN X GPU card.", "startOffset": 40, "endOffset": 44}], "year": 2017, "abstractText": "Recently, machine learning methods have provided a broad spectrum of original and efficient algorithms based on Deep Neural Networks (DNN) to automatically predict an outcome with respect to a sequence of inputs. Recurrent hidden cells allow these DNN-based models to manage long-term dependencies such as Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM). Nevertheless, these RNNs process a single input stream in one (LSTM) or two (Bidirectional LSTM) directions. But most of the information available nowadays is from multistreams or multimedia documents, and require RNNs to process these information synchronously during the training. This paper presents an original LSTM-based architecture, named Parallel LSTM (PLSTM), that carries out multiple parallel synchronized input sequences in order to predict a common output. The proposed PLSTM method could be used for parallel sequence classification purposes. The PLSTM approach is evaluated on an automatic telecast genre sequences classification task and compared with different state-of-theart architectures. Results show that the proposed PLSTM method outperforms the baseline n-gram models as well as the state-of-the-art LSTM approach.", "creator": "LaTeX with hyperref package"}}}