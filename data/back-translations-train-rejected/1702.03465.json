{"id": "1702.03465", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2017", "title": "Enabling Robots to Communicate their Objectives", "abstract": "Our ultimate goal is to efficiently enable end-users to correctly anticipate a robot's behavior in novel situations. This behavior is often a direct result of the robot's underlying objective function. Our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then selecting those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives. We consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be \\emph{exact} in their IRL inference. We thus introduce two factors to define candidate approximate-inference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what the robot will do in test situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior.", "histories": [["v1", "Sat, 11 Feb 2017 22:39:39 GMT  (2932kb,D)", "http://arxiv.org/abs/1702.03465v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.LG", "authors": ["sandy h huang", "david held", "pieter abbeel", "anca d dragan"], "accepted": false, "id": "1702.03465"}, "pdf": {"name": "1702.03465.pdf", "metadata": {"source": "CRF", "title": "Enabling Robots to Communicate their Objectives", "authors": ["Sandy H. Huang", "David Held", "Pieter Abbeel", "Anca D. Dragan"], "emails": [], "sections": [{"heading": null, "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "II. ALGORITHMIC TEACHING OF OBJECTIVE FUNCTIONS", "text": "We model how humans derive the objective function of a robot from its behavior, so that we can use this model for generative, informative behavioral examples."}, {"heading": "A. Preliminaries", "text": "We assume that the objective (or reward) function of the robot is represented as a linear combination of characteristics 2, weighted by any \u03b8 * [1]: R\u03b8 * (st, at, st + 1; E) = \u03b8 * * (st, at, st + 1; E), (1) where st is the state at the time t, at is the action taken at the time t, and E is the description of the environment (or world). In the case of driving, E contains information about the lanes, trajectories of other cars and the starting state of the robot. Considering environment E, the parameters of the objective function determine the (optimal) trajectory of the robot."}, {"heading": "B. Algorithmic Teaching Framework", "text": "We model the human observer by starting with a previous P (\u03b8) on what a patient might be and actualizing his belief as he observes the action of the robot. The robot behaves optimally in relation to the target induced by him, but, as Fig. 1 shows, the details of the environment (e.g. the position of the car in the vicinity and the target of the robot) influence the behavior and therefore influence what effects the behavior has on the person's belief. To best utilize this effect, we look for a sequence of environments E1: n, so that when the person observes the optimal trajectories in these environments, their updated belief sets maximum probability on the correct trajectories, i.e. that they consider the maximum trajectories of the robot as probability: arg max E1: nP: n (3) In order to solve this optimization problem, the robot must model as examples that actualize the person's faith, P (2001)."}, {"heading": "C. Exact-Inference IRL as a Special Case", "text": "Inverse Reinforcement Learning (IRL) [16] extracts an objective function from observed behavior by assuming that the observed behavior optimizes a specific goal from a group of candidates. If this assumption is correct, IRL finds an objective function that assigns maximum reward (or minimum cost) to the observed behavior. Algorithmic teaching has been applied to precisely derived IRL learners [5]: The learner eliminates all objective functions that would not assign maximum reward to the observed behavior, which can be expressed by the model in (4) over a certain distribution for P. (5) This assumes that people assign probability 0 pathways that are not perfectly optimal in relation to the candidate, giving these candidates a zero probability."}, {"heading": "D. Approximate-Inference Models", "text": "We introduce a space of approach inference models that are observed by manipulation of two factors in a 2-by-3-factor design. (Deterministic vs. Probabilistic Effect) We introduce a space of approach inference models that are completely eliminated by manipulation of two factors in a 2-by-3-factor design. (Deterministic vs. Probabilistic Effect) We have shown that it is impossible (because they were not a global optimum for the objectively induced physical effect), or we have left it in the mix by rewarding it with the same probability as the other remaining phenomena. We envision two ways to relax the assumption that a person can perfectly identify whether a trajectory is given a global optimum, is a path for observed trajectories to either eliminate the premise or to keep it in the running trajectories, but to be more conservative about which it is eliminated. If this person is not perfect enough to eliminate the trajectories, then that person is not enough for this trajectway to eliminate the trajectories."}, {"heading": "E. Example Selection", "text": "Considering a learning model M that predicts PM, which is estimated by evenly scanning candidates E1: n, our approach greedily selects the environment Et to maximize this probability. We allow the model to select up to ten examples; it stops early if no additional example improves this probability. This greedy approach is nearly optimal for deterministic effects with a unified prediction, since in this case, maximizing PM (prediction of PM (prediction of E1: t) equals maximizing PM (prediction of PM). This function does not decrease because adding sample trajectories E1: t can only eliminate candidate paths, but cannot add."}, {"heading": "F. Hyperparameter Selection", "text": "We would like to select values for hyperparameters \u03c4 and \u03bb (for deterministic and probable effects respectively) that accurately model human learning in this domain. \u03c4 and \u03bb influence the informativeness of examples. Therefore, if \u03c4 is too large, most environments will be less informative, since the sample path will be within a certain distance of optimal trajectories of many phenomena, so that these phenomena will not be eliminated. Therefore, PM (\u03b8] E1: n) will be low. On the other hand, we expect humans to be teachable (i.e., that some environments cannot be too large) and that only one or a few examples will be selected before any further improvement in fine dust pollution can be achieved (\u03b8). We expect people to be teachable (i.e. that one or more examples cannot be too large) and to have approximate rather than exact conclusions (i.e., they cannot be too small), so that they would benefit from observing several examples instead of two examples."}, {"heading": "III. EXAMPLE DOMAIN", "text": "We evaluate how our proposed models will affect the driving style of a simulated autonomous car (we are always sinful).In this area, participants will experience examples (in the simulation) of how the car will drive, with the aim of being able to predict how it will drive when actually driving in it. We will model the dynamics of the car with the bicycle-vehicle model [23].Let the condition of a car represent either x = [x y] > the control input where (x, y) the coordinates of the rear axle of the car are, while the dynamics of the car is the heading of the car, v isits velocity, and \u03b1 is the steering angle. Let u = [uu2] > represent the control input where u1 is the change in the steering angle and u2 is the acceleration. Let L be the distance between the front rear and rear axles of the car. Then the dynamic model of the vehicle is [x]."}, {"heading": "IV. ANALYSIS OF APPROXIMATE INFERENCE MODELS WITH IDEAL USERS", "text": "In Sec. II-D, we have introduced six possible approximate user models M. They all model people as evaluation candidates based on the distance between the IRV orbit they observe and the optimal orbit in relation to \u03b8, but they differ in what the distance metric is, and whether they completely eliminate the candidates (deterministic effect) or gently rebalance them (probabilistic effect).Here, we examine how well algorithmic teaching works with these models for teaching with the ideal users. First, we generate a sequence of examples for each of our approximate models M by greedily maximizing PM values (neoliberal effect).We also examine the sequence for the exact inference model and include a random sequence, for a total of eight sequences. Types of selected examples. Fig. 3 summarizes the types of examples that each algorithm selects for its optimal sequence."}, {"heading": "V. USER STUDY", "text": "We now evaluate whether models with approximate conclusions are useful for real users and not for ideal users."}, {"heading": "A. Experiment Design", "text": "We manipulate variables. We manipulate whether the algorithmic teaching presupposes exact conclusions or approximate conclusions. For the approximate conclusion case, we manipulate two variables: the effect of the approximate conclusion (either deterministic or probabilistic) and the distance measurement (reward-based, euclidean-based or strategic-based), in a 2-by-3 factory design, for a total of six approximate follow-up models. For the strategy-based distance measurement, the nature of the effect does not matter, since distances are either 0 or \u221e, so there are five unique approximate inference models. We show the participant a training environment at a time, in the order that the examples were selected by each algorithm. Dependent measures. In the end, we are interested in how well human participants learn a specific setting of reward parameters, which we correct from the training examples. Since we cannot ask them to test a vaccine (we can write the vaccine or drive them ourselves, as we can)."}, {"heading": "B. Analysis", "text": "The number of examples. Different algorithms produce different numbers of examples. Exact inference IRL could produce as few as one example (and does so in our case). Approximate conference models produce more, and the random principle can produce an almost unlimited amount if it is allowed. Thus, one possible confrontation in our experiment is the number of examples. We checked if this is in fact a mix-up: do you help more examples? Surprisingly, we found no correlation between the number of examples and performance: the subject-Pearson correlation coefficient is r = 0.03 (Fig. 6, left). This suggests that sample quantity is less important than sample quality. We start our analysis by comparing the different models of approximate inference. We have measured a factorial ANOVA value at distance and determinism as factors (Fig. 5, left). We found a marginal effect for distance (F.)."}, {"heading": "VI. UTILITY OF ALGORITHMIC TEACHING", "text": "So far, we have tested our central hypothesis for this work that taking approximate IRL conclusions into account can actually improve the performance of algorithmic instruction in humans. Although this is very promising, we also wanted to test the usefulness of algorithmic instruction itself: whether this new algorithm is preferable not only to algorithmic instruction with exact conclusions, but to the robot that does not actively teach. Instead, the person must learn from the robot's behavior in environments he or she happens to encounter."}, {"heading": "A. Baselining Performance", "text": "The average age of the 28 unfiltered participants was 33.3 (SD = 9.6). The gender ratio was 0.46 female. Controlling for Confounds. There are several variables that could thwart this study. First, if we generate the random sequence, we could have very good luck or bad luck and produce a particularly informative or less informative one. To avoid this, we randomly perform 1000 random sequences with the desired number of examples and sort them based on PM. First, if we generate the random sequence, we could have very good luck or bad luck and produce a particularly informative one. To avoid this, we stitch 1000 random sequences with the desired number of examples and sort them based on PM."}, {"heading": "B. Coverage-Augmented Algorithmic Teaching", "text": "Since range correlates with better user performance, we add a congruent term to our trajectory optimization."}, {"heading": "C. User Study on Coverage", "text": "Next, we conduct a study to test the benefits of coverage. Manipulated variables. We manipulate two variables: whether we add coverage to the training examples and whether we use a user model to generate the examples or to sample evenly. We choose our best model for the former, ca. *. From the previous experiment, we have already obtained user performance data along the no-coverage dimension - for random and ca. * - so that we perform this experiment only for the two new conditions that include coverage, which we will call Coverage Random and Coverage. We generate random sequences with coverage by selecting exactly a random environment from each of the eight environmental and trajectory strategy classes. Dependent measures. We maintain the same dependent measures as in our previous Mechanical Turk Experiment (Sec. V-A) hypothesis. We hypothesize coverage by improving performance in both environments, tractor strategy and margins."}, {"heading": "VII. DISCUSSION", "text": "Abstract: We are taking a step toward communicating robotics with humans. We found that an approximation model that uses a deterministic euclidean update of candidate reward function parameters performs best in conveying real users and outperforms algorithmic teaching based on accurate conclusions. We also found that this model performs significantly better when expanded to include a coverage goal than when it passively familiarizes the user with the robot. Limitations and future work. Our results reveal the promise of an algorithmic conveyance of objective robot functions. Our model of human learning is far from perfect, and more work is needed to explore the process that humans use to extrapolate from observed robot behavior. In addition, in this work we focused on the physical behavior of the robot as a communication channel, because humans naturally derive benefit functions from it. Future work could complement this by other channels, such as visual explanations."}, {"heading": "VIII. ACKNOWLEDGMENTS", "text": "This research was partially funded by Intel Labs, an NSF CAREER Award (# 1351028), and the Berkeley Deep Drive Consortium. Sandy Huang was supported by an NSF Fellowship."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y. Ng"], "venue": "In Proceedings of the Twenty-First International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Recent developments in algorithmic teaching", "author": ["Frank J. Balbach", "Thomas Zeugmann"], "venue": "In Proceedings of the Third International Conference on Language and Automata Theory and Applications,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Teaching classification boundaries to humans", "author": ["Sumit Basu", "Janara Christensen"], "venue": "In Proceedings of the Twenty- Seventh AAAI Conference on Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Curriculum learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the Twenty-Sixth Annual International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Algorithmic and human teaching of sequential decision tasks", "author": ["Maya Cakmak", "Manuel Lopes"], "venue": "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Familiarization to robot motion", "author": ["Anca Dragan", "Siddhartha Srinivasa"], "venue": "In Proceedings of the Ninth International Conference on Human-Robot Interaction,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Effects of robot motion on humanrobot collaboration", "author": ["Anca Dragan", "Shira Bauman", "Jodi Forlizzi", "Siddhartha Srinivasa"], "venue": "In Proceedings of the Tenth International Conference on Human-Robot Interaction,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Generating human-like motion for robots", "author": ["Michael J. Gielniak", "C. Karen Liu", "Andrea L. Thomaz"], "venue": "International Journal of Robotics Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "On the complexity of teaching", "author": ["Sally A. Goldman", "Michael J. Kearns"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "The na\u0131\u0308ve utility calculus: Computational principles underlying commonsense psychology", "author": ["Julian Jara-Ettinger", "Hyowon Gwen", "Laura E. Schulz", "Joshua B. Tenenbaum"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "How do humans teach: On curriculum learning and teaching dimension", "author": ["Faisal Khan", "Bilge Mutlu", "Xiaojin Zhu"], "venue": "In Proceedings of the Twenty-Fourth Annual Conference on Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "New potentials for datadriven intelligent tutoring system development and optimization", "author": ["Kenneth R. Koedinger", "Emma Brunskill", "Ryan Shaun Joazeiro de Baker", "Elizabeth A. McLaughlin", "John C. Stamper"], "venue": "AI Magazine,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Continuous inverse optimal control with locally optimal examples", "author": ["Sergey Levine", "Vladlen Koltun"], "venue": "In Proceedings of the Twenty-Ninth International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Generating natural motion in an  android by mapping human motion", "author": ["Daisuke Matsui", "Takashi Minato", "Karl F. MacDorman", "Hiroshi Ishiguro"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "An analysis of approximations for maximizing submodular set functions", "author": ["G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher"], "venue": "Mathematical Programming,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1978}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Andrew Y. Ng", "Stuart Russell"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Optimal Teaching for Limited-Capacity Human Learners", "author": ["Kaustubh R Patil", "Xiaojin Zhu", "\u0141 ukasz Kope\u0107", "Bradley C Love"], "venue": "In Proceedings of the Twenty-Seventh Annual Conference on Neural Information Processing Systems", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Faster Teaching by POMDP Planning", "author": ["Anna N. Rafferty", "Emma Brunskill", "Thomas L. Griffiths", "Patrick Shafto"], "venue": "In Proceedings of the Fifteenth International Conference on Artificial Intelligence in Education,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["Stuart J. Russell", "Peter Norvig"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Information gathering actions over human internal state", "author": ["Dorsa Sadigh", "S. Shankar Sastry", "Sanjit A. Seshia", "Anca Dragan"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Joint action: Bodies and minds moving together", "author": ["Natalie Sebanz", "Harold Bekkering", "Gnther Knoblich"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Near-optimally teaching the crowd to classify", "author": ["Adish Singla", "Ilija Bogunovic", "Gabor Bartok", "Amin Karbasi", "Andreas Krause"], "venue": "In Proceedings of the Thirty-First International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Investigation of a combined slip control braking and closed loop four wheel steering system for an automobile during combined hard braking and severe steering", "author": ["Saied Taheri", "E. Harry Law"], "venue": "In Proceedings of the 1990 American Control Conference,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1990}, {"title": "One and done? Optimal decisions from very few samples", "author": ["Edward Vul", "Noah D. Goodman", "Thomas L. Griffiths", "Joshua B. Tenenbaum"], "venue": "Cognitive Science,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Machine teaching: An inverse problem to machine learning and an approach toward optimal education", "author": ["Xiaojin Zhu"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Brian D. Ziebart", "Andrew Maas", "J. Andrew Bagnell", "Anind Dey"], "venue": "In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Models of cooperative teaching and learning", "author": ["Sandra Zilles", "Steffen Lange", "Robert Holte", "Martin Zinkevich"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}], "referenceMentions": [{"referenceID": 5, "context": "There are many reasons why it is beneficial for humans to be able to anticipate a robot\u2019s movements, from subjective comfort [6] to ease of coordination when working with and around the robot [7, 21].", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "There are many reasons why it is beneficial for humans to be able to anticipate a robot\u2019s movements, from subjective comfort [6] to ease of coordination when working with and around the robot [7, 21].", "startOffset": 192, "endOffset": 199}, {"referenceID": 20, "context": "There are many reasons why it is beneficial for humans to be able to anticipate a robot\u2019s movements, from subjective comfort [6] to ease of coordination when working with and around the robot [7, 21].", "startOffset": 192, "endOffset": 199}, {"referenceID": 5, "context": "However, people have difficulty anticipating robot actions [6, 8, 14].", "startOffset": 59, "endOffset": 69}, {"referenceID": 7, "context": "However, people have difficulty anticipating robot actions [6, 8, 14].", "startOffset": 59, "endOffset": 69}, {"referenceID": 13, "context": "However, people have difficulty anticipating robot actions [6, 8, 14].", "startOffset": 59, "endOffset": 69}, {"referenceID": 18, "context": "A robot\u2019s behavior in any situation is a direct consequence of the objective (or reward) function the robot is optimizing: (most) robots are rational agents, acting to maximize expected cumulative reward [19].", "startOffset": 204, "endOffset": 208}, {"referenceID": 12, "context": "For instance, a car might trade off between features related to collision avoidance and efficiency [13], with more \u201caggressive\u201d P( \u03b8 | \u03be 1 )", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "cars prioritizing efficiency at the detriment of, say, distance to obstacles [20].", "startOffset": 77, "endOffset": 81}, {"referenceID": 9, "context": "People naturally make inferences about an agent\u2019s objective function when observing behavior [10], and get better over time at anticipating how robots will act [6].", "startOffset": 93, "endOffset": 97}, {"referenceID": 5, "context": "People naturally make inferences about an agent\u2019s objective function when observing behavior [10], and get better over time at anticipating how robots will act [6].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "In order to choose the most informative example behaviors for communicating a robot\u2019s objective function to humans, we take an algorithmic teaching approach [2, 9, 12, 18, 25, 27]: we model how humans make inferences about the robot\u2019s objective function from examples of its optimal behavior, to generate examples that increase the probability of the correct inferred objective function.", "startOffset": 157, "endOffset": 179}, {"referenceID": 8, "context": "In order to choose the most informative example behaviors for communicating a robot\u2019s objective function to humans, we take an algorithmic teaching approach [2, 9, 12, 18, 25, 27]: we model how humans make inferences about the robot\u2019s objective function from examples of its optimal behavior, to generate examples that increase the probability of the correct inferred objective function.", "startOffset": 157, "endOffset": 179}, {"referenceID": 11, "context": "In order to choose the most informative example behaviors for communicating a robot\u2019s objective function to humans, we take an algorithmic teaching approach [2, 9, 12, 18, 25, 27]: we model how humans make inferences about the robot\u2019s objective function from examples of its optimal behavior, to generate examples that increase the probability of the correct inferred objective function.", "startOffset": 157, "endOffset": 179}, {"referenceID": 17, "context": "In order to choose the most informative example behaviors for communicating a robot\u2019s objective function to humans, we take an algorithmic teaching approach [2, 9, 12, 18, 25, 27]: we model how humans make inferences about the robot\u2019s objective function from examples of its optimal behavior, to generate examples that increase the probability of the correct inferred objective function.", "startOffset": 157, "endOffset": 179}, {"referenceID": 24, "context": "In order to choose the most informative example behaviors for communicating a robot\u2019s objective function to humans, we take an algorithmic teaching approach [2, 9, 12, 18, 25, 27]: we model how humans make inferences about the robot\u2019s objective function from examples of its optimal behavior, to generate examples that increase the probability of the correct inferred objective function.", "startOffset": 157, "endOffset": 179}, {"referenceID": 26, "context": "In order to choose the most informative example behaviors for communicating a robot\u2019s objective function to humans, we take an algorithmic teaching approach [2, 9, 12, 18, 25, 27]: we model how humans make inferences about the robot\u2019s objective function from examples of its optimal behavior, to generate examples that increase the probability of the correct inferred objective function.", "startOffset": 157, "endOffset": 179}, {"referenceID": 15, "context": "The opposite problem, machines inferring objective functions from observed human behavior, can be solved using Inverse Reinforcement Learning (IRL) [16].", "startOffset": 148, "endOffset": 152}, {"referenceID": 4, "context": "Prior work has investigated how to teach an objective function through example behavior to machine learners running IRL [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 4, "context": "1 People do not have direct access to configuration-space trajectory and the exact environment state, whereas robots do, at least in kinesthetic teaching (and in [5]).", "startOffset": 162, "endOffset": 165}, {"referenceID": 23, "context": "People also cannot necessarily distinguish between a perfectly optimal trajectory for one objective and an ever-so-slightly suboptimal one [24].", "startOffset": 139, "endOffset": 143}, {"referenceID": 2, "context": "1Prior work has applied algorithmic teaching to teach humans, primarily for binary classification of images [3, 4, 11, 22].", "startOffset": 108, "endOffset": 122}, {"referenceID": 3, "context": "1Prior work has applied algorithmic teaching to teach humans, primarily for binary classification of images [3, 4, 11, 22].", "startOffset": 108, "endOffset": 122}, {"referenceID": 10, "context": "1Prior work has applied algorithmic teaching to teach humans, primarily for binary classification of images [3, 4, 11, 22].", "startOffset": 108, "endOffset": 122}, {"referenceID": 21, "context": "1Prior work has applied algorithmic teaching to teach humans, primarily for binary classification of images [3, 4, 11, 22].", "startOffset": 108, "endOffset": 122}, {"referenceID": 16, "context": "show accounting for human limitations (in their case limiting the number of recalled examples) improves teaching performance [17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 0, "context": "We assume the robot\u2019s objective (or reward) function is represented as a linear combination of features2 weighted by some \u03b8\u2217 [1]:", "startOffset": 125, "endOffset": 128}, {"referenceID": 15, "context": "Inverse Reinforcement Learning (IRL) [16] extracts an objective function from observed behavior by assuming that the observed behavior is optimizing some objective from a set of candidates.", "startOffset": 37, "endOffset": 41}, {"referenceID": 4, "context": "Algorithmic teaching has been used with exact-inference IRL learners [5]: the learner eliminates all objective functions which would not assign maximum reward to the observed behavior.", "startOffset": 69, "endOffset": 72}, {"referenceID": 21, "context": "5We noticed that normalizing this distribution produced very similar results to leaving it unnormalized, so that is what we do in our experiments, analogous to other algorithmic teaching work not based on reward functions [22].", "startOffset": 222, "endOffset": 226}, {"referenceID": 25, "context": "MaxEnt IRL [26] is an IRL algorithm that assumes demonstrations are noisy (i.", "startOffset": 11, "endOffset": 15}, {"referenceID": 14, "context": "This greedy approach is near-optimal for deterministic effect with a uniform prior, since in this case maximizing PM(\u03b8 \u2217|\u03be\u03b8 E1:t) is equivalent to maximizing \u2212 \u2211 \u03b8 PM(\u03b8|\u03be \u2217 E1:t ), which is a non-decreasing monotonic submodular function [15].", "startOffset": 237, "endOffset": 241}, {"referenceID": 22, "context": "We model the dynamics of the car with the bicycle vehicle model [23].", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "Final velocity, non-autonomous car (if acceleration time 6= 0) [20, 30, 70, 80]", "startOffset": 63, "endOffset": 79}], "year": 2017, "abstractText": "Our ultimate goal is to efficiently enable end-users to correctly anticipate a robot\u2019s behavior in novel situations. This behavior is often a direct result of the robot\u2019s underlying objective function. Our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then selecting those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives. We consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be exact in their IRL inference. We thus introduce two factors to define candidate approximateinference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what the robot will do in test situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior.", "creator": "LaTeX with hyperref package"}}}