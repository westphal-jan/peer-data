{"id": "1702.03470", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2017", "title": "Vector Embedding of Wikipedia Concepts and Entities", "abstract": "Using deep learning for different machine learning tasks such as image classification and word embedding has recently gained many attentions. Its appealing performance reported across specific Natural Language Processing (NLP) tasks in comparison with other approaches is the reason for its popularity. Word embedding is the task of mapping words or phrases to a low dimensional numerical vector. In this paper, we use deep learning to embed Wikipedia Concepts and Entities. The English version of Wikipedia contains more than five million pages, which suggest its capability to cover many English Entities, Phrases, and Concepts. Each Wikipedia page is considered as a concept. Some concepts correspond to entities, such as a person's name, an organization or a place. Contrary to word embedding, Wikipedia Concepts Embedding is not ambiguous, so there are different vectors for concepts with similar surface form but different mentions. We proposed several approaches and evaluated their performance based on Concept Analogy and Concept Similarity tasks. The results show that proposed approaches have the performance comparable and in some cases even higher than the state-of-the-art methods.", "histories": [["v1", "Sun, 12 Feb 2017 00:23:04 GMT  (22kb)", "http://arxiv.org/abs/1702.03470v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ehsan sherkat", "evangelos milios"], "accepted": false, "id": "1702.03470"}, "pdf": {"name": "1702.03470.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Evangelos Milios"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 2.03 470v 1 [cs.C L] 12 Feb 2017Keywords: Wikipedia, Concept Embedding, Vector Representation"}, {"heading": "1 Introduction", "text": "Recently, many researchers [10,13] demonstrated the capabilities of deep learning for tasks of natural language processing such as word embedding. Word embedding is the task of representing each term with a low-dimensional (typically less than 1000) numeric vector. Distributed representation of words showed a better performance than traditional approaches for tasks such as word analogy [10]. Some words are entities, i.e. the name of an organization, person, film, etc. On the other side, some terms and phrases have a page or a definition in a knowledge base such as Wikipedia, which are called concepts. For example, there is a page in Wikipedia for data mining or computer science concepts. Both concepts and entities are valuable resources to better understand semantic terms and a text. In this essay, we used deep learning to represent Wikipedia concepts and entities with numerical vectors. We make the following contributions: - Broad coverage of words and concepts: 1.7 million terms of this Wikipedia concepts, English concepts and German concepts-concepts"}, {"heading": "2 Related Works", "text": "Word2Vec and GloVe are two groundbreaking approaches to word embedding. Recently, other methods have been introduced that attempt to improve both the performance and quality of word embedding [3] by using multilingual correlation. A method based on Word2Vec is proposed by Mikolov et al. for word embedding. [10] In the first step, you will find the words that appear more frequently in 1 https: / / github.com / ehsansherkat / ConVeccollect as separate, and then replace them with a single token. Finally, the vector for phrases is learned in the same way as individual word embedding. One of the features of this approach is that both words and phrases are in the same vector space. Embedding methods [1] with Deep Neural Networks are similar to the objectives of this paper. Graph representation has been used for information management in many real world problems. The challenge of extracting this graph from deeper information is the use."}, {"heading": "3 Distributed Representation of Concepts", "text": "In fact, it is such that it is a matter of a way in which it is about a way in which people live in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in"}, {"heading": "4 Evaluation", "text": "It is a question of whether the people in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the UK, in the USA, in the USA, in the USA, in the United Kingdom, in the USA, in the USA, in the USA, in the USA, in the United Kingdom, in the USA, in the USA, in the USA, in the USA, in the USA, in the United Kingdom, in the USA, in the USA, in the USA, in the United Kingdom, in the USA, in the USA, in the USA, in the United Kingdom, in the USA, in the USA, in the United Kingdom, in the USA, in the USA, in the United Kingdom, in the USA, in the USA, in the United Kingdom, in the USA, in the United Kingdom, in the USA, in the USA, in the United Kingdom, in the United Kingdom, in the USA, in the United Kingdom, in the United Kingdom, in the USA, in the United Kingdom, in the United Kingdom, in the USA, in the United Kingdom, in the United Kingdom, in the USA, in the United Kingdom, in the USA, in the USA, in the United Kingdom, in the USA, in the USA, in the United Kingdom, in the USA, in the USA, in the United Kingdom, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the"}, {"heading": "5 Conclusion", "text": "This paper presents various approaches to embedding Wikipedia concepts. Although the proposed approaches are only used between Wikipedia links (anchors), they have a performance that is as good or even higher than the state of the art for concept analogy and concept similarity tasks. Unlike word embedding, Wikipedia concept embedding is not ambiguous, so there is a different vector for concepts with similar user interface shapes but different mentions. This feature is important for many NLP tasks such as Named Entity Recognition, Text Similarity and Document Clustering or Classification. In the future, we plan to use multiple resources such as information boxes, multilingual version of a Wikipedia page, categories and syntactical features of a page to improve the quality of Wikipedia concepts."}, {"heading": "Appendix A: Python Libraries", "text": "The following libraries are used to extract and process the Wikipedia corpus: - Wikiextractor: www.github.com / attardi / wikiextractor - Mwparserfromhell: www.github.com / earwig / mwparserfromhell - Wikipedia 1.4.0: www.pypi.python.org / pypi / wikipediaThe following libraries are used to implement and evaluate Word2Vec and Doc2Vec: - Gensim: www.pypi.python.org / pypi / gensim - Eval-word-vectors [2]: www.github.com / mfaruqui / eval-word-vectors"}, {"heading": "Appendix B: Pruning Wikipedia Pages", "text": "List of rules used to crop useless pages from the Wikipedia corpus: - Part have < ns0: redirect > tag in their XML file. - There is \"Category:\" in the first part of the page name. - There is \"File:\" in the first part of the page name. - There is \"Template:\" in the first part of the page name. - Anchor with \"(clarification)\" in the first part of the page name. - There is \"Anchor with\" in the first part of the page name. - There is \"List of\" in the first part of the page name. - There is \"Portal:\" in the first part of the page name. - There is \"Draft:\" in the first part of the page name. - There is \"MediaWiki:\" in the first part of the page name. - There is \"Help:\" in the first part of the page name. - There is \"Wikipedia: in the first part of the page name.\""}], "references": [{"title": "Deep neural networks for learning graph representations", "author": ["S. Cao", "W. Lu", "Q. Xu"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Community evaluation and exchange of word vectors at wordvectors.org", "author": ["M. Faruqui", "C. Dyer"], "venue": "In Proceedings of ACL: System Demonstrations,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["M. Faruqui", "C. Dyer"], "venue": "Association for Computational Linguistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "In Proceedings of the 10th International Conference on World Wide Web, WWW", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Large-scale learning of word relatedness with constraints", "author": ["G. Halawi", "G. Dror", "E. Gabrilovich", "Y. Koren"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["F. Hill", "R. Reichart", "A. Korhonen"], "venue": "Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Authoritative sources in a hyperlinked environment", "author": ["J.M. Kleinberg"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["T. Luong", "R. Socher", "C.D. Manning"], "venue": "In CoNLL,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["G.A. Miller", "W.G. Charles"], "venue": "Language and cognitive processes,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1991}, {"title": "Learning to link with wikipedia", "author": ["D. Milne", "I.H. Witten"], "venue": "In Proceedings of the 17th ACM Conference on Information and Knowledge Management,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Hubs in space: Popular nearest neighbors in high-dimensional data", "author": ["M. Radovanovi\u0107", "A. Nanopoulos", "M. Ivanovi\u0107"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Domain-specific semantic relatedness from wikipedia structure: A case study in biomedical text", "author": ["A. Sajadi", "E. Milios", "V. Ke\u0161elj", "J.C. Janssen"], "venue": "In International Conference on Intelligent Text Processing and Computational Linguistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Cross-lingual wikification using multilingual embeddings", "author": ["C.-T. Tsai", "D. Roth"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Recently, many researchers [10,13] showed the capabilities of deep learning for natural language processing tasks such as word embedding.", "startOffset": 27, "endOffset": 34}, {"referenceID": 12, "context": "Recently, many researchers [10,13] showed the capabilities of deep learning for natural language processing tasks such as word embedding.", "startOffset": 27, "endOffset": 34}, {"referenceID": 9, "context": "Distributed representation of words showed better performance than traditional approaches for tasks such as word analogy [10].", "startOffset": 121, "endOffset": 125}, {"referenceID": 9, "context": "For example, top nine similar terms to \u2019Amazon\u2019 based on pre-trained Google\u2019s vectors in Word2Vec [10] and GloVe [13] models are in Table 1.", "startOffset": 98, "endOffset": 102}, {"referenceID": 12, "context": "For example, top nine similar terms to \u2019Amazon\u2019 based on pre-trained Google\u2019s vectors in Word2Vec [10] and GloVe [13] models are in Table 1.", "startOffset": 113, "endOffset": 117}, {"referenceID": 2, "context": "Recently, other methods have been introduced that try to both improve the performance and quality of the word embedding [3] by using multilingual correlation.", "startOffset": 120, "endOffset": 123}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Graph embedding methods [1] using Deep Neural Networks are similar to the goals of this paper.", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "A uniform sampling method named as Truncated Random Walk was presented in [14].", "startOffset": 74, "endOffset": 78}, {"referenceID": 6, "context": "A graph embedding method for Wikipedia using a similarity inspired by the HITS algorithm [7] was presented by Sajadi et al.", "startOffset": 89, "endOffset": 92}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "A Wikipedia concept similarity index based on in-links and out-links of a page was proposed by Milne and Witten [12].", "startOffset": 112, "endOffset": 116}, {"referenceID": 16, "context": "The idea of using Anchor texts inside Wikipedia for learning phrase vectors is being used in some other researches [17] as well.", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "ConVec: The Wikipedia dataset obtained as a result of previous steps was used for training a Skip-grammodel [10] with negative sampling instead of hierarchical softmax.", "startOffset": 108, "endOffset": 112}, {"referenceID": 9, "context": "Skip-gram has been shown to give a better result in comparison to the Bag of Words (CBOW) model [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "In this case, we tried to fine-tune the vectors with Glove 6B dataset trained on Wikipedia and Gigaword datasets [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 7, "context": "[8] for Document embedding.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Phrase Analogy Task: To evaluate the quality of the Concept vectors, we used the phrase analogy dataset in [10] which contains 3,218 questions.", "startOffset": 107, "endOffset": 111}, {"referenceID": 3, "context": "1 WS-REL [4] 251 114 0.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "5566 2 SIMLEX [6] 961 513 0.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "2152 3 WS-SIM [4] 200 83 0.", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "6101 4 RW [9] 1182 874 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "2161 5 WS-ALL [4] 349 142 0.", "startOffset": 14, "endOffset": 17}, {"referenceID": 14, "context": "5945 6 RG [15] 62 35 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "5894 7 MC [11] 28 15 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "4706 8 MTurk [5] 283 155 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "2503 3 WS-MAN [4] 224 0.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "6554 4 WS-411 [4] 314 0.", "startOffset": 14, "endOffset": 17}, {"referenceID": 11, "context": "Wikipedia Miner [12] is a well-known approach to find the similarity between two Wikipedia pages based on their input and output links.", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "We also compared the results with another structural based similarity approach called HitSim [16].", "startOffset": 93, "endOffset": 97}], "year": 2017, "abstractText": "Using deep learning for different machine learning tasks such as image classification and word embedding has recently gained many attentions. Its appealing performance reported across specific Natural Language Processing (NLP) tasks in comparison with other approaches is the reason for its popularity. Word embedding is the task of mapping words or phrases to a low dimensional numerical vector. In this paper, we use deep learning to embed Wikipedia Concepts and Entities. The English version of Wikipedia contains more than five million pages, which suggest its capability to cover many English Entities, Phrases, and Concepts. Each Wikipedia page is considered as a concept. Some concepts correspond to entities, such as a person\u2019s name, an organization or a place. Contrary to word embedding, Wikipedia Concepts Embedding is not ambiguous, so there are different vectors for concepts with similar surface form but different mentions. We proposed several approaches and evaluated their performance based on Concept Analogy and Concept Similarity tasks. The results show that proposed approaches have the performance comparable and in some cases even higher than the state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}