{"id": "1702.03500", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2017", "title": "Concept Drift Adaptation by Exploiting Historical Knowledge", "abstract": "Incremental learning with concept drift has often been tackled by ensemble methods, where models built in the past can be re-trained to attain new models for the current data. Two design questions need to be addressed in developing ensemble methods for incremental learning with concept drift, i.e., which historical (i.e., previously trained) models should be preserved and how to utilize them. A novel ensemble learning method, namely Diversity and Transfer based Ensemble Learning (DTEL), is proposed in this paper. Given newly arrived data, DTEL uses each preserved historical model as an initial model and further trains it with the new data via transfer learning. Furthermore, DTEL preserves a diverse set of historical models, rather than a set of historical models that are merely accurate in terms of classification accuracy. Empirical studies on 15 synthetic data streams and 4 real-world data streams (all with concept drifts) demonstrate that DTEL can handle concept drift more effectively than 4 other state-of-the-art methods.", "histories": [["v1", "Sun, 12 Feb 2017 07:35:49 GMT  (413kb,D)", "http://arxiv.org/abs/1702.03500v1", "First version"]], "COMMENTS": "First version", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yu sun", "ke tang", "zexuan zhu", "xin yao"], "accepted": false, "id": "1702.03500"}, "pdf": {"name": "1702.03500.pdf", "metadata": {"source": "CRF", "title": "Concept Drift Adaptation by Exploiting Historical Knowledge", "authors": ["Yu Sun", "Zexuan Zhu", "Xin Yao"], "emails": ["sunyu123@mail.ustc.edu.cn;", "ketang@ustc.edu.cn.", "zhuzx@szu.edu.cn.", "X.Yao@cs.bham.ac.uk."], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Basic Concepts and Notations", "text": "In incremental learning, at each step t a data block Dt = {(xt1, yt1), (xt2, yt2), \u00b7 \u00b7, (xtn, ytn)} is received, which is generated from the distribution pt (x, y), where xti is a vector ofar Xiv: 170 2.03 500v 1 [cs.L G] 12 Feb 20172 attribute values and yti is a class name. Conceptual drift can therefore be defined as a change in the underlying distribution, e.g. pt (x, y) 6 = pt \u2212 1 (x, y). It should be noted that a specific case of a data block is a single data example, more commonly referred to as online learning [10]. At each step t, the learning objective of incremental learning is similar to that of incremental learning, i.e. to obtain a good model Ft for pt (x, y), which is as a sequence f \u00b7 f \u00b7 x, a natural learning value (x), y (x), a (1), y (x)."}, {"heading": "B. Concept Drift Handling Techniques", "text": "In this context, it should be noted that this project is a project which is, first and foremost, a project."}, {"heading": "III. THE PROPOSED APPROACH", "text": "The existing ensemble methods for incremental learning, as discussed in Section II, share a common weakness. This means that when a new block of data arrives, all approaches use the obtained historical models without adapting them to the new training data."}, {"heading": "A. A Diversity-based Model Preservation", "text": "The selection of historical models to maintain the DTEL framework can be seen as a general problem of the selection of historical models from M, where selected models are used as initial models, which are further refined with a new incoming data block (possibly generated by a deviated concept) and combined into an overall ensemble. In this context, it is the performance of the adapted models after 4 further training sessions that counts, and not the performance of the original models on the new incoming data block. It is acknowledged in the ensemble learning literature [20], [21] that in a suitable combination scheme, diversity between the individual models is crucial. Diversity between individual models should be promoted, which could be implemented through diverse educational data, different starting models, different learning algorithms [20] diversity could play an even more important role in DTEL in the event of concept shifts. Specifically, without prior knowledge of the relationships between different models, which are not trivial in order to know in practice, a historical model could adapt well to a new data scheme."}, {"heading": "B. Adapting Historical Models through Knowledge Transfer", "text": "Since different learning machines have different learning mechanisms, adapting a historical model to a new data block is a model-dependent problem. DTEL uses decision trees as a basis for learning. A knowledge transfer method was developed to adapt a previously trained decision tree to a new data block. Remember that the process of growing a decision tree gradually divides the attribute space into small regions. Each region corresponds to a leaf node of the tree and is given a class name. The structure of the tree inherently contains knowledge from previous subtasks, while the class names associated with the received regions determine the decision boundary. Therefore, the proposed knowledge transfer method aims to preserve the structure of a historical decision tree while adapting to the new data block. To be precise, this is done in two steps, as the following describes in detail. Step 1: Place all examples in the new data stem, the Dt, in the leaf nodes, and replace the class markings accordingly."}, {"heading": "C. Detailed Steps of DTEL", "text": "Given the two key components described in Sections IIIA and III-B, the detailed steps of the DTEL are classified for each tree. (Suppose the pieces of data are represented in Algorithm 2.) Suppose the pieces of data are classified in Algorithm 2. (Suppose the pieces of data D1 \u00b7 \u00b7 \u00b7 Dt arrive sequentially.) Classification and the Regression Tree (CART) [25] are used as the base learning tree in DTEL. (DTEL first builds a decision tree called f1, with the first piece of data and the new tree combined in an archive f1. Then, when a new data block, say Dt, arrives, the obtained decision tree (s) is adapted to Dt and a new decision tree is built from scratch Dt. The adapted trees and the new tree are combined to form the final ensemble for the time step t. Meanwhile, the new tree is used to update the archive of the choice scheme kept under the historical model II."}, {"heading": "IV. EXPERIMENT", "text": "Empirical studies have been carried out to assess the performance of DTEL. Different types of concept drift are involved, and state-of-the-art algorithms are compared in the experiment.The algorithms are mainly assessed under three aspects, i.e. classification performance for each chunk, total performance for an entire data stream and time efficiency. These approaches use different methods for concept drift adaptation. SEA, Learn + +.NSE and AUE2 are ensemble methods to exploit historical knowledge, while TIX uses historical knowledge by introducing it as new features in a transfer mode. SEA and Learn + +.NSE use different methods for concept drift adaptation. SEA, Learn +.NSE use the historical model directly in an ensemble, while AUE2 updates each historical model with the new chunk of data."}, {"heading": "A. Comparison on Synthetic Data", "text": "In order to comprehensively investigate the performance of the DTEL, five types of concept drift were tested in the experiment (b = 2.5). When working with synthetic data, we know exactly what the type of concept drift is and how dramatically the data distribution changes. Therefore, it is important to use the synthetic data for a detailed analysis of the approaches of concept drift adaptation. \u2022 SEA mobile hyperplane concepts (SEA) [6] involve 3 features with a value between 0 and 10. Only two features (i.e. x1 and x2) are relevant, and x3 is a noisy feature with a random value. The class designation of the data in this concept is simulated by the axis 1 + bx2 \u2264 / > conceptual class to simulate the value of the changes during the learning process."}, {"heading": "B. Comparison on Real-world Data", "text": "In fact, it is the case that you will be able to put yourself at the top without being able to do what you want to do in order to do it."}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "This paper presents a new approach to ensemble learning, namely DTEL, for incremental learning with concept drift. DTEL uses a diversity-based selection criterion to obtain models that have already been trained. Instead of being applied directly to an ensemble for the current concept, the obtained models are further adapted to the current concept through transfer learning. Empirical studies on both synthetic and real data streams show the advantages of DTEL over a number of state-of-the-art incremental learning methods. The biggest potential drawback of DTEL is that it is more expensive in terms of computation than the comparative methods. Although this disadvantage could be mitigated by parallel implementation of DTEL, as it can be paralleled naturally, it is still worth investigating other methods to reduce the complexity of DTEL. For example, it may be unnecessary to train all the models that have been obtained. Alternatively, some heuristic rules may be designed to identify the ones that are most likely to be used in school models."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank..."}], "references": [{"title": "Interpretability of sudden concept drift in medical informatics domain", "author": ["G. Stiglic", "P. Kokol"], "venue": "2011 IEEE 11th International Conference on Data Mining Workshops, Dec 2011, pp. 609\u2013613.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Concept drift-oriented adaptive and dynamic support vector machine ensemble with time window in corporate financial risk prediction", "author": ["J. Sun", "H. Li", "H. Adeli"], "venue": "IEEE Trans. Syst., Man, and Cybern.: Syst.,, vol. 43, pp. 801\u2013813, July 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Online ensemble learning of data streams with gradually evolved classes", "author": ["Y. Sun", "K. Tang", "L.L. Minku", "S. Wang", "X. Yao"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 28, no. 6, pp. 1532\u2013 1545, June 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Learn++: an incremental learning algorithm for supervised neural networks", "author": ["R. Polikar", "L. Upda", "S. Upda", "V. Honavar"], "venue": "IEEE Trans. Syst., Man, and Cybern. C, vol. 31, no. 4, pp. 497\u2013508, Nov 2001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Incremental learning from stream data", "author": ["H. He", "S. Chen", "K. Li", "X. Xu"], "venue": "IEEE Trans. Neural Netw., vol. 22, no. 12, pp. 1901\u20131914, Dec 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1901}, {"title": "A streaming ensemble algorithm (sea) for large-scale classification", "author": ["W.N. Street", "Y. Kim"], "venue": "KDD. New York, NY, USA: ACM, 2001, pp. 377\u2013382.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Reacting to different types of concept drift: The accuracy updated ensemble algorithm", "author": ["D. Brzezinski", "J. Stefanowski"], "venue": "IEEE Trans. Neural Netw. Learning Syst., vol. 25, no. 1, pp. 81\u201394, Jan 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey on concept drift adaptation", "author": ["J. a. Gama", "I. \u017dliobait\u0117", "A. Bifet", "M. Pechenizkiy", "A. Bouchachia"], "venue": "ACM Comput. Surv., vol. 46, no. 4, pp. 44:1\u201344:37, Mar. 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "An overview of concept drift applications", "author": ["I. \u017dliobait\u0117", "M. Pechenizkiy", "J. a. Gama"], "venue": "Big Data Analysis: New Algorithms for a New Society, ser. Studies in Big Data, N. Japkowicz and J. Stefanowski, Eds. Springer International Publishing, 2016, vol. 16, pp. 91\u2013114.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Resampling-based ensemble methods for online class imbalance learning", "author": ["S. Wang", "L.L. Minku", "X. Yao"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 27, no. 5, pp. 1356\u20131368, May 2015.  11", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning in the presence of concept drift and hidden contexts", "author": ["G. Widmer", "M. Kubat"], "venue": "Machine Learning, vol. 23, no. 1, pp. 69\u2013101, 1996.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Mining time-changing data streams", "author": ["G. Hulten", "L. Spencer", "P. Domingos"], "venue": "KDD. New York, NY, USA: ACM, 2001, pp. 97\u2013106.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning from time-changing data with adaptive windowing", "author": ["R.G. Albert Bifet"], "venue": "SDM, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning with drift detection", "author": ["J. a. Gama", "P. Medas", "G. Castillo", "P. Rodrigues"], "venue": "SBIA 2004, ser. LNCS. Springer Berlin Heidelberg, 2004, vol. 3171, pp. 286\u2013295.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Early drift detection method", "author": ["A. Bifet"], "venue": "Fourth Int\u2019l Workshop on Knowledge Discovery from Data Streams, vol. 6, 2006, pp. 77\u201386.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Ddd: A new ensemble approach for dealing with concept drift", "author": ["L. Minku", "X. Yao"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 24, no. 4, pp. 619\u2013633, April 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Incremental learning of concept drift in nonstationary environments", "author": ["R. Elwell", "R. Polikar"], "venue": "IEEE Trans. Neural Netw., vol. 22, no. 10, pp. 1517\u20131531, Oct 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Tackling concept drift by temporal inductive transfer", "author": ["G. Forman"], "venue": "SIGIR. New York, NY, USA: ACM, 2006, pp. 252\u2013259.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Dynamic integration of classifiers for handling concept drift", "author": ["A. Tsymbal", "M. Pechenizkiy", "P. Cunningham", "S. Puuronen"], "venue": "Information Fusion, vol. 9, no. 1, pp. 56 \u2013 68, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Diversity creation methods: a survey and categorisation", "author": ["G. Brown", "J. Wyatt", "R. Harris", "X. Yao"], "venue": "Information Fusion, vol. 6, no. 1, pp. 5 \u2013 20, 2005, diversity in Multiple Classifier Systems.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "An analysis of diversity measures", "author": ["E.K. Tang", "P.N. Suganthan", "X. Yao"], "venue": "Machine Learning, vol. 65, no. 1, pp. 247\u2013271, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345\u2013 1359, Oct 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "On the association of attributes in statistics: With illustrations from the material of the childhood society, &c", "author": ["G.U. Yule"], "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, vol. 194, no. 252-261, pp. 257\u2013319, 1900.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1900}, {"title": "Classification and regression trees", "author": ["L. Breiman", "J. Friedman", "C.J. Stone", "R.A. Olshen"], "venue": "Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1984}, {"title": "Mining high-speed data streams", "author": ["P. Domingos", "G. Hulten"], "venue": "KDD. New York, NY, USA: ACM, 2000, pp. 71\u201380.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "The impact of diversity on online ensemble learning in the presence of concept drift", "author": ["L. Minku", "A. White", "X. Yao"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 22, no. 5, pp. 730\u2013742, May 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Incremental learning from noisy data", "author": ["J.C. Schlimmer", "R.H. Granger", "Jr."], "venue": "Mach. Learn., vol. 1, no. 3, pp. 317\u2013354, Mar. 1986. [Online]. Available: http://dx.doi.org/10.1023/A:1022810614389", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1986}, {"title": "UCI machine learning repository", "author": ["K. Bache", "M. Lichman"], "venue": "2013. [Online]. Available: http://archive.ics.uci.edu/ml", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Dem\u0161ar"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 1\u201330, Dec. 2006.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": ", medical informatics [1], financial data analysis [2], social networks [3], et al.", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": ", medical informatics [1], financial data analysis [2], social networks [3], et al.", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": ", medical informatics [1], financial data analysis [2], social networks [3], et al.", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "In particular, the learning machines should be updated without access to previous data, such that there is no need to store or re-process the previous data [4], [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 4, "context": "In particular, the learning machines should be updated without access to previous data, such that there is no need to store or re-process the previous data [4], [5].", "startOffset": 161, "endOffset": 164}, {"referenceID": 3, "context": "Ensemble methods [4], [6] offer a natural approach to", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "Ensemble methods [4], [6] offer a natural approach to", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "It can be observed from the literature [7] that ensemble methods have been used frequently in many advanced incremental learning algorithms and have achieved great successes.", "startOffset": 39, "endOffset": 42}, {"referenceID": 7, "context": "This phenomenon, referred to as concept drift, is one of the key challenges that incremental learning approaches [8] [9], including those based on ensembles, need to deal with.", "startOffset": 113, "endOffset": 116}, {"referenceID": 8, "context": "This phenomenon, referred to as concept drift, is one of the key challenges that incremental learning approaches [8] [9], including those based on ensembles, need to deal with.", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "It should be noted that a special case of a data chunk is a single data example, which is more often referred to as online learning [10].", "startOffset": 132, "endOffset": 136}, {"referenceID": 10, "context": "The sliding window methods [11], [12], [13], which are mainly applied in the online learning scenario, preserve part of the most recently arrived data and update the current model with both the preserved data and the newly arrived training example.", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": "The sliding window methods [11], [12], [13], which are mainly applied in the online learning scenario, preserve part of the most recently arrived data and update the current model with both the preserved data and the newly arrived training example.", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "The sliding window methods [11], [12], [13], which are mainly applied in the online learning scenario, preserve part of the most recently arrived data and update the current model with both the preserved data and the newly arrived training example.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "Some other methods [14], [15], [16] explicitly involve a concept drift detection module in the learning algorithm.", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "Some other methods [14], [15], [16] explicitly involve a concept drift detection module in the learning algorithm.", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "Some other methods [14], [15], [16] explicitly involve a concept drift detection module in the learning algorithm.", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "Otherwise, the current model, which may be either a single learner [14] or an ensemble [16], is discarded and a new model is built from scratch.", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "Otherwise, the current model, which may be either a single learner [14] or an ensemble [16], is discarded and a new model is built from scratch.", "startOffset": 87, "endOffset": 91}, {"referenceID": 5, "context": "For such reasons, ensemble methods, which preserve historical models, are gaining more popularity in recent years [6], [7], [17].", "startOffset": 114, "endOffset": 117}, {"referenceID": 6, "context": "For such reasons, ensemble methods, which preserve historical models, are gaining more popularity in recent years [6], [7], [17].", "startOffset": 119, "endOffset": 122}, {"referenceID": 16, "context": "For such reasons, ensemble methods, which preserve historical models, are gaining more popularity in recent years [6], [7], [17].", "startOffset": 124, "endOffset": 128}, {"referenceID": 5, "context": "Typical examples of such approaches include the Streaming Ensemble Algorithm (SEA) [6], the Temporal Inductive Transfer (TIX) approach [18], the Dynamic Integration", "startOffset": 83, "endOffset": 86}, {"referenceID": 17, "context": "Typical examples of such approaches include the Streaming Ensemble Algorithm (SEA) [6], the Temporal Inductive Transfer (TIX) approach [18], the Dynamic Integration", "startOffset": 135, "endOffset": 139}, {"referenceID": 18, "context": "of Classifiers (DIC) approach [19], the Learn++ algorithm [4] in Non-Stationary Environments (Learn++.", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "of Classifiers (DIC) approach [19], the Learn++ algorithm [4] in Non-Stationary Environments (Learn++.", "startOffset": 58, "endOffset": 61}, {"referenceID": 16, "context": "NSE [17]) and Accuracy Updated Ensemble (AUE2) [7].", "startOffset": 4, "endOffset": 8}, {"referenceID": 6, "context": "NSE [17]) and Accuracy Updated Ensemble (AUE2) [7].", "startOffset": 47, "endOffset": 50}, {"referenceID": 15, "context": "Although the idea of ensembles is also adopted in the Diversity for Dealing with Drifts (DDD) method [16], we distinguish it from the above-mentioned ensemble methods as DDD does not preserve historical models and the ensembles used in that context could be regarded as a single model for time step t.", "startOffset": 101, "endOffset": 105}, {"referenceID": 19, "context": "None of the existing ensemble methods for incremental learning has considered ensemble diversity explicitly, although diversity has been shown to play a crucial role in ensembles [20], [21].", "startOffset": 179, "endOffset": 183}, {"referenceID": 20, "context": "None of the existing ensemble methods for incremental learning has considered ensemble diversity explicitly, although diversity has been shown to play a crucial role in ensembles [20], [21].", "startOffset": 185, "endOffset": 189}, {"referenceID": 21, "context": "However, viewing concepts C1 and C2 from the same incremental learning task as the source and target domains of transfer learning [22], it is reasonable to assume that C1 and C2 are correlated with each other.", "startOffset": 130, "endOffset": 134}, {"referenceID": 19, "context": "It is well acknowledged in the ensemble learning literature [20], [21] that, with an appropriate combination scheme, diversity among individual models are essential.", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "It is well acknowledged in the ensemble learning literature [20], [21] that, with an appropriate combination scheme, diversity among individual models are essential.", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": "Diversity between individual models should be encouraged, which could be implemented by diverse training data, diverse initial models, different learning algorithms [20].", "startOffset": 165, "endOffset": 169}, {"referenceID": 20, "context": "In general, any diversity measure [21] proposed for ensemble learning could be used for this purpose.", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "In this work, the Yules Q-statistic [23] is employed since it is one of the most popular diversity measures in the literature.", "startOffset": 36, "endOffset": 40}, {"referenceID": 23, "context": "The classification and regression tree (CART) [25], is employed as the base learner in DTEL.", "startOffset": 46, "endOffset": 50}, {"referenceID": 6, "context": "Among the combination schemes described in Section II, the weighted voting scheme used by AUE2 is employed because AUE2 showed the best overall performance among ensemble methods for incremental learning [7].", "startOffset": 204, "endOffset": 207}, {"referenceID": 5, "context": "The following representative algorithms are compared in the experiments: SEA [6], Learn++.", "startOffset": 77, "endOffset": 80}, {"referenceID": 16, "context": "NSE [17], AUE2 [7], and TIX [18].", "startOffset": 4, "endOffset": 8}, {"referenceID": 6, "context": "NSE [17], AUE2 [7], and TIX [18].", "startOffset": 15, "endOffset": 18}, {"referenceID": 17, "context": "NSE [17], AUE2 [7], and TIX [18].", "startOffset": 28, "endOffset": 32}, {"referenceID": 23, "context": "Specifically, the traditional decision tree method CART [25] is applied in SEA, Learn++.", "startOffset": 56, "endOffset": 60}, {"referenceID": 24, "context": "Since AUE2 needs to use an on-line model as the base learner, Hoeffding tree [26], an online decision tree method, was applied.", "startOffset": 77, "endOffset": 81}, {"referenceID": 5, "context": "According to the suggestion in [6], the ensemble size is set to 25 for the compared algorithms, unless mentioned otherwise.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "Based on the previous research [6], [7], [14], [17], [27], [28], five widely used synthetic concept drifts are employed in our experiment, described as follows.", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "Based on the previous research [6], [7], [14], [17], [27], [28], five widely used synthetic concept drifts are employed in our experiment, described as follows.", "startOffset": 36, "endOffset": 39}, {"referenceID": 13, "context": "Based on the previous research [6], [7], [14], [17], [27], [28], five widely used synthetic concept drifts are employed in our experiment, described as follows.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "Based on the previous research [6], [7], [14], [17], [27], [28], five widely used synthetic concept drifts are employed in our experiment, described as follows.", "startOffset": 47, "endOffset": 51}, {"referenceID": 25, "context": "Based on the previous research [6], [7], [14], [17], [27], [28], five widely used synthetic concept drifts are employed in our experiment, described as follows.", "startOffset": 53, "endOffset": 57}, {"referenceID": 26, "context": "Based on the previous research [6], [7], [14], [17], [27], [28], five widely used synthetic concept drifts are employed in our experiment, described as follows.", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "\u2022 SEA moving hyperplane concepts (SEA) [6] involves 3 features with a value between 0 and 10.", "startOffset": 39, "endOffset": 42}, {"referenceID": 6, "context": "\u2022 Rotating concepts (ROT) [7], [17] rotates the decision boundary or data points to simulate the change of data distribution.", "startOffset": 26, "endOffset": 29}, {"referenceID": 16, "context": "\u2022 Rotating concepts (ROT) [7], [17] rotates the decision boundary or data points to simulate the change of data distribution.", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "\u2022 Circle concepts (CIR) [14], [27] applies a circle as the decision boundary in a 2-dimensional feature space and simulates the concept drift by changing the radius of the circle, i.", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "\u2022 Circle concepts (CIR) [14], [27] applies a circle as the decision boundary in a 2-dimensional feature space and simulates the concept drift by changing the radius of the circle, i.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "\u2022 Sine concepts (SIN) [14], [27] determines the label of data by a sine curve in a 2-dimensional feature space, which is defined as follow.", "startOffset": 22, "endOffset": 26}, {"referenceID": 25, "context": "\u2022 Sine concepts (SIN) [14], [27] determines the label of data by a sine curve in a 2-dimensional feature space, which is defined as follow.", "startOffset": 28, "endOffset": 32}, {"referenceID": 4, "context": "In the experiment, all of the data locate in the area of [-5, 5] for both dimensions.", "startOffset": 57, "endOffset": 64}, {"referenceID": 25, "context": "\u2022 STAGGER Boolean concepts (STA) [27], [28] generates the data with categorical features using a set of rules to determine the class label.", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "\u2022 STAGGER Boolean concepts (STA) [27], [28] generates the data with categorical features using a set of rules to determine the class label.", "startOffset": 39, "endOffset": 43}, {"referenceID": 25, "context": "According to [27] and [28], the features and values are color \u2208 {red(R), blue(B), green(G)}, shape \u2208 {circle(C), square(S), triangle(T)}, and size \u2208 {small(S), medium(M), large(L)}.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "According to [27] and [28], the features and values are color \u2208 {red(R), blue(B), green(G)}, shape \u2208 {circle(C), square(S), triangle(T)}, and size \u2208 {small(S), medium(M), large(L)}.", "startOffset": 22, "endOffset": 26}, {"referenceID": 27, "context": "\u2022 Covertype [29] is a real-world dataset for describing the observation of a forest area with 51 cartographic variables.", "startOffset": 12, "endOffset": 16}, {"referenceID": 27, "context": "\u2022 PokerHand [29] describes the suits and ranks of a hand of five playing cards.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "\u2022 Electricity, a widely used dataset [14], [7], is collected from the New South Wales Electricity Market in Australia, containing 45,312 instances dated from 7 May 1996 to 5 December 1998.", "startOffset": 37, "endOffset": 41}, {"referenceID": 6, "context": "\u2022 Electricity, a widely used dataset [14], [7], is collected from the New South Wales Electricity Market in Australia, containing 45,312 instances dated from 7 May 1996 to 5 December 1998.", "startOffset": 43, "endOffset": 46}, {"referenceID": 28, "context": "To make a comprehensive comparison, a Friedman test [30] is conducted based on the average accuracy results on both synthetic data streams (Table III) and real-world data streams (Table V), as shown in Table VI.", "startOffset": 52, "endOffset": 56}], "year": 2017, "abstractText": "Incremental learning with concept drift has often been tackled by ensemble methods, where models built in the past can be re-trained to attain new models for the current data. Two design questions need to be addressed in developing ensemble methods for incremental learning with concept drift, i.e., which historical (i.e., previously trained) models should be preserved and how to utilize them. A novel ensemble learning method, namely Diversity and Transfer based Ensemble Learning (DTEL), is proposed in this paper. Given newly arrived data, DTEL uses each preserved historical model as an initial model and further trains it with the new data via transfer learning. Furthermore, DTEL preserves a diverse set of historical models, rather than a set of historical models that are merely accurate in terms of classification accuracy. Empirical studies on 15 synthetic data streams and 4 real-world data streams (all with concept drifts) demonstrate that DTEL can handle concept drift more effectively than 4 other state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}