{"id": "1702.03584", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2017", "title": "Similarity Preserving Representation Learning for Time Series Analysis", "abstract": "A considerable amount of machine learning algorithms take matrices as their inputs. As such, they cannot directly analyze time series data due to its temporal nature, usually unequal lengths, and complex properties. This is a great pity since many of these algorithms are effective, robust, efficient, and easy to use. In this paper, we bridge this gap by proposing an efficient representation learning framework that is able to convert a set of time series with equal or unequal lengths to a matrix format. In particular, we guarantee that the pairwise similarities between time series are well preserved after the transformation. Therefore, the learned feature representation is particularly suitable to the class of learning problems that are sensitive to data similarities. Given a set of $n$ time series, we first construct an $n\\times n$ partially observed similarity matrix by randomly sampling $O(n \\log n)$ pairs of time series and computing their pairwise similarities. We then propose an extremely efficient algorithm that solves a highly non-convex and NP-hard problem to learn new features based on the partially observed similarity matrix. We use the learned features to conduct experiments on both data classification and clustering tasks. Our extensive experimental results demonstrate that the proposed framework is both effective and efficient.", "histories": [["v1", "Sun, 12 Feb 2017 22:38:42 GMT  (39kb,D)", "http://arxiv.org/abs/1702.03584v1", null], ["v2", "Thu, 9 Mar 2017 02:03:48 GMT  (39kb,D)", "http://arxiv.org/abs/1702.03584v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["qi lei", "jinfeng yi", "roman vaculin", "lingfei wu", "inderjit s dhillon"], "accepted": false, "id": "1702.03584"}, "pdf": {"name": "1702.03584.pdf", "metadata": {"source": "CRF", "title": "Similarity Preserving Representation Learning for Time Series Analysis", "authors": ["Qi Lei", "Jinfeng Yi", "Roman Vaculin", "Lingfei Wu", "Inderjit S. Dhillon", "Thomas J. Watson"], "emails": ["inderjit}@ices.utexas.edu,", "wuli}@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if it were a matter of a way in which people in the most different parts of the world move in a way in which they find their way in the most different parts of the world. (...) It is as if people in the most different parts of the world in which they live, live and work find themselves in the most different parts of the world. (...) It is as if people in the most different parts of the world in which they live, live and live in the most different parts of the world in which they live, live and work. (...) It is as if people in the most different parts of the world in the most different parts of the world in which they live, live and live. \"(...) It is as if people in the most different parts of the world in the most different parts of the world in the most different parts of the world in the most different cultures, live and live in the most different parts of the world in the most different parts of the world in different cultures.\" (...) It is as if people in the most different parts of the world in the most different parts of the world in the most different parts of the most different parts of the world in the most different parts of the world in the most different parts of the world in which they live, live, live and live and live and live and live. (...) It is as if people in the most different cultures in the most different parts of the most different parts of the world in the most different parts of the most different parts of the world in the most different parts of the world in the most different parts of the most different parts of the world in different parts of the world in different cultures."}, {"heading": "2 Related Work", "text": "In this section, we review the existing work on the representation of learning functions for time series data. Among them, a family of methods uses a series of derived features to represent time series. For example, [Nanopoulos et al., 2001] proposed to use the mean, standard deviation, kurtosis, and skew of time series to represent control diagram patterns. [Wang et al., 2006] introduced a number of features, such as trend, seasonality, serial correlation, chaos, nonlinearity, and self-similarity in the division of different types of time series. [Deng et al., 2013] used some easy-to-calculate features, such as mean, standard deviation, and slope, to derive temporal meaning curves to automate time series classification. To automate the selection of features for time series classification, [Fulcher and Jones, 2014] proposed a greedy forward method that would automatically select features from thousands of choices."}, {"heading": "3 Similarity Preserving Representation Learning for Time Series Analysis", "text": "In this section, we first present the general idea of our similarity-preserving learning framework for displaying time series, and then propose an extremely efficient algorithm that is significantly faster than the original idea."}, {"heading": "3.1 Problem Definition and General Framework", "text": "There are a number of time series in which the temporal order between the time zones is varied. (...) There are a number of approaches in which the temporal order between the time zones can be varied. (...) There are a number of approaches in which we use a mapping function f: T \u2192 Rd that analyzes the most commonly used similarities in the analysis of static data. (...) There are a number of approaches that can be calculated by a number of functions. (...) In this paper, we use Dynamic Time Distortions (DTW) to measure this similarity, as it provides the best measurement for a wide variety of time series. (...)"}, {"heading": "3.2 A Much More Efficient Algorithm", "text": "This year we are at a stage where we are not yet in a position to seek a solution, where we are not yet in a position to find a solution, where we are in a position to find a solution."}, {"heading": "4 Experiments", "text": "In this section, we evaluate the proposed framework, i.e. Similarity PreservIng RepresentAtion Learning (SPIRAL for short), in terms of both data classification and clustering tasks. For both tasks, we learn new feature representations by specifying the number of features d = 30, the number of iterations I = 20 and the sample size. Subsequently, we feed the learned features into some static models and compare them with the state-of-the-art time series classification and clustering algorithms. As the DTW function is also called by some basic algorithms, we specify the same DTW window size min (40, [average time series length / 10]) for all DTW algorithms evaluated here. All results were published on a Linux server with an Intel Xeon 2.40 GHz CPU and 256 GB of memory."}, {"heading": "4.1 Classification Task", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they"}, {"heading": "4.2 Clustering Task", "text": "This year, it will be in a position to achieve the objectives I have mentioned."}, {"heading": "5 Conclusions", "text": "In this paper, we propose a similarity-preserving learning framework for the presentation of time series. In view of a number of n time series, the key idea is to first generate a partially observed similarity matrix with O (n log n) observed similarities of the DTW and then factorize this matrix to learn a new characteristic representation. To this end, we propose the first symmetrical matrix factoring algorithm on a partially observed matrix. The proposed algorithm updates variables by exact cyclic coordinate descend and has very low computational costs in each iteration. The characteristic representation learned by the proposed framework preserves the similarities of the DTW raw time series data and is general enough to be applicable to a variety of learning problems. Our empirical studies of classification and cluster tasks verify the effectiveness and efficiency of the proposed method."}], "references": [{"title": "The knot book: an elementary introduction to the mathematical theory of knots", "author": ["Colin Conrad Adams"], "venue": "American Mathematical Soc.,", "citeRegEx": "Adams. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Low-rank and sparse modeling of highdimensional vector autoregressions", "author": ["Basu", "Michailidis", "2015] Sumanta Basu", "George Michailidis"], "venue": null, "citeRegEx": "Basu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Basu et al\\.", "year": 2015}, {"title": "Gesture recognition using skeleton data with weighted dynamic time warping", "author": ["S. Celebi", "A. Aydin", "T. Temiz", "T. Arici"], "venue": "VISAPP (1), pages 620\u2013625", "citeRegEx": "Celebi et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "pages 126\u2013133", "author": ["Kin-Pong Chan", "Ada Wai-Chee Fu. Efficient time series matching by wavelets. In ICDE"], "venue": "IEEE,", "citeRegEx": "Chan and Fu. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Xgboost: A scalable tree boosting system", "author": ["Tianqi Chen", "Carlos Guestrin"], "venue": "arXiv preprint arXiv:1603.02754,", "citeRegEx": "Chen and Guestrin. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "The ucr time series classification archive", "author": ["Yanping Chen", "Eamonn Keogh", "Bing Hu", "Nurjahan Begum", "Anthony Bagnall", "Abdullah Mueen", "Gustavo Batista"], "venue": "July", "citeRegEx": "Chen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Information Sciences", "author": ["Houtao Deng", "George Runger", "Eugene Tuv", "Martyanov Vladimir. A time series forest for classification", "feature extraction"], "venue": "239:142\u2013153,", "citeRegEx": "Deng et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "time warping", "author": ["Alon Efrat", "Quanfu Fan", "Suresh Venkatasubramanian. Curve matching"], "venue": "and light fields: New algorithms for computing similarity between curves. Journal of Mathematical Imaging and Vision, 27(3):203\u2013216,", "citeRegEx": "Efrat et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Cognitive science", "author": ["Jeffrey L Elman. Finding structure in time"], "venue": "14(2):179\u2013211,", "citeRegEx": "Elman. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Communications in Mathematical Physics", "author": ["L\u00e1szl\u00f3 Erd\u0151s", "Benjamin Schlein", "Horng-Tzer Yau. Local semicircle law", "complete delocalization for wigner random matrices"], "venue": "287(2):641\u2013655,", "citeRegEx": "Erd\u0151s et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "volume 23", "author": ["Christos Faloutsos", "Mudumbai Ranganathan", "Yannis Manolopoulos. Fast subsequence matching in time-series databases"], "venue": "ACM,", "citeRegEx": "Faloutsos et al.. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "JMLR, 9(Aug):1871\u2013 1874,", "citeRegEx": "Fan et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "IEEE Transactions on Knowledge and Data Engineering", "author": ["Ben D Fulcher", "Nick S Jones. Highly comparative feature-based time-series classification"], "venue": "26(12):3026\u20133037,", "citeRegEx": "Fulcher and Jones. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Matrix completion has no spurious local minimum", "author": ["R. Ge", "J. Lee", "T. Ma"], "venue": "NIPS, pages 2973\u2013 2981", "citeRegEx": "Ge et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "pages 1064\u20131072", "author": ["Cho-Jui Hsieh", "Inderjit S Dhillon. Fast coordinate descent methods with variable selection for non-negative matrix factorization. In KDD"], "venue": "ACM,", "citeRegEx": "Hsieh and Dhillon. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Knowledge and information Systems", "author": ["Eamonn Keogh", "Kaushik Chakrabarti", "Michael Pazzani", "Sharad Mehrotra. Dimensionality reduction for fast similarity search in large time series databases"], "venue": "3(3):263\u2013 286,", "citeRegEx": "Keogh et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "ACM SIGMOD Record", "author": ["Flip Korn", "Hosagrahar V Jagadish", "Christos Faloutsos. Efficiently supporting ad hoc queries in large datasets of time sequences"], "venue": "26(2):289\u2013300,", "citeRegEx": "Korn et al.. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "A review of unsupervised feature learning and deep learning for time-series modeling", "author": ["M. L\u00e4ngkvist", "L. Karlsson", "A. Loutfi"], "venue": "Pattern Recognition Letters, 42:11\u201324", "citeRegEx": "L\u00e4ngkvist et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Time series clustering: Complex is simpler! In ICML", "author": ["Lei Li", "B Aditya Prakash"], "venue": "pages 185\u2013192,", "citeRegEx": "Li and Prakash. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Experiencing sax: a novel symbolic representation of time series", "author": ["J. Lin", "E. Keogh", "L. Wei", "S. Lonardi"], "venue": "Data Mining and Knowledge Discovery, 15(2):107\u2013144", "citeRegEx": "Lin et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Distribution of eigenvalues for some sets of random matrices", "author": ["V. Mar\u010denko", "L. Pastur"], "venue": "Mathematics of the USSR-Sbornik, 1(4):457", "citeRegEx": "Mar\u010denko and Pastur. 1967", "shortCiteRegEx": null, "year": 1967}, {"title": "Voice recognition algorithms using mel frequency cepstral coefficient (mfcc) and dynamic time warping (dtw) techniques", "author": ["Lindasalwa Muda", "Mumtaj Begam", "I Elamvazuthi"], "venue": "arXiv preprint arXiv:1003.4083,", "citeRegEx": "Muda et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Information Retrieval for Music and Motion", "author": ["Meinard M\u00fcller. Dtw-based motion comparison", "retrieval"], "venue": "pages 211\u2013226,", "citeRegEx": "M\u00fcller. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Feature-based classification of timeseries data", "author": ["A. Nanopoulos", "R. Alcock", "Y. Manolopoulos"], "venue": "International Journal of Computer Research, 10(3):49\u201361", "citeRegEx": "Nanopoulos et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Inaccuracies of shape averaging method using dynamic time warping for time series data", "author": ["V. Niennattrakul", "C. Ratanamahatana"], "venue": "International conference on computational science, pages 513\u2013520. Springer", "citeRegEx": "Niennattrakul and Ratanamahatana. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "k-shape: Efficient and accurate clustering of time series", "author": ["John Paparrizos", "Luis Gravano"], "venue": "ACM SIGMOD, pages 1855\u20131870. ACM,", "citeRegEx": "Paparrizos and Gravano. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks, 61:85\u2013 117,", "citeRegEx": "Schmidhuber. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "pages 1\u20135", "author": ["Samsu Sempena", "Nur Ulfa Maulidevi", "Peb Ruswono Aryan. Human action recognition using dynamic time warping. In International Conference on Electrical Engineering", "Informatics"], "venue": "IEEE,", "citeRegEx": "Sempena et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Technical report", "author": ["James H Stock", "Mark W Watson. Implications of dynamic factor models for var analysis"], "venue": "National Bureau of Economic Research,", "citeRegEx": "Stock and Watson. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "pages 270\u2013289", "author": ["Ruoyu Sun", "Zhi-Quan Luo. Guaranteed matrix completion via nonconvex factorization. In FOCS"], "venue": "IEEE,", "citeRegEx": "Sun and Luo. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "CoRR", "author": ["Arnaud Vandaele", "Nicolas Gillis", "Qi Lei", "Kai Zhong", "Inderjit S. Dhillon. Coordinate descent methods for symmetric nonnegative matrix factorization"], "venue": "abs/1509.01404,", "citeRegEx": "Vandaele et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Combination of dynamic time warping and multivariate analysis for the comparison of comprehensive two-dimensional", "author": ["Vial et al", "2009] J\u00e9r\u00f4me Vial", "Hicham No\u00e7airi", "Patrick Sassiat", "Sreedhar Mallipatu", "Guillaume Cognon", "Didier Thi\u00e9baut", "B\u00e9atrice Teillet", "Douglas N Rutledge"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Data mining and knowledge Discovery", "author": ["Xiaozhe Wang", "Kate Smith", "Rob Hyndman. Characteristic-based clustering for time series data"], "venue": "13(3):335\u2013 364,", "citeRegEx": "Wang et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Experimental comparison of representation methods and distance measures for time series data", "author": ["X. Wang", "A. Mueen", "H. Ding", "G. Trajcevski", "P. Scheuermann", "E. Keogh"], "venue": "Data Mining and Knowledge Discovery, 26(2):275\u2013309", "citeRegEx": "Wang et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "pages 1033\u20131040", "author": ["Xiaopeng Xi", "Eamonn Keogh", "Christian Shelton", "Li Wei", "Chotirat Ann Ratanamahatana. Fast time series classification using numerosity reduction. In ICML"], "venue": "ACM,", "citeRegEx": "Xi et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "10 challenging problems in data mining research", "author": ["Qiang Yang", "Xindong Wu"], "venue": "International Journal of Information Technology & Decision Making, 5(04):597\u2013604,", "citeRegEx": "Yang and Wu. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "pages 765\u2013774", "author": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Si Si", "Inderjit Dhillon. Scalable coordinate descent approaches to parallel matrix factorization for recommender systems. In ICDM"], "venue": "IEEE,", "citeRegEx": "Yu et al.. 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 35, "context": "It is considered by [Yang and Wu, 2006] as one of the 10 most challenging problems in data mining.", "startOffset": 20, "endOffset": 39}, {"referenceID": 17, "context": "The latter category of models, which usually take matrices as their inputs, cannot directly analyze time series data due to its temporal nature, usually unequal lengths, and complex properties [L\u00e4ngkvist et al., 2014].", "startOffset": 193, "endOffset": 217}, {"referenceID": 29, "context": "In order to significantly reduce the running time, we follow the setting of matrix completion [Sun and Luo, 2015] by assuming that the similarity matrix A is of low-rank.", "startOffset": 94, "endOffset": 113}, {"referenceID": 28, "context": "This is a very natural assumption since DTW algorithm captures the co-movements of time series, which has shown to be driven by only a small number of latent factors [Stock and Watson, 2005; Basu and Michailidis, 2015].", "startOffset": 166, "endOffset": 218}, {"referenceID": 23, "context": "For instance, [Nanopoulos et al., 2001] proposed to use the mean, standard deviation, kurtosis, and skewness of time series to represent control chart patterns.", "startOffset": 14, "endOffset": 39}, {"referenceID": 32, "context": "[Wang et al., 2006] introduced a set of features such as trend, seasonality, serial correlation, chaos, nonlinearity, and self-similarity to partition different types of time series.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "[Deng et al., 2013] used some easy to compute features such as mean, standard deviation and slope temporal importance curves to guide time series classification.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "In order to automate the selection of features for time series classification, [Fulcher and Jones, 2014] proposed a greedy forward method that can automatically select features from thousands of choices.", "startOffset": 79, "endOffset": 104}, {"referenceID": 10, "context": "Besides, several techniques have been proposed to represent time series by a certain types of transformation, such as discrete Fourier transformation [Faloutsos et al., 1994], discrete cosine transformation [Korn et al.", "startOffset": 150, "endOffset": 174}, {"referenceID": 16, "context": ", 1994], discrete cosine transformation [Korn et al., 1997], discrete wavelet transformation [Chan and Fu, 1999], piecewise aggregate approximation [Keogh et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 3, "context": ", 1997], discrete wavelet transformation [Chan and Fu, 1999], piecewise aggregate approximation [Keogh et al.", "startOffset": 41, "endOffset": 60}, {"referenceID": 15, "context": ", 1997], discrete wavelet transformation [Chan and Fu, 1999], piecewise aggregate approximation [Keogh et al., 2001], and symbolic aggregate approximation [Lin et al.", "startOffset": 96, "endOffset": 116}, {"referenceID": 19, "context": ", 2001], and symbolic aggregate approximation [Lin et al., 2007].", "startOffset": 46, "endOffset": 64}, {"referenceID": 8, "context": "In addition, deep learning models such as Elman recurrent neural network [Elman, 1990] and long short-term memory [Schmidhuber, 2015] are capable of modeling complex structures of time series data and learn a layer of feature representations.", "startOffset": 73, "endOffset": 86}, {"referenceID": 26, "context": "In addition, deep learning models such as Elman recurrent neural network [Elman, 1990] and long short-term memory [Schmidhuber, 2015] are capable of modeling complex structures of time series data and learn a layer of feature representations.", "startOffset": 114, "endOffset": 133}, {"referenceID": 22, "context": "Due to its superior performance, DTW has been successfully applied to many applications, including computer animation [M\u00fcller, 2007], surveillance [Sempena et al.", "startOffset": 118, "endOffset": 132}, {"referenceID": 27, "context": "Due to its superior performance, DTW has been successfully applied to many applications, including computer animation [M\u00fcller, 2007], surveillance [Sempena et al., 2011], gesture recognition [Celebi et al.", "startOffset": 147, "endOffset": 169}, {"referenceID": 2, "context": ", 2011], gesture recognition [Celebi et al., 2013], signature matching [Efrat et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 7, "context": ", 2013], signature matching [Efrat et al., 2007], protein sequence alignment [Vial et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 21, "context": ", 2009], and speech recognition [Muda et al., 2010].", "startOffset": 32, "endOffset": 51}, {"referenceID": 0, "context": "Since the inner product space can be induced from the normed space using \u3008x, y\u3009 = (\u2016x\u2016 + \u2016y\u2016 \u2212 \u2016x \u2212 y\u2016)/2 [Adams, 2004], we generate the DTW similarity by", "startOffset": 106, "endOffset": 119}, {"referenceID": 28, "context": "DTW algorithm measures the level of co-movement between time series, which has shown to be dictated by only a small number of latent factors [Stock and Watson, 2005; Basu and Michailidis, 2015].", "startOffset": 141, "endOffset": 193}, {"referenceID": 20, "context": "Since the matrix A is a special case of Wigner random matrix, the gaps between its consecutive eigenvalues should not be small [Mar\u010denko and Pastur, 1967], which implies the low-rank property since most of its energy is concentrated in its top eigenvalues [Erd\u0151s et al.", "startOffset": 127, "endOffset": 154}, {"referenceID": 9, "context": "Since the matrix A is a special case of Wigner random matrix, the gaps between its consecutive eigenvalues should not be small [Mar\u010denko and Pastur, 1967], which implies the low-rank property since most of its energy is concentrated in its top eigenvalues [Erd\u0151s et al., 2009].", "startOffset": 256, "endOffset": 276}, {"referenceID": 29, "context": "Based on the theory of matrix completion [Sun and Luo, 2015], only O(n log n) randomly sampled entries are needed to perfectly recover an n \u00d7 n low-rank matrix.", "startOffset": 41, "endOffset": 60}, {"referenceID": 13, "context": "Although [Ge et al., 2016] shows that the symmetric matrix factorization problem can be solved via (stochastic) gradient descent algorithm as well, our proposed coordinate descent algorithm has the following two advantages: (i) our CD algorithm directly updates each coordinate to the optimum in each iteration, thus does not need to tune parameters such as", "startOffset": 9, "endOffset": 26}, {"referenceID": 30, "context": "iteration [Vandaele et al., 2015; Hsieh and Dhillon, 2011; Yu et al., 2012].", "startOffset": 10, "endOffset": 75}, {"referenceID": 14, "context": "iteration [Vandaele et al., 2015; Hsieh and Dhillon, 2011; Yu et al., 2012].", "startOffset": 10, "endOffset": 75}, {"referenceID": 36, "context": "iteration [Vandaele et al., 2015; Hsieh and Dhillon, 2011; Yu et al., 2012].", "startOffset": 10, "endOffset": 75}, {"referenceID": 5, "context": "This hypothesis is verified by a convergence test conducted on the UCR Non-Invasive Fetal ECG Thorax1 testbed [Chen et al., 2015].", "startOffset": 110, "endOffset": 129}, {"referenceID": 5, "context": ", FordA, ECG5000, PhalangesOutlinesCorrect (POC), ItalyPowerDemand (IPD), and HandOutlines (HO), from the UCR Time Series Repository [Chen et al., 2015].", "startOffset": 133, "endOffset": 152}, {"referenceID": 4, "context": "Given the feature representations learned by the proposed SPIRAL framework, we fed them into the algorithms of XGBoost [Chen and Guestrin, 2016] and `2-regularized logistic regression that is implemented in LibLinear [Fan et Table 1: Description of Classification Data sets", "startOffset": 119, "endOffset": 144}, {"referenceID": 33, "context": "These approaches, denoted as SPIRAL-XGB and SPIRAL-LR, respectively, are compared with the state-of-theart time series classification algorithms NN-DTW [Wang et al., 2013] and Long Short-Term Memory (LSTM) [Schmidhuber, 2015].", "startOffset": 152, "endOffset": 171}, {"referenceID": 26, "context": ", 2013] and Long Short-Term Memory (LSTM) [Schmidhuber, 2015].", "startOffset": 42, "endOffset": 61}, {"referenceID": 34, "context": "Besides, NN-DTW is usually a very strong baseline that is considered as hard-to-beat in the literature [Xi et al., 2006; Wang et al., 2013].", "startOffset": 103, "endOffset": 139}, {"referenceID": 33, "context": "Besides, NN-DTW is usually a very strong baseline that is considered as hard-to-beat in the literature [Xi et al., 2006; Wang et al., 2013].", "startOffset": 103, "endOffset": 139}, {"referenceID": 25, "context": "We fed the features learned by the proposed framework into the kMeans algorithm as our clustering method, and compare it to the state-of-the-art time series clustering algorithm k-Shape [Paparrizos and Gravano, 2015], which has been shown to outperform many state-of-the-art partitional, hierarchical, and spectral clustering approaches.", "startOffset": 186, "endOffset": 216}, {"referenceID": 18, "context": "Besides, we also compare our method with clustering algorithms kMeansDTW and CLDS [Li and Prakash, 2011] since our ideas are similar in some respects.", "startOffset": 82, "endOffset": 104}, {"referenceID": 24, "context": "Although it looks similar to the idea of our SPIRAL-kMeans that also utilizes the DTW and kMeans algorithms, it is less desirable than SPIRAL-kMeans mainly because: (i) kMeans-DTW suffers from a very high computational cost since it needs to compute the pairwise DTW distances between all the time series and all the cluster centers at each iteration; and (ii) the DTW distance does not satisfy the triangle inequality, thus can make the cluster centers computed by averaging multiple time series drift out of the cluster [Niennattrakul and Ratanamahatana, 2007].", "startOffset": 522, "endOffset": 562}], "year": 2017, "abstractText": "A considerable amount of machine learning algorithms take matrices as their inputs. As such, they cannot directly analyze time series data due to its temporal nature, usually unequal lengths, and complex properties. This is a great pity since many of these algorithms are effective, robust, efficient, and easy to use. In this paper, we bridge this gap by proposing an efficient representation learning framework that is able to convert a set of time series with equal or unequal lengths to a matrix format. In particular, we guarantee that the pairwise similarities between time series are well preserved after the transformation. Therefore, the learned feature representation is particularly suitable to the class of learning problems that are sensitive to data similarities. Given a set of n time series, we first construct an n\u00d7n partially observed similarity matrix by randomly samplingO(n log n) pairs of time series and computing their pairwise similarities. We then propose an extremely efficient algorithm that solves a highly non-convex and NP-hard problem to learn new features based on the partially observed similarity matrix. We use the learned features to conduct experiments on both data classification and clustering tasks. Our extensive experimental results demonstrate that the proposed framework is both effective and efficient.", "creator": "LaTeX with hyperref package"}}}