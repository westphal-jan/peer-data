{"id": "1702.03592", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2017", "title": "Graph Neural Networks and Boolean Satisfiability", "abstract": "In this paper we explore whether or not deep neural architectures can learn to classify Boolean satisfiability (SAT). We devote considerable time to discussing the theoretical properties of SAT. Then, we define a graph representation for Boolean formulas in conjunctive normal form, and train neural classifiers over general graph structures called Graph Neural Networks, or GNNs, to recognize features of satisfiability. To the best of our knowledge this has never been tried before. Our preliminary findings are potentially profound. In a weakly-supervised setting, that is, without problem specific feature engineering, Graph Neural Networks can learn features of satisfiability.", "histories": [["v1", "Sun, 12 Feb 2017 23:12:01 GMT  (269kb,D)", "http://arxiv.org/abs/1702.03592v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["benedikt b\\\"unz", "matthew lamm"], "accepted": false, "id": "1702.03592"}, "pdf": {"name": "1702.03592.pdf", "metadata": {"source": "CRF", "title": "Graph Neural Networks and Boolean Satisfiability", "authors": ["Benedikt B\u00fcnz", "Matthew Lamm"], "emails": ["buenz@cs.stanford.edu", "mlamm@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Related Work and preliminary exploration", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Circuit solvers", "text": "The idea of using neural architectures to solve combinatorial optimization problems initially gained traction during what might be called the \"first wave\" of neural network research. Hopfield and Tank (1985) famously developed a neural architecture that could solve some cases of the driving salesman problem; in another case, Johnson (1989) used neural architecture to encode 3-SAT. The general scheme of earlier approaches was to define a circuit through which an objective function worldwide achieves optimum values only if it is satisfactorily mapped in the Boolean Polytope. While these architectures allow the re-interpretation of analog problems in digital form, the goal is permeated in sufficiently complex cases by local optimizations that make gradient-based optimization difficult. Furthermore, they do not learn in any way from data and in this sense do not utilize the power of neural networks proposed by newer researchers. Results of our own implementation of a network in Johnson's Anhang A. style can be found in this article."}, {"heading": "2.2 Deep learning, NLP, and Boolean logic", "text": "Neural network classifiers have recently performed well in semantic tasks such as sentiment analysis (Socher et al., 2012). Sentence logic is a different kind of language, but one with semantics no less: the designation of a Boolean expression is typically considered as its truth value when assigning an assignment to its variables. Like natural language, this designation is the result of a compositional operation with respect to the truth values of the variables contained in the formula. Recursive neural networks were developed to use the tree-like syntactical structures of natural language to grasp complex aspects of meaning composition, an important point at which the latent structure of a CNF formula deviates from natural language. Logical conjunctions and disjunctions are entirely comutative and there is no natural interpretation of CNF syntax as tree-like."}, {"heading": "3 Theoretical Properties of SAT", "text": "The difficulty of the SAT problems shows a hard phase shift phenomenon that stubbornly confuses theorists of predictability. In k-SAT problems, where the number of atoms per CNF clause is set to exactly k, there is a dramatic increase in the percentage of unsatisfactory problems according to a very specific clause-to-letter ratio. In 3-SAT, for example, the phase shift control rate \u03b1cr \u2248 4.3 (Saitta et al., 2011; Haim and Walsh, 2009). It is further understood that for problems far enough to the left of the phase shift, the relative distance between the solutions is small. There is another threshold after which the solution space is gradually split into exponentially many clusters. In the phase shift, solutions to formulas satisfactory are then scattered in interoperability."}, {"heading": "4 Graph Neural Networks", "text": "Graph Neural Networks, or GNNs, denote a class of neural networks that implement functions of the form \u03c4 (G, n) \u0440Rm that map a graph G and one of its nodes into a m-dimensional euclidean space. Scarselli et al. (2009a) show that GNs approximate all functions on graphs that fulfill the maintenance of the unfolding equivalence. Informally, this means that GNNs do not yield unique results only when input graphs have certain highly specific symmetries. Consequently, GNs are able to count the degree of node, the degree of node second order, and the detection of cliques of a given size in a set of graphs (Scarselli et al. 2009b). Given a graph G = (N, E) where N is a set of nodes between them, a set of edges is NNNNNs that deals with a parric relationship between them."}, {"heading": "5 CNF formulas as graphs", "text": "As already discussed, the satisfaction of a Boolean formula depends on whether or not the formula encodes a (potentially very complex) conflict between the variables that define it. It seems true that if a weakly supervised algorithm is to learn to recognize these conflict patterns, it must at least be supplied with relational information between variables in a formula, for example, whether a variable is negated in a given formula or whether two variables occur in a clause or not. Additionally, this view is motivated by negative results in Appendix B, so that neural networks for natural-language sentimental semantics have no satisfaction characteristics."}, {"heading": "5.1 CNF graphs and the phase shift", "text": "To our knowledge, there has been no work that has directly or indirectly investigated the effects of phase shift parameters on CNFs as graphs. In a probable scenario, a weakly supervised learner such as a Graph Neural Net will be more effective at detecting characteristics of satisfaction in graphs that describe CNF formulas that are far from phase shift than in CNF formulas that are close to it. This effect is described by (Devlin and O'Sullivan, 2008): Optimized search solvers will have to perform more traceability steps the closer they get to phase shift. In another, more general scenario, the ability to learn from graph representations will be different in some way before, during phase shift, and thereafter."}, {"heading": "5.2 CNF graphs for GNNs", "text": "From the implementation point of view, the variable type is the simpler of the two representations. For GNNs with multiple node types, it makes sense to implement type-specific transition functions (Scarselli et al., 2009b). Since we are just beginning to understand the behavior of the GNNs, we limit ourselves to considering the variable graph type in our experiments. A variable GNN graph G contains twice as many nodes in the Boolean formula \u03c6, with which it corresponds. Binary names indicate whether a node is a negated word or not (xn vs. \u00ac xn), and equivalently indexed variables are linked by a special edge. Nodes xn and xn \u2032, because n 6 = n \u2032 are connected by an edge when they appear together in a clause in a clause in the clause in the clause. In our experiment, edges are called ln-Rm, where the maximum number of clauses are selected if they are identical to the other clauses in two clauses."}, {"heading": "6 Data Generation and Experiments", "text": "CNF formulas encode a very powerful and theoretically studied classifier: the clause-to-atom ratio. In order to determine whether GNNs can learn anything beyond this intrinsic classifier, we created several training sets, each with a fixed clause-to-atom ratio. In addition, to analyze the effect of phase shift in the learning ability of GNNNs, we opted for the use of uniformly generated 3-SAT instances (RAND-SAT). These instances have a fixed number of clauses and atoms, and exactly 3 literals per clause. Atoms appear in clauses with uniform probability and are negated with uniform probability. We created 3 sets of clause-to-atom ratios of (4,4,6,6,10). According to the clause-to atom ratio, the probability that a formula is satisfactory is approximately 90% in the first data set, 50% in the second and 10% in the third data set."}, {"heading": "7 Conclusions and Future Work", "text": "Initially, we tried to compare Boolean satisfaction with the semantic classification of natural language, an area in which neural networks are state-of-the-art for certain tasks. Although our experiments in this regard were conducted in favor of later discoveries, they point to an interesting discovery: it does not appear that Boolean formulas in conjunctival normal form can be made in any principle-driven way \"like\" expressions in natural language. Our results suggest that the classification of graphs can offer a new way of thinking about the theoretical properties of SAT, and that SAT can be used as a test domain for the expressive properties of neural networks in a novel way. Our results suggest that the classification of graphs can represent a new way of thinking about the theoretical properties of SAT, and that SAT can be used as a test domain for the expressive properties of neural learning nerves."}, {"heading": "A Circuit Solvers", "text": "As mentioned in section 2 of the neural architecture, we can only find a solution if we have a specific SAT instance containing a function called an Assignment-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector-Vector"}, {"heading": "B Recursive Neural Networks", "text": "As described in Section 5, we claim that CNF formulas are best represented as graphs. Furthermore, forward-looking neural networks are not able to work on graphs because they are general function approximators for functions defined via Euclidean space. Recursive neural networks (RNs) are able to classify tree structures. As a first experiment, we investigated whether the success of RNNs in natural language processing had any implications for the classification of Boolean formulas. In this environment, we interpret disjunctive CNF clauses as words within a \"sentence,\" and represent them by means of vector representations of clauses defined in Appendix A. Unlike sentences in natural language, Boolean formulas do not have a tree-like syntactical structure. The clauses in a formula are per se commodifiable and their meaning composition is not recursive or scopal."}], "references": [{"title": "The complexity of theorem-proving procedures", "author": ["Stephen A. Cook"], "venue": "In Proceedings of the Third Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Cook.,? \\Q1971\\E", "shortCiteRegEx": "Cook.", "year": 1971}, {"title": "Satisfiability as a classification problem", "author": ["David Devlin", "Barry O\u2019Sullivan. B"], "venue": "In Proc. of the 19th Irish Conf. on Artificial Intelligence and Cognitive Science,", "citeRegEx": "Devlin and B.,? \\Q2008\\E", "shortCiteRegEx": "Devlin and B.", "year": 2008}, {"title": "Restart strategy selection using machine learning techniques", "author": ["Shai Haim", "Toby Walsh"], "venue": "Theory and Applications of Satisfiability Testing - SAT 2009,", "citeRegEx": "Haim and Walsh.,? \\Q2009\\E", "shortCiteRegEx": "Haim and Walsh.", "year": 2009}, {"title": "Tank. neural computation of decisions in optimization problems", "author": ["D.W.J.J. Hopfield"], "venue": "Biological Cybernetics,", "citeRegEx": "Hopfield,? \\Q1985\\E", "shortCiteRegEx": "Hopfield", "year": 1985}, {"title": "A neural network approach to the 3-satisfiability problem", "author": ["James L. Johnson"], "venue": "Journal of Parallel and Distributed Computing,", "citeRegEx": "Johnson.,? \\Q1989\\E", "shortCiteRegEx": "Johnson.", "year": 1989}, {"title": "Phase Transitions in Machine Learning", "author": ["Lorenza Saitta", "Attilio Giordana", "Antoine Cornujols"], "venue": null, "citeRegEx": "Saitta et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Saitta et al\\.", "year": 2011}, {"title": "Computational capabilities of graph neural networks", "author": ["Franco Scarselli", "Marco Gori", "Ah Chung Tsoi", "Markus Hagenbuchner", "Gabriele Monfardini"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Scarselli et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Scarselli et al\\.", "year": 2009}, {"title": "The graph neural network model", "author": ["Franco Scarselli", "Marco Gori", "Ah Chung Tsoi", "Markus Hagenbuchner", "Gabriele Monfardini"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Scarselli et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Scarselli et al\\.", "year": 2009}, {"title": "Local search strategies for satisfiability testing", "author": ["Bart Selman", "Henry Kautz", "Bram Cohen"], "venue": "In DIMACS SERIES IN DISCRETE MATHEMATICS AND THEORETICAL COMPUTER SCIENCE,", "citeRegEx": "Selman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Selman et al\\.", "year": 1995}, {"title": "Semantic Compositionality Through Recursive Matrix-Vector Spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "It has been shown that SAT is NP-complete (Cook, 1971); this implies that even the hardest problems in NP can be expressed as a SAT problem.", "startOffset": 42, "endOffset": 54}, {"referenceID": 8, "context": "Modern SAT solvers exist that can solve extremely large instances of SAT in a matter of milliseconds (Selman et al., 1995).", "startOffset": 101, "endOffset": 122}, {"referenceID": 9, "context": "Elsewhere, neural networks have shown great promise in reasoning about some subclasses of graphs, such as the treelike structures of natural language syntax (Socher et al., 2012).", "startOffset": 157, "endOffset": 178}, {"referenceID": 0, "context": "It has been shown that SAT is NP-complete (Cook, 1971); this implies that even the hardest problems in NP can be expressed as a SAT problem. On the other hand many SAT problems turn out to be easy in practice. Modern SAT solvers exist that can solve extremely large instances of SAT in a matter of milliseconds (Selman et al., 1995). This disparity motivates the search for properties that make SAT instances difficult. SAT is a self-reducible problem. Given an oracle determining whether a problem instance is satisfiable or not, one can find a satisfying assignment in time linear in the number of variables. This has motivated recent work by Devlin and O\u2019Sullivan (2008) examining the performance of a host of machine-learning classifiers for satisfiability.", "startOffset": 43, "endOffset": 674}, {"referenceID": 3, "context": "Hopfield and Tank (1985) famously developed a neural architecture that could solve some instances of the traveling salesman problem.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "Hopfield and Tank (1985) famously developed a neural architecture that could solve some instances of the traveling salesman problem. In another instance Johnson (1989) used a neural architecture to encode 3-SAT.", "startOffset": 0, "endOffset": 168}, {"referenceID": 9, "context": "Neural network classifiers have recently achieved high performance on natural language semantics tasks such as sentiment analysis (Socher et al., 2012).", "startOffset": 130, "endOffset": 151}, {"referenceID": 5, "context": "3 (Saitta et al., 2011; Haim and Walsh, 2009).", "startOffset": 2, "endOffset": 45}, {"referenceID": 2, "context": "3 (Saitta et al., 2011; Haim and Walsh, 2009).", "startOffset": 2, "endOffset": 45}, {"referenceID": 5, "context": "(Saitta et al., 2011) The phase shift and related phenomena likely have important implications for a learner of features of satisfiability and unsatisfiability.", "startOffset": 0, "endOffset": 21}, {"referenceID": 6, "context": "Scarselli et al. (2009a) show that GNNs approximate any functions on graphs that satisfy preservation of unfolding equivalence.", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": "3 (Saitta et al., 2011) (Haim and Walsh, 2009).", "startOffset": 2, "endOffset": 23}, {"referenceID": 2, "context": ", 2011) (Haim and Walsh, 2009).", "startOffset": 8, "endOffset": 30}, {"referenceID": 9, "context": "In keeping with the sentiment classification task of Socher et al. (2012), each node in the tree is labeled according to the satisfiability of the subproblem.", "startOffset": 53, "endOffset": 74}], "year": 2017, "abstractText": "In this paper we explore whether or not deep neural architectures can learn to classify Boolean satisfiability (SAT). We devote considerable time to discussing the theoretical properties of SAT. Then, we define a graph representation for Boolean formulas in conjunctive normal form, and train neural classifiers over general graph structures called Graph Neural Networks, or GNNs, to recognize features of satisfiability. To the best of our knowledge this has never been tried before. Our preliminary findings are potentially profound. In a weakly-supervised setting, that is, without problem specific feature engineering, Graph Neural Networks can learn features of satisfiability.", "creator": "LaTeX with hyperref package"}}}