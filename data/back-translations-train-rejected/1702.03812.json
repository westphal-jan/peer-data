{"id": "1702.03812", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2017", "title": "Reservoir Computing Using Non-Uniform Binary Cellular Automata", "abstract": "The Reservoir Computing (RC) paradigm utilizes a dynamical system, i.e., a reservoir, and a linear classifier, i.e., a read-out layer, to process data from sequential classification tasks. In this paper the usage of Cellular Automata (CA) as a reservoir is investigated. The use of CA in RC has been showing promising results. In this paper, selected state-of-the-art experiments are reproduced. It is shown that some CA-rules perform better than others, and the reservoir performance is improved by increasing the size of the CA reservoir itself. In addition, the usage of parallel loosely coupled CA-reservoirs, where each reservoir has a different CA-rule, is investigated. The experiments performed on quasi-uniform CA reservoir provide valuable insights in CA reservoir design. The results herein show that some rules do not work well together, while other combinations work remarkably well. This suggests that non-uniform CA could represent a powerful tool for novel CA reservoir implementations.", "histories": [["v1", "Mon, 13 Feb 2017 15:23:06 GMT  (1660kb,D)", "http://arxiv.org/abs/1702.03812v1", null]], "reviews": [], "SUBJECTS": "cs.ET cs.AI", "authors": ["stefano nichele", "magnus s gundersen"], "accepted": false, "id": "1702.03812"}, "pdf": {"name": "1702.03812.pdf", "metadata": {"source": "CRF", "title": "Reservoir Computing Using Non-Uniform Binary Cellular Automata", "authors": ["Stefano Nichele", "Magnus S. Gundersen"], "emails": ["stefano.nichele@hioa.no", "magnugun@stud.ntnu.no"], "sections": [{"heading": null, "text": "Keywords - Reservoir Computing, Cellular Automata, Parallel Reservoir, Recurrent Neural Networks, Non-Uniform Cellular Automata.I. INTRODUCTIONReal life problems often require the processing of time series data. Systems that process such data must remember inputs from earlier time steps in order to make correct predictions in future time steps, i.e. they must have some kind of memory. Recurrent Neural Networks (RNN) have been proven to have such memory [11]. Unfortunately, the formation of RNNs using traditional methods, i.e., gradient descent, is difficult [2]. A relatively novel approach called Reservoir Computing (RC) has been proposed [13], [20] to mitigate this problem. RC divides the RNN into two parts; the untrained recursive part, i.e. a reservoir computing part, i.e. a trainable feed-forward read part."}, {"heading": "II. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Reservoir Computing", "text": "In fact, most of them will be able to abide by the rules that they have applied in practice."}, {"heading": "C. Cellular Automata", "text": "It is a complex, decentralized and highly parallel system in which calculations can be made [23]. Such a system usually consists of a grid of cells with a current state. The state of a cell is determined by the update function f, which has a function of the adjacent states."}, {"heading": "D. Cellular automata in reservoir computing", "text": "In this context, it should be noted that this is a purely speculative development, which has clearly slowed down in recent years. (...) In this context, it should be noted that this is a purely speculative development. (...) In this context, it should be noted that the effects of the crisis have increased significantly in recent years. (...) In this context, it should be noted that the effects of the crisis on the economy, the economy, the economy, the economy and public finances, on the economy, on the economy, on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy, on the economy and on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy and on the economy, on the economy and on the economy, on the economy, on the economy and on the economy and on the economy, on the economy and on the economy, on the economy and on the economy, on the economy and on the economy and on the economy, on the economy and on the economy, on the economy and on the economy and on the economy, on the economy and on the economy and on the economy, on the economy and on the economy and on the economy, on the economy, on the economy and on the economy and on the"}, {"heading": "E. ReCA system implementations", "text": "There are only a few examples in the current phase of research. Yilmaz [30], [31] has implemented a ReCA system with elementary CA and Game of Life [6]. In the next phase, a working ReCA system is also shown in his Master Theory (by Nichele). The approaches used are similar, but there are some key differences: 1) Encoding and random mappings: In the encoding phase, random permutations are used over the same input vector. This encoding scheme can be seen in Figure 4. The permutation procedure is repeated R number of times, because it has been experimentally observed that several random mappings improve performance. In [5] a similar approach is used that the input vector is mapped to a vector larger than the input vector itself. The size of this mapping vector is \"automated\" by a parameter."}, {"heading": "III. EXPERIMENTAL SETUP", "text": "The basic architecture implemented in this thesis is shown in Fig. 9. The encoder is based on the architecture described in [5]. In this thesis, the parameter C is introduced as a metric of how large the resulting mapping vector should be. The concatenation procedure is adapted by [31]. After encoding (random mappings), the vectors are concatenated to a large vector. This vector is then passed into the reservoir, as described in Section II-E3. The time transition function is adapted by [5]. Mappings from the encoder are stored and used as a basis on which new inputs are mapped, as described in Section II-E4."}, {"heading": "A. Use of parallel CA-reservoirs in RC", "text": "This paper proposes the use of parallel reservoirs; the concept is presented in Fig. 11. At the boundary conditions, i.e. the cell at the very end of the reservoir, the cell located within the other reservoir is treated as a cell in its own reservoir, which causes information / calculations to flow between the reservoirs (loosely coupled). Different rules in the reservoirs may allow one to solve different aspects of the same problem or even two problems at the same time. [5] examines both the temporal parity and the temporal density problem [14]. Which rule is most suitable for a task is still an open research question. The features and classes described in Section II-C are useful knowledge, but they do not describe exactly why some rules work better than others in different tasks. Fig. 12 shows an example of the parallel system, with rule 90 on the left and rule 182 on the right."}, {"heading": "B. Measuring computational complexity of a CA-reservoir", "text": "The size of the canister is crucial for the success of the system. In this essay, the canister size is measured with R \u0443 I \u043a C. As shown in section III-A, the size of the canisters remains the same for both the single-rule canisters and the two-rule canisters. This is crucial for directly comparing their performance."}, {"heading": "IV. RESULTS", "text": "The parameters for the used 5-bit memory task can be found in Table I. In the virtually uniform CA storage system, the same parameters are used as in the single-container system with a combination of two rules. The tested rule combinations are shown in Table II."}, {"heading": "A. Results from the single ReCA-system", "text": "The results of the single reservoir ReCA system are in Table III. The results in this paper are significantly better than in [5]. However, we see a similar trend. Rules 102 and 105 could yield promising results, while rule 180 was not particularly well suited for this task, with the exception of rules 90 and 165, where the results in Table III are very accurate. [31] also achieves very promising results from rule 90."}, {"heading": "B. Results from the parallel (non-uniform) ReCA-system", "text": "The results can be seen in Table IV. It can be observed that a combination of rules that performed well in Table III, in combination, seem to produce good results. However, a combination of rules, e.g. 60 and 102, 153 and 195, resulted in worse results than the rules themselves. We can observe the same tendencies as in the individual races; higher R and I generally lead to better results."}, {"heading": "V. ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Single reservoir ReCA-system", "text": "The complexity of the reservoir is a useful metric for comparing different approaches. If we examine rule 90, we can find that for I = 4, R = 8, and C = 10, the success rate is 100%. In this configuration, the size of the reservoir is 4% 8% 10 = 320. [31] Although lower values of R and I also give 100%, the success rate for R = 4 and I = 4 is 97.5%, again 100% for I = 4 and R = 8. [31] reported a 100% success rate for the same task with R = 32 and I = 16. The C parameter was set to 1, giving a size of the reservoir of 32% 16% 1 = 512 (feed-forward architecture). [31] The results of the 5-bit task were also presented using the recurring architecture. 100% success rate was given with I = 32 and R = 45. This results in a reservoir size of 32% 45 = 1440. These results were designed to determine the relationship between the dispositor and the dispositor during the 5 time period."}, {"heading": "B. Parallel reservoir (non-uniform) ReCA-system", "text": "As described in [28], rule 165 is the supplement to rule 90. If we look at the results of the single CA in Table III, we can see that rule 90 and rule 165 work very similarly. If we look at one of the worst performing combinations of rules in the experiments, namely rule 153 and rule 195, we get some useful insights, as in Fig. 13. Here, it is possible to determine that the interaction of the rules creates a \"black\" region in the middle (between the rules), effectively reducing the size of the reservoir. As described in [27], rule 153 and rule 192 are the mirrored supplements. rule 105 is an interesting rule that can be combined with others."}, {"heading": "VI. CONCLUSION", "text": "Results have shown that some CA rules in combination work better than others. Good combinations tend to have some relationship, such as complementary CA tasks. Rules that are mirrored compliments do not work well together because they effectively reduce the size of the reservoir. The concept is still very new, and much research remains to be done, both in terms of the use of non-uniform CA tasks, and in terms of ReCA systems in general. As discussed earlier, finding the best combination of rules is not trivial. If we look only at the use of two different rules, the control space grows from just 256 singler reservoir options to 256! 2! 254! 254 = 2640 different rules that are well aligned to two different rules."}], "references": [{"title": "Learning longterm dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Real-time computation at the edge of chaos in recurrent neural networks", "author": ["Nils Bertschinger", "Thomas Natschl\u00e4ger"], "venue": "Neural computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Exploring physical reservoir computing using random boolean networks", "author": ["Aleksander Vognild Burkow"], "venue": "Master\u2019s thesis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Investigation of elementary cellular automata for reservoir computing", "author": ["Emil Taylor Bye"], "venue": "Master\u2019s thesis,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "The game of life", "author": ["John Conway"], "venue": "Scientific American,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1970}, {"title": "Universality in elementary cellular automata", "author": ["Matthew Cook"], "venue": "Complex systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Pattern recognition in a bucket", "author": ["Chrisantha Fernando", "Sampsa Sojakka"], "venue": "In European Conference on Artificial Life,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Introduction to random boolean networks", "author": ["Carlos Gershenson"], "venue": "arXiv preprint nlin/0408006,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Unifying quality metrics for reservoir networks", "author": ["Thomas E Gibbons"], "venue": "In Neural Networks (IJCNN), The 2010 International Joint Conference on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Deep learning. Book in preparation for", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "The echo state approach to analysing and training recurrent neural networks-with an erratum note", "author": ["Herbert Jaeger"], "venue": "Bonn, Germany: German National Research Center for Information Technology GMD Technical Report,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Long short-term memory in echo state networks: Details of a simulation study", "author": ["Herbert Jaeger"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Is there a liquid state machine in the bacterium escherichia coli", "author": ["Ben Jones", "Dov Stekel", "Jon Rowe", "Chrisantha Fernando"], "venue": "IEEE Symposium on Artificial Life,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Computation at the edge of chaos: phase transitions and emergent computation", "author": ["Chris G Langton"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "Edge of chaos and prediction of computational performance for neural circuit models", "author": ["Robert Legenstein", "Wolfgang Maass"], "venue": "Neural Networks,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Reservoir computing trends", "author": ["Mantas Luko\u0161evi\u010dius", "Herbert Jaeger", "Benjamin Schrauwen"], "venue": "KI-Ku\u0308nstliche Intelligenz,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "An experimental study on cellular automata reservoir in pathological sequence learning", "author": ["Mrwan Margem", "Ozg\u00fcr Yilmaz"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "The\u201d liquid computer\u201d: A novel strategy for real-time computing on time series", "author": ["Thomas Natschl\u00e4ger", "Wolfgang Maass", "Henry Markram"], "venue": "Special issue on Foundations of Information Processing of Telematik,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Scikit-learn: Machine learning in python", "author": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "The emergence of cellular computing", "author": ["Moshe Sipper"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1999}, {"title": "Computational capabilities of random automata networks for reservoir computing", "author": ["David Snyder", "Alireza Goudarzi", "Christof Teuscher"], "venue": "Physical Review E,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Theory of self-reproducing automata", "author": ["John Von Neumann", "Arthur W Burks"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1966}, {"title": "rule 60.\u201d from mathworld \u2013 a wolfram web resource. http://mathworld.wolfram.com/Rule60.html", "author": ["Eric W Weisstein"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "rule 90.\u201d from mathworld \u2013 a wolfram web resource. http://mathworld.wolfram.com/Rule90.html", "author": ["Eric W Weisstein"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "A new kind of science, volume 5", "author": ["Stephen Wolfram"], "venue": "Wolfram media Champaign,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}, {"title": "Reservoir computing using cellular automata", "author": ["Ozgur Yilmaz"], "venue": "arXiv preprint arXiv:1410.0162,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Connectionist-symbolic machine intelligence using cellular automata based reservoir-hyperdimensional computing", "author": ["Ozgur Yilmaz"], "venue": "arXiv preprint arXiv:1503.00851,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Recurrent Neural Networks (RNN) have been shown to possess such memory [11].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": ", gradient descent, is difficult [2].", "startOffset": 33, "endOffset": 36}, {"referenceID": 11, "context": "A fairly novel approach called Reservoir Computing (RC) has been proposed [13], [20] to mitigate this problem.", "startOffset": 74, "endOffset": 78}, {"referenceID": 18, "context": "A fairly novel approach called Reservoir Computing (RC) has been proposed [13], [20] to mitigate this problem.", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "In this paper, an RC-system is investigated, and a computational model called Cellular Automata (CA) [26] is used as the reservoir.", "startOffset": 101, "endOffset": 105}, {"referenceID": 28, "context": "This approach to RC was proposed in [30], and further studied in [31], [5], and [19].", "startOffset": 36, "endOffset": 40}, {"referenceID": 29, "context": "This approach to RC was proposed in [30], and further studied in [31], [5], and [19].", "startOffset": 65, "endOffset": 69}, {"referenceID": 3, "context": "This approach to RC was proposed in [30], and further studied in [31], [5], and [19].", "startOffset": 71, "endOffset": 74}, {"referenceID": 17, "context": "This approach to RC was proposed in [30], and further studied in [31], [5], and [19].", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "they are not aware of their own outputs [11].", "startOffset": 40, "endOffset": 44}, {"referenceID": 23, "context": "Examples include image classification [25], or playing the board game GO [22].", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "Examples include image classification [25], or playing the board game GO [22].", "startOffset": 73, "endOffset": 77}, {"referenceID": 9, "context": "However, when trying to solve problems that include sequential data, such as sentence-analysis, they often fall short [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 9, "context": "Recurrent Neural Networks (RNNs) can overcome this problem [11], being able to process sequential data through memory of previous inputs which are remembered by the network.", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "However, networks with recurrent connections are notoriously difficult to train by using traditional methods [2].", "startOffset": 109, "endOffset": 112}, {"referenceID": 11, "context": "The field of RC has been proposed independently by two approaches, namely Echo State Networks (ESN) [13] and Liquid State Machines (LSM) [20].", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "The field of RC has been proposed independently by two approaches, namely Echo State Networks (ESN) [13] and Liquid State Machines (LSM) [20].", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "Perhaps the most important feature is the Echo state property [13].", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "In the case of ESN, this is achieved by scaling of the connection weights of the recurrent nodes in the reservoir [18].", "startOffset": 114, "endOffset": 118}, {"referenceID": 1, "context": "As discussed in [3], the reservoir should preferably exhibit edge of chaos behaviors [16], in order to allow for high computational power [10].", "startOffset": 16, "endOffset": 19}, {"referenceID": 14, "context": "As discussed in [3], the reservoir should preferably exhibit edge of chaos behaviors [16], in order to allow for high computational power [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 8, "context": "As discussed in [3], the reservoir should preferably exhibit edge of chaos behaviors [16], in order to allow for high computational power [10].", "startOffset": 138, "endOffset": 142}, {"referenceID": 6, "context": "In [8] an actual bucket of water is implemented as a reservoir for speech-recognition, and in [15] the E.", "startOffset": 3, "endOffset": 6}, {"referenceID": 13, "context": "In [8] an actual bucket of water is implemented as a reservoir for speech-recognition, and in [15] the E.", "startOffset": 94, "endOffset": 98}, {"referenceID": 22, "context": "In [24] and more recently in [4], the usage of Random Boolean Networks (RBN) reservoirs is explored.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [24] and more recently in [4], the usage of Random Boolean Networks (RBN) reservoirs is explored.", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "RBNs can be considered as an abstraction of CA [9], and is thereby a related approach to the one presented in this paper.", "startOffset": 47, "endOffset": 50}, {"referenceID": 24, "context": "Cellular Automaton (CA) is a computational model, first proposed by Ulam and von Neumann in the 1940s [26].", "startOffset": 102, "endOffset": 106}, {"referenceID": 21, "context": "It is a complex, decentralized and highly parallel system, in which computations may emerge [23] through local interactions and without any form of centralized control.", "startOffset": 92, "endOffset": 96}, {"referenceID": 5, "context": "Some CA have been proved to be Turing complete [7], i.", "startOffset": 47, "endOffset": 50}, {"referenceID": 14, "context": "having all properties required for computation; that is transmission, storage and modification of information [16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 27, "context": "The numbering of the rules follows the naming convention described by Wolfram [29], where the resulting binary string is converted to a base 10 number.", "startOffset": 78, "endOffset": 82}, {"referenceID": 27, "context": "The rules may be divided into four qualitative classes [29], that exhibit different properties when evolved; class I: evolves to a static state, class II: evolves to a periodic structure, class III: evolves to chaotic patterns and class IV: evolves to complex patterns.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "Class I and II rules will fall into an attractor after a short while [16], and behave orderly.", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": ", at the edge of chaos [16].", "startOffset": 23, "endOffset": 27}, {"referenceID": 28, "context": "As proposed in [30], CA may be used as reservoir of dynamical systems.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "Such system is referred to as ReCA in [19], and the same name is therefore adopted in this paper.", "startOffset": 38, "endOffset": 42}, {"referenceID": 28, "context": "The projection of the input to the CA-reservoir can be done in two different ways [30].", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "See [31] for more details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "As discussed in section II-A, a reservoir often operates at the edge of chaos [10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 29, "context": "Figure adapted from [31]", "startOffset": 20, "endOffset": 24}, {"referenceID": 29, "context": "In [31] a speedup of 1.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "5-3X in the number of operations compared to the ESN [14] approach is reported.", "startOffset": 53, "endOffset": 57}, {"referenceID": 28, "context": "Yilmaz [30], [31] has implemented a ReCA system with elementary CA and Game of Life [6].", "startOffset": 7, "endOffset": 11}, {"referenceID": 29, "context": "Yilmaz [30], [31] has implemented a ReCA system with elementary CA and Game of Life [6].", "startOffset": 13, "endOffset": 17}, {"referenceID": 4, "context": "Yilmaz [30], [31] has implemented a ReCA system with elementary CA and Game of Life [6].", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "Bye [5] also demonstrated a functioning ReCA-system in his master\u2019s thesis (supervised by Nichele).", "startOffset": 4, "endOffset": 7}, {"referenceID": 29, "context": "1) Encoding and random mappings: In the encoding stage, [31] used random permutations over the same input-vector.", "startOffset": 56, "endOffset": 60}, {"referenceID": 29, "context": "The encoding used in [31].", "startOffset": 21, "endOffset": 25}, {"referenceID": 3, "context": "The encoding used in [5].", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "In [5] a similar approach was used.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "In the work herein, the approach described in [5] is used, but with a modification.", "startOffset": 46, "endOffset": 49}, {"referenceID": 29, "context": "2) Feed-forward or recurrent: [31] proposed both a feedforward and a recurrent design.", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "[5] only described a recurrent design.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "In the recurrent architecture, [31] concatenates the R number of permutations into one large vector of length (R \u2217 input length) before propagating it in a reservoir of the same width as this vector.", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "Time transition used in [31].", "startOffset": 24, "endOffset": 28}, {"referenceID": 3, "context": "[5] adapted a different approach, the same one that was also used by the feed-forward architecture in [31], where the R different permutations are iterated in separate reservoirs, and the different reservoirs are then concatenated before they are used by the classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "[5] adapted a different approach, the same one that was also used by the feed-forward architecture in [31], where the R different permutations are iterated in separate reservoirs, and the different reservoirs are then concatenated before they are used by the classifier.", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "The encoder is based on the architecture described in [5].", "startOffset": 54, "endOffset": 57}, {"referenceID": 29, "context": "The concatenation procedure is adapted from [31].", "startOffset": 44, "endOffset": 48}, {"referenceID": 3, "context": "The time-transition function is adapted from [5].", "startOffset": 45, "endOffset": 48}, {"referenceID": 19, "context": "The classifier used in this paper is a Support Vector Machine, as implemented in the Python machine learning framework scikit-learn [21].", "startOffset": 132, "endOffset": 136}, {"referenceID": 9, "context": "Classical feed-forward architectures are known to have issues with temporal tasks [11].", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "In order to test the ReCA-system at a temporal task, the 5-bit task [12] is chosen in this paper.", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "More details on the 5-bit memory task can be found in [14].", "startOffset": 54, "endOffset": 58}, {"referenceID": 3, "context": "In [5], both the temporal parity and the temporal density task [14] are investigated.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "In [5], both the temporal parity and the temporal density task [14] are investigated.", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "The results in this paper are significantly better than what was reported in [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 29, "context": "In [31] very promising results from rule 90 are also achieved.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "[31] reported a 100% success-rate on the same task with R = 32 and I = 16.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] also presented results on the 5-bit task using the recurrent architecture.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "As described in [28], rule 165 is the complement of rule 90.", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "As described in [27], rule 153 and 192 are the mirrored complements.", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "As described in [29], the rule does not have any compliments or any mirrored compliments.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": ", with lambda-parameter [16], Lyapunov exponent [17] or other metrics, it may be possible to pinpoint promising rules.", "startOffset": 24, "endOffset": 28}, {"referenceID": 15, "context": ", with lambda-parameter [16], Lyapunov exponent [17] or other metrics, it may be possible to pinpoint promising rules.", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "In [14] a wide range of different tasks is presented.", "startOffset": 3, "endOffset": 7}], "year": 2017, "abstractText": "The Reservoir Computing (RC) paradigm utilizes a dynamical system, i.e., a reservoir, and a linear classifier, i.e., a read-out layer, to process data from sequential classification tasks. In this paper the usage of Cellular Automata (CA) as a reservoir is investigated. The use of CA in RC has been showing promising results. In this paper, selected state-of-the-art experiments are reproduced. It is shown that some CA-rules perform better than others, and the reservoir performance is improved by increasing the size of the CA reservoir itself. In addition, the usage of parallel loosely coupled CA-reservoirs, where each reservoir has a different CA-rule, is investigated. The experiments performed on quasi-uniform CA reservoir provide valuable insights in CAreservoir design. The results herein show that some rules do not work well together, while other combinations work remarkably well. This suggests that non-uniform CA could represent a powerful tool for novel CA reservoir implementations. Keywords\u2014Reservoir Computing, Cellular Automata, Parallel Reservoir, Recurrent Neural Networks, Non-Uniform Cellular Automata.", "creator": "LaTeX with hyperref package"}}}