{"id": "1702.03856", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2017", "title": "Towards speech-to-text translation without speech recognition", "abstract": "We explore the problem of translating speech to text in low-resource scenarios where neither automatic speech recognition (ASR) nor machine translation (MT) are available, but we have training data in the form of audio paired with text translations. We present the first system for this problem applied to a realistic multi-speaker dataset, the CALLHOME Spanish-English speech translation corpus. Our approach uses unsupervised term discovery (UTD) to cluster repeated patterns in the audio, creating a pseudotext, which we pair with translations to create a parallel text and train a simple bag-of-words MT model. We identify the challenges faced by the system, finding that the difficulty of cross-speaker UTD results in low recall, but that our system is still able to correctly translate some content words in test data.", "histories": [["v1", "Mon, 13 Feb 2017 16:30:23 GMT  (348kb,D)", "http://arxiv.org/abs/1702.03856v1", "To appear in EACL 2017 (short papers)"]], "COMMENTS": "To appear in EACL 2017 (short papers)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sameer bansal", "herman kamper", "adam lopez", "sharon goldwater"], "accepted": false, "id": "1702.03856"}, "pdf": {"name": "1702.03856.pdf", "metadata": {"source": "CRF", "title": "Towards speech-to-text translation without speech recognition", "authors": ["Sameer Bansal", "Herman Kamper", "Adam Lopez", "Sharon Goldwater"], "emails": ["alopez}@inf.ed.ac.uk,", "kamperh@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to put ourselves at the top, and that we are able, that we are able to assert ourselves, that we are able, that we are able to put ourselves at the top, \"he said."}, {"heading": "2 From unsupervised term discovery to direct speech-to-text translation", "text": "For UTD, we use the Zero Resource Toolkit (ZRTools; Jansen and Van Durme, 2011).1 ZRTools uses Dynamic Time Warping (DTW) to detect pairs of acoustically similar audio segments, and then uses graph clustering on overlapping pairs to form a hard clustering of the detected segments. Replacing each detected segment with its unique cluster label or pseudoterm results in a partial, noisy transcription or pseudotext (fig. 1).Creating a translation model from this data presents us with a difficulty that does not arise in the parallel texts normally used to train translation models: the pseudotext does not represent all source words, as the detected segments do not cover the full audio (fig. 1). Therefore, we must not assume that our MT model can fully restore the test translation of a translation."}, {"heading": "3 Dataset", "text": "Although we did not have access to a resource-poor dataset, there is a corpus of noisy multilingualism that simulates many of the conditions we expect in our motivational applications: the CALLHOME dataset for Spanish-English translation (LDC2014T23; Post el., 2013).4 We conducted UTD over all 104 telephone calls that link 11 hours of audio with Spanish transcripts and their crowdsourced translations into English. Transcripts contain 168,195 Spanish word marks (10,674 types), and the translations contain 159,777 English word marks (6,723 types).Although our system does not require Spanish transcripts, we use them to evaluate UTD and simulate a perfect UTD system called oracle.For MT training, we use the pseudo-text and translations of 50 calls, and we filter stopwords in the theory."}, {"heading": "4 Analysis of challenges from UTD", "text": "Our system is based on the pseudo-text of ZRTools (the only freely available UTD system known to us), which presents several challenges for MT. We have used the standard parameters of ZRTools, and it is possible to set them to our task, but we will leave this to future work."}, {"heading": "4.1 Assigning wrong words to a cluster", "text": "Since UTD is not monitored, the detected clusters are loud. Fig. 1 shows an example of an incorrect match between the acoustically similar \"que \u0301 tal vas con\" and \"te trabajo y\" in statements B and C. Such discrepancies in turn influence the translation distribution due to c2. Many of these errors are due to correspondences between the speakers, which are known to be more difficult for UTD (Carlin et al., 2011; Kamper et al., 2015; Bansal et al., 2017). Most of the matches in our corpus occur across conversations, but these are also the least accurate (Table 1). Matches within statements that always come from the same loudspeaker are the most reliable, but account for the smallest proportion of detected pairs. Matches within telephone conversations lie between them. Overall, the average cluster purity is only 34%, meaning that 66% of the detected patterns do not match the most frequently in their cluster type."}, {"heading": "4.2 Splitting words across different clusters", "text": "Although most UTD matches occur between loudspeakers, the recall of matches between loudspeakers is lower than for matches between the same speakers. As a result, the same word from different speakers often appears in multiple clusters, preventing the model from learning good translations. ZRTools detects 15,089 clusters in 5http: / / www.nltk.org / our data, although there are only 10,674 word types. Only 1,614 of the clusters correspond one-to-one to a unique word type, while a many-to-one mapping of the rest covers only 1,819 gold types (leaving 7,241 gold types without a corresponding cluster). Fragmentation of words across clusters makes translation of pseudoterms impossible if they occur only in the test and not in training. Table 2 shows that these pseudotexts occur frequently outside the vocabulary (OOV), especially in the splitting of the loudspeakers themselves, but also in the acoustic differences in their frequency."}, {"heading": "4.3 UTD is sparse, giving low coverage", "text": "UTD is most reliable for long and frequently repeated patterns, so many spoken words are not represented in the pseudotext, as shown in Fig. 1. We found that the patterns discovered by ZRTools correspond to only 28% of the audio. This low coverage reduces the size of the training data, affects the alignment quality and impairs the translation, which is only possible if pseudoterms are present. UTD does not produce any pseudoterm at all in almost half of the utterances."}, {"heading": "5 Speech translation experiments", "text": "Since it translates only a handful of words in each sentence, BLEU, which measures the accuracy of word strings, can be an inappropriate measure of accuracy. 6 Instead, we calculate precision and recall about6BLEU values for monitored language translation systems trained on our data. (2014).The substantive words in the translation. We allow the system to guess K-words by pseudo-erm, so for each utterance, we calculate the number of correct predictions as corr @ K = | pred @ K-gold |, where pred @ K is the multiset of words predicted by K predictions and gold is the multiset of substantive words in the reference translation. For utterances where the reference translation does not contain substantive words, we use neutrality values to make a correct statement."}, {"heading": "6 Conclusions and future work", "text": "Our results show that it is possible to build a language translation system that uses only source-language audio paired with target-language text, which can be useful in many situations where no other language technology is available. Our analysis also shows several possible improvements. Poor matches between speakers and low audio coverage prevent our system from achieving a high recall rate, suggesting that language functions that are effective in multispeaker environments can be improved (Kamper et al., 2015; Kamper et al., 2016a), and the normalization of speakers (Zeghidour et al., 2016). Finally, Bansal et al. (2017) has recently shown that UTD can be improved by using the translations themselves as a source of information, suggesting shared learning as an attractive area for future work."}, {"heading": "Acknowledgments", "text": "We thank David Chiang and Antonios Anastasopoulos for sharing the CALLHOME speech and transcripts; Aren Jansen for supporting ZRTools; and Marco Damonte, Federico Fancellu, Sorcha Gilroy, Ida Szubert, Nikolay Bogoychev, Naomi Saphra, Joana Ribeiro and Clara Vania for comments on previous drafts. This work was partially supported by a Scholar Award from the James S McDonnell Foundation and a Research Award from the Google Faculty."}], "references": [{"title": "Learning a translation model from word lattices", "author": ["Adams et al.2016a] Oliver Adams", "Graham Neubig", "Trevor Cohn", "Steven Bird"], "venue": "In Proc. Interspeech", "citeRegEx": "Adams et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2016}, {"title": "Learning a lexicon and translation model from phoneme lattices", "author": ["Adams et al.2016b] Oliver Adams", "Graham Neubig", "Trevor Cohn", "Steven Bird", "Quoc Truong Do", "Satoshi Nakamura"], "venue": "In Proc. EMNLP", "citeRegEx": "Adams et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2016}, {"title": "An unsupervised probability model for speech-to-translation alignment of low-resource languages", "author": ["David Chiang", "Long Duong"], "venue": "In Proc. EMNLP", "citeRegEx": "Anastasopoulos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Anastasopoulos et al\\.", "year": 2016}, {"title": "Weakly supervised spoken term discovery using cross-lingual side information", "author": ["Bansal et al.2017] Sameer Bansal", "Herman Kamper", "Sharon Goldwater", "Adam Lopez"], "venue": "In Proc. ICASSP", "citeRegEx": "Bansal et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2017}, {"title": "Listen and translate: A proof of concept for end-to-end speech-to-text translation", "author": ["Olivier Pietquin", "Christophe Servan", "Laurent Besacier"], "venue": "In NIPS Workshop on End-to-end Learning for Speech and Audio Process-", "citeRegEx": "Berard et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Berard et al\\.", "year": 2016}, {"title": "Towards speech translation of non written languages", "author": ["Bowen Zhou", "Yuqing Gao"], "venue": "In Proc. SLT", "citeRegEx": "Besacier et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Besacier et al\\.", "year": 2006}, {"title": "Automatic speech recognition for under-resourced languages: A survey", "author": ["Etienne Barnard", "Alexey Karpov", "Tanja Schultz"], "venue": "Speech Communication,", "citeRegEx": "Besacier et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Besacier et al\\.", "year": 2014}, {"title": "Natural language processing with Python. O\u2019Reilly Media", "author": ["Bird et al.2009] Steven Bird", "Ewan Klein", "Edward Loper"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Brown et al.1993] Peter F Brown", "Vincent J Della Pietra", "Stephen A Della Pietra", "Robert L Mercer"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Rapid evaluation of speech representations for spoken term discovery", "author": ["Samuel Thomas", "Aren Jansen", "Hynek Hermansky"], "venue": "In Proc. Interspeech", "citeRegEx": "Carlin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Carlin et al\\.", "year": 2011}, {"title": "An attentional model for speech translation without transcription", "author": ["Duong et al.2016] Long Duong", "Antonios Anastasopoulos", "David Chiang", "Steven Bird", "Trevor Cohn"], "venue": "In Proc. NAACL HLT", "citeRegEx": "Duong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2016}, {"title": "A simple, fast, and effective reparameterization of IBM model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A Smith"], "venue": "In Proc. ACL", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["Jansen", "Van Durme2011] Aren Jansen", "Benjamin Van Durme"], "venue": "In Proc. ASRU", "citeRegEx": "Jansen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jansen et al\\.", "year": 2011}, {"title": "Unsupervised neural network based feature extraction using weak top-down constraints", "author": ["Kamper et al.2015] Herman Kamper", "Micha Elsner", "Aren Jansen", "Sharon Goldwater"], "venue": "In Proc. ICASSP", "citeRegEx": "Kamper et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kamper et al\\.", "year": 2015}, {"title": "2016a. A segmental framework for fully-unsupervised largevocabulary speech recognition", "author": ["Aren Jansen", "Sharon Goldwater"], "venue": "arXiv preprint arXiv:1606.06950", "citeRegEx": "Kamper et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kamper et al\\.", "year": 2016}, {"title": "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings", "author": ["Aren Jansen", "Sharon Goldwater"], "venue": null, "citeRegEx": "Kamper et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kamper et al\\.", "year": 2016}, {"title": "Some insights from translating conversational telephone speech", "author": ["Kumar et al.2014] Gaurav Kumar", "Matt Post", "Daniel Povey", "Sanjeev Khudanpur"], "venue": "In Proc. ICASSP", "citeRegEx": "Kumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2014}, {"title": "Unsupervised lexicon discovery from acoustic input", "author": ["Lee et al.2015] Chia-ying Lee", "T O\u2019Donnell", "James Glass"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Utterance classification in speech-tospeech translation for zero-resource languages in the hospital administration", "author": ["Martin et al.2015] Lara J Martin", "Andrew Wilkinson", "Sai Sumanth Miryala", "Vivian Robison", "Alan W Black"], "venue": null, "citeRegEx": "Martin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2015}, {"title": "Crowdsourced translation for emergency response in Haiti: the global collaboration of local knowledge", "author": ["Robert Munro"], "venue": "In AMTA Workshop on Collaborative Crowdsourcing for Translation", "citeRegEx": "Munro.,? \\Q2010\\E", "shortCiteRegEx": "Munro.", "year": 2010}, {"title": "Unsupervised pattern discovery in speech", "author": ["Park", "Glass2008] Alex S Park", "James Glass"], "venue": "IEEE Trans. Audio, Speech, Language Process.,", "citeRegEx": "Park et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Park et al\\.", "year": 2008}, {"title": "A global averaging method for dynamic time warping, with applications to clustering", "author": ["Alain Ketterlin", "Pierre Gan\u00e7arski"], "venue": "Pattern Recognition,", "citeRegEx": "Petitjean et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Petitjean et al\\.", "year": 2011}, {"title": "Improved speechto-text translation with the Fisher and Callhome Spanish\u2013English speech translation corpus", "author": ["Post et al.2013] Matt Post", "Gaurav Kumar", "Adam Lopez", "Damianos Karakos", "Chris Callison-Burch", "Sanjeev Khudanpur"], "venue": null, "citeRegEx": "Post et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Post et al\\.", "year": 2013}, {"title": "Spoken language translation", "author": ["Waibel", "Fugen2008] Alex Waibel", "Christian Fugen"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Waibel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Waibel et al\\.", "year": 2008}, {"title": "Joint learning of speaker and phonetic similarities with Siamese networks", "author": ["Gabriel Synnaeve", "Nicolas Usunier", "Emmanuel Dupoux"], "venue": "In Proc. Interspeech", "citeRegEx": "Zeghidour et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zeghidour et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "But high-quality ASR requires hundreds of hours of transcribed audio, while high-quality MT requires millions of words of parallel text\u2014resources available for only a tiny fraction of the world\u2019s estimated 7,000 languages (Besacier et al., 2014).", "startOffset": 222, "endOffset": 245}, {"referenceID": 5, "context": "Nevertheless, there are important low-resource settings in which even limited speech translation would be of immense value: documentation of endangered languages, which often have no writing system (Besacier et al., 2006; Martin et al., 2015); and crisis response, for which text applications have proven useful (Munro, 2010), but only help literate populations.", "startOffset": 198, "endOffset": 242}, {"referenceID": 18, "context": "Nevertheless, there are important low-resource settings in which even limited speech translation would be of immense value: documentation of endangered languages, which often have no writing system (Besacier et al., 2006; Martin et al., 2015); and crisis response, for which text applications have proven useful (Munro, 2010), but only help literate populations.", "startOffset": 198, "endOffset": 242}, {"referenceID": 19, "context": ", 2015); and crisis response, for which text applications have proven useful (Munro, 2010), but only help literate populations.", "startOffset": 77, "endOffset": 90}, {"referenceID": 6, "context": "For example, Duong et al. (2016) and Anastasopoulos et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 0, "context": "(2016) and Anastasopoulos et al. (2016) presented models that align audio to translated text, but neither used these models to try to translate new utterances (in fact, the latter model cannot make such predictions).", "startOffset": 11, "endOffset": 40}, {"referenceID": 0, "context": "(2016) and Anastasopoulos et al. (2016) presented models that align audio to translated text, but neither used these models to try to translate new utterances (in fact, the latter model cannot make such predictions). Berard et al. (2016) did develop a direct speech to translation system, but presented results only on a corpus of synthetic audio with a small number of speakers.", "startOffset": 11, "endOffset": 238}, {"referenceID": 17, "context": "Our simple system (\u00a72) builds on unsupervised speech processing (Versteegh et al., 2015; Lee et al., 2015; Kamper et al., 2016b), and in particular on unsupervised term discovery (UTD), which creates hard clusters of repeated word-like units in raw speech (Park and Glass, 2008; Jansen and Van Durme, 2011).", "startOffset": 64, "endOffset": 128}, {"referenceID": 22, "context": "We test our system on the CALLHOME Spanish-English speech translation corpus (Post et al., 2013), a noisy multi-speaker corpus of telephone calls in a variety of Spanish diar X iv :1 70 2.", "startOffset": 77, "endOffset": 96}, {"referenceID": 8, "context": "In these conditions, the language modeling and ordering assumptions of most MT models are unwarranted, so we instead use a simple bag-of-words translation model based only on co-occurrence: IBM Model 1 (Brown et al., 1993) with a Dirichlet prior over translation distributions, as learned by fast align (Dyer et al.", "startOffset": 202, "endOffset": 222}, {"referenceID": 11, "context": ", 1993) with a Dirichlet prior over translation distributions, as learned by fast align (Dyer et al., 2013).", "startOffset": 88, "endOffset": 107}, {"referenceID": 21, "context": "In a more realistic setup, we could use the training audio to construct a consensus representation of each pseudoterm (Petitjean et al., 2011; Anastasopoulos et al., 2016), then use DTW to identify its occurrences in test data to translate.", "startOffset": 118, "endOffset": 171}, {"referenceID": 2, "context": "In a more realistic setup, we could use the training audio to construct a consensus representation of each pseudoterm (Petitjean et al., 2011; Anastasopoulos et al., 2016), then use DTW to identify its occurrences in test data to translate.", "startOffset": 118, "endOffset": 171}, {"referenceID": 7, "context": "translations with NLTK (Bird et al., 2009).", "startOffset": 23, "endOffset": 42}, {"referenceID": 9, "context": "Many of these errors are due to cross-speaker matches, which are known to be more challenging for UTD (Carlin et al., 2011; Kamper et al., 2015; Bansal et al., 2017).", "startOffset": 102, "endOffset": 165}, {"referenceID": 13, "context": "Many of these errors are due to cross-speaker matches, which are known to be more challenging for UTD (Carlin et al., 2011; Kamper et al., 2015; Bansal et al., 2017).", "startOffset": 102, "endOffset": 165}, {"referenceID": 3, "context": "Many of these errors are due to cross-speaker matches, which are known to be more challenging for UTD (Carlin et al., 2011; Kamper et al., 2015; Bansal et al., 2017).", "startOffset": 102, "endOffset": 165}, {"referenceID": 16, "context": "BLEU scores for supervised speech translation systems trained on our data can be found in Kumar et al. (2014).", "startOffset": 90, "endOffset": 110}, {"referenceID": 13, "context": "speaker settings (Kamper et al., 2015; Kamper et al., 2016a) and speaker normalization (Zeghidour et al.", "startOffset": 17, "endOffset": 60}, {"referenceID": 24, "context": ", 2016a) and speaker normalization (Zeghidour et al., 2016).", "startOffset": 35, "endOffset": 59}, {"referenceID": 3, "context": "Finally, Bansal et al. (2017) recently showed that UTD can be improved using the translations themselves as a source of information, which suggests joint learning as an attractive area for future work.", "startOffset": 9, "endOffset": 30}, {"referenceID": 10, "context": "On the other hand, poor precision is most likely due to the simplicity of our MT model, and designing a model whose assumptions match our data conditions is an important direction for future work, which may combine our approach with insight from recent, quite different audio-to-translation models (Duong et al., 2016; Anastasopoulos et al., 2016; Adams et al., 2016a; Adams et al., 2016b; Berard et al., 2016).", "startOffset": 298, "endOffset": 410}, {"referenceID": 2, "context": "On the other hand, poor precision is most likely due to the simplicity of our MT model, and designing a model whose assumptions match our data conditions is an important direction for future work, which may combine our approach with insight from recent, quite different audio-to-translation models (Duong et al., 2016; Anastasopoulos et al., 2016; Adams et al., 2016a; Adams et al., 2016b; Berard et al., 2016).", "startOffset": 298, "endOffset": 410}, {"referenceID": 4, "context": "On the other hand, poor precision is most likely due to the simplicity of our MT model, and designing a model whose assumptions match our data conditions is an important direction for future work, which may combine our approach with insight from recent, quite different audio-to-translation models (Duong et al., 2016; Anastasopoulos et al., 2016; Adams et al., 2016a; Adams et al., 2016b; Berard et al., 2016).", "startOffset": 298, "endOffset": 410}], "year": 2017, "abstractText": "We explore the problem of translating speech to text in low-resource scenarios where neither automatic speech recognition (ASR) nor machine translation (MT) are available, but we have training data in the form of audio paired with text translations. We present the first system for this problem applied to a realistic multi-speaker dataset, the CALLHOME Spanish-English speech translation corpus. Our approach uses unsupervised term discovery (UTD) to cluster repeated patterns in the audio, creating a pseudotext, which we pair with translations to create a parallel text and train a simple bag-of-words MT model. We identify the challenges faced by the system, finding that the difficulty of cross-speaker UTD results in low recall, but that our system is still able to correctly translate some content words in test data.", "creator": "LaTeX with hyperref package"}}}