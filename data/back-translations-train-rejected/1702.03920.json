{"id": "1702.03920", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2017", "title": "Cognitive Mapping and Planning for Visual Navigation", "abstract": "We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person viewpoints and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the planner, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. Our experiments demonstrate that CMP outperforms both reactive strategies and standard memory-based architectures and performs well in novel environments. Furthermore, we show that CMP can also achieve semantically specified goals, such as 'go to a chair'.", "histories": [["v1", "Mon, 13 Feb 2017 18:52:04 GMT  (1879kb,D)", "http://arxiv.org/abs/1702.03920v1", "Under review for CVPR 2017. Project webpage:this https URL"], ["v2", "Sun, 23 Apr 2017 01:59:30 GMT  (1812kb,D)", "http://arxiv.org/abs/1702.03920v2", "To Appear at CVPR 2017. Project website with code, models, simulation environment and videos:this https URL"]], "COMMENTS": "Under review for CVPR 2017. Project webpage:this https URL", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG cs.RO", "authors": ["saurabh gupta", "james davidson", "sergey levine", "rahul sukthankar", "jitendra malik"], "accepted": false, "id": "1702.03920"}, "pdf": {"name": "1702.03920.pdf", "metadata": {"source": "CRF", "title": "Cognitive Mapping and Planning for Visual Navigation", "authors": ["Saurabh Gupta", "James Davidson", "Sergey Levine", "Rahul Sukthankar", "Jitendra Malik"], "emails": ["malik}@eecs.berkeley.edu,", "sukthankar}@google.com"], "sections": [{"heading": null, "text": "The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified common architecture for mapping and planning, so that mapping is driven by the planner's needs, and b) spatial memory with the ability to plan the world in the face of an incomplete set of observations. CMP constructs a top-down map of the world and uses a differentiated neural network planner to produce the next action at every step. Accumulated belief in the world enables the agent to track visited regions of the environment. Our experiments show that CMP outperforms both reactive strategies and standard memory-based architectures and functions well in novel environments. In addition, we show that CMP can also achieve semantically specified goals, such as \"go to a chair.\""}, {"heading": "1. Introduction", "text": "In fact, the fact is that most of them will be able to move to another world in which they are able to find themselves."}, {"heading": "2. Overview", "text": "Figure 1 shows an overview of our proposed cognitive and planning architecture. Our approach consists of two parts, a card reader and a planner. Our proposed card reader merges with information that it has absorbed over time to create an egocentric view of the world. This process is repeated at each step to achieve the goal."}, {"heading": "3. Problem Setup", "text": "Before describing the details of our proposed architecture, we present some modeling assumptions, notation and details of the experimental setup. To be able to focus on the high-level cartography and planning problem, we remove confusing factors resulting from the low control of the robot by performing our experiments in simulated real indoor environments. Studying the problem in simulation makes it easier to conduct exhaustive evaluation experiments, while the use of scanned real environments allows us to maintain the richness and complexity of real scenes of the robot. We also examine only the static version of the problem, although extensions of dynamic environments would be interesting to explore in future working environments. We model the robot as a cylinder of a fixed radius and height, equipped with vision sensors (RGB cameras or depth cameras) mounted at a fixed height and oriented to a fixed distance, the robot is equipped with relatively low controls."}, {"heading": "3.1. Experimental Testbed", "text": "We are conducting our experiments with the large-scale 3D Indoor Spaces (S3DIS) introduced by Armeni et al. [5] at Stanford. The data set consists of 3D scans collected in 6 large-scale indoor areas, taken from 3 different educational and office buildings. The data set was collected with the Matterport scanner [1]. We worked with meshes for these environments obtained from the authors. Scans from 2 buildings were used for training, and the agents were tested for scans from the 3rd building. We pre-processed the meshes to calculate the space the robot can pass through. Top views of the obtained traverable space are shown in Section A, showing the complexity of the environments we work with, and the differences in layouts between training and testing environments. Remember that the action space of the Ax robot consists of macro actions."}, {"heading": "4. Mapping", "text": "In this section, we will describe how the mapping portion of our learned network can be integrated into a top-down 2D representation of the environment (learning to use statistical structures in the world) from the previous time. Note that, unlike analytical mapping systems, the map in our model amounts to a latent representation. As it is fed directly into a learned planning module, we not only need to encode pure representations of free space, but can instead act as a general spatial memory - that is, the model can learn to store within the map all the information that is most useful for the creation of successful plans. However, to make a discussion concrete in this section, we assume that the mapper predicts free space that the architecture of the mapper is illustrated in Figure 2. At each step of time, we maintain a cumulative estimate of free space within the coordinate frame of the robot."}, {"heading": "5. Planning", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "6. Joint Architecture", "text": "Our final architecture, Cognitive Mapping and Planning (CMP), assembles the mapper and planner described above. At any time, the mapper updates its multi-scale belief in the world based on current observation. This updated belief is an input to the planner who outputs the actions to be taken. As previously described, all parts of the network are differentiated and enable end-to-end training, and no additional direct monitoring is required to train the mapping module - instead of creating maps that match a certain truthful space, the mapper simply needs to create maps that the planner can use to choose effective action. Training Procedure We train the CMP network with fully supervised training using DAGER [54]. We generate training paths by scanning an arbitrary target location on the map, and sample a starting point that is within K actions."}, {"heading": "7. Experiments", "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "8. Related Work", "text": "This year, the time has come for us to be able to go in search of a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is able to find a solution, that is capable of finding a solution."}, {"heading": "9. Discussion", "text": "In this paper, we have introduced a novel end-to-end neural architecture for navigation in novel environments. Our architecture learns to map from the perspective of the first person and uses a planner with the learned map to plan actions that lead to different goals in the environment. Our experiments show that such an approach outperforms other direct methods that do not use explicit mapping and planning modules. While our work represents exciting progress towards problems that have not been considered from a learning perspective, much more needs to be done to solve the problem of goal-oriented visual navigation in novel environments. A central limitation of our work is the assumption of perfect odometry. Robots operating in the real world do not have perfect odometry, and a model that takes into account factors in uncertain motion is essential before such a model can be used in the real world."}, {"heading": "A. Environment Visualization", "text": "In this section, we show the maps for the environments we have studied in this work. Figure A1, Figure A2, Figure A3 show the environments in which we have trained and validated. Figure A4 shows the environments in which we have tested. Note that the overall test environment comes from a different building (no floors were used for training or validation in this building). In addition, we draw some examples of geometric navigation problems in these environments, the task is to go from the circular node to the star node. The red bar in the corner denotes a length of 32 units (12.80 meters) in each of the environment."}, {"heading": "B. Computing backward flow field \u03c1 from egomotion", "text": "Consider a robot that rotates around its position around an angle \u03b8 and then pushes t-units forward. Corresponding points p in the original top view and p \u2032 in the new top view are related to each other as follows (R\u03b8 is a rotation matrix that rotates a point around an angle \u03b8): p \u2032 = Rt\u03b8p \u2212 t or p = R\u03b8 (p \u2032 + t) (3) Thus, taking into account the ego motion \u03b8 and t for each point in the new top view, we can calculate the position in the original top view from which it emerged. Figure A1: Maps for areas 1 and 6. Light range shows the moving space. Red bar in the corner indicates a length of 32 units (12.80 meters). We also show some geometric navigation problems in these environments, task A1: Maps for areas 1 and 6. Light range shows the moving space. Figure A2: Maps for areas 52 and 3. Light range indicates the moving space Red bar indicates the problems in the corner."}], "references": [{"title": "Software available from tensorflow.org", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Exploratory gradient boosting for reinforcement learning in complex domains", "author": ["D. Abel", "A. Agarwal", "F. Diaz", "A. Krishnamurthy", "R.E. Schapire"], "venue": "arXiv preprint arXiv:1603.04119,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Pixels to voxels: modeling visual representation in the human brain", "author": ["P. Agrawal", "D. Stansbury", "J. Malik", "J.L. Gallant"], "venue": "arXiv preprint arXiv:1407.5104,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "3d semantic parsing of largescale indoor spaces", "author": ["I. Armeni", "O. Sener", "A.R. Zamir", "H. Jiang", "I. Brilakis", "M. Fischer", "S. Savarese"], "venue": "CVPR,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Active visual object search in unknown environments using uncertain semantics", "author": ["A. Aydemir", "A. Pronobis", "M. G\u00f6belbecker", "P. Jensfelt"], "venue": "IEEE Transactions on Robotics, 29(4):986\u20131002,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A markovian decision process", "author": ["R. Bellman"], "venue": "Technical report, DTIC Document,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1957}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "Advances in Neural Information Processing Systems, pages 1171\u20131179,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Playing doom with slamaugmented deep reinforcement learning", "author": ["S. Bhatti", "A. Desmaison", "O. Miksik", "N. Nardelli", "N. Siddharth", "P.H. Torr"], "venue": "arXiv preprint arXiv:1612.00380,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Model-free episodic control", "author": ["C. Blundell", "B. Uria", "A. Pritzel", "Y. Li", "A. Ruderman", "J.Z. Leibo", "J. Rae", "D. Wierstra", "D. Hassabis"], "venue": "arXiv preprint arXiv:1606.04460,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepnav: Learning to navigate large cities", "author": ["S. Brahmbhatt", "J. Hays"], "venue": "arXiv preprint arXiv:1701.09135,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "Deepdriving: Learning affordance for direct perception in autonomous driving", "author": ["C. Chen", "A. Seff", "A. Kornhauser", "J. Xiao"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2722\u20132730,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning transferable policies for monocular reactive mav control", "author": ["S. Daftry", "J.A. Bagnell", "M. Hebert"], "venue": "arXiv preprint arXiv:1608.00627,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Mobile robot localisation using active vision", "author": ["A.J. Davison", "D.W. Murray"], "venue": "European Conference on Computer Vision, pages 809\u2013825. Springer,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "CVPR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Rl : Fast reinforcement learning via slow reinforcement learning", "author": ["Y. Duan", "J. Schulman", "X. Chen", "P.L. Bartlett", "I. Sutskever", "P. Abbeel"], "venue": "arXiv preprint arXiv:1611.02779,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Sonar-based real-world mapping and navigation", "author": ["A. Elfes"], "venue": "IEEE Journal on Robotics and Automation, 3(3):249\u2013265,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1987}, {"title": "Using occupancy grids for mobile robot perception and navigation", "author": ["A. Elfes"], "venue": "Computer, 22(6):46\u201357,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1989}, {"title": "Vision-based autonomous mapping and exploration using a quadrotor mav", "author": ["F. Fraundorfer", "L. Heng", "D. Honegger", "G.H. Lee", "L. Meier", "P. Tanskanen", "M. Pollefeys"], "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 4557\u20134564. IEEE,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual simultaneous localization and mapping: a survey", "author": ["J. Fuentes-Pacheco", "J. Ruiz-Ascencio", "J.M. Rend\u00f3n- Mancha"], "venue": "Artificial Intelligence Review, 43(1),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "A machine learning approach to visual perception of forest trails for mobile robots", "author": ["A. Giusti", "J. Guzzi", "D.C. Cire\u015fan", "F.-L. He", "J.P. Rodr\u0131\u0301guez", "F. Fontana", "M. Faessler", "C. Forster", "J. Schmidhuber", "G. Di Caro"], "venue": "IEEE Robotics and Automation Letters,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["S. Gu", "T. Lillicrap", "I. Sutskever", "S. Levine"], "venue": "arXiv preprint arXiv:1603.00748,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Perceptual organization and recognition of indoor scenes from RGB-D images", "author": ["S. Gupta", "P. Arbel\u00e1ez", "J. Malik"], "venue": "CVPR,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross modal distillation for supervision transfer", "author": ["S. Gupta", "J. Hoffman", "J. Malik"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Backprop kf: Learning discriminative deterministic state estimators", "author": ["T. Haarnoja", "A. Ajay", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1605.07148,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning longrange vision for autonomous off-road driving", "author": ["R. Hadsell", "P. Sermanet", "J. Ben", "A. Erkan", "M. Scoffier", "K. Kavukcuoglu", "U. Muller", "Y. LeCun"], "venue": "Journal of Field Robotics, 26(2):120\u2013144,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Memorybased control with recurrent neural networks", "author": ["N. Heess", "J.J. Hunt", "T.P. Lillicrap", "D. Silver"], "venue": "arXiv preprint arXiv:1512.04455,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Rgb-d mapping: Using depth cameras for dense 3d modeling of indoor environments", "author": ["P. Henry", "M. Krainin", "E. Herbst", "X. Ren", "D. Fox"], "venue": "In the 12th International Symposium on Experimental Robotics (ISER. Citeseer,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera", "author": ["S. Izadi", "D. Kim", "O. Hilliges", "D. Molyneaux", "R. Newcombe", "P. Kohli", "J. Shotton", "S. Hodges", "D. Freeman", "A. Davison", "A. Fitzgibbon"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Plato: Policy learning using adaptive trajectory optimization", "author": ["G. Kahn", "T. Zhang", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1603.00622,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Real-time obstacle avoidance for manipulators and mobile robots", "author": ["O. Khatib"], "venue": "The international journal of robotics research, 5(1):90\u201398,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1986}, {"title": "Autonomous helicopter flight via reinforcement learning", "author": ["H. Kim", "M.I. Jordan", "S. Sastry", "A.Y. Ng"], "venue": "Advances in neural information processing systems, page None,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2003}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Policy gradient reinforcement learning for fast quadrupedal locomotion", "author": ["N. Kohl", "P. Stone"], "venue": "Robotics and Automation, 2004. Proceedings. ICRA\u201904. 2004 IEEE International Conference on, volume 3, pages 2619\u20132624. IEEE,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "View-based maps", "author": ["K. Konolige", "J. Bowman", "J. Chen", "P. Mihelich", "M. Calonder", "V. Lepetit", "P. Fua"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic labeling of 3d point clouds for indoor scenes", "author": ["H. Koppula", "A. Anand", "T. Joachims", "A. Saxena"], "venue": "NIPS,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "The spatial semantic hierarchy", "author": ["B. Kuipers"], "venue": "Artificial intelligence, 119(1):191\u2013233,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2000}, {"title": "A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations", "author": ["B. Kuipers", "Y.-T. Byun"], "venue": "Robotics and autonomous systems, 8(1):47\u201363,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1991}, {"title": "Planning Algorithms", "author": ["S.M. LaValle"], "venue": "Cambridge University Press, Cambridge, U.K.,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "Rapidly-exploring random trees: Progress and prospects", "author": ["S.M. Lavalle", "J.J. Kuffner Jr"], "venue": "Algorithmic and Computational Robotics: New Directions. Citeseer,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2000}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "Journal of Machine Learning Research, 17(39):1\u201340,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "IJCV,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning to navigate in complex environments", "author": ["P. Mirowski", "R. Pascanu", "F. Viola", "H. Soyer", "A. Ballard", "A. Banino", "M. Denil", "R. Goroshin", "L. Sifre", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1611.03673,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Visual odometry", "author": ["D. Nist\u00e9r", "O. Naroditsky", "J. Bergen"], "venue": "Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on, volume 1, pages I\u2013652. IEEE,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2004}, {"title": "Control of memory, active perception, and action in minecraft", "author": ["J. Oh", "V. Chockalingam", "S. Singh", "H. Lee"], "venue": "arXiv preprint arXiv:1605.09128,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["J. Peters", "S. Schaal"], "venue": "Neural networks, 21(4):682\u2013 697,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2008}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["S. Ross", "G.J. Gordon", "D. Bagnell"], "venue": "AISTATS, volume 1, page 6,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2011}, {"title": "CAD)RL: Real singelimage flight without a singel real image", "author": ["F. Sadeghi", "S. Levine"], "venue": "arXiv preprint arXiv:1611.04201,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2016}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel"], "venue": "CoRR, abs/1502.05477,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "A comparison and evaluation of multi-view stereo reconstruction algorithms", "author": ["S.M. Seitz", "B. Curless", "J. Diebel", "D. Scharstein", "R. Szeliski"], "venue": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906), volume 1, pages 519\u2013528. IEEE,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2006}, {"title": "Value iteration networks", "author": ["A. Tamar", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1602.02867,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "Probabilistic robotics", "author": ["S. Thrun", "W. Burgard", "D. Fox"], "venue": "MIT press,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2005}, {"title": "Cognitive maps in rats and men", "author": ["E.C. Tolman"], "venue": "Psychological review, 55(4):189,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1948}, {"title": "Learning a world model and planning with a self-organizing, dynamic neural system", "author": ["M. Toussaint"], "venue": "NIPS,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2003}, {"title": "Recurrent policy gradients", "author": ["D. Wierstra", "A. F\u00f6rster", "J. Peters", "J. Schmidhuber"], "venue": "Logic Journal of IGPL, 18(5):620\u2013 634,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}, {"title": "Generic 3d representation via pose estimation and matching", "author": ["A.R. Zamir", "T. Wekel", "P. Agrawal", "C. Wei", "J. Malik", "S. Savarese"], "venue": "European Conference on Computer Vision, pages 535\u2013553. Springer,", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep reinforcement learning with successor features for navigation across similar environments", "author": ["J. Zhang", "J.T. Springenberg", "J. Boedecker", "W. Burgard"], "venue": "arXiv preprint arXiv:1612.05533,", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep neural network policies with continuous memory states", "author": ["M. Zhang", "Z. McCarthy", "C. Finn", "S. Levine", "P. Abbeel"], "venue": "2016 IEEE International Conference on Robotics and Automation (ICRA), pages 520\u2013527. IEEE,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2016}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Y. Zhu", "R. Mottaghi", "E. Kolve", "J.J. Lim", "A. Gupta", "L. Fei- Fei", "A. Farhadi"], "venue": "arXiv preprint arXiv:1609.05143,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Classical SLAM based approaches [14, 59] first build a 3D map using LIDAR, depth, or structure from motion, and then plan paths in this map.", "startOffset": 32, "endOffset": 40}, {"referenceID": 57, "context": "Classical SLAM based approaches [14, 59] first build a 3D map using LIDAR, depth, or structure from motion, and then plan paths in this map.", "startOffset": 32, "endOffset": 40}, {"referenceID": 44, "context": "proaches that go directly from pixels to actions [46, 50, 66] without going through explicit model or state estimation steps.", "startOffset": 49, "endOffset": 61}, {"referenceID": 48, "context": "proaches that go directly from pixels to actions [46, 50, 66] without going through explicit model or state estimation steps.", "startOffset": 49, "endOffset": 61}, {"referenceID": 64, "context": "proaches that go directly from pixels to actions [46, 50, 66] without going through explicit model or state estimation steps.", "startOffset": 49, "endOffset": 61}, {"referenceID": 64, "context": "[66] which trains a reactive system to output navigation macro-actions directly from visual input.", "startOffset": 0, "endOffset": 4}, {"referenceID": 58, "context": "In contrast, experiments have shown that even rats build mental maps of the environments in which they live [60], and can find shortcuts through them that a reactive agent is unable to discover.", "startOffset": 108, "endOffset": 112}, {"referenceID": 49, "context": "This amounts to assuming perfect visual odometry [51], which can itself be learned [25], but we defer the joint learning problem to future work.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "This amounts to assuming perfect visual odometry [51], which can itself be learned [25], but we defer the joint learning problem to future work.", "startOffset": 83, "endOffset": 87}, {"referenceID": 3, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "using [24] 78.", "startOffset": 6, "endOffset": 10}, {"referenceID": 32, "context": "Bi-linear sampling allows us to back-propagate gradients from ft to ft\u22121 [34], which will make it possible to train this model end to end.", "startOffset": 73, "endOffset": 77}, {"referenceID": 25, "context": "It is composed of a convolutional encoder which uses residual connections [27] and produces a representation of the scene in the 2D image space.", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "This choice for an analytic update function was made to keep the overall architecture simple and can be replaced with more sophisticated functions like those realized by LSTMs [31].", "startOffset": 176, "endOffset": 180}, {"referenceID": 56, "context": "[58], who observed that a particular type of planning algorithm called value iteration [7] can be implemented as a neural network with alternating convolutions and channel-wise max pooling operations, allowing the planner to be differentiated with respect to its inputs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[58], who observed that a particular type of planning algorithm called value iteration [7] can be implemented as a neural network with alternating convolutions and channel-wise max pooling operations, allowing the planner to be differentiated with respect to its inputs.", "startOffset": 87, "endOffset": 90}, {"referenceID": 56, "context": "[58] also showed that this reformulation of value iteration can also be used to learn the planner (the parameters in the convolutional layer of the planner) by providing supervision for the optimal action for each state.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "Hierarchical Planning Value iteration networks as presented in [58](v2) are impractical to use for any longhorizon planning problem.", "startOffset": 63, "endOffset": 67}, {"referenceID": 56, "context": "To alleviate this problem, we extend the hierarchical version presented in [58](v1).", "startOffset": 75, "endOffset": 79}, {"referenceID": 52, "context": "Training Procedure We train the CMP network using fully supervised training using DAGGER [54].", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "We use scheduled sampling [8], where we anneal the probability of sampling from the expert trajectories from 1 to 0 using inverse sigmoid decay.", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "All our models are trained asynchronously with 16 parallel GPU workers and 16 parameter servers using TensorFlow [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 36, "context": "We used ADAM [38] to optimize our loss function and trained for 60K iterations with a learning rate of 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 30, "context": "0001 to regularize the network and use batch-norm [32].", "startOffset": 50, "endOffset": 54}, {"referenceID": 26, "context": "We use ResNet-50 [28] pre-trained on ImageNet [15] to represent our RGB images.", "startOffset": 17, "endOffset": 21}, {"referenceID": 13, "context": "We use ResNet-50 [28] pre-trained on ImageNet [15] to represent our RGB images.", "startOffset": 46, "endOffset": 50}, {"referenceID": 22, "context": "We transfer supervision from RGB images to depth images using cross modal distillation [24] between RGB-D image pairs rendered from meshes in the training set to obtain a pre-trained ResNet-50 model to represent depth images.", "startOffset": 87, "endOffset": 91}, {"referenceID": 52, "context": "Since the goal of this paper is to study various architectures for navigation we train all these architectures the same way using DAGGER [54] as described earlier.", "startOffset": 137, "endOffset": 141}, {"referenceID": 3, "context": "Problems for this task are generated by first sampling a start node on the graph and then sampling an end node which is within 32 steps from the starting node and preferably in another room or in the hallway (we use room and hallway annotations from the dataset [5]).", "startOffset": 262, "endOffset": 265}, {"referenceID": 64, "context": "Note that this architecture is similar to the one used in [66].", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "[5] and label nodes in the graph Gx,\u03b8 as being \u2018chair\u2019 nodes if they were within 80 cm of a chair.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Visualizations To better understand the representation learned by the mapper, we train readout functions on the learned mapper representation to predict free space [4].", "startOffset": 164, "endOffset": 167}, {"referenceID": 15, "context": "The standard approach is to decompose the problem into two separate stages: (1) mapping the environment, and (2) planning a path through the constructed map [17, 36].", "startOffset": 157, "endOffset": 165}, {"referenceID": 34, "context": "The standard approach is to decompose the problem into two separate stages: (1) mapping the environment, and (2) planning a path through the constructed map [17, 36].", "startOffset": 157, "endOffset": 165}, {"referenceID": 57, "context": "A comprehensive survey of classical approaches for mapping and planning can be found in [59], we summarize some of the important research here.", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "[20] in context of range sensors as well as RGB images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Mapping has also been the focus of a large body of work in computer vision using RGB and RGB-D sensors [30, 33, 47, 57].", "startOffset": 103, "endOffset": 119}, {"referenceID": 31, "context": "Mapping has also been the focus of a large body of work in computer vision using RGB and RGB-D sensors [30, 33, 47, 57].", "startOffset": 103, "endOffset": 119}, {"referenceID": 45, "context": "Mapping has also been the focus of a large body of work in computer vision using RGB and RGB-D sensors [30, 33, 47, 57].", "startOffset": 103, "endOffset": 119}, {"referenceID": 55, "context": "Mapping has also been the focus of a large body of work in computer vision using RGB and RGB-D sensors [30, 33, 47, 57].", "startOffset": 103, "endOffset": 119}, {"referenceID": 24, "context": "Recent learning based approaches [26,63] study the problem in isolation thus only learning generic task-independent maps.", "startOffset": 33, "endOffset": 40}, {"referenceID": 61, "context": "Recent learning based approaches [26,63] study the problem in isolation thus only learning generic task-independent maps.", "startOffset": 33, "endOffset": 40}, {"referenceID": 43, "context": "Path planning in these inferred maps, has also been well studied, with pioneering works from LaValle [45], and well summarized in [44].", "startOffset": 101, "endOffset": 105}, {"referenceID": 42, "context": "Path planning in these inferred maps, has also been well studied, with pioneering works from LaValle [45], and well summarized in [44].", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "Numerous works have also studied the joint problem of mapping and planning [18, 19], which include works which relax the need for pre-mapping by learning and incrementally updating the map while navigating.", "startOffset": 75, "endOffset": 83}, {"referenceID": 17, "context": "Numerous works have also studied the joint problem of mapping and planning [18, 19], which include works which relax the need for pre-mapping by learning and incrementally updating the map while navigating.", "startOffset": 75, "endOffset": 83}, {"referenceID": 38, "context": "[40] and Aydemir et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[6] proposed approaches which leveraged semantics for more informed navigation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 40, "context": "[42, 43] introduce a cognitive mapping model using hierarchical abstractions of maps.", "startOffset": 0, "endOffset": 8}, {"referenceID": 41, "context": "[42, 43] introduce a cognitive mapping model using hierarchical abstractions of maps.", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "Semantics have also been associated with 3D environments more generally [23, 41].", "startOffset": 72, "endOffset": 80}, {"referenceID": 39, "context": "Semantics have also been associated with 3D environments more generally [23, 41].", "startOffset": 72, "endOffset": 80}, {"referenceID": 35, "context": "As an alternative to separating out discrete mapping and planning phases, reinforcement learning methods could be used to directly learn policies for such robotic tasks using reinforcement learning [37,39,53].", "startOffset": 198, "endOffset": 208}, {"referenceID": 37, "context": "As an alternative to separating out discrete mapping and planning phases, reinforcement learning methods could be used to directly learn policies for such robotic tasks using reinforcement learning [37,39,53].", "startOffset": 198, "endOffset": 208}, {"referenceID": 51, "context": "As an alternative to separating out discrete mapping and planning phases, reinforcement learning methods could be used to directly learn policies for such robotic tasks using reinforcement learning [37,39,53].", "startOffset": 198, "endOffset": 208}, {"referenceID": 48, "context": "Recent work in deep reinforcement learning (DRL) has proposed to learn policies in an end-to-end manner [50,56] from pixels to actions.", "startOffset": 104, "endOffset": 111}, {"referenceID": 54, "context": "Recent work in deep reinforcement learning (DRL) has proposed to learn policies in an end-to-end manner [50,56] from pixels to actions.", "startOffset": 104, "endOffset": 111}, {"referenceID": 20, "context": "A number of works [22, 49, 56] have proposed improvements to DRL algorithms.", "startOffset": 18, "endOffset": 30}, {"referenceID": 47, "context": "A number of works [22, 49, 56] have proposed improvements to DRL algorithms.", "startOffset": 18, "endOffset": 30}, {"referenceID": 54, "context": "A number of works [22, 49, 56] have proposed improvements to DRL algorithms.", "startOffset": 18, "endOffset": 30}, {"referenceID": 27, "context": "[29,49,52,62,65] study how to incorporate memory into such neural network based models.", "startOffset": 0, "endOffset": 16}, {"referenceID": 47, "context": "[29,49,52,62,65] study how to incorporate memory into such neural network based models.", "startOffset": 0, "endOffset": 16}, {"referenceID": 50, "context": "[29,49,52,62,65] study how to incorporate memory into such neural network based models.", "startOffset": 0, "endOffset": 16}, {"referenceID": 60, "context": "[29,49,52,62,65] study how to incorporate memory into such neural network based models.", "startOffset": 0, "endOffset": 16}, {"referenceID": 63, "context": "[29,49,52,62,65] study how to incorporate memory into such neural network based models.", "startOffset": 0, "endOffset": 16}, {"referenceID": 56, "context": "[58] study how explicit planning can be incorporated in such agents, but do not consider the case of first-person visual navigation, nor provide a framework for memory or mapping.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[52] study the generalization behavior of these algorithms to novel environments they have not been trained on.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 10, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 11, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 19, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 33, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 47, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 50, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 56, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 59, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 64, "context": "In the context of navigation, learning and DRL has been used to obtain policies [3, 12, 13, 21, 35, 49, 52, 58, 61, 66].", "startOffset": 80, "endOffset": 119}, {"referenceID": 10, "context": "Some of these works, such as [12,21,35], focus on the problem of learning low level controllers for effectively maneuvering around obstacles directly from raw sensor data.", "startOffset": 29, "endOffset": 39}, {"referenceID": 19, "context": "Some of these works, such as [12,21,35], focus on the problem of learning low level controllers for effectively maneuvering around obstacles directly from raw sensor data.", "startOffset": 29, "endOffset": 39}, {"referenceID": 33, "context": "Some of these works, such as [12,21,35], focus on the problem of learning low level controllers for effectively maneuvering around obstacles directly from raw sensor data.", "startOffset": 29, "endOffset": 39}, {"referenceID": 8, "context": "Others, such as [10, 52, 58], focus on the planning problem associated with navigation under full state observation [58], designing strategies for faster learning via episodic control [10], or incorporate memory into DRL algorithms to ease generalization to new environments.", "startOffset": 16, "endOffset": 28}, {"referenceID": 50, "context": "Others, such as [10, 52, 58], focus on the planning problem associated with navigation under full state observation [58], designing strategies for faster learning via episodic control [10], or incorporate memory into DRL algorithms to ease generalization to new environments.", "startOffset": 16, "endOffset": 28}, {"referenceID": 56, "context": "Others, such as [10, 52, 58], focus on the planning problem associated with navigation under full state observation [58], designing strategies for faster learning via episodic control [10], or incorporate memory into DRL algorithms to ease generalization to new environments.", "startOffset": 16, "endOffset": 28}, {"referenceID": 56, "context": "Others, such as [10, 52, 58], focus on the planning problem associated with navigation under full state observation [58], designing strategies for faster learning via episodic control [10], or incorporate memory into DRL algorithms to ease generalization to new environments.", "startOffset": 116, "endOffset": 120}, {"referenceID": 8, "context": "Others, such as [10, 52, 58], focus on the planning problem associated with navigation under full state observation [58], designing strategies for faster learning via episodic control [10], or incorporate memory into DRL algorithms to ease generalization to new environments.", "startOffset": 184, "endOffset": 188}, {"referenceID": 64, "context": "Most of this research (with the notable exception of [66]) focuses on the task of navigation in synthetic maze like environments which have little structure to them.", "startOffset": 53, "endOffset": 57}, {"referenceID": 64, "context": "[66].", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "Most notable among these is the work from Sadeghi and Levine [55] that shows that simulated mobility policies can transfer to the real world.", "startOffset": 61, "endOffset": 65}, {"referenceID": 46, "context": "[48] study sources of auxiliary supervision for better training of visual navigation policies with reinforcement learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] incorporate SLAM based maps along with inferred semantics for improving the performance at playing Doom.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Brahmbhatt and Hays [11] study the task of navigation in cities using Google Street View data.", "startOffset": 20, "endOffset": 24}, {"referenceID": 62, "context": "[64] and Duan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] study the task of faster learning for related navigation tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9, 16, 48] show results in synthetic maze-like environments, and only [11, 55, 64] show results with images from the real world.", "startOffset": 0, "endOffset": 11}, {"referenceID": 14, "context": "[9, 16, 48] show results in synthetic maze-like environments, and only [11, 55, 64] show results with images from the real world.", "startOffset": 0, "endOffset": 11}, {"referenceID": 46, "context": "[9, 16, 48] show results in synthetic maze-like environments, and only [11, 55, 64] show results with images from the real world.", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[9, 16, 48] show results in synthetic maze-like environments, and only [11, 55, 64] show results with images from the real world.", "startOffset": 71, "endOffset": 83}, {"referenceID": 53, "context": "[9, 16, 48] show results in synthetic maze-like environments, and only [11, 55, 64] show results with images from the real world.", "startOffset": 71, "endOffset": 83}, {"referenceID": 62, "context": "[9, 16, 48] show results in synthetic maze-like environments, and only [11, 55, 64] show results with images from the real world.", "startOffset": 71, "endOffset": 83}], "year": 2017, "abstractText": "We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person viewpoints and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the planner, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. Our experiments demonstrate that CMP outperforms both reactive strategies and standard memory-based architectures and performs well in novel environments. Furthermore, we show that CMP can also achieve semantically specified goals, such as \u201cgo to a chair\u201d.", "creator": "LaTeX with hyperref package"}}}