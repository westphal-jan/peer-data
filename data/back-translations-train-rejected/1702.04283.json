{"id": "1702.04283", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2017", "title": "Exploring loss function topology with cyclical learning rates", "abstract": "We present observations and discussion of previously unreported phenomena discovered while training residual networks. The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results. These behaviors were identified through the application of Cyclical Learning Rates (CLR) and linear network interpolation. Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training. For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates. Files to replicate these results are available at", "histories": [["v1", "Tue, 14 Feb 2017 16:46:19 GMT  (471kb,D)", "http://arxiv.org/abs/1702.04283v1", "Submitted as an ICLR 2017 Workshop paper"]], "COMMENTS": "Submitted as an ICLR 2017 Workshop paper", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["leslie n smith", "nicholay topin"], "accepted": false, "id": "1702.04283"}, "pdf": {"name": "1702.04283.pdf", "metadata": {"source": "CRF", "title": "EXPLORING LOSS FUNCTION TOPOLOGY WITH CYCLICAL LEARNING RATES", "authors": ["Leslie N. Smith", "Nicholay Topin"], "emails": ["leslie.smith@nrl.navy.mil", "ntopin1@umbc.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "A core area of the physical sciences over the centuries has been the development of tools to explore experimentally invisible aspects of nature. Revolutionary discoveries, such as the structure of atoms and DNA, would not have been possible without evidence from previous experimental evidence. Currently, most deep learning experimental results are reported in a limited number of standard methods. When using Convolutionary Neural Networks (CNNs) for classification, the authors generally report on the top k accuracy or error / loss of held test or validation samples after training. Optionally, a chart of these values is provided during training. Although it is certainly important to report these results because they are a measure of success, we argue that it is valuable to adopt a new perspective and report additional behaviors during training. (a) Test accuracy during standard training for three different initial learning rates. (b) Test Accuracy for Advanced Learning Results Results = 56-Net for Advanced Learning Progress = 0.1 and Net = 0.52-Net for Advanced Learning Progress = 0.1."}, {"heading": "2 SUPER-CONVERGENCE", "text": "Our next step was to try cyclical learning rates with a triangular policy for a number of cycles with the LR between 0.1 and 0.35. A triangular policy is a learning rate plan that repeatedly increases linearly and then decreases the learning rate between set limits (i.e. a minimum and maximum learning rate).A cycle consists of two steps and the step size is the number of iterations over which the learning rates pass from minimum to maximum or vice versa. Multiple counterintuitive results appear in Figure 2a, which shows the test accuracy, test losses and training losses for CLR with a step size of 10,000 iterations. This figure shows an anomaly that occurs when the LR increases from 0.1 to 0.35. The training loss increases significantly by four orders of magnitude, with the learning rate of approximately 0.255 (note the protocol scale used for the vertical axis) and the learning convergence being greater."}, {"heading": "3 NETWORK INTERPOLATION", "text": "Based on our results described above, we believe it is reasonable to ask ourselves whether the solutions at each of the five peaks in Figure 2b are exactly the same minimum that will be rediscovered. We have used the method of Goodfellow et al. (2014) and Im et al. (2016) to compare the solutions found at the end of each learning cycle (i.e. for iterations of 20 000, 40 000,...). In short, a series of network configurations has been created by performing an elementary linear interpolation between a pair of solution weights (i.e. netnew = \u03b1 net1 + (1 \u2212 \u03b1).net2, for a range of \u03b1 values). If the solution pair represents the same minimum, the interpolation should have a single concave shape as seen in Figure 3a, which is the result of the interpolation between states found during a regular training process."}, {"heading": "4 CONCLUSION", "text": "We believe that the underlying reasons for these observed patterns are a reflection of the loss function topology and that a constantly changing learning rate provides information about this topology. Furthermore, we believe that this loss function topology will lead to insights into the formation of neural networks, and we are actively looking for an employee to help us create a theoretical analysis of these phenomena. The results presented here represent only a fraction of the results we have obtained. Similar results are obtained with ResNet-20, ResNet-56 and ResNet-110 to Cifar-10 and Cifar-100. We are cataloging the effect of different solvers, hyperparameter values and architectures. Each architecture, hyperparameter value assignment and dataset have their own patterns; some of them follow an unusual / unexpected pattern. In addition, we are investigating whether the phenomena of high polar parameter values and architectures in relation to the results of network investigations have a wide range of measurements that we use to improve the results of the network."}], "references": [{"title": "Qualitatively characterizing neural network optimization problems", "author": ["Ian J Goodfellow", "Oriol Vinyals", "Andrew M Saxe"], "venue": "arXiv preprint arXiv:1412.6544,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "An empirical analysis of deep network loss surfaces", "author": ["Daniel Jiwoong Im", "Michael Tao", "Kristin Branson"], "venue": "arXiv preprint arXiv:1612.04010,", "citeRegEx": "Im et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Im et al\\.", "year": 2016}, {"title": "Cyclical learning rates for training neural networks", "author": ["Leslie N. Smith"], "venue": "arXiv preprint arXiv:1506.01186,", "citeRegEx": "Smith.,? \\Q2015\\E", "shortCiteRegEx": "Smith.", "year": 2015}, {"title": "Cyclical learning rates for training neural networks", "author": ["Leslie N. Smith"], "venue": "In Proceedings of the IEEE Winter Conference on Applied Computer Vision,", "citeRegEx": "Smith.,? \\Q2017\\E", "shortCiteRegEx": "Smith.", "year": 2017}], "referenceMentions": [{"referenceID": 2, "context": "The learning rate (LR) range test and cyclical learning rates (CLR) are described in (Smith, 2015) and (Smith, 2017).", "startOffset": 85, "endOffset": 98}, {"referenceID": 3, "context": "The learning rate (LR) range test and cyclical learning rates (CLR) are described in (Smith, 2015) and (Smith, 2017).", "startOffset": 103, "endOffset": 116}, {"referenceID": 0, "context": "We adopted the method of Goodfellow et al. (2014) and Im et al.", "startOffset": 25, "endOffset": 50}, {"referenceID": 0, "context": "We adopted the method of Goodfellow et al. (2014) and Im et al. (2016) to compare the solutions obtained at the end of each learning rate cycle (i.", "startOffset": 25, "endOffset": 71}, {"referenceID": 1, "context": "We found that the solution at the end of each cycle is distinct, which is in line with the results reported in (Im et al., 2016), where the authors show that different initializations lead to different solutions.", "startOffset": 111, "endOffset": 128}, {"referenceID": 0, "context": "This is in line with the results shown in Goodfellow et al. (2014) (Figures 1 and 2), though the authors do not discuss this not its potential to improve test accuracy.", "startOffset": 42, "endOffset": 67}], "year": 2017, "abstractText": "We present observations and discussion of previously unreported phenomena discovered while training residual networks. The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results.1 These behaviors were identified through the application of Cyclical Learning Rates (CLR) and linear network interpolation. Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training. For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates.", "creator": "LaTeX with hyperref package"}}}