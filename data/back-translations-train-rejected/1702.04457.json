{"id": "1702.04457", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2017", "title": "Automated Phrase Mining from Massive Text Corpora", "abstract": "As one of the fundamental tasks in text analysis, phrase mining aims at extracting quality phrases from a text corpus. Phrase mining is important in various tasks including automatic term recognition, document indexing, keyphrase extraction, and topic modeling. Most existing methods rely on complex, trained linguistic analyzers, and thus likely have unsatisfactory performance on text corpora of new domains and genres without extra but expensive adaption. Recently, a few data-driven methods have been developed successfully for extraction of phrases from massive domain-specific text. However, none of the state-of-the-art models is fully automated because they require human experts for designing rules or labeling phrases.", "histories": [["v1", "Wed, 15 Feb 2017 03:35:03 GMT  (1693kb,D)", "https://arxiv.org/abs/1702.04457v1", null], ["v2", "Sat, 11 Mar 2017 19:33:41 GMT  (5373kb,D)", "http://arxiv.org/abs/1702.04457v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jingbo shang", "jialu liu", "meng jiang", "xiang ren", "clare r voss", "jiawei han"], "accepted": false, "id": "1702.04457"}, "pdf": {"name": "1702.04457.pdf", "metadata": {"source": "CRF", "title": "Automated Phrase Mining from Massive Text Corpora", "authors": ["Jingbo Shang", "Jialu Liu", "Meng Jiang", "Xiang Ren", "Clare R Voss", "Jiawei Han"], "emails": ["hanj}@illinois.edu", "2jialu@google.com", "3clare.r.voss.civ@mail.mil"], "sections": [{"heading": null, "text": "In this paper, we propose a novel framework for automated phrase removal, AutoPhrase, that leverages this large amount of high-quality phrases effectively and performs better than limited, human-flagged phrases. In addition, we develop a POS-based phrase segmentation model that incorporates the shallow syntactical information into part-of-speech tags (POS tags) to further increase performance when a POS tagger is available. Note that AutoPhrase can support any language as long as there is a general knowledge base (e.g. Wikipedia) in that language, while it benefits from a POS tagger but does not require one. Compared to modern methods, the new method has shown significant improvements in effectiveness across five real datasets in different areas and languages."}, {"heading": "1. INTRODUCTION", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country"}, {"heading": "2. RELATED WORK", "text": "In fact, we will be able to go in search of a solution that will enable us, that will enable us to find a solution that will enable us, that will enable us to find a solution that will enable us, that will enable us to find a solution that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position to find a solution. \""}, {"heading": "3. PRELIMINARIES", "text": "The goal of this paper is to develop an automated phrase removal method to extract high-quality phrases from a large collection of documents without human identification efforts and with only limited, flat linguistic analysis. The main inputs to the automated phrase removal task are a corpus and a knowledge base. The input corpus is a textual word sequence in a specific language and a specific domain requiring human effort, of arbitrary length. The output is a ranking of phrases of decreasing quality. The workflow is completely different from our previous domain-independent phrase removal method, which requires human effort [13], although the phrase removal method and the features used during the phrase removal phase are the same. In this paper, we propose a robust Positive-Only Distance Training to minimize human effort and improve a POS-guided phrase removal method as a basic conceptual design in this section."}, {"heading": "4. METHODOLOGY", "text": "In this section, we focus on introducing our two new techniques. 4See https: / / github.com / shangjingbo1226 / AutoPhrase for more details."}, {"heading": "4.1 Robust Positive-Only Distant Training", "text": "To evaluate the quality of each phrase script, we first need to select hundreds of different quality texts from millions of candidates, and then label them with binary labels. (For example, to create such a label, we need to create hundreds of positive labels and annual reports, because this approach does not provide any clues as to how to identify candidates \"phrase scripts.) In this paper, we present a method that uses only existing general knowledge, without considering other human effects. (For example, to create such a label, we need to look at clinical reports and annual reports, because this approach does not provide any clues as to how to identify the phrase scripts.)"}, {"heading": "4.2 POS-Guided Phrasal Segmentation", "text": "The question, which arises, is, whether it is a POS-marked word sequence in m-segments, which is conditioned by a borderline sequence. (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.) (.). (.). (.). (.). (.). (.). (.). (.) (.) (.). (.) (.) (.) (. (.) (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.).). (.).). (.).). (.).). (.).). (.).).). (.).). (.).).).). (.).). (.).).).). (.).). (.).).).). (.).).).). (.).).). (.).).).). (.).).). (.).).).).). (.).). (.).).).).).). (.).)."}, {"heading": "4.3 Complexity Analysis", "text": "The time complexity of the most time-consuming components in our framework, such as common n gram, function extraction, POS-driven phrase segmentation, are all O (| | |) assuming that the maximum number of words in a phrase is a small constant (e.g. n \u2264 6), with the total number of words in the corpus | |. Therefore, AutoPhrase is linear to the body size and therefore very efficient and scalable. Meanwhile, each component can be grouped by phrases or sentences in an almost block-free manner."}, {"heading": "5. EXPERIMENTS", "text": "In this section, we will apply the proposed method to extract high-quality phrases from five massive corpora texts in three areas (scientific papers, annual reports and Wikipedia articles) and in three languages (English, Spanish and Chinese). We will compare the proposed method with many other methods to demonstrate its high performance, then we will examine the robustness of the proposed Positive-Only Distance Training and its performance compared to labeling by experts. The advantage of including POS tags in phrasal segmentation will also be demonstrated. Finally, we will present case studies."}, {"heading": "5.1 Datasets", "text": "To confirm that the proposed Positive-Only Remote Training can work effectively in different areas and that POS guided phrase segmentation can effectively support multiple languages, we have five large collections of text in different areas and languages, as shown in Table 1: Abstracts of English Computer Science Papers from DBLP6, English6https: / / aminer.org / citationbusiness reviews from Yelp7, Wikipedia Articles8 in English (EN), Spanish (ES) and Chinese (CN). From the existing general knowledge database Wikipedia, we extract popular mentions of units by analyzing intra-wiki citations within wiki contents9. On each data set, the intersection between the extracted popular mentions and the phrase candidates generated forms the positive pool. Therefore, the size of the positive pool in different data sets of the same language may vary."}, {"heading": "5.2 Compared Methods", "text": "SegPhrase10 / WrapSegPhrae11: In English domain-specific text corpora, our latest work SegPhrase surpassed phrase extraction [6], keyphrase extraction [23, 19], and noun phrase chunking methods. WrapSegPhrase extends SegPhrase to different languages by asking domain experts to annotate a representative set of 300 quality / interior phrases."}, {"heading": "5.3 Experimental Settings", "text": "Implementation. Pre-processing includes Lucene and Stanford NLP tokenizers and TreeTagger's POS tagger. Our documented code package has been published and maintained in GitHub14. Default parameters. We set the minimum support threshold as 30. The maximum number of words in a phrase is set to 6 according to the designations of domain experts. These are two parameters required by all methods. Further parameters that are compared by were set according to the open source tools or the original works. Human comment. We rely on human evaluators to assess the quality of phrases that cannot be identified by any knowledge base. Specifically, on each data set we randomly stab 500 such phrases from the predicted phrases of each method in the experiments. These selected phrases are mixed in a common pool and independently evaluated by 3 reviewers. We allow reviewers to use search engines when unknown phrases are found."}, {"heading": "5.4 Overall Performance", "text": "Overall, AutoPhrase yields the best results, both in terms of precision and recall. Significant benefits can be observed, especially on two non-English ES and CN records. For example, on the ES record, the recall of AutoPhrase is about 20% higher than the second-best method (SegPhrase) in absolute value. Meanwhile, there is a visible precision gap between AutoPhrase and the best baseline. The phrase chunking-based methods TF-IDF and TextRank work poorly because the extraction and ranking are modeled separately and the pre-trained complex linguistic analysts are not extended to domain-specific companies. TextRank usually starts with a higher precision than TF-IDF, but its memory is very low."}, {"heading": "5.5 Distant Training Exploration", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "5.6 POS-guided Phrasal Segmentation", "text": "We are also interested in how much performance gain we can achieve by including POS tags in this segmentation model, especially for different languages. We select Wikipedia article records in three different languages: EN, ES and CN.Figure 8 compares the results of AutoPhrase and AutoSegPhrase with the best base methods as references. AutoPhrase even outperforms AutoSegPhrase on the English record EN, although it has been shown that the length penalty works reasonably well in English [13]. The Spanish record ES has similar observations. In addition, the advantage of AutoPhrase on the CN record becomes more significant, indicating the poor generality of the length penalty. In summary, the inclusion of POS tags during phrase segmentation can work better than equally punitive phrases of equal length."}, {"heading": "5.7 Case Study", "text": "We present a case study on the extracted phrases as in Table 2. The best rated phrases are mostly named units, which makes sense for the Wikipedia article records. Even in the long tail, there are still many high-quality phrases. For example, we have the Great Woodpecker (a bird species) and the D-shaped Great Woodpecker (i.e. computer science and technology) with about 100,000. In fact, we have more than 345K and 116K phrases with a phrase quality of more than 0.5 on the EN- and CN-data sets respectively."}, {"heading": "5.8 Efficiency Evaluation", "text": "To investigate both time and memory efficiency, we select the three largest datasets: EN, ES and CN. Figures 9 (a) and 9 (b) evaluate the runtime and maximum memory usage of AutoPhrase using 10 threads in different proportions of three datasets each. Both time and memory are linear to the size of text corpora. In addition, AutoPhrase can also be paralleled almost block-free and shows linear acceleration in Figure 10 (c). In addition, AutoPhrase achieves approximately eight to eleven-fold acceleration and approximately five to seven-fold improvement in memory usage compared to the previous state-of-the-art phrase degradation method SegPhrase and its variants WrapSegPhrase on three datasets, as shown in Table 3. These improvements are achieved through more efficient indexing and more thorough parallelization."}, {"heading": "6. SINGLE-WORD PHRASES", "text": "AutoPhrase can be expanded to model individual phrases that can recall improvements in different datasets by about 10% to 30%. To study the effects of modeling high-quality single-word phrases, we select the three Wikipedia article datasets in different languages: EN, ES, and CN."}, {"heading": "6.1 Quality Estimation", "text": "In linguistic analysis, however, a phrase is not just a group of multiple words, but possibly a single word as long as it functions as a component in the syntax of a sentence [8]. As a large proportion (ranging from 10% to 30% on different sets of data based on our experiments) of high-quality phrases, we should consider single-word phrases (e.g. dUIUCc, dIllinois oisc, and dUSAc), as well as multi-word phrases, in order to achieve high recall in formulation. Considering the criteria for high-quality phrases, since single-word phrases cannot be broken down into two or more parts, the concordance and completeness of a word are no longer definable, so we revise the requirements for the quality of single-word phrases as. \""}, {"heading": "6.2 Experiments", "text": "We have a similar human remark to the one in the work. In other words, we sample 500 wiki-undetected phrases from the returned phrases (both single-word and multi-word phrases) of each method in experiments of the work. Therefore, we have new pools on the data sets EN, ES and CN. Also, the class-internal correlations (ICCs) are all more than 0.9, which shows the correspondence. Figure 10 compares all the methods based on these new pools. Note that all methods except SegPhrase / WrapSegPhrase extract single-word phrases. Significant memory advantages can always be observed on all data sets of EN, ES and CN. The memory differences between AutoPhrase + and AutoPhrase, which range from 10% to 30%, illuminate the importance of modeling single-word phrases. Everywhere in two latin-language datasets, AutoPhrase and AutoPhrase, AutoPhrase + and AutoPhrase, the earlier one and the Chinese phrases are located, however, there is a similar human remark to the earlier one."}, {"heading": "7. CONCLUSIONS", "text": "In this article, we present a framework for automated phrase mining with two novel techniques: the robust, remotely usable positive training and POS-guided phrase segmentation, which includes parts-of-speech (POS) tags to develop an automated phrase mining framework AutoPhrase. Our extensive experiments show that AutoPhrase is domain independent, outperforms other methods of phrase mining, and effectively supports multiple languages (e.g. English, Spanish, and Chinese) with minimal human effort. In addition, the inclusion of high-quality one-word phrases (e.g. dUIUCc and dUSAc) leads to an approximately 10% to 30% increase in memory and research into better indexing strategies and more thorough parallelization, resulting in an eight to eleven-fold acceleration of runtime and approximately 80% to 86% memory savings compared to Segante Phrase readers can try out our published grid of interest code."}, {"heading": "8. REFERENCES", "text": "[1] A. Allahverdyan and A. Galstyan. Comparative analysis of viterbi training and maximum probelihood estimation for hmms. In NIPS, pages 1674-1682, 2011. [2] L. Breiman. Randomizing outputs to increase prediction accuracy. Machine Learning, 40 (3): 229-242, 2000. [3] K.-h. Chen and H.-H. Chen. Extracting noun phrases from large-scale texts: A hybrid approach and its automatic evaluation. In ACL, 1994. [4] M.-C. De Marneffe, B. MacCartney, C. D. Manning, et al. Generating typed dependency parses from structure parses. In Proceedings of LREC, volume 6, pages 449-454, 2006. [5] P. Deane. A nonparametric method for extraction of candidate phrasal terms."}], "references": [{"title": "Comparative  analysis of viterbi training and maximum likelihood estimation for hmms", "author": ["A. Allahverdyan", "A. Galstyan"], "venue": "NIPS, pages 1674\u20131682", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Randomizing outputs to increase prediction accuracy", "author": ["L. Breiman"], "venue": "Machine Learning, 40(3):229\u2013242", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Extracting noun phrases from large-scale texts: A hybrid approach and its automatic evaluation", "author": ["K.-h. Chen", "H.-H. Chen"], "venue": "In ACL,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "et al", "author": ["M.-C. De Marneffe", "B. MacCartney", "C.D. Manning"], "venue": "Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449\u2013454", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "A nonparametric method for extraction of candidate phrasal terms", "author": ["P. Deane"], "venue": "ACL", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Scalable topical phrase mining from text", "author": ["A. El-Kishky", "Y. Song", "C. Wang", "C.R. Voss", "J. Han"], "venue": "corpora. VLDB,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Noun-phrase analysis in unrestricted text for information retrieval", "author": ["D.A. Evans", "C. Zhai"], "venue": "Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 17\u201324. Association for Computational Linguistics", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Linguistic terms and concepts", "author": ["G. Finch"], "venue": "Macmillan Press Limited", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic recognition of multi-word terms", "author": ["K. Frantzi", "S. Ananiadou", "H. Mima"], "venue": "the c-value/nc-value method. JODL, 3(2):115\u2013130", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Extremely randomized trees", "author": ["P. Geurts", "D. Ernst", "L. Wehenkel"], "venue": "Machine learning, 63(1):3\u201342", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Simple semi-supervised dependency parsing", "author": ["T. Koo", "X. Carreras", "M. Collins"], "venue": "ACL-HLT", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Is it harder to parse chinese", "author": ["R. Levy", "C. Manning"], "venue": "or the chinese treebank? In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 439\u2013446. Association for Computational Linguistics", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Mining quality phrases from massive text corpora", "author": ["J. Liu", "J. Shang", "C. Wang", "X. Ren", "J. Han"], "venue": "Proceedings of 2015 ACM SIGMOD International Conference on Management of Data", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic keyphrase extraction by bridging vocabulary gap", "author": ["Z. Liu", "X. Chen", "Y. Zheng", "M. Sun"], "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 135\u2013144. Association for Computational Linguistics", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Switching class labels to generate classification ensembles", "author": ["G. Mart\u00ednez-Mu\u00f1oz", "A. Su\u00e1rez"], "venue": "Pattern Recognition, 38(10):1483\u20131494", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["R. McDonald", "F. Pereira", "K. Ribarov", "J. Haji\u010d"], "venue": "EMNLP", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Textrank: Bringing order into texts", "author": ["R. Mihalcea", "P. Tarau"], "venue": "ACL", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Universal dependencies v1: A multilingual treebank collection", "author": ["J. Nivre", "M.-C. de Marneffe", "F. Ginter", "Y. Goldberg", "J. Hajic", "C.D. Manning", "R. McDonald", "S. Petrov", "S. Pyysalo", "N. Silveira"], "venue": "In Proceedings of the 10th International Conference on Language Resources and Evaluation", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Towards the web of concepts: Extracting concepts from large datasets", "author": ["A. Parameswaran", "H. Garcia-Molina", "A. Rajaraman"], "venue": "Proceedings of the Very Large Data Bases Conference (VLDB),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Automatic glossary extraction: beyond terminology identification", "author": ["Y. Park", "R.J. Byrd", "B.K. Boguraev"], "venue": "COLING", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "The use of classifiers in sequential inference", "author": ["V. Punyakanok", "D. Roth"], "venue": "NIPS", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Treetagger| a language independent part-of-speech tagger", "author": ["H. Schmid"], "venue": "Institut f\u00fcr Maschinelle Sprachverarbeitung, Universit\u00e4t Stuttgart, 43:28", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1995}, {"title": "Kea: Practical automatic keyphrase extraction", "author": ["I.H. Witten", "G.W. Paynter", "E. Frank", "C. Gutwin", "C.G. Nevill-Manning"], "venue": "Proceedings of the fourth ACM conference on Digital libraries, pages 254\u2013255. ACM", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "A unified statistical model for the identification of english basenp", "author": ["E. Xun", "C. Huang", "M. Zhou"], "venue": "ACL", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "A comparative evaluation of term recognition algorithms", "author": ["Z. Zhang", "J. Iria", "C.A. Brewster", "F. Ciravegna"], "venue": "LREC", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 8, "context": "Most existing methods [9, 20, 25] rely on complex, trained linguistic analyzers (e.", "startOffset": 22, "endOffset": 33}, {"referenceID": 19, "context": "Most existing methods [9, 20, 25] rely on complex, trained linguistic analyzers (e.", "startOffset": 22, "endOffset": 33}, {"referenceID": 24, "context": "Most existing methods [9, 20, 25] rely on complex, trained linguistic analyzers (e.", "startOffset": 22, "endOffset": 33}, {"referenceID": 12, "context": "Our latest domain-independent method SegPhrase [13] outperforms many other approaches [9, 20, 25, 5, 19, 6], but still needs domain experts to first carefully select hundreds of varying-quality phrases from millions of candidates, and then annotate them with binary labels.", "startOffset": 47, "endOffset": 51}, {"referenceID": 8, "context": "Our latest domain-independent method SegPhrase [13] outperforms many other approaches [9, 20, 25, 5, 19, 6], but still needs domain experts to first carefully select hundreds of varying-quality phrases from millions of candidates, and then annotate them with binary labels.", "startOffset": 86, "endOffset": 107}, {"referenceID": 19, "context": "Our latest domain-independent method SegPhrase [13] outperforms many other approaches [9, 20, 25, 5, 19, 6], but still needs domain experts to first carefully select hundreds of varying-quality phrases from millions of candidates, and then annotate them with binary labels.", "startOffset": 86, "endOffset": 107}, {"referenceID": 24, "context": "Our latest domain-independent method SegPhrase [13] outperforms many other approaches [9, 20, 25, 5, 19, 6], but still needs domain experts to first carefully select hundreds of varying-quality phrases from millions of candidates, and then annotate them with binary labels.", "startOffset": 86, "endOffset": 107}, {"referenceID": 4, "context": "Our latest domain-independent method SegPhrase [13] outperforms many other approaches [9, 20, 25, 5, 19, 6], but still needs domain experts to first carefully select hundreds of varying-quality phrases from millions of candidates, and then annotate them with binary labels.", "startOffset": 86, "endOffset": 107}, {"referenceID": 18, "context": "Our latest domain-independent method SegPhrase [13] outperforms many other approaches [9, 20, 25, 5, 19, 6], but still needs domain experts to first carefully select hundreds of varying-quality phrases from millions of candidates, and then annotate them with binary labels.", "startOffset": 86, "endOffset": 107}, {"referenceID": 5, "context": "Our latest domain-independent method SegPhrase [13] outperforms many other approaches [9, 20, 25, 5, 19, 6], but still needs domain experts to first carefully select hundreds of varying-quality phrases from millions of candidates, and then annotate them with binary labels.", "startOffset": 86, "endOffset": 107}, {"referenceID": 21, "context": ", more than 20 languages in TreeTagger [22]3), the POS-guided phrasal segmentation can be applied in many scenarios.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "In contrast to keyphrase extraction [17, 23, 14], this task goes beyond the scope of single documents and provides useful cross-document signals.", "startOffset": 36, "endOffset": 48}, {"referenceID": 22, "context": "In contrast to keyphrase extraction [17, 23, 14], this task goes beyond the scope of single documents and provides useful cross-document signals.", "startOffset": 36, "endOffset": 48}, {"referenceID": 13, "context": "In contrast to keyphrase extraction [17, 23, 14], this task goes beyond the scope of single documents and provides useful cross-document signals.", "startOffset": 36, "endOffset": 48}, {"referenceID": 8, "context": "de/~schmid/tools/ TreeTagger/ has conducted extensive studies typically referred to as automatic term recognition [9, 20, 25], for the computational task of extracting terms (such as technical phrases).", "startOffset": 114, "endOffset": 125}, {"referenceID": 19, "context": "de/~schmid/tools/ TreeTagger/ has conducted extensive studies typically referred to as automatic term recognition [9, 20, 25], for the computational task of extracting terms (such as technical phrases).", "startOffset": 114, "endOffset": 125}, {"referenceID": 24, "context": "de/~schmid/tools/ TreeTagger/ has conducted extensive studies typically referred to as automatic term recognition [9, 20, 25], for the computational task of extracting terms (such as technical phrases).", "startOffset": 114, "endOffset": 125}, {"referenceID": 6, "context": "This topic also attracts attention in the information retrieval (IR) community [7, 19] since selecting appropriate indexing terms is critical to the improvement of search engines where the ideal indexing units represent the main concepts in a corpus, not just literal bag-of-words.", "startOffset": 79, "endOffset": 86}, {"referenceID": 18, "context": "This topic also attracts attention in the information retrieval (IR) community [7, 19] since selecting appropriate indexing terms is critical to the improvement of search engines where the ideal indexing units represent the main concepts in a corpus, not just literal bag-of-words.", "startOffset": 79, "endOffset": 86}, {"referenceID": 20, "context": "Supervised noun phrase chunking techniques [21, 24, 3] exploit such tagged documents to automatically learn rules for identifying noun phrase boundaries.", "startOffset": 43, "endOffset": 54}, {"referenceID": 23, "context": "Supervised noun phrase chunking techniques [21, 24, 3] exploit such tagged documents to automatically learn rules for identifying noun phrase boundaries.", "startOffset": 43, "endOffset": 54}, {"referenceID": 2, "context": "Supervised noun phrase chunking techniques [21, 24, 3] exploit such tagged documents to automatically learn rules for identifying noun phrase boundaries.", "startOffset": 43, "endOffset": 54}, {"referenceID": 10, "context": "Other methods may utilize more sophisticated NLP technologies such as dependency parsing to further enhance the precision [11, 16].", "startOffset": 122, "endOffset": 130}, {"referenceID": 15, "context": "Other methods may utilize more sophisticated NLP technologies such as dependency parsing to further enhance the precision [11, 16].", "startOffset": 122, "endOffset": 130}, {"referenceID": 24, "context": "Some methods rely on other reference corpora for the calibration of \u201ctermhood\u201d [25].", "startOffset": 79, "endOffset": 83}, {"referenceID": 4, "context": "To overcome this limitation, data-driven approaches opt instead to make use of frequency statistics in the corpus to address both candidate generation and quality estimation [5, 19, 6, 13].", "startOffset": 174, "endOffset": 188}, {"referenceID": 18, "context": "To overcome this limitation, data-driven approaches opt instead to make use of frequency statistics in the corpus to address both candidate generation and quality estimation [5, 19, 6, 13].", "startOffset": 174, "endOffset": 188}, {"referenceID": 5, "context": "To overcome this limitation, data-driven approaches opt instead to make use of frequency statistics in the corpus to address both candidate generation and quality estimation [5, 19, 6, 13].", "startOffset": 174, "endOffset": 188}, {"referenceID": 12, "context": "To overcome this limitation, data-driven approaches opt instead to make use of frequency statistics in the corpus to address both candidate generation and quality estimation [5, 19, 6, 13].", "startOffset": 174, "endOffset": 188}, {"referenceID": 12, "context": "Instead, they rely on large corpora containing hundreds of thousands of documents to help deliver superior performance [13].", "startOffset": 119, "endOffset": 123}, {"referenceID": 18, "context": "In [19], several indicators, including frequency and comparison to super/sub-sequences, were proposed to extract n-grams that reliably indicate frequent, concise concepts.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "Deane [5] proposed a heuristic metric over frequency distribution based on Zipfian ranks, to measure lexical association for phrase candidates.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "[6] proposed to mine significant phrases based on frequency as well as document context following a bottom-up fashion, which only considers a part of quality phrase criteria, popularity and concordance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Our previous work [13] succeeded at integrating phrase quality estimation with phrasal segmentation to further rectify the initial set of statistical features, based on local occurrence context.", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "Unlike previous methods which are purely unsupervised, [13] required a small set of phrase labels to train its phrase quality estimator.", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "The work flow is completely different form our previous domainindependent phrase mining method requiring human effort [13], although the phrase candidates and the features used during phrase quality (re-)estimation are the same.", "startOffset": 118, "endOffset": 122}, {"referenceID": 7, "context": "A phrase is defined as a sequence of words that appear consecutively in the text, forming a complete semantic unit in certain contexts of the given documents [8].", "startOffset": 158, "endOffset": 161}, {"referenceID": 12, "context": "The phrase quality is defined to be the probability of a word sequence being a complete semantic unit, meeting the following criteria [13]: \u2022 Popularity: Quality phrases should occur with sufficient frequency in the given document collection.", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "wn) \u2208 [0, 1] where dw1w2 .", "startOffset": 6, "endOffset": 12}, {"referenceID": 12, "context": "1 Robust Positive-Only Distant Training To estimate the phrase quality score for each phrase candidate, our previous work [13] required domain experts to first carefully select hundreds of varying-quality phrases from millions of candidates, and then annotate them with binary labels.", "startOffset": 122, "endOffset": 126}, {"referenceID": 1, "context": "com/kno10/WikipediaEntities all phrase candidates is called a perturbed training set [2], because the labels of some (\u03b4 in the figure) quality phrases are switched from positive to negative.", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "An interesting property of this sampling procedure is that the random selection of phrase candidates for building perturbed training sets creates classifiers that have statistically independent errors and similar erring probability [2, 15].", "startOffset": 232, "endOffset": 239}, {"referenceID": 14, "context": "An interesting property of this sampling procedure is that the random selection of phrase candidates for building perturbed training sets creates classifiers that have statistically independent errors and similar erring probability [2, 15].", "startOffset": 232, "endOffset": 239}, {"referenceID": 9, "context": "Therefore, we naturally adopt random forest [10], which is verified, in the statistics literature, to be robust and efficient.", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "Compared to the phrasal segmentation in our previous work [13], the POS-guided phrasal segmentation addresses the completeness requirement in a context-aware way, instead of equivalently penalizing phrase candidates of the same length.", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "wrc|t) \u2208 [0, 1]", "startOffset": 9, "endOffset": 15}, {"referenceID": 12, "context": "Note that the length penalty model in our previous work [13] is a special case when all values of \u03b4(tx, ty) are the same.", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "Since the phrase segments function as a constituent in the syntax of a sentence, they usually have weak dependence on each other [8, 13].", "startOffset": 129, "endOffset": 136}, {"referenceID": 12, "context": "Since the phrase segments function as a constituent in the syntax of a sentence, they usually have weak dependence on each other [8, 13].", "startOffset": 129, "endOffset": 136}, {"referenceID": 0, "context": "As shown in Algorithm 2, we choose Viterbi Training, or Hard EM in literature [1] to iteratively optimize parameters, because Viterbi Training converges fast and results in sparse and simple models for Hidden Markov Model-like tasks [1].", "startOffset": 78, "endOffset": 81}, {"referenceID": 0, "context": "As shown in Algorithm 2, we choose Viterbi Training, or Hard EM in literature [1] to iteratively optimize parameters, because Viterbi Training converges fast and results in sparse and simple models for Hidden Markov Model-like tasks [1].", "startOffset": 233, "endOffset": 236}, {"referenceID": 5, "context": "SegPhrase10/WrapSegPhrae11: In English domain-specific text corpora, our latest work SegPhrase outperformed phrase mining [6], keyphrase extraction [23, 19], and noun phrase chunking methods.", "startOffset": 122, "endOffset": 125}, {"referenceID": 22, "context": "SegPhrase10/WrapSegPhrae11: In English domain-specific text corpora, our latest work SegPhrase outperformed phrase mining [6], keyphrase extraction [23, 19], and noun phrase chunking methods.", "startOffset": 148, "endOffset": 156}, {"referenceID": 18, "context": "SegPhrase10/WrapSegPhrae11: In English domain-specific text corpora, our latest work SegPhrase outperformed phrase mining [6], keyphrase extraction [23, 19], and noun phrase chunking methods.", "startOffset": 148, "endOffset": 156}, {"referenceID": 17, "context": "Parsers of all three languages are available in Stanford NLP tools [18, 4, 12].", "startOffset": 67, "endOffset": 78}, {"referenceID": 3, "context": "Parsers of all three languages are available in Stanford NLP tools [18, 4, 12].", "startOffset": 67, "endOffset": 78}, {"referenceID": 11, "context": "Parsers of all three languages are available in Stanford NLP tools [18, 4, 12].", "startOffset": 67, "endOffset": 78}, {"referenceID": 16, "context": "Two ranking heuristics are considered: \u2022 TF-IDF ranks the extracted phrases by their term frequency and inverse document frequency in the given documents; \u2022 TextRank: An unsupervised graph-based ranking model for keyword extraction [17].", "startOffset": 232, "endOffset": 236}, {"referenceID": 12, "context": "AutoPhrase outperforms AutoSegPhrase even on the English dataset EN, though it has been shown the length penalty works reasonably well in English [13].", "startOffset": 146, "endOffset": 150}, {"referenceID": 7, "context": "In linguistic analysis, however, a phrase is not only a group of multiple words, but also possibly a single word, as long as it functions as a constituent in the syntax of a sentence [8].", "startOffset": 183, "endOffset": 186}], "year": 2017, "abstractText": "As one of the fundamental tasks in text analysis, phrase mining aims at extracting quality phrases from a text corpus. Phrase mining is important in various tasks such as information extraction/retrieval, taxonomy construction, and topic modeling. Most existing methods rely on complex, trained linguistic analyzers, and thus likely have unsatisfactory performance on text corpora of new domains and genres without extra but expensive adaption. Recently, a few data-driven methods have been developed successfully for extraction of phrases from massive domain-specific text. However, none of the state-of-the-art models is fully automated because they require human experts for designing rules or labeling phrases. Since one can easily obtain many quality phrases from public knowledge bases to a scale that is much larger than that produced by human experts, in this paper, we propose a novel framework for automated phrase mining, AutoPhrase, which leverages this large amount of high-quality phrases in an effective way and achieves better performance compared to limited human labeled phrases. In addition, we develop a POS-guided phrasal segmentation model, which incorporates the shallow syntactic information in part-of-speech (POS) tags to further enhance the performance, when a POS tagger is available. Note that, AutoPhrase can support any language as long as a general knowledge base (e.g., Wikipedia) in that language is available, while benefiting from, but not requiring, a POS tagger. Compared to the state-of-the-art methods, the new method has shown significant improvements in effectiveness on five real-world datasets across different domains and languages.", "creator": "LaTeX with hyperref package"}}}