{"id": "1702.04488", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2017", "title": "Transfer Deep Learning for Low-Resource Chinese Word Segmentation with a Novel Neural Network", "abstract": "Recent works have been shown effective in using neural networks for Chinese word segmentation. However, these models rely on large-scale data and are less effective for low-resource datasets because of insufficient training data. Thus, we propose a transfer learning method to improve low-resource word segmentation by leveraging high-resource corpora. First, we train a teacher model on high-resource corpora and then use the learned knowledge to initialize a student model. Second, a weighted data similarity method is proposed to train the student model on low-resource data with the help of high-resource corpora. Finally, given that insufficient data puts forward higher requirements for feature extraction, we propose a novel neural network which improves feature learning. Experiment results show that our work significantly improves the performance on low-resource datasets: 2.3% and 1.5% F-score on PKU and CTB datasets. Furthermore, this paper achieves state-of-the-art results: 96.1%, and 96.2% F-score on PKU and CTB datasets. Besides, we explore an asynchronous parallel method on neural word segmentation to speed up training. The parallel method accelerates training substantially and is almost five times faster than a serial mode.", "histories": [["v1", "Wed, 15 Feb 2017 07:37:55 GMT  (287kb,D)", "http://arxiv.org/abs/1702.04488v1", null], ["v2", "Thu, 16 Feb 2017 06:16:09 GMT  (287kb,D)", "http://arxiv.org/abs/1702.04488v2", null], ["v3", "Sun, 7 May 2017 12:53:13 GMT  (227kb,D)", "http://arxiv.org/abs/1702.04488v3", null], ["v4", "Wed, 17 May 2017 01:52:45 GMT  (227kb,D)", "http://arxiv.org/abs/1702.04488v4", null], ["v5", "Thu, 14 Sep 2017 11:10:13 GMT  (790kb,D)", "http://arxiv.org/abs/1702.04488v5", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jingjing xu", "xu sun"], "accepted": false, "id": "1702.04488"}, "pdf": {"name": "1702.04488.pdf", "metadata": {"source": "CRF", "title": "Transfer Learning for Low-Resource Chinese Word Segmentation with a Novel Neural Network", "authors": ["Jingjing Xu"], "emails": ["xusun}@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "2 Transfer Learning by Leveraging High-Resource Datasets", "text": "Table 1 shows that previous neural word segmentation models are less effective for low-resource datasets = adapted data for students because these models focus only on indomain-monitored learning. In addition, there are plenty of corpora that consist of massive annotated texts. For scenarios where we have insufficiently labeled data, transfer learning is an effective method to improve the task. Motivated by this, we propose a transfer learning method to use resource-intensive corporations. First, we propose a teacher-student model to initialize a model with the knowledge learned. We train a teacher model based on a dataset that contains a large amount of training data (e.g. MSR). The learned parameters are used to initialize a student model. Therefore, the student model is trained on the parameters learned and not initialized randomly. Second, the student model is trained on low-resource data."}, {"heading": "3 Unified Global-Local Neural Networks for Feature Extraction", "text": "Unlike previous networks, which focus on a single type of trait: either complicated local traits or global dependencies, our network has an advantage in combining complicated local traits with long dependencies, both of which are necessary for CWS and should not be neglected. Our network is built on a simple encoder structure that uses the number of words in local combinations and a decoder to capture long dependencies. Figure 1 illustrates the model architecture. First, words are represented by embedding, stored in a search table."}, {"heading": "4 Mini-Batch Asynchronous Parallel Learning", "text": "With the development of multicore computers, there is a growing interest in parallel techniques. Researchers have proposed several schemes (Zhao and Huang, 2013; Zinkevich et al., 2010), but most of them require a latch so that acceleration is limited. Asynchronous parallel learning methods without latch can maximize acceleration ratio. Existing asynchronous parallel learning methods, however, are mainly for sparse models. In dense models, such as neural networks, asynchronous parallel learning processes produce gradient noises that are very frequent and inevitable (Figure 2). Reading and reading conflicts interrupt the sequence of learning processes, and reading and writing processes lead to incorrect gradients. However, Sun (2016) has shown that the learning process can still be consistent with gradient errors (Figure 2)."}, {"heading": "5 Experiments", "text": "The proposed model is evaluated on the basis of three sets of data: MSR, PKU and CTB. Table 3 shows the details of these data sets. We treat MSR as a resource-intensive data set, PKU and CTB as resource-poor data sets. MSR and PKU are provided by the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005). CTB comes from the Chinese TreeBank 8.02 and is divided into training and testing sets in this work. Randomly, 10% of the training sets are divided into development sets that are used to select the appropriate hyperparameters. All idioms, numbers and continuous English2https: / / katalog.ldc.upenn.edu / LDC2013T21 characters are replaced by special flags. Improvements achieved by an idiom dictionary are very limited, less than 0.1% F-score on all data sets."}, {"heading": "5.1 Setup", "text": "We evaluate the minibatch size m in serial mode and select m = 16. Similarly, the window size w is set to 5, the fixed learning rate \u03b1 to 0.01, the dimension 3https: / / catalog.ldc.upenn.edu / LDC2003T09 to 4https: / / code.google.com / archive / p / word2veccharacter embedings and hidden layer d to 100. d = 100 is a good balance between model speed and performance. Inspired by Pei et al. (2014), Bigram functions are also applied to our model. Specifically, each Bigram embedding is presented as a single vector. Bigram embedding is initialized randomly. We ignore many Bigram features that occur only once or twice, as these Bigram features are not only useless, but also form a Bigram lookup table."}, {"heading": "5.2 Results and Discussions", "text": "This year, it has never been as good as it has been this year."}, {"heading": "6 Related Work", "text": "Next, we briefly review neural word segmentation, transfer learning in CWS, and asynchronous parallel learning methodology. Neural word segmentation. Zheng et al. (2013) used a two-tiered network and adapted a general neural network architecture in Collobert et al. (2011). However, Pei et al. (2014) used a tensor framework to capture the functional combinations. Chen et al. (2016) applied the transformation-based neural framework to Chinese segmentation. Cai and Zhao (2016) proposed a novel neural framework that uses context windows to thoroughly eliminate and complete segmentation histories. Transfer Learning in CWS. Sun and Xu (2011) introduced many statistical features ranging from unmarked in-domain OS data to synchronous parallel networks."}, {"heading": "7 Conclusions", "text": "The main problem with low-resource word segmentation is insufficient training data. Therefore, we propose a transfer learning method to improve the task by using resource-intensive data sets. First, it is difficult for a randomly initialized model to achieve the expected results on resource-poor data sets. Therefore, we propose a teacher-student framework for initializing a student model. Second, the student model is trained on a resource-poor data set with the help of resource-poor companies. To avoid shifting the data distribution, a weighted data similarity is proposed. Finally, our transfer learning method is implemented by a novel neural network that improves feature learning. Experimental results show that our work largely improves the performance of resource-poor data sets compared to state-of-the-art models. Finally, our parallel training method provides significant acceleration and is nearly 5x faster than a serial mode."}], "references": [{"title": "Neural word segmentation learning for chinese", "author": ["Deng Cai", "Hai Zhao."], "venue": "Meeting of the Association for Computational Linguistics.", "citeRegEx": "Cai and Zhao.,? 2016", "shortCiteRegEx": "Cai and Zhao.", "year": 2016}, {"title": "Gated recursive neural network for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Xuanjing Huang."], "venue": "ACL (1). The Association for Computer Linguistics, pages 1744\u20131753.", "citeRegEx": "Chen et al\\.,? 2015a", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang."], "venue": "EMNLP. The Association for Computational Linguistics, pages 1197\u20131206.", "citeRegEx": "Chen et al\\.,? 2015b", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "J. Mach. Learn. Res. 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "The second international chinese word segmentation bakeoff", "author": ["Thomas Emerson."], "venue": "Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing. pages 123\u2013133.", "citeRegEx": "Emerson.,? 2005", "shortCiteRegEx": "Emerson.", "year": 2005}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Computer Science .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning. Num-", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Domain adaptation for crf-based chinese word segmentation using free annotations", "author": ["Yijia Liu", "Yue Zhang", "Wanxiang Che", "Ting Liu", "Fan Wu."], "venue": "Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, EMNLP. ACL, pages 864\u2013874.", "citeRegEx": "Liu et al\\.,? 2014", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Accurate linear-time chinese word segmentation via embedding matching", "author": ["Jianqiang Ma", "Erhard W Hinrichs."], "venue": "ACL (1). pages 1733\u20131743.", "citeRegEx": "Ma and Hinrichs.,? 2015", "shortCiteRegEx": "Ma and Hinrichs.", "year": 2015}, {"title": "Delaytolerant algorithms for asynchronous distributed online learning", "author": ["H.B. Mcmahan", "M. Streeter."], "venue": "Advances in Neural Information Processing Systems 4:2915\u20132923.", "citeRegEx": "Mcmahan and Streeter.,? 2014", "shortCiteRegEx": "Mcmahan and Streeter.", "year": 2014}, {"title": "Maxmargin tensor neural network for chinese word segmentation", "author": ["Wenzhe Pei", "Tao Ge", "Baobao Chang."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computa-", "citeRegEx": "Pei et al\\.,? 2014", "shortCiteRegEx": "Pei et al\\.", "year": 2014}, {"title": "Chinese segmentation and new word detection using conditional random fields", "author": ["Fuchun Peng", "Fangfang Feng", "Andrew McCallum."], "venue": "Proceedings of the 20th International Conference on Computational Linguistics. Association for Computational", "citeRegEx": "Peng et al\\.,? 2004", "shortCiteRegEx": "Peng et al\\.", "year": 2004}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Benjamin Recht", "Christopher R\u00e9", "Stephen J. Wright", "Feng Niu."], "venue": "NIPS. pages 693\u2013701.", "citeRegEx": "Recht et al\\.,? 2011", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Enhancing chinese word segmentation using unlabeled data", "author": ["Weiwei Sun", "Jia Xu."], "venue": "Conference", "citeRegEx": "Sun and Xu.,? 2011", "shortCiteRegEx": "Sun and Xu.", "year": 2011}, {"title": "Structure regularization for structured prediction", "author": ["Xu Sun."], "venue": "Advances in Neural Information Processing Systems 27. pages 2402\u20132410.", "citeRegEx": "Sun.,? 2014", "shortCiteRegEx": "Sun.", "year": 2014}, {"title": "Asynchronous parallel learning for neural networks and structured models with dense features", "author": ["Xu Sun."], "venue": "COLING.", "citeRegEx": "Sun.,? 2016", "shortCiteRegEx": "Sun.", "year": 2016}, {"title": "Feature-frequency-adaptive on-line training for fast and accurate natural language processing", "author": ["Xu Sun", "Wenjie Li", "Houfeng Wang", "Qin Lu."], "venue": "Computational Linguistics 40(3):563\u2013586.", "citeRegEx": "Sun et al\\.,? 2014", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection", "author": ["Xu Sun", "Houfeng Wang", "Wenjie Li."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Sun et al\\.,? 2012", "shortCiteRegEx": "Sun et al\\.", "year": 2012}, {"title": "A conditional random field word segmenter", "author": ["Huihsin Tseng."], "venue": "In Fourth SIGHAN Workshop on Chinese Language Processing.", "citeRegEx": "Tseng.,? 2005", "shortCiteRegEx": "Tseng.", "year": 2005}, {"title": "Chinese Word Segmentation as LMR Tagging", "author": ["N. Xue", "L. Shen."], "venue": "Proceedings of the 2nd SIGHAN Workshop on Chinese Language Processing.", "citeRegEx": "Xue and Shen.,? 2003", "shortCiteRegEx": "Xue and Shen.", "year": 2003}, {"title": "Type-supervised domain adaptation for joint segmentation and pos-tagging", "author": ["Meishan Zhang", "Yue Zhang", "Wanxiang Che", "Ting Liu."], "venue": "EACL. pages 588\u2013597.", "citeRegEx": "Zhang et al\\.,? 2014", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Transition-based neural word segmentation", "author": ["Meishan Zhang", "Yue Zhang", "Guohong Fu."], "venue": "Meeting of the Association for Computational Linguistics. pages 421\u2013431.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Subword-based tagging by conditional random fields for chinese word segmentation", "author": ["Ruiqiang Zhang", "Genichiro Kikui", "Eiichiro Sumita."], "venue": "Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume:", "citeRegEx": "Zhang et al\\.,? 2006", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}, {"title": "Chinese segmentation with a word-based perceptron algorithm", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics, Prague, Czech Re-", "citeRegEx": "Zhang and Clark.,? 2007", "shortCiteRegEx": "Zhang and Clark.", "year": 2007}, {"title": "A unified character-based tagging framework for chinese word segmentation", "author": ["Hai Zhao", "Changning Huang", "Mu Li", "Bao-Liang Lu."], "venue": "ACM Trans. Asian Lang. Inf. Process. 9(2).", "citeRegEx": "Zhao et al\\.,? 2010", "shortCiteRegEx": "Zhao et al\\.", "year": 2010}, {"title": "Minibatch and parallelization for online large margin structured learning", "author": ["Kai Zhao", "Liang Huang."], "venue": "HLT-NAACL. The Association for Computational Linguistics, pages 370\u2013379.", "citeRegEx": "Zhao and Huang.,? 2013", "shortCiteRegEx": "Zhao and Huang.", "year": 2013}, {"title": "Deep learning for chinese word segmentation and pos tagging", "author": ["Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu."], "venue": "EMNLP. ACL, pages 647\u2013657.", "citeRegEx": "Zheng et al\\.,? 2013", "shortCiteRegEx": "Zheng et al\\.", "year": 2013}, {"title": "Parallelized stochastic gradient descent", "author": ["Martin Zinkevich", "Markus Weimer", "Lihong Li", "Alex J. Smola."], "venue": "J.D. Lafferty, C.K.I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing", "citeRegEx": "Zinkevich et al\\.,? 2010", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 19, "context": "com/jincy520/Low-Resource-CWS The most widely used approaches (Xue and Shen, 2003; Peng et al., 2004) treat CWS as a sequence labelling problem in which each character is assigned with a tag.", "startOffset": 62, "endOffset": 101}, {"referenceID": 11, "context": "com/jincy520/Low-Resource-CWS The most widely used approaches (Xue and Shen, 2003; Peng et al., 2004) treat CWS as a sequence labelling problem in which each character is assigned with a tag.", "startOffset": 62, "endOffset": 101}, {"referenceID": 6, "context": "Many exsiting techniques, such as conditional random fields, have been successfully applied to CWS (Lafferty et al., 2001; Tseng, 2005; Zhao et al., 2010; Sun and Xu, 2011; Sun et al., 2014).", "startOffset": 99, "endOffset": 190}, {"referenceID": 18, "context": "Many exsiting techniques, such as conditional random fields, have been successfully applied to CWS (Lafferty et al., 2001; Tseng, 2005; Zhao et al., 2010; Sun and Xu, 2011; Sun et al., 2014).", "startOffset": 99, "endOffset": 190}, {"referenceID": 24, "context": "Many exsiting techniques, such as conditional random fields, have been successfully applied to CWS (Lafferty et al., 2001; Tseng, 2005; Zhao et al., 2010; Sun and Xu, 2011; Sun et al., 2014).", "startOffset": 99, "endOffset": 190}, {"referenceID": 13, "context": "Many exsiting techniques, such as conditional random fields, have been successfully applied to CWS (Lafferty et al., 2001; Tseng, 2005; Zhao et al., 2010; Sun and Xu, 2011; Sun et al., 2014).", "startOffset": 99, "endOffset": 190}, {"referenceID": 16, "context": "Many exsiting techniques, such as conditional random fields, have been successfully applied to CWS (Lafferty et al., 2001; Tseng, 2005; Zhao et al., 2010; Sun and Xu, 2011; Sun et al., 2014).", "startOffset": 99, "endOffset": 190}, {"referenceID": 0, "context": "Collobert et al. (2011) developed a general neural architecture for sequence labelling tasks.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Collobert et al. (2011) developed a general neural architecture for sequence labelling tasks. Pei et al. (2014) used convolutioanl neural networks to capture local features within a fixed size window.", "startOffset": 0, "endOffset": 112}, {"referenceID": 0, "context": "Chen et al. (2015a) proposed gated recursive neural networks to model feature combinations.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "The gating mechanism was also used by Cai and Zhao (2016).", "startOffset": 38, "endOffset": 58}, {"referenceID": 17, "context": "MSR 86919 3985 Zhang and Clark (2007) 97.", "startOffset": 15, "endOffset": 38}, {"referenceID": 12, "context": "2 Sun et al. (2012) 97.", "startOffset": 2, "endOffset": 20}, {"referenceID": 9, "context": "4 Pei et al. (2014) 97.", "startOffset": 2, "endOffset": 20}, {"referenceID": 0, "context": "2 Cai and Zhao (2016) 96.", "startOffset": 2, "endOffset": 22}, {"referenceID": 17, "context": "PKU 19055 1945 Zhang and Clark (2007) 94.", "startOffset": 15, "endOffset": 38}, {"referenceID": 12, "context": "5 Sun et al. (2012) 95.", "startOffset": 2, "endOffset": 20}, {"referenceID": 9, "context": "4 Pei et al. (2014) 95.", "startOffset": 2, "endOffset": 20}, {"referenceID": 0, "context": "2 Cai and Zhao (2016) 95.", "startOffset": 2, "endOffset": 22}, {"referenceID": 12, "context": "Existing asynchronous parallel learning methods are mainly for sparse models (Recht et al., 2011; Mcmahan and Streeter, 2014).", "startOffset": 77, "endOffset": 125}, {"referenceID": 9, "context": "Existing asynchronous parallel learning methods are mainly for sparse models (Recht et al., 2011; Mcmahan and Streeter, 2014).", "startOffset": 77, "endOffset": 125}, {"referenceID": 9, "context": ", 2011; Mcmahan and Streeter, 2014). For dense models, like neural networks, asynchronous parallel methods bring inevitable gradient noises. However, the theoretical analysis by Sun (2016) showed that the learning process with gradient errors can still be convergent on neural models.", "startOffset": 8, "endOffset": 189}, {"referenceID": 1, "context": "Unlike GRNN (Chen et al., 2015a) which has a limit that inputs must be two vectors, filter recursive networks are proposed to break this limit by introducing filter mechanism which controls the input size.", "startOffset": 12, "endOffset": 32}, {"referenceID": 0, "context": "Unlike GRNN (Chen et al., 2015a) which has a limit that inputs must be two vectors, filter recursive networks are proposed to break this limit by introducing filter mechanism which controls the input size. Motivated by Chen et al. (2015a) and Cai and Zhao (2016), the gating mechanism has been shown effective to model feature combinations.", "startOffset": 13, "endOffset": 239}, {"referenceID": 0, "context": "(2015a) and Cai and Zhao (2016), the gating mechanism has been shown effective to model feature combinations.", "startOffset": 12, "endOffset": 32}, {"referenceID": 25, "context": "Researchers have proposed several schemes (Zhao and Huang, 2013; Zinkevich et al., 2010), but most of them require locking so the speedup is limited.", "startOffset": 42, "endOffset": 88}, {"referenceID": 27, "context": "Researchers have proposed several schemes (Zhao and Huang, 2013; Zinkevich et al., 2010), but most of them require locking so the speedup is limited.", "startOffset": 42, "endOffset": 88}, {"referenceID": 5, "context": "We find that Adam (Kingma and Ba, 2014) is a practical method to train large neural networks.", "startOffset": 18, "endOffset": 39}, {"referenceID": 13, "context": "Nevertheless, Sun (2016) proved that the learning process with gradient errors can still be convergent.", "startOffset": 14, "endOffset": 25}, {"referenceID": 14, "context": "Motivated by structure regularization (Sun, 2014), we split each sentence into several fixed-length mini-sentences.", "startOffset": 38, "endOffset": 49}, {"referenceID": 4, "context": "MSR and PKU are provided by the second International Chinese Word Segmentation Bakeoff (Emerson, 2005).", "startOffset": 87, "endOffset": 102}, {"referenceID": 10, "context": "Inspired by Pei et al. (2014), bigram features are applied to our model as well.", "startOffset": 12, "endOffset": 30}, {"referenceID": 24, "context": "Unigram Zheng et al. (2013) 92.", "startOffset": 8, "endOffset": 28}, {"referenceID": 9, "context": "4 * * * Pei et al. (2014) 94.", "startOffset": 8, "endOffset": 26}, {"referenceID": 0, "context": "0 * * * Cai and Zhao (2016) 95.", "startOffset": 8, "endOffset": 28}, {"referenceID": 9, "context": "Bigram Pei et al. (2014) * * 95.", "startOffset": 7, "endOffset": 25}, {"referenceID": 8, "context": "2 * * * Ma and Hinrichs (2015) * * 95.", "startOffset": 8, "endOffset": 31}, {"referenceID": 8, "context": "2 * * * Ma and Hinrichs (2015) * * 95.1 * * * Zhang et al. (2016) * * 95.", "startOffset": 8, "endOffset": 66}, {"referenceID": 14, "context": "5 * * * Sun et al. (2012) * * 95.", "startOffset": 8, "endOffset": 26}, {"referenceID": 1, "context": "Given that a dictionary used in Chen et al. (2015a) is not publicly released, our work is not comparable with it.", "startOffset": 32, "endOffset": 52}, {"referenceID": 22, "context": "Zheng et al. (2013) used a two-layer network and adapted a general neural network architecture in Collobert et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "(2013) used a two-layer network and adapted a general neural network architecture in Collobert et al. (2011). Pei et al.", "startOffset": 85, "endOffset": 109}, {"referenceID": 1, "context": "(2013) used a two-layer network and adapted a general neural network architecture in Collobert et al. (2011). Pei et al. (2014) used a tensor framework to capture feature combination.", "startOffset": 85, "endOffset": 128}, {"referenceID": 1, "context": "Chen et al. (2015a) proposed gated recursive neural networks to model feature combinations of context characters.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Chen et al. (2015a) proposed gated recursive neural networks to model feature combinations of context characters. Chen et al. (2015b) used LSTM to model long distance dependencies in a sentence.", "startOffset": 0, "endOffset": 134}, {"referenceID": 0, "context": "Cai and Zhao (2016) proposed a novel neural framework which thoroughly eliminated context win-", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Sun and Xu (2011) introduced many statistical features from unlabeled in-domain data to enhance supervised method in CWS.", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "Sun and Xu (2011) introduced many statistical features from unlabeled in-domain data to enhance supervised method in CWS. Zhang et al.(2014) used type-supervised domain adaptation for joint Chinese word segmentation and POS-tagging.", "startOffset": 0, "endOffset": 141}, {"referenceID": 7, "context": "Liu et al. (2014) adopted freely available data to help improve the performance on CWS.", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "Recently, a viriety of asynchronous parallel learning methods have been developed (Recht et al., 2011; Mcmahan and Streeter, 2014).", "startOffset": 82, "endOffset": 130}, {"referenceID": 9, "context": "Recently, a viriety of asynchronous parallel learning methods have been developed (Recht et al., 2011; Mcmahan and Streeter, 2014).", "startOffset": 82, "endOffset": 130}, {"referenceID": 9, "context": ", 2011; Mcmahan and Streeter, 2014). Those asynchronous methods have shown to be more effective than synchronous parallel learning. However, existing asynchronous parallel learning methods are mainly for sparse parameter models to avoid the problem of gradient error. For dense parameter models like neural networks, asynchronous parallel methods bring gradient errors. The theoretical analysis work of Sun (2016) showed that the learning process with gradient errors can still be convergent on neural models.", "startOffset": 8, "endOffset": 414}], "year": 2017, "abstractText": "Recent works have been shown effective in using neural networks for Chinese word segmentation. However, these models rely on large-scale data and are less effective for low-resource datasets because of insufficient training data. Thus, we propose a transfer learning method to improve low-resource word segmentation by leveraging high-resource corpora. First, we train a teacher model on high-resource corpora and then use the learned knowledge to initialize a student model. Second, a weighted data similarity method is proposed to train the student model on low-resource data with the help of highresource corpora. Finally, given that insufficient data puts forward higher requirements for feature extraction, we propose a novel neural network which improves feature learning. Experiment results show that our work significantly improves the performance on low-resource datasets: 2.3% and 1.5% F-score on PKU and CTB datasets. Furthermore, this paper achieves state-of-the-art results: 96.1%, and 96.2% F-score on PKU and CTB datasets1. Besides, we explore an asynchronous parallel method on neural word segmentation to speed up training. The parallel method accelerates training substantially and is almost five times faster than a serial mode.", "creator": "LaTeX with hyperref package"}}}