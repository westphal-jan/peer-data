{"id": "1702.04683", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2017", "title": "Distributed deep learning on edge-devices: feasibility via adaptive compression", "abstract": "The state-of-the-art results provided by deep learning come at the price of an intensive use of computing resources. The leading frameworks (eg., TensorFlow) are executed on GPUs or on high-end servers in datacenters. On the other end, there is a proliferation of personal devices with possibly free CPU cycles. In this paper, we ask the following question: Is distributed deep learning computation on WAN connected devices feasible, in spite of the traffic caused by learning tasks? We show that such a setup rises some important challenges, most notably the ingress traffic that the servers hosting the up-to-date model have to sustain. In order to reduce this stress, we propose AdaComp, a new algorithm for compressing worker updates to the model on the server. Applicable to stochastic gradient descent based approaches, it combines efficient gradient selection and learning rate modulation. We then experiment and measure the impact of compression and device reliability on the accuracy of learned models. To do so, we leverage an emulator platform we developed, that embeds the TensorFlow code into Linux containers. We report a reduction of the total amount of data sent by workers to the server by two order of magnitude (eg., 191-fold reduction for a convolutional network on the MNIST dataset), when compared to the standard algorithm based on asynchronous stochastic gradient descent, while maintaining model accuracy.", "histories": [["v1", "Wed, 15 Feb 2017 16:57:24 GMT  (1431kb,D)", "http://arxiv.org/abs/1702.04683v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["corentin hardy", "erwan le merrer", "bruno sericola"], "accepted": false, "id": "1702.04683"}, "pdf": {"name": "1702.04683.pdf", "metadata": {"source": "CRF", "title": "Distributed deep learning on edge-devices: feasibility via adaptive compression", "authors": ["Corentin Hardy", "Erwan Le Merrer", "Bruno Sericola", "\u2217Technicolor \u2020INRIA"], "emails": [], "sections": [{"heading": null, "text": "This year, it has come to the point that it will only be once before there is such a process, in which there is such a process."}, {"heading": "II. DISTRIBUTED DEEP LEARNING AND THE PARAMETER SERVER MODEL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Basics on DNN training on a single core", "text": "Deep Neural Networks (or DNNs) are machine learning models used for supervised or unsupervised tasks. They consist of a large sequence of layers. Each layer l is the result of a nonlinear transformation of the previous layer (or input data), which is provided with a weight matrix Vl and a bias vector \u03b2l. In this thesis, we focus on supervised learning, using a DNN to approximate a target function f (e.g. a classification function for images). For example, a DNN must adjust its parametric function f \u0432: X \u2192 Y, with a vector containing the set of all parameters, i.e., all inputs of matrices Vl and vectors \u03b2l. The training is performed by minimizing a loss function L, which shows how good the DNN is at approaching f.L."}, {"heading": "B. Distributed Stochastic Gradient Descent in the Parameter Server Model", "text": "In order to speed up the DNN training, Dean et al. have proposed in [7] the parameter server model as a way to distribute the calculation across up to a hundred machines. The main idea is the parallelization of the data calculation: Each computing node (or worker) processes a subset of the training data asynchronously. For the sake of simplicity, we report on the core notations in Table I. The parameter server model is presented in Figure 2. In a nutshell, the PS starts with the initialization of the vector of parameters in order to learn, as well as with the distribution of the training data D to the workers w1, w2, w3. Each worker runs asynchronously, and whenever it is ready, the current version of the parameter vector from the PS takes an SGD step, performs an SGD step and sends the vector update (which reflects the learning on its data) to the PS. In Figure 2, the worker receives the status of the update w2 \u2212 3 step, a GD is sent simultaneously."}, {"heading": "III. DISTRIBUTED DEEP LEARNING ON EDGE-DEVICES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Execution setup", "text": "In this context, we assume that the training data is with the employees; for example, this serves as a basis for privacy-preserving scenarios [25] in which the users have their photos at home and want to participate in the calculation of a global model for photo classification, without, however, sending their personal data to a cloud service. The formal execution model remains identical to the PS model: workers have enough memory to host a copy of the data, the n-workers are shuffled, and finally they agree to participate in the task of machine learning (tolerance for malicious behavior does not fall within the scope of this paper). The strict aspect of our setup is the lower connectivity capacity of the workers and their expected lower reliability [27]. In terms of connectivity, therefore, we have a DL device or an ASb upload cable with a capacity of 100 MB, i.e. we send the cable to the point where it is pulled up."}, {"heading": "B. Staleness mitigation", "text": "Asynchronous vector calls and updates of workers cause disruption of the SGD due to the durability of local SGDs (w). Durability, proportional to the number of workers (see [28], [16] or [8] for a more in-depth explanation of the phenomenon), is an important problem for asynchronous SGDs in general. High durability is to be avoided, as workers who are relatively slow contribute to a durability that has (possibly significantly) evolved since they last fetched their copy of SGD. This factor is known to slow the convergence of calculations [28]. To cope with this, the work [28] and [20] suggest adjusting the learning rate \u03b1 depending on the current durability in order to reduce the impact of outdated updates, which also reduces the number of updates required for training."}, {"heading": "C. Reducing ingress traffic at the PS", "text": "This year is the highest in the history of the country."}, {"heading": "V. EXPERIMENTAL EVALUATION", "text": "In this section, we present the results obtained by AdaComp on two different DNNs and compare them with those of two competitors (described in Section V-B).2Please note that the best low-level data representation for update encoding is outside the scope of this essay: at best, it leads to a reduction in a small factor of Python serialization (i.e., the TensorFlow language), while in this essay, we aim for two orders of magnitude reduction of the network footprint by targeting the algorithmic scope of distributed deep learning. Algorithm 2 AdaComp on the parameter server 1: Procedure PS (\u03b1, I) 2: Initialize your data (0) with random values. 3: for i \u2190 0, I 4: Get a push 5: \u03b1 (i)."}, {"heading": "A. Experimental platform", "text": "To evaluate the value of AdaComp in a controlled and monitored environment, we choose to emulate the entire system on a single powerful server. The server allows us to eliminate the hardware and configuration constraints. We choose to represent edge devices through Linux containers (LXC) on a Debian high-end server (Intel (R) Xeon (R) CPU E5-2667 v3 @ 3.20 GHz CPUs, a total of 32 cores and 1 / 2 TB of RAM. Each of them runs a TensorFlow session to train the DNN3 locally. Traffic between LXCs can then be managed by the host machine with virtual Ethernet connections (or Veth). An experiment on this platform is as follows. A set of n LXC containers is used to represent the workers who have access to a ratio of 1 / n of the training data in a container."}, {"heading": "B. Experimental setup, metrics and competitors", "text": "We are experimenting with our setup using the MNIST dataset [15], which is also used by our competitor [25]. The goal is to build an image classifier that can recognize handwritten digits (i.e., 10 classes). The dataset consists of a training dataset with 60,000 images, i.e., 6,000 per class, and a test dataset with 10, 0003. Note that TensorFlow is also run in containers while running in a data center environment [1]. Each image is represented by 28x28 pixels with an 8-bit grayscale matrix. Experiments start n = 200 workers, and a PS corresponding to the scale of the operation that the TensorFlow paper [2].1) Learning with two models: We are testing two different DNN models to build the classifier. The first is a multi-layer Percepon (MLP) that contains three fully connected layers."}, {"heading": "C. Accuracy results", "text": "All experiments are carried out until the PS has received a total of 250,000 updates from the workers, which is a ceiling for the convergence of the best approaches we present in this section. Most experiments have been carried out 3 times; the diagrams show mean, minimum and maximum values of moving average accuracy for each run. The final test accuracy for an experiment is the mean of the maximum accuracy achieved by each individual runner. The first experiment is to classify the MNIST model. Figure 3 (left) shows the accuracy of the model, which depends on the number of participants. Figure 3 (right) reports on the accuracy as a function of attack traffic measured by the PS. ASGD, Comp-ASGD and AdaComp are experimented within the same setup. For ASGD, we use two different batch sizes: b = 1 (representing the baseline value we assume for all others)."}, {"heading": "VI. RELATED WORK", "text": "The parameter server is popular for distributing SGD with e.g. DistBelief [7], Adam [4], and TensorFlow [2] as the most important ones. Increasing the number of workers in such an asynchronous environment leads to maintenance problems that are synchronized either algorithmically or by means of 2). In [28], W. Zhang et al. proposes to adjust the learning rate according to the current durability for each new update. They divide the learning rate by durability to limit the effects of highly obsolete updates."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we discussed the main implications of performing distributed deep learning training tasks on edge devices. Due to device upload limitations and their lower reliability in such a configuration, asynchronous SGD is a natural solution for performing learning tasks; however, we emphasized that the amount of data that must be transferred over the Internet is considerable, especially the intrusive traffic from PS if no action is taken to customize algorithms. We suggested AdaComp, a new algorithm for compressing updates by adjusting to their individual parameter durability. We show that this leads to a 251 or 191-fold reduction in intrusive traffic on the parameter server, compared to the asynchronous SGD algorithm (for the MLP or CNN models on the MNIST dataset), and for an equally improved accuracy, we believe that reducing this large amount of data traffic would allow for the intrusive deployment of multiple parameters in the MNIST dataset."}], "references": [{"title": "Tensorflow: Largescale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I.J. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. J\u00f3zefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D.G. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P.A. Tucker", "V. Vanhoucke", "V. Vasudevan", "F.B. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": "CoRR, abs/1603.04467", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Tensorflow: A system for large-scale machine learning", "author": ["M. Abadi", "P. Barham", "J. Chen", "Z. Chen", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "G. Irving", "M. Isard", "M. Kudlur", "J. Levenberg", "R. Monga", "S. Moore", "D.G. Murray", "B. Steiner", "P. Tucker", "V. Vasudevan", "P. Warden", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": "OSDI", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Revisiting distributed synchronous SGD", "author": ["J. Chen", "R. Monga", "S. Bengio", "R. Jozefowicz"], "venue": "International Conference on Learning Representations Workshop Track", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["T. Chilimbi", "Y. Suzue", "J. Apacible", "K. Kalyanaraman"], "venue": "OSDI", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for youtube recommendations", "author": ["P. Covington", "J. Adams", "E. Sargin"], "venue": "Recys", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "M", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao"], "venue": "aurelio Ranzato, A. Senior, P. Tucker, K. Yang, Q. V. Le, and A. Y. Ng. Large scale distributed deep networks. In NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Model accuracy and runtime tradeoff in distributed deep learning: A systematic study", "author": ["S. Gupta", "W. Zhang", "F. Wang"], "venue": "In ICDM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Federated optimization: Distributed optimization beyond the datacenter", "author": ["J. Konecn\u00fd", "B. McMahan", "D. Ramage"], "venue": "CoRR, abs/1511.03575", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Federated Learning: Strategies for Improving Communication", "author": ["J. Kone\u010dn\u00fd", "H. Brendan McMahan", "F.X. Yu", "P. Richt\u00e1rik", "A. Theertha Suresh", "D. Bacon"], "venue": "Efficiency. CoRR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "G. Orr and M. K., editors, Neural Networks: Tricks of the trade. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "and C", "author": ["Y. LeCun", "C. Cortes"], "venue": "J. Burges. The mnist database of handwritten digits", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["X. Lian", "Y. Huang", "Y. Li", "J. Liu"], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, NIPS. Curran Associates, Inc.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "A measurement study of the wuala on-line storage service", "author": ["T. Mager", "E. Biersack", "P. Michiardi"], "venue": "P2P", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "and B", "author": ["H.B. McMahan", "E. Moore", "D. Ramage"], "venue": "A. y Arcas. Federated learning of deep networks using model averaging. CoRR, abs/1602.05629", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Faster Asynchronous SGD", "author": ["A. Odena"], "venue": "ArXiv e-prints,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Deep face recognition", "author": ["O.M. Parkhi", "A. Vedaldi", "A. Zisserman"], "venue": "BMVC", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "NIPS", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "IJCV, 115(3):211\u2013252", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Benchmarking state-of-the-art deep learning software tools", "author": ["S. Shi", "Q. Wang", "P. Xu", "X. Chu"], "venue": "CoRR, abs/1608.07249", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Privacy-preserving deep learning", "author": ["R. Shokri", "V. Shmatikov"], "venue": "CCS", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Greening the Internet with Nano Data Centers", "author": ["V. Valancius", "N. Laoutaris", "L. Massouli\u00e9", "C. Diot", "P. Rodriguez"], "venue": "CoNext", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Staleness-aware async-sgd for distributed deep learning", "author": ["W. Zhang", "S. Gupta", "X. Lian", "J. Liu"], "venue": "CoRR, abs/1511.05950", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Deep learning recently permitted significant improvements over state-of-the-art techniques for building classification models for instance [12].", "startOffset": 139, "endOffset": 143}, {"referenceID": 20, "context": "Its use spans over a large spectrum of applications, from face recognition in [21], to natural language processing with word2vec [19] and to video recommendation in YouTube [6].", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "Its use spans over a large spectrum of applications, from face recognition in [21], to natural language processing with word2vec [19] and to video recommendation in YouTube [6].", "startOffset": 129, "endOffset": 133}, {"referenceID": 5, "context": "Its use spans over a large spectrum of applications, from face recognition in [21], to natural language processing with word2vec [19] and to video recommendation in YouTube [6].", "startOffset": 173, "endOffset": 176}, {"referenceID": 11, "context": "Typically, the last image recognition DNNs, such as [12] or [9], leverage very large datasets (like Imagenet [23]) during the learning phase; this leads to the processing of over 10TB of data.", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "Typically, the last image recognition DNNs, such as [12] or [9], leverage very large datasets (like Imagenet [23]) during the learning phase; this leads to the processing of over 10TB of data.", "startOffset": 60, "endOffset": 63}, {"referenceID": 22, "context": "Typically, the last image recognition DNNs, such as [12] or [9], leverage very large datasets (like Imagenet [23]) during the learning phase; this leads to the processing of over 10TB of data.", "startOffset": 109, "endOffset": 113}, {"referenceID": 1, "context": ", TensorFlow [2] with GPU-enabled servers and 16Gbps network ports).", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "The dominant parameter server computing model, introduced by Google in 2012 [7], uses a set of workers for parallel processing, while a few central servers (denoted the parameter server (PS) hereafter for simplicity) are managing shared states modified by those workers.", "startOffset": 76, "endOffset": 79}, {"referenceID": 23, "context": "Since DNN models are large (from thousands to billions parameters [24]), placing those worker tasks over edge-devices imply significant updates transfer over the Internet.", "startOffset": 66, "endOffset": 70}, {"referenceID": 1, "context": "To illustrate the feasibility question, we implement the largest distribution scenario considered in the TensorFlow paper [2], where 200 machines are collaborating to learn a model.", "startOffset": 122, "endOffset": 125}, {"referenceID": 24, "context": "technique for sending updates from workers to the PS, using gradient selection [25].", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "\u2206\u0398(i) is computed by a backpropagation step [14] on D at time i.", "startOffset": 44, "endOffset": 48}, {"referenceID": 6, "context": "proposed in [7] the parameter server model, as a way to distribute computation onto up to hundreds of machines.", "startOffset": 12, "endOffset": 15}, {"referenceID": 27, "context": "Note that in the PS model, the fact that n workers are training their local vector in parallel and then send the updates introduces concurrency, also known as staleness [28].", "startOffset": 169, "endOffset": 173}, {"referenceID": 26, "context": ", personal computers of volunteers with SETI@home, or home gateways [27]) and the datacenter network by the Internet.", "startOffset": 68, "endOffset": 72}, {"referenceID": 24, "context": "In this context, we assume that the training data reside with the workers; this serves for instance as a basis for privacy-preserving scenarios [25], where users have their photos at home, and want to contribute to the computing of a global photo classification model, but without sending their personal data to a cloud service.", "startOffset": 144, "endOffset": 148}, {"referenceID": 26, "context": "The stringent aspect of our setup is the lower connectivity capacity of workers, and their expected smaller reliability [27].", "startOffset": 120, "endOffset": 124}, {"referenceID": 27, "context": "The staleness, proportional to the number of workers (please refer to [28], [16] or [8] for in depth phenomenon explanation), is an important issue in asynchronous SGD in general.", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "The staleness, proportional to the number of workers (please refer to [28], [16] or [8] for in depth phenomenon explanation), is an important issue in asynchronous SGD in general.", "startOffset": 76, "endOffset": 80}, {"referenceID": 7, "context": "The staleness, proportional to the number of workers (please refer to [28], [16] or [8] for in depth phenomenon explanation), is an important issue in asynchronous SGD in general.", "startOffset": 84, "endOffset": 87}, {"referenceID": 27, "context": "This factor is known for slowing down the computation convergence [28].", "startOffset": 66, "endOffset": 70}, {"referenceID": 27, "context": "In order to cope with it, works [28] and [20] propose to adapt the learning rate \u03b1 as a function of the current staleness to reduce the impact of stale updates, which will also reduce the number of updates needed to train \u0398.", "startOffset": 32, "endOffset": 36}, {"referenceID": 19, "context": "In order to cope with it, works [28] and [20] propose to adapt the learning rate \u03b1 as a function of the current staleness to reduce the impact of stale updates, which will also reduce the number of updates needed to train \u0398.", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "in [8] show that a good precision is achievable with a given number of epochs1 by keeping n \u00d7 b constant (with n the number of workers and b the mini-batch size).", "startOffset": 3, "endOffset": 6}, {"referenceID": 24, "context": "[25] proposed a compression mechanism for reducing the size of the updates sent from each worker to the PS, named Selective Stochastic Gradient Descent.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "As we shall see in the evaluation Section, the use of approach [25] manages to reduce the size of updates, but at the cost of accuracy.", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "[22] consider parameters as independent during the SGD process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "This selection permits to balance learning across DNN layers and better reflects computed gradients as compared to a random selection (or solely the largest ones across the whole model) as in [25].", "startOffset": 192, "endOffset": 196}, {"referenceID": 7, "context": "The PS keeps a trace of all received updates at given timestamps (as in [8], [28]).", "startOffset": 72, "endOffset": 75}, {"referenceID": 27, "context": "The PS keeps a trace of all received updates at given timestamps (as in [8], [28]).", "startOffset": 77, "endOffset": 81}, {"referenceID": 27, "context": "We use the update equation inspired by [28]:", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "We experiment our setup with the MNIST dataset [15], also used by our competitor [25].", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "We experiment our setup with the MNIST dataset [15], also used by our competitor [25].", "startOffset": 81, "endOffset": 85}, {"referenceID": 0, "context": "3Note that TensorFlow is also running in containers, while executed in a datacenter environment [1].", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "The experiments are launching n = 200 workers, and one PS, corresponding to the scale of operation reported by the TensorFlow paper [2].", "startOffset": 132, "endOffset": 135}, {"referenceID": 24, "context": "paper [25].", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "The second is a Convolutional Neural Network (CNN), consisting of two convolutional layers and two fullconnected layers (211, 690 parameters), and is taken from the Keras library [5] for the MNIST dataset.", "startOffset": 179, "endOffset": 182}, {"referenceID": 15, "context": "2) Competitors: We compare the performance of AdaComp to the ones of 1) the basic Async-SGD method (which we denote ASGD) [16] as a baseline.", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "2) the Comp-ASGD algorithm, which is similar to ASGD, but implements a gradient selection as described in [25].", "startOffset": 106, "endOffset": 110}, {"referenceID": 12, "context": "2) CNN: We perform the next experiment using a CNN model, that is known to result in a better accuracy on image classification [13], [12], [26].", "startOffset": 127, "endOffset": 131}, {"referenceID": 11, "context": "2) CNN: We perform the next experiment using a CNN model, that is known to result in a better accuracy on image classification [13], [12], [26].", "startOffset": 133, "endOffset": 137}, {"referenceID": 25, "context": "2) CNN: We perform the next experiment using a CNN model, that is known to result in a better accuracy on image classification [13], [12], [26].", "startOffset": 139, "endOffset": 143}, {"referenceID": 6, "context": ", DistBelief [7], Adam [4], and TensorFlow [2] as the principal ones.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": ", DistBelief [7], Adam [4], and TensorFlow [2] as the principal ones.", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": ", DistBelief [7], Adam [4], and TensorFlow [2] as the principal ones.", "startOffset": 43, "endOffset": 46}, {"referenceID": 27, "context": "1) In [28], W.", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "A recent paper of Odena [20] proposes to maintain the averaged variance of each parameters to weight new updates more precisely.", "startOffset": 24, "endOffset": 28}, {"referenceID": 27, "context": "[28] propose the n-softSync method where the PS waits a fraction 1/n of all workers at each iteration.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] shows that a synchronous SGD could be more efficient if the PS does not wait the last k workers at each iteration.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "At the other extreme of the parallelism versus communication tradeoff, so-called federated optimization has been introduced [11], [10], [18].", "startOffset": 124, "endOffset": 128}, {"referenceID": 9, "context": "At the other extreme of the parallelism versus communication tradeoff, so-called federated optimization has been introduced [11], [10], [18].", "startOffset": 130, "endOffset": 134}, {"referenceID": 17, "context": "At the other extreme of the parallelism versus communication tradeoff, so-called federated optimization has been introduced [11], [10], [18].", "startOffset": 136, "endOffset": 140}, {"referenceID": 26, "context": "This is for instance the case with so called nano data centers [27], where home gateways are the devices chosen for best effort general computation tasks.", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "Another example is the Wuala data storage service, that stores part of user data in a peer-to-peer fashion on edge-devices, to reduce the storage operational costs of the service provider [17].", "startOffset": 188, "endOffset": 192}], "year": 2017, "abstractText": "The state-of-the-art results provided by deep learning come at the price of an intensive use of computing resources. The leading frameworks (e.g., TensorFlow) are executed on GPUs or on high-end servers in datacenters. On the other end, there is a proliferation of personal devices with possibly free CPU cycles. In this paper, we ask the following question: Is distributed deep learning computation on WAN connected devices feasible, in spite of the traffic caused by learning tasks? We show that such a setup rises some important challenges, most notably the ingress traffic that the servers hosting the up-to-date model have to sustain. In order to reduce this stress, we propose AdaComp, a new algorithm for compressing worker updates to the model on the server. Applicable to stochastic gradient descent based approaches, it combines efficient gradient selection and learning rate modulation. We then experiment and measure the impact of compression and device reliability on the accuracy of learned models. To do so, we leverage an emulator platform we developed, that embeds the TensorFlow code into Linux containers. We report a reduction of the total amount of data sent by workers to the server by two order of magnitude (e.g., 191-fold reduction for a convolutional network on the MNIST dataset), when compared to the standard algorithm based on asynchronous stochastic gradient descent, while maintaining model accuracy.", "creator": "LaTeX with hyperref package"}}}