{"id": "1702.04770", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2017", "title": "Training Language Models Using Target-Propagation", "abstract": "While Truncated Back-Propagation through Time (BPTT) is the most popular approach to training Recurrent Neural Networks (RNNs), it suffers from being inherently sequential (making parallelization difficult) and from truncating gradient flow between distant time-steps. We investigate whether Target Propagation (TPROP) style approaches can address these shortcomings. Unfortunately, extensive experiments suggest that TPROP generally underperforms BPTT, and we end with an analysis of this phenomenon, and suggestions for future work.", "histories": [["v1", "Wed, 15 Feb 2017 20:56:30 GMT  (1677kb,D)", "http://arxiv.org/abs/1702.04770v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["sam wiseman", "sumit chopra", "marc'aurelio ranzato", "arthur szlam", "ruoyu sun", "soumith chintala", "nicolas vasilache"], "accepted": false, "id": "1702.04770"}, "pdf": {"name": "1702.04770.pdf", "metadata": {"source": "CRF", "title": "Training Language Models Using Target-Propagation", "authors": ["Sam Wiseman", "Sumit Chopra", "Arthur Szlam", "Ruoyu Sun", "Nicolas Vasilache"], "emails": ["swiseman@seas.harvard.edu", "spchopra@fb.com", "ranzato@fb.com", "aszlam@fb.com", "ruoyu@fb.com", "soumith@fb.com", "ntv@fb.com"], "sections": [{"heading": "1 Introduction", "text": "Modern RNNs are trained almost exclusively by means of abbreviated Back-Propagation Through Time (BPTT) (Elman, 1990; Werbos, 1990; Mikolov et al., 2010). Despite their popularity, BPTT training has two major disadvantages, namely that it is inherently sequential (and therefore difficult to parallelise) and that it shortens the number of time steps through which gradient information can spread. Inspired by the recent success stories of target propagation (TPROP) in training, we examine the formation of RNNs with TPROP, an idea that has been repeatedly proposed in literature (LeCun, 1986, 1988; Mirowski and LeCun, 2009)."}, {"heading": "1.1 Review: Truncated BPTT", "text": "It is incomprehensible that the number of acquired terms in K (ht, ht-1), (1) y-t = f (ht), (2) where g and f are non-linear, parametric functions of their inputs (ht), (2) where g and f (ht) = softmax (Wyht), where it characterizes the logical sigmoid function, is incomprehensible by using g (xt, ht-1) = Wxxt + Whht \u2212 1), and f (ht) = softmax (Wyht), where it characterizes the logistic sigmoid function. It is typical to apply RNs to sequence ediction problems that have a well-defined loss (y-t, yt) at any time step in which we obtain the desired tryt classes."}, {"heading": "2 Target Propagation", "text": "To address the problems associated with the truncated BPTT algorithm, we propose to form RNNs with a slightly modified loss. Instead of processing the losses in time steps (as in Eq.3), which have the effect that the ht is only implicitly instantiated, we treat the ht instead as explicit variables that need to be reoptimized. However, to maintain the recurring property of the model, we add additional constraints to the loss that encourage neighboring hidden states to adhere roughly to the parametric repetition. Specifically, we do not define h as the predicted hidden state at the time t, as follows: h-t = g (xt, ht-1) (4) y-t = f (h-t). (5) Note that the equation (4) uses the independent variable ht-1 on the right; otherwise, it is equivalent to Equation (1)."}, {"heading": "2.1 The Blocked Target-Propagation Algorithm (BTPROP)", "text": "The remaining hidden states are implicitly defined by repetition (1), which further constrains the model during training. We refer to the partial sequence of length B, which consists of schedules starting with a free variable and ending as a \"block\" before the next free variable. Note that with block size B = 1, we restore the TPROP formulation in the previous section. Advantages of BTPROP We emphasize that BTPROP loss provides an approach to solving the problems with the truncated BPTT loss identified in Section 1.1. First, the independence of the limitless time steps in different blocks suggests that training can be efficiently parallelized by distributing a large number of (contiguous) blocks to each machine in a cluster."}, {"heading": "3 Training", "text": "The loss in Eq.6 can be considered as a consequence of an equation-limited formulation of the transverse entropy loss. In particular, if we introduce equality constraints between each ht and h-t as well as a dual variable number for each constraint, the Lagranger can be written (up to a constant) asLaug (H, \u03b8, U) = T = 1 '(f (h-t), xi + 1) + \u03bb 2 | h-t + ut | 2, (7) defined in the form of Eq.6 with C, defined as C (h-t, ht) = 12 | h-t-t-t + ut | 2. It is natural to minimize the now-unlimited loss (7) by using an alternative direction method of multipliers (ADMM) defined as C (h-t, ht)."}, {"heading": "4 Experiments", "text": "We conduct word-level language modeling experiments on the Penn Tree Bank (PTB) (Marcus et al., 1exceptions include various forms of multiplicative RNNNs (Wu et al., 2016) without nonlinearity. In this case, alternate minimizations can be performed with the smallest squares. We experimented along these lines, but found such methods to undercut gradient-based approaches, which are also much more flexible. 1993) and Text8 datasets. 2 For all experiments, we use single-layer Gated Recurrent Unit (GRU) RNs (Cho et al., 2014) (without dropout (Srivastava et al., 2014)), and we use Adagrad (Duchi et al., 2011) for optimization. We report on the perplexity achieved by the validation dataset after freezing the final parameters obtained during the alternating optimization process."}, {"heading": "4.1 Results", "text": "We only report validation figures because the training performance between BTPROP and BPTT was generally comparable, which is disappointing because it can be shown that BTPROP with a single H-step is essentially comparable to BPTT. Importantly, however, BTPROP performance does not tend to improve with additional H-steps. It is disappointing because it can be shown that BTPROP with a single H-step is essentially equivalent to BPTT. See also the supplemental material for more rigorous formulation and detection. In addition, the table shows that both batch BTPROP and batch BPTT are significantly exceeded."}, {"heading": "4.2 BTPROP as L2 Regularizer", "text": "Further experiments summarized in Table 2 indicate that the small increases in BTPROP performance in the minibatch setting (see Table 1) are due to the regulating effect of the L2 penalty in (7) and not to training with longer dependencies. There, we compare the validation pill obtained in minibatch PTB with H steps = 1 and B = 20 with no H optimization (which improves (i.e. decreases) helplessness) and without H optimization, but also with the setting \u03bb = 0 (which significantly violates (i.e. increases) helplessness)."}, {"heading": "4.3 A Possible Explanation and Future Work", "text": "One explanation for the H optimization that impairs performance is that its relatively unrestricted nature may allow for hidden states that reduce training loss without generalizing parameters. In fact, many of the reported successes of TPROP training have been associated with very limited problems, such as those with binary hidden state limitations (CarreiraPerpin and Raziperchikolaei, 2015) or sparseness limitations (Kavukcuoglu et al., 2009). To test this hypothesis, we present in Figure 3 the perplexity obtained from the BTPROP with B = 10 (for both 2 and 5 H optimization steps), and the perplexity obtained from the BPTT with K = 10, as the dimensionality of the hidden state increases. We see that BPTT does in fact meet the BTPROP for small hidden states, but is relatively insufficient as our state size increases."}, {"heading": "A Supplemental Material", "text": "A.1 Connection between BPTT and TPROP Here we show that if using the Penalty Method loss (or, equivalent, the ADMM loss while keeping the dual variables ut set to 0), doing BTPROP with H-steps = 1 and \u03b8-steps = 1 results in gradients in respect that are equal to the gradient descent.Formal, let's \"(gaxy, ht \u2212 1) be a pertime-step loss, if: (1) theH are initialized such that ht = g\u03b8 (xt, ht \u2212 1) the ht are updated with vanilla gradient descent.Formal, let\" (gaxy, ht \u2212 1) be a pertime-step loss, when the parameters of g. Also definition 'pm =' (ht) + gt, yht, that we are we have 'pm, that we have' pm that '(ht) + ght, that we have yht. \""}], "references": [{"title": "How auto-encoders could provide credit assignment in deep networks via target propagation", "author": ["Yoshua Bengio."], "venue": "arXiv preprint arXiv:1407.7906 .", "citeRegEx": "Bengio.,? 2014", "shortCiteRegEx": "Bengio.", "year": 2014}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Stephen P. Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein."], "venue": "Foundations and Trends in Machine Learning 3(1):1\u2013122.", "citeRegEx": "Boyd et al\\.,? 2011", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Hashing with binary autoencoders", "author": ["Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n", "Ramin Raziperchikolaei."], "venue": "CVPR. pages 557\u2013566.", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n and Raziperchikolaei.,? 2015", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n and Raziperchikolaei.", "year": 2015}, {"title": "Distributed optimization of deeply nested systems", "author": ["Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n", "Weiran Wang."], "venue": "Proceedings of AISTATS. pages 10\u201319.", "citeRegEx": "Carreira.Perpi\u00f1\u00e1n and Wang.,? 2014", "shortCiteRegEx": "Carreira.Perpi\u00f1\u00e1n and Wang.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Variational methods for the solution of problems of equilibrium and vibrations", "author": ["Richard Courant"], "venue": "Bull. Amer. Math. Soc 49(1):1\u201323.", "citeRegEx": "Courant,? 1943", "shortCiteRegEx": "Courant", "year": 1943}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research 12(Jul):2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman."], "venue": "Cognitive Science 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "A dual algorithm for the solution of nonlinear variational problems via finite element approximation", "author": ["Daniel Gabay", "Bertrand Mercier."], "venue": "Computers & Mathematics with Applications 2(1):17\u2013", "citeRegEx": "Gabay and Mercier.,? 1976", "shortCiteRegEx": "Gabay and Mercier.", "year": 1976}, {"title": "Sur l\u2019approximation, par \u00e9l\u00e9ments finis d\u2019ordre un, et la r\u00e9solution, par p\u00e9nalisation-dualit\u00e9 d\u2019une classe de probl\u00e8mes de dirichlet non lin\u00e9aires", "author": ["Roland Glowinski", "A Marroco."], "venue": "Revue fran\u00e7aise d\u2019automatique, informatique, recherche", "citeRegEx": "Glowinski and Marroco.,? 1975", "shortCiteRegEx": "Glowinski and Marroco.", "year": 1975}, {"title": "Multiplier and gradient methods", "author": ["Magnus R Hestenes."], "venue": "Journal of optimization theory and applications 4(5):303\u2013320.", "citeRegEx": "Hestenes.,? 1969", "shortCiteRegEx": "Hestenes.", "year": 1969}, {"title": "Learning invariant features through topographic filter maps", "author": ["Koray Kavukcuoglu", "Marc\u2019Aurelio Ranzato", "Rob Fergus", "Yann LeCun"], "venue": "In CVPR. IEEE", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2009}, {"title": "On large-batch training for deep learning: Generalization gap and sharp minima", "author": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang."], "venue": "arXiv:1609.04836 .", "citeRegEx": "Keskar et al\\.,? 2016", "shortCiteRegEx": "Keskar et al\\.", "year": 2016}, {"title": "A cost function for internal representations", "author": ["Anders Krogh", "CI Thorbergsson", "John A Hertz."], "venue": "NIPS. pages 733\u2013740.", "citeRegEx": "Krogh et al\\.,? 1989", "shortCiteRegEx": "Krogh et al\\.", "year": 1989}, {"title": "Mod\u00e8les connexionnistes de l\u2019apprentissage", "author": ["Yann Le Cun."], "venue": "Ph.D. thesis, Paris 6.", "citeRegEx": "Cun.,? 1987", "shortCiteRegEx": "Cun.", "year": 1987}, {"title": "Learning processes in an asymmetric threshold network", "author": ["Yann LeCun."], "venue": "Springer-Verlag, pages 233\u2013 240.", "citeRegEx": "LeCun.,? 1986", "shortCiteRegEx": "LeCun.", "year": 1986}, {"title": "Theoretical framework for backpropagation", "author": ["Yann LeCun."], "venue": "Proceedings of the 1988 Connectionist Models Summer School. Morgan Kaufmann, CMU, Pittsburgh, Pa, pages 21\u201328.", "citeRegEx": "LeCun.,? 1988", "shortCiteRegEx": "LeCun.", "year": 1988}, {"title": "Difference target propagation", "author": ["Dong-Hyun Lee", "Saizheng Zhang", "Asja Fischer", "Yoshua Bengio."], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, pages 498\u2013515.", "citeRegEx": "Lee et al\\.,? 2015", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafit", "L. Burget", "J. Cernock", "S. Khudanpur."], "venue": "INTERSPEECH.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Dynamic factor graphs for time series modeling", "author": ["Piotr Mirowski", "Yann LeCun."], "venue": "Machine Learning and Knowledge Discovery in Databases (ECML/PKDD). Springer, ISBN:978-3-642-041730, volume 5782, pages 128\u2013143.", "citeRegEx": "Mirowski and LeCun.,? 2009", "shortCiteRegEx": "Mirowski and LeCun.", "year": 2009}, {"title": "Numerical optimization, second edition", "author": ["Jorge Nocedal", "Stephen J Wright."], "venue": "Numerical optimization pages 497\u2013528.", "citeRegEx": "Nocedal and Wright.,? 2006", "shortCiteRegEx": "Nocedal and Wright.", "year": 2006}, {"title": "A method for non-linear constraints in minimization problems\u201d", "author": ["Michael JD Powell"], "venue": null, "citeRegEx": "Powell.,? \\Q1967\\E", "shortCiteRegEx": "Powell.", "year": 1967}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Training neural networks without gradients: A scalable ADMM approach", "author": ["Gavin Taylor", "Ryan Burmeister", "Zheng Xu", "Bharat Singh", "Ankit Patel", "Tom Goldstein."], "venue": "Proceedings of ICML. pages 2722\u20132731.", "citeRegEx": "Taylor et al\\.,? 2016", "shortCiteRegEx": "Taylor et al\\.", "year": 2016}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos."], "venue": "Proceedings of the IEEE 78(10):1550\u20131560.", "citeRegEx": "Werbos.,? 1990", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "On multiplicative integration with recurrent neural networks", "author": ["Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov."], "venue": "NIPS. pages 2856\u20132864.", "citeRegEx": "Wu et al\\.,? 2016", "shortCiteRegEx": "Wu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "Modern RNNs are trained almost exclusively using truncated Back-Propagation Through Time (BPTT) (Elman, 1990; Werbos, 1990; Mikolov et al., 2010).", "startOffset": 96, "endOffset": 145}, {"referenceID": 25, "context": "Modern RNNs are trained almost exclusively using truncated Back-Propagation Through Time (BPTT) (Elman, 1990; Werbos, 1990; Mikolov et al., 2010).", "startOffset": 96, "endOffset": 145}, {"referenceID": 19, "context": "Modern RNNs are trained almost exclusively using truncated Back-Propagation Through Time (BPTT) (Elman, 1990; Werbos, 1990; Mikolov et al., 2010).", "startOffset": 96, "endOffset": 145}, {"referenceID": 3, "context": "Inspired by the recent reports of success of Target-Propagation (TPROP) in training nonrecurrent deep networks (Carreira-Perpi\u00f1\u00e1n and Wang, 2014; Lee et al., 2015; Taylor et al., 2016), we explore training RNNs with TPROP, an idea that has been suggested repeatedly in the literature (LeCun, 1986, 1988; Mirowski and LeCun, 2009).", "startOffset": 111, "endOffset": 184}, {"referenceID": 17, "context": "Inspired by the recent reports of success of Target-Propagation (TPROP) in training nonrecurrent deep networks (Carreira-Perpi\u00f1\u00e1n and Wang, 2014; Lee et al., 2015; Taylor et al., 2016), we explore training RNNs with TPROP, an idea that has been suggested repeatedly in the literature (LeCun, 1986, 1988; Mirowski and LeCun, 2009).", "startOffset": 111, "endOffset": 184}, {"referenceID": 24, "context": "Inspired by the recent reports of success of Target-Propagation (TPROP) in training nonrecurrent deep networks (Carreira-Perpi\u00f1\u00e1n and Wang, 2014; Lee et al., 2015; Taylor et al., 2016), we explore training RNNs with TPROP, an idea that has been suggested repeatedly in the literature (LeCun, 1986, 1988; Mirowski and LeCun, 2009).", "startOffset": 111, "endOffset": 184}, {"referenceID": 20, "context": ", 2016), we explore training RNNs with TPROP, an idea that has been suggested repeatedly in the literature (LeCun, 1986, 1988; Mirowski and LeCun, 2009).", "startOffset": 107, "endOffset": 152}, {"referenceID": 15, "context": "Such approaches have been motivated by appealing to biological plausibility, numerical stability, computational parallelizability, and its conduciveness to constrained training (LeCun, 1986; Le Cun, 1987; LeCun, 1988; Krogh et al., 1989; Bengio, 2014; Carreira-Perpi\u00f1\u00e1n and Wang, 2014; Lee et al., 2015).", "startOffset": 177, "endOffset": 303}, {"referenceID": 16, "context": "Such approaches have been motivated by appealing to biological plausibility, numerical stability, computational parallelizability, and its conduciveness to constrained training (LeCun, 1986; Le Cun, 1987; LeCun, 1988; Krogh et al., 1989; Bengio, 2014; Carreira-Perpi\u00f1\u00e1n and Wang, 2014; Lee et al., 2015).", "startOffset": 177, "endOffset": 303}, {"referenceID": 13, "context": "Such approaches have been motivated by appealing to biological plausibility, numerical stability, computational parallelizability, and its conduciveness to constrained training (LeCun, 1986; Le Cun, 1987; LeCun, 1988; Krogh et al., 1989; Bengio, 2014; Carreira-Perpi\u00f1\u00e1n and Wang, 2014; Lee et al., 2015).", "startOffset": 177, "endOffset": 303}, {"referenceID": 0, "context": "Such approaches have been motivated by appealing to biological plausibility, numerical stability, computational parallelizability, and its conduciveness to constrained training (LeCun, 1986; Le Cun, 1987; LeCun, 1988; Krogh et al., 1989; Bengio, 2014; Carreira-Perpi\u00f1\u00e1n and Wang, 2014; Lee et al., 2015).", "startOffset": 177, "endOffset": 303}, {"referenceID": 3, "context": "Such approaches have been motivated by appealing to biological plausibility, numerical stability, computational parallelizability, and its conduciveness to constrained training (LeCun, 1986; Le Cun, 1987; LeCun, 1988; Krogh et al., 1989; Bengio, 2014; Carreira-Perpi\u00f1\u00e1n and Wang, 2014; Lee et al., 2015).", "startOffset": 177, "endOffset": 303}, {"referenceID": 17, "context": "Such approaches have been motivated by appealing to biological plausibility, numerical stability, computational parallelizability, and its conduciveness to constrained training (LeCun, 1986; Le Cun, 1987; LeCun, 1988; Krogh et al., 1989; Bengio, 2014; Carreira-Perpi\u00f1\u00e1n and Wang, 2014; Lee et al., 2015).", "startOffset": 177, "endOffset": 303}, {"referenceID": 9, "context": "It is natural to minimize the now-unconstrained loss (7) using an Alternating Direction Method of Multipliers (ADMM) style approach (Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011), which results in the following meta-algorithm:", "startOffset": 132, "endOffset": 205}, {"referenceID": 8, "context": "It is natural to minimize the now-unconstrained loss (7) using an Alternating Direction Method of Multipliers (ADMM) style approach (Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011), which results in the following meta-algorithm:", "startOffset": 132, "endOffset": 205}, {"referenceID": 1, "context": "It is natural to minimize the now-unconstrained loss (7) using an Alternating Direction Method of Multipliers (ADMM) style approach (Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011), which results in the following meta-algorithm:", "startOffset": 132, "endOffset": 205}, {"referenceID": 21, "context": "The first, which we refer to as the Penalty Method (PM) (Courant et al., 1943; Nocedal and Wright, 2006), is identical to ADMM, except that it avoids the use of dual variables entirely, and so skips step 3.", "startOffset": 56, "endOffset": 104}, {"referenceID": 10, "context": "The second, the Augmented Langrangian Method (ALM) (Hestenes, 1969; Powell, 1967; Nocedal and Wright, 2006), minimizes jointly over H, \u03b8 before updating the dual variables, effectively merging steps 1.", "startOffset": 51, "endOffset": 107}, {"referenceID": 22, "context": "The second, the Augmented Langrangian Method (ALM) (Hestenes, 1969; Powell, 1967; Nocedal and Wright, 2006), minimizes jointly over H, \u03b8 before updating the dual variables, effectively merging steps 1.", "startOffset": 51, "endOffset": 107}, {"referenceID": 21, "context": "The second, the Augmented Langrangian Method (ALM) (Hestenes, 1969; Powell, 1967; Nocedal and Wright, 2006), minimizes jointly over H, \u03b8 before updating the dual variables, effectively merging steps 1.", "startOffset": 51, "endOffset": 107}, {"referenceID": 26, "context": "Exceptions to this include various forms of multiplicative RNNs (Wu et al., 2016) with no non-linearity.", "startOffset": 64, "endOffset": 81}, {"referenceID": 4, "context": "2 For all experiments we use single-layer Gated Recurrent Unit (GRU) RNNs (Cho et al., 2014) (without Dropout (Srivastava et al.", "startOffset": 74, "endOffset": 92}, {"referenceID": 23, "context": ", 2014) (without Dropout (Srivastava et al., 2014)), and we use Adagrad (Duchi et al.", "startOffset": 25, "endOffset": 50}, {"referenceID": 6, "context": ", 2014)), and we use Adagrad (Duchi et al., 2011) for optimization.", "startOffset": 29, "endOffset": 49}, {"referenceID": 12, "context": "Furthermore, it is clear from the table that both Batch BTPROP and Batch BPTT are significantly outperformed by their minibatch counterparts, presumably due to the regularization effect of updating parameters after seeing only a subset of the data (see Keskar et al. (2016) for a discussion of this phenomenon).", "startOffset": 253, "endOffset": 274}, {"referenceID": 11, "context": "Indeed, many of the reported successes of TPROP-style training have involved very constrained problems, such as those with binary hidden-state constraints (CarreiraPerpi\u00f1\u00e1n and Raziperchikolaei, 2015) or sparsity constraints (Kavukcuoglu et al., 2009).", "startOffset": 225, "endOffset": 251}], "year": 2017, "abstractText": "While Truncated Back-Propagation through Time (BPTT) is the most popular approach to training Recurrent Neural Networks (RNNs), it suffers from being inherently sequential (making parallelization difficult) and from truncating gradient flow between distant time-steps. We investigate whether Target Propagation (TPROP) style approaches can address these shortcomings. Unfortunately, extensive experiments suggest that TPROP generally underperforms BPTT, and we end with an analysis of this phenomenon, and suggestions for future work.", "creator": "LaTeX with hyperref package"}}}