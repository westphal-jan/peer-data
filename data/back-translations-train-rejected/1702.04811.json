{"id": "1702.04811", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2017", "title": "An Analysis of Ability in Deep Neural Networks", "abstract": "Deep neural networks (DNNs) have set state of the art results in many machine learning and NLP tasks. However, we do not have a strong understanding of what DNN models learn. In this paper, we examine learning in DNNs through analysis of their outputs. We compare DNN performance directly to a human population, and use characteristics of individual data points such as difficulty to see how well models perform on easy and hard examples. We investigate how training size and the incorporation of noise affect a DNN's ability to generalize and learn. Our experiments show that unlike traditional machine learning models (e.g., Naive Bayes, Decision Trees), DNNs exhibit human-like learning properties. As they are trained with more data, they are more able to distinguish between easy and difficult items, and performance on easy items improves at a higher rate than difficult items. We find that different DNN models exhibit different strengths in learning and are robust to noise in training data.", "histories": [["v1", "Wed, 15 Feb 2017 23:04:09 GMT  (76kb,D)", "http://arxiv.org/abs/1702.04811v1", "10 pages, 2 figures, 3 tables"], ["v2", "Thu, 29 Jun 2017 00:23:16 GMT  (194kb,D)", "http://arxiv.org/abs/1702.04811v2", "9 pages plus references, 4 figures, 2 tables"]], "COMMENTS": "10 pages, 2 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["john p lalor", "hao wu", "tsendsuren munkhdalai", "hong yu"], "accepted": false, "id": "1702.04811"}, "pdf": {"name": "1702.04811.pdf", "metadata": {"source": "CRF", "title": "An Analysis of Machine Learning Intelligence", "authors": ["John P. Lalor", "Hao Wu", "Tsendsuren Munkhdalai", "Hong Yu"], "emails": ["lalor@cs.umass.edu,", "hao.wu.5@bc.edu,", "tsendsuren.munkhdalai@umassmed.edu", "hong.yu@umassmed.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to fulfil them."}, {"heading": "2 Background and Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Item Response Theory", "text": "IRT is a psychometric method of scale construction and evaluation. It is commonly used in educational tests to construct, evaluate, or evaluate standardized tests. It is used both to design tests and to analyze human responses (classified as right or wrong) to a number of questions (referred to as \"items\"). IRT collectively models an individual's ability and item characteristics to predict performance (Baker and Kim, 2004). IRT models assume that: (i) individuals differ from each other in terms of unobserved latent trait dimension (referred to as \"ability\" or \"factor\"), (ii) the likelihood of correctly responding to an item is a function of a person's ability, (iii) responses to different item levels of the person are independent of each other, and (iv) responses from different individuals are independent of each other (Lalor et al., 2016).A common IRT model for estimating an individual latent item is an item that is a traepatistic (the individual's) response to a particular item (the individual's) being independent of each other."}, {"heading": "2.1.1 Interpreting IRT Results", "text": "The IRT estimate refers to the human population whose answers were used to estimate item parameters. For example, the estimated ability of a person (or the NLP model) of 1.2 is interpreted as 1.2 standard deviations above the average ability in the population. Generally, the traditional total number of correct answers has no such quantitative significance. This value can be converted into a percentile to indicate which percentage of a person's ability is higher than. For this work, we convert all IRT estimates into their perceptible representations."}, {"heading": "2.2 Recognizing Textual Entailment", "text": "RTE attempts to classify semantic relationships between two sets of sentences (Dagan et al., 2006). In the case of text (T) and the hypothesis (H), the pair of sentences contains T H, if a person who has read T would conclude that H is true. If a person would conclude that H is wrong, then H contradicts T. If the two sentences have nothing to do with each other, the pair is considered neutral. Table 1 shows examples of T-H pairs and their respective classifications. In relation to IRT, each sentence pair is considered a unique piece. Recently, deep learning models have discontinued and exceeded the state of the art in RTE (Rockta \ufffd schel et al., 2016; Munkhdalai and Yu, 2016), in part due to the availability of the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015). SNLI consists of over 550K humanized pairs of sentences and is significantly larger than previously used sets."}, {"heading": "2.3 Related Work", "text": "Lalor et al. (2016) introduced the idea of applying the IRT evaluation to NLP tasks. They created a scale using IRT for RTE and evaluated a single neural LSTM network to demonstrate the effectiveness of the evaluation, but did not evaluate other NLP models or tasks. Mart\u00ed'nez-Plumed et al. (2016) consider IRT in the context of evaluating ML models, but do not use human populations to calibrate the models. They attempt to adapt IRT models from the ML models without initially identifying good items from a human population, and obtain ICCs with negative tendencies that are difficult to interpret using existing IRT assumptions. Bruce and Wiebe (1999) modeled latent features of data to identify correct labeling. There was also work in modeling individuals to identify bad annotators Hoal probability (2013) and the probability (2013), but neither modelling them together or the probability of NP."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Model Selection", "text": "For our experiments, we tested four traditional ML models: Naive Bayes (NB), Logistic Regression (LR), K-Nearest Neighbors (KNN), Decision Trees (DT), and three representative DNN models. For the traditional ML models, we used a basic implementation from a popular ML consistency framework (Pedregosa et al., 2011). Since our goal is not to extend the state of the art, but to compare learning behavior, we used standard parameters in all cases. We tested DNN networks that had previously proven to be good using the SNLI dataset or other NLP tasks: A revolutionary neural network (CNN) for sentence classification (Kim, 2014); The LSTM model for digital networks that was published with the SNLI dataset (Bowman et al., 2015); Neural Semantic Encoder (NSE), a memorymer, and NNN (MDN)."}, {"heading": "3.2 IRT Models and Test Sets", "text": "In order to model individual item characteristics and evaluate our models using IRT, we adjust a set of IRT models. For the data, we obtained the human response patterns and IRT models from Lalor et al. (2016). The data consists of approximately 1000 responses from human commentators for a selection of the SNLI dataset (Bowman et al., 2015). 5 IRT models (and subsequent test sets) were adjusted by Lalor et al. (2016), based on the interannotator agreement from the original SNLI dataset: (i) Conditional elements with 5 out of 5 agreements (5E), (ii) Conditional elements with 5 out of 5 agreements (5C), (iii) Conditional elements with 5 out of 5 agreements (5C), (iii) Neutral elements with 5 out of 5 agreements (5N), (iv) Conditional elements with 4 out of 5 agreements (4C), and (v) Neutral elements with 5 agreements (5C), (5C), which allow us to measure IRT's ability to use certain items with IRT's data strength and (IRT)."}, {"heading": "3.3 Dataset Characteristics", "text": "For a qualitative analysis of performance, we tried to understand how the models behave in relation to certain semantic and syntactic properties observed in the data. We manually classified the items in the IRT test sets to determine whether the performance varied between the items based on the properties of the sentence pairs. We classified the items in the IRT test sets into the following groups. 2 Each sentence pair was classified into one of the following 3 groups based on the sentence structure: A) P & H are both sentences. B) A sentence and a noun phrase. C) P & H are both noun phrases and one or more of the following 9 groups.: D) Aligned lexical entanglements or contradictions. E) The difference between H and P is the mixing of a word or phrase. F) H is not relevant at all to P G) P & H have related subjects, unrelated whether objects. H) Restrictive or P is added to adjectives H, or P is added to adjectives H)."}, {"heading": "4 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Model Performance", "text": "We report on theta percentiles on the IRT test sets and accuracy on the IRT test set as a baseline comparison of models. Table 2 shows that the DNN2Some of the categories from http: / / nlp.stanford.edu / blog / the-stanford-nli-corpus-revisited / models that had the highest accuracy compared to other ML models also scored well on IRT. Our results show that the RNN models (LSTM and NSE) outperformed the CNN model in both accuracy and IRT. However, the IRT values are much more sensitive to the identification of powerful RNN models. The NSE model, which had a high overall accuracy (84.06%), is lower than the values when compared to the LSTM model. Since IRT considers the individual items that are correctly answered as metrics, it may be the case that the specific point response value for a lower NSE pattern is a lower than the NSE one."}, {"heading": "4.2 Analysis of Learning in NLP Models", "text": "Next, we perform an analysis of the performance of the different NLP models when they are trained with different training sets. First, we analyze the performance qualitatively according to the groups described in \u00a7 3.3. Then, we perform more quantitative analyses of the model performance: (i) performance in relation to the sound in the training set, (ii) performance in relation to the size of the training set, and (iii) performance on simple and difficult test sets."}, {"heading": "4.2.1 Learning From Noisy Data", "text": "To do this, we randomly select a section of our training set (5%, 20%, 35% or 50%) and replace the correct label with an incorrect one. Figure 1 shows how the DNN and non-DNN models perform when trained with loud data. As expected, accuracy drops when noise is added to the training data in most DNN and non-DNN NLP models. Since the IRT values for the traditional ML models are initially low, they are not as sensitive to noise. Accuracy values are almost random, so adding noise will not further degrade performance, both in terms of accuracy and in RT.With respect to the DNN models, the LSTM model is more significantly affected by noise than the NLSs and NSE models. With only 5% of the training set, performance in IRT models will be drastically degraded, both in terms of accuracy and in IT.The LSTM model is more significantly affected by noise than the NLSLSs and NSE models, and with only 5% of the target values, the performance in IRT models will be affected drastically. The type of noise values may affect the Ss, and the SL values for the long-term dependencies, and SSE models may affect the SL values, and SL values for the SL values, etc."}, {"heading": "4.2.2 Learning with More Data", "text": "However, our results (Figure 2) show that when performance of DNNs is measured by accuracy, all performance of DNNs increases almost linearly with training size. In contrast, IRT is more sensitive to training size and demonstrates the benefit of using IRT as an evaluation yardstick. For small training sets, the network cannot learn, and IRT values reflect this. As the size of training sets increases from 5000, performance in terms of training size improves and continues to improve as more data is used for training. In addition, there is a non-linear trend in performance for DNN models when training size increases. In human learning, a popular teaching method is NLN, where learning methods are non-linear."}, {"heading": "4.2.3 Learning Easy and Difficult Items", "text": "This year it has come to the point where it will be able to put itself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "4.2.4 Interpreting Patterns Learned", "text": "Table 3 shows the total number of sentence pairs correctly answered by the fully trained models for each category, as well as the change compared to the number of correctly answered sentence pairs for the models trained with only 100 examples. The categories in Table 3 are consistent with the labels in Section 3.3. As Table 3 shows, the DNN models outperform both the traditional ML models when trained on the full training set, and a consistent improvement with the increased training size. While the performance in some subsets for the traditional models decreases, the performance for each subset is just as good or better with a full training set for the DNN models. Our results show that both LSTM and NSE perform comparably in most cases. The LSTM model performs slightly better when dealing with restrictive words or phrases (H, I), Summary (M) and Conclusion (M)."}, {"heading": "5 Discussion and Future Work", "text": "In this paper, we demonstrate the reliability of IRT as a metric for RTE. IRT values are consistent in that they separate models based on performance in a similar way to accuracy, while we provide more information regarding the questions correctly answered by each model. Our experiments have shown that the IRT metric is consistent with standard accuracy by identifying powerful and low-performing models. We also examine the results of a number of models for RTE and show the performance gains in DNN models as the training size increases. Not only does raw performance improve, but performance in semantic subsets of the test set systematically improves. As the DNN models are trained with more data, we see learning patterns emerge that mirror those of humans. While with little training, simple and difficult items have a similar probability that the NLP models will be correctly answered."}], "references": [{"title": "Item Response Theory: Parameter Estimation Techniques, Second Edition", "author": ["Frank B. Baker", "Seock-Ho Kim."], "venue": "CRC Press.", "citeRegEx": "Baker and Kim.,? 2004", "shortCiteRegEx": "Baker and Kim.", "year": 2004}, {"title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings", "author": ["Tolga Bolukbasi", "Kai-Wei Chang", "James Y Zou", "Venkatesh Saligrama", "Adam T Kalai."], "venue": "D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and", "citeRegEx": "Bolukbasi et al\\.,? 2016", "shortCiteRegEx": "Bolukbasi et al\\.", "year": 2016}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "D. Christopher Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Associa-", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Recognizing subjectivity: A case study in manual tagging", "author": ["Rebecca F. Bruce", "Janyce M. Wiebe."], "venue": "Nat. Lang. Eng. 5(2):187\u2013205. https://doi.org/10.1017/S1351324999002181.", "citeRegEx": "Bruce and Wiebe.,? 1999", "shortCiteRegEx": "Bruce and Wiebe.", "year": 1999}, {"title": "Deep blue", "author": ["Murray Campbell", "A.Joseph Hoane", "Feng hsiung Hsu."], "venue": "Artificial Intelligence 134(1):57 \u2013 83. https://doi.org/10.1016/S00043702(01)00129-1.", "citeRegEx": "Campbell et al\\.,? 2002", "shortCiteRegEx": "Campbell et al\\.", "year": 2002}, {"title": "The role of nonlinear pedagogy in physical education", "author": ["Jia Yi Chow", "Keith Davids", "Chris Button", "Rick Shuttleworth", "Ian Renshaw", "Duarte Arajo."], "venue": "Review of Educational Research 77(3):251\u2013 278. https://doi.org/10.3102/003465430305615.", "citeRegEx": "Chow et al\\.,? 2007", "shortCiteRegEx": "Chow et al\\.", "year": 2007}, {"title": "The PASCAL Recognising Textual Entailment Challenge", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini."], "venue": "Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising", "citeRegEx": "Dagan et al\\.,? 2006", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Maximum likelihood estimation of observer error-rates using the em algorithm", "author": ["A. Philip Dawid", "Allan M. Skene."], "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics) 28(1):20\u201328. https://doi.org/10.2307/2346806.", "citeRegEx": "Dawid and Skene.,? 1979", "shortCiteRegEx": "Dawid and Skene.", "year": 1979}, {"title": "Building watson: An overview of the deepqa project. AI magazine 31(3):59\u201379", "author": ["David Ferrucci", "Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya A Kalyanpur", "Adam Lally", "J William Murdock", "Eric Nyberg", "John Prager"], "venue": null, "citeRegEx": "Ferrucci et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ferrucci et al\\.", "year": 2010}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka."], "venue": "CoRR abs/1410.5401. http://arxiv.org/abs/1410.5401.", "citeRegEx": "Graves et al\\.,? 2014", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "The unreasonable effectiveness of data", "author": ["Alon Halevy", "Peter Norvig", "Fernando Pereira."], "venue": "IEEE Intelligent Systems 24(2):8\u201312. https://doi.org/10.1109/MIS.2009.36.", "citeRegEx": "Halevy et al\\.,? 2009", "shortCiteRegEx": "Halevy et al\\.", "year": 2009}, {"title": "Learning whom to trust with mace", "author": ["Dirk Hovy", "Taylor Berg-Kirkpatrick", "Ashish Vaswani", "Eduard Hovy."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Hovy et al\\.,? 2013", "shortCiteRegEx": "Hovy et al\\.", "year": 2013}, {"title": "Proceedings of the 2016 ICML workshop on human interpretability in machine learning (WHI 2016)", "author": ["Been Kim", "Dmitry M. Malioutov", "Kush R. Varshney."], "venue": "CoRR abs/1607.02531. http://arxiv.org/abs/1607.02531.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, pages 1746\u20131751.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Building machines that learn and think like people", "author": ["Brenden M. Lake", "Tomer D. Ullman", "Joshua B. Tenenbaum", "Samuel J. Gershman."], "venue": "Behavioral and Brain Sciences pages 1\u2013101. https://doi.org/10.1017/S0140525X16001837.", "citeRegEx": "Lake et al\\.,? 2016", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "Building an evaluation scale using item response theory", "author": ["John P. Lalor", "Hao Wu", "Hong Yu."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 648\u2013657.", "citeRegEx": "Lalor et al\\.,? 2016", "shortCiteRegEx": "Lalor et al\\.", "year": 2016}, {"title": "Rationalizing neural predictions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 107\u2013117.", "citeRegEx": "Lei et al\\.,? 2016", "shortCiteRegEx": "Lei et al\\.", "year": 2016}, {"title": "Visualizing and understanding neural models in nlp", "author": ["Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Making sense of item response theory in machine learning", "author": ["Fernando Mart\u0131\u0301nez-Plumed", "Ricardo B.C. Prud\u0142ncio", "Adolfo Martnez Us", "Jos Hernndez-Orallo"], "venue": "In ECAI. IOS Press,", "citeRegEx": "Mart\u0131\u0301nez.Plumed et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mart\u0131\u0301nez.Plumed et al\\.", "year": 2016}, {"title": "A logical calculus of the ideas immanent in nervous activity", "author": ["Warren S McCulloch", "Walter Pitts."], "venue": "The bulletin of mathematical biophysics 5(4):115\u2013 133.", "citeRegEx": "McCulloch and Pitts.,? 1943", "shortCiteRegEx": "McCulloch and Pitts.", "year": 1943}, {"title": "Neural semantic encoders", "author": ["Tsendsuren Munkhdalai", "Hong Yu."], "venue": "CoRR abs/1607.04315. http://arxiv.org/abs/1607.04315.", "citeRegEx": "Munkhdalai and Yu.,? 2016", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "The benefits of a model of annotation", "author": ["Rebecca J. Passonneau", "Bob Carpenter."], "venue": "Transactions of the Association of Computational Linguistics 2:311\u2013 326. http://aclweb.org/anthology/Q14-1025.", "citeRegEx": "Passonneau and Carpenter.,? 2014", "shortCiteRegEx": "Passonneau and Carpenter.", "year": 2014}, {"title": "Scikit-learn: Machine learning in python", "author": ["Matthieu Perrot", "\u00c9douard Duchesnay."], "venue": "J. Mach. Learn. Res. 12:2825\u20132830.", "citeRegEx": "Perrot and Duchesnay.,? 2011", "shortCiteRegEx": "Perrot and Duchesnay.", "year": 2011}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomas Kocisky", "Phil Blunsom."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2016", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver."], "venue": "International Conference on Learning Representations. Puerto Rico.", "citeRegEx": "Schaul et al\\.,? 2016", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis."], "venue": "Nature 529:484\u2013 503. https://doi.org/10.1038/nature16961.", "citeRegEx": "Sutskever et al\\.,? 2016", "shortCiteRegEx": "Sutskever et al\\.", "year": 2016}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["Bradly C. Stadie", "Sergey Levine", "Pieter Abbeel."], "venue": "CoRR abs/1507.00814. http://arxiv.org/abs/1507.00814.", "citeRegEx": "Stadie et al\\.,? 2015", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "A dynamic systems approach to the development of cognition and action", "author": ["Esther Thelen", "Linda B Smith."], "venue": "MIT press.", "citeRegEx": "Thelen and Smith.,? 1996", "shortCiteRegEx": "Thelen and Smith.", "year": 1996}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals."], "venue": "CoRR abs/1611.03530. http://arxiv.org/abs/1611.03530.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "Artificial neural networks were inspired by a mathematical model of neurons in the human brain (McCulloch and Pitts, 1943).", "startOffset": 95, "endOffset": 122}, {"referenceID": 4, "context": "With recent advancements and rapid improvements of DNNs, the number of tasks where DNN models outperform humans continues to grow (Campbell et al., 2002; Ferrucci et al., 2010; Silver et al., 2016; Schaul et al., 2016; Stadie et al., 2015).", "startOffset": 130, "endOffset": 239}, {"referenceID": 8, "context": "With recent advancements and rapid improvements of DNNs, the number of tasks where DNN models outperform humans continues to grow (Campbell et al., 2002; Ferrucci et al., 2010; Silver et al., 2016; Schaul et al., 2016; Stadie et al., 2015).", "startOffset": 130, "endOffset": 239}, {"referenceID": 24, "context": "With recent advancements and rapid improvements of DNNs, the number of tasks where DNN models outperform humans continues to grow (Campbell et al., 2002; Ferrucci et al., 2010; Silver et al., 2016; Schaul et al., 2016; Stadie et al., 2015).", "startOffset": 130, "endOffset": 239}, {"referenceID": 26, "context": "With recent advancements and rapid improvements of DNNs, the number of tasks where DNN models outperform humans continues to grow (Campbell et al., 2002; Ferrucci et al., 2010; Silver et al., 2016; Schaul et al., 2016; Stadie et al., 2015).", "startOffset": 130, "endOffset": 239}, {"referenceID": 28, "context": "However, studies have shown that DNNs have the ability to \u201dmemorize\u201d training data and therefore easily overfit (Zhang et al., 2016).", "startOffset": 112, "endOffset": 132}, {"referenceID": 12, "context": "Because of this, interpretability in DNNs has emerged as an important research area (Kim et al., 2016; Li et al., 2016).", "startOffset": 84, "endOffset": 119}, {"referenceID": 17, "context": "Because of this, interpretability in DNNs has emerged as an important research area (Kim et al., 2016; Li et al., 2016).", "startOffset": 84, "endOffset": 119}, {"referenceID": 16, "context": "standing how and what a network learns can eliminate bias and overfitting in models and help us understand why certain predictions have been made (Lei et al., 2016; Bolukbasi et al., 2016).", "startOffset": 146, "endOffset": 188}, {"referenceID": 1, "context": "standing how and what a network learns can eliminate bias and overfitting in models and help us understand why certain predictions have been made (Lei et al., 2016; Bolukbasi et al., 2016).", "startOffset": 146, "endOffset": 188}, {"referenceID": 14, "context": "does not consider which questions are answered correctly and what that says about a model\u2019s ability to generalize and remember what it has learned (Lake et al., 2016).", "startOffset": 147, "endOffset": 166}, {"referenceID": 14, "context": "does not consider which questions are answered correctly and what that says about a model\u2019s ability to generalize and remember what it has learned (Lake et al., 2016). It has recently been shown that traditional measures of learning in Machine Learning (ML) cannot accurately explain the success of DNN models Zhang et al. (2016). In this paper we compare the performance of different DNN and traditional ML models to the intelligence of a human population and study whether and how they learn \u201cintelligence.", "startOffset": 148, "endOffset": 330}, {"referenceID": 15, "context": "IRT was introduced as a more descriptive metric for evaluating NLP systems (Lalor et al., 2016).", "startOffset": 75, "endOffset": 95}, {"referenceID": 0, "context": "IRT jointly models an individual\u2019s ability and item characteristics to predict performance (Baker and Kim, 2004).", "startOffset": 91, "endOffset": 112}, {"referenceID": 15, "context": "\u201cfactor\u201d), (ii) the probability of correctly answering an item is a function of the person\u2019s ability, (iii) responses to different items are independent of each other for a given ability level of the person, and (iv) responses from different individuals are independent of each other (Lalor et al., 2016).", "startOffset": 284, "endOffset": 304}, {"referenceID": 6, "context": "RTE attempts to classify semantic relationships between two sentences (Dagan et al., 2006).", "startOffset": 70, "endOffset": 90}, {"referenceID": 23, "context": "Recently deep learning models have set and surpassed state of the art results in RTE (Rockt\u00e4schel et al., 2016; Munkhdalai and Yu, 2016), in part due to the availability of the Stanford Natural Lan-", "startOffset": 85, "endOffset": 136}, {"referenceID": 20, "context": "Recently deep learning models have set and surpassed state of the art results in RTE (Rockt\u00e4schel et al., 2016; Munkhdalai and Yu, 2016), in part due to the availability of the Stanford Natural Lan-", "startOffset": 85, "endOffset": 136}, {"referenceID": 2, "context": "guage Inference (SNLI) corpus (Bowman et al., 2015).", "startOffset": 30, "endOffset": 51}, {"referenceID": 20, "context": "Passonneau and Carpenter (2014) model the probability a label is correct along with the probability of an annotator to label an item correctly according to the Dawid and Skene (1979) model, but", "startOffset": 0, "endOffset": 32}, {"referenceID": 7, "context": "Passonneau and Carpenter (2014) model the probability a label is correct along with the probability of an annotator to label an item correctly according to the Dawid and Skene (1979) model, but", "startOffset": 160, "endOffset": 183}, {"referenceID": 13, "context": "set or other NLP tasks: A Convolutional neural network (CNN) for sentence classification (Kim, 2014); The LSTM neural network model released with the SNLI data set (Bowman et al.", "startOffset": 89, "endOffset": 100}, {"referenceID": 2, "context": "set or other NLP tasks: A Convolutional neural network (CNN) for sentence classification (Kim, 2014); The LSTM neural network model released with the SNLI data set (Bowman et al., 2015); Neural Semantic Encoder (NSE), a memoryaugmented RNN (Munkhdalai and Yu, 2016).", "startOffset": 164, "endOffset": 185}, {"referenceID": 20, "context": ", 2015); Neural Semantic Encoder (NSE), a memoryaugmented RNN (Munkhdalai and Yu, 2016).", "startOffset": 62, "endOffset": 87}, {"referenceID": 2, "context": "The data consists of approximately 1000 human annotator responses for a selection of the SNLI data set (Bowman et al., 2015).", "startOffset": 103, "endOffset": 124}, {"referenceID": 14, "context": "For data we obtained the human response patterns and IRT models from Lalor et al. (2016). The data consists of approximately 1000 human annotator responses for a selection of the SNLI data set (Bowman et al.", "startOffset": 69, "endOffset": 89}, {"referenceID": 15, "context": "els (and subsequent test sets) were fit by Lalor et al. (2016), based on the inter-annotator agreement from the original SNLI data set: (i) entailment items with 5 of 5 agreement (5E), (ii) contradiction items with 5 of 5 agreement (5C), (iii) neutral items with 5 of 5 agreement (5N), (iv) contradiction items with 4 of 5 agreement (4C), and (v) neutral items with 4 of 5 agreement (4N).", "startOffset": 43, "endOffset": 63}, {"referenceID": 15, "context": "5 of answering the question correctly, which is appropriate for analyzing NLP models (Lalor et al., 2016).", "startOffset": 85, "endOffset": 105}, {"referenceID": 15, "context": "It may be the case that IRT scores for entailment drop so quickly because the entailment test set is very easy (Lalor et al., 2016).", "startOffset": 111, "endOffset": 131}, {"referenceID": 10, "context": "the training set size when training a model, performance will also increase (Halevy et al., 2009).", "startOffset": 76, "endOffset": 97}, {"referenceID": 5, "context": "In human learning, a popular teaching methodology is a nonlinear teaching pedagogy, where curricula are flexible and adapted according to students\u2019 learning styles and speeds (Chow et al., 2007).", "startOffset": 175, "endOffset": 194}, {"referenceID": 27, "context": "The nonlinear pedagogy seems to suggest a nonlinear learning process in humans (Thelen and Smith, 1996), which is comparable to the nonlinear trend found in the DNN models as shown by our analyses.", "startOffset": 79, "endOffset": 103}, {"referenceID": 9, "context": "Previous work has shown that memory networks are able to learn faster than LSTMs (Graves et al., 2014).", "startOffset": 81, "endOffset": 102}, {"referenceID": 15, "context": "We must consider the specific data subset for this result, as the 5E subset is composed of very easy items (Lalor et al., 2016).", "startOffset": 107, "endOffset": 127}, {"referenceID": 10, "context": "in each DNN model that we tested, we can say that by increasing training size for an NLP model, not only does the expected overall performance increase (Halevy et al., 2009), but the models exhibit a more human-like learning capability with", "startOffset": 152, "endOffset": 173}], "year": 2017, "abstractText": "Deep neural networks (DNNs) have set state of the art results in many machine learning and NLP tasks. However, we do not have a strong understanding of what DNN models learn. In this paper, we examine learning in DNNs through analysis of their outputs. We compare DNN performance directly to a human population, and use characteristics of individual data points such as difficulty to see how well models perform on easy and hard examples. We investigate how training size and the incorporation of noise affect a DNN\u2019s ability to generalize and learn. Our experiments show that unlike traditional machine learning models (e.g., Naive Bayes, Decision Trees), DNNs exhibit human-like learning properties. As they are trained with more data, they are more able to distinguish between easy and difficult items, and performance on easy items improves at a higher rate than difficult items. We find that different DNN models exhibit different strengths in learning and are robust to noise in training data.", "creator": "LaTeX with hyperref package"}}}