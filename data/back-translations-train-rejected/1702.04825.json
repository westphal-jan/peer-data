{"id": "1702.04825", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2017", "title": "Learning to Use Learners' Advice", "abstract": "In this paper, we study a variant of the framework of online learning using expert advice with limited/bandit feedback---we consider each expert a learning entity and thereby capture scenarios that are more realistic and practical for real-world applications. In our setting, the feedback at any time $t$ is limited in a sense that it is only available to the expert $i^t$ that has been selected by the central algorithm (forecaster), i.e., only the expert $i^t$ receives feedback from the environment and gets to learn at time $t$. We consider a generic black-box approach whereby the forecaster doesn't control or know the learning dynamics of the experts apart from knowing the following no-regret learning property: the average regret of any expert $j$ vanishes at a rate of at least $O(t_j^{\\beta-1})$ with $t_j$ learning steps where $\\beta \\in [0, 1]$ is a parameter. We prove the following hardness result: without any coordination between the forecaster and the experts, it is impossible to design a forecaster achieving no-regret guarantees in the worst-case. In order to circumvent this hardness result, we consider a practical assumption allowing the forecaster to \"guide\" the learning process of the experts by filtering/blocking some of the feedbacks observed by them from the environment, i.e., not allowing the selected expert $i^t$ to learn at time $t$ for some time steps. Then, we design a novel no-regret learning algorithm \\algo for this problem setting by carefully guiding the feedbacks observed by experts. We prove that \\algo achieves the worst-case expected cumulative regret of $O(T^\\frac{1}{2 - \\beta})$ after $T$ time steps and matches the regret bound of $\\Theta(T^\\frac{1}{2})$ for the special case of multi-armed bandits.", "histories": [["v1", "Thu, 16 Feb 2017 00:22:16 GMT  (258kb,D)", "https://arxiv.org/abs/1702.04825v1", null], ["v2", "Fri, 17 Feb 2017 21:39:29 GMT  (360kb,D)", "http://arxiv.org/abs/1702.04825v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["adish singla", "hamed hassani", "reas krause"], "accepted": false, "id": "1702.04825"}, "pdf": {"name": "1702.04825.pdf", "metadata": {"source": "CRF", "title": "Learning to Use Learners\u2019 Advice", "authors": ["Adish Singla", "Hamed Hassani", "Andreas Krause"], "emails": ["ADISH.SINGLA@INF.ETHZ.CH", "HAMED@INF.ETHZ.CH", "KRAUSEA@ETHZ.CH"], "sections": [{"heading": null, "text": "In the spirit of competition against the best action after the event on the problem of multi-armed bandits, our goal is to make competitive the cumulative losses that the algorithm could suffer through the constant selection of an expert. We prove the following hardness result: Without any coordination between the forecaster and the experts, it is impossible to design a forecaster who offers guarantees without regrets. In order to circumvent this hardness result, we consider a practical assumption that allows the forecaster to \"guide\" the learning process of the experts by filtering / blocking some of the feedback they observe from the environment, i.e. not allowing the selected expert to learn in some increments of time. Subsequently, we design a novel learning algorithm without regrets, which allows LEARNEXP to address this problem by carefully managing the feedback observed by experts. We prove that LEARNEXP in the worst case achieves the expected cumulative regret of O (1 T-T), which lasts for several steps (T-T)."}, {"heading": "1. Introduction", "text": "Many real-world applications involve repeated decisions under uncertainty - for example, the selection of one of the various items recommended to the user, the dynamic allocation of resources to the available stock options in a financial market, or the sequential decision on the next medical test in healthcare. In addition, feedback in this context is often limited in the sense that only the loss / reward associated with the actions taken by the system is observed, the so-called bandit feedback setting. Online learning through expert advice with bandit / limited feedback is a well-researched framework to model the above application scenarios (Freund und Schapire, 1995; Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) and addresses the fundamental question of how a learning algorithm should exchange research (the cost of acquiring new information) for exploitation (the cost of acquiring new information and dealing with it)."}, {"heading": "1.1. Motivating Applications", "text": "The modelling of experts as learning units realistically captures many practical scenarios of how these experts would be defined / encountered in real-world applications, such as seeking advice from fellow players or friends, aggregating predictive recommendations from trading agents or different marketplaces, product tests with human participants who could adapt over time, gathering information from crowd-sourcing participants who could learn over time, the problem of meta-learning and hyperparameter tuning, where different learning algorithms are treated as experts (cf. Baram et al. (2004); Hsu and Lin (2015), and many more. As a concrete ongoing example, we consider the problem of learning to offer users personalized offers / discount coupons that allow new companies to create incentives and attract more customers (cf. Edelman et al. (2011; Singla et al., 2016). An emerging trend is, for example, that deal aggregator websites such as Y1 provide personalized coupon services by selecting the corresponding recommendation algorithms for live users on a daily basis."}, {"heading": "1.2. Experts as Learning Entities: Challenges and Our Results", "text": "We provide an overview of our approach, the main challenges in designing a forecaster without regrets, but it is important that we apply these terms (i.e., we consider an online attitude similar to predicting the opposing online learning).We consider an online prediction similar to predicting the opposing online learning using expert advice that can only be advised by an expert. (cf. Kale (2014) for further discussion of this aspect).Specifically, the forecaster selects an expert whom he chooses for an action that is recommended by the expert and causes a loss. The notion of regret, i.e. if the experts do not perform learning activities, is the concept of regret well suited (Auer et al., 2002)."}, {"heading": "2. The Model", "text": "We have the following units in our problem definition: (i) an algorithm ALGO as a forecaster; (ii) the adversary ADV acting on behalf of the environment; and (iii) N experts, the EXPj-j-j-1,.. N \"(henceforth referred to as [N])."}, {"heading": "2.1. Specification of the Interaction", "text": "It is about the question of to what extent it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and in which it is about a way and in which it is about a way, in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way and a way and a way it is about a way in which it is about a way in which it is about a way and a way and a way and a way it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way and a way it is about a way and a way and a way in which it is about a way and a way in which it is about a way and a way and a way it is about a way in which it is about a way and a way and a way and a way it is about a way in which it is about a way and a way and a way and a way"}, {"heading": "2.2. Specification of the Experts", "text": "We consider a generic black box approach in which ALGO does not know and cannot control the internal dynamics of the experts. (In order to formalize the goal and guarantees we are aiming for, we now provide a generic specification of the experts.) Let us designate an instance of feedback received by EXPit through a tupel h = (atit, xt, f t (atit). (Let us designate the feedback history for EXPj, i.e. a ordered sequence of feedback instances observed by EXPj up to a certain time.) The length of the learning steps for EXPj = (h1, h2,.) let us designate the feedback history for EXPj, i.e. a ordered sequence of feedback instances observed by EXPJ up to a certain time. The length of the learning steps for EXPj = (h2,.) let us designate the number of learning steps recommended for EXPj, i.e. an ordered sequence of feedback instances observed by EXPJ up to a certain time."}, {"heading": "2.3. Our Objective: No-Regret Guarantees", "text": "Intuitively, we want to be competitive w.r.t. the cumulative losses that the algorithm could sustain by following the policy of always using the advice of a single expert - such a policy ensures that the individual expert gets more feedback to improve his learning state and thus cause less cumulative losses. However, this is a difficult problem when the experts are units of learning. What can go wrong, for example, is that the best expert may have a slow learning / convergence rate, resulting in high initial losses, essentially \"weighing down\" the algorithm of that expert. This in turn further exacerbates the problem for the best expert in the bank feedback environment, as that expert is less selected and will have fewer learning steps to improve his condition. This adds new challenges to the classical trade-off between exploration and exploitation, indicating that the need to address this problem at a higher rate will be forced on the best expert in the bank feedback environment.Let's start by making the 2006 classic Cesa-Cneesa Cneesa literature dependent on Cneesa-al, and the 2002 Cneesa-Cesa literature."}, {"heading": "3. Hardness Result", "text": "We show in this section that in the absence of coordination between the forecasters and the experts, it is impossible to design a forecaster who does not provide guarantees for the worst-case scenario."}, {"heading": "4. Our Algorithm LEARNEXP", "text": "In this section, we present a practical assumption that allows the forecaster to \"guide\" the learning process of experts, and then we design our main algorithm LEARNEXP with verifiable no-regrets guarantees."}, {"heading": "4.1. Guided Feedbacks", "text": "In order to circumvent the hardship shown in Section 3, we are now considering a practical assumption motivated by the application setting of deal aggregator sites, as in Section 1. Normally, a deal aggregator site interacts with users on behalf of the individual daily deal marketplaces (experts) and could therefore control the feedback flow to these marketplaces. Therefore, we allow the forecaster to \"direct\" the learning process of the experts by filtering / blocking a portion of the feedback that the experts in the area receive. Remember that the selected expert EXPit could block the feedback at this point in time, according to the interaction model presented in Section 2. We are now looking at the setting with the following additional power in the forecaster's hands: In order to guide the expert learning process, the forecaster at this point in time t could block the feedback, i.e. the expert EXPit would not be observing feedback at this point in time and therefore not learning (as all the experts were not selected at this point)."}, {"heading": "4.2. Algorithm LEARNEXP", "text": "With this additional power of the forecasters to guide feedback, we develop our main algorithm LEARNEXP, presented in algorithm 2. The selection strategy of the algorithm LEARNEXP is comparable to the EXP family of algorithms, and in particular it is equivalent to the EXP3 algorithm by Auer et al. (2002) The core message of the feedback observed by the experts is presented in lines 10,11 and 12,5, whereby the selected experts always observe feedback in time t - for this protocol applies the hardness result of theorem 1. Our algorithm LEARNEXP instead decides whether the expert should observe / use the feedback on the basis of the result."}, {"heading": "4.3. Theoretical Guarantees", "text": "Next, we will analyze the theoretical guarantees of our LEARNEXP algorithm. One approach is to consider a specific class of no-regrets learning algorithms that experts implement and provide guarantees for this course. Instead, we will introduce a novel, generic notion of \"soft\" no-regrets learning - our theoretical guarantees will then be demonstrated to those experts who show no regrets and have a smooth learning dynamics. Next, we will introduce this notion and discuss (see Proposition 2) the class of no-regrets learning algorithms that also fulfill the limitation of a smooth learning dynamics."}, {"heading": "4.3.1. SMOOTH LEARNING DYNAMICS", "text": "In our bandit feedback setting, not all experts can observe feedback at a specific time step, and therefore the history of feedback instances received by a particular expert is, of course, \"sparse.\" In order to formalize the behavior of the learning algorithm under this sparse feedback, we are now introducing a new concept, called smooth learning dynamics, to complement the learning dynamics defined in (1) without regret. Let's look at the same fixed sequence S used in the definition (1) and an expert EXPj. However, instead of observing feedback at every step, we say that the expert EXPj can observe feedback only sporadically at a rate of \u03b1 (0, 1) - call this a \"sparse story,\" which is called Hlj. Then the limitation of smooth learning dynamics ensures that the expected regret of the expert EXPj is observed only sporadically at a rate of \u03b1 (1, - \"sparse programming,\" \u03b1 \"-\" \u03b1. \""}, {"heading": "4.3.2. NO-REGRET GUARANTEES OF LEARNEXP", "text": "Next, we will prove the no-remorse guarantees of our algorithm LEARNEXP, which are formally set out in Theorem 3. The following theorem (in which only the leading terms of T are given and other constants such as N are dropped) offers the no-remorse guarantees of LEARNEXP against the best expert in retrospect according to (2). The proof will be provided in the appendix. Theorem 3 Let T be the fixed time horizon. Consider that the best expert j, j, has no remorse, has smooth learning dynamics, which are parameterized by \u03b2j, 1, 0 and LEARNEXP by entering \u03b2 [0, 1] in such a way that \u03b2, \u03b22, \u03b2, \u03b2, 2, \u03b2, \u03b2, \u03b2, \u03b2, \u03b2, 2, \u03b2, 2, \u03b2, 2, \u03b2, 2, \u03b2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5."}, {"heading": "5. Background and Related Work", "text": "In this section we give an overview of the relevant literature."}, {"heading": "5.1. Background", "text": "The groundbreaking work of Littlestone and Warmuth (1994); Cesa-Bianchi et al. (1997) initiated the study on the use of expert advice on prediction problems, and Freund and Schapire (1995) introduced the HEDGE algorithm for the general problem of dynamic resource allocation using expert advice. To address this, Auer et al. (2002) expanded this framework to include the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4 and their variants), which observe only the loss / reward associated with the actions taken by the system, called bandit feedback setting."}, {"heading": "5.2. Related Work", "text": "In fact, it is a very rare disease, and it is a disease that is usually a serious one."}, {"heading": "6. Conclusions", "text": "In this paper, we examined the online learning framework on the basis of expert advice with bandit feedback with an important practical consideration: how do we use the advice of the experts if they are themselves learning units? As a first contribution, we demonstrated the hardness result, which states that it is impossible to achieve guarantees without regret if the experts receive direct feedback from the environment and there is no further coordination between forecasters / experts. Our hardness result sheds light on the complexity of the problem in applying this online learning framework to applications in the real world, where it is natural for experts to show learning dynamics. Then, we looked at a practical assumption of \"guided\" feedback, where the forecaster can block / filter the feedback expected from the chosen environment. Under this setting, we proposed a novel algorithm LEARNEXP - we proved that LEARNEXP is the worst possible cumulative regret of O (T 1 2) \u2212 T after the individual parameter is achieved at the time steps of the duration."}, {"heading": "Appendix A. Proof of Theorem 1", "text": "In fact, the fact is that most of us are able to show ourselves what we are doing to save the world. (...) In the second half of the last decade, the world has changed into another world. (...) In the second half of the last decade, the world has changed. (...) In the third half of the last decade, the world has changed. (...) In the second half of the last decade, the world has changed. (...) In the second half of the last decade, the world has changed. (...) In the second half of the last century, the world has changed. (...) In the second half of the last decade, the world has changed. (...) In the second half of the last decade, the world has changed. (...) In the second half of the last decade, the world has changed. (...) In the second half of the last century, the world has changed. (...) In the second half of the last decade, the world has changed."}, {"heading": "Appendix B. Proof of Proposition 2", "text": "We will show that such an algorithm exhibits smooth learning dynamics. \u2212 See that OCP exhibits remorse for the size O (\u221a T) (i.e. \u03b2j = 1 / 2) (i.e. \u03b2j = 1 / 2) (i.e. \u03b2j = 1 / 2). \u2212 See more precisely, we have the following OCP algorithm, which runs according to algorithm 3. \u2212 See that the dynamics of learning is equivalent for OCP to proving that Alpha-OCP suffers remorse for the size O. \u2212 See more closely, we have the following Lemma.Algorithms 3: Alpha-OCP 1 Problem: Convex set S; Sequence of convex functions f t t: S \u2192 R + 2 Parameters: Learning rates t for t (T) 3: w0 Alpha-OCP."}, {"heading": "Appendix C. Proof of Theorem 3", "text": "In this section, we provide proof of the no-regret guarantees of our algorithms LEARNEXP = \u03b2 = \u03b2 repentance. We follow a step-by-step approach, starting from the limits of external regret of LEARNEXP. (2002), we can specify the following limits of external regret of our algorithms LEARNEXP against each expert EXPk, where we apply the limits of external regret, whereby these limits given by external regret are observed only w.r.t. to the post-hoc sequence of actions performed and losses during the execution of the algorithms LEARNEXP. (lt) (x t, Htit, LEARNEXP)) \u2212 E (T, T = 1 lt.) \u2212 E (."}], "references": [{"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert Schapire"], "venue": "In ICML,", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Corralling a band of bandit algorithms", "author": ["Alekh Agarwal", "Haipeng Luo", "Behnam Neyshabur", "Robert E. Schapire"], "venue": null, "citeRegEx": "Agarwal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2016}, {"title": "Online bandit learning against an adaptive adversary: from regret to policy regret", "author": ["Raman Arora", "Ofer Dekel", "Ambuj Tewari"], "venue": "In ICML,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches", "author": ["Baruch Awerbuch", "Robert D Kleinberg"], "venue": "In STOC,", "citeRegEx": "Awerbuch and Kleinberg.,? \\Q2004\\E", "shortCiteRegEx": "Awerbuch and Kleinberg.", "year": 2004}, {"title": "Online choice of active learning algorithms", "author": ["Yoram Baram", "Ran El-Yaniv", "Kobi Luz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Baram et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Baram et al\\.", "year": 2004}, {"title": "Partial monitoring \u2013 Classification, regret bounds, and algorithms", "author": ["G\u00e1bor Bart\u00f3k", "Dean P Foster", "D\u00e1vid P\u00e1l", "Alexander Rakhlin", "Csaba Szepesv\u00e1ri"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2014}, {"title": "Optimal exploration-exploitation in a multi-armedbandit problem with non-stationary rewards", "author": ["Omar Besbes", "Yonatan Gur", "Assaf J. Zeevi"], "venue": "In NIPS,", "citeRegEx": "Besbes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Besbes et al\\.", "year": 2014}, {"title": "Contextual bandit algorithms with supervised learning guarantees", "author": ["Alina Beygelzimer", "John Langford", "Lihong Li", "Lev Reyzin", "Robert E Schapire"], "venue": "In AISTATS,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2011}, {"title": "Learning, regret minimization, and equilibria", "author": ["Avrim Blum", "Yishay Monsour"], "venue": null, "citeRegEx": "Blum and Monsour.,? \\Q2007\\E", "shortCiteRegEx": "Blum and Monsour.", "year": 2007}, {"title": "Regret analysis of stochastic and nonstochastic multiarmed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D.P. Helmbold", "D. Haussler", "R. Schapire", "M. Warmuth"], "venue": "Journal of the ACM,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "To groupon or not to groupon: The profitability of deep discounts", "author": ["Benjamin Edelman", "Sonia Jaffe", "Scott Duke Kominers"], "venue": "Marketing Letters,", "citeRegEx": "Edelman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Edelman et al\\.", "year": 2011}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "In COLT,", "citeRegEx": "Freund and Schapire.,? \\Q1995\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1995}, {"title": "Bandit processes and dynamic allocation indices", "author": ["John C Gittins"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Gittins.,? \\Q1979\\E", "shortCiteRegEx": "Gittins.", "year": 1979}, {"title": "Active learning by learning", "author": ["Wei-Ning Hsu", "Hsuan-Tien Lin"], "venue": "In AAAI,", "citeRegEx": "Hsu and Lin.,? \\Q2015\\E", "shortCiteRegEx": "Hsu and Lin.", "year": 2015}, {"title": "Multiarmed bandits with limited expert advice", "author": ["Satyen Kale"], "venue": "In COLT, pages 107\u2013122,", "citeRegEx": "Kale.,? \\Q2014\\E", "shortCiteRegEx": "Kale.", "year": 2014}, {"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["J. Langford", "T. Zhang"], "venue": "In NIPS,", "citeRegEx": "Langford and Zhang.,? \\Q2007\\E", "shortCiteRegEx": "Langford and Zhang.", "year": 2007}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Lihong Li", "Wei Chu", "John Langford", "Robert E Schapire"], "venue": "In WWW,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth"], "venue": "Info and Computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Adaptive bandits: Towards the best history-dependent strategy", "author": ["Odalric-Ambrym Maillard", "R\u00e9mi Munos"], "venue": "In AISTATS,", "citeRegEx": "Maillard and Munos.,? \\Q2011\\E", "shortCiteRegEx": "Maillard and Munos.", "year": 2011}, {"title": "Tighter bounds for multi-armed bandits with expert advice", "author": ["H.B. McMahan", "M.J. Streeter"], "venue": "In COLT,", "citeRegEx": "McMahan and Streeter.,? \\Q2009\\E", "shortCiteRegEx": "McMahan and Streeter.", "year": 2009}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2011}, {"title": "Actively learning hemimetrics with applications to eliciting user preferences", "author": ["Adish Singla", "Sebastian Tschiatschek", "Andreas Krause"], "venue": "In ICML,", "citeRegEx": "Singla et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Singla et al\\.", "year": 2016}, {"title": "Adapting to a changing environment: the brownian restless bandits", "author": ["Aleksandrs Slivkins", "Eli Upfal"], "venue": "In COLT, pages 343\u2013354,", "citeRegEx": "Slivkins and Upfal.,? \\Q2008\\E", "shortCiteRegEx": "Slivkins and Upfal.", "year": 2008}, {"title": "Fast convergence of regularized learning in games", "author": ["Vasilis Syrgkanis", "Alekh Agarwal", "Haipeng Luo", "Robert E Schapire"], "venue": "In NIPS,", "citeRegEx": "Syrgkanis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Syrgkanis et al\\.", "year": 2015}, {"title": "Restless bandits: Activity allocation in a changing world", "author": ["P. Whittle"], "venue": "Journal of applied probability,", "citeRegEx": "Whittle.,? \\Q1988\\E", "shortCiteRegEx": "Whittle.", "year": 1988}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [{"referenceID": 14, "context": "Online learning using expert advice with bandit/limited feedback is a well-studied framework to model the above-mentioned application settings (Freund and Schapire, 1995; Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) and addresses the fundamental question of how a learning algorithm should trade-off exploration (the cost of acquiring new information) versus exploitation (acting greedily based on current information to minimize instantaneous losses).", "startOffset": 143, "endOffset": 251}, {"referenceID": 3, "context": "Online learning using expert advice with bandit/limited feedback is a well-studied framework to model the above-mentioned application settings (Freund and Schapire, 1995; Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) and addresses the fundamental question of how a learning algorithm should trade-off exploration (the cost of acquiring new information) versus exploitation (acting greedily based on current information to minimize instantaneous losses).", "startOffset": 143, "endOffset": 251}, {"referenceID": 11, "context": "Online learning using expert advice with bandit/limited feedback is a well-studied framework to model the above-mentioned application settings (Freund and Schapire, 1995; Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) and addresses the fundamental question of how a learning algorithm should trade-off exploration (the cost of acquiring new information) versus exploitation (acting greedily based on current information to minimize instantaneous losses).", "startOffset": 143, "endOffset": 251}, {"referenceID": 10, "context": "Online learning using expert advice with bandit/limited feedback is a well-studied framework to model the above-mentioned application settings (Freund and Schapire, 1995; Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) and addresses the fundamental question of how a learning algorithm should trade-off exploration (the cost of acquiring new information) versus exploitation (acting greedily based on current information to minimize instantaneous losses).", "startOffset": 143, "endOffset": 251}, {"referenceID": 13, "context": "As a concrete running example, we consider the problem of learning to offer personalized deals / discount coupons to users enabling new businesses to incentivize and attract more customers (Edelman et al., 2011; Singla et al., 2016).", "startOffset": 189, "endOffset": 232}, {"referenceID": 24, "context": "As a concrete running example, we consider the problem of learning to offer personalized deals / discount coupons to users enabling new businesses to incentivize and attract more customers (Edelman et al., 2011; Singla et al., 2016).", "startOffset": 189, "endOffset": 232}, {"referenceID": 13, "context": "However, these marketplaces (experts) themselves would be learning to optimize the coupons to offer, for instance, the discount price or the type of the coupon based on historic interactions with users (Edelman et al., 2011).", "startOffset": 202, "endOffset": 224}, {"referenceID": 5, "context": "Baram et al. (2004); Hsu and Lin (2015)), and many more.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Baram et al. (2004); Hsu and Lin (2015)), and many more.", "startOffset": 0, "endOffset": 40}, {"referenceID": 3, "context": "We consider an online setting similar to that of adversarial online learning using experts\u2019 advice with bandit feedback (Auer et al., 2002).", "startOffset": 120, "endOffset": 139}, {"referenceID": 3, "context": ", when the experts are not learning entities, the EXP3 algorithm (Auer et al., 2002) is well-suited for this problem setting achieving the optimal regret bounds.", "startOffset": 65, "endOffset": 84}, {"referenceID": 3, "context": "We consider an online setting similar to that of adversarial online learning using experts\u2019 advice with bandit feedback (Auer et al., 2002). However, to keep the presentation more general (e.g., we do not necessarily require that the sets of actions are shared across different experts), the forecaster selects / seeks advice from only one expert it at any time t (cf. Kale (2014) for more discussion about this aspect).", "startOffset": 121, "endOffset": 381}, {"referenceID": 3, "context": "We consider an online setting similar to that of adversarial online learning using experts\u2019 advice with bandit feedback (Auer et al., 2002). However, to keep the presentation more general (e.g., we do not necessarily require that the sets of actions are shared across different experts), the forecaster selects / seeks advice from only one expert it at any time t (cf. Kale (2014) for more discussion about this aspect). More specifically, at time t, the forecaster selects an expert it, performs an action ait recommended by the expert i t, and incurs a loss l(ait) set by the adversary. The notion of regret. In the standard framework, i.e., when the experts are not learning entities, the EXP3 algorithm (Auer et al., 2002) is well-suited for this problem setting achieving the optimal regret bounds. However, it is important to note that the classical notion of external regret used in the literature (cf. Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Bubeck and Cesa-Bianchi (2012)) does not provide any meaningful guarantees in our setting in terms of competing against the \u201cbest\u201d expert.", "startOffset": 121, "endOffset": 929}, {"referenceID": 3, "context": "We consider an online setting similar to that of adversarial online learning using experts\u2019 advice with bandit feedback (Auer et al., 2002). However, to keep the presentation more general (e.g., we do not necessarily require that the sets of actions are shared across different experts), the forecaster selects / seeks advice from only one expert it at any time t (cf. Kale (2014) for more discussion about this aspect). More specifically, at time t, the forecaster selects an expert it, performs an action ait recommended by the expert i t, and incurs a loss l(ait) set by the adversary. The notion of regret. In the standard framework, i.e., when the experts are not learning entities, the EXP3 algorithm (Auer et al., 2002) is well-suited for this problem setting achieving the optimal regret bounds. However, it is important to note that the classical notion of external regret used in the literature (cf. Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Bubeck and Cesa-Bianchi (2012)) does not provide any meaningful guarantees in our setting in terms of competing against the \u201cbest\u201d expert.", "startOffset": 121, "endOffset": 961}, {"referenceID": 3, "context": "We consider an online setting similar to that of adversarial online learning using experts\u2019 advice with bandit feedback (Auer et al., 2002). However, to keep the presentation more general (e.g., we do not necessarily require that the sets of actions are shared across different experts), the forecaster selects / seeks advice from only one expert it at any time t (cf. Kale (2014) for more discussion about this aspect). More specifically, at time t, the forecaster selects an expert it, performs an action ait recommended by the expert i t, and incurs a loss l(ait) set by the adversary. The notion of regret. In the standard framework, i.e., when the experts are not learning entities, the EXP3 algorithm (Auer et al., 2002) is well-suited for this problem setting achieving the optimal regret bounds. However, it is important to note that the classical notion of external regret used in the literature (cf. Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Bubeck and Cesa-Bianchi (2012)) does not provide any meaningful guarantees in our setting in terms of competing against the \u201cbest\u201d expert.", "startOffset": 121, "endOffset": 993}, {"referenceID": 14, "context": "Somewhat surprisingly, this hardness result holds when playing against an oblivious (non-adaptive) adversary and even if restricting the experts to be implementing some well-studied online learning algorithms, for instance, the HEDGE algorithm (Freund and Schapire, 1995).", "startOffset": 244, "endOffset": 271}, {"referenceID": 11, "context": "Somewhat surprisingly, this hardness result holds when playing against an oblivious (non-adaptive) adversary and even if restricting the experts to be implementing some well-studied online learning algorithms, for instance, the HEDGE algorithm (Freund and Schapire, 1995). The fundamental challenge leading to this hardness result arises from the fact that the forecaster\u2019s selection strategy affects the feedback sequences observed by the experts which in turn alters their learning process. \u201cGuided\u201d feedbacks and achieving no-regret guarantees. In order to circumvent this hardness result, we consider the following practical assumption: we allow the forecaster to \u201cguide\u201d the learning process of the experts by filtering/blocking some of the feedbacks observed by them from the environment, i.e., the selected expert it would not learn at time t for some time steps. For instance, in the motivating application of offering personalized deals to users, the deal-aggregator site (forecaster) often primarily interacts with users on behalf of the individual daily-deal marketplaces (experts) and hence can control the flow of feedback to these marketplaces. Alternatively, we note that this process of guiding and restricting the feedback can be achieved via coordination between the forecaster and the selected expert it with a 1-bit of communication at time t. Given this additional control, we design a novel algorithm LEARNEXP for the forecaster which carefully guides the feedbacks observed by experts. We prove that LEARNEXP achieves the worst-case expected cumulative regret of O(T 1 2\u2212\u03b2 ) after T time steps against an oblivious adversary for a rich family of no-regret learning algorithms that experts may be implementing. For the special case of multiarmed bandits, algorithm LEARNEXP is equivalent to that of the well-studied EXP3 algorithm and hence matches the optimal regret bound of \u0398(T 1 2 ). Connections to the existing results. Maillard and Munos (2011) studied the problem of competing against an adaptive adversary when the adversary\u2019s reward generation policy is restricted to a pre-specified set of known models.", "startOffset": 245, "endOffset": 1973}, {"referenceID": 8, "context": "Bubeck and Cesa-Bianchi (2012) for a variant of the algorithm).", "startOffset": 0, "endOffset": 31}, {"referenceID": 0, "context": "Our work is also related to contemporary work by Agarwal et al. (2016), who study a variant of the problem tackled by Maillard and Munos (2011) and also prove a hardness result similar to that of ours.", "startOffset": 49, "endOffset": 71}, {"referenceID": 0, "context": "Our work is also related to contemporary work by Agarwal et al. (2016), who study a variant of the problem tackled by Maillard and Munos (2011) and also prove a hardness result similar to that of ours.", "startOffset": 49, "endOffset": 144}, {"referenceID": 0, "context": "Our work is also related to contemporary work by Agarwal et al. (2016), who study a variant of the problem tackled by Maillard and Munos (2011) and also prove a hardness result similar to that of ours. Note that, in applications where the experts directly receive feedback from the environment, implementing the strategies of Maillard and Munos (2011); Agarwal et al.", "startOffset": 49, "endOffset": 352}, {"referenceID": 0, "context": "Our work is also related to contemporary work by Agarwal et al. (2016), who study a variant of the problem tackled by Maillard and Munos (2011) and also prove a hardness result similar to that of ours. Note that, in applications where the experts directly receive feedback from the environment, implementing the strategies of Maillard and Munos (2011); Agarwal et al. (2016) would require the forecaster to communicate the probability p with which the expert it was selected at time t.", "startOffset": 49, "endOffset": 375}, {"referenceID": 11, "context": ", T (henceforth denoted as [T ]); for simplicity we assume that T is known in advance to the algorithm and the results in this paper can be extended to an unknown horizon via the usual doubling trick (Cesa-Bianchi and Lugosi, 2006).", "startOffset": 200, "endOffset": 231}, {"referenceID": 11, "context": "The feedback could be more general, for instance, receiving a binary signal of rejection or acceptance of the offered deal when an expert is implementing a dynamic pricing based algorithm via the partial monitoring framework (Cesa-Bianchi and Lugosi, 2006; Bart\u00f3k et al., 2014).", "startOffset": 225, "endOffset": 277}, {"referenceID": 6, "context": "The feedback could be more general, for instance, receiving a binary signal of rejection or acceptance of the offered deal when an expert is implementing a dynamic pricing based algorithm via the partial monitoring framework (Cesa-Bianchi and Lugosi, 2006; Bart\u00f3k et al., 2014).", "startOffset": 225, "endOffset": 277}, {"referenceID": 3, "context": "we will use lmax = 1 (Auer et al., 2002).", "startOffset": 21, "endOffset": 40}, {"referenceID": 14, "context": "We consider an oblivious (non-adaptive adversary) as is usual in the literature (Freund and Schapire, 1995; Auer et al., 2002), i.", "startOffset": 80, "endOffset": 126}, {"referenceID": 3, "context": "We consider an oblivious (non-adaptive adversary) as is usual in the literature (Freund and Schapire, 1995; Auer et al., 2002), i.", "startOffset": 80, "endOffset": 126}, {"referenceID": 3, "context": "Let us begin by looking at the classic notion of external regret used in the literature (Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012).", "startOffset": 88, "endOffset": 169}, {"referenceID": 11, "context": "Let us begin by looking at the classic notion of external regret used in the literature (Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012).", "startOffset": 88, "endOffset": 169}, {"referenceID": 10, "context": "Let us begin by looking at the classic notion of external regret used in the literature (Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012).", "startOffset": 88, "endOffset": 169}, {"referenceID": 2, "context": "Let us begin by looking at the classic notion of external regret used in the literature (Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012). Given that the experts are learning entities, naturally the losses incurred at any time step are dependent on the history of the forecaster\u2019s actions as that history defines the current learning state of the individual experts. Given this subtle issue of history dependent losses, the usual notion of external regret does not provide any meaningful guarantees in terms of competing against the \u201cbest expert in hindsight\u201d (see below for a formal definition); the bounds given by the external regret are only w.r.t. the post hoc sequence of actions performed and losses observed during the execution of the algorithm (cf. Maillard and Munos (2011); Arora et al.", "startOffset": 89, "endOffset": 817}, {"referenceID": 2, "context": "Maillard and Munos (2011); Arora et al. (2012); McMahan and Streeter (2009) for more discussion on this).", "startOffset": 27, "endOffset": 47}, {"referenceID": 2, "context": "Maillard and Munos (2011); Arora et al. (2012); McMahan and Streeter (2009) for more discussion on this).", "startOffset": 27, "endOffset": 76}, {"referenceID": 14, "context": "Somewhat surprisingly, we prove this hardness result when playing against an oblivious (non-adaptive) adversary and when restricting the experts to be implementing the well-studied HEDGE algorithm (Freund and Schapire, 1995).", "startOffset": 197, "endOffset": 224}, {"referenceID": 14, "context": "The expert EXP1 plays the HEDGE algorithm (Freund and Schapire, 1995), i.", "startOffset": 42, "endOffset": 69}, {"referenceID": 3, "context": "The selection strategy of the algorithm LEARNEXP is similar to the EXP family of algorithms, and in particular is equivalent to the EXP3 algorithm by Auer et al. (2002). The core idea of guiding the feedbacks observed by experts is presented in Lines 10,11, and 12.", "startOffset": 150, "endOffset": 169}, {"referenceID": 23, "context": "Proposition 2 A rich class of no-regret online learning algorithms based on gradient-descent style updates have smooth learning dynamics including the Online Mirror Descent family of algorithms with exact or estimated gradients (Shalev-Shwartz, 2011) and Online Convex Programming via greedy projections (Zinkevich, 2003).", "startOffset": 228, "endOffset": 250}, {"referenceID": 28, "context": "Proposition 2 A rich class of no-regret online learning algorithms based on gradient-descent style updates have smooth learning dynamics including the Online Mirror Descent family of algorithms with exact or estimated gradients (Shalev-Shwartz, 2011) and Online Convex Programming via greedy projections (Zinkevich, 2003).", "startOffset": 304, "endOffset": 321}, {"referenceID": 3, "context": "For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.", "startOffset": 46, "endOffset": 119}, {"referenceID": 22, "context": "For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.", "startOffset": 46, "endOffset": 119}, {"referenceID": 8, "context": "For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.", "startOffset": 46, "endOffset": 119}, {"referenceID": 22, "context": "Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al.", "startOffset": 117, "endOffset": 145}, {"referenceID": 8, "context": "Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al.", "startOffset": 188, "endOffset": 214}, {"referenceID": 4, "context": ", 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al.", "startOffset": 37, "endOffset": 67}, {"referenceID": 5, "context": ", 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015).", "startOffset": 137, "endOffset": 176}, {"referenceID": 16, "context": ", 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015).", "startOffset": 137, "endOffset": 176}, {"referenceID": 25, "context": "(Slivkins and Upfal, 2008)), thereby Slivkins and Upfal (2008) considered a type of restless bandits where the change in state is governed by a more gradual process with stochastic rewards depending upon the state.", "startOffset": 0, "endOffset": 26}, {"referenceID": 9, "context": "The seminal work of Littlestone and Warmuth (1994); Cesa-Bianchi et al.", "startOffset": 20, "endOffset": 51}, {"referenceID": 5, "context": "The seminal work of Littlestone and Warmuth (1994); Cesa-Bianchi et al. (1997) initiated the study of using expert advice for prediction problems, and Freund and Schapire (1995) introduced the algorithm HEDGE for the general problem of dynamically allocating resources among a set of options using expert advice.", "startOffset": 52, "endOffset": 79}, {"referenceID": 5, "context": "The seminal work of Littlestone and Warmuth (1994); Cesa-Bianchi et al. (1997) initiated the study of using expert advice for prediction problems, and Freund and Schapire (1995) introduced the algorithm HEDGE for the general problem of dynamically allocating resources among a set of options using expert advice.", "startOffset": 52, "endOffset": 178}, {"referenceID": 3, "context": "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback.", "startOffset": 16, "endOffset": 35}, {"referenceID": 3, "context": "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees\u2014we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006).", "startOffset": 16, "endOffset": 596}, {"referenceID": 3, "context": "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees\u2014we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006). Furthermore, this framework is very generic and versatile to capture many complex real-world scenarios.", "startOffset": 16, "endOffset": 678}, {"referenceID": 3, "context": "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees\u2014we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006). Furthermore, this framework is very generic and versatile to capture many complex real-world scenarios. For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.g., information about user preferences) that may not be shared among experts and may not be available to the algorithm. Furthermore, no statistical assumptions are needed on the process generating context or losses/rewards over time. Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015). 5.2. Related Work Next, we review research work that is relevant to the problem studied in this paper. Markovian, rested, and restless bandits. The seminal work of Gittins (1979) considered Markovian bandits where each action/arm is associated with its own stochastic MDP and introduces the Gittins index to find an optimal sequential policy.", "startOffset": 16, "endOffset": 1759}, {"referenceID": 3, "context": "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees\u2014we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006). Furthermore, this framework is very generic and versatile to capture many complex real-world scenarios. For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.g., information about user preferences) that may not be shared among experts and may not be available to the algorithm. Furthermore, no statistical assumptions are needed on the process generating context or losses/rewards over time. Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015). 5.2. Related Work Next, we review research work that is relevant to the problem studied in this paper. Markovian, rested, and restless bandits. The seminal work of Gittins (1979) considered Markovian bandits where each action/arm is associated with its own stochastic MDP and introduces the Gittins index to find an optimal sequential policy. Note that the arm changes its state only when it is pulled, hence also termed as rested bandits. Whittle (1988) considered an extension termed restless bandits where all the arms change their reward distributions at every time step according to their associated stochastic MDP.", "startOffset": 16, "endOffset": 2035}, {"referenceID": 3, "context": "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees\u2014we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006). Furthermore, this framework is very generic and versatile to capture many complex real-world scenarios. For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.g., information about user preferences) that may not be shared among experts and may not be available to the algorithm. Furthermore, no statistical assumptions are needed on the process generating context or losses/rewards over time. Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015). 5.2. Related Work Next, we review research work that is relevant to the problem studied in this paper. Markovian, rested, and restless bandits. The seminal work of Gittins (1979) considered Markovian bandits where each action/arm is associated with its own stochastic MDP and introduces the Gittins index to find an optimal sequential policy. Note that the arm changes its state only when it is pulled, hence also termed as rested bandits. Whittle (1988) considered an extension termed restless bandits where all the arms change their reward distributions at every time step according to their associated stochastic MDP. Restless bandits are notoriously difficult to tackle (cf. (Slivkins and Upfal, 2008)), thereby Slivkins and Upfal (2008) considered a type of restless bandits where the change in state is governed by a more gradual process with stochastic rewards depending upon the state.", "startOffset": 16, "endOffset": 2322}, {"referenceID": 3, "context": "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees\u2014we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006). Furthermore, this framework is very generic and versatile to capture many complex real-world scenarios. For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.g., information about user preferences) that may not be shared among experts and may not be available to the algorithm. Furthermore, no statistical assumptions are needed on the process generating context or losses/rewards over time. Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015). 5.2. Related Work Next, we review research work that is relevant to the problem studied in this paper. Markovian, rested, and restless bandits. The seminal work of Gittins (1979) considered Markovian bandits where each action/arm is associated with its own stochastic MDP and introduces the Gittins index to find an optimal sequential policy. Note that the arm changes its state only when it is pulled, hence also termed as rested bandits. Whittle (1988) considered an extension termed restless bandits where all the arms change their reward distributions at every time step according to their associated stochastic MDP. Restless bandits are notoriously difficult to tackle (cf. (Slivkins and Upfal, 2008)), thereby Slivkins and Upfal (2008) considered a type of restless bandits where the change in state is governed by a more gradual process with stochastic rewards depending upon the state. Besbes et al. (2014) considered another type of restless bandits with stochastic reward functions, however these distributions change adversarially with a budget on the allowed variation.", "startOffset": 16, "endOffset": 2495}, {"referenceID": 21, "context": "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012).", "startOffset": 129, "endOffset": 175}, {"referenceID": 2, "context": "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012).", "startOffset": 129, "endOffset": 175}, {"referenceID": 19, "context": "Another perspective on tackling some of the applications we mentioned above is the contextual bandit framework (Li et al., 2010; Langford and Zhang, 2007; Agarwal et al., 2014).", "startOffset": 111, "endOffset": 176}, {"referenceID": 18, "context": "Another perspective on tackling some of the applications we mentioned above is the contextual bandit framework (Li et al., 2010; Langford and Zhang, 2007; Agarwal et al., 2014).", "startOffset": 111, "endOffset": 176}, {"referenceID": 0, "context": "Another perspective on tackling some of the applications we mentioned above is the contextual bandit framework (Li et al., 2010; Langford and Zhang, 2007; Agarwal et al., 2014).", "startOffset": 111, "endOffset": 176}, {"referenceID": 9, "context": "An orthogonal line of research studies the interaction of agents in multiplayer games where each agent uses a no-regret learning algorithm (Blum and Monsour, 2007; Syrgkanis et al., 2015).", "startOffset": 139, "endOffset": 187}, {"referenceID": 26, "context": "An orthogonal line of research studies the interaction of agents in multiplayer games where each agent uses a no-regret learning algorithm (Blum and Monsour, 2007; Syrgkanis et al., 2015).", "startOffset": 139, "endOffset": 187}, {"referenceID": 0, "context": "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012). Arora et al. (2012) studied online learning with bandit feedback against a nonoblivious adversary with bounded memory and introduced the notion of policy regret instead of the usual notion of external regret.", "startOffset": 156, "endOffset": 197}, {"referenceID": 0, "context": "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012). Arora et al. (2012) studied online learning with bandit feedback against a nonoblivious adversary with bounded memory and introduced the notion of policy regret instead of the usual notion of external regret. Maillard and Munos (2011) studied competing against adaptive adversary when the adversary\u2019s reward generation policy is restricted to a pre-specified set of known models.", "startOffset": 156, "endOffset": 412}, {"referenceID": 0, "context": "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012). Arora et al. (2012) studied online learning with bandit feedback against a nonoblivious adversary with bounded memory and introduced the notion of policy regret instead of the usual notion of external regret. Maillard and Munos (2011) studied competing against adaptive adversary when the adversary\u2019s reward generation policy is restricted to a pre-specified set of known models. However, none of the frameworks of non-oblivious/adaptive adversary listed above model learning dynamics in our setting: It would require an adversary with unbounded memory to apply the results of Arora et al. (2012), and an adversary with unbounded number of models to apply the techniques of Maillard and Munos (2011).", "startOffset": 156, "endOffset": 774}, {"referenceID": 0, "context": "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012). Arora et al. (2012) studied online learning with bandit feedback against a nonoblivious adversary with bounded memory and introduced the notion of policy regret instead of the usual notion of external regret. Maillard and Munos (2011) studied competing against adaptive adversary when the adversary\u2019s reward generation policy is restricted to a pre-specified set of known models. However, none of the frameworks of non-oblivious/adaptive adversary listed above model learning dynamics in our setting: It would require an adversary with unbounded memory to apply the results of Arora et al. (2012), and an adversary with unbounded number of models to apply the techniques of Maillard and Munos (2011). Contextual bandits.", "startOffset": 156, "endOffset": 877}, {"referenceID": 0, "context": ", 2010; Langford and Zhang, 2007; Agarwal et al., 2014). We refer the reader to the paper by McMahan and Streeter (2009) for more discussion on the connection between the framework of contextual bandits and learning using expert advice with bandit feedback.", "startOffset": 34, "endOffset": 121}, {"referenceID": 14, "context": "The expert EXP1 plays the HEDGE algorithm (Freund and Schapire, 1995), i.", "startOffset": 42, "endOffset": 69}, {"referenceID": 23, "context": "1 in Shalev-Shwartz (2011), w\u0303\u03c4 = w\u03c4 , and the fact that R(\u00b7)/\u03b1 is a 1/(\u03b7\u03b1)-strongly convex function, we obtain", "startOffset": 5, "endOffset": 27}, {"referenceID": 3, "context": "1 from Auer et al. (2002), we can state the following bounds on the external regret of our algorithm LEARNEXP against any expert EXPk where k \u2208 [N ].", "startOffset": 7, "endOffset": 26}], "year": 2017, "abstractText": "In this paper, we study a variant of the framework of online learning using expert advice with limited/bandit feedback. We consider each expert as a learning entity, seeking to more accurately reflecting certain real-world applications. In our setting, the feedback at any time t is limited in a sense that it is only available to the expert i that has been selected by the central algorithm (forecaster), i.e., only the expert i receives feedback from the environment and gets to learn at time t. We consider a generic black-box approach whereby the forecaster does not control or know the learning dynamics of the experts apart from knowing the following no-regret learning property: the average regret of any expert j vanishes at a rate of at leastO(t j ) with tj learning steps where \u03b2 \u2208 [0, 1] is a parameter. In the spirit of competing against the best action in hindsight in multi-armed bandits problem, our goal here is to be competitive w.r.t. the cumulative losses the algorithm could receive by following the policy of always selecting one expert. We prove the following hardness result: without any coordination between the forecaster and the experts, it is impossible to design a forecaster achieving no-regret guarantees. In order to circumvent this hardness result, we consider a practical assumption allowing the forecaster to \u201cguide\u201d the learning process of the experts by filtering/blocking some of the feedbacks observed by them from the environment, i.e., not allowing the selected expert i to learn at time t for some time steps. Then, we design a novel no-regret learning algorithm LEARNEXP for this problem setting by carefully guiding the feedbacks observed by experts. We prove that LEARNEXP achieves the worst-case expected cumulative regret ofO(T 1 2\u2212\u03b2 ) after T time steps and matches the regret bound of \u0398(T 1 2 ) for the special case of multi-armed bandits.", "creator": "LaTeX with hyperref package"}}}