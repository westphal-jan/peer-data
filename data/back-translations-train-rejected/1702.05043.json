{"id": "1702.05043", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2017", "title": "Unbiased Online Recurrent Optimization", "abstract": "The novel Unbiased Online Recurrent Optimization (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is a modification of NoBackTrack that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models. Computationally, UORO is as costly as Truncated Backpropagation Through Time (TBPTT). Contrary to TBPTT, UORO is guaranteed to provide unbiased gradient estimates, and does not favor short-term dependencies. The downside is added noise, requiring smaller learning rates.", "histories": [["v1", "Thu, 16 Feb 2017 16:38:08 GMT  (154kb,D)", "http://arxiv.org/abs/1702.05043v1", "11 pages, 5 figures"], ["v2", "Mon, 27 Mar 2017 15:29:33 GMT  (157kb,D)", "http://arxiv.org/abs/1702.05043v2", "11 pages, 5 figures"], ["v3", "Tue, 23 May 2017 11:42:03 GMT  (251kb,D)", "http://arxiv.org/abs/1702.05043v3", "11 pages, 5 figures"]], "COMMENTS": "11 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["corentin tallec", "yann ollivier"], "accepted": false, "id": "1702.05043"}, "pdf": {"name": "1702.05043.pdf", "metadata": {"source": "CRF", "title": "Unbiased Online Recurrent Optimization", "authors": ["Corentin Tallec", "Yann Ollivier"], "emails": [], "sections": [{"heading": null, "text": "For synthetic tasks, UORO overcomes several deficiencies in TBPTT. For example, if a parameter has a positive short-term but negative long-term impact, TBPTT may require abbreviations that are much larger than the intrinsic time range of interactions, while UORO performs well thanks to the impartiality of its gradients."}, {"heading": "Introduction", "text": "In fact, we will be able to go in search of a solution that will enable us, that will enable us to find a solution that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position. \""}, {"heading": "1 Notation and tangent forward propagation", "text": "\"Consider a non-recursive computational graph (e.g. a forward-looking neural network) that calculates F: Rdata \u00b7 Rparams \u2192 Routput v 7 \u2192 F (v) typically v = (x, \u03b8) with x: Rdata and \u03b8: Rparams. Backpropagation is the correct multiplication of a line vector from the output space, \u03b4o: Routput, with the Jacobian of F, \u2202 F / \u2202 v [LBOM96]; we call it F. backprop (v, \u03b4o): = \u03b4o (\u2202 F / \u0445 v). With respect to x: Backpropagation generates a pair (\u03b4o: F / \u2202 x), \u03b4o: F / ITV v:). It can be efficiently compiled using the usual algorithms. In what follows, we also use tangent forward propagation: the forward-directed propagation of an infinitesimal value of v, defined as a biforcal vector and indictor."}, {"heading": "2 Unbiased Online Recurrent Optimization", "text": "Consider a recursive model or dynamic system with a smooth transition period, which can then be replaced by an estimate. (1) Most current recurrent architectures fall within this framework. (2) The LSTMs are slightly reformulated in terms of F, ct, ht. (3) The UORO algorithms calculate an unbiased estimate of the course of loss in terms of a streaming mode, provided that it is possible to advance one step of the dynamic system, i.e. This estimate can be achieved by estimating the course of loss in terms of a streaming mode, provided it is possible to propagate one step forward. (3) The UORO algorithms calculate an unbiased estimate of the course of loss in terms of a streaming mode, provided that it is possible to advance one step of the dynamic system."}, {"heading": "3 Multi-step UORO", "text": "UORO provides an unbiased estimate of the gradient. However, this estimate comes at a cost when injecting noise into the gradient, which requires lower learning rates. To reduce noise on short-term gradients, UORO can be used on top of the shortened BPTT to correct the gradients of TBPTT and make it unbiased. Formally, this only requires the application of algorithm 1 to a new transition function F T, which is just T consecutive steps of the original model F. Then, as with TBPTT, the backpropagation process in algorithm 1 becomes a backpropagation over the final T steps. The loss of a step of F T is the sum of the losses of the last T steps of F, namely 't + Tt + 1: = t + T-k = t + 1'k. Algorithm 1: A step of UORO (from time t to t + 1)"}, {"heading": "Data:", "text": "- xt + 1, o-t + 1, st and \u03b8: input, target, recursive state and parameter - s-t column vector of size state, \u03b8-t row vector of size parameters, so that E s-t-\u03b8-t = \u2202 st / \u2202 \u03b8 - SGDOpt and \u03b7t + 1: stochastic optimizer and its learning rate"}, {"heading": "Result:", "text": "- \"t + 1, st + 1\" and \"p\": Loss, recurrent state and updated parameters - \"t + 1\" and \"t + 1\" - \"t + 1\" - \"t + 1\" (ot + 1, o + 1) - \"t + 1\" (ot + 1, o + 1) / * Calculation of the next state and loss * / (ot + 1, st + 1) = \"t + 1\" (ot + 1, o + 1) / * Calculation of the deviation test * / * \"t\" - \"t\" - \"t\" - \"t\" - \"t\" - \"t\" - \"-\" t \"-\" - \"t\" - \"-\" t."}, {"heading": "4 Experiments", "text": "We tested UORO on synthetic cases with temporary dependencies that have TBPTT = = learning difficulties due to myopia or improper time scale balancing. UORO overcomes these deficiencies and competes with or largely exceeds the TBPTT.Influence characters. The first test case illustrates the learning of a scalar parameter that positively influences the time dependence on a factor of 10 or so.Consider the linear dynamicsst + 1 = branch + (branch +.,.,.,.) in the wrong direction exploding parameter, unless the time dependence exceeds the range of a factor of 10 or so.Consider the linear dynamicsst + 1 = branch + (.,.,.,.,.) with a square matrix of size n."}, {"heading": "Conclusion", "text": "We introduced UORO, an algorithm to train recurrent neural networks in a streaming, memory-free way. UORO is easy to implement, requires as little computing time as TBPTT, and manages the myopia of TBPTT at the expense of noise injection. UORO has been shown to provide an unbiased estimate of loss, making it theoretically reasonable for low learning rates. Furthermore, experimental results suggest that the added noise does not unreasonably affect the learning process and that UORO is able to solve some problems where the shortened BPTT fails."}, {"heading": "A Unbiasedness of gradient estimates: proof", "text": "Sentence 1: Consider the sequences determined in Algorithm 1: \"t,\" \"t\" and \"g,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" \"t,\" t, \"t,\" \"t,\" t, \"\" t, \"\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t, \"t,\" t."}], "references": [{"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Technical Report UCB/EECS2010-24, EECS Department,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the \u201cecho state network", "author": ["Herbert Jaeger"], "venue": null, "citeRegEx": "Jaeger.,? \\Q2002\\E", "shortCiteRegEx": "Jaeger.", "year": 2002}, {"title": "Decoupled neural interfaces using synthetic gradients", "author": ["Max Jaderberg", "Wojciech Marian Czarnecki", "Simon Osindero", "Oriol Vinyals", "Alex Graves", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Jaderberg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2016}, {"title": "Optimization and Applications of Echo State Networks with Leaky-Integrator Neurons", "author": ["Herbert Jaeger", "Mantas Luko\u0161evi\u010dius", "Dan Popovici", "Udo Siewert"], "venue": "Neural Networks,", "citeRegEx": "Jaeger et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Jaeger et al\\.", "year": 2007}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Efficient backprop", "author": ["Yann LeCun", "L\u00e9on Bottou", "Genevieve B. Orr", "Klaus-Robert M\u00fcller"], "venue": "Neural Networks: Tricks of the Trade,", "citeRegEx": "LeCun et al\\.,? \\Q1996\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1996}, {"title": "A Monte Carlo EM approach for partially observable diffusion processes: Theory and applications to neural networks", "author": ["Javier R. Movellan", "Paul Mineiro", "R.J. Williams"], "venue": "Neural Comput.,", "citeRegEx": "Movellan et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Movellan et al\\.", "year": 2002}, {"title": "Real-time computing without stable states: A new framework for neural computation based on perturbations", "author": ["Wolfgang Maass", "Thomas Natschl\u00e4ger", "Henry Markram"], "venue": "Neural Comput.,", "citeRegEx": "Maass et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Maass et al\\.", "year": 2002}, {"title": "Training recurrent networks online without backtracking", "author": ["Yann Ollivier", "Corentin Tallec", "Guillaume Charpiat"], "venue": "CoRR, abs/1507.07680,", "citeRegEx": "Ollivier et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ollivier et al\\.", "year": 2015}, {"title": "Backpropagation-decorrelation: online recurrent learning with O(N) complexity", "author": ["Jochen J. Steil"], "venue": "In Neural Networks,", "citeRegEx": "Steil.,? \\Q2004\\E", "shortCiteRegEx": "Steil.", "year": 2004}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["Ronald J. Williams", "David Zipser"], "venue": "Neural Comput.,", "citeRegEx": "Williams and Zipser.,? \\Q1989\\E", "shortCiteRegEx": "Williams and Zipser.", "year": 1989}], "referenceMentions": [], "year": 2017, "abstractText": "The novel Unbiased Online Recurrent Optimization (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is a modification of NoBackTrack [OTC15] that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models. Computationally, UORO is as costly as Truncated Backpropagation Through Time (TBPTT). Contrary to TBPTT, UORO is guaranteed to provide unbiased gradient estimates, and does not favor short-term dependencies. The downside is added noise, requiring smaller learning rates. On synthetic tasks, UORO is found to overcome several deficiencies of TBPTT. For instance, when a parameter has a positive short-term but negative long-term influence, TBPTT may require truncation lengths substantially larger than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.", "creator": "LaTeX with hyperref package"}}}