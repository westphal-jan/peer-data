{"id": "1702.05053", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2017", "title": "Addressing the Data Sparsity Issue in Neural AMR Parsing", "abstract": "Neural attention models have achieved great success in different NLP tasks. How- ever, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we de- scribe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural atten- tion model and our results are also compet- itive against state-of-the-art systems that do not use extra linguistic resources.", "histories": [["v1", "Thu, 16 Feb 2017 17:09:12 GMT  (209kb)", "http://arxiv.org/abs/1702.05053v1", "Accepted by EACL-17"]], "COMMENTS": "Accepted by EACL-17", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xiaochang peng", "chuan wang", "daniel gildea", "nianwen xue"], "accepted": false, "id": "1702.05053"}, "pdf": {"name": "1702.05053.pdf", "metadata": {"source": "CRF", "title": "Addressing the Data Sparsity Issue in Neural AMR Parsing", "authors": ["Xiaochang Peng", "Chuan Wang", "Daniel Gildea"], "emails": ["gildea}@cs.rochester.edu", "xuen}@brandeis.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.05 053v 1 [cs.C L] February 16, 2017Addressing the Data Sparsity Issue in Neural AMR ParsingXiaochang Peng * 1, Chuan Wang * 2, Daniel Gildea1 and Nianwen Xue21University of Rochester {xpeng, gildea} @ cs.rochester.edu 2Brandeis University {cwang24, xuen} @ brandeis.eduAbstractNeural attention models have achieved great success in various NLP tasks, but they have not fulfilled their promise regarding the AMR analysis task due to the problem of data sparseness. In this paper we describe a sequence-to-sequence model for AMR analysis and present various ways to address the problem of data spareness. We show that our methods achieve significant improvements compared to a neural attention model and that our results also compete with state-of-the-art systems that do not use additional linguistic resources."}, {"heading": "1 Introduction", "text": "The meaning of a sentence is interpreted as a rooted, directed graph. Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges that represent the relationships between the concepts that connect them. AMR concepts, however, consist of predicate sensory perceptions, known as entity notes, and in some cases simple expressions of the English words. AMR relationships consist of core semantic roles drawn by the propbank (Palmer et al., 2005) as well as very fine-grained semantic relationships defined specifically for AMR. These characteristics make AMR representation useful in issues such as answering and semantic machine-based translation. The task of AMR graphic translation is to create natural language strings to AMR semantic graphs."}, {"heading": "2 Sequence-to-sequence Parsing Model", "text": "Our model is based on an existing sequence-tosequence parsing model (Vinyals et al., 2015), which is similar to models of neural machine translation."}, {"heading": "2.1 Encoder-Decoder", "text": "Encoder. The encoder learns a context-aware representation for each position of the input sequence by putting the input w1,.., wm into a sequence of hidden layers h1,.., hm. To model the left and right context of each input position, we use a bidirectional LSTM (Bahdanau et al., 2014). First, the word of the input sequence can be considered a recurring function defined as: h fw i = f (xi, h fw i \u2212 1) (1). Then, these embedding serve as input to dead RNs: a forward RNN and a backward RNN \u2212 d sequence. The forward RNN can be considered a recurring function defined as: h fw i = f (h fw i \u2212 1) (1) (1). Here, the recurring function f we use is long-short memory (STM) (LM)."}, {"heading": "2.2 Parse Tree as Target Sequence", "text": "Vinyals et al. (2015) developed a reversible way of converting the parse tree into a sequence, which they call linearization, in the order of depth traversing. Figure 2 shows an example of the linearization result. The target vocabulary consists of 128 symbols. In practice, they found that the use of the attention model is more data-efficient and works well in the parsing task. Furthermore, they reversed the input set and normalized the part-of-speech tag. After decoding, the output sequence tree is recovered from the output sequence of the decoder in a post-processing process. Overall, the sequence draft model can keep up with the performance of the Berkeley parser (Petrov et al., 2006)."}, {"heading": "3 AMR Linearization", "text": "Barzdins and Gosko (2016) present a similar linearization method in which the deepest transversal result of an AMR graph is used as an AMR sequence (see Figure 3). AMR's parentheses structure is difficult to maintain because the prediction of the relation (with left parentheses) and the prediction of an isolated right parentheses are not correlated. As a result, the result of the linearization of the AMR graph usually shows parentheses that do not match. We present a linearization strategy that captures the parentheses structure of the AMR and the connection between relationships and concepts. Figure 3 shows the linearization result of the AMR graph shown in Figure 1. Each relationship connects the head concept with a subgraph structure rooted in the tail concept that shows a branch below the head concept. We use the relationette and the left parentheses to show the beginning of the second parentheses (to show the end of the second parentheses) of the subgraph."}, {"heading": "4 Dealing with the Data Sparsity Issue", "text": "While sequence-to-sequence models can be successfully applied to component analysis, they do not work well when analyzing AMR, as Barzdins and Gosko (2016) have shown. The biggest bottleneck is that the size of the target vocabulary for analyzing AMR is much larger than that of components, tens of thousands compared to 128, and the size of the training data is less than half of what is available for analysis. In this section, we propose a categorization method that significantly reduces the size of the target vocabulary because the alignment from the attention model to the relatively small dataset does not work well. To compensate for the alignment errors caused by the attention model, we propose to supplement the oversight with an alignment produced by an external aligner that can use lexical information to overcome the limit of data size."}, {"heading": "4.1 AMR Categorization", "text": "We define several types of categories and categorize low-frequency words into these categorities.1. DATE: We reduce all units of dates to this category, ignoring details of the specific unit of date.2. NE {ent}: We reduce all named entity subgraphs to this category, where ent is the root label of each subgraph, such as country or person. 3. -VERB-: We assign low-frequency variables (n < 50) to this category. 4. -SURF-: We assign non-predictive low-frequency variables (n < 50) to this category. 5. -CONST-: We assign constants other than numbers, \"-\" questioning, \"\" expressive, \"\" \"compelling\" to this category. 6. -RET-: We assign all verified concepts to this category. 7. -VERBAL-: We also assign the verbal abbreviation list from the AMR list, although this large category is subordinated to the relatively frequent category of 2000."}, {"heading": "4.2 Categorize Source Sequence", "text": "The origin categories also have the same problems. For example, the number 1997 is traced back to \"DATE,\" we cannot simply generalize it: / / amr.isi.edu / lists / verbalization-listv1.06.txt to the training data. Also, some special 6-digit date formats like \"YYYMMDD\" are hard to learn when dealing with the topic. Our basic approach is to generalize or spin them to use the same categories as \"YMDD.\""}, {"heading": "4.3 Recovering AMR graph", "text": "During decoding, our output sequences usually have categories, and we must assign each category to AMR concepts or subgraphs. If we categorize the tokens in each set prior to decoding, we store the mapping from each category to its original token as Table D. Because we use the same set of categories on both the source and destination pages, we heuristically align the target category from left to right with its source counterpart. Given Table D, we know which source-side token it comes from, and use the most common concept or subgraph of the token to replace the category."}, {"heading": "4.4 Supervised Attention Model", "text": "In this section, we propose to learn the attention vector in a controlled manner. There are two main motivations behind this: First, the neural attention model typically uses millions of data points to train the model, which learns a reasonably reasonable attention vector that forces the decoder to focus on the input sequences at each output time step (Bahdanau et al., 2014; Vinyals et al., 2015). However, we have only 16k sentences in the AMR training data and our output vocabulary size is quite large, making it difficult for the model to learn a meaningful alignment between the input sequence and AMR concepts / relationships. Second, as argued by Liu et al. (2016), the output sequence model attempts to calculate the attention vector and predict the current output labeling simultaneously, making it impossible for the learning progression to merge the output text from the entire output information."}, {"heading": "5 Experiments", "text": "We are evaluating our system based on the published data set (LDC2015E86) for SemEval 2016 Task 8 on the importance of representation analysis (May 2016), which includes 16,833 training sessions, 1,368 development and 1,371 test sets covering mainly areas such as newsrooms, discussion forums, etc. All results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013)."}, {"heading": "5.1 Experiment Settings", "text": "Then we extract the designated units with the Illinois Named Entity Tagger (Ratinov and Roth, 2009). To train all neural AMR parsing systems, we use 256 for both the hidden layer size and the word embedding size. Stochastic gradient descent is used to optimize cross-entropy loss function and we set the drop-out rate to 0.5. We train our model for 150 epochs with an initial learning rate of 0.5 and a decay factor of 0.95 if the model does not improve in the last 3 epochs."}, {"heading": "5.2 Baseline Model", "text": "Our basic model is a simple sequence sequence model used in the constituent parsing task (Vinyals et al., 2015). While they use a 3-layer deep LSTM, we use only a single-layer LSTM for both encoders and decoders, since our data is relatively small and empirical comparisons show that stacking additional layers does not help in our case. AMR linearization follows Section 3 without categorization. As we do not restrict the input / output vocabulary in this case, our vocabulary is quite large: 10,886 for output vocabulary and 2,2892 for input vocabulary. We set it to 10,000 and 20,000, respectively, and replace the output vocabulary with UNK."}, {"heading": "5.3 Impact of Re-Categorization", "text": "We first examine the influence of the use of categorization on the input and output sequence. Table 1 shows the smatch score on the development set.We see from the table that re-categorization improved the F-score by 13 points on the development set. As mentioned in Section 4.1, by setting the low frequency threshold n to 50 and reclassifying it into a reduced set of types, we now reduce the input / output vocabulary size to (2,000, 6,000).This significantly reduces the label spareness and allows the neural attention model to learn a better representation on this small scale. Another advantage of this method is that although AMR tries to abstract surface shapes and maintain the semantic meaning structure of the set, a large part of the concepts come from the surface shape and have exactly the same string shape in both the input and AMR diagrams."}, {"heading": "5.4 Impact of Supervised Alignment", "text": "There are two existing AMR aligners: one is a rules-based aligner that comes with JAMR (Flanigan et al., 2014), which defines the traditional alignment patterns that greedily match AMR graph fragments and input tokens; another is an unattended aligner (Pourdamghani et al., 2014) that applies the traditional alignment method in machine translation. Although it is evaluated for different sets of manual alignment, both aligners achieved a value of 90% F1. Here, we choose the second aligner because it covers broader areas. Different alignment configurations To balance the order of learning and alignment agreement, we empirically adjust the hyperparameter conditions and set them to 0.3. For the external alignment that we use as a reference, we convert it to a vector with the same probability as in Section 4.4."}, {"heading": "5.5 Results", "text": "In this section, we report on our end result on the test set of SemEval 2016 Task 8 and compare our model with other parsers. We train our model using re-categorization and monitored attention with hyperparameters matched to the test set. Then, we apply our trained model to the test set. First, we compare our model with the existing sequence-based AMR analysis model of Barzdins and Gosko (2016). As shown in Table 4, the word-level model in Barzdins and Gosko (2016) is essentially our base model. The second model they use is a character-based sequence sequence model. Our model can also be considered a word-level model; however, due to the carefully designed categorization and monitored attention, our system outperforms both results many times. Table 5 gives the comparison of our system to some teams participating in SemEval16 Task."}, {"heading": "6 Discussion", "text": "In this paper, we have proposed several methods to make the sequence-to-sequence model competitive with traditional AMR parsing systems. Although we have not surpassed the state-of-the-art system with conventional methods, our results show the effectiveness of our approaches to reducing the savings problem when the sequence-to-sequence model is trained on a relatively small dataset. Our work could be aligned with efforts to address resource-poor data problems in building the end-to-end neural network model. In neural machine translation, the attention model is traditionally trained on millions of sentence pairs, while faced with language pairs with limited resources, the performance of the neural MT system tends to be downgraded (Zoph et al, 2016)."}, {"heading": "7 Conclusion", "text": "Neural attention models have achieved great success in various NLP tasks, but they have not been as successful in parsing AMR due to the problem of data sparseness. In this paper, we have described a sequence-to-sequence model for parsing AMR and outlined various ways to address the problem of data sparseness. We show that our methods have resulted in a significant improvement over a neural attention model and that our model is also competitive compared to models that do not use additional linguistic resources."}], "references": [{"title": "Broad-coverage CCG semantic parsing with AMR", "author": ["Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1699\u20131710, Lisbon, Portugal, September. Associa-", "citeRegEx": "Artzi et al\\.,? 2015", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyunghyunCho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Abstract meaning representation", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider"], "venue": null, "citeRegEx": "Banarescu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Riga at semeval-2016 task 8: Impact of smatch extensions and character-level neural translation on AMR parsing accuracy", "author": ["Guntis Barzdins", "Didzis Gosko."], "venue": "arXiv preprint arXiv:1604.01278.", "citeRegEx": "Barzdins and Gosko.,? 2016", "shortCiteRegEx": "Barzdins and Gosko.", "year": 2016}, {"title": "Smatch: an evaluation metric for semantic feature structures", "author": ["Shu Cai", "Kevin Knight."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 748\u2013752, Sofia, Bulgaria, August.", "citeRegEx": "Cai and Knight.,? 2013", "shortCiteRegEx": "Cai and Knight.", "year": 2013}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "A discriminative graph-based parser for the abstract meaning representation", "author": ["Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational", "citeRegEx": "Flanigan et al\\.,? 2014", "shortCiteRegEx": "Flanigan et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Neural Machine Translation with Supervised Attention", "author": ["L. Liu", "M. Utiyama", "A. Finch", "E. Sumita."], "venue": "ArXiv e-prints, September.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Semeval-2016 task 8: Meaning representation parsing", "author": ["Jonathan May."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 1063\u20131073, San Diego, California, June. Association for Computational", "citeRegEx": "May.,? 2016", "shortCiteRegEx": "May.", "year": 2016}, {"title": "The proposition bank: An annotated corpus of semantic roles", "author": ["Martha Palmer", "Daniel Gildea", "Paul Kingsbury."], "venue": "Computational Linguistics, 31(1):71\u2013106.", "citeRegEx": "Palmer et al\\.,? 2005", "shortCiteRegEx": "Palmer et al\\.", "year": 2005}, {"title": "UofR at semeval-2016 task 8: Learning synchronous hyperedge replacement grammar for AMR parsing", "author": ["Xiaochang Peng", "Daniel Gildea."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 1185\u2013", "citeRegEx": "Peng and Gildea.,? 2016", "shortCiteRegEx": "Peng and Gildea.", "year": 2016}, {"title": "A synchronous hyperedge replacement grammar based approach for AMR parsing", "author": ["Xiaochang Peng", "Linfeng Song", "Daniel Gildea."], "venue": "Proceedings of the Nineteenth Conference on Computational", "citeRegEx": "Peng et al\\.,? 2015", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Associa-", "citeRegEx": "Petrov et al\\.,? 2006", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Aligning English strings with abstract meaning representation graphs", "author": ["Nima Pourdamghani", "Yang Gao", "Ulf Hermjakob", "Kevin Knight."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages", "citeRegEx": "Pourdamghani et al\\.,? 2014", "shortCiteRegEx": "Pourdamghani et al\\.", "year": 2014}, {"title": "Parsing English into abstract meaning representation using syntaxbased machine translation", "author": ["Michael Pust", "Ulf Hermjakob", "Kevin Knight", "Daniel Marcu", "Jonathan May."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natu-", "citeRegEx": "Pust et al\\.,? 2015", "shortCiteRegEx": "Pust et al\\.", "year": 2015}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Lev Ratinov", "Dan Roth."], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009), pages 147\u2013155, Boulder, Colorado,", "citeRegEx": "Ratinov and Roth.,? 2009", "shortCiteRegEx": "Ratinov and Roth.", "year": 2009}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems, pages 2773\u20132781.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Boosting transition-based AMR parsing with refined actions and auxiliary analyzers", "author": ["Chuan Wang", "Nianwen Xue", "Sameer Pradhan."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In-", "citeRegEx": "Wang et al\\.,? 2015a", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "A transition-based algorithm for AMR parsing", "author": ["Chuan Wang", "Nianwen Xue", "Sameer Pradhan."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Wang et al\\.,? 2015b", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "CAMR at semeval2016 task 8: An extended transition-based AMR parser", "author": ["Chuan Wang", "Sameer Pradhan", "Xiaoman Pan", "Heng Ji", "Nianwen Xue."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016),", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Transfer Learning for Low-Resource Neural Machine Translation", "author": ["B. Zoph", "D. Yuret", "J. May", "K. Knight."], "venue": "ArXiv e-prints, April.", "citeRegEx": "Zoph et al\\.,? 2016", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph.", "startOffset": 38, "endOffset": 62}, {"referenceID": 10, "context": "AMR relations consist of core semantic roles drawn from the Propbank (Palmer et al., 2005) as well as very fine-grained semantic relations defined specifically for AMR.", "startOffset": 69, "endOffset": 90}, {"referenceID": 3, "context": "Barzdins and Gosko (2016) applied a similar approach where AMR graphs are linearized using depth-first search and both concepts and relations are treated as tokens (see Figure 3).", "startOffset": 0, "endOffset": 26}, {"referenceID": 18, "context": "Our model is based on an existing sequence-tosequence parsing model (Vinyals et al., 2015), which is similar to models used in neural machine translation.", "startOffset": 68, "endOffset": 90}, {"referenceID": 1, "context": "To model the left and right contexts of each input position, we use a bidirectional LSTM (Bahdanau et al., 2014).", "startOffset": 89, "endOffset": 112}, {"referenceID": 7, "context": "Here the recurrent function f we use is LongShort-Term-Memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 69, "endOffset": 103}, {"referenceID": 13, "context": "Overall, the sequence-tosequence model is able to match the performance of the Berkeley Parser (Petrov et al., 2006).", "startOffset": 95, "endOffset": 116}, {"referenceID": 3, "context": "-TOP-( describe-01 ARG0( person name( name op1( \u201cRyan\u201d )op1 )name )ARG0 ARG1( person -RET- )ARG1 ARG2( genius )ARG2 )-TOPBarzdins and Gosko (2016)", "startOffset": 121, "endOffset": 147}, {"referenceID": 3, "context": "While sequence-to-sequence models can be successfully applied to constituent parsing, they do not work well on the AMR parsing task as shown by Barzdins and Gosko (2016). The main bottleneck is that the size of target vocabulary for AMR parsing is much larger than constituent parsing, tens of thousands in comparison with 128, and the", "startOffset": 144, "endOffset": 170}, {"referenceID": 1, "context": "constrains the decoder to put a focus on the input sequences (Bahdanau et al., 2014; Vinyals et al., 2015).", "startOffset": 61, "endOffset": 106}, {"referenceID": 18, "context": "constrains the decoder to put a focus on the input sequences (Bahdanau et al., 2014; Vinyals et al., 2015).", "startOffset": 61, "endOffset": 106}, {"referenceID": 8, "context": "ond, as argued by Liu et al. (2016), the sequenceto-sequence model tries to calculate the attention vector and predict the current output label simulta-", "startOffset": 18, "endOffset": 36}, {"referenceID": 8, "context": "Similar to the method used by Liu et al. (2016), we add an additional loss to the original objective function to model the disagreement between the reference alignment and the soft alignment produced by the attention mechanism.", "startOffset": 30, "endOffset": 48}, {"referenceID": 9, "context": "(LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016).", "startOffset": 71, "endOffset": 82}, {"referenceID": 4, "context": "2) (Cai and Knight, 2013).", "startOffset": 3, "endOffset": 25}, {"referenceID": 16, "context": "the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009).", "startOffset": 58, "endOffset": 82}, {"referenceID": 18, "context": "Our baseline model is a plain sequence-tosequence model which has been used in the constituent parsing task (Vinyals et al., 2015).", "startOffset": 108, "endOffset": 130}, {"referenceID": 6, "context": "There are two existing AMR aligners: one is a rule-based aligner coming with JAMR (Flanigan et al., 2014), which defines regular expression patterns to greedily match between AMR graph fragment and input token spans; another one is an unsupervised aligner (Pourdamghani et al.", "startOffset": 82, "endOffset": 105}, {"referenceID": 14, "context": ", 2014), which defines regular expression patterns to greedily match between AMR graph fragment and input token spans; another one is an unsupervised aligner (Pourdamghani et al., 2014) which adopts the traditional word alignment method in machine translation.", "startOffset": 158, "endOffset": 185}, {"referenceID": 3, "context": "Firstly, we compare our model to the existing sequence-to-sequence AMR parsing model of Barzdins and Gosko (2016). As shown in table 4, the word-level model in Barzdins and Gosko (2016) is basically our baseline model.", "startOffset": 88, "endOffset": 114}, {"referenceID": 3, "context": "Firstly, we compare our model to the existing sequence-to-sequence AMR parsing model of Barzdins and Gosko (2016). As shown in table 4, the word-level model in Barzdins and Gosko (2016) is basically our baseline model.", "startOffset": 88, "endOffset": 186}, {"referenceID": 3, "context": "52 Barzdins and Gosko (2016) - - 0.", "startOffset": 3, "endOffset": 29}, {"referenceID": 3, "context": "52 Barzdins and Gosko (2016) - - 0.37 Barzdins and Gosko (2016) - - 0.", "startOffset": 3, "endOffset": 64}, {"referenceID": 3, "context": "Barzdins and Gosko (2016) is the word-level neural AMR parser, Barzdins and Gosko (2016) is the character-level neural AMR", "startOffset": 0, "endOffset": 26}, {"referenceID": 3, "context": "Barzdins and Gosko (2016) is the word-level neural AMR parser, Barzdins and Gosko (2016) is the character-level neural AMR", "startOffset": 0, "endOffset": 89}, {"referenceID": 20, "context": "tend on the state-of-the-art system CAMR (Wang et al., 2015b; Wang et al., 2015a; Wang et al., 2016), here we just pick typical teams that represent different approaches.", "startOffset": 41, "endOffset": 100}, {"referenceID": 19, "context": "tend on the state-of-the-art system CAMR (Wang et al., 2015b; Wang et al., 2015a; Wang et al., 2016), here we just pick typical teams that represent different approaches.", "startOffset": 41, "endOffset": 100}, {"referenceID": 21, "context": "tend on the state-of-the-art system CAMR (Wang et al., 2015b; Wang et al., 2015a; Wang et al., 2016), here we just pick typical teams that represent different approaches.", "startOffset": 41, "endOffset": 100}, {"referenceID": 12, "context": "Our approach is competitive with the SHRG (Synchronous Hyperedge Replacement Grammar) method of Peng et al. (2015), which does not require a dependency parser and uses SHRG to formalize the string-tograph problem as a chart parsing task.", "startOffset": 96, "endOffset": 115}, {"referenceID": 11, "context": "52 Peng and Gildea (2016) 0.", "startOffset": 3, "endOffset": 26}, {"referenceID": 22, "context": "model is traditionally trained on millions of sentence pairs, while facing low-resource language pairs, the neural MT system performance tends to downgrade (Zoph et al., 2016).", "startOffset": 156, "endOffset": 175}, {"referenceID": 22, "context": "model is traditionally trained on millions of sentence pairs, while facing low-resource language pairs, the neural MT system performance tends to downgrade (Zoph et al., 2016). There has been growing interest in tackling sparsity/low-resource problem in neural MT. Zoph et al. (2016) use a transfer learning method to first pre-train the neu-", "startOffset": 157, "endOffset": 284}, {"referenceID": 5, "context": "Firat et al. (2016) builds a multilingual neural system where the attention mechanism can be shared between different language pairs.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "However, as shown in previous works (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015), using dependency features helps improve the parsing performance significantly because of the linguistic similarity between the dependency tree and AMR structure.", "startOffset": 36, "endOffset": 118}, {"referenceID": 20, "context": "However, as shown in previous works (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015), using dependency features helps improve the parsing performance significantly because of the linguistic similarity between the dependency tree and AMR structure.", "startOffset": 36, "endOffset": 118}, {"referenceID": 0, "context": "However, as shown in previous works (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015), using dependency features helps improve the parsing performance significantly because of the linguistic similarity between the dependency tree and AMR structure.", "startOffset": 36, "endOffset": 118}, {"referenceID": 15, "context": "However, as shown in previous works (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; Pust et al., 2015), using dependency features helps improve the parsing performance significantly because of the linguistic similarity between the dependency tree and AMR structure.", "startOffset": 36, "endOffset": 118}, {"referenceID": 0, "context": ", 2015b; Artzi et al., 2015; Pust et al., 2015), using dependency features helps improve the parsing performance significantly because of the linguistic similarity between the dependency tree and AMR structure. An interesting extension would be to use a linearized dependency tree as the source sequence and apply sequence-to-sequence to generate the AMR graph. Our parser could also benefit from the modeling techniques in Wu et al. (2016).", "startOffset": 9, "endOffset": 441}], "year": 2017, "abstractText": "Neural attention models have achieved great success in different NLP tasks. However, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}