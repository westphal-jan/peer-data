{"id": "1702.05270", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2017", "title": "Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers from Vision", "abstract": "People can refer to quantities in a visual scene by using either exact cardinals (e.g. one, two, three) or natural language quantifiers (e.g. few, most, all). In humans, these two processes underlie fairly different cognitive and neural mechanisms. Inspired by this evidence, the present study proposes two models for learning the objective meaning of cardinals and quantifiers from visual scenes containing multiple objects. We show that a model capitalizing on a 'fuzzy' measure of similarity is effective for learning quantifiers, whereas the learning of exact cardinals is better accomplished when information about number is provided.", "histories": [["v1", "Fri, 17 Feb 2017 09:26:10 GMT  (762kb,D)", "http://arxiv.org/abs/1702.05270v1", "Accepted at EACL2017. 7 pages"]], "COMMENTS": "Accepted at EACL2017. 7 pages", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["sandro pezzelle", "marco marelli", "raffaella bernardi"], "accepted": false, "id": "1702.05270"}, "pdf": {"name": "1702.05270.pdf", "metadata": {"source": "CRF", "title": "Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers from Vision", "authors": ["Sandro Pezzelle", "Marco Marelli", "Raffaella Bernardi"], "emails": ["sandro.pezzelle@unitn.it", "marco.marelli@ugent.be", "raffaella.bernardi@unitn.it"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive themselves if they don't put themselves in a position to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "2 Data", "text": "To test our hypothesis, we need a data set of visual scenes that decisively include multiple objects. In addition, some objects in the scene should be repeated so that we could say, for example, that out of 5 objects, \"three\" / \"most\" are dogs. Although a large number of image sets are currently available (see Lin et al. (2014) among many others), no one fully meets these requirements. Typically, images are an outstanding object, and even if several outstanding objects exist, only a handful of cases contain both targets and distractions (Zhang et al., 2015; Zhang et al., 2016). To work around these problems, we experiment with synthetic visual scenes (hence scenarios) that consist of a maximum of 9 images each representing an object. The choice of using a \"patchwork\" of objects that represent images is motivated by the need to represent a relatively large variability."}, {"heading": "2.1 Building the scenarios", "text": "We use images from ImageNet (Deng et al., 2009). Based on the complete list of 203 concepts and corresponding images extracted by Cassani (2014), we discarded the concepts whose corresponding word in the large corpus had a low / zero frequency (Baroni et al., 2014). To get rid of the problems associated with concept identification, we used a single representation for each of the 188 selected concepts. Technically, we calculated a centric vector by avoiding the 4096-dimensional visual features of the corresponding images extracted from the fc7 of CNN (Simonyan and Zisserman, 2014). We used the VGG-19 model pre-trained on the ImageNet ILSVRC data (Russakovsky et al., 2015), which was implemented in the MatConvNet toolbox (Vedalc and Lenalc)."}, {"heading": "2.2 Datasets", "text": "We created a dataset for Cs and a dataset for Qs, each containing 4512 scenarios.1 Then we divided each of the two into a 3008 dataset for training and validation and a 1504 dataset for testing dataset.1 The two datasets were divided according to their \"combinations,\" i.e. the mixture of targets and deflectors in the scenario. As reported in Table 1, we maintained 4 different combinations for each C / Q in the train and 2 in the test. Note that the numberer refers to the number of targets, the denominator refers to the total number of objects. Thus, the number of deflections is derived from the difference between the two values. To illustrate, in Train-q \"few\" are represented by scenarios 1 / 6, 2 / 5, 2 / 7 and 3 / 8, while in Test-q \"few\" is represented by the deflection sequence of the two values."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Only-vision evaluation", "text": "In a first step, we perform a preliminary evaluation aimed at exploring our visual data. If our intuition about the information encoded by the two measures of similarity is correct (see \u00a7 1), a visual representation of our scenarios is provided on the right side of Figure 4, while Figure 1 is only intended to provide a more intuitive overview of the task at hand. To test our hypothesis, we should note that cosine is more effective than dot product to avoid negative values, while the latter should be better for Cs than cosine. In addition, Qs / Cs should be on an ordered scale. To test our hypothesis, we calculate cosine distances (i.e. 1 \u2212 cosine to avoid negative values) and dot-product similarity for each target scenario pair both in training as well as in testing (e.g. dog vs 2 / 5 dogs). Figure 2 reports on the distribution of Qs in relation to cosine products (on the right and in cosine products)."}, {"heading": "3.2 Cross-modal mapping", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "4 Discussion", "text": "We suggest that the meaning of Cs and Qs can be learned by means of a language-to-vision mapping, and we show that two models that rely on dot product and cosine take better account of Cs and Qs, respectively. In future research, we plan to further investigate this problem by using images from the real scene so as not to limit the visual data. Furthermore, we plan to experiment with a broader set of quantifiers (e.g. some, almost all, etc.) and higher cardinals. In particular, the latter investigation would allow us to verify whether our approach is suitable for the (potentially infinite) set of \"cardinal functions\" outside the subidation range. In this case, we could observe that the models continue to make cognitively plausible errors by selecting objects that come close to the target in the ordered scale. This evidence, we believe, would further motivate our \"a quantified expression, a function,\" which is partially inspired by the numerical evidence in the brain, taking into account so-called a certain number in the so-called human evidence."}, {"heading": "Acknowledgments", "text": "We are very grateful to Germ\u00e1n Kruszewski for his valuable contribution to developing and discussing the intuition behind this work, to Marco Baroni, Aur\u00e9lie Herbelot, Gemma Boleda and Ravi Shekhar for their advice and feedback, and to NVIDIA Corporation for supporting the GPUs used in our research and to iV & L Net (ICT COST Action IC1307) for funding the author's second research visit aimed at working on this project."}], "references": [{"title": "Crosslinguistic relations between quantifiers and numerals in language acquisition: Evidence from Japanese", "author": ["Barner et al.2009] David Barner", "Amanda Libenson", "Pierina Cheung", "Mayu Takasaki"], "venue": "Journal of experimental child psychology,", "citeRegEx": "Barner et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Barner et al\\.", "year": 2009}, {"title": "Don\u2019t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": null, "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Salient object detection: A benchmark", "author": ["Borji et al.2015] Ali Borji", "Ming-Ming Cheng", "Huaizu Jiang", "Jia Li"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Borji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Borji et al\\.", "year": 2015}, {"title": "Distributional semantics for child directed speech: A multimodal approach", "author": ["Giovanni Cassani"], "venue": null, "citeRegEx": "Cassani.,? \\Q2014\\E", "shortCiteRegEx": "Cassani.", "year": 2014}, {"title": "Counting everyday objects in everyday scenes. arXiv preprint arXiv:1604.03505", "author": ["Ramakrishna Vedantam", "Ramprasaath RS", "Dhruv Batra", "Devi Parikh"], "venue": null, "citeRegEx": "Chattopadhyay et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chattopadhyay et al\\.", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng et al.2009] Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "The development of \u2018most\u2019 comprehension and its potential dependence on counting ability in preschoolers", "author": ["Len Taing", "Jeffrey Lidz"], "venue": "Language Learning and Development,", "citeRegEx": "Halberda et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Halberda et al\\.", "year": 2008}, {"title": "Asymmetries in the acquisition of numbers and quantifiers", "author": ["Anna Papafragou", "Lila Gleitman", "Rochel Gelman"], "venue": "Language learning and development,", "citeRegEx": "Hurewitz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hurewitz et al\\.", "year": 2006}, {"title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C Lawrence Zitnick", "Ross Girshick"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Microsoft COCO: Common objects in context", "author": ["Lin et al.2014] Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "The neuronal code for number", "author": ["Andreas Nieder"], "venue": "Nature Reviews Neuroscience", "citeRegEx": "Nieder.,? \\Q2016\\E", "shortCiteRegEx": "Nieder.", "year": 2016}, {"title": "Subitizing reflects visuo-spatial object individuation capacity", "author": ["Antonia Fumarola", "Alessandro Chinello", "David Melcher"], "venue": null, "citeRegEx": "Piazza et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Piazza et al\\.", "year": 2011}, {"title": "The meaning of \u2018most\u2019: Semantics, numerosity and psychology", "author": ["Jeffrey Lidz", "Tim Hunter", "Justin Halberda"], "venue": "Mind & Language,", "citeRegEx": "Pietroski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pietroski et al\\.", "year": 2009}, {"title": "Rapid and accurate processing of multiple objects in briefly presented scenes", "author": ["Railo et al.2016] Henry Railo", "Veli-Matti Karhu", "Jeremy Mast", "Henri Pesonen", "Mika Koivisto"], "venue": "Journal of vision,", "citeRegEx": "Railo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Railo et al\\.", "year": 2016}, {"title": "ImageNet large scale visual recognition challenge", "author": ["Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Six does not just mean a lot: Preschoolers see number words as specific", "author": ["Sarnecka", "Gelman2004] Barbara W Sarnecka", "Susan A Gelman"], "venue": null, "citeRegEx": "Sarnecka et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sarnecka et al\\.", "year": 2004}, {"title": "Learning to count with deep object features", "author": ["Segu\u00ed et al.2015] Santi Segu\u00ed", "Oriol Pujol", "Jordi Vitria"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "Segu\u00ed et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Segu\u00ed et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Look, some green circles!\u2019: Learning to quantify from images", "author": ["Angeliki Lazaridou", "Gemma Boleda", "Aur\u00e9lie Herbelot", "Sandro Pezzelle", "Raffaella Bernardi"], "venue": "In Proceedings of the 5th Workshop on Vision", "citeRegEx": "Sorodoc et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sorodoc et al\\.", "year": 2016}, {"title": "MatConvNet \u2013 Convolutional Neural Networks for MATLAB", "author": ["Vedaldi", "Lenc2015] Andrea Vedaldi", "Karel Lenc"], "venue": "Proceeding of the ACM Int. Conf. on Multimedia", "citeRegEx": "Vedaldi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedaldi et al\\.", "year": 2015}, {"title": "Dissociated neural correlates of quantity processing of quantifiers, numbers, and numerosities", "author": ["Wei et al.2014] Wei Wei", "Chuansheng Chen", "Tao Yang", "Han Zhang", "Xinlin Zhou"], "venue": "Human brain mapping,", "citeRegEx": "Wei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2014}, {"title": "Children\u2019s acquisition of the number words and the counting system", "author": ["Karen Wynn"], "venue": "Cognitive psychology,", "citeRegEx": "Wynn.,? \\Q1992\\E", "shortCiteRegEx": "Wynn.", "year": 1992}, {"title": "Salient object subitizing", "author": ["Zhang et al.2015] Jianming Zhang", "Shugao Ma", "Mehrnoosh Sameki", "Stan Sclaroff", "Margrit Betke", "Zhe Lin", "Xiaohui Shen", "Brian Price", "Radomir Mech"], "venue": "In Proceedings of the IEEE Conference on Computer Vision", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Salient object subitizing", "author": ["Zhang et al.2016] Jianming Zhang", "Shuga Ma", "Mehrnoosh Sameki", "Stan Sclaroff", "Margrit Betke", "Zhe Lin", "Xiaohui Shen", "Brian Price", "Radom\u00edr M\u011bch"], "venue": "arXiv preprint arXiv:1607.07525", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "Although they share a number of syntactic, semantic and pragmatic properties (Hurewitz et al., 2006), and they are both learned in a fairly stable order of acquisition across languages (Wynn, 1992; Katsos et al.", "startOffset": 77, "endOffset": 100}, {"referenceID": 22, "context": ", 2006), and they are both learned in a fairly stable order of acquisition across languages (Wynn, 1992; Katsos et al., 2016), these quantity expressions underlie fairly different cognitive and neural mechanisms.", "startOffset": 92, "endOffset": 125}, {"referenceID": 7, "context": "First, they are handled differently by the language acquisition system, with children recognizing their disparate characteristics since early development, even before becoming \u2018full-counters\u2019 (Hurewitz et al., 2006; Sarnecka and Gelman, 2004; Barner et al., 2009).", "startOffset": 192, "endOffset": 263}, {"referenceID": 0, "context": "First, they are handled differently by the language acquisition system, with children recognizing their disparate characteristics since early development, even before becoming \u2018full-counters\u2019 (Hurewitz et al., 2006; Sarnecka and Gelman, 2004; Barner et al., 2009).", "startOffset": 192, "endOffset": 263}, {"referenceID": 21, "context": "Second, while the neural processing of cardinals relies on the brain region devoted to the representation of quantities, quantifiers rather elicit regions for general semantic processing (Wei et al., 2014).", "startOffset": 187, "endOffset": 205}, {"referenceID": 0, "context": "or proportions of sets (Barner et al., 2009).", "startOffset": 23, "endOffset": 44}, {"referenceID": 13, "context": "As a consequence, speakers can reliably answer questions involving quantifiers even in contexts that preclude counting (Pietroski et al., 2009), as well as children lacking exact cardinality concepts can understand and appropriately use quantifiers in grounded contexts (Halberda et al.", "startOffset": 119, "endOffset": 143}, {"referenceID": 6, "context": ", 2009), as well as children lacking exact cardinality concepts can understand and appropriately use quantifiers in grounded contexts (Halberda et al., 2008; Barner et al., 2009).", "startOffset": 134, "endOffset": 178}, {"referenceID": 0, "context": ", 2009), as well as children lacking exact cardinality concepts can understand and appropriately use quantifiers in grounded contexts (Halberda et al., 2008; Barner et al., 2009).", "startOffset": 134, "endOffset": 178}, {"referenceID": 2, "context": "Our approach is thus different from salient objects detection, where the distinction targets/distractors is missing (Borji et al., 2015; Zhang et al., 2015; Zhang et al., 2016).", "startOffset": 116, "endOffset": 176}, {"referenceID": 23, "context": "Our approach is thus different from salient objects detection, where the distinction targets/distractors is missing (Borji et al., 2015; Zhang et al., 2015; Zhang et al., 2016).", "startOffset": 116, "endOffset": 176}, {"referenceID": 24, "context": "Our approach is thus different from salient objects detection, where the distinction targets/distractors is missing (Borji et al., 2015; Zhang et al., 2015; Zhang et al., 2016).", "startOffset": 116, "endOffset": 176}, {"referenceID": 17, "context": "With respect to cardinals, our approach is similar to (Segu\u00ed et al., 2015), who propose a model for counting people in natural ar X iv :1 70 2.", "startOffset": 54, "endOffset": 74}, {"referenceID": 4, "context": "scenes, and to more recent work aimed at counting either everyday objects in natural images (Chattopadhyay et al., 2016) or geometrical objects with attributes in synthetic scenes (Johnson et al.", "startOffset": 92, "endOffset": 120}, {"referenceID": 8, "context": ", 2016) or geometrical objects with attributes in synthetic scenes (Johnson et al., 2016).", "startOffset": 67, "endOffset": 89}, {"referenceID": 19, "context": "With respect to quantifiers, our approach is similar to (Sorodoc et al., 2016), who use quantifiers no, some, and all to quantify over sets of colored dots.", "startOffset": 56, "endOffset": 78}, {"referenceID": 23, "context": "Typically, images depict one salient object and even when multiple salient objects are present, only a handful of cases contain both targets and distractors (Zhang et al., 2015; Zhang et al., 2016).", "startOffset": 157, "endOffset": 197}, {"referenceID": 24, "context": "Typically, images depict one salient object and even when multiple salient objects are present, only a handful of cases contain both targets and distractors (Zhang et al., 2015; Zhang et al., 2016).", "startOffset": 157, "endOffset": 197}, {"referenceID": 9, "context": "Although a large number of image datasets are currently available (see Lin et al. (2014) among many others), no one fully satisfies these requirements.", "startOffset": 71, "endOffset": 89}, {"referenceID": 0, "context": "Cs up to 4 are acquired by children incrementally at subsequent stages of their development, with higher numbers being learned upon this knowledge with the ability of counting (Barner et al., 2009).", "startOffset": 176, "endOffset": 197}, {"referenceID": 12, "context": "their exact number can be immediately and effortlessly grasped) due to which they are usually referred to as \u2018subitizing\u2019 range (Piazza et al., 2011; Railo et al., 2016).", "startOffset": 128, "endOffset": 169}, {"referenceID": 14, "context": "their exact number can be immediately and effortlessly grasped) due to which they are usually referred to as \u2018subitizing\u2019 range (Piazza et al., 2011; Railo et al., 2016).", "startOffset": 128, "endOffset": 169}, {"referenceID": 5, "context": "We use images from ImageNet (Deng et al., 2009).", "startOffset": 28, "endOffset": 47}, {"referenceID": 1, "context": "Starting from the full list of 203 concepts and corresponding images extracted by Cassani (2014), we discarded those concepts whose corresponding word had low/null frequency in the large corpus used in (Baroni et al., 2014).", "startOffset": 202, "endOffset": 223}, {"referenceID": 15, "context": "We used the VGG-19 model pretrained on the ImageNet ILSVRC data (Russakovsky et al., 2015) implemented in the MatConvNet toolbox (Vedaldi and Lenc, 2015).", "startOffset": 64, "endOffset": 90}, {"referenceID": 2, "context": "Starting from the full list of 203 concepts and corresponding images extracted by Cassani (2014), we discarded those concepts whose corresponding word had low/null frequency in the large corpus used in (Baroni et al.", "startOffset": 82, "endOffset": 97}, {"referenceID": 10, "context": "a 400-d embedding built with the CBOW architecture of word2vec (Mikolov et al., 2013) and the best-predictive parameters of Baroni et al.", "startOffset": 63, "endOffset": 85}, {"referenceID": 1, "context": ", 2013) and the best-predictive parameters of Baroni et al. (2014) on a 2.", "startOffset": 46, "endOffset": 67}, {"referenceID": 6, "context": "This mapping, we conjecture, would mimic the multimodal mechanism by which children acquire the meaning of both Cs and Qs (see Halberda et al. (2008)).", "startOffset": 127, "endOffset": 150}, {"referenceID": 11, "context": "This evidence, we believe, would further motivate our \u2018one quantifed expression, one function\u2019 approach, which is partially inspired by the evidence that, in human brain, so-called number neurons are tuned to preferred numbers (Nieder, 2016).", "startOffset": 227, "endOffset": 241}], "year": 2017, "abstractText": "People can refer to quantities in a visual scene by using either exact cardinals (e.g. one, two, three) or natural language quantifiers (e.g. few, most, all). In humans, these two processes underlie fairly different cognitive and neural mechanisms. Inspired by this evidence, the present study proposes two models for learning the objective meaning of cardinals and quantifiers from visual scenes containing multiple objects. We show that a model capitalizing on a \u2018fuzzy\u2019 measure of similarity is effective for learning quantifiers, whereas the learning of exact cardinals is better accomplished when information about number is provided.", "creator": "LaTeX with hyperref package"}}}