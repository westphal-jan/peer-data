{"id": "1702.05398", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2017", "title": "Experiment Segmentation in Scientific Discourse as Clause-level Structured Prediction using Recurrent Neural Networks", "abstract": "We propose a deep learning model for identifying structure within experiment narratives in scientific literature. We take a sequence labeling approach to this problem, and label clauses within experiment narratives to identify the different parts of the experiment. Our dataset consists of paragraphs taken from open access PubMed papers labeled with rhetorical information as a result of our pilot annotation. Our model is a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells that labels clauses. The clause representations are computed by combining word representations using a novel attention mechanism that involves a separate RNN. We compare this model against LSTMs where the input layer has simple or no attention and a feature rich CRF model. Furthermore, we describe how our work could be useful for information extraction from scientific literature.", "histories": [["v1", "Fri, 17 Feb 2017 15:39:21 GMT  (222kb,D)", "http://arxiv.org/abs/1702.05398v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["pradeep dasigi", "gully a p c burns", "eduard hovy", "anita de waard"], "accepted": false, "id": "1702.05398"}, "pdf": {"name": "1702.05398.pdf", "metadata": {"source": "CRF", "title": "Experiment Segmentation in Scientific Discourse as Clause-level Structured Prediction using Recurrent Neural Networks", "authors": ["Pradeep Dasigi", "Gully A.P.C. Burns", "Eduard Hovy", "Anita de Waard"], "emails": [], "sections": [{"heading": "Introduction", "text": "An important part of science is the communication of results. There are well-established rhetorical guidelines (Alley, 1996) for scientific writing, which cover disciplines and thus narratives that describe evidence within a scientific investigation, are expected to have a certain structure. Typically, the description begins with certain background information that has already been proven, followed by some motivating hypotheses to introduce the experiment, the methods that are based on these results, the conclusions that are drawn from it. Understanding this structure is important as it allows the higher level of construction of the general reasoning of the paper. The reader assembles the pieces to understand what was done, why it was done, what prior knowledge it builds and / or refutes, and with what certainty the final conclusions should be accepted. Without such a general model of the experiment, the reader has nothing but basic assertions. In this work, our goal is to identify these elements of discourse that represent an experiment narrative elements."}, {"heading": "Related Work", "text": "There are a number of approaches that indicate that it is a way in which people put themselves at the centre. (...) There are a number of approaches in which people are put at the centre. (...) There are a number of approaches in which people are put at the centre. (...) There are a number of approaches in which people are put at the centre. (...) There are a number of approaches in which people are put at the centre. (...) There is a number of approaches in which people are put at the centre. (...) \"There is only\" There is a number of approaches in which people are put at the centre. \"(...) There is a number of approaches in which people are put at the centre.\" (...) There is a way in which people are put at the centre. \"(...) There is a way in which people are put at the centre. (...) There is a way in which people are put at the centre. (...)"}, {"heading": "Approach", "text": "Dre rf\u00fc eid rf\u00fc eid rf\u00fc rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the r"}, {"heading": "Input to LSTM", "text": "A weighted sum of the input tensor is calculated, with the weights coming from the attention model, and it is added to the LSTM.Dsum [i,:] = A [i,:].D [i,:,:] \u0109Rc \u00b7 d (9) The above equation shows the composite representation of the ith set, which is stored as an ith line in dsum."}, {"heading": "Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Implementation Details", "text": "We used the 200 dimension vectors trained on Pubmed Central data from Pyysalo et al. (2013) as input representations and projected them onto 50d to control the parameter space of the entire model. The projection operator is trained along with the rest of the pipeline. LSTMs were implemented 2 with Keras (Chollet, 2015) and Attention with Theano (Bergstra et al., 2010). We trained for2Code, which is publicly available at https: / / github.com / edvisees / sciDTatmost 100 epochs, while monitoring the accuracy of the provided validation data at early stop. As an optimization algorithm, we used ADAM (Kingma and Ba, 2014)."}, {"heading": "Data Preprocessing, Annotations and Pipelines", "text": "We created a scientific discourse that highlighted data sets from 75 papers on intercellular cancers from the Open Access3 subset of Pubmed Central. Using a multi-strand pre-processing pipeline, we extracted the result sections of each of these papers and analyzed all sentences using the Stanford parser (Socher et al., 2013), which separated the main and subordinate sentences of each sentence we processed in a sequence across individual paragraphs. We asked domain experts to label each of these clauses using the seven label taxonomy proposed by De Waard and Pander Maat (2012), and we added a no-label for those clauses that do not fall into one category. Each sequence in the data set matches the clauses extracted from one paragraph, so we assume that paragraphs represent minimal experimental narratives."}, {"heading": "Results and Analysis", "text": "One baseline model we are comparing to is a conditional random field (Lafferty, McCallum and Pereira, 2001) that uses as features part of language tags, the identities of verbs and adverbs, the presence of figure references and quotations, and handmade lexicon characteristics 5 that indicate certain discourse types. In addition, we are also testing a variant of our model that does not use attention in the input layer and that simply obtains the sentence vectors as an average of the vectors of words in them. Table 2 shows accuracy values and weighted averages of f-scores from 5-fold cross-validation of the two baseline models and the two attention-based models. The performance of the averaged input weights SciDT is comparable to the CRF model, whereas the two attention models perform better."}, {"heading": "Discussion and Conclusion", "text": "The results show that our attention-based composition mechanism, which is used to encode clauses, adds value to the LSTM model. Visualizations show that the clause context model actually learns to take into account words that are important for the final tagging decision. In the future, we will expand the idea of contextual attention to treat words at context level at paragraph level. Our system can complement existing IE tools that are based on scientific literature and provide useful epistemic and contextual characteristics. The structure of experiments provides additional context information that can help various downstream tasks. For event conferences, the structure can be used to precisely resolve anaphora linkages. For example, a reference made in an implication statement may likely point to an entity in a result statement that follows from it. It is worth noting that the information systems that we use are not helpful in small taxephonics."}], "references": [{"title": "The craft of scientific writing", "author": ["M. Alley"], "venue": "Springer Science & Business Media.", "citeRegEx": "Alley,? 1996", "shortCiteRegEx": "Alley", "year": 1996}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Theano: A cpu and gpu math compiler in python", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "van der Walt, S., and Millman, J., eds., Proceedings of the 9th Python in Science Conference, 3 \u2013 10.", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/ fchollet/keras.", "citeRegEx": "Chollet,? 2015", "shortCiteRegEx": "Chollet", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research 12(Aug):2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Verb form indicates discourse segment type in biological research", "author": ["A. De Waard", "H. Pander Maat"], "venue": null, "citeRegEx": "Waard and Maat,? \\Q2012\\E", "shortCiteRegEx": "Waard and Maat", "year": 2012}, {"title": "Identifying the information structure of scientific abstracts: an investigation of three different schemes", "author": ["Y. Guo", "A. Korhonen", "M. Liakata", "I.S. Karolinska", "L. Sun", "U. Stenius"], "venue": "Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, 99\u2013", "citeRegEx": "Guo et al\\.,? 2010", "shortCiteRegEx": "Guo et al\\.", "year": 2010}, {"title": "Analyzing the dynamics of research by extracting key aspects of scientific papers", "author": ["S. Gupta", "C. Manning"], "venue": "Proceedings of 5th International Joint Conference on Natural Language Processing, 1\u20139. Chiang Mai, Thailand: Asian Federation of Natural Language Processing.", "citeRegEx": "Gupta and Manning,? 2011", "shortCiteRegEx": "Gupta and Manning", "year": 2011}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Kocisky", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "Advances in Neural Information Processing Systems, 1684\u20131692.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Identifying sections in scientific abstracts using conditional random fields", "author": ["K. Hirohata", "N. Okazaki", "S. Ananiadou", "M. Ishizuka", "M.I. Biocentre"], "venue": null, "citeRegEx": "Hirohata et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hirohata et al\\.", "year": 2008}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba,? 2014", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F.C. Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Automatic recognition of conceptualization zones in scientific articles and two life science applications", "author": ["M. Liakata", "S. Saha", "S. Dobnik", "C. Batchelor", "D. Rebholz-Schuhmann"], "venue": "Bioinformatics (Oxford, England) 28(7):991\u20131000.", "citeRegEx": "Liakata et al\\.,? 2012", "shortCiteRegEx": "Liakata et al\\.", "year": 2012}, {"title": "Zones of conceptualisation in scientific papers: a window to negative and speculative statements", "author": ["M. Liakata"], "venue": "Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, 1\u20134. Association for Computational Linguistics.", "citeRegEx": "Liakata,? 2010", "shortCiteRegEx": "Liakata", "year": 2010}, {"title": "Rhetorical structure theory: Toward a functional theory of text organization", "author": ["W.C. Mann", "S.A. Thompson"], "venue": "Text-Interdisciplinary Journal for the Study of Discourse 8(3):243\u2013281.", "citeRegEx": "Mann and Thompson,? 1988", "shortCiteRegEx": "Mann and Thompson", "year": 1988}, {"title": "The theory and practice of discourse parsing and summarisation", "author": ["D. Marcu"], "venue": null, "citeRegEx": "Marcu,? \\Q2000\\E", "shortCiteRegEx": "Marcu", "year": 2000}, {"title": "An annotation scheme for a rhetorical analysis of biology articles", "author": ["Y. Mizuta", "N. Collier"], "venue": "LREC, 1737\u2013 1740.", "citeRegEx": "Mizuta and Collier,? 2004", "shortCiteRegEx": "Mizuta and Collier", "year": 2004}, {"title": "Evaluating a meta-knowledge annotation scheme for bio-events", "author": ["R. Nawaz", "P. Thompson", "S. Ananiadou"], "venue": "Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, 69\u201377. Association for Computational Linguistics.", "citeRegEx": "Nawaz et al\\.,? 2010", "shortCiteRegEx": "Nawaz et al\\.", "year": 2010}, {"title": "The medical research paper: Structure and functions", "author": ["K.N. Nwogu"], "venue": "English for specific purposes 16(2):119\u2013 138.", "citeRegEx": "Nwogu,? 1997", "shortCiteRegEx": "Nwogu", "year": 1997}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, 1532\u201343.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Distributional semantics resources for biomedical text processing", "author": ["S. Pyysalo", "F. Ginter", "H. Moen", "T. Salakoski", "S. Ananiadou"], "venue": "Proceedings of Languages in Biology and Medicine.", "citeRegEx": "Pyysalo et al\\.,? 2013", "shortCiteRegEx": "Pyysalo et al\\.", "year": 2013}, {"title": "Parsing with compositional vector grammars", "author": ["R. Socher", "J. Bauer", "C.D. Manning", "A.Y. Ng"], "venue": "In Proceedings of the ACL conference. Citeseer.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Approaches to studying cellular signaling: a primer for morphologists", "author": ["K.K.H. Svoboda", "W.R. Reenstra"], "venue": "The Anatomical record 269(2):123\u2013139.", "citeRegEx": "Svoboda and Reenstra,? 2002", "shortCiteRegEx": "Svoboda and Reenstra", "year": 2002}, {"title": "Discourse-level argumentation in scientific articles: human and automatic annotation", "author": ["S. Teufel", "M. Moens"], "venue": "Towards Standards and Tools for Discourse Tagging, 84\u201393.", "citeRegEx": "Teufel and Moens,? 1999", "shortCiteRegEx": "Teufel and Moens", "year": 1999}, {"title": "Summarizing scientific articles: experiments with relevance and rhetorical status", "author": ["S. Teufel", "M. Moens"], "venue": "Computational linguistics 28(4):409\u2013445.", "citeRegEx": "Teufel and Moens,? 2002", "shortCiteRegEx": "Teufel and Moens", "year": 2002}, {"title": "Argumentative Zoning: Information Extraction from Scientific Text", "author": ["S. Teufel"], "venue": "Ph.D. Dissertation, School of Cognitive Science, University of Edinburgh, Edinburg.", "citeRegEx": "Teufel,? 2000", "shortCiteRegEx": "Teufel", "year": 2000}, {"title": "Grammar as a foreign language", "author": ["O. Vinyals", "\u0141. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2755\u20132763.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "A First Course in Systems Biology", "author": ["E. Voit"], "venue": "Garland Science, 1st edition.", "citeRegEx": "Voit,? 2012", "shortCiteRegEx": "Voit", "year": 2012}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "arXiv preprint arXiv:1502.03044.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "There are well established rhetorical guidelines (Alley, 1996) for scientific writing that are used across disciplines and consequently, narratives describing evidence within a scientific investigation are expected to have a certain structure.", "startOffset": 49, "endOffset": 62}, {"referenceID": 14, "context": "There exist several proposals for experiment discourse models (Liakata, 2010; Nawaz, Thompson, and Ananiadou, 2010; Mizuta and Collier, 2004; Nwogu, 1997).", "startOffset": 62, "endOffset": 154}, {"referenceID": 17, "context": "There exist several proposals for experiment discourse models (Liakata, 2010; Nawaz, Thompson, and Ananiadou, 2010; Mizuta and Collier, 2004; Nwogu, 1997).", "startOffset": 62, "endOffset": 154}, {"referenceID": 19, "context": "There exist several proposals for experiment discourse models (Liakata, 2010; Nawaz, Thompson, and Ananiadou, 2010; Mizuta and Collier, 2004; Nwogu, 1997).", "startOffset": 62, "endOffset": 154}, {"referenceID": 14, "context": "There exist several proposals for experiment discourse models (Liakata, 2010; Nawaz, Thompson, and Ananiadou, 2010; Mizuta and Collier, 2004; Nwogu, 1997). We adopt the discourse type taxonomy for biological papers suggested by De Waard and Pander Maat (2012), and define our problem as identifying the discourse type of each clause in a given experiment description.", "startOffset": 63, "endOffset": 260}, {"referenceID": 27, "context": "Typically, researchers in this field use a number of small-scale experimental assays to investigate molecular events, see Voit (2012) and Svoboda and Reenstra (2002) for textbook and review introductions.", "startOffset": 122, "endOffset": 134}, {"referenceID": 23, "context": "Typically, researchers in this field use a number of small-scale experimental assays to investigate molecular events, see Voit (2012) and Svoboda and Reenstra (2002) for textbook and review introductions.", "startOffset": 138, "endOffset": 166}, {"referenceID": 22, "context": "Teufel and Moens (2002) and Teufel and Moens (1999) describe argumentative zoning (AZ), a way of classifying scientific papers at a sentence level, into zones, thus extracting the structure from entire papers.", "startOffset": 0, "endOffset": 24}, {"referenceID": 22, "context": "Teufel and Moens (2002) and Teufel and Moens (1999) describe argumentative zoning (AZ), a way of classifying scientific papers at a sentence level, into zones, thus extracting the structure from entire papers.", "startOffset": 0, "endOffset": 52}, {"referenceID": 9, "context": "Hirohata et al. (2008) use a 4-way classification scheme for abstracts of scientific papers for identifying objectives, methods, results and conclusions.", "startOffset": 0, "endOffset": 23}, {"referenceID": 9, "context": "Hirohata et al. (2008) use a 4-way classification scheme for abstracts of scientific papers for identifying objectives, methods, results and conclusions. Liakata (2010)", "startOffset": 0, "endOffset": 169}, {"referenceID": 13, "context": "51 for LibSVM classifiers (Liakata et al., 2012).", "startOffset": 26, "endOffset": 48}, {"referenceID": 26, "context": "In these studies, the focus of research is largely centered on modeling the discourse being used to construct a scientific argument, driving towards understanding \u201csentiment expressed towards cited work, ownership of ideas, and speech acts which express rhetorical statements typical for scientific argumentation\u201d (Teufel, 2000).", "startOffset": 314, "endOffset": 328}, {"referenceID": 6, "context": "Guo et al. (2010) used SVM and Na\u0131\u0308ve Bayes classifiers to compare the three schemes described above.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "Guo et al. (2010) used SVM and Na\u0131\u0308ve Bayes classifiers to compare the three schemes described above. Gupta and Manning (2011) also studied the problem of extracting focus, techniques and the domain of research papers to identify the influence of research communities over each other.", "startOffset": 0, "endOffset": 127}, {"referenceID": 4, "context": "Particularly in sequence labeling tasks in text, (Collobert et al., 2011) words are represented as vectors Type Definition", "startOffset": 49, "endOffset": 73}, {"referenceID": 8, "context": "Attention has been used for complex tasks like question answering (Hermann et al., 2015) and machine translation (Bahdanau, Cho, and Bengio, 2014).", "startOffset": 66, "endOffset": 88}, {"referenceID": 27, "context": "In sequence-tosequence learning problems like machine translation (Bahdanau, Cho, and Bengio, 2014), parsing (Vinyals et al., 2015) and image caption generation (Xu et al.", "startOffset": 109, "endOffset": 131}, {"referenceID": 29, "context": ", 2015) and image caption generation (Xu et al., 2015), one network is used to encode the input modality, and a different network to decode into the output modality, with the decoder using attention to learn parts of the input to attend to for generating a given part of the output sequence.", "startOffset": 37, "endOffset": 54}, {"referenceID": 16, "context": "It is generally accepted that relations between non-overlapping chunks of text need to be considered to account for the overall meaning (Marcu, 2000).", "startOffset": 136, "endOffset": 149}, {"referenceID": 15, "context": "While there are many discourse theories (see Marcu (2000), chapter 2 for an overview), Rhetorical Structure Theory (RST) by Mann and Thompson (1988), received a lot of attention.", "startOffset": 45, "endOffset": 58}, {"referenceID": 15, "context": "While there are many discourse theories (see Marcu (2000), chapter 2 for an overview), Rhetorical Structure Theory (RST) by Mann and Thompson (1988), received a lot of attention.", "startOffset": 124, "endOffset": 149}, {"referenceID": 10, "context": "Finally, Dsumm is fed to the Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) Hochreiter and Schmidhuber (1997) cells, to label the clauses.", "startOffset": 95, "endOffset": 129}, {"referenceID": 3, "context": "LSTMs were implemented 2 using Keras (Chollet, 2015) and attention using Theano (Bergstra et al.", "startOffset": 37, "endOffset": 52}, {"referenceID": 2, "context": "LSTMs were implemented 2 using Keras (Chollet, 2015) and attention using Theano (Bergstra et al., 2010).", "startOffset": 80, "endOffset": 103}, {"referenceID": 19, "context": "We used the 200 dimension vectors trained on Pubmed Central data by Pyysalo et al. (2013) as input representations and projected them down to 50d to keep the parameter space of the entire model under control.", "startOffset": 68, "endOffset": 90}, {"referenceID": 11, "context": "We used ADAM (Kingma and Ba, 2014) as the optimization algorithm.", "startOffset": 13, "endOffset": 34}, {"referenceID": 22, "context": "Using a multithreaded preprocessing pipeline, we extracted the Results sections of each of those papers, and parsed all the sentences using the Stanford Parser (Socher et al., 2013).", "startOffset": 160, "endOffset": 181}, {"referenceID": 22, "context": "Using a multithreaded preprocessing pipeline, we extracted the Results sections of each of those papers, and parsed all the sentences using the Stanford Parser (Socher et al., 2013). This process separated the main and subordinate clauses of each sentence that we process as a sequence over separate paragraphs. We asked domain experts to label each of those clauses using the seven label taxonomy suggested by De Waard and Pander Maat (2012). We also added a None label for those clauses that do not fall under any category.", "startOffset": 161, "endOffset": 443}], "year": 2017, "abstractText": "We propose a deep learning model for identifying structure within experiment narratives in scientific literature. We take a sequence labeling approach to this problem, and label clauses within experiment narratives to identify the different parts of the experiment. Our dataset consists of paragraphs taken from open access PubMed papers labeled with rhetorical information as a result of our pilot annotation. Our model is a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells that labels clauses. The clause representations are computed by combining word representations using a novel attention mechanism that involves a separate RNN. We compare this model against LSTMs where the input layer has simple or no attention and a feature rich CRF model. Furthermore, we describe how our work could be useful for information extraction from scientific literature.", "creator": "LaTeX with hyperref package"}}}