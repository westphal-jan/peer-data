{"id": "1702.05531", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2017", "title": "Analysis and Optimization of fastText Linear Text Classifier", "abstract": "The paper [1] shows that simple linear classifier can compete with complex deep learning algorithms in text classification applications. Combining bag of words (BoW) and linear classification techniques, fastText [1] attains same or only slightly lower accuracy than deep learning algorithms [2-9] that are orders of magnitude slower. We proved formally that fastText can be transformed into a simpler equivalent classifier, which unlike fastText does not have any hidden layer. We also proved that the necessary and sufficient dimensionality of the word vector embedding space is exactly the number of document classes. These results help constructing more optimal linear text classifiers with guaranteed maximum classification capabilities. The results are proven exactly by pure formal algebraic methods without attracting any empirical data.", "histories": [["v1", "Fri, 17 Feb 2017 22:10:28 GMT  (631kb)", "http://arxiv.org/abs/1702.05531v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vladimir zolotov", "david kung"], "accepted": false, "id": "1702.05531"}, "pdf": {"name": "1702.05531.pdf", "metadata": {"source": "CRF", "title": "Analysis and Optimization of fastText Linear Text Classifier", "authors": ["Vladimir Zolotov", "David Kung"], "emails": ["zolotov@us.ibm.com,", "kung@us.ibm.com"], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Background", "text": "This year it is more than ever before in the history of the city."}, {"heading": "3. Equivalent transformation of LBoW classifier", "text": "The following theorem defines this transformation and proves the strict equivalence of the resulting classifier and its precursors. Theorem 1. For each LBoW classifier with m document classes, there is a strictly equivalent classifier with m-dimensional word vectors. This classifier has no hidden layers. Proof. Combining formulas (1) and (2), we get the following formula for a classification vector calculated by an LBoW classifier C = (X, B): This classifier has no hidden layers. Proof.Proof.Combining formulas (1) and (2), we get the following formula for a classification vector calculated by an LBoW classifier C = (X, B)."}, {"heading": "4. Minimum dimensionality of word vectors of LBoW classifier", "text": "rE \"s rf\u00fc eid rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rtef\u00fc the rf\u00fc the rtef\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rtef\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "5. Further reduction of dimensionality", "text": "In fact, it is such that it is a way in which people are able to live in, to live, to live, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to play, to work, to play, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to work, to play, to play, to work, to work, to work, to work, to play, to work, to work, to work, to play, to work, to work, to work, to work, to work, to work, to work, to work, to work, to play, to play, to play, to play, to work, to work, to play, to play, to work, to play, to play, to play, to play, to work, to play, to play, to play, to play, to play, to work, to play, to play, to play, to play, to play, to play, to play, to play, to work, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to play, to"}, {"heading": "6. Conclusions", "text": "The proven theorems show that word vectors can contain all the information that the LBoW classifier can learn from the training materials, and that no additional layers are required.There are text classification problems that require word vectors with as many coordinates as the number of document classes. Extending the length of word vectors beyond the number of document classes cannot improve the accuracy of an LBoW classifier. The ways to improve accuracy are either to extend the dictionary by including word combinations or subwords, or to introduce non-linear functions that take into account the nonlinearity of word interactions. Any LBoW classifier that resembles fasttext can be converted into a corresponding classifier without a hidden layer, with word vectors having as many coordinates as many document classes can be discerned.The transformation occurs by explicitly recalculating word vectors and does not require retraining or tuning of the constructed classifier."}, {"heading": "7. References", "text": "1. Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov. Bag of tricks for efficient text classification. arXiv preprint arXiv: 1607.01759. 2. Xiang Zhang and Yann LeCun. 2015. Understanding text from the ground up. 4. Yijun Xiao and Kyunghyun Cho. 2016. Efficient document classification at the sign level by combining convolution and recurring laymen. arXiv preprint arXiv: 1602.00367. 5. Alexis Conneau, Holger Schwenk, Lo\u0131c Barrault and Yann Lecun."}], "references": [{"title": "Text understanding from scratch. arXiv preprint arXiv:1502.01710", "author": ["Xiang Zhang", "Yann LeCun"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Efficient character-level document classification by combining convolution and recurrent layers. arXiv preprint arXiv:1602.00367", "author": ["Yijun Xiao", "Kyunghyun Cho"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Very deep convolutional networks for natural language processing", "author": ["Alexis Conneau", "Holger Schwenk", "Lo\u0131c Barrault", "Yann Lecun"], "venue": "arXiv preprint arXiv:1606.01781", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Yfcc100m: The new data in multimedia research", "author": ["Bart Thomee", "David A Shamma", "Gerald Friedland", "Benjamin Elizalde", "Karl Ni", "Douglas Poland", "Damian Borth", "Li-Jia Li"], "venue": "Volume 59,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "tagspace: Semantic embeddings from hashtags", "author": ["Jason Weston", "Sumit Chopra", "Keith Adams"], "venue": "In EMNLP", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Combining bag of words (BoF) and linear classification techniques, fastText [1] attains same or only slightly lower accuracy than deep learning algorithms [2-9] that are orders of magnitude slower.", "startOffset": 155, "endOffset": 160}, {"referenceID": 1, "context": "Combining bag of words (BoF) and linear classification techniques, fastText [1] attains same or only slightly lower accuracy than deep learning algorithms [2-9] that are orders of magnitude slower.", "startOffset": 155, "endOffset": 160}, {"referenceID": 2, "context": "Combining bag of words (BoF) and linear classification techniques, fastText [1] attains same or only slightly lower accuracy than deep learning algorithms [2-9] that are orders of magnitude slower.", "startOffset": 155, "endOffset": 160}, {"referenceID": 3, "context": "Combining bag of words (BoF) and linear classification techniques, fastText [1] attains same or only slightly lower accuracy than deep learning algorithms [2-9] that are orders of magnitude slower.", "startOffset": 155, "endOffset": 160}, {"referenceID": 4, "context": "Combining bag of words (BoF) and linear classification techniques, fastText [1] attains same or only slightly lower accuracy than deep learning algorithms [2-9] that are orders of magnitude slower.", "startOffset": 155, "endOffset": 160}, {"referenceID": 5, "context": "Combining bag of words (BoF) and linear classification techniques, fastText [1] attains same or only slightly lower accuracy than deep learning algorithms [2-9] that are orders of magnitude slower.", "startOffset": 155, "endOffset": 160}, {"referenceID": 6, "context": "Combining bag of words (BoF) and linear classification techniques, fastText [1] attains same or only slightly lower accuracy than deep learning algorithms [2-9] that are orders of magnitude slower.", "startOffset": 155, "endOffset": 160}, {"referenceID": 7, "context": "Combining bag of words (BoF) and linear classification techniques, fastText [1] attains same or only slightly lower accuracy than deep learning algorithms [2-9] that are orders of magnitude slower.", "startOffset": 155, "endOffset": 160}, {"referenceID": 0, "context": "Their tool fastText [1] can be trained to the accuracy achieved with more complex deep learning algorithms [2-9], but orders of magnitude faster, even without using a high-performance GPU.", "startOffset": 107, "endOffset": 112}, {"referenceID": 1, "context": "Their tool fastText [1] can be trained to the accuracy achieved with more complex deep learning algorithms [2-9], but orders of magnitude faster, even without using a high-performance GPU.", "startOffset": 107, "endOffset": 112}, {"referenceID": 2, "context": "Their tool fastText [1] can be trained to the accuracy achieved with more complex deep learning algorithms [2-9], but orders of magnitude faster, even without using a high-performance GPU.", "startOffset": 107, "endOffset": 112}, {"referenceID": 3, "context": "Their tool fastText [1] can be trained to the accuracy achieved with more complex deep learning algorithms [2-9], but orders of magnitude faster, even without using a high-performance GPU.", "startOffset": 107, "endOffset": 112}, {"referenceID": 4, "context": "Their tool fastText [1] can be trained to the accuracy achieved with more complex deep learning algorithms [2-9], but orders of magnitude faster, even without using a high-performance GPU.", "startOffset": 107, "endOffset": 112}, {"referenceID": 5, "context": "Their tool fastText [1] can be trained to the accuracy achieved with more complex deep learning algorithms [2-9], but orders of magnitude faster, even without using a high-performance GPU.", "startOffset": 107, "endOffset": 112}, {"referenceID": 6, "context": "Their tool fastText [1] can be trained to the accuracy achieved with more complex deep learning algorithms [2-9], but orders of magnitude faster, even without using a high-performance GPU.", "startOffset": 107, "endOffset": 112}, {"referenceID": 7, "context": "Their tool fastText [1] can be trained to the accuracy achieved with more complex deep learning algorithms [2-9], but orders of magnitude faster, even without using a high-performance GPU.", "startOffset": 107, "endOffset": 112}, {"referenceID": 0, "context": "For getting higher accuracy, fastText uses some other tricks: filtering rare words, considering letter ngrams (sub-words) [2], word n-grams, etc.", "startOffset": 122, "endOffset": 125}], "year": 2017, "abstractText": "The paper [1] shows that simple linear classifier can compete with complex deep learning algorithms in text classification applications. Combining bag of words (BoF) and linear classification techniques, fastText [1] attains same or only slightly lower accuracy than deep learning algorithms [2-9] that are orders of magnitude slower. We proved formally that fastText can be transformed into a simpler equivalent classifier, which unlike fastText does not have any hidden layer. We also proved that the necessary and sufficient dimensionality of the word vector embedding space is exactly the number of document classes. These results help constructing more optimal linear text classifiers with guaranteed maximum classification capabilities. The results are proven exactly by pure formal algebraic methods without attracting any empirical data.", "creator": "Microsoft\u00ae Word 2016"}}}