{"id": "1702.05571", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2017", "title": "Thresholding based Efficient Outlier Robust PCA", "abstract": "We consider the problem of outlier robust PCA (OR-PCA) where the goal is to recover principal directions despite the presence of outlier data points. That is, given a data matrix $M^*$, where $(1-\\alpha)$ fraction of the points are noisy samples from a low-dimensional subspace while $\\alpha$ fraction of the points can be arbitrary outliers, the goal is to recover the subspace accurately. Existing results for \\OR-PCA have serious drawbacks: while some results are quite weak in the presence of noise, other results have runtime quadratic in dimension, rendering them impractical for large scale applications.", "histories": [["v1", "Sat, 18 Feb 2017 05:00:04 GMT  (106kb)", "http://arxiv.org/abs/1702.05571v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yeshwanth cherapanamjeri", "prateek jain", "praneeth netrapalli"], "accepted": false, "id": "1702.05571"}, "pdf": {"name": "1702.05571.pdf", "metadata": {"source": "CRF", "title": "Thresholding based Efficient Outlier Robust PCA", "authors": ["Yeshwanth Cherapanamjeri", "Prateek Jain Praneeth Netrapalli"], "emails": ["t-yecher@microsoft.com", "prajain@microsoft.com", "praneeth@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.05 571v 1 [cs.L G] 18 Feb 20In this paper, we provide a novel threshold-based iterative algorithm with maximum linear iteration complexity in data size. Furthermore, the fraction of outliers, \u03b1, that our method can handle is closely bound to constants and provides near-optimal computational complexity for general noise control. For the special case where the outliers originate from a low-dimensional subspace with additive Gaussian noise, we show that a modification of our threshold-based method leads to a significant improvement in recovery errors (of subspace) even if a large fraction of outliers is present."}, {"heading": "1 Introduction", "text": "A major reason for the success of PCA is that it can be used efficiently and even counterproductively. Unfortunately, SVD is fragile in terms of outliers and can lead to arbitrarily inaccurate main directions in the presence of even a small number of outliers. So, designing an outlier PCA (OR-PCA) is critical in terms of outliers and can lead to arbitrarily inaccurate main directions in the presence of outliers. Formally, setting the OR-PCA is as follows: There is a data matrix M (D)."}, {"heading": "1.1 Related Works", "text": "In this section we will discuss related work and compare existing results with ours. Existing theoretical results for OR-PCA fall into two categories: a) The first category of approaches, which are more consistent with our own work, are based on Outlier Pursuit (Xu et al. [2012a]), which optimizes a convex relaxation of the OR-PCA problem, in which ranking and column saving constraints are replaced by the trace standard (sum of individual values) and [sum of column lengths) penalties. That is, they solve the following optimization problem: (Outlier Pursuit): ranking and column saving constraints are replaced by the trace standard (sum of individual values) and [sum of column lengths) penalties. In other words, they solve the following optimization problem: (Outlier Pursuit): ranking and slit saving constraints are replaced by the trace standard (sum of column lengths)."}, {"heading": "1.2 Notations", "text": "We use the following notations in this essay: For a vector v, V and V denote the V norm, for a matrix M, M and M the M norm, while the V norm denotes the Frobenius norm of M. \u03c3k (M) the kst largest singular value of M. SVD denotes the singular value decay of a matrix. SVDr (M) refers to the rank-r SVD of M. In the face of a matrix M, Mi denotes the ith column of M, while Mi: the ith column of M. In the face of a matrix M, Rd and a row S, MS is defined as (MS) i = {Mi for i, 0 otherwise.M\\ S denotes the ith column of M."}, {"heading": "2 Problem Formulation", "text": "In this section we formally present the discontinuation of the work. We get M * = D * + C * * * * * * *, where the columns of D * * * Inlier and C * * * are outliers. Only a fraction of the points are outliers, i.e., only an alpha fraction of the columns of C * are not zero. Overall, we consider three scenarios: \u2022 OR-PCA (silent setting): The points in D * are completely in a low-dimensional subspace, i.e. D * = L * is a rank-r matrix. \u2022 OR-PCAN (low-noise setting): The points in D * are approximately in a low-dimensional subspace, i.e. D * = L + N *, where L * is the rank-r projection of D * and N * is the rank-r matrix."}, {"heading": "3 Our Results", "text": "In this section we will present our results for the three areas mentioned above."}, {"heading": "3.1 OR-PCA \u2013 Noiseless Setting", "text": "Let us remember that in the silent environment M * = D * + C * we observe where D * is a row-r, \u00b5-incoherent matrix corresponding to clean data points and the column support of C * is at most \u03b1n. Our main result for this setting is the following sentence. Let M *, D * and C * be as described above. If \u03b1 \u2264 1128\u00b52r, then algorithm 1 runs with the parameters \u03c1 = 1128\u00b52r and T = log 10n \u0445 M * 2 *, a subspace U results so that the guarantee of theorem 1 can immediately be converted into a binding on the partial space distance between U and that spanned by the columns of D *. In particular, we obtain a dense (I \u2212 UU) D \u00b2 -ricular construction."}, {"heading": "3.2 OR-PCAN \u2013 Arbitrary Noise", "text": "Here we observe that M * = D * + C *, where D * is a matrix with an almost low rank, i.e., D * = L * + N *, where L * is the best rank r approximation to D * and represents a p * incoherent matrix, while N * is a noise matrix. C * is again column-thin with at most a fraction of the columns, which is not zero. Theorem 2 (Arbitrary Noise). However, let us consider the above setting. If \u03b1 \u2264 1128\u00b52r, then algorithm 2 is when executed with the parameters \u03c1 = 1128\u00b52r, \u03b7 = 2\u00b5 \u221a r n and T = log 20 \u0445 M * 2 \u00b7 n iterations, the result is a subspace U, so that: (I \u2212 UU) L \u00b2, then algorithm 2, when executed, n \u00b0 n \u00b0, is the value of L \u00b2."}, {"heading": "3.3 OR-PCAG\u2013 Gaussian Noise", "text": "Here we observe M * = D * + C *, where D * is an almost low matrix, i.e., D * = L * + N *, where L * is a rank, \u00b5 incoherent matrix, while N * is a Gaussian matrix, where each entry is iid sampled by N (0, \u03c32). C * in turn is a column sparse with at most a fraction of the columns that are not zero. Theorem 3 (Gaussian data). Consider the above setting. Suppose \u03b1 11024\u00b52r. Then algorithm 3 stops after most T = n iterations and returns a sub-space U that is not zero. (I \u2212 UU) L * 2 \u2264 4 \u00b0, that a part of d \u00b2 we consider with a probability of 1 \u2212 U."}, {"heading": "4 Outlier Robust PCA: Noiseless Setting", "text": "In this section we present our algorithm TORP (algorithm 1), which is applicable to the specific case of noise-free data, i.e. if we can explain the basic ideas behind our algorithm approach and analysis techniques in a relatively simple way, the goal will be to estimate the individual vectors of the L-column. However, this setting can lead to singular vectors that are far from the U-column, because a few columns of the C-column can be so large that they can include the entire singular vectors in their direction. Instead, our algorithm tries to exploit two important structural properties of the problem: the economy of the C-column and the inaccuracy of the L-column. Our algorithm maintains a column-sparing estimation of the C-column (t) of the C-column."}, {"heading": "5 Outlier Robust PCA: General Noise", "text": "In this section, we present our algorithms for the general cases of Outlier Robust PCA: \"This is a complicated estimate.\" \"This is the goal to reconstruct the individual vectors of the outliers.\" \"Our algorithm for the general OR-PCA problem is based on the TORP algorithm, but with additional complexity.\" \"This is the algorithm that builds the outliers of the outliers C (t) and the main direction U (t) with two threshold operators along the SVD.\" \"But due to the noise N (t) it will cause arbitrary perturbations of the singular vectors of the U (t), the smaller eigenvalues of the L (t) and hence cannot be restored.\" To allay these concerns, our TORP-N algorithms proceed. \""}, {"heading": "6 Outlier Robust PCA: Gaussian Noise", "text": "In this section, we present our algorithm for the specific case of the Outlier Robust PCA problem, when inlier points are generated using a standard Gaussian noise model, that is, when D * = L * + N * Rd \u00b7 n, in which each input of the noise matrix N * * (*) is sampled (i.e. from N (0, \u03c32). Our result for arbitrary N * (Theorem 2) estimates U * to a zero F error, which is in relation to Gaussian noise. (However, using a slight deviation from Algorithm 2 and exploiting the noise structure, Algorithm 3 is able to estimate U * to a zero of O (d / log d), which is better than the previous one of O (d / log d) d). At a high level, the philosophy of our TORP-G algorithms is similar to TOR.eP, i.e."}, {"heading": "7 Proof Overview", "text": "In this section we give a brief overview of our analysis of the three main results."}, {"heading": "7.1 Noiseless Setting\u2014Theorem 1", "text": "In this section we present the proof of the subthreshold of U\\ S."}, {"heading": "7.2 Arbitrary Noise\u2014Theorem 2", "text": "Let us now briefly discuss the proof for theorem 2. In fact, we prove a stronger result: theorem 4. Leave M * = L * + C * + N * so that L * * meets assumption 1 and C * has a column density from 0 to 1128\u00b52r. Furthermore, let us assume that N * * * F * * * * k 16 for some k *. Then, algorithm 1 returns with 0 to 1128\u00b52r and 1 to 2\u00b5 \u00b0 r n with T = log 20 x M * 2 \u00b7 n * a subspace U that looks like this: it (I \u2212 UU \u00b2) L * * * * F * 3 to 2\u00ba k (U \u00b2 k)."}, {"heading": "7.3 Gaussian Noise\u2014Theorem 3", "text": "Our analysis of TORP-G shows that the algorithm is highly likely to maintain the following critical invariant: Invariant 1. We assume that the following condition applies to the two threshold steps used in Algorithm 3.1: If a column i 6 \"Supp (C\") is exceeded, then the following condition applies: N \"N\" i \"N\" N \"N\" N \"N\" N \"N\" N \"N\" 2 \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N N N N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \"N\" N \""}, {"heading": "8 Conclusions and Future Works", "text": "In this paper, we investigated the problem of outlier PCAs. We proposed a novel threshold-based approach that, under normal regularity conditions, can accurately restore the top main directions of clean data points as long as the number of outliers is less than O (1 / r), which is theoretically close to constant factors. For silent or arbitrary noise cases, our algorithms are based on two thresholds that entitle operators to detect outliers, and result in a better recovery compared to existing methods essentially in the same time as PCA vanilla. For Gauss noise, we get improved recovery guarantees, but at a higher running time.Although our limits have significant improvements over existing ones, they are still weaker than guarantees obtained by vanilla PCAs opposed to SVD in main outliers. For example, our errors are limited at arbitrary noise levels in the Frobenius standard."}, {"heading": "A Supplementary Results and Preliminaries", "text": "Here we will give and prove some useful results to prove our most important theorems. We will start by redefining Weyl's Disturbance Efficiency of Bhatia (1997). \u2212 Furthermore, let us show that the individual values of the sum of two matrices with a separate column are greater than those of the two matrices separately. \u2212 Let us show A-Rd \u00b7 n and B-Rd \u00b7 n two matrices with a separate column. \u2212 Let us show that the sum of two matrices with a separate column is greater than the two matrices separately. \u2212 Let A-Rd \u00b7 n and B-Rd \u00b7 n two matrices with a separate column."}, {"heading": "B Proof of Technical Lemmas", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Proof of Lemma 1", "text": "Proof. We prove the problem by a number of inequalities: The inequality results from the fact that U\\ S contains the uppermost single vectors of M\\ S."}, {"heading": "B.2 Proof of Lemma 2", "text": "Proof. The problem can be proved by the following series of inequalities: (1): (1): (1): (1): (1): (1): (1): (1): (1): (1): (2): (1): (2): (1): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2)): (2): (2): (2)): (2): (2): (2)): (2): (2)) (2): (2): (2): (2) (2): (2): (2)) (2): (2): (2) (2): (2): (2)): (2) (2): (2) (2): (2): (2): (2)): (4) (4) (4) (4) (4) (4): (4) (4): (4) (4): (4) (4) (4) (4) (4): (4) (4) (4) (4) (4): (4) (4) (4) (4)"}, {"heading": "B.3 Lemma 10", "text": "Lemma 10. Consider the setting of theorem 2. Let S [n] denote each subset of columns of M * such that | S | \u2264 3\u03c1n. Let us also assume that the matrix M * (L *, F *, C *) 16 stands for some k * [r]. Let M *\\ S (L *, N *, S, C *\\ S) denote the matrix M * (L *, N *, C *) that is projected onto the columns, not in S. Let U\\ S\u0440\\ SV denote the matrix M * (L *, N *, C *) that is projected onto the columns."}, {"heading": "SVD of M\u2217\\S for some k", "text": "This means that the columns of L + N and C are fragmented, so that we come to the conclusion that the columns of L + N are fragmented. (6) We have first fragmented the quantitative supports of L + N and C, so that we come to this conclusion (L + S + N). (M) That is, we have fragmented the quantitative supports of L + N and SU. (6) We have first the quantitative support of L (1) and then the quantitative support of L (S + N). (N) We have the quantitative support of L (S + N). (N) The quantitative support of L and the quantitative support of L (S). (N) We have the quantitative support of L and the quantitative support of L (S)."}, {"heading": "B.4 Proof of Lemma 11", "text": "If U\\ S\u0430\\ SV S (U \u00b2 S (V \u00b2 S)) is the rank-k SVD of M \u00b2 S (L \u00b2 S), then the following applies: i, 1 \u2264 i \u2264 n: I \u2212 U\\ SU\\ S) (L \u00b2 i + N \u00b2 i)."}, {"heading": "Proof.", "text": "Followed by (I \u2212 U\\ SU S) (I \u2212 U\\ SU S) (I \u2212 U\\ SU S) (I \u2212 U\\ SU S) (I \u2212 U\\ SU S) (I \u2212 U\\ SU S) (I \u2212 U\\ SU S) (I \u2212 S + N) (I \u2212 S\\ SU S) (I \u2212 S + N) (I \u2212 S + N) (I \u2212 S + N) (I \u2212 S + N) (I \u2212 S + N) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S (I \u2212 S) (I \u2212 S) (I \u2212 S (I \u2212 S) (I \u2212 S) (I \u2212 S (I \u2212 S) (I \u2212 S (I \u2212 S) (I \u2212 S (I \u2212 S) (I \u2212 S (I \u2212 S) (I \u2212 S (I \u2212 S) (I \u2212 S) (I \u2212 S (I \u2212 S) (I \u2212 S) (I \u2212 S (I \u2212 S) (I \u2212 S (I \u2212 S) (I \u2212 S) (I \u2212 S (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S) (I \u2212 S (I \u2212 S) (I \u2212"}, {"heading": "B.5 Proof of Lemma 3", "text": "Evidence that S (t) is the columns of C (t) and M (t) and M (t) and M (t) and M (t) and M (t) and T (t) and T (t) and T (t) and T (t) and T (t) and T (t) and T (t) and T (t) and T (t) and T (t) and T (t) and T (t), T) and T (t) and T (t), T (t) and T (t), T) and T (t), T) and T (t), T (t) and T (t), T (t) and T (t), T (t) and T (t), T (t) and T (t)."}, {"heading": "B.6 Lemma 12", "text": "Lemma 12: We now define the set T (v, v).We define the set T (v, v).We define the set Sv (v, v).We define the set Sv (v, v).We define the set Sv (v, v).We define the set Sv (v, v).We define the set Sv (v, v).We define the set Sv (v, v).We define the set Sv (u, v).We define it (v).We define it (v).We define the set Sv (v, v).We define the set Sv (v, v).We define it (v).We define it (v).We define it (v).We define it (v).We define it (.v).We define it (.v).We define it (.v).We define it (.v).We define it (.v (.v).We define it (.v).We define it (.v).(.v (.v).We define it (.v).v (.v (.v).We define it (.v).v (.v (.v).v (.v (.v).v (.v (.v).v (.v).v (.v (.v).v (.v (.v).v (.v).v (.v (.v).v (.v (.v).v (.v).v (.v (.v).v (.v (.v).v (.v (.v).v (.v).v (.v (.v).v (.v (.v).v (.v (.v).v (.v (.v).v (.v (.v).v (.v (.v).v (.v (.v (.v).v (.v).v (.v (.v (.v).v (.v).v (.v (.v (.v (.v).v (.v).v (.v (.v).v (.v (.v"}, {"heading": "B.7 Lemma 13", "text": "s say that the conditions of theorem 3. Let us designate the matrices M * (L *, N *, C *) so that they are limited to the columns that are not contained in S. Furthermore, let us limit the matrices M * (L *, N *, C *) to the columns that are not contained in S. Let us limit [U\\ S, \u03a3\\ S, V\\ S] = SVDr + 1 (M *) \u2212 PE (L * i + N * i)."}, {"heading": "B.8 Proof of Lemma 4", "text": "Prove that under the conditions of Theorem 3 the following three conditions apply with a probability of at least 1 \u00b0 C. # i (2 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (3 \u00b0 C) (3 \u00b0 C) (3 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C \u00b0 C) (4 \u00b0 C \u00b0 C) (4 \u00b0 C \u00b0 C) (4 \u00b0 C \u00b0 C) (4 \u00b0 C) (4 \u00b0 C \u00b0 C) (4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C \u00b0 (4 \u00b0 C) (4 \u00b0 4 \u00b0 C) (4 \u00b0 4 \u00b0 4 \u00b0 C) (4 \u00b0 4 \u00b0 C) (4 \u00b0 C) (4 \u00b0 C \u00b0 4 \u00b0 C) (4 \u00b0 C \u00b0 C) (4 \u00b0 C \u00b0 C) (4 \u00b0 C \u00b0 C \u00b0 C) (4 \u00b0 C \u00b0 C \u00b0 C) (4 \u00b0 C \u00b0 C \u00b0 (4 \u00b0 C \u00b0 C \u00b0 C \u00b0 C) (4 \u00b0 C \u00b0 C \u00b0 C) (4 \u00b0 C \u00b0 C \u00b0 C \u00b0 C \u00b0 C) (4 \u00b0 C \u00b0 C \u00b0 C \u00b0 (4 \u00b0 C \u00b0 C \u00b0 C \u00b0 C \u00b0 C \u00b0 C) (4 \u00b0 C \u00b0 C \u00b0 C \u00b0 C \u00b0) (4 \u00b0 C \u00b0 (4 \u00b0 C \u00b0 C \u00b0 C \u00b0 C \u00b0 C \u00b0) (4 \u00b0 C \u00b0 C \u00b0 C \u00b0) (4 \u00b0 C \u00b0 C \u00b0 C \u00b0 (4 \u00b0 C \u00b0 C \u00b0) (4 \u00b0 C \u00b0 C \u00b0 (4 \u00b0 C \u00b0 C \u00b0 C \u00b0 C \u00b0) (4 \u00b0 C \u00b0) (4 \u00b0 C \u00b0 C \u00b0 C \u00b0) (4 \u00b0 C \u00b0 C \u00b0 C \u00b0 (4 \u00b0 C \u00b0 C \u00b0 C \u00b0 C \u00b0) (4 \u00b0 C \u00b0 C \u00b0) (4 \u00b0 C \u00b0 C \u00b0 C \u00b0 C \u00b0 4 \u00b0 C \u00b0 4 \u00b0 C \u00b0 4 \u00b0 4 \u00b0) (4 \u00b0) (4 \u00b0 C \u00b0 C \u00b0 C \u00b0 C \u00b0 4 \u00b0 4 \u00b0 4 \u00b0 C \u00b0 4 \u00b0 C \u00b0 4"}, {"heading": "C Gaussian Noise: Proof of Theorem 3", "text": "For our decisions of c1 and c2 and n we have that # 1 (+ 1), # 2 (+ 1), # 2 (+ 2), # 3 (+ 2), # 3 (+ 2), # 4 (+ 2), # 4 (+ 2), # 4 (+ 2), # 4 (+ 2), # 4 (+ 2), # 4 (+ 2), # 4 (+ 2), # 4 (+ 2), # 4 (+ 2), # 4 (+ 2), # 4 (+ 2), # 4 (+ 2), # 4 (+ 2), # 4 (+ 2), # 4 (+ 2), # 4 (+ 2), # 5 (+ 2), # 2 (+ 2), # 2, # 2 (+ 2), # 2, \"."}, {"heading": "D TORP-BIN", "text": "In this section we propose an improvement to algorithm 2, which uses binary search instead of a linear scan in the outer iteration, thus improving the runtime of algorithm 2 by almost a factor r."}, {"heading": "D.1 Algorithm", "text": "In this section we present our algorithm (see algorithm 4) for OR-PCAN, which increases the runtime of algorithm 2 by almost a factor r. The most important finding is that the inner iteration of algorithm 2 is independent of the value of k in the outer iteration, except for the rank of the projection. In algorithm 4, we use the binary search on k instead of a linear scan, which reduces the number of outer iterations from O (r) to O (log r). Algorithm 4: Binary search based on TORP (TORP-BIN) 1: Input: Corrupted Matrix M: Corrupted Matrix M: Rd \u00d7 n, Target Rank r, Expressivity parameter, Number of inner iterations T: Minor search based on TORP (TORP-BIN) 1: Input: Corrupted Matrix M: Corrupted Rd \u00d7 n, Target Rank, Expressivity Parameter, Number of inner iterations T: Fraction Threshold, Number of Iterations (xT)."}, {"heading": "D.2 Analysis", "text": "In this section we will give a theoretical guarantee for algorithm 4.Theorem 7. Let us assume the conditions of theorem 2. Suppose that for all \u03b1 \u2264 1128\u00b52r, algorithm 4 is returned when executed with the parameters \u03c1 = 1128\u00b52r, \u03b7 N \u0445 F \u2264 \u03c3k (L \u0445) n and T = log 20n\u03c31 (M) n, a subspace U is returned that is satisfactory:. We start by limiting the runtime of the algorithm. Note that due to the binary search, the algorithm does not avail itself of outer iterations for at most O (log r).Let us accept the number of outer iterations. Let us not accept the number of outer iterations. Let us not accept the value of the algorithm (Let us take advantage of the value of the algorithm).Let us accept the number of outer iterations. Let us not accept the number of the iteration.Let us accept the number of the number of the number of the number of the iteration.Let us accept the number of the number of the number of the number of the algorithm."}, {"heading": "E Fast Projection Operator", "text": "In this section we will describe a fast algorithm to calculate the projection operator on the ellipsoid in algorithm 2. Formally, we get an orthogonal base U-Rd \u00b7 r, a positive diagonal matrix \u03a3, a boundary b and a vector x. Let E = {y: y = U\u0109z for a little more than z-Rd-b}. The goal is to calculate the projection of the vector x on the set E."}, {"heading": "E.1 Algorithm", "text": "In this section we present our algorithm (algorithm 5) for calculating the projection on the set E. We show that the projection operation amounts to a univariate optimization problem of a monotonous function. Afterwards, we perform a binary search at an interval in which the solution is guaranteed."}, {"heading": "E.2 Analysis", "text": "Theorem 8: Let U-Rd \u00b7 r be an orthonormal matrix, and we will be a positive diagonal matrix (max.). Then, for all b-Rd cases (max.) and for all x-Rd cases (max.). First, we define the convex optimization problem that corresponds to the projection of the operator PE. Then we have: PE (x) = argmin y-Rp-Rp-Rp-Rp-Rp-cases where E: = y-Rp-Rp-Rp-Rp-Rp-2, y-Rp-Rp-Rp-Rp-2, y-Rp-Rp-Rp-Rp-2, y-Rp-Rp-Rp-Rp-Rp-2, y-Rp-Rp-Rp-Rp-Rp-Rp-Rp-case."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We consider the problem of outlier robust PCA (OR-PCA) where the goal is to recover principal<lb>directions despite the presence of outlier data points. That is, given a data matrix M\u2217, where (1 \u2212 \u03b1)<lb>fraction of the points are noisy samples from a low-dimensional subspace while \u03b1 fraction of the points can<lb>be arbitrary outliers, the goal is to recover the subspace accurately. Existing results for OR-PCA have<lb>serious drawbacks: while some results are quite weak in the presence of noise, other results have runtime<lb>quadratic in dimension, rendering them impractical for large scale applications.<lb>In this work, we provide a novel thresholding based iterative algorithm with per-iteration complexity<lb>at most linear in the data size. Moreover, the fraction of outliers, \u03b1, that our method can handle is tight<lb>up to constants while providing nearly optimal computational complexity for a general noise setting. For<lb>the special case where the inliers are obtained from a low-dimensional subspace with additive Gaussian<lb>noise, we show that a modification of our thresholding based method leads to significant improvement<lb>in recovery error (of the subspace) even in the presence of a large fraction of outliers.", "creator": "LaTeX with hyperref package"}}}