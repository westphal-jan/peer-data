{"id": "1702.05624", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2017", "title": "Reproducing and learning new algebraic operations on word embeddings using genetic programming", "abstract": "Word-vector representations associate a high dimensional real-vector to every word from a corpus. Recently, neural-network based methods have been proposed for learning this representation from large corpora. This type of word-to-vector embedding is able to keep, in the learned vector space, some of the syntactic and semantic relationships present in the original word corpus. This, in turn, serves to address different types of language classification tasks by doing algebraic operations defined on the vectors. The general practice is to assume that the semantic relationships between the words can be inferred by the application of a-priori specified algebraic operations. Our general goal in this paper is to show that it is possible to learn methods for word composition in semantic spaces. Instead of expressing the compositional method as an algebraic operation, we will encode it as a program, which can be linear, nonlinear, or involve more intricate expressions. More remarkably, this program will be evolved from a set of initial random programs by means of genetic programming (GP). We show that our method is able to reproduce the same behavior as human-designed algebraic operators. Using a word analogy task as benchmark, we also show that GP-generated programs are able to obtain accuracy values above those produced by the commonly used human-designed rule for algebraic manipulation of word vectors. Finally, we show the robustness of our approach by executing the evolved programs on the word2vec GoogleNews vectors, learned over 3 billion running words, and assessing their accuracy in the same word analogy task.", "histories": [["v1", "Sat, 18 Feb 2017 15:29:01 GMT  (35kb)", "http://arxiv.org/abs/1702.05624v1", "17 pages, 7 tables, 8 figures. Python code available fromthis https URL"]], "COMMENTS": "17 pages, 7 tables, 8 figures. Python code available fromthis https URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["roberto santana"], "accepted": false, "id": "1702.05624"}, "pdf": {"name": "1702.05624.pdf", "metadata": {"source": "CRF", "title": "Reproducing and learning new algebraic operations on word embeddings using genetic programming", "authors": ["Roberto Santana"], "emails": ["roberto.santana@ehu.es"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.05 624v 1 [cs.C L] 1Word vector representations associate a high-dimensional real vector with every word of a corpus. Recently, neural network-based methods have been proposed to learn this representation of large corpus. This type of word-to-vector embedding is capable of preserving in the learned vector space some of the syntactic and semantic relationships that exist in the original word corpus. This, in turn, serves to address various types of language classification tasks by performing algebraic operations defined on the vectors.The general practice is to assume that the semantic relations between the words can be derived through the application of a-priori specified algebraic operations.Our general goal in this essay is to show that it is possible to learn methods for word composition in semantic spaces.Instead of linear, the compositional method as an algebraic operation, we will be able to express it as a GP rather than a linear generic program, or they will be generated as a GP."}, {"heading": "1 Introduction", "text": "In fact, it is as if most people who are able to identify themselves are not able to identify themselves. (...) It is not as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves. (...) It is as if they are able to identify themselves."}, {"heading": "2 Vector-based representations of words", "text": "In this section we will briefly discuss some of the foundations on which our work is based. We will discuss semantic spaces and the approach that generates word embedding using flat neural networks."}, {"heading": "2.1 Semantic spaces", "text": "In semantic spaces, words are represented as vectors of real numbers, all vectors with the same dimension. Alternatively, we will use the terms \"word vectors\" or \"embedding\" to refer to the mapping between words and vectors. To organize our analysis, we will consider two key questions in semantic spaces: i) The possible compositional relationships between the word vectors. ii) The methods used to learn the representations. Compositional models are designed to capture the semantics of a multiword construction from the semantics of its constituents. The underlying idea is that the vector representations of two or more words could be transformed to obtain the representation of the multiword constructions."}, {"heading": "2.2 Learning word embeddings using neural networks", "text": "In [16], two models based on neural networks were proposed to learn embedding: Skip-gram and Continuous Bags of Words (CBOW) models. Skip-gram learns to predict the surrounding words of a given word in a sentence. CBOW learns to predict the word most likely to be in the middle given the surrounding words. We focus on the CBOW model. CBOW is a forward-looking neural network language model [1] with a number of additional changes. The main difference is that the hidden layer has been removed. The reason behind this modification was to explore simpler models. They cannot represent the nonlinear interactions that neural networks with hidden layers can do, but they are much more efficient to learn from millions of words. The CBOW network also uses a Huffman binary tree for a more efficient representation of the word vocabulary and a hierarchical softCmax-1 scheme shows BOW schematic representation."}, {"heading": "2.3 Generation of the embeddings", "text": "To generate the embeddings we are working with in this thesis, we have used text8.zip corpus1. This corpus was extracted from Wikipedia2. It comprises 71291 words.We are using the original word2vec implementation3 by Mikolov et al. [18, 16] to train the CBOW network from the corpus and generate embedings.The parameters used by the word2vec program to generate the embedding3 are in Table 1.The CBOW is generated only once, in relation to the GP implementation, the most important parameter being the vector size. A larger vector size can allow a more accurate representation of the words. However, the vector size also influences the calculation costs of algebraic operations between the words that are used intensively, while GP is looking for an optimal way to compose the words. In order to optimize the scalability and robustness of the words developed by GP, we are also looking for a way to intensively evaluate the method of using the word between a GP, which is much more robust."}, {"heading": "3 Genetic programming", "text": "In fact, it is the case that most of us will be able to play by the rules that they have established in recent years, and that they will be able to play by the rules, \"he said.\" It is not as if they play by the rules. \"He pointed out that the EU Commission is able to play by the rules of the EU treaties,\" but it is not as if it plays by the rules of the EU. \""}, {"heading": "4 Related work", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "6 Description of the GP approach", "text": "Automatic learning of the composition of words is possible in the specific problem that we use, and the GP task, given the vector representations of three words that define a question, is to produce a vector whose narrowest word in the corpus is one that answers the question correctly. We will mainly use the CBOW model that we learned with word2vec to determine which word vector is the model that encodes a particular word, or to find which word is the model whose coding vector is closest to a target word vector. The pseudocode of the GP algorithm we used is shown in Algorithm 1. It is an easy implementation of tree-based genetic programming. The selection method is truncation selection, selecting individuals according to their fitness, the best 100 solutions are kept for crossover and mutation. Uniform mutation of the mutation of the interaction between the individual genome trees and the random substitution of a tree."}, {"heading": "6.1 GP operators", "text": "All operators are defined on vectors of the same dimension. There are two classes of operators: binary and simple. Operators +, \u2212 and \u0445 have the following meanings: vector addition, vector subtraction or vector component-by-component multiplication; while% corresponds to a protected division (usual division, except that a division by zero results in zero for one of the vector components). We have rejected the possibility of including fixed (vector) random constants as terminals of the programs, as they can depend on the size of the vector, and our goal was to produce programs that are scalable to any vector dimension. To reduce the complexity of the programs, we have set a constraint d = 10 on the depth of the trees."}, {"heading": "6.2 Fitness function", "text": "In fact, most of us will be able to find ourselves in the position in which we find ourselves."}, {"heading": "7 Experiments", "text": "The main objective of the experiments is to determine the quality of the programs created by GP. We will compare their results for the word analogy task with those obtained by applying the linear algebraic rule ~ d = ~ c \u2212 ~ a + ~ b, which is commonly used for the composition of the words for this problem. In addition, we will evaluate the transferability of the best programs by applying them to a vector space comprising 3 \u00d7 106 vector spaces, approximately 100 times larger than the vector space in which we learned the programs. For each fitness function and each group of questions of the algorithms described in Table 3, 30 independent runs were performed. In total, 9 x 30 = 270 runs were performed. Each group of questions was divided into a training and test set with the same number of questions."}, {"heading": "7.1 Numerical results", "text": "In fact, the fact is that most of us will be able to feel as if they were able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be."}, {"heading": "7.2 Evaluating answers and evolved programs", "text": "Of the 270 programs tested, 8 corresponded to the algebraic rule. Four of these programs are shown in Figure 2. You can see how the same rule is implemented in different ways by adding only the operators, sub- and neg. These results show that GP, as an algorithm for creating word compositions, can automatically learn compositional methods designed by humans. We also analyzed those GP programs that exceeded the algebraic rule. An example of this type of program is shown in Figure 3. It was the best program found for the group of questions. Its accuracy using the word2vec GoogleNews vectors was 74.17, above the 72.75 accuracy of the algebraic rule for the same group of questions.The tree shown in Figure 3 is a slight modification of the algebraic rule. Instead of adding ARG2 to the rule, this program adds 54 G2 and increases the accuracy of the trend values contained in these programs."}, {"heading": "7.3 Discussion", "text": "We return to the research questions raised at the beginning of this paper and try to answer them on the basis of the results of our experiments. \u2022 Can one learn meaningful vector algebraic operations from training examples for embedding representations? Yes, they can be learned in a relatively short amount of time. \u2022 If so, is general medicine a viable approach to this? Yes, general medicine is a natural solution to this type of problem, and even simple implementations can deal with the problem. \u2022 How do GP programs fare in terms of the algebraic rule often applied to vector representations? GP programs can learn the same human-designed rule and therefore come to the same results. They can also exceed these results, but at least for the class of word vector representation and the basic tree-based GP approach, the improvements are moderate. \u2022 Are GP-developed programs transferable via linguistic tasks, vector representations and corpora? Definitely. The high transferability of the groups over these general questions can be increased by the generality of the two generalities."}, {"heading": "8 Conclusions and future work", "text": "While semantic spaces and word vector representations are able to capture some of the semantic relationships between words, compositional methods are necessary to extend their use to multi-word constructions. In this paper, we have proposed presenting compositional vector operations as simple programs that can be learned automatically from data. We have shown that with the help of GP it is possible to encode a series of vector operations as a program, that the programs can be further developed to achieve greater accuracy than the human rules designed to manipulate the words, and that the programs are valid for data sets other than those from which they were learned, i.e. they are portable programs. Furthermore, our results suggest that it is possible to learn programs with vector vocabulary small to moderate sizes and then test them in larger areas where evaluating a program is more costly."}, {"heading": "8.1 Future work", "text": "As guidelines for the future work we consider the following:"}, {"heading": "8.1.1 Use alternative methods for the word vector generation", "text": "While GP approaches can explore a wide range of possible word compositions, the usefulness of more complicated programs is limited to a certain extent by the type of relationships that vectors can encode. For example, if the methods used to construct embeddings do not allow non-linear relationships between vectors, improvements in GP programs are marginal compared to purely linear algebraic composition technicians. Therefore, it would be important to test the automatic generation of word compositions with GP on word vectors generated by different methods."}, {"heading": "8.1.2 Evolve functions for the similarity metric", "text": "Since it has been shown that the type of similarity metric can significantly influence the accuracy results [15], it makes sense to learn this function as well. A difficulty is that the output of this function will be a numerical value, not a vector, like the other operators used in the current GP representation. Furthermore, the evaluation of an alternative similarity metric implies the use of candidate metrics to calculate distances to all vectors in the vector space, a process that can be very expensive from a computational point of view."}, {"heading": "8.1.3 Combining different word representations", "text": "Turian et al. [30] have shown that combining different word representations can improve accuracy for supervised classification tasks in NLP [30]. We envisage the development of programs that are able to combine different word vector representations."}, {"heading": "8.1.4 Using more sophisticated GP approaches", "text": "From the perspective of research in the field of genetic programming, word embedding opens up an interesting line of research. Further research is needed to determine which of the more complex GP approaches is most suitable for its application to semantic spaces. \u2022 Possible lines of research include: \u2022 Alternative GP representations: In addition to trees, other GP representations such as grammars [23, 4] and Cartesian GP [19] could be considered. \u2022 More complex descriptions of compositional operators: An open question is to what extent more complex functions can better exploit the underlying semantic relationships between word vectors. This could be investigated by adding other algebraic operators to the group of GP functions, including ternary operators. \u2022 Another possibility is to represent the composition of vectors with ensembles of GP programs [2]. \u2022 Reuse of problem information: Approaches that are able to improve building blocks [10] between word vectors or corpora of different portions of potential candidates by transferring them to the direction of the drug program as a behavioral program."}, {"heading": "Acknowledgments", "text": "This work was supported by the IT-609-13 programme (Basque Government), TIN2016-78365-R (Spanish Ministry of Economy, Industry and Competitiveness) and the Brazilian CNPq programme Science without Borders No: 400125 / 2014-5."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "Journal of machine learning research, 3(Feb):1137\u20131155,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Evolving diverse ensembles using genetic programming for classification with unbalanced data", "author": ["U. Bhowan", "M. Johnston", "M. Zhang", "X. Yao"], "venue": "IEEE Transactions on Evolutionary Computation, 17(3):368\u2013386,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["W. Blacoe andM. Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Learning probabilistic tree grammars for genetic programming", "author": ["P.A. Bosman", "E.D. De Jong"], "venue": "International Conference on Parallel Problem Solving from Nature, pages 192\u2013201. Springer,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "An analysis of the solution space for genetically programmed term-weighting schemes in information retrieval", "author": ["R. Cummins", "C. O\u2019Riordan"], "venue": "editor, 17th Artificial Intelligence and Cognitive Science Conference (AICS", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Mart\u0131\u0301nez-Carranza. Term-weighting learning via genetic programming for text classification", "author": ["H.J. Escalante", "M.A. Garc\u0131\u0301a-Lim\u00f3n", "A. Morales-Reyes", "M. Graff", "M. Montes-y G\u00f3mez", "E.F. Morales"], "venue": "Knowledge-Based Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "DEAP: Evolutionary algorithms made easy", "author": ["F.-A. Fortin", "D. Rainville", "M.-A.G. Gardner", "M. Parizeau", "C. Gagn\u00e9"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["E. Grefenstette", "M. Sadrzadeh"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1394\u20131404. Association for Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Reusing building blocks of extracted knowledge to solve complex, large-scale Boolean problems", "author": ["M. Iqbal", "W. Browne", "M. Zhang"], "venue": "Evolutionary Computation, IEEE Transactions on, 18(4):465\u2013480, Aug", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["M. Iyyer", "J.L. Boyd-Graber", "L.M.B. Claudino", "R. Socher", "H. Daum\u00e9 III"], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 633\u2013644,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Predication", "author": ["W. Kintsch"], "venue": "Cognitive science, 25(2):173\u2013202,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Genetic Programming: On the Programming of Computers by Means of Natural Selection", "author": ["J.R. Koza"], "venue": "The MIT Press, Cambridge, MA,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1992}, {"title": "Behavioral program synthesis: Insights and prospects", "author": ["K. Krawiec", "J. Swan", "U.-M. OReilly"], "venue": "Genetic Programming Theory and Practice XIII, pages 169\u2013183. Springer,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["O. Levy", "Y. Goldberg"], "venue": "Proceedings of the Eighteenth Conference on Computational Language Learning, pages 171\u2013180,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, abs/1301.3781,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever"], "venue": "CoRR, abs/1309.4168,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Cartesian genetic programming", "author": ["J.F. Miller", "P. Thomson"], "venue": "European Conference on Genetic Programming, pages 121\u2013132. Springer,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2000}, {"title": "Composition in distributional models of semantics", "author": ["J. Mitchell", "M. Lapata"], "venue": "Cognitive science, 34(8):1388\u20131429,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Strongly typed genetic programming", "author": ["D.J. Montana"], "venue": "Evolutionary computation, 3(2):199\u2013 230,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Improving the effectiveness of information retrieval with genetic programming", "author": ["N. Oren"], "venue": "Master\u2019s thesis, Faculty of Science of the University of Witwatersrand, Johannesburg,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Grammatical evolution", "author": ["M. ONeil", "C. Ryan"], "venue": "Grammatical Evolution, pages 33\u201347. Springer,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing (EMNLP), volume 14, pages 1532\u2013 1543,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "A field guide to genetic programming", "author": ["R. Poli", "W.B. Langdon", "N.F. McPhee", "J.R. Koza"], "venue": "Lulu.com,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Software framework for topic modelling with large corpora", "author": ["R. \u0158eh\u016f\u0159ek", "P. Sojka"], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta, May", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["R. Socher", "E.H. Huang", "J. Pennington", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of the 2011 Conference Advances in Neural Information Processing Systems 24, NIPS, volume 24, pages 801\u2013809,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of the conference on empirical methods in natural language processing, pages 151\u2013161. Association for Computational Linguistics,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to rank", "author": ["A. Trotman"], "venue": "Information Retrieval, 8(3):359\u2013381,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394. Association for Computational Linguistics,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Similarity of semantic relations", "author": ["P.D. Turney"], "venue": "Computational Linguistics, 32(3):379\u2013416,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Combining heterogeneous models for measuring relational similarity", "author": ["A. Zhila", "W.-t. Yih", "C. Meek", "G. Zweig", "T. Mikolov"], "venue": "In Proceedings of the 2013 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}], "referenceMentions": [{"referenceID": 29, "context": "In particular, machine learning methods that use this representation have been proposed for named entity recognition [30], question answering [11], machine translation [17], etc.", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "In particular, machine learning methods that use this representation have been proposed for named entity recognition [30], question answering [11], machine translation [17], etc.", "startOffset": 142, "endOffset": 146}, {"referenceID": 16, "context": "In particular, machine learning methods that use this representation have been proposed for named entity recognition [30], question answering [11], machine translation [17], etc.", "startOffset": 168, "endOffset": 172}, {"referenceID": 30, "context": "Another convenient feature of vector word spaces is that the word vectors are able to capture attributional similarities [31] between words.", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "For instance, these regularities can be manifested as constant vector offsets between pairs of words sharing a particular relationship [18, 16].", "startOffset": 135, "endOffset": 143}, {"referenceID": 15, "context": "For instance, these regularities can be manifested as constant vector offsets between pairs of words sharing a particular relationship [18, 16].", "startOffset": 135, "endOffset": 143}, {"referenceID": 14, "context": "It has been suggested that the linguistic regularities that vector representations produced by neural networks exhibit are not a consequence of the embedding process itself, but are well preserved by it however [15].", "startOffset": 211, "endOffset": 215}, {"referenceID": 14, "context": "This seems confirmed by the fact that for other types of vector representations linear algebraic operations can also produce meaningful results [15, 24].", "startOffset": 144, "endOffset": 152}, {"referenceID": 23, "context": "This seems confirmed by the fact that for other types of vector representations linear algebraic operations can also produce meaningful results [15, 24].", "startOffset": 144, "endOffset": 152}, {"referenceID": 12, "context": "In this context, genetic programming (GP) [13] arises as a natural candidate.", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "We frame the general question of finding a suitable transformation of word vectors on the more specific word analogy task [16, 24].", "startOffset": 122, "endOffset": 130}, {"referenceID": 23, "context": "We frame the general question of finding a suitable transformation of word vectors on the more specific word analogy task [16, 24].", "startOffset": 122, "endOffset": 130}, {"referenceID": 19, "context": "To formalize the analysis of methods for word composition, Mitchell and Lapata [20] define p = f(u, v,R,K) as the composition of vectors u and v.", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "In [20], additive models such as the one represented by Eq.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "Other compositional models that consider contextual information (neighboring words) have been also proposed [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 4, "context": "Neural networks have been applied for implementing the latter approach [5, 32].", "startOffset": 71, "endOffset": 78}, {"referenceID": 31, "context": "Neural networks have been applied for implementing the latter approach [5, 32].", "startOffset": 71, "endOffset": 78}, {"referenceID": 15, "context": "In this paper we have used vectors learned by the application of shallow neural networks as proposed in [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "In [16], two neural-network based models have been proposed to learn embeddings: Skip-gram and Continuous Bags of words (CBOW) models.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "CBOW is a feed-forward neural net language model [1] with a number of added changes.", "startOffset": 49, "endOffset": 52}, {"referenceID": 15, "context": "Figure 1: Continuous Bag-of-Words (CBOW) model as proposed in [16].", "startOffset": 62, "endOffset": 66}, {"referenceID": 15, "context": "Figure 1 shows a schematic representation of the CBOW architecture [16].", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "In the results reported in [16], the best results were obtained using k = 4.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "We use the original word2vec implementation of Mikolov et al [18, 16] to train the CBOW network from the corpus and generate embeddings.", "startOffset": 61, "endOffset": 69}, {"referenceID": 15, "context": "We use the original word2vec implementation of Mikolov et al [18, 16] to train the CBOW network from the corpus and generate embeddings.", "startOffset": 61, "endOffset": 69}, {"referenceID": 12, "context": "Genetic programming [13, 25] is a domain-independent method for the automatic creation of programs that solve a given problem.", "startOffset": 20, "endOffset": 28}, {"referenceID": 24, "context": "Genetic programming [13, 25] is a domain-independent method for the automatic creation of programs that solve a given problem.", "startOffset": 20, "endOffset": 28}, {"referenceID": 14, "context": "Levy and Goldberg [15] investigate the question of how to recover the relational similarities in word embeddings.", "startOffset": 18, "endOffset": 22}, {"referenceID": 15, "context": "[16] to solve analogy recovery is equivalent to searching for a word that maximizes a linear combination of three word similarities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "While our research is related to the work presented in [15], our focus is on the operations involved in the generation of the candidate word vector, not on the way the match between the generated vector and the word vectors in the corpus is assessed.", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "Pennington et al [24] introduce global log-bilinear regression models as an alternative to shallow neural-networks to produce word embeddings.", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "The algebraic rule they use to solve the word analogy task is the same as that originally introduced in [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "Although they applied the distance measure previously presented by Levy and Goldberg [15], they report that this distance did not produce better results than the original one.", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "The work presented in [24] is relevant for our research since it confirms that the usability of the word vector algebraic rule extends over vector representations obtained using a variety of model types and algorithms.", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "Socher et al [27, 28] propose a method for compositional representation of words that learns a binary parse tree for each input phrase or sentence.", "startOffset": 13, "endOffset": 21}, {"referenceID": 27, "context": "Socher et al [27, 28] propose a method for compositional representation of words that learns a binary parse tree for each input phrase or sentence.", "startOffset": 13, "endOffset": 21}, {"referenceID": 8, "context": "Grefenstette et al [9] propose associating different levels of meaning for words with different types of representations.", "startOffset": 19, "endOffset": 22}, {"referenceID": 20, "context": "In principle, GP approaches could cater for joint use of vector and matrix representation by means of strongly typed GP [21] or other GP variants that guarantee type constraint enforcement.", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "In [3], three word vector representations and three compositional methods (addition of word vectors, multiplication of vectors, and the aforementioned deep recursive autoencoder approach) are combined to evaluate their applicability to estimate phrase similarity and paraphrase detection (i.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "However, GP has been applied to a miscellany of tasks in information retrieval [6, 7, 22, 29].", "startOffset": 79, "endOffset": 93}, {"referenceID": 6, "context": "However, GP has been applied to a miscellany of tasks in information retrieval [6, 7, 22, 29].", "startOffset": 79, "endOffset": 93}, {"referenceID": 21, "context": "However, GP has been applied to a miscellany of tasks in information retrieval [6, 7, 22, 29].", "startOffset": 79, "endOffset": 93}, {"referenceID": 28, "context": "However, GP has been applied to a miscellany of tasks in information retrieval [6, 7, 22, 29].", "startOffset": 79, "endOffset": 93}, {"referenceID": 21, "context": "In particular, Oren [22] combines GP with vector-based representation of documents for information retrieval.", "startOffset": 20, "endOffset": 24}, {"referenceID": 28, "context": "Two related areas where GP has been applied are document ranking [29] and term-weighting learning [6, 7].", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "Two related areas where GP has been applied are document ranking [29] and term-weighting learning [6, 7].", "startOffset": 98, "endOffset": 104}, {"referenceID": 6, "context": "Two related areas where GP has been applied are document ranking [29] and term-weighting learning [6, 7].", "startOffset": 98, "endOffset": 104}, {"referenceID": 17, "context": "[18] in which questions are separated into 13 groups.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "It is based on the EA software DEAP [8] and the gensim package, a Python-based implementation of NLP Those included in the DEAP library used to implement the algorithms http://deap.", "startOffset": 36, "endOffset": 39}, {"referenceID": 25, "context": "algorithms [26].", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "As a consequence, these vectors contain all words for the 13 original groups of questions introduced in [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "Since it has been shown that the type of similarity metric can critically influence the accuracy results [15], it makes sense to learn this function as well.", "startOffset": 105, "endOffset": 109}, {"referenceID": 29, "context": "Turian et al [30] have shown that combining different word representations can improve accuracy for supervised classification tasks in NLP [30].", "startOffset": 13, "endOffset": 17}, {"referenceID": 29, "context": "Turian et al [30] have shown that combining different word representations can improve accuracy for supervised classification tasks in NLP [30].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "\u2022 Alternative GP representations: In addition to trees, other GP representations such as grammars [23, 4] and Cartesian GP [19] could be considered.", "startOffset": 98, "endOffset": 105}, {"referenceID": 3, "context": "\u2022 Alternative GP representations: In addition to trees, other GP representations such as grammars [23, 4] and Cartesian GP [19] could be considered.", "startOffset": 98, "endOffset": 105}, {"referenceID": 18, "context": "\u2022 Alternative GP representations: In addition to trees, other GP representations such as grammars [23, 4] and Cartesian GP [19] could be considered.", "startOffset": 123, "endOffset": 127}, {"referenceID": 1, "context": "Another possibility is representing the composition of vectors with ensembles of GP programs [2].", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "\u2022 Reusing problem information: Approaches able to identify and transfer building blocks [10] between word vectors or corpora of varying dimensions arise as potential candidates.", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "In general, algorithms that advocate a more efficient use of the information displayed by the behavior of the GP programs [14] could lead to better solutions and reveal additional insights in learning compositional methods.", "startOffset": 122, "endOffset": 126}], "year": 2017, "abstractText": "Word-vector representations associate a high dimensional real-vector to every word from a corpus. Recently, neural-network based methods have been proposed for learning this representation from large corpora. This type of word-to-vector embedding is able to keep, in the learned vector space, some of the syntactic and semantic relationships present in the original word corpus. This, in turn, serves to address different types of language classification tasks by doing algebraic operations defined on the vectors. The general practice is to assume that the semantic relationships between the words can be inferred by the application of a-priori specified algebraic operations. Our general goal in this paper is to show that it is possible to learn methods for word composition in semantic spaces. Instead of expressing the compositional method as an algebraic operation, we will encode it as a program, which can be linear, nonlinear, or involve more intricate expressions. More remarkably, this program will be evolved from a set of initial random programs by means of genetic programming (GP). We show that our method is able to reproduce the same behavior as human-designed algebraic operators. Using a word analogy task as benchmark, we also show that GP-generated programs are able to obtain accuracy values above those produced by the commonly used human-designed rule for algebraic manipulation of word vectors. Finally, we show the robustness of our approach by executing the evolved programs on the word2vec GoogleNews vectors, learned over 3 billion runningwords, and assessing their accuracy in the same word analogy task. keywords: semantic spaces, compositional methods, word2vec, genetic programming, word vectors", "creator": "LaTeX with hyperref package"}}}