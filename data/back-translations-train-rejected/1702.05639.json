{"id": "1702.05639", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2017", "title": "Deep Stochastic Configuration Networks with Universal Approximation Property", "abstract": "This paper focuses on the development of randomized approaches for building deep neural networks. A supervisory mechanism is proposed to constrain the random assignment of the hidden parameters (i.e., all biases and weights within the hidden layers). Full-rank oriented criterion is suggested and utilized as a termination condition to determine the number of nodes for each hidden layer, and a pre-defined error tolerance is used as a global indicator to decide the depth of the learner model. The read-out weights attached with all direct links from each hidden layer to the output layer are incrementally evaluated by the least squares method. Such a class of randomized leaner models with deep architecture is termed as deep stochastic configuration networks (DeepSCNs), of which the universal approximation property is verified with rigorous proof. Given abundant samples from a continuous distribution, DeepSCNs can speedily produce a learning representation, that is, a collection of random basis functions with the cascaded inputs together with the read-out weights. Simulation results with comparisons on function approximation align with the theoretical findings.", "histories": [["v1", "Sat, 18 Feb 2017 18:18:32 GMT  (289kb)", "http://arxiv.org/abs/1702.05639v1", "Manuscript submitted to IJCAI17 on Feb. 19, 2017"], ["v2", "Sat, 20 May 2017 02:38:56 GMT  (290kb)", "http://arxiv.org/abs/1702.05639v2", "Manuscript submitted to NIPS on May 19, 2017"]], "COMMENTS": "Manuscript submitted to IJCAI17 on Feb. 19, 2017", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["dianhui wang", "ming li"], "accepted": false, "id": "1702.05639"}, "pdf": {"name": "1702.05639.pdf", "metadata": {"source": "CRF", "title": "Deep Stochastic Configuration Networks: Universal Approximation and Learning Representation", "authors": ["Dianhui Wang", "Ming Li"], "emails": ["dh.wang@latrobe.edu.au"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.05 639v 1 [cs.L G] 18 Feb 20"}, {"heading": "1 Introduction", "text": "In fact, it is. (...) It is. (...) It is. (...) It is. (...) Most of us have not managed to retaliate. (...) Most of us have not managed to retaliate. (...) Most of us have not managed to retaliate. (...) Most of us have done it. (...) Most of us have done it. (...) Most of us have not managed to retaliate. (...) Most of us have done it. (...) Most of us have done it. (...). (...) We have done it. (...) We have done it. (...) We have done it. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (... It is. (...) It is. (...) It is. \"It is. (...\" It is. \"It is. (...\" It is. \"It is.\" It is. (... \"It is.\" It. \"It is.\" It. \"It is. (...\" It is. \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It.\" It. \"It. (...\" It. \"It.\" It. \""}, {"heading": "2 Related Work", "text": "Some researchers expected the usefulness of randomization in the development of neural networks [29]. Indeed, it was found that a random filter in conventional neural networks performs slightly worse than a well-designed filter, which normally requires advance planning and discriminatory coordination. These experimental results have good potential to improve the computing power of conventional systems without significantly degrading performance."}, {"heading": "3 Deep Stochastic Configuration Networks", "text": "This section provides some basics of DeepSCNs, including a theoretical result on the universal approximation property, interpretation of the learning representation, description of the algorithm and some technical notes."}, {"heading": "3.1 Universal Approximation Property", "text": "Allow L2 (D) the space of all Lebesgue-measurable vector-weighted functions F = [f1, f2,.., fm]: R d \u2192 Rm on a compact quantity D-Rd, using the L2 standard, which is defined as a hidden function: = (m \u00b2 q = 1, fq (x) | 2dx) 1 / 2 < (1) and inner product, which is called < F >: = m \u00b2 q = 1 < fq, gq > = m \u00b2 q = 1, Dfq (x) dx, (2), where G = [g1, g2,.,., gm]: R d \u2192 Rm.Given a target function F: Rd \u2192 Rm, assume a DeepSCN with n hidden layers and each layer has hidden nodes (k = 1,.,."}, {"heading": "3.2 Learning Representation", "text": "In recent decades, many exciting research and advances have been reported in the literature [12, 23], which attempts to fulfill the following three characteristics: fidelity, conciseness, and interpretability. Learning models may come in various forms, but must essentially share the universal adaptation characteristic in order to achieve fidelity. Theory 1 above ensures that DeepSCNs meet the essential requirement for learning representation well. Due to the specific architecture of DeepSCNs, learning representation becomes fairly simple and comprehensible. Results from the first hidden layer contain two parts: a series of random basic functions with the original inputs and associated read weights between the first hidden layer and the output layer. Apparently, one can define a learning representation from the first hidden layer (actually an SCN model), which characterizes the main feature of the target function."}, {"heading": "3.3 Algorithm Description", "text": "A training dataset with input X = {x1, x2,.., xN}, xi = [xi, 1.,., xi, d] T-Rd and output T = {t1, t2,.., tN}, where ti = [ti, 1., ti, m] T-Rm, i = 1.,., N. Denote E (n) Ln \u2212 1 = E (n) Ln \u2212 1 (X) = [E (n) Ln \u2212 1,1 (X),.,., E (n) Ln \u2212 n new hidden node is added to the n-th layer, where E (n) Ln \u2212 1, q (X) (X) = [E (ltLn) Ln \u2212 h, q \u2212 xi \u2212 n \u2212 n,."}, {"heading": "3.4 Further Remark", "text": "Essentially, the universal approximation property ensures the ability of DeepSCNs for both data modeling and signal representation. Therefore, DeepSCNs can be used either as predictive models or as feature extractors in domain applications. In case of regression problems, one may be more interested in the predictability of a learning model than in its ability to learn. Unfortunately, it is almost impossible to directly establish a specific correlation between these two performances, which means that better learning performance does not always imply a solid generalization. In this respect, consistency concept becomes a meaningful measure of the quality of learning machines. In our practical experience, DeepSCNs show a very good consistency performance, and it is assumed that these merits are linked both to the proposed full-blown terminal criterion and to the supervisory mechanism for assigning random hidden parameters. In classification problems, DeepSCNs can create sample sample sample data directly to CNs, so that we can use it as a class discriminator for CNS characteristics."}, {"heading": "4 Empirical Demonstration", "text": "In this section, some simulation results are presented to illustrate the advantages of DeepSCNs over SCNs."}, {"heading": "4.1 Effectiveness and Consistency", "text": "Figure 2 shows the training or test performance, with L = 200 for SCN, M = 4 and L (n) max = 50 for each layer in DeepSCN. It is clear that DeepSCN approaches the target function faster than SCN within a given tolerance, and the consistency relationship between learning and generalization can be observed. In fact, both flat and deep SCNs share this beautiful consistency characteristic. It should be clearly noted that our statement on consistency comes from a large number of experimental observations and theoretical justifications are expected. Figure 2 shows three independent studies suggesting that DeepSCN outperforms SCN in terms of effectiveness. In this test, we use rk = 1 \u2212 10 \u2212 k, k = 1, 2,..., 7 in step 16 of the DSCN algorithm. With different settings, performance could be further verified in terms of both effectiveness and consistency by the results in Figure 5."}, {"heading": "4.2 Signal Representation", "text": "From a mathematical point of view, both SCN and DeepSCN can generate a number of random base functions, the difference between which lies in the way in which the random base functions are generated that relate to the input variables and random parameters of learning models. It is difficult to quantify the quality of learning representations. In this thesis, we view a learning representation positively when the distribution of the read weights in (14) has solid statistical features, such as lower expectations and standard deviations. Figure 3 shows two normalized distributions of the read weights (converted to [-1.1] based on 200 coefficients from SCN and DeepSCN) and displays the approximation results for the test data set next to the distributions. As can be seen, DeepSCN has a number of concentrated read-out weights and performs much better in generalization than SCN, which actually has a fairly poor distribution of the weights with these few peaks."}, {"heading": "4.3 Model Capacity versus Rank Deficiency", "text": "In common sense, the capacity of a learner model can be measured by using a trade-off metric of learning and generalization performance. In addition, we can evaluate the capacity of a learner model by looking at its interpretability. In fact, it is always desirable to have a learner model that balances learning, generalization, and model complexity. For DeepSCN, due to its specific configuration in modeling, it is very interesting and useful to link model capacity not only with the number of nodes for each hidden layer, but also closely with some algebraic properties of the hidden output matrix on each hidden layer. Technically, it is very interesting and useful to look at the relationship between the rank deficiency of the hidden output matrix and the generalization capability of the model. To do this, the degree of rank deficiency is calculated by p = rank (H) or definition node = 1 output matrix, where the existing Lx is calculated from the output matrix."}, {"heading": "4.4 Robustness Analysis", "text": "In general, robustness refers to the ability of a system to maintain its performance while being exposed to external input, changes in internal structure and / or shifts in parameter setting. Obviously, the quality or performance of DeepSCNs depends on parameter setting. To investigate the robustness of the DSCN algorithm, we limit our study of the learning parameter r to only 11 values. A similar robustness analysis on another set of important learning parameters is not reported here. Figure 5 shows the training and testing performance for both SCN and DeepSCN with three different settings of r, that is, randomly extracting 10 real numbers from the open interval (0.9, 0.99) and arranging them in increasing order to form a sentence r1, and specifying the sentence r = {r1, 1 \u2212 10 \u2212 6}."}, {"heading": "5 Conclusions", "text": "Although much empirical evidence shows that deep neural networks have great potential for displaying learning, it is still blind to end users to deploy the network architecture in such a way that the resulting deep learning model has sufficient capacity to approximate signals in some way. Apart from concerns about architecture, there is much interest in fast learning algorithms. In this paper, we would like to summarize our technical contributions to the field as follows: \u2022 The supervisory mechanism characterized by the inequalities (5) and (7) is key to the development of the DeepSCN framework originally proposed in [32] and could be used as a unique feature to differentiate our SCN framework from other randomized learning models. \u2022 The scope of random parameters at each level of DeepSCN could be adaptively updated."}], "references": [{"title": "Learning the number of neurons in deep networks", "author": ["J. Alvarez", "M. Salzmann"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Provable bounds for learning some deep representations", "author": ["S. Arora", "A. Bhaskara", "R. Ge", "T. Ma"], "venue": "Proceedings of the 31th International Conference on Machine Learning, pages 584\u2013592", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "Journal of Machine Learning Research, 13:281\u2013305", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["E. Bingham", "H. Mannila"], "venue": "Proceedings of the 7th International Conference on Knowledge Discovery and Data Mining, pages 245\u2013250", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G. Arous", "Y. LeCun"], "venue": "Proceedings of the 18th International Conference on Artificial Intelligence and Statistics, pages 192\u2013204", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, pages 2528\u20132536", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning, pages 921\u2013928", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Beyond simple features: A large-scale feature search approach to unconstrained face recognition", "author": ["D. Cox", "N. Pinto"], "venue": "Proceedings of 2011 IEEE International Conference on Automatic Face & Gesture Recognition and Workshops, pages 8\u201315", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, pages 2933\u20132941", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "The power of depth for feedforward neural networks", "author": ["R. Eldan", "O. Shamir"], "venue": "arXiv preprint arXiv:1512.03965", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressed sensing: theory and applications", "author": ["Y.C. Eldar", "G. Kutyniok"], "venue": "Cambridge University Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Texture synthesis using convolutional neural networks", "author": ["L. Gatys", "A. Ecker", "M. Bethge"], "venue": "Advances in Neural Information Processing Systems, pages 262\u2013270", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks with random gaussian weights: A universal classification strategy? IEEE Transactions on Signal Processing", "author": ["R. Giryes", "G. Sapiro", "A. Bronstein"], "venue": "64(13):3444\u20133457", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "MIT Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "A powerful generative model using random weights for the deep image representation", "author": ["K. He", "Y. Wang", "J. Hopcroft"], "venue": "Advances In Neural Information Processing Systems, pages 631\u2013639", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, 18(7):1527\u20131554", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "A new constructive algorithm for architectural and functional adaptation of artificial neural networks", "author": ["M. Islam", "M. Sattar", "M. Amin", "X. Yao", "K. Murase"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(6):1590\u20131605", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "What is the best multi-stage architecture for object recognition? In Proceedings of the 12th IEEE International Conference on Computer Vision", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. Lecun"], "venue": "pages 2146\u20132153", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Random feedback weights support learning in deep neural networks", "author": ["T. Lillicrap", "D. Cownden", "D. Tweed", "C. Akerman"], "venue": "arXiv preprint arXiv:1411.0247", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical Learning with Sparsity: The Lasso and Generalizations", "author": ["C.M. O\u2019Brien"], "venue": "Wiley Online Library,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "A high-throughput screening approach to discovering good forms of biologically inspired visual representation", "author": ["N. Pinto", "D. Doukhan", "J. DiCarlo", "D. Cox"], "venue": "PLoS Computational Biology, 5(11):e1000579", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Representational power of restricted Boltzmann machines and deep belief networks", "author": ["N. Roux", "Y. Bengio"], "venue": "Neural Computation, 20(6):1631\u20131649", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning internal representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature 323 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1986}, {"title": "On random weights and unsupervised feature learning", "author": ["A. Saxe", "P. Koh", "Z. Chen", "M. Bhand", "B. Suresh", "A. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning, pages 1089\u20131096", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["A. Saxe", "J. McClelland", "S. Ganguli"], "venue": "Proceedings of International Conference on Learning Representations", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Randomness in neural networks: An overview", "author": ["S. Scardapane", "D. Wang"], "venue": "WIREs Data Mining and Knowledge Discovery", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, 61:85\u2013117", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Proceedings of International Conference on Learning Representations", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic configuration networks: Fundamentals and algorithms", "author": ["D. Wang", "M. Li"], "venue": "arXiv preprint arXiv:1702.03180", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2017}, {"title": "The no-prop algorithm: A new learning algorithm for multilayer neural networks", "author": ["B. Widrow", "A. Greenblatt", "Y. Kim", "D. Park"], "venue": "Neural Networks, 37:182\u2013188", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural network architectures and learning algorithms", "author": ["B.M. Wilamowski"], "venue": "IEEE Industrial Electronics Magazine, 3(4):56\u201363", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M. Zeiler", "R. Fergus"], "venue": "arXiv preprint arXiv:1301.3557", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 20, "context": "[21, 30].", "startOffset": 0, "endOffset": 8}, {"referenceID": 29, "context": "[21, 30].", "startOffset": 0, "endOffset": 8}, {"referenceID": 3, "context": "The success of deep learning is attributed to its representation capability for visual data [4, 18].", "startOffset": 92, "endOffset": 99}, {"referenceID": 17, "context": "The success of deep learning is attributed to its representation capability for visual data [4, 18].", "startOffset": 92, "endOffset": 99}, {"referenceID": 14, "context": "In view of some empirical evidence, DNNs are becoming increasingly popular because of a hypothesis that a deep learner model can be exponentiallymore efficient at representing some functions than a shallow one [15].", "startOffset": 210, "endOffset": 214}, {"referenceID": 10, "context": "Formal analyses of the representation power and learning complexity of DNNs can be found in [11, 25].", "startOffset": 92, "endOffset": 100}, {"referenceID": 24, "context": "Formal analyses of the representation power and learning complexity of DNNs can be found in [11, 25].", "startOffset": 92, "endOffset": 100}, {"referenceID": 20, "context": "Although deep learning schemes draw tremendous attention for their overwhelming high performance for some complex data modelling tasks [21], two key issues should be concerned seriously in model design: the architecture determination for DNNs and the training strategies for deep architectures.", "startOffset": 135, "endOffset": 139}, {"referenceID": 0, "context": "In [1], Alvarez and Salzmann introduced an approach to automatically determine the number of nodes at each layer of the DNN by using group sparsity regularizers.", "startOffset": 3, "endOffset": 6}, {"referenceID": 16, "context": "These obstacles also lie in the process of fine-tuning a pre-trainedmodel obtained by the greedy layer-wise unsupervised learning strategy proposed in [17], as the parameters within all layers need to be optimized according to to an objective cost function.", "startOffset": 151, "endOffset": 155}, {"referenceID": 31, "context": "This paper is built on recent work reported in [32] where the way used to construct shallow neural networks with randomness (termed as stochastic configuration networks, SCNs) is original, innovative and effective.", "startOffset": 47, "endOffset": 51}, {"referenceID": 31, "context": "The success of SCNs can guarantee the convergence of error sequence approaching to certain accuracy, if a moderate number of hidden nodes are generated [32].", "startOffset": 152, "endOffset": 156}, {"referenceID": 28, "context": "Some researchers anticipated the usefulness of randomization in the development of neural networks [29].", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "In [20], it was found that a random filter in convolutional neural networks can perform slightly worse than a well-designed filter, which usually needs pre-training and discriminative", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "addressed this issue in [27] and showed that the results obtained from the randomized learner are comparable to that after regular pre-training and fine-tuning processes.", "startOffset": 24, "endOffset": 28}, {"referenceID": 7, "context": "Coates and Ng [8] tried to use random weights in unsupervised learning, and their experimental results suggested that randomization can be helpful to build large sized models very rapidly, with much ease in training and encoding than sparse coding techniques.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "Randomness was also concerned in the selection of local receptive fields [7], and the pooling operations of deep convolutional neural networks [35].", "startOffset": 73, "endOffset": 76}, {"referenceID": 34, "context": "Randomness was also concerned in the selection of local receptive fields [7], and the pooling operations of deep convolutional neural networks [35].", "startOffset": 143, "endOffset": 147}, {"referenceID": 1, "context": "investigated the learning of autoencoders with random weights and demonstrated that it is possible to train them in polynomial time under some restrictions on the network depth [2].", "startOffset": 177, "endOffset": 180}, {"referenceID": 21, "context": "To speed the training process of back-propagation (BP) in DNNs, authors in [22] proposed a random feedbackmechanism by multiplying error signals with random weights, which contributes to fast extracting useful information from signals sent through the connections.", "startOffset": 75, "endOffset": 79}, {"referenceID": 8, "context": "Motivated by a series of works reported in [9, 24], where they empirically showed some successful learning techniques based on randomization, Giryes et al.", "startOffset": 43, "endOffset": 50}, {"referenceID": 23, "context": "Motivated by a series of works reported in [9, 24], where they empirically showed some successful learning techniques based on randomization, Giryes et al.", "startOffset": 43, "endOffset": 50}, {"referenceID": 13, "context": "[14] theoretically proved that DNNs with random Gaussian weights can perform a stable embedding of the original data, permitting a stable recovery of the data from the learning representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "In [33], the authors stated that the capacity of multilayer perceptronsmainly depends on the number of nodes in the last hidden layer and associated output weights.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "Indeed, their method came up with the same philosophy of random projection for dimensionality reduction [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 15, "context": "In [16], some investigations on the functionality of randomized DNNs for deep visualization were empirically conducted.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In [19], a constructive algorithm was proposed for building cascaded networks, where all hidden nodes are directly linked to the output nodes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "In [34], the BP-like algorithm was developed for training this type of cascaded networks with specified architecture.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "Based on Theorem 7 in [32], we can easily obtain that", "startOffset": 22, "endOffset": 26}, {"referenceID": 25, "context": "The idea of learning internal representation can be traced back to 80\u2019s [26].", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "In the past decades, many exciting researches and progresses have been reported in literature [12, 23], where attempts are made to meet the following three properties: fidelity, sparsity and interpretability.", "startOffset": 94, "endOffset": 102}, {"referenceID": 22, "context": "In the past decades, many exciting researches and progresses have been reported in literature [12, 23], where attempts are made to meet the following three properties: fidelity, sparsity and interpretability.", "startOffset": 94, "endOffset": 102}, {"referenceID": 0, "context": "3e 2 , x \u2208 [0, 1].", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "In Figure 3, two normalized distributions of the read-out weights (converted into [-1,1] on the basis of 200 coefficients from both SCN and DeepSCN) are plotted, and the approximation results for the test dataset are displayed beside the distributions, respectively.", "startOffset": 82, "endOffset": 88}, {"referenceID": 31, "context": "\u2022 The supervisory mechanism characterised by the inequalities (5) and (7) is the key for developing DeepSCN framework, which was originally proposed in [32] and could be used as a unique feature to distinguish our SCN framework from other randomized learner models.", "startOffset": 152, "endOffset": 156}], "year": 2017, "abstractText": "This paper focuses on the development of randomized approaches for building deep neural networks. A supervisory mechanism is proposed to constrain the random assignment of the hidden parameters (i.e., all biases and weights within the hidden layers). Full-rank oriented criterion is suggested and utilized as a termination condition to determine the number of nodes for each hidden layer, and a pre-defined error tolerance is used as a global indicator to decide the depth of the learner model. The read-out weights attached with all direct links from each hidden layer to the output layer are incrementally evaluated by the least squares method. Such a class of randomized leaner models with deep architecture is termed as deep stochastic configuration networks (DeepSCNs), of which the universal approximation property is verified with rigorous proof. Given abundant samples from a continuous distribution, DeepSCNs can speedily produce a learning representation, that is, a collection of random basis functions with the cascaded inputs together with the read-out weights. Simulation results with comparisons on function approximation align with the theoretical findings.", "creator": "LaTeX with hyperref package"}}}