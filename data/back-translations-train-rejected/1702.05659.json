{"id": "1702.05659", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2017", "title": "On Loss Functions for Deep Neural Networks in Classification", "abstract": "Deep neural networks are currently among the most commonly used classifiers. Despite easily achieving very good performance, one of the best selling points of these models is their modular design - one can conveniently adapt their architecture to specific needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others. While one can find impressively wide spread of various configurations of almost every aspect of the deep nets, one element is, in authors' opinion, underrepresented - while solving classification problems, vast majority of papers and applications simply use log loss. In this paper we try to investigate how particular choices of loss functions affect deep models and their learning dynamics, as well as resulting classifiers robustness to various effects. We perform experiments on classical datasets, as well as provide some additional, theoretical insights into the problem. In particular we show that L1 and L2 losses are, quite surprisingly, justified classification objectives for deep nets, by providing probabilistic interpretation in terms of expected misclassification. We also introduce two losses which are not typically used as deep nets objectives and show that they are viable alternatives to the existing ones.", "histories": [["v1", "Sat, 18 Feb 2017 21:39:36 GMT  (1236kb,D)", "http://arxiv.org/abs/1702.05659v1", "Presented at Theoretical Foundations of Machine Learning 2017 (TFML 2017)"]], "COMMENTS": "Presented at Theoretical Foundations of Machine Learning 2017 (TFML 2017)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["katarzyna janocha", "wojciech marian czarnecki"], "accepted": false, "id": "1702.05659"}, "pdf": {"name": "1702.05659.pdf", "metadata": {"source": "CRF", "title": "On Loss Functions for Deep Neural Networks in Classification", "authors": ["Katarzyna Janocha", "Wojciech Marian Czarnecki"], "emails": ["kasiajanocha@gmail.com,", "lejlot@google.com"], "sections": [{"heading": "1 Introduction", "text": "In recent years, deep learning (DL) research has evolved rapidly, evolving from tricky pre-school routines [6] to a highly modular, adaptable framework for building machine learning systems for various problems, ranging from image recognition [5], speech recognition and synthesis [9] to complex AI systems [11]. One of the greatest advantages of DL is the enormous flexibility in designing each part of the architecture, which leads to numerous possibilities to set priority data about data within the model itself [6], to find the most efficient activation functions [2] or learning algorithms [4]. However, to the best of the authors \"knowledge, most of the community still holds an element almost entirely in place - when it comes to classification, we use protocol losses (applied to the softmax activation of the network). In this paper, we try to address this problem by conducting both theoretical and empirical analyses of the effects of networks that relate to the deeper training functions."}, {"heading": "L1 L1 loss \u2016y \u2212 o\u20161", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "L2 L2 loss \u2016y \u2212 o\u201622", "text": "In most cases, losses are losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form of losses in the form"}, {"heading": "2 Theory", "text": "Let's start with interesting properties of the Lp functions, which are normally considered purely regressive losses that should not be used in the classification. (L1) In this section we show that - despite their regression1See Proposition 13 Roots - they still have a reasonable probability interpretation for the classification and can be used as the main classification objective. (In this section we use the following notation: (xi, yi)} Ni = 1 - Rd \u00b7 {0, 1} K is a training set, an id sample of unknown P (x, y) and designates a function that produces probability estimates (usually sigmoid or softmax).Proposition 1. L1 loss that is applied to the probability estimates (y | x) leads to minimization of the expected probability."}, {"heading": "3 Experiments", "text": "It is about the question of whether and in what form people will be able to survive themselves, and about the question of whether they see themselves in a position to survive themselves, and about the question of whether they are able to survive themselves. (...) It is about the question of whether people are able to survive themselves. (...) It is about the question of whether people are able to survive themselves. (...) It is about the question of whether people are able to survive themselves. (...) It is about the question of whether they are able to survive themselves. (...) It is about the question of whether they are able to survive themselves. (...) It is about the question of whether they are able to survive themselves. (...) It is about the question of whether they are able to survive themselves, whether they are able to survive themselves. (...) It is about the question of whether they are able to survive themselves. (...) It is about the question of whether they are able to survive themselves. (...) It is about the question of whether they are able to survive themselves."}, {"heading": "4 Conclusions", "text": "We believe that the results obtained will lead to a reversible adoption of various losses in DL work - with the loss of classification being indisputably preferred until now. In the theoretical section, we show that losses that are thought to be predominantly applicable to regression have a valid probabilistic interpretation when applied to deep network-based classifiers. We also provide theoretical arguments as to why their use might lead to slower training, which may be one of the reasons why DL practitioners have not yet taken advantage of this path. Our experiments lead to two crucial conclusions: firstly, that intuitions drawn from linear models are rarely transferred to highly nonlinear deep networks; secondly, that losses are preferred depending on the application of the deep model - losses other than the loss of the protocol - and, in particular, from purely precise, focused research, square hinge losses seem to be a better choice."}], "references": [{"title": "The loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "In AISTATS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Maximum entropy linear manifold for learning discriminative lowdimensional representation", "author": ["Wojciech Marian Czarnecki", "Rafal Jozefowicz", "Jacek Tabor"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Exploring strategies for training deep neural networks", "author": ["Hugo Larochelle", "Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Pascal Lamblin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "The mnist database of handwritten digits", "author": ["Yann LeCun", "Corinna Cortes", "Christopher JC Burges"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Wavenet: A generative model for raw audio", "author": ["Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "10 Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1609.03499,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Information theoretic learning", "author": ["Jose C Principe", "Dongxin Xu", "John Fisher"], "venue": "Unsupervised adaptive filtering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1929}, {"title": "Deep learning using linear support vector machines", "author": ["Yichuan Tang"], "venue": "arXiv preprint arXiv:1306.0239,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "It evolved from tricky pretraining routines [6] to a highly modular, customisable framework for building machine learning systems for various problems, spanning from image recognition [5], voice recognition and synthesis [9] to complex AI systems [11].", "startOffset": 44, "endOffset": 47}, {"referenceID": 4, "context": "It evolved from tricky pretraining routines [6] to a highly modular, customisable framework for building machine learning systems for various problems, spanning from image recognition [5], voice recognition and synthesis [9] to complex AI systems [11].", "startOffset": 184, "endOffset": 187}, {"referenceID": 7, "context": "It evolved from tricky pretraining routines [6] to a highly modular, customisable framework for building machine learning systems for various problems, spanning from image recognition [5], voice recognition and synthesis [9] to complex AI systems [11].", "startOffset": 221, "endOffset": 224}, {"referenceID": 9, "context": "It evolved from tricky pretraining routines [6] to a highly modular, customisable framework for building machine learning systems for various problems, spanning from image recognition [5], voice recognition and synthesis [9] to complex AI systems [11].", "startOffset": 247, "endOffset": 251}, {"referenceID": 5, "context": "One of the biggest advantages of DL is enormous flexibility in designing each part of the architecture, resulting in numerous ways of putting priors over data inside the model itself [6], finding the most efficient activation functions [2] or learning algorithms [4].", "startOffset": 183, "endOffset": 186}, {"referenceID": 1, "context": "One of the biggest advantages of DL is enormous flexibility in designing each part of the architecture, resulting in numerous ways of putting priors over data inside the model itself [6], finding the most efficient activation functions [2] or learning algorithms [4].", "startOffset": 236, "endOffset": 239}, {"referenceID": 3, "context": "One of the biggest advantages of DL is enormous flexibility in designing each part of the architecture, resulting in numerous ways of putting priors over data inside the model itself [6], finding the most efficient activation functions [2] or learning algorithms [4].", "startOffset": 263, "endOffset": 266}, {"referenceID": 11, "context": "[13] showed that well fitted hinge loss can outperform log loss based networks in typical classification tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] also considered L1 loss as a deep net objective.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "L1 L1 loss \u2016y \u2212 o\u20161 L2 L2 loss \u2016y \u2212 o\u20162 L1 \u25e6 \u03c3 expectation loss \u2016y \u2212 \u03c3(o)\u20161 L2 \u25e6 \u03c3 regularised expectation loss \u2016y \u2212 \u03c3(o)\u20162 L\u221e \u25e6 \u03c3 Chebyshev loss maxj |\u03c3(o) \u2212 y| hinge hinge [13] (margin) loss \u2211 j max(0, 1 2 \u2212 \u0177 o) hinge squared hinge (margin) loss \u2211 j max(0, 1 2 \u2212 \u0177 o) hinge cubed hinge (margin) loss \u2211 j max(0, 1 2 \u2212 \u0177 o) log log (cross entropy) loss \u2212 \u2211 j y (j) log \u03c3(o) log squared log loss \u2212 \u2211 j [y (j) log \u03c3(o)]", "startOffset": 174, "endOffset": 178}, {"referenceID": 2, "context": "tan Tanimoto loss \u2212 \u2211 j \u03c3(o) y \u2016\u03c3(o)\u20162+\u2016y\u20162\u2212 \u2211 j \u03c3(o) (j)y(j) DCS Cauchy-Schwarz Divergence [3] \u2212 log \u2211 j \u03c3(o) y \u2016\u03c3(o)\u20162\u2016y\u20162", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "The first one is checkerboard \u2013 4 class classification problem where [-1,1] square is divided into 64 small squares with cyclic class assignment.", "startOffset": 69, "endOffset": 75}, {"referenceID": 3, "context": "Training is performed using Adam [4] with learning rate of 0.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "Let us now proceed with one of the most common datasets used in deep learning community \u2013 MNIST [7].", "startOffset": 96, "endOffset": 99}, {"referenceID": 10, "context": "We train network consisting from 0 to 5 hidden layers, each followed by ReLU activation function and dropout [12] with 50% probability.", "startOffset": 109, "endOffset": 113}, {"referenceID": 3, "context": "Each hidden layer consists of 512 neurons, and whole model is trained using Adam [4] with learning rate of 0.", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "At the same time this information theoretic measure is very rarely used in DL community, and rather exploited in shallow learning (for both classification [3] and clustering [10]).", "startOffset": 155, "endOffset": 158}, {"referenceID": 8, "context": "At the same time this information theoretic measure is very rarely used in DL community, and rather exploited in shallow learning (for both classification [3] and clustering [10]).", "startOffset": 174, "endOffset": 178}], "year": 2017, "abstractText": "Deep neural networks are currently among the most commonly used classifiers. Despite easily achieving very good performance, one of the best selling points of these models is their modular design \u2013 one can conveniently adapt their architecture to specific needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others. While one can find impressively wide spread of various configurations of almost every aspect of the deep nets, one element is, in authors\u2019 opinion, underrepresented \u2013 while solving classification problems, vast majority of papers and applications simply use log loss. In this paper we try to investigate how particular choices of loss functions affect deep models and their learning dynamics, as well as resulting classifiers robustness to various effects. We perform experiments on classical datasets, as well as provide some additional, theoretical insights into the problem. In particular we show that L1 and L2 losses are, quite surprisingly, justified classification objectives for deep nets, by providing probabilistic interpretation in terms of expected misclassification. We also introduce two losses which are not typically used as deep nets objectives and show that they are viable alternatives to the existing ones.", "creator": "LaTeX with hyperref package"}}}