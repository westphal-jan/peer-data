{"id": "1702.05793", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2017", "title": "Harmonic Grammar, Optimality Theory, and Syntax Learnability: An Empirical Exploration of Czech Word Order", "abstract": "This work presents a systematic theoretical and empirical comparison of the major algorithms that have been proposed for learning Harmonic and Optimality Theory grammars (HG and OT, respectively). By comparing learning algorithms, we are also able to compare the closely related OT and HG frameworks themselves. Experimental results show that the additional expressivity of the HG framework over OT affords performance gains in the task of predicting the surface word order of Czech sentences. We compare the perceptron with the classic Gradual Learning Algorithm (GLA), which learns OT grammars, as well as the popular Maximum Entropy model. In addition to showing that the perceptron is theoretically appealing, our work shows that the performance of the HG model it learns approaches that of the upper bound in prediction accuracy on a held out test set and that it is capable of accurately modeling observed variation.", "histories": [["v1", "Sun, 19 Feb 2017 20:37:40 GMT  (645kb,D)", "http://arxiv.org/abs/1702.05793v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ann irvine", "mark dredze"], "accepted": false, "id": "1702.05793"}, "pdf": {"name": "1702.05793.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Mark Dredze", "Ann Irvine"], "emails": ["annirvine@gmail.com,", "mdredze@cs.jhu.edu"], "sections": [{"heading": null, "text": "Keywords: Optimal Theory, Harmonic Grammar, Learning Ability, CzechAnn Irvine and Mark Dredze Department of Computer Science, Johns Hopkins University 3400 N. Charles St. Baltimore, MD 21218 Tel.: 1-540-460-3663 Email: annirvine @ gmail.com, mdredze @ cs.jhu.eduar Xiv: 170 2.05 793v 1 [cs.C L] 19 Feb 2017"}, {"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Background", "text": "In fact, it is so that most of them are able to survive themselves by going in search of themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "3 Motivation", "text": "It follows the great intuition of work on the second, the algorithmic challenge: the algorithms that should learn human learning online, from one sentence to another. It follows directly from the recent theoretical discussions on such algorithms (Pater, 2008; Magri, 2010; Boersma and Pater, 2008) as well as a wealth of work on the properties of the perceptron algorithm (Rosenblatt, 1958). Furthermore, the models learned from the algorithms are empirically evaluated, which should be accurately predicted, including the patterns of variation on possible surface shapes. Sections 5 and 6, give systematic theoretical and empirical comparisons of standard algorithms that we use in the OT and HG literature including Constraint Demotion (CD), the perceptron algorithms, the work and the work on the work."}, {"heading": "7 Predicting Variation", "text": "The attributes weights that the perception algorithm learns are designed to predict the individual best word sequences in the series of possible orders. However, in this section we would like to predict a distribution by word orders and compare this distribution with the distribution patterns observed for each input pattern (see Table 2).13 To use the learned perception patterns to predict such probability distributions, we use Noisy HG, proposed by Boersma and Pater (2008). That is, for each limitation, we take a sample from a zero mean Gaussian14 and add it to the current weight value. For each input pattern, we repeatedly use samples and predict how often each label is predicted. We define the probability distribution by output labels. For both the GLA and the perceptron, we take 1,000 samples of the predicted word sequence for each input patent.Inserting order of noise in the weights we learn directly explains the GLA and the ranking."}, {"heading": "8 Conclusion", "text": "In this context, it should be noted that this model is not a model, but a model that is a model."}, {"heading": "A Learning Example", "text": "This section gives a concrete example of how the perceptron algorithm is used to learn a predictor of the Czech word order from the input data described above and the constraints described above --.- The weight vector has elements corresponding to each of the 12 constraints -S-R V-L V-R O-R O-O O O-R O-R T-1 T-R C-L F-L F-L-R 9 3 3 2 2 4 4 4 4 4 4 4 4 4 8 Where S stands for subject, V for verb or predicate, O for object, T for topic, C for contrastive topic, F for focus, L for aligned left and R for aligned rights.- Input point 1: x x = Subj-Contrastive Topic, Verb focus, or SC-VF-OF-OF y = SVO- For each word job label, determine attributes values and calculate the result under the current parameters S-S-S-S 1 S-S 1 S-S 1 S-S, S-S-S 1 S-S 1 S-S 1 S-S, S-S-S 1 S-S-S 1 S-S 1 S-S 1 S, S-S-S-S-S 1 S-S 1-S-S-S 1-S 1-S, S-S-S-S-S 1-S-S 1-S-S-S 1-S 1-S, S-S-S-1-S-S-S-1-S-S-S-1-S-S-S 1-S-S, S-S-1-S-S-1-S-S-S-S-1-S-1-S-S-1-S-S-S-1-S-1-S-S-1-S-S-1-S-S-1-S-S-1-S-1-S-S-1-S-1-S-S-1-S-1-S-1-S-1-S-1-S-1-S-1-S-S-S-1-1-S-1-1-S-S-S-"}], "references": [{"title": "Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit", "author": ["S Bird", "E Klein", "E Loper"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "How we learn variation, optionality, and probability", "author": ["O\u2019Reilly", "Beijing Boersma P"], "venue": "Proceedings of the Institute of Phonetic Studies,", "citeRegEx": "O.Reilly and P,? \\Q1997\\E", "shortCiteRegEx": "O.Reilly and P", "year": 1997}, {"title": "Word order and constraint interaction", "author": ["J Costa"], "venue": "Seminarios de Linguistica Costa J", "citeRegEx": "Costa,? \\Q1997\\E", "shortCiteRegEx": "Costa", "year": 1997}, {"title": "Environment prototypicality in syntactic alternation", "author": ["Syntax", "G MIT Press Doyle", "R Levy"], "venue": "Proceedings of the 34th Annual Meeting", "citeRegEx": "Syntax et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Syntax et al\\.", "year": 2008}, {"title": "Learning OT constraint rankings using a maximum entropy model", "author": ["S Goldwater", "M Johnson"], "venue": "Phonology (SIGPHON),", "citeRegEx": "Goldwater and Johnson,? \\Q2003\\E", "shortCiteRegEx": "Goldwater and Johnson", "year": 2003}, {"title": "Variation within Optimality Theory, pp", "author": ["J Hajic", "B Hladka", "P Pajas"], "venue": null, "citeRegEx": "Hajic et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hajic et al\\.", "year": 2001}, {"title": "An automatic procedure for topic-focus identification", "author": ["E Haji\u010dov\u00e1", "P Sgall", "H Skoumalova"], "venue": "Workshop on Linguistic Databases,", "citeRegEx": "Haji\u010dov\u00e1 et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Haji\u010dov\u00e1 et al\\.", "year": 1995}, {"title": "Topic-focus articulation, tripartite structures, and semantic content", "author": ["E Haji\u010dov\u00e1", "BH Partee", "P Sgall"], "venue": "Cumulativity in grammatical variation", "citeRegEx": "Haji\u010dov\u00e1 et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Haji\u010dov\u00e1 et al\\.", "year": 1998}, {"title": "Meeting of the North East Linguistics Society (NELS", "author": ["M Johnson"], "venue": "Proceedings of the European Association", "citeRegEx": "Johnson,? \\Q2009\\E", "shortCiteRegEx": "Johnson", "year": 2009}, {"title": "Noise tolerant variants of the Perceptron algorithm", "author": ["R Khardon", "G Wachman", "J Collins"], "venue": "Journal of Machine Learning", "citeRegEx": "Khardon et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Khardon et al\\.", "year": 2005}, {"title": "Configuring topic and focus in Russian. In: CSLI Dissertations in Linguistics", "author": ["TH King"], "venue": "The Annals of Mathematical Statistics", "citeRegEx": "King,? \\Q1995\\E", "shortCiteRegEx": "King", "year": 1995}, {"title": "Can connectionism contribute to syntax? Harmonic Grammar, with an application", "author": ["Y Miyata", "P Smolensky"], "venue": null, "citeRegEx": "Legendre et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Legendre et al\\.", "year": 1990}, {"title": "Harmonic grammar a formal multi level connectionist theory of linguistic well", "author": ["G Legendre", "Y Miyata", "P Smolensky"], "venue": null, "citeRegEx": "Legendre et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Legendre et al\\.", "year": 1990}, {"title": "Deriving output probabilities in child Mandarin", "author": ["MA Cambridge"], "venue": null, "citeRegEx": "Cambridge,? \\Q2004\\E", "shortCiteRegEx": "Cambridge", "year": 2004}, {"title": "The interaction of syntax and semantics: A harmonic grammar account of split", "author": ["Y Miyata", "P Smolensky"], "venue": null, "citeRegEx": "Legendre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Legendre et al\\.", "year": 2006}, {"title": "The optimality theory-harmonic grammar connection", "author": ["Irvine Ann", "G Mark Dredze Legendre", "A Sorace", "P Smolensky"], "venue": null, "citeRegEx": "Ann et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ann et al\\.", "year": 2006}, {"title": "HG has no computational advantages over OT: consequences for the theory of OT online", "author": ["G Magri"], "venue": null, "citeRegEx": "Magri,? \\Q2010\\E", "shortCiteRegEx": "Magri", "year": 2010}, {"title": "HG has no computational advantages over OT: new tools for computational OT. Linguistic Inquiry McCarthy JJ (2003) OT constraints are categorical", "author": ["G Magri"], "venue": "IJN, ENS Available", "citeRegEx": "Magri,? \\Q2011\\E", "shortCiteRegEx": "Magri", "year": 2011}, {"title": "Gradual learning and convergence. Linguistic Inquiry", "author": ["J Pater"], "venue": "Cognitive Science", "citeRegEx": "Pater,? \\Q2008\\E", "shortCiteRegEx": "Pater", "year": 2008}, {"title": "Harmonic grammar with linear programming: From linear systems", "author": ["C Potts", "J Pater", "K Jesney", "R Bhatt", "M Becker"], "venue": "Proceedings of the Annual Meeting of the Linguistic Society", "citeRegEx": "Potts et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Potts et al\\.", "year": 2010}, {"title": "Optimality theory: Constraint interaction in generative grammar", "author": ["A Prince", "P Smolensky"], "venue": null, "citeRegEx": "Prince and Smolensky,? \\Q1993\\E", "shortCiteRegEx": "Prince and Smolensky", "year": 1993}, {"title": "Optimality Theory: Constraint Interaction in Generative Grammar", "author": ["A Prince", "P Smolensky"], "venue": "Blackwell Publishers Rosenblatt F", "citeRegEx": "Prince and Smolensky,? \\Q2004\\E", "shortCiteRegEx": "Prince and Smolensky", "year": 2004}], "referenceMentions": [{"referenceID": 17, "context": "Recent work (Magri, 2011) has pointed out that any HG learning algorithm can be adapted to learn an OT grammar and, thus, OT learnability is no more computationally complex than HG learnability.", "startOffset": 12, "endOffset": 25}, {"referenceID": 4, "context": "We compare the perceptron and GLA and the prediction accuracies of the grammars they learn with a Maximum Entropy model, another popular model of HG/OT learnability (Jaeger and Rosenbach, 2003; Goldwater and Johnson, 2003; Hayes and Wilson, 2008).", "startOffset": 165, "endOffset": 246}, {"referenceID": 14, "context": "However, recently there has been a resurgence of interest in the Harmonic Grammar (HG) framework (Legendre et al, 1990a,b,c, 2006a,b; Coetzee and Pater, 2008; Pater, 2009; Jesney and Tessier, 2009; Potts et al, 2010), which is closely related to OT. In this work, we theoretically and empirically compare learning algorithms for each and, by doing so, are able to compare the frameworks themselves. As noted in recent work, including Pater (2008), Boersma and Pater (2008), and Magri (2010), the perceptron learning algorithm is well-established in the Machine Learning field and is a natural choice for modeling human grammar acquisition.", "startOffset": 146, "endOffset": 447}, {"referenceID": 14, "context": "However, recently there has been a resurgence of interest in the Harmonic Grammar (HG) framework (Legendre et al, 1990a,b,c, 2006a,b; Coetzee and Pater, 2008; Pater, 2009; Jesney and Tessier, 2009; Potts et al, 2010), which is closely related to OT. In this work, we theoretically and empirically compare learning algorithms for each and, by doing so, are able to compare the frameworks themselves. As noted in recent work, including Pater (2008), Boersma and Pater (2008), and Magri (2010), the perceptron learning algorithm is well-established in the Machine Learning field and is a natural choice for modeling human grammar acquisition.", "startOffset": 146, "endOffset": 473}, {"referenceID": 14, "context": "As noted in recent work, including Pater (2008), Boersma and Pater (2008), and Magri (2010), the perceptron learning algorithm is well-established in the Machine Learning field and is a natural choice for modeling human grammar acquisition.", "startOffset": 79, "endOffset": 92}, {"referenceID": 19, "context": "We assume that our readers are familiar with the Optimality Theory (OT) framework proposed in Prince and Smolensky (1993) and Harmonic Grammar (HG) proposed in Legendre et al (1990a), and we only briefly review each here.", "startOffset": 94, "endOffset": 122}, {"referenceID": 19, "context": "We assume that our readers are familiar with the Optimality Theory (OT) framework proposed in Prince and Smolensky (1993) and Harmonic Grammar (HG) proposed in Legendre et al (1990a), and we only briefly review each here.", "startOffset": 94, "endOffset": 183}, {"referenceID": 18, "context": "Pater et al (2007); Coetzee and Pater (2008); Boersma and Pater (2008); Pater (2009); Potts et al (2010).", "startOffset": 0, "endOffset": 19}, {"referenceID": 18, "context": "Pater et al (2007); Coetzee and Pater (2008); Boersma and Pater (2008); Pater (2009); Potts et al (2010).", "startOffset": 0, "endOffset": 45}, {"referenceID": 18, "context": "Pater et al (2007); Coetzee and Pater (2008); Boersma and Pater (2008); Pater (2009); Potts et al (2010).", "startOffset": 0, "endOffset": 71}, {"referenceID": 18, "context": "Pater et al (2007); Coetzee and Pater (2008); Boersma and Pater (2008); Pater (2009); Potts et al (2010).", "startOffset": 0, "endOffset": 85}, {"referenceID": 18, "context": "Pater et al (2007); Coetzee and Pater (2008); Boersma and Pater (2008); Pater (2009); Potts et al (2010). However, it remains to be seen empirically if it offers advantages over OT to justify its additional complexity.", "startOffset": 0, "endOffset": 105}, {"referenceID": 18, "context": "Pater et al (2007); Coetzee and Pater (2008); Boersma and Pater (2008); Pater (2009); Potts et al (2010). However, it remains to be seen empirically if it offers advantages over OT to justify its additional complexity. Jesney and Tessier (2009) provide some evidence that ganging-up effects can be empirically observed.", "startOffset": 0, "endOffset": 245}, {"referenceID": 8, "context": "As pointed out in Johnson (2009), there are two, potentially separate, grammar learning problems.", "startOffset": 18, "endOffset": 33}, {"referenceID": 4, "context": "The research literature on the learnability of an OT model of speakers\u2019 knowledge of grammar and their production patterns, including variation, is extensive and includes the influential papers by Jaeger and Rosenbach (2003) and Goldwater and Johnson (2003). In contrast to the plethora of work on learning phonological grammars given both underlying and surface word forms, Jarosz (2006) learns", "startOffset": 229, "endOffset": 258}, {"referenceID": 4, "context": "The research literature on the learnability of an OT model of speakers\u2019 knowledge of grammar and their production patterns, including variation, is extensive and includes the influential papers by Jaeger and Rosenbach (2003) and Goldwater and Johnson (2003). In contrast to the plethora of work on learning phonological grammars given both underlying and surface word forms, Jarosz (2006) learns", "startOffset": 229, "endOffset": 389}, {"referenceID": 18, "context": "Pater (2008) points out a flaw in the GLA algorithm and gives a high level discussion of the perceptron learning algorithm as a possible alternative.", "startOffset": 0, "endOffset": 13}, {"referenceID": 18, "context": "Pater (2008) points out a flaw in the GLA algorithm and gives a high level discussion of the perceptron learning algorithm as a possible alternative. Boersma and Pater (2008) extend this idea and propose a Harmonic Grammar version of the GLA algorithm (HG-GLA), which is similar to the perceptron algorithm.", "startOffset": 0, "endOffset": 175}, {"referenceID": 18, "context": "It follows directly from recent theoretical discussions of such algorithms (Pater, 2008; Magri, 2010; Boersma and Pater, 2008) as well as a plethora of work on the properties of the perceptron algorithm (Rosenblatt, 1958).", "startOffset": 75, "endOffset": 126}, {"referenceID": 16, "context": "It follows directly from recent theoretical discussions of such algorithms (Pater, 2008; Magri, 2010; Boersma and Pater, 2008) as well as a plethora of work on the properties of the perceptron algorithm (Rosenblatt, 1958).", "startOffset": 75, "endOffset": 126}, {"referenceID": 10, "context": "Using Russian as an example language, King (1995) discusses the interaction between these levels of structural encoding and, in particular, the implications for the scope of information structure elements.", "startOffset": 38, "endOffset": 50}, {"referenceID": 18, "context": "4 Following Boersma and Pater (2008), in this work, all attribute values are one of the following: 1, indicating that the input and hypothesis explicitly complies with a constraint; 0, indicating that a constraint is vacuously satisfied; or \u22121, indicating the input and hypothesis violate a constraint.", "startOffset": 24, "endOffset": 37}, {"referenceID": 18, "context": "4 Following Boersma and Pater (2008), in this work, all attribute values are one of the following: 1, indicating that the input and hypothesis explicitly complies with a constraint; 0, indicating that a constraint is vacuously satisfied; or \u22121, indicating the input and hypothesis violate a constraint. That is, the harmony, or linear combination of weights, of a hypothesis is a function of the constraints that it complies with as well as those that it violates. Boersma and Pater (2008) point out that this setup is not explicitly disallowed by the original formulation of OT, which did not distinguish between overt constraint satisfaction and vacuous constraint satisfaction.", "startOffset": 24, "endOffset": 490}, {"referenceID": 18, "context": "Recently, Boersma and Pater (2008) proposed a Harmonic Grammar version of the GLA, HG-GLA.", "startOffset": 22, "endOffset": 35}, {"referenceID": 4, "context": "Goldwater and Johnson (2003) use a MaxEnt model to learn probabilistic phonological grammars both with and without variation.", "startOffset": 0, "endOffset": 29}, {"referenceID": 4, "context": "Goldwater and Johnson (2003) use a MaxEnt model to learn probabilistic phonological grammars both with and without variation. That work uses a MaxEnt model to explain variation in Finnish genitive plural endings as accurately as the GLA model does and it claims that the model is more theoretically sound than the GLA. Jaeger and Rosenbach (2003) use a MaxEnt model to predict variation in the English genitive construction9 and provide some evidence that it is superior to the GLA model because of its ability to allow for ganging-up effects of low ranked constraints.", "startOffset": 0, "endOffset": 347}, {"referenceID": 4, "context": "Goldwater and Johnson (2003) use a MaxEnt model to learn probabilistic phonological grammars both with and without variation. That work uses a MaxEnt model to explain variation in Finnish genitive plural endings as accurately as the GLA model does and it claims that the model is more theoretically sound than the GLA. Jaeger and Rosenbach (2003) use a MaxEnt model to predict variation in the English genitive construction9 and provide some evidence that it is superior to the GLA model because of its ability to allow for ganging-up effects of low ranked constraints. Finally, Hayes and Wilson (2008) use a MaxEnt model to learn a model of phonotactic patterns and Pater et al (2010) explore the model\u2019s convergence properties.", "startOffset": 0, "endOffset": 603}, {"referenceID": 4, "context": "Goldwater and Johnson (2003) use a MaxEnt model to learn probabilistic phonological grammars both with and without variation. That work uses a MaxEnt model to explain variation in Finnish genitive plural endings as accurately as the GLA model does and it claims that the model is more theoretically sound than the GLA. Jaeger and Rosenbach (2003) use a MaxEnt model to predict variation in the English genitive construction9 and provide some evidence that it is superior to the GLA model because of its ability to allow for ganging-up effects of low ranked constraints. Finally, Hayes and Wilson (2008) use a MaxEnt model to learn a model of phonotactic patterns and Pater et al (2010) explore the model\u2019s convergence properties.", "startOffset": 0, "endOffset": 686}, {"referenceID": 18, "context": "13 In order to use the perceptron\u2019s learned weights to predict such probability distributions, we use Noisy HG, which was proposed by Boersma and Pater (2008). That is, for each constraint, we take a noise sample from a zero-mean Gaussian14 and add it to the current weight value.", "startOffset": 146, "endOffset": 159}], "year": 2017, "abstractText": "This work presents a systematic theoretical and empirical comparison of the major algorithms that have been proposed for learning Harmonic and Optimality Theory grammars (HG and OT, respectively). By comparing learning algorithms, we are also able to compare the closely related OT and HG frameworks themselves. Experimental results show that the additional expressivity of the HG framework over OT affords performance gains in the task of predicting the surface word order of Czech sentences. We compare the perceptron with the classic Gradual Learning Algorithm (GLA), which learns OT grammars, as well as the popular Maximum Entropy model. In addition to showing that the perceptron is theoretically appealing, our work shows that the performance of the HG model it learns approaches that of the upper bound in prediction accuracy on a held out test set and that it is capable of accurately modeling observed variation.", "creator": "LaTeX with hyperref package"}}}