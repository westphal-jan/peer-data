{"id": "1702.05796", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2017", "title": "Collaborative Deep Reinforcement Learning", "abstract": "Besides independent learning, human learning process is highly improved by summarizing what has been learned, communicating it with peers, and subsequently fusing knowledge from different sources to assist the current learning goal. This collaborative learning procedure ensures that the knowledge is shared, continuously refined, and concluded from different perspectives to construct a more profound understanding. The idea of knowledge transfer has led to many advances in machine learning and data mining, but significant challenges remain, especially when it comes to reinforcement learning, heterogeneous model structures, and different learning tasks. Motivated by human collaborative learning, in this paper we propose a collaborative deep reinforcement learning (CDRL) framework that performs adaptive knowledge transfer among heterogeneous learning agents. Specifically, the proposed CDRL conducts a novel deep knowledge distillation method to address the heterogeneity among different learning tasks with a deep alignment network. Furthermore, we present an efficient collaborative Asynchronous Advantage Actor-Critic (cA3C) algorithm to incorporate deep knowledge distillation into the online training of agents, and demonstrate the effectiveness of the CDRL framework using extensive empirical evaluation on OpenAI gym.", "histories": [["v1", "Sun, 19 Feb 2017 21:13:45 GMT  (1606kb,D)", "http://arxiv.org/abs/1702.05796v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kaixiang lin", "shu wang", "jiayu zhou"], "accepted": false, "id": "1702.05796"}, "pdf": {"name": "1702.05796.pdf", "metadata": {"source": "META", "title": "Collaborative Deep Reinforcement Learning", "authors": ["Kaixiang Lin", "Shu Wang", "Jiayu Zhou"], "emails": ["linkaixi@msu.edu", "sw498@cs.rutgers.edu", "jiayuz@msu.edu"], "sections": [{"heading": null, "text": "CCS CONCEPTS \u2022 Computingmethodologies \u2192 Machine learning; Reinforcement learning; Transfer learning; KEYWORDS Knowledge distillation; Transfer learning; Deep reinforcement learning ACM Reference format: Kaixiang Lin, Shu Wang, and Jiayu Zhou. 1997. Collaborative Deep Reinforcement Learning. In Proceedings of ACM Woodstock conference, El Paso, Texas USA, July 1997 (WOODSTOCK '97), 9 pages. DOI: 10.475 / 123 4"}, {"heading": "1 INTRODUCTION", "text": "In fact, most people are able to move to another world in which they are in the position in which they find themselves."}, {"heading": "2 RELATEDWORK", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "3 BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Reinforcement Learning", "text": "In this thesis, we consider the standard reinforcement learning, in which each agent interacts with his own environment over a number of individual time steps. e Agent's goal is to select an action in step t that maximizes the sum of future rewards {rt} in a decreasing manner: Rt = \u2211 i = 0 \u03b3irt + i, where scalar \u03b3 (0, 1] is a discount rate. e Agent's goal is to further maximize the function of the state value V (st) = E [Rt | s = st], which estimates the expected discounted rate of return based on the state st, taking action according to the policy guidelines until the end of the game. e Agent's goal in learning reinforcement is to maximize the expected rate of return. As we mainly discuss the design and behavior of an agent throughout the paper, we disregard the concentration of the agent."}, {"heading": "3.2 Asynchronous Advantage actor-critic algorithm (A3C)", "text": "The asynchronous advantage-actor-critic algorithm (A3C) [19] introduces several agents in parallel and updates asynchronously a global common target-policy network \u03c0 (a | s, \u03b8p) and a value network V (s, \u03b8v). Each actor interacts independently with the environment. At each step, the actor takes an action based on the probability distribution generated by the political network. If he plays out an n-stage rollout or reaches the end state, the rewards are used to calculate the advantage with the output of the value function. e Updates of the political network are carried out by applying the gradient: approximp log\u03c0 (at | st; depenp) A (st, at; \u03b8v), with the advantage function A (st, at; depenv) determined by the following values: \u00b2 T \u2212 t \u2212 1 i = 0 irt + i \u2212 tV \u2212 ropp \u2212 V (st; v) the net function represents the benefit function A (v) in each case where the number of steps is performed."}, {"heading": "3.3 Knowledge distillation", "text": "Knowledge distillation [15] is a transfer learning approach that distils knowledge from a teacher network into a student network using a temperature parameterized \"so targets\" (i.e. a probability distribution over a group of classes). It has been shown that it can accelerate training with less data, since the gradient of \"so targets\" contains much more information than the gradient of \"hard targets\" (e.g. 0, 1 monitoring). To be more precise, the log vector z \"Rd\" for d actions can be converted into a probability distribution h \"(0, 1) d by such a max function that is raised with temperature: h\" i \"= so max\" z. \""}, {"heading": "4 COLLABORATIVE DEEP REINFORCEMENT LEARNING FRAMEWORK", "text": "In this section, we present the proposed Framework for Collaborative Deep Reinforcement Learning (CDRL). Before detailing our method, an underlying assumption is formulated as follows: Assumption 1. If there is a universe that contains all tasks E = {e1, e2,..., e \u221e} and Ki is the appropriate knowledge to master each task, then this is a formal description of our common sense that each task pair is not absolutely isolated from each other, which has been implicitly used as a basic assumption in most previous transfer studies [11, 24, 26]. Therefore, we focus on reducing common knowledge across multiple tasks rather than providing strategies for selecting tasks that share knowledge as widely as possible, which can lead to our future work. The goal is to make the best possible use of existing knowledge to help each other."}, {"heading": "4.1 Collaborative deep reinforcement learning", "text": "In order to integrate this type of collaboration into the training of DRL agents, we must formally include Collaborative Deep Reinforcement Learning (CDRL) in the framework in which the Collaborative Deep Tasks (CDRL) are implemented within the framework of the following measures: De nition 4.1. Give independent environments {\u03b51, \u03b52,..., \u03b5m} ofm tasks {e1, e2,..., em}, the Corresponding Agents {e1, \u00a42, \u00a42,..., \u00a4m} are collaboratively trained actors to maximize rewards. \u2022 Environment is not limited to environments: e m environments can be completely dierent or with some duplications. \u2022 In any environment, it interacts only with the appropriate agent."}, {"heading": "4.2 Deep knowledge distillation", "text": "Since before the introduction of knowledge distillation [15] we tried to form a student network that could behave similar to the teacher network by using the teacher's logits as supervision, but even if the dimensionality of the action space is the same between the tasks, the probability of action of the task allocation for the individual tasks can vary greatly, as we have shown in Figure 5 (a) and (b). The action options represented by the logis of the action space are usually di erent from task to task. If we directly force a student network to mix tasks, it could be trained in the wrong direction, and it could perform worse than isolated education."}, {"heading": "4.3 Collaborative Asynchronous Advantage Actor-Critic", "text": "In this section, we introduce the proposed asynchronous benefit activist-critic (cA3C). As we have described in Section 4.1, the agents run in parallel; each agent undergoes the same training process as described in Algorithm 1. As it shows, the training of agents can be divided into two parts: the first part is to interact with the environment, receive the reward, and calculate the gradients to minimize the loss of value and political loss based on Generalized Advantage Estimation (GAE); the second part is to interact with the source agents so that the tasks distilled by the agents can be transferred from Agent 2 and used as supervision. More specifically, the pseudo-code in Algorithm 1 is an envolved version of A3C."}, {"heading": "5 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Training and Evaluation", "text": "This work involves training and evaluation in OpenAI Gym [5], a toolkit that includes a collection of benchmark problems such as classic Atari games using Arcade Learning Environment (ALE) [4], classic control games, etc. Similar to the standard RL se ing, an agent is stimulated in an environment by performing an action and receiving rewards and observations at each step. Agent training is divided into episodes, and the goal is to maximize the expectation of total reward per episode or achieve higher performance with as few episodes as possible."}, {"heading": "5.2 Certi cated Homogeneous transfer", "text": "In this area, we are able to improve the e-mail communication between the individual countries. E-mail addresses of the individual countries are able to enter the e-mail addresses of the individual countries."}, {"heading": "5.3 Certi cated Heterogeneous Transfer", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "5.4 Collaborative Deep Reinforcement Learning", "text": "In previous experiments, we assumed that there was a well-trained Pong expert, and we transferred knowledge from the Pong expert to the bowling student through deep knowledge distillation. A more difficult task is that both bowling and Pong are trained from the ground up. In this experiment, we show that the CDRL framework can still be effective in this regard. In this experiment, we train a bowling network and a Pong network from the ground up using the proposed cA3C algorithm. E-Pong agents are trained only with GAE interactions, and the target bowling gets supervision of GAE interactions and distilled knowledge from Pong about a deep orientation network. We start training the Deep Orientation Network in 3 million steps and perform deep knowledge distillation in 4 million steps, with the Pong agents still being updated from the environment. We find that in this experiment the Teacher Network is constantly being updated from knowledge to 15 steps."}, {"heading": "6 CONCLUSION", "text": "Finally, we propose a collaborative, in-depth learning framework that can address the transfer of knowledge between heterogeneous tasks. In this context, we propose a deep knowledge distillation to adapt the range of diversified tasks to the use of a deep alignment network. Furthermore, we develop an e cient cA3C algorithm and demonstrate its effectiveness through comprehensive evaluation on OpenAI gyms."}, {"heading": "ACKNOWLEDGMENTS", "text": "is supported in part by the Office of Naval Research (ONR) under grant number N00014-14-1-0631 and the National Science Foundation under grant number IIS-1565596, IIS-1615597."}], "references": [{"title": "Cross channel optimized marketing by reinforcement learning", "author": ["Naoki Abe", "Naval Verma", "Chid Apte", "Robert Schroko"], "venue": "In SIGKDD", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning. In Under review as a conference paper at ICLR 2017", "author": ["Coline Devin"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "\u008ce Arcade Learning Environment: An evaluation platform for general agents", "author": ["Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "J. Artif. Intell. Res.(JAIR)", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Multi-agent reinforcement learning: An overview", "author": ["Lucian Bu\u015foniu", "Robert Babu\u0161ka", "Bart De Schu\u008aer"], "venue": "In Innovations in multi-agent systems and applications-1", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "A behavior-based scheme using reinforcement learning for autonomous underwater vehicles", "author": ["Marc Carreras", "Junku Yuh", "Joan Batlle", "Pere Ridao"], "venue": "IEEE Journal of Oceanic Engineering 30,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Collaborative Learning: Cognitive and Computational Approaches. Advances in Learning and Instruction Series. ERIC", "author": ["Pierre Dillenbourg"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Multi-task feature learning", "author": ["A Evgeniou", "Massimiliano Pontil"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Regularized multi\u2013task learning", "author": ["\u008ceodoros Evgeniou", "Massimiliano Pontil"], "venue": "In SIGKDD", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Collaborative learning enhances critical thinking", "author": ["Anuradha A Gokhale"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "Coordinated reinforcement learning", "author": ["Carlos Guestrin", "Michail Lagoudakis", "Ronald Parr"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Sapiens: a brief history of humankind", "author": ["Yuval N. Harari"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Geo\u0082rey Hinton", "Oriol Vinyals", "Je\u0082 Dean"], "venue": "arXiv preprint arXiv:1503.02531", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Max Jaderberg", "Volodymyr Mnih", "Wojciech Marian Czarnecki", "Tom Schaul", "Joel Z Leibo", "David Silver", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Collaborative multiagent reinforcement learning by payo\u0082 propagation", "author": ["Jelle R Kok", "Nikos Vlassis"], "venue": "JMLR 7,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Playing FPS games with deep reinforcement learning", "author": ["Guillaume Lample", "Devendra Singh Chaplot"], "venue": "arXiv preprint arXiv:1609.05521", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang"], "venue": "TKDE 22,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Actormimic: Deep multitask and transfer reinforcement learning", "author": ["Emilio Pariso\u008ao", "Jimmy Lei Ba", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1511.06342", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "A two-stage weighting framework for multi-source domain adaptation", "author": ["Qian Sun", "Rita Cha\u008aopadhyay", "Sethuraman Panchanathan", "Jieping Ye"], "venue": "In NIPS", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents", "author": ["Ming Tan"], "venue": "In ICML", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1993}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Ma\u008ahew E Taylor", "Peter Stone"], "venue": "JMLR 10,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Agent based decision support system using reinforcement learning under emergency circumstances", "author": ["Devinder \u008capa", "In-Sung Jung", "Gi-Nam Wang"], "venue": "In International Conference on Natural Computation", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2005}, {"title": "Simultaneous deep transfer across domains and tasks", "author": ["Eric Tzeng", "Judy Ho\u0082man", "Trevor Darrell", "Kate Saenko"], "venue": "In ICCV", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Yu Zhang", "Dit-Yan Yeung"], "venue": "arXiv preprint arXiv:1203.3536", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Facial landmark detection by deep multi-task learning", "author": ["Zhanpeng Zhang", "Ping Luo", "Chen Change Loy", "Xiaoou Tang"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "MALSAR: Multi-task learning via structural regularization", "author": ["Jiayu Zhou", "Jianhui Chen", "Jieping Ye"], "venue": "Arizona State University", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Yuke Zhu", "Roozbeh Mo\u008aaghi", "Eric Kolve", "Joseph J Lim", "Abhinav Gupta", "Li Fei-Fei", "Ali Farhadi"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "As a social animal, the ability to collaborate awoke the cognitive revolution and reveals the prosperous history of human [14].", "startOffset": 122, "endOffset": 126}, {"referenceID": 5, "context": "In disciplines of cognitive science, education and psychology, collaborative learning, a situation in which a group of people learn to achieve a set of tasks together, has been advocated throughout previous studies [9].", "startOffset": 215, "endOffset": 218}, {"referenceID": 8, "context": "process, as well [12].", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": ", [16, 18, 20, 28]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 14, "context": ", [16, 18, 20, 28]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 22, "context": "great transformative power to impact many industries with data mining and machine learning techniques such as clinical decision support [32], marketing [2], \u0080nance [1], visual navigation [37], and", "startOffset": 136, "endOffset": 140}, {"referenceID": 0, "context": "great transformative power to impact many industries with data mining and machine learning techniques such as clinical decision support [32], marketing [2], \u0080nance [1], visual navigation [37], and", "startOffset": 152, "endOffset": 155}, {"referenceID": 27, "context": "great transformative power to impact many industries with data mining and machine learning techniques such as clinical decision support [32], marketing [2], \u0080nance [1], visual navigation [37], and", "startOffset": 187, "endOffset": 191}, {"referenceID": 4, "context": "autonomous driving [8].", "startOffset": 19, "endOffset": 22}, {"referenceID": 15, "context": "Although there are many existing e\u0082orts towards e\u0082ective algorithms for DRL [19, 21], the computational cost still imposes signi\u0080cant challenges as training DRL for even a ar X iv :1 70 2.", "startOffset": 76, "endOffset": 84}, {"referenceID": 12, "context": "transfer from other related tasks or well-trained deep models to facilitate training has drawn lots of a\u008aention in the community [16, 24\u2013 26, 31].", "startOffset": 129, "endOffset": 145}, {"referenceID": 21, "context": "transfer from other related tasks or well-trained deep models to facilitate training has drawn lots of a\u008aention in the community [16, 24\u2013 26, 31].", "startOffset": 129, "endOffset": 145}, {"referenceID": 11, "context": "Existing transfer learning can be categorized into two classes according to the means that knowledge is transferred: data transfer [15, 24, 26] and model transfer [10, 24, 34, 35].", "startOffset": 131, "endOffset": 143}, {"referenceID": 17, "context": "Existing transfer learning can be categorized into two classes according to the means that knowledge is transferred: data transfer [15, 24, 26] and model transfer [10, 24, 34, 35].", "startOffset": 131, "endOffset": 143}, {"referenceID": 6, "context": "Existing transfer learning can be categorized into two classes according to the means that knowledge is transferred: data transfer [15, 24, 26] and model transfer [10, 24, 34, 35].", "startOffset": 163, "endOffset": 179}, {"referenceID": 17, "context": "Existing transfer learning can be categorized into two classes according to the means that knowledge is transferred: data transfer [15, 24, 26] and model transfer [10, 24, 34, 35].", "startOffset": 163, "endOffset": 179}, {"referenceID": 24, "context": "Existing transfer learning can be categorized into two classes according to the means that knowledge is transferred: data transfer [15, 24, 26] and model transfer [10, 24, 34, 35].", "startOffset": 163, "endOffset": 179}, {"referenceID": 25, "context": "Existing transfer learning can be categorized into two classes according to the means that knowledge is transferred: data transfer [15, 24, 26] and model transfer [10, 24, 34, 35].", "startOffset": 163, "endOffset": 179}, {"referenceID": 7, "context": "For example, in the regularized MTL models such as [11, 36], tasks with the same feature space are related through some structured regularization.", "startOffset": 51, "endOffset": 59}, {"referenceID": 26, "context": "For example, in the regularized MTL models such as [11, 36], tasks with the same feature space are related through some structured regularization.", "startOffset": 51, "endOffset": 59}, {"referenceID": 25, "context": "Another example is the multi-task deep neural network, where di\u0082erent tasks share parts of the network structures [35].", "startOffset": 114, "endOffset": 118}, {"referenceID": 11, "context": "On the other hand, the recently developed data transfer (also known as knowledge distillation or mimic learning) [15, 24, 26] embeds the source model knowledge into data points.", "startOffset": 113, "endOffset": 125}, {"referenceID": 17, "context": "On the other hand, the recently developed data transfer (also known as knowledge distillation or mimic learning) [15, 24, 26] embeds the source model knowledge into data points.", "startOffset": 113, "endOffset": 125}, {"referenceID": 11, "context": "\u008cen they are used as knowledge bridge to train target models, which can have di\u0082erent structures as compared to the source model [6, 15].", "startOffset": 129, "endOffset": 136}, {"referenceID": 17, "context": "extensively trained and performs very well), how can we maximize the information that can be used in the training of other related tasks? Some model transfer approaches directly use the weights from the trained model to initialize the new task [24], which can only be done when the model structures are the same.", "startOffset": 244, "endOffset": 248}, {"referenceID": 17, "context": "tasks are signi\u0080cantly di\u0082erent from each other in nature [24].", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "One feasible way to conduct transfer under this scenario is that agents of multiple tasks share part of their network parameters [26, 35].", "startOffset": 129, "endOffset": 137}, {"referenceID": 1, "context": "Another solution is to learn a domain invariant feature space shared by all tasks [3].", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "Meanwhile they could potentially interact with each other [7, 13, 17, 30].", "startOffset": 58, "endOffset": 73}, {"referenceID": 9, "context": "Meanwhile they could potentially interact with each other [7, 13, 17, 30].", "startOffset": 58, "endOffset": 73}, {"referenceID": 13, "context": "Meanwhile they could potentially interact with each other [7, 13, 17, 30].", "startOffset": 58, "endOffset": 73}, {"referenceID": 20, "context": "Meanwhile they could potentially interact with each other [7, 13, 17, 30].", "startOffset": 58, "endOffset": 73}, {"referenceID": 9, "context": "maximize a shared reward measurement [13, 17].", "startOffset": 37, "endOffset": 45}, {"referenceID": 13, "context": "maximize a shared reward measurement [13, 17].", "startOffset": 37, "endOffset": 45}, {"referenceID": 16, "context": "Another relevant research topic is domain adaption in the \u0080eld of transfer learning [23, 29, 33].", "startOffset": 84, "endOffset": 96}, {"referenceID": 19, "context": "Another relevant research topic is domain adaption in the \u0080eld of transfer learning [23, 29, 33].", "startOffset": 84, "endOffset": 96}, {"referenceID": 23, "context": "Another relevant research topic is domain adaption in the \u0080eld of transfer learning [23, 29, 33].", "startOffset": 84, "endOffset": 96}, {"referenceID": 19, "context": "\u008ce authors in [29] proposed a two-stage domain adaptation framework that considers the di\u0082erences among marginal probability distributions of domains, as well as conditional probability distributions of tasks.", "startOffset": 14, "endOffset": 18}, {"referenceID": 23, "context": "In [33], the marginal distributions of the source and the target domain are aligned by training a network, which maps inputs into a domain invariant representation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "In [3], an invariant feature space is learned to transfer skills between two agents.", "startOffset": 3, "endOffset": 6}, {"referenceID": 25, "context": "\u008ce multi-task deep neural network (MTDNN) [35] transfers knowledge among tasks by sharing pa-", "startOffset": 42, "endOffset": 46}, {"referenceID": 11, "context": "In [15], the authors proposed to compress cumbersome models (teachers) to more simple models (students), where the simple models are trained by a dataset (knowledge) dis-", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "[19]", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "In [24], a single multi-task policy network is trained by utilizing a set of expert Deep Q-Network (DQN) of source games.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "\u008ce asynchronous advantage actor-critic (A3C) algorithm [19] launches multiple agents in parallel and asynchronously updates a global shared target policy network \u03c0 (a |s,\u03b8p ) as well as a value network V (s,\u03b8v ).", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "Knowledge distillation [15] is a transfer learning approach that distills the knowledge from a teacher network to a student network", "startOffset": 23, "endOffset": 27}, {"referenceID": 11, "context": "\u008cen the knowledge distillation can be completed by optimize the following Kullback-Leibler divergence (KL) with temperature \u03c4 [15, 26].", "startOffset": 126, "endOffset": 134}, {"referenceID": 7, "context": "\u008cis is a formal description of our common sense that any pair of tasks are not absolutely isolated from each other, which has been implicitly used as a fundamental assumption by most prior transfer learning studies [11, 24, 26].", "startOffset": 215, "endOffset": 227}, {"referenceID": 17, "context": "\u008cis is a formal description of our common sense that any pair of tasks are not absolutely isolated from each other, which has been implicitly used as a fundamental assumption by most prior transfer learning studies [11, 24, 26].", "startOffset": 215, "endOffset": 227}, {"referenceID": 5, "context": "\u008ce agent \u0434i is not necessary to be at same level as \u201dcollaborative\u201d de\u0080ned in cognitive science [9].", "startOffset": 96, "endOffset": 99}, {"referenceID": 9, "context": "Notice that our de\u0080nition is very di\u0082erent from the previously de\u0080ned collaborative multiagent Markow Decision Process (collaborative multiagent MDP) [13, 17] where a set of agents select a global joint action to maximize the sum of their individual rewards and the environment is transi\u008aed to a new state based on that joint action.", "startOffset": 150, "endOffset": 158}, {"referenceID": 13, "context": "Notice that our de\u0080nition is very di\u0082erent from the previously de\u0080ned collaborative multiagent Markow Decision Process (collaborative multiagent MDP) [13, 17] where a set of agents select a global joint action to maximize the sum of their individual rewards and the environment is transi\u008aed to a new state based on that joint action.", "startOffset": 150, "endOffset": 158}, {"referenceID": 15, "context": "the-art is A3C [19] can be categorized as one homogeneous CDRL method with advantage actor-critic interaction.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "As we introduced before, knowledge distillation [15] is trying to train a student network that can behave similarly to the teacher network by utilizing the logits from the teacher as supervision.", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "As it shows, the training of agent \u04341 can be separated into two parts: \u008ce \u0080rst part is to interact with the environment, get the reward and compute the gradients to minimize the value loss and policy loss based on Generalized Advantage Estimation (GAE) [27].", "startOffset": 253, "endOffset": 257}, {"referenceID": 15, "context": "Since the main asynchronous framework is the same as A3C, we still use the A3C to denote this algorithm although the updating is the not the same as advantage actor-critic algorithm used in original A3C paper [19].", "startOffset": 209, "endOffset": 213}, {"referenceID": 18, "context": "Require: Global shared parameter vectors \u0398p and \u0398v and global shared counter T = 0; Agent-speci\u0080c parameter vectors \u0398p and \u0398\u2032 v , GAE [27] parameters \u03b3 and \u03bb.", "startOffset": 134, "endOffset": 138}, {"referenceID": 2, "context": "(ALE) [4], classic control games, etc.", "startOffset": 6, "endOffset": 9}], "year": 2017, "abstractText": "Besides independent learning, human learning process is highly improved by summarizing what has been learned, communicating it with peers, and subsequently fusing knowledge from di\u0082erent sources to assist the current learning goal. \u008cis collaborative learning procedure ensures that the knowledge is shared, continuously re\u0080ned, and concluded from di\u0082erent perspectives to construct a more profound understanding. \u008ce idea of knowledge transfer has led to many advances in machine learning and data mining, but signi\u0080cant challenges remain, especially when it comes to reinforcement learning, heterogeneous model structures, and di\u0082erent learning tasks. Motivated by human collaborative learning, in this paper we propose a collaborative deep reinforcement learning (CDRL) framework that performs adaptive knowledge transfer among heterogeneous learning agents. Speci\u0080cally, the proposed CDRL conducts a novel deep knowledge distillation method to address the heterogeneity among di\u0082erent learning tasks with a deep alignment network. Furthermore, we present an e\u0081cient collaborative Asynchronous Advantage Actor-Critic (cA3C) algorithm to incorporate deep knowledge distillation into the online training of agents, and demonstrate the e\u0082ectiveness of the CDRL framework using extensive empirical evaluation on OpenAI gym.", "creator": "LaTeX with hyperref package"}}}