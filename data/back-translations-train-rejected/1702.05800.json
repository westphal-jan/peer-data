{"id": "1702.05800", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Feb-2017", "title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.", "histories": [["v1", "Sun, 19 Feb 2017 21:51:48 GMT  (570kb,D)", "http://arxiv.org/abs/1702.05800v1", null], ["v2", "Sat, 18 Mar 2017 23:02:17 GMT  (0kb,I)", "http://arxiv.org/abs/1702.05800v2", "This article will be superseded byarXiv:1604.00981"]], "reviews": [], "SUBJECTS": "cs.DC cs.AI cs.LG", "authors": ["xinghao pan", "jianmin chen", "rajat monga", "samy bengio", "rafal jozefowicz"], "accepted": false, "id": "1702.05800"}, "pdf": {"name": "1702.05800.pdf", "metadata": {"source": "CRF", "title": "REVISITING DISTRIBUTED SYNCHRONOUS SGD", "authors": ["Jianmin Chen", "Xinghao Pan", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "emails": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com", "xinghao@eecs.berkeley.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, at a time when we are not yet in a position to find a solution, at a time when we are not yet in a position to find a solution, we will find ourselves in a position to find a solution, \"he said."}, {"heading": "1.1 PRELIMINARIES AND NOTATION", "text": "In view of a data set X = {xi: i = 1,.., | X |}, our goal is to learn the parameters of a model in relation to an empirical loss function f, defined as f (\u03b8) \u2206 = 1 | X | \u2211 | X | i = 1 F (xi; \u03b8), where F (xi; \u03b8) is the loss in relation to a data point xi and the model project.A stochastic optimization algorithm of the first order achieves this by iteratively updating the stochastic gradient G \u0445 = F (xi; \u03b8), which is computed randomly, resulting in a sequence of models."}, {"heading": "2 ASYNCHRONOUS STOCHASTIC OPTIMIZATION", "text": "An approach to a distributed stochastic gradient algorithm was presented in Dean et al. (2012), consisting of two main components. Firstly, the parameters of the model are distributed across several servers, depending on the architecture. This set of servers is referred to as a parameter server. Secondly, there may be several workers who process data in parallel and communicate with the parameter servers. \u2022 Gradients of loss are then calculated in relation to these parameters, as follows: \u2022 The workforce fetches the most up-to-date parameters of the model from the parameter servers, which are needed to process the current mini-batches. \u2022 Gradients of loss in relation to these parameters are calculated. \u2022 Finally, these gradients are sent back to the parameter servers, which then update the corresponding data.Since each worker communicates with the parameter servers independently of the others, this is referred to as Asynchronous Stochastic."}, {"heading": "2.1 IMPACT OF STALENESS ON TEST ACCURACY", "text": "To mimic the setting on a smaller scale, we trained a state-of-the-art MNIST CNN model, but simulated staleness using old gradients for parameter updates. Details of the model and training can be found in Appendix A.1. The best final classification error on a test set was 0.36%, which increased from 20 steps to 0.47% on average slope staleness, and up to 0.79% at 50 steps (see Figure 2). After the average simulated staleness was selected to exceed 15 steps, the results began to deteriorate significantly and the training itself became much less stable. We had to use the following tricks to prevent the results from inflating: \u2022 Slowly, staleness increases in the first 3 phases of the training, which mimics the number of asynchronous workers and is also very important in practice for some of the models we experimented with."}, {"heading": "3 REVISTING SYNCHRONOUS STOCHASTIC OPTIMIZATION", "text": "Both Dean et al. (2012) and Chilimbi et al. (2014) use versions of Async-SGD, where the main problem is that each worker calculates gradients over a potentially old version of the model. To eliminate this discrepancy, we propose here to reconsider a synchronous version of the distributed stochastic gradient (Sync-SGD), or more generally, synchronous stochastic optimization (Sync-Opt), where the parameter servers wait for all workers to send their gradients, aggregate them, and send the updated parameters to all workers (Sync-SGD), ensuring that the actual algorithm is a true stochastic gradients model, with an effective batch size corresponding to the sum of all the workers \"minibatch sizes. While this approach solves the stalness problem, it also introduces the potential problem that the actual update time depends on the slowest worker."}, {"heading": "3.1 STRAGGLER EFFECTS", "text": "The use of backup workers is motivated by the need to mitigate slow stragglers while maximizing the calculation. We study the effects of stragglers on the traditional model. We ran synchronously with N = 100 workers, and 19 parameter sera on the inception model. If we use a variable as a proxy, we collect for each iteration both the start time of the iteration and the time in which the kth gradient of this variable arrives. These times are shown in Figure 3 for k = 50, 90, 97, 99, 100."}, {"heading": "4 EXPERIMENTS", "text": "In this section we present our empirical comparisons of synchronous and asynchronous distributed stochastic optimization algorithms as applied to models such as Inception and PixelCNN. All experiments in this article use the TensorFlow system (Abadi et al., 2015)."}, {"heading": "4.1 METRICS OF COMPARISON: FASTER CONVERGENCE, BETTER OPTIMUM", "text": "We are interested in two benchmarks for our empirical validation: (1) test error or accuracy and (2) speed of convergence. We point out that it is possible for non-convex deep learning models to adapt more quickly to a worse local optimum. Here, we show a simple example of Inception using different learning rates. We have performed Sync-Opt on Inception with N = 100 and b = 6, but the initial learning rate varies between 1,125 and 9.0. (Learning rates are reduced exponentially with iterations.) Table 2 shows that smaller \u03b30 converge faster, but with inferior test accuracy. Focusing speed at an early stage of training could lead to misleading conclusions if we do not take the final convergence into account. Thus, Figure 3b shows that the use of \u03b30 = 1,125 reaches an accuracy of 1.5 x 75%, but slower for 77.75% and does not achieve higher precision."}, {"heading": "4.2 INCEPTION", "text": "We conducted experiments with the Inception model (Szegedy et al., 2016), which was trained on the ImageNet Challenge dataset (Russakovsky et al., 2015), with the task of classifying images from 1000 categories. We used several configurations that vary N + b from 53 to 212 workers. Further details of the training are shown in Appendix A.3. One epoch is a synchronous iteration for Sync-Opt or a complete run of N updates for Async-Opt, which represent similar amounts of computation.The results of this experiment are shown in Figure 8.Figure 8b, which shows that Sync-Opt outperforms Async-Opt in terms of test accuracy: Sync-Opt achieves 0.5% better test accuracy than Async-Opt for comparable N + b workers. In addition, Sync-Opt 6h and 18h converges faster than Async-Opt are needed for async-106 or slower-Opc-3b workers, respectively."}, {"heading": "4.3 PIXELCNN EXPERIMENTS", "text": "The second model we experimented with is PixelCNN (Oord et al., 2016), a deep neural network with conditional image generation, which we trained on the data set CIFAR-10 (Krizhevsky & Hinton, 2009). Configurations of N + b = 1, 8, 16 workers were used; for Sync-Opt we always used b = 1 backup worker. Further details are shown in Appendix A.4.Convergence of negative test log probability (NLL) on PixelCNN in Figure 9a, where lower is better. Note that Sync-Opt receives a lower NLL than Async-Opt; in fact, Async-Opt is even exceeded by the serial RMSProp with N = 1 workers, with performance deteriorating when N increases from 8 to 16. Figure 9b further shows the time needed to reach the test NLL Sync-Opt is not reached once from this time < NL is not reduced to this time < NL = 2.40)."}, {"heading": "5 RELATED WORK", "text": "In recent years, we have focused a lot of attention on multilateral and distributed optimization algorithms. Asynchronous algorithms include Recht et al. (2011); Duchi et al. (2013); Zhang et al. (2015a); Reddi et al. (2015); Leblond et al. (2016). Implementations of asynchronous optimization include Xing et al. (2015); Li et al. (2014); Chilimbi et al. (2014). Trials have also been conducted in Zinkevich et al. (2010) and Zhang & Jordan (2015) to improve synchronous SGD. An alternative solution, \"Softsync et al.,\" was proposed in Zhang et al. (2015b), which proposed combining gradients from multiple machines before performing asynchronous SGD updating, thus reducing the effective shelf life of gradients."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "In this paper, we have shown how both synchronous and asynchronous distributed stochastic optimization suffer from their respective weaknesses of stragglers and stragglers, which has motivated us to develop synchronous stochastic optimization with reserve workers, which is proving to be a viable and scalable strategy. We are currently experimenting with various types of data sets, including word-level speech models, where parts of the model (the embedding layers) are often very sparse, resulting in very different communication constraints. We are also working to further improve the performance of synchronous training, such as combining gradients of multiple workers sharing the same machine before sending them to the parameter servers to reduce communication effort."}, {"heading": "A DETAILS OF MODELS AND TRAINING", "text": "The model we use in our experiments is a 4-layer CNN, which has 3x3 filters with max-pooling and weight normalization in each layer. We trained the model with SGD for 25 epochs and rated the performance on the exponential moving average. We also used small image rotations and zoom as a data augmentation scheme. A.2 INCEPTION, SECTION 3.1For our initial learning rate it was determined that we trained the Inzeption (Szegedy et al) model on the ImageNet Challenge dataset al (Russakovsky et al., 2015) 10 parameter servers were used, and each worker was equipped with a k40 GPU.The basic optimizer."}], "references": [{"title": "Revisiting distributed synchronous sgd", "author": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "venue": "arXiv preprint arXiv:1604.00981,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["T. Chilimbi", "Y. Suzue", "J. Apacible", "K. Kalyanaraman"], "venue": "In Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation,", "citeRegEx": "Chilimbi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chilimbi et al\\.", "year": 2014}, {"title": "Distributed deep learning using synchronous stochastic gradient descent", "author": ["Dipankar Das", "Sasikanth Avancha", "Dheevatsa Mudigere", "Karthikeyan Vaidynathan", "Srinivas Sridharan", "Dhiraj Kalamkar", "Bharat Kaul", "Pradeep Dubey"], "venue": "arXiv preprint arXiv:1602.06709,", "citeRegEx": "Das et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Das et al\\.", "year": 2016}, {"title": "Taming the wild: A unified analysis of hogwild-style algorithms", "author": ["Christopher M De Sa", "Ce Zhang", "Kunle Olukotun", "Christopher R\u00e9"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sa et al\\.", "year": 2015}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G.S. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q.V. Le", "M.Z. Mao", "M.A. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "The tail at scale", "author": ["Jeffrey Dean", "Luiz Andr Barroso"], "venue": "Communications of the ACM,", "citeRegEx": "Dean and Barroso.,? \\Q2013\\E", "shortCiteRegEx": "Dean and Barroso.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Estimation, optimization, and parallelism when data is sparse", "author": ["John Duchi", "Michael I Jordan", "Brendan McMahan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Duchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2013}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "On large-batch training for deep learning: Generalization gap and sharp minima", "author": ["Nitish Shirish Keskar", "Dheevatsa Mudigere", "Jorge Nocedal", "Mikhail Smelyanskiy", "Ping Tak Peter Tang"], "venue": "arXiv preprint arXiv:1609.04836,", "citeRegEx": "Keskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Keskar et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Asaga: Asynchronous parallel saga", "author": ["R\u00e9mi Leblond", "Fabian Pedregosa", "Simon Lacoste-Julien"], "venue": "arXiv preprint arXiv:1606.04809,", "citeRegEx": "Leblond et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Leblond et al\\.", "year": 2016}, {"title": "Scaling distributed machine learning with the parameter server", "author": ["Mu Li", "David G Andersen", "Jun Woo Park", "Alexander J Smola", "Amr Ahmed", "Vanja Josifovski", "James Long", "Eugene J Shekita", "Bor-Yiing Su"], "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Perturbed iterate analysis for asynchronous stochastic optimization", "author": ["Horia Mania", "Xinghao Pan", "Dimitris Papailiopoulos", "Benjamin Recht", "Kannan Ramchandran", "Michael I Jordan"], "venue": "arXiv preprint arXiv:1507.06970,", "citeRegEx": "Mania et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mania et al\\.", "year": 2015}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Oriol Vinyals", "Lasse Espeholt", "Alex Graves", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1606.05328,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "On variance reduction in stochastic gradient descent and its asynchronous variants", "author": ["Sashank J Reddi", "Ahmed Hefny", "Suvrit Sra", "Barnabas Poczos", "Alex J Smola"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Reddi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Reddi et al\\.", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": "In International Journal of Computer Vision,", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Rethinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "In ArXiv", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Ako: Decentralised deep learning with partial gradient exchange", "author": ["Pijika Watcharapichat", "Victoria Lopez Morales", "Raul Castro Fernandez", "Peter Pietzuch"], "venue": "In Proceedings of the Seventh ACM Symposium on Cloud Computing,", "citeRegEx": "Watcharapichat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Watcharapichat et al\\.", "year": 2016}, {"title": "Petuum: A new platform for distributed machine learning on big data", "author": ["Eric P Xing", "Qirong Ho", "Wei Dai", "Jin Kyu Kim", "Jinliang Wei", "Seunghak Lee", "Xun Zheng", "Pengtao Xie", "Abhimanu Kumar", "Yaoliang Yu"], "venue": "IEEE Transactions on Big Data,", "citeRegEx": "Xing et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2015}, {"title": "Deep learning with elastic averaging sgd", "author": ["Sixin Zhang", "Anna E Choromanska", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Staleness-aware async-sgd for distributed deep learning", "author": ["Wei Zhang", "Suyog Gupta", "Xiangru Lian", "Ji Liu"], "venue": "arXiv preprint arXiv:1511.05950,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Splash: User-friendly programming interface for parallelizing stochastic algorithms", "author": ["Yuchen Zhang", "Michael I Jordan"], "venue": "arXiv preprint arXiv:1506.07552,", "citeRegEx": "Zhang and Jordan.,? \\Q2015\\E", "shortCiteRegEx": "Zhang and Jordan.", "year": 2015}, {"title": "Parallelized stochastic gradient descent", "author": ["Martin Zinkevich", "Markus Weimer", "Lihong Li", "Alex J Smola"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Zinkevich et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "The recent success of deep learning approaches for domains like speech recognition (Hinton et al., 2012) and computer vision (Ioffe & Szegedy, 2015) stems from many algorithmic improvements but also from the fact that the size of available training data has grown significantly over the years, together with the computing power, in terms of both CPUs and GPUs.", "startOffset": 83, "endOffset": 104}, {"referenceID": 6, "context": "Currently, popular distributed training algorithms include mini-batch versions of stochastic gradient descent (SGD) and other stochastic optimization algorithms such as AdaGrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), and ADAM (Kingma & Ba, 2014).", "startOffset": 177, "endOffset": 197}, {"referenceID": 0, "context": "edu This is an extension of our ICLR 2016 workshop extended abstract (Chen et al., 2016).", "startOffset": 69, "endOffset": 88}, {"referenceID": 4, "context": "An approach for a distributed stochastic gradient descent algorithm was presented in Dean et al. (2012), consisting of two main ingredients.", "startOffset": 85, "endOffset": 104}, {"referenceID": 1, "context": "A similar approach was later proposed by Chilimbi et al. (2014). Async-Opt is presented in Algorithms 1 and 2.", "startOffset": 41, "endOffset": 64}, {"referenceID": 1, "context": "A similar approach was later proposed by Chilimbi et al. (2014). Async-Opt is presented in Algorithms 1 and 2. In practice, the updates of Async-Opt are different than those of serially running the stochastic optimization algorithm for two reasons. Firstly, the read operation (Algo 1 Line 2) on a worker may be interleaved with updates by other workers to different parameter servers, so the resultant \u03b8\u0302k may not be consistent with any parameter incarnation \u03b8. Secondly, model updates may have occurred while a worker is computing its stochastic gradient; hence, the resultant gradients are typically computed with respect to outdated parameters. We refer to these as stale gradients, and its staleness as the number of updates that have occurred between its corresponding read and update operations. Understanding the theoretical impact of staleness is difficult work and the topic of many recent papers, e.g. Recht et al. (2011); Duchi et al.", "startOffset": 41, "endOffset": 933}, {"referenceID": 1, "context": "A similar approach was later proposed by Chilimbi et al. (2014). Async-Opt is presented in Algorithms 1 and 2. In practice, the updates of Async-Opt are different than those of serially running the stochastic optimization algorithm for two reasons. Firstly, the read operation (Algo 1 Line 2) on a worker may be interleaved with updates by other workers to different parameter servers, so the resultant \u03b8\u0302k may not be consistent with any parameter incarnation \u03b8. Secondly, model updates may have occurred while a worker is computing its stochastic gradient; hence, the resultant gradients are typically computed with respect to outdated parameters. We refer to these as stale gradients, and its staleness as the number of updates that have occurred between its corresponding read and update operations. Understanding the theoretical impact of staleness is difficult work and the topic of many recent papers, e.g. Recht et al. (2011); Duchi et al. (2013); Leblond et al.", "startOffset": 41, "endOffset": 954}, {"referenceID": 1, "context": "A similar approach was later proposed by Chilimbi et al. (2014). Async-Opt is presented in Algorithms 1 and 2. In practice, the updates of Async-Opt are different than those of serially running the stochastic optimization algorithm for two reasons. Firstly, the read operation (Algo 1 Line 2) on a worker may be interleaved with updates by other workers to different parameter servers, so the resultant \u03b8\u0302k may not be consistent with any parameter incarnation \u03b8. Secondly, model updates may have occurred while a worker is computing its stochastic gradient; hence, the resultant gradients are typically computed with respect to outdated parameters. We refer to these as stale gradients, and its staleness as the number of updates that have occurred between its corresponding read and update operations. Understanding the theoretical impact of staleness is difficult work and the topic of many recent papers, e.g. Recht et al. (2011); Duchi et al. (2013); Leblond et al. (2016); Reddi et al.", "startOffset": 41, "endOffset": 977}, {"referenceID": 1, "context": "A similar approach was later proposed by Chilimbi et al. (2014). Async-Opt is presented in Algorithms 1 and 2. In practice, the updates of Async-Opt are different than those of serially running the stochastic optimization algorithm for two reasons. Firstly, the read operation (Algo 1 Line 2) on a worker may be interleaved with updates by other workers to different parameter servers, so the resultant \u03b8\u0302k may not be consistent with any parameter incarnation \u03b8. Secondly, model updates may have occurred while a worker is computing its stochastic gradient; hence, the resultant gradients are typically computed with respect to outdated parameters. We refer to these as stale gradients, and its staleness as the number of updates that have occurred between its corresponding read and update operations. Understanding the theoretical impact of staleness is difficult work and the topic of many recent papers, e.g. Recht et al. (2011); Duchi et al. (2013); Leblond et al. (2016); Reddi et al. (2015);", "startOffset": 41, "endOffset": 998}, {"referenceID": 20, "context": "To better understand this dependence in real models, we collected staleness statistics on a Async-Opt run with 40 workers on a 18-layer Inception model (Szegedy et al., 2016) trained on the ImageNet Challenge dataset (Russakovsky et al.", "startOffset": 152, "endOffset": 174}, {"referenceID": 19, "context": ", 2016) trained on the ImageNet Challenge dataset (Russakovsky et al., 2015), as shown in Table 1.", "startOffset": 50, "endOffset": 76}, {"referenceID": 3, "context": "De Sa et al. (2015); Mania et al.", "startOffset": 3, "endOffset": 20}, {"referenceID": 3, "context": "De Sa et al. (2015); Mania et al. (2015), most of which focus on individual algorithms, under strong assumptions that may not hold up in practice.", "startOffset": 3, "endOffset": 41}, {"referenceID": 3, "context": "Both Dean et al. (2012) and Chilimbi et al.", "startOffset": 5, "endOffset": 24}, {"referenceID": 1, "context": "(2012) and Chilimbi et al. (2014) use versions of Async-SGD where the main potential problem is that each worker computes gradients over a potentially old version of the model.", "startOffset": 11, "endOffset": 34}, {"referenceID": 20, "context": "We conducted experiments on the Inception model (Szegedy et al., 2016) trained on ImageNet Challenge dataset (Russakovsky et al.", "startOffset": 48, "endOffset": 70}, {"referenceID": 19, "context": ", 2016) trained on ImageNet Challenge dataset (Russakovsky et al., 2015), where the task is to classify images out of 1000 categories.", "startOffset": 46, "endOffset": 72}, {"referenceID": 16, "context": "The second model we experimented on is PixelCNN (Oord et al., 2016), a conditional image generation deep neural network, which we train on the CIFAR-10 (Krizhevsky & Hinton, 2009) dataset.", "startOffset": 48, "endOffset": 67}, {"referenceID": 10, "context": "Asynchronous algorithms include Recht et al. (2011); Duchi et al.", "startOffset": 32, "endOffset": 52}, {"referenceID": 4, "context": "(2011); Duchi et al. (2013); Zhang et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 4, "context": "(2011); Duchi et al. (2013); Zhang et al. (2015a); Reddi et al.", "startOffset": 8, "endOffset": 50}, {"referenceID": 4, "context": "(2011); Duchi et al. (2013); Zhang et al. (2015a); Reddi et al. (2015); Leblond et al.", "startOffset": 8, "endOffset": 71}, {"referenceID": 4, "context": "(2011); Duchi et al. (2013); Zhang et al. (2015a); Reddi et al. (2015); Leblond et al. (2016). Implementations of asynchronous optimization include Xing et al.", "startOffset": 8, "endOffset": 94}, {"referenceID": 4, "context": "(2011); Duchi et al. (2013); Zhang et al. (2015a); Reddi et al. (2015); Leblond et al. (2016). Implementations of asynchronous optimization include Xing et al. (2015); Li et al.", "startOffset": 8, "endOffset": 167}, {"referenceID": 4, "context": "(2011); Duchi et al. (2013); Zhang et al. (2015a); Reddi et al. (2015); Leblond et al. (2016). Implementations of asynchronous optimization include Xing et al. (2015); Li et al. (2014); Chilimbi et al.", "startOffset": 8, "endOffset": 185}, {"referenceID": 1, "context": "(2014); Chilimbi et al. (2014). Attempts have also been made in Zinkevich et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 1, "context": "(2014); Chilimbi et al. (2014). Attempts have also been made in Zinkevich et al. (2010) and Zhang & Jordan (2015) to algorithmically improve synchronous SGD.", "startOffset": 8, "endOffset": 88}, {"referenceID": 1, "context": "(2014); Chilimbi et al. (2014). Attempts have also been made in Zinkevich et al. (2010) and Zhang & Jordan (2015) to algorithmically improve synchronous SGD.", "startOffset": 8, "endOffset": 114}, {"referenceID": 1, "context": "(2014); Chilimbi et al. (2014). Attempts have also been made in Zinkevich et al. (2010) and Zhang & Jordan (2015) to algorithmically improve synchronous SGD. An alternative solution, \u201csoftsync\u201d, was presented in Zhang et al. (2015b), which proposed batching gradients from multiple machines before performing an asynchronous SGD update, thereby reducing the effective staleness of gradients.", "startOffset": 8, "endOffset": 233}, {"referenceID": 1, "context": "(2014); Chilimbi et al. (2014). Attempts have also been made in Zinkevich et al. (2010) and Zhang & Jordan (2015) to algorithmically improve synchronous SGD. An alternative solution, \u201csoftsync\u201d, was presented in Zhang et al. (2015b), which proposed batching gradients from multiple machines before performing an asynchronous SGD update, thereby reducing the effective staleness of gradients. Similar to our proposal, softsync avoids stragglers by not forcing updates to wait for the slowest worker. However, softsync allows the use of stale gradients but we do not. The two solutions provide different explorations of the trade-off between high accuracy (by minimizing staleness) and fast throughput (by avoiding stragglers). Watcharapichat et al. (2016) introduces a distributed deep learning system without parameter servers, by having workers interleave gradient computation and communication in a round-robin pattern.", "startOffset": 8, "endOffset": 755}, {"referenceID": 1, "context": "(2014); Chilimbi et al. (2014). Attempts have also been made in Zinkevich et al. (2010) and Zhang & Jordan (2015) to algorithmically improve synchronous SGD. An alternative solution, \u201csoftsync\u201d, was presented in Zhang et al. (2015b), which proposed batching gradients from multiple machines before performing an asynchronous SGD update, thereby reducing the effective staleness of gradients. Similar to our proposal, softsync avoids stragglers by not forcing updates to wait for the slowest worker. However, softsync allows the use of stale gradients but we do not. The two solutions provide different explorations of the trade-off between high accuracy (by minimizing staleness) and fast throughput (by avoiding stragglers). Watcharapichat et al. (2016) introduces a distributed deep learning system without parameter servers, by having workers interleave gradient computation and communication in a round-robin pattern. Like Async-Opt, this approach suffers from staleness. We also note that in principle, workers in Sync-Opt can double as parameter servers and execute the update operations and avoid the need to partition hardware resources between workers and servers. Das et al. (2016) analyzes distributed stochastic optimization and optimizes the system by solving detailed system balance equations.", "startOffset": 8, "endOffset": 1192}, {"referenceID": 1, "context": "(2014); Chilimbi et al. (2014). Attempts have also been made in Zinkevich et al. (2010) and Zhang & Jordan (2015) to algorithmically improve synchronous SGD. An alternative solution, \u201csoftsync\u201d, was presented in Zhang et al. (2015b), which proposed batching gradients from multiple machines before performing an asynchronous SGD update, thereby reducing the effective staleness of gradients. Similar to our proposal, softsync avoids stragglers by not forcing updates to wait for the slowest worker. However, softsync allows the use of stale gradients but we do not. The two solutions provide different explorations of the trade-off between high accuracy (by minimizing staleness) and fast throughput (by avoiding stragglers). Watcharapichat et al. (2016) introduces a distributed deep learning system without parameter servers, by having workers interleave gradient computation and communication in a round-robin pattern. Like Async-Opt, this approach suffers from staleness. We also note that in principle, workers in Sync-Opt can double as parameter servers and execute the update operations and avoid the need to partition hardware resources between workers and servers. Das et al. (2016) analyzes distributed stochastic optimization and optimizes the system by solving detailed system balance equations. We believe this approach is complimentary to our work, and could potentially be applied to guide the choice of systems configurations for Sync-Opt. Keskar et al. (2016) suggests that large batch sizes for synchronous stochastic optimization leads to poorer generalization.", "startOffset": 8, "endOffset": 1477}, {"referenceID": 1, "context": "(2014); Chilimbi et al. (2014). Attempts have also been made in Zinkevich et al. (2010) and Zhang & Jordan (2015) to algorithmically improve synchronous SGD. An alternative solution, \u201csoftsync\u201d, was presented in Zhang et al. (2015b), which proposed batching gradients from multiple machines before performing an asynchronous SGD update, thereby reducing the effective staleness of gradients. Similar to our proposal, softsync avoids stragglers by not forcing updates to wait for the slowest worker. However, softsync allows the use of stale gradients but we do not. The two solutions provide different explorations of the trade-off between high accuracy (by minimizing staleness) and fast throughput (by avoiding stragglers). Watcharapichat et al. (2016) introduces a distributed deep learning system without parameter servers, by having workers interleave gradient computation and communication in a round-robin pattern. Like Async-Opt, this approach suffers from staleness. We also note that in principle, workers in Sync-Opt can double as parameter servers and execute the update operations and avoid the need to partition hardware resources between workers and servers. Das et al. (2016) analyzes distributed stochastic optimization and optimizes the system by solving detailed system balance equations. We believe this approach is complimentary to our work, and could potentially be applied to guide the choice of systems configurations for Sync-Opt. Keskar et al. (2016) suggests that large batch sizes for synchronous stochastic optimization leads to poorer generalization. Our effective batch size increases linearly with the number of workers N . However, we did not observe this effect in our experiments; we believe we are not yet in the large batch size regime examined by Keskar et al. (2016).", "startOffset": 8, "endOffset": 1806}], "year": 2017, "abstractText": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.", "creator": "LaTeX with hyperref package"}}}