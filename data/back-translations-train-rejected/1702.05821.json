{"id": "1702.05821", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Post-edit Analysis of Collective Biography Generation", "abstract": "Text generation is increasingly common but often requires manual post-editing where high precision is critical to end users. However, manual editing is expensive so we want to ensure this effort is focused on high-value tasks. And we want to maintain stylistic consistency, a particular challenge in crowd settings. We present a case study, analysing human post-editing in the context of a template-based biography generation system. An edit flow visualisation combined with manual characterisation of edits helps identify and prioritise work for improving end-to-end efficiency and accuracy.", "histories": [["v1", "Mon, 20 Feb 2017 00:23:06 GMT  (73kb,D)", "http://arxiv.org/abs/1702.05821v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bo han", "will radford", "ana\\\"is cadilhac", "art harol", "rew chisholm", "ben hachey"], "accepted": false, "id": "1702.05821"}, "pdf": {"name": "1702.05821.pdf", "metadata": {"source": "CRF", "title": "Post-edit Analysis of Collective Biography Generation", "authors": ["Bo Han", "Will Radford", "Ana\u00efs Cadilhac", "Art Harol", "Andrew Chisholm", "Ben Hachey"], "emails": ["bhan@hugo.ai", "wradford@hugo.ai", "acadilhac@hugo.ai", "aharol@hugo.ai", "achisholm@hugo.ai", "bhachey@hugo.ai", "@id_aa_carmack", "@inc"], "sections": [{"heading": null, "text": "Keywords text generation, collective intelligence, evaluation"}, {"heading": "1. INTRODUCTION", "text": "Understanding the nature and scope of the manual effort is critical to prioritizing how the generation can be improved and which editing guidelines should be implemented. Recent work also indicates that some post-editing can be automated, with significant improvements in a common machine translation task (+ 5.5 BLEU score) [2]. Post-editing analysis also has a long history in evaluating text generation tasks, especially machine translation [1, 3]. The closest work to us [4] is post-editing 2,728 pairs of system-edited texts from a weather forecasting system, reporting a range of edits, including individual style preferences and corrections. We build on this work to perform post-editing analysis that complements existing assessment metrics such as ROUGE / BLEU. We are introducing a visualization that helps identify and prioritize improvements, and use data from a commercial biography system to provide them."}, {"heading": "2. SYSTEM OVERVIEW", "text": "Our case study evaluates a collective biography generation system that is part of a larger commercial tool for searching and summarizing information about a person on the Web. Input includes facts and events that are automatically derived from social and news sources (e.g. name, affiliations, education, skills, investments); facts and events are determined from a virtual tool that guides internal crowd users through the search process to select a range of relevant and diverse sources; the core generation capacity uses 26 templates to produce an average of 2.8 suggested sentences per biography; e.g.: Philip is the co-founder of High Fidelity, Inc. since January 2013 and an investor in Milk, Sunglass.io, Akili Interactive, Crowdfunder and more, and specializes in virtual worlds, start-ups and software development; he received a BS degree in physics from the University of California, San Diego in 1992."}, {"heading": "3. VISUALISING EDIT FLOW", "text": "Sentence alignment We first alignn proposed with final sets using a minimum Levenshtein ratio of 0.8, defined as (l (a + b) \u2212 d (a, b)) / l (a + b) where l (a + b) is the combined length of the input strings and d (a, b) is the Levenshtein edit distance between the input strings. Suggested biographies have a total of 28,842 sets and 525,555 tokens, while final biographies have a total of 33,238 sets and 580,521 tokens. So final biographies tend to have more sentences (3.2 versus 2.8), but each sentence is shorter on average (17.5 versus 18.2). Edit flow Figure 1 visualizes the flow of tokens from suggestion to final biography using a sankey chart. The total number of tokens in proposed sentences is divided into two categories: 231,347 (44%) are selected by users at Looking at 756% and 756% are full and 293%."}, {"heading": "4. CHARACTERISING EDITS", "text": "Selected From token alignments in selected sets, we follow a total of 1,833 substitution, 621 deletion and 468 insertion phrase pairs and 4,568, 1,343 and 1,284 cases. Overall, selection edits tend to lead to shorter sentences with an average of 1.8 tokens before they are edited, compared with 1.6 post-editing. Table 1 lists some of the most common editing actions. We observe that the top edits are generally grammatical (e.g. \"the Engineer '7,\" \"an Engineer\") or stylistic (e.g. \"received a BFA' 7,\" holds a BFA \"). Other edits deal with capitalization consistency or errors from collected facts (e.g.\" the University '7, \"the University\" microsoft office' 7, \"\" Microsoft Office \").Not selected by the set alignment threshold, we find that 67% of recommended sets that were some monentententsets with the final biography."}, {"heading": "5. DISCUSSION", "text": "It is encouraging that most new sentences contain proposed content, but these sentences still represent significant human costs, suggesting that end-to-end efficiency can be improved most by extending generation templates to provide editors with more style and content options, and then the next priority is to automate post-editing where possible, learned from data here or from external resources. Another problem that was discovered in the analysis is the lack of clear editorial guidelines, with users sometimes making contradictory edits. Editorial-style strategies are difficult to implement with crowd-sourced editing, but we expect the gradual introduction of highly precise rules should actively lead editors toward a more consistent style. In the current work, we are implementing the changes proposed above and developing a positive cycle to improve both efficiency and quality over time."}, {"heading": "6. REFERENCES", "text": "[1] W. Aziz, S. Castilho and L. Specia. PET: a tool for post-editing and assessing machine translation. In LREC, pp. 3982-3987, 2012. [2] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow, M. Huck, A. Jimeno Yepes, P. Koehn, V. Logacheva, C. Monz, M. Negri, A. Neveol, M. Neves, M. Popel, M. Post, R. Rubino, C. Scarton, L. Specia, M. Turchi, K. Verspoor, and M. Zampieri. Results of the 2016 Conference on Machine Translation. In WMT, pp. 131-198, 2016. [3] C. Scarton. Document-Level Discourse and Information on the Valuation of Language Editions. In NAACL Student Research Workshop, pp. 118-125, 2015. [4] S. G. Sripada, E. Reiter and L. Hawizy Evaluating Post-ILG, pp. 1700-ILG."}], "references": [{"title": "PET: a tool for post-editing and assessing machine translation", "author": ["W. Aziz", "S. Castilho", "L. Specia"], "venue": "LREC, pages 3982\u20133987", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Findings of the 2016 conference on machine translation", "author": ["O. Bojar", "R. Chatterjee", "C. Federmann", "Y. Graham", "B. Haddow", "M. Huck", "A. Jimeno Yepes", "P. Koehn", "V. Logacheva", "C. Monz", "M. Negri", "A. Neveol", "M. Neves", "M. Popel", "M. Post", "R. Rubino", "C. Scarton", "L. Specia", "M. Turchi", "K. Verspoor", "M. Zampieri"], "venue": "WMT, pages 131\u2013198", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Discourse and document-level information for evaluating language output tasks", "author": ["C. Scarton"], "venue": "NAACL Student Research Workshop, pages 118\u2013125", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluating an NLG system using post-editing", "author": ["S.G. Sripada", "E. Reiter", "L. Hawizy"], "venue": "IJCAI, pages 1700\u20131701", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 1, "context": "5 BLEU score) [2].", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "Post-edit analysis also has a long history in the evaluation of text generation tasks, in particular machine translation [1, 3].", "startOffset": 121, "endOffset": 127}, {"referenceID": 2, "context": "Post-edit analysis also has a long history in the evaluation of text generation tasks, in particular machine translation [1, 3].", "startOffset": 121, "endOffset": 127}, {"referenceID": 3, "context": "The closest work to ours [4] is a post-edit evaluation of 2,728 pairs of system-edited texts from a system for generating weather forecasts, reporting a range of edits including individual style preferences as well as corrections.", "startOffset": 25, "endOffset": 28}], "year": 2017, "abstractText": "Text generation is increasingly common but often requires manual post-editing where high precision is critical to end users. However, manual editing is expensive so we want to ensure this effort is focused on high-value tasks. And we want to maintain stylistic consistency, a particular challenge in crowd settings. We present a case study, analysing human post-editing in the context of a templatebased biography generation system. An edit flow visualisation combined with manual characterisation of edits helps identify and prioritise work for improving end-to-end efficiency and accuracy.", "creator": "LaTeX with hyperref package"}}}