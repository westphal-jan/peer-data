{"id": "1702.05860", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Robust Sparse Estimation Tasks in High Dimensions", "abstract": "In this paper we initiate the study of whether or not sparse estimation tasks can be performed efficiently in high dimensions, in the robust setting where an $\\eps$-fraction of samples are corrupted adversarially. We study the natural robust version of two classical sparse estimation problems, namely, sparse mean estimation and sparse PCA in the spiked covariance model. For both of these problems, we provide the first efficient algorithms that provide non-trivial error guarantees in the presence of noise, using only a number of samples which is similar to the number required for these problems without noise. In particular, our sample complexities are sublinear in the ambient dimension $d$. Our work also suggests evidence for new computational-vs-statistical gaps for these problems (similar to those for sparse PCA without noise) which only arise in the presence of noise.", "histories": [["v1", "Mon, 20 Feb 2017 05:22:55 GMT  (35kb)", "https://arxiv.org/abs/1702.05860v1", null], ["v2", "Tue, 28 Feb 2017 20:49:30 GMT  (35kb)", "http://arxiv.org/abs/1702.05860v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["jerry li"], "accepted": false, "id": "1702.05860"}, "pdf": {"name": "1702.05860.pdf", "metadata": {"source": "CRF", "title": "Robust Sparse Estimation Tasks in High Dimensions", "authors": ["Jerry Li"], "emails": ["jerryzli@mit.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.05 860v 2 [cs.L G] 28 February 20In this paper, we initiate the investigation of whether sparse estimation tasks can be performed efficiently in high dimensions, in a robust environment where a \u03b5-fraction of the samples is counterproductively corrupted. We investigate the natural robust version of two classically sparse estimation problems, namely sparse mean estimation and sparse PCA in the pointed covariance model. For both of these problems, we provide the first efficient algorithms that provide non-trivial error guarantees in the presence of noise, using only a number of samples similar to the number required for these problems without noise. In particular, our sample complexities in the ambient dimension d are sublinear. Our work also suggests evidence of new computational versus statistical gaps for these problems (similar to those for sparse PCA without noise) that occur only in the presence of noise."}, {"heading": "1 Introduction", "text": "Over the past few decades, there has been a great deal of work in machine learning and statistics on how to exploit thrift in high-dimensional data analysis. Motivated by the ever-increasing quantity and dimensionality of data, the goal at a high level is to use the underlying thrift of natural data to extract a number of samples that are sublinear in the dimensionality of data. In this paper, we will look at the unmonitored environment in which we have sample access to some distribution problems with underlying thrift, and our goal is to restore this distribution by exploiting this structure. Two natural and well-studied problems in this environment that exploit thrift and the PCA are thrifty. In both problems, the common theme is assumed to be finding a differentiated sparse direction of the Gaussian data."}, {"heading": "1.1 Our Contribution", "text": "We examine the natural robust versions of two classic, well-studied statistical tasks concerning economy, namely the low mean estimate and the sparse PCA. Here we obtain a series of d-dimensional samples from N (\u00b5, I), where there is an effective algorithm gap that gives a series of clues of size O (k 2 log d). Our main contribution is the following: Theorem 1.2 (informal, see theorem 2.1). There is an efficient algorithm that outputs a series of clues of size O (k 2 log d) from N (\u00b5, I), where the p-sparse, so that with a high probability that we will list the 2 \u2264. The restoration guarantee we achieve, namely O (p), is the optimal guarantee for the optimal guarantee."}, {"heading": "1.2 Related Work", "text": "As has already been mentioned, the number of unemployed in the United States has increased many times over the previous year."}, {"heading": "2 Definitions", "text": "If there is a matrix, we let the two distribution models impose on us a vector in which the two distribution patterns F, G over Rd, we let dTV (F, G) = 1 2, Rd - G, dx - the total variation distance between the two distribution patterns. We will study the following contamination model: Definition 2.1 (\u03b5 corruption). We will say a series of examples X1, X2, Xn is a set of corrupt examples from a distribution D if it is generated by the process."}, {"heading": "3 Preliminaries", "text": "In this section, we present technical preparatory work that we will need throughout the entire essay."}, {"heading": "3.1 Naive pruning", "text": "We will need the following (straightforward) pre-processing subroutine of [DKK + 16] to remove all dots that are more than (d) from the true meaning.Fact 3.1 (c.f. Fact 4.18 in [DKK + 16]).Letter X1,.., Xn is one of \u03b5-corrupt samples of N (\u00b5, I), and leave (GDP > 0).There is an algorithm NAIVEPRUNE (X1,..., Xn, \u03b4) that expires in O (\u03b5d2n2) time, so that we most likely (1) NAIVEPRUNE will not remove any uncorrupted dots, and (2) if Xi is not removed from NAIVEPRUNE, then \"Xi \u2212 \u00b5\" is 2 \u2264 O (\u0438 d log (n / \u043c)).If these two conditions occur, we will say that NAIVEPRUNE is successful."}, {"heading": "3.2 Concentration inequalities", "text": "In this section we specify some concentration inequalities that we need in the rest of the work. These \"pro-vector\" and \"pro-matrix\" concentration guarantees are known and result from (scalar) Chernoff limits, see e.g. [DKK + 16].Fact 3.2 (pro-vector-gauss concentration).Fix \u03b5, \u03b4 > 0. Let v-Rd become a unit vector, and let X1,.., Xn-N (0, I), wheren = (log 1 / 2).Then we have probability 1 \u2212 \u03b4, and assume that M-Rd \u00b7 d will be a symmetric matrix."}, {"heading": "3.3 The set Sn,\u03b5", "text": "We define for each n, \u03b5 the setSn, \u03b5 = {w-Rn: n-\u03b5 i = 1wi = 1 and 0 \u2264 wi \u2264 1 (1 \u2212 \u03b5) n, \u0435i}. (1) We make the following observation: If we allow myself to be the vector whose ith coordinate is 1 / | I |, if I am I and 0 otherwise, we have Sn, \u03b5 = conv {wI: | I | = (1 \u2212 \u03b5) n}. The amount Sn, \u03b5 plays a key role in our algorithms. We consider elements in Sn, \u03b5 as weights that we place on our sample points, where a higher weight indicates a higher confidence that the sample is intact, and a lower weight indicates a higher confidence that the sample is corrupted."}, {"heading": "4 Concentration for sparse estimation problems via dual norms", "text": "In this section, we give a neat way to detect concentration limits for various objects occurring in sparse PCA and sparse mean estimation problems by observing that these are cases of a very general \"meta-algorithm,\" which we call dual standard maximization, but this will prove crucial to prove the correctness of our algorithms for robust sparse recovery. This may sound similar to the \"dual certificate techniques\" commonly used in the sparse estimation literature, but these techniques are actually quite different. Definition 4.1 (dual standard maximization) is a Hilbert space with an internal product < \u00b7, \u00b7 >. Fix any amount of S H. Then, the dual standard produced by S is defined by 4.1 (dual standard maximization) S = supy S | < x, y > |. The dual standard mixer by Vgxik, we are referred to as &lt.u = < S = < S = dx)."}, {"heading": "4.1 Concentration for dual norm maximization", "text": "We now show how the above concentration imbalances allow us to derive very strong concentration results for the dual norm. (...) We show how the above concentration imbalances allow us to achieve very strong concentration results for the dual norm. (...) In fact, these results are crucial for the adjustment of the convex programming framework for economical estimation tasks. (...) In addition, they allow us to provide a simple proof that the L1-relaxation for economical PCA.Corollary 4.1 works. (...) We leave X1,. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (.). (...). (...). (...). (...). (...). (...).). (. (.). (.).). (.). (.).).). (...). (. (.). (.). (.).). (.).).).). (...). (...).). (. (...).). (. (.).). (. (.).). (. (.).)."}, {"heading": "4.2 Concentration for Sn,\u03b5", "text": "We will demand the following concentration inequalities for weighted sums from Gaussians, with the weights coming from Sn, \u03b5, since these objects will naturally occur in our algorithms. We will follow these limits by applying the above limits and then carefully defining the limits of all possible subsets of (n) subsets. We must be careful here, as the number of things we limit by increments as n increases. We will include the proofs in Appendix C. Theorem 4.5. Let us fix the limits of (n) 1 / 2 and (n) 1 and (n) and fix k (n). There is a protocol 1 = O (n), so that for each individual event > 1, ifX1,. \u2212 Xn, N (0, I) and n = (min (d, k2) + protocol (d 2 k2) + protocol 1 / 2) and protocol 1 / 3 (n)."}, {"heading": "5 A robust algorithm for robust sparse mean estimation", "text": "(This section is dedicated to the description of an algorithm RECOVERROBUSTSMEAN for the robust learning of Gaussian sequence models and the proof of the following theorem: Theorem 5.1. Fix \u03b5, \u03c4 > 0. Leave \u03b7 = O (\u03b5 \u221a log 1 / \u03b5). In view of an \u03b5-corrupted set of size models n of N (\u00b5, I), where \u00b5 is k-sparsen = \u044b (min (k2, d) + log (d2k2) + log 1 / \u03c4\u03b72), RECOVERROBUSTSMEAN then issues a \u00b5, so that we have such an analogy with probability 1 \u2212 \u03c4. \u2212 GDP \u2212 \u00b5 2 \u2264 Xi \u2212 olacle. \u2212 Our algorithm is based on the convex programming framework developed in [DKK + 16]. Roughly speaking, the algorithm behaves as follows."}, {"heading": "5.1 Additional preliminaries", "text": "During this section, we let X1,.., Xn denote a series of faulty samples from N (\u00b5, I), where \u00b5 k-sparse. We let Sgood denote the set of undamaged samples, and we let Sbad denote the set of damaged samples. For each set of weights w-Sn, \u03b5, we let wg = i-Sgood wi and w-Sbad wi. During this section, we will condition the following three occurring deterministic events: NAIVEPRUNE (X1,.,., Xn) succeeds, (6) succeeds, (6) succeeds Sgood wi (Xi \u2212 \u00b5), Uk, (w-Sn), Sn, 2\u03b5, and (7) succeeds, (Xi \u2212 Sgood wi (Xi \u2212 \u00b5) T-wgI if it is good."}, {"heading": "5.2 The separation oracle", "text": "Our main result in this section is the description of a polynomial time algorithm (ROBUSTSMEANORACLE)."}, {"heading": "5.3 Putting it all together", "text": "We now have the ingredients to prove our main theorems. In view of what we have, our complete algorithm RECOVERROBUSTSMEAN is also simple: first we run NAIVEPRUNE, then we run APPROXRECOVERROBUSTSMEAN on the basis of the given points to output a series of weight values. We then output a series of weight values. (X1,.,.) The algorithm is formally defined in Algorithm 2.Algorithm 2. (Xn,.) An efficient algorithm for robust, sparse estimation 1: Function RECOVERROBUSTSMEAN (X1,.,.,.) 2: Let S be the set output by NAIVEPRUNE (X1,.,.,.). WLOG assumes that we have S = [n]. 3: Let w \u00b2 = APPROXCOVERROBUSTMESTAN (2)."}, {"heading": "6 An algorithm for robust sparse PCA detection", "text": "In this section, we specify an efficient algorithm for detecting a spiked covariance matrix in the presence of opposing noise. Our algorithm is relatively simple: We ask for the amount of weights w-Sn, \u03b5, so that the empirical second moment with these weights shows a minimal deviation from the identity in the dual Xk norm. We can write this as a convex program. Then, we check the value of the optimal solution of this convex program. If this value is small, then we say that it is N (0, I). If this value is large, then we say that it is N (0, I + \u03c1vvvT). We refer to the former as case 1 and the latter as case 2. The formal description of this algorithm is given in the algorithm. Algorithm 3 Learning a spiked covariance model, then we say that it is N (0, I + \u03c1vvvT). We refer to the former as case 1 and the latter as case 2. We refer to this covariance algorithm as the latter and we say that in the EC1 coalgorithm is ST1 and the latter as the description of the EC1."}, {"heading": "6.1 Implementing DETECTROBUSTSPCA", "text": "First, we show that the above algorithm can be implemented efficiently. In fact, by taking the dual of the SDP that defines the dual SDP solution techniques, we can rewrite this problem as an SDP with (up to constant factor blow-ups) the same number of constraints and variables, and that we can therefore solve it using traditional SDP solution techniques. Alternatively, we can observe that to optimize algorithm 4 using ellipsoid or cutting-plane methods, it is sufficient to create a separating hyperplane for the constraint (11), which is exactly what dual-standard maximization allows us to do efficiently. It is straightforward to show that the volume of Sn, \u03b5 \u00d7 Xk is most exponential in the relevant parameters."}, {"heading": "6.2 Proof of Theorem 2.2", "text": "We now show that Algorithm 4 provides the guarantees required for Theorem 2. First, we show that when we are in Case 1, it is small: Lemma 6.1. Let it happen. Let it happen. Let it happen. Let it happen. Let it happen. Let it happen. Let it happen. Let it happen. Let it happen. Let it happen. Let it happen. Let it happen. Let it happen. Let it happen. Let it happen. Let it happen. Let it happen. Let it happen. Let it happen."}, {"heading": "7 An algorithm for robust sparse PCA recovery", "text": "Perhaps the first naive attempt would be simply to run the same SDP in (11) and hope that the dual-standard maximizer will provide you with enough information to restore the hidden sting, which would more or less correspond to the simplest modification of the sparse PCA in the unrugged environment that one might hope would not provide trivial information in that environment, but this cannot work for the following simple reason: The value of the SDP is always at least O (\u03c1), as we argued in Section 6. Therefore, noise can pretend to be another sparse vector u orthogonal to v, so that the covariance with noise looks like wg (I + \u03c1vvT) + wg\u03c1uT, so that the value of the SDP can be minimized with the uniform set of weights. Then, it is certainly proven that both vvvT and uT are small dual SDP forces, so we make dual maximization impossible."}, {"heading": "7.1 The algorithm", "text": "Our algorithms and analyses will make decisive use of the following convex set, which represents a further relaxation of Xk: W (2) k = {X) Rd \u00b7 d: tr (X) \u2264 2, HB (X) 2 \u2264 1, HB (X) 1 \u2264 3k, X 0}.Our algorithm, formally specified in Algorithm 4, will look like this: We will solve a convex program that simultaneously selects a weight in Sn, \u03b5 and a matrix A (Wk) to minimize the Wk distance between the sample covariance with these weights and A. Our output will then be only the uppermost eigenvector of A.Algorithm 4 Learning a spiked covariance model, robustly 1: Function RECOVERROBUSTSPCA (X1,.., Xn) before sample covariance with these weights, and A. Our output will then be only the uppermost eigenvector of A.Algorithm 4 Learning a USPROBUSTRUSTRUSTRUST1 function (X1)."}, {"heading": "7.2 More concentration bounds", "text": "Before we can prove the correctness of our algorithm, we need a few concentrations of imbalances for the set Wk.Lemma 7.1. Fix \u03b5, \u03b4 > 0. Leave X1,. \u2212 I. \u2212 Z \u2212 Z \u2212 Z \u2212 N (0, I), where n as in Theorem is 4.2. Note then that the probability is 1 \u2212 Z \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 i = 0 \u2212 iX2i + 1k \u00b2. Furthermore, for every i \u2212 Z \u2212 Z \u00b2 n \u00b2 n \u00b2, if we have taken = min (2 i + 1k) 2) + log (2i + 1k) 2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2, \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2, \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2, \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2, \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "7.3 Proof of Theorem 2.3", "text": "In the rest of this section we will respond to the following deterministic event: Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption (Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption = Area consumption + Area consumption (Area consumption + Area consumption). The rest of this section is dedicated to the proof of the following theorems, which directly implies Theorem2.3.Theorem 7.3. Fix \u03b5, and let the area consumption, area consumption, area consumption, and / or:"}, {"heading": "Acknowledgements", "text": "The author would like to thank Ankur Moitra for helpful advice throughout the project and Michael Cohen for some surprisingly useful conversations."}, {"heading": "B Omitted Details from Section 4", "text": "In this section, we will briefly consider the well-known, non-robust algorithms for the sparse recovery and for the sparse PCA and write them with our language. It turns out that the simple thresholds THRESHOLDMEAN in algorithm 5 are not sufficient to achieve recovery: Algorithm 5 Thresholding for the sparse mean estimate 1: function THRESHOLDMEAN (X1) 2: Let us allow it. (Let S have the series of k coordinates with the greatest magnitude 4: Let us. (Let us leave it that way)."}, {"heading": "C Omitted Proofs from Section 4", "text": "The proof for theorem 4.5. Fix n as in theorem 4.5, and leave \u03b41 = (n \u03b5n) \u2212 1 \u03b4. By convexity of Sn, \u03b5 and the objective function, it is sufficient to show that with probability 1 \u2212 \u03b5 the following applies: By convexity of Sn, \u03b5 and the objective function, the following applies: By convexity of Sn, \u03b5 and the objective function, i = 1wiXi. (19) By convexity 4.1, this occurs with probability 1 \u2212 O (Connecticut).Fix any I [n], so that I | = (1 \u2212 \u03b5) n. Applied by convexposition 4.1 to Ic, we have that there is some universal constant C, so long as the constant C \u00b7 min (d, k \u00b2) + protocol (n)."}, {"heading": "D Computational Barriers for sample optimal robust sparse mean estimation", "text": "We suspect that the rate achieved by Theorem 5.1 for computationally efficient algorithms (down to log factors) is narrow. Intuitively, the biggest difficulty is that the distinction between N (\u00b51, I) and N (\u00b52, I) in corrupt samples seems to require second-moment (or higher) information by nature, for any first-moment information alone this is undoubtedly insufficient. In this sparse environment, this is very problematic, as it naturally requires us to detect a large sparse eigenvector of empirical covariance, which is more or less reduced to the problem solved by (16). This, in turn, requires us to resort to the problem solved by SDPs for sparse PCAs, which we know that second-moment samples (k2 log d / \u03b52) are necessary to detect non-trivial behavior."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In this paper we initiate the study of whether or not sparse estimation tasks can be performed efficiently in high dimensions, in the robust setting where an \u03b5-fraction of samples are corrupted adversarially. We study the natural robust version of two classical sparse estimation problems, namely, sparse mean estimation and sparse PCA in the spiked covariance model. For both of these problems, we provide the first efficient algorithms that provide non-trivial error guarantees in the presence of noise, using only a number of samples which is similar to the number required for these problems without noise. In particular, our sample complexities are sublinear in the ambient dimension d. Our work also suggests evidence for new computational-vs-statistical gaps for these problems (similar to those for sparse PCA without noise) which only arise in the presence of noise.", "creator": "LaTeX with hyperref package"}}}