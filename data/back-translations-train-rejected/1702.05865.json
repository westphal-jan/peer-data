{"id": "1702.05865", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Hemingway: Modeling Distributed Optimization Algorithms", "abstract": "Distributed optimization algorithms are widely used in many industrial machine learning applications. However choosing the appropriate algorithm and cluster size is often difficult for users as the performance and convergence rate of optimization algorithms vary with the size of the cluster. In this paper we make the case for an ML-optimizer that can select the appropriate algorithm and cluster size to use for a given problem. To do this we propose building two models: one that captures the system level characteristics of how computation, communication change as we increase cluster sizes and another that captures how convergence rates change with cluster sizes. We present preliminary results from our prototype implementation called Hemingway and discuss some of the challenges involved in developing such a system.", "histories": [["v1", "Mon, 20 Feb 2017 05:51:18 GMT  (1623kb,D)", "http://arxiv.org/abs/1702.05865v1", "Presented at ML Systems Workshop at NIPS, Dec 2016"]], "COMMENTS": "Presented at ML Systems Workshop at NIPS, Dec 2016", "reviews": [], "SUBJECTS": "cs.DC cs.AI cs.LG", "authors": ["xinghao pan", "shivaram venkataraman", "zizheng tai", "joseph gonzalez"], "accepted": false, "id": "1702.05865"}, "pdf": {"name": "1702.05865.pdf", "metadata": {"source": "CRF", "title": "Hemingway: Modeling Distributed Optimization Algorithms", "authors": ["Xinghao Pan", "Shivaram Venkataraman", "Zizheng Tai", "Joseph Gonzalez"], "emails": ["jegonzal}@cs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most people who are able to survive themselves, to survive themselves, \"he told the German press agency in an interview with the\" Frankfurter Allgemeine Zeitung \"(Monday).\" I do not believe that we will be able to change the world, \"he told the\" Welt am Sonntag. \"\" I do not believe that we will be able to change the world, \"he told the\" Welt am Sonntag, \"\" die Welt am Sonntag, \"\" die Welt am Sonntag, \"\" die Welt am Sonntag, \"\" die Welt am Sonntag, \"\" die Welt am Sonntag, \"\" die Welt am Sonntag, \"\" die Welt am Sonntag, \"\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Welt am Sonntag, \"die Welt am Welt am Sonntag,\" die Welt am Welt am Sonntag, \"die Welt am Welt am Sonntag,\" die Welt am Welt am Sonntag, \"die Welt am Welt am Sonntag,\" die Welt am Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag,\" die Welt am Sonntag, \"die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag, die Welt am Sonntag,"}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Distributed computing", "text": "The widespread adoption of cloud computing platforms such as Amazon EC2, Microsoft Azure, Google Compute Engine, etc., means that users can now choose their computing substrate in terms of the number of cores, memory per machine, and also the number of computers to use. However, additional choices come with their own challenges; the performance of machine learning can vary significantly depending on the resources chosen, and thus a number of recent efforts [13, 31] focus on recommending the best cluster configuration for a given workload. One of the important decisions users have to make is the choice of cluster size or degree of parallelism to use. A higher degree of parallelism usually reduces computing time, but could increase the time spent on communication. Especially in the context of iterative optimization algorithms, the degree of parallelism varies according to iteration, and we examine its effects in Section 2.3."}, {"heading": "2.2 Distributed optimization algorithms", "text": "Large-scale optimization algorithms used in practice include best-in-class methods based on parallel SGD [35, 5, 36], coordinate descend methods [14, 19, 32], and quasi-Newton methods such as L-BFGS [22, 23]. These algorithms are typically iterative, and each iteration can be expressed by a bulk-synchronous step in a distributed framework such as Hadoop MapReduce or Spark [33]. One of the main differences between the different algorithms is how their convergence rates change as the degree of parallelism increases, because methods such as Full Descent (GD), in which the gradient is evaluated on all data points in each iteration, the convergence rate remains independent of parallelism. However, this is not the case for stochastic methods such as Mini-Batch SGD. For Mini-Batch SGD with Batch-Size, the miter error is 18 after it has been performed for IT only."}, {"heading": "2.3 Case Study", "text": "To emphasize convergence and performance differences, we perform a simple experiment to predict a single digit (5) with the MNIST dataset. We have CoCoA [14] on Apache Spark [33] with the linear cluster in which each machine has 48 cores, and also calculate the primary sub-optimality at the end of each iteration. We vary the degree of parallelism in which each machine has 48 GB of RAM and 480 GB of RAM."}, {"heading": "3 Modeling Optimization Algorithms", "text": "In the previous section, we have seen how the changing degree of parallelism affects performance and convergence. Next, we describe our approach to solving this problem at Hemingway. We start with high goals for our system and then discuss how to divide the problem into two parts: modeling the system and modeling the algorithm."}, {"heading": "3.1 Goals and assumptions", "text": "The goal of our system, Hemingway, is to develop an interface through which the user can specify an end-to-end requirement and the system automatically selects the appropriate algorithm and degree of parallelism to be used. Such a system can be used in conjunction with existing libraries of machine learning algorithms such as MLlib [20], Vowpal Wabbit [16], SystemML [11], etc. Specifically, we would like to support use cases of the form: for a relative error target, we choose the fastest algorithm and configuration; or for a target latency of t-seconds, we choose an algorithm that achieves the minimum training loss. An idealized example of our system is Figure 2.Our assumption is that the data set used by the distributed optimization algorithm is static and that the algorithm is iterative with each iteration expressed as synchronous parallelism (BSP)."}, {"heading": "3.2 Overall model", "text": "Our goal is to develop a model that can predict the value of h so that we can compare different configurations and optimization algorithms. Our main finding in this work is that we can divide this task into two models: a system model f (m), which returns the time per iteration of given m machines, and a convergence model g (i, m), which predicts the objective value after i-iterations of given m machines. Thus, the overall model can be obtained by combining our two models above: h (t, m) = g (t / f (m), m). The main advantage of this approach is that we can train both models independently and reuse them based on changes. For example, if there are new types of machine or network hardware changes in a data center, we can only retrain the system and reuse the convergence model."}, {"heading": "3.2.1 Modeling the system", "text": "In order to build the model at the system level, we propose the reuse of the approach in Ernest [31], a performance prediction framework that minimizes the time and resources spent on building a performance model. We summarize the main ideas in Ernest below and in particular how they are applied to ML algorithms. The response variable in our model is the runtime f (m) of each BSP iteration as a function of the number of machines. The functional form of f is guided by different components and, like they2 In a BSP job, machines perform iterative local calculations before communicating information (typically an updated model or history) with other machines. It is crucial that a synchronization barrier ensures that all machines complete their calculations and communications for the IERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERIERI"}, {"heading": "3.2.2 Modeling algorithm convergence", "text": "Most common optimization algorithms today take an iterative approach to minimizing an objective value. We capture this behavior with a bivariable function g (i, m), which can help us compile a functional value for g (i, m) after the algorithm for i-iterations has been executed on m machines. Optimization algorithms are typically accompanied by analyses of convergence rates of upper limits; these rates can help us compile a functional form for g (i, m). CoCoA, for example, has a convergence rate of upper limits of g (i, m) \u2264 (1 \u2212 c0m) i c1, where c0 and c1 are data-dependent constants. However, we point out that the actual observed convergence rates may differ from the theoretical upper limits, so it is important not to excessively restrict g's functional form. In this paper, we have adopted linear forms of g to facilitate adjustment to the smallest or smallest of squares."}, {"heading": "4 Preliminary Experiments With CoCoA+", "text": "We demonstrated Hemingway's ability to model the convergence of algorithms by adapting a linear model to a sample run of CoCoA +. We used CoCoA + to solve a binary classification problem on MNIST, and varied the degree of parallelism m from 1 to 128 in powers of 2. The algorithm was terminated when the original sub-optimality reached 1e \u2212 4 or after 500 iterations. We then adapted a linear model to log using LassoCV from scikit-learn [24] (P (i, m) \u2212 P terms were used as features of our model. We also collected data to estimate the Ernest system model using Amazon EC2 R3.xlarge instances with 30.5 GB of RAM and 4 cores each.Figure 3 (a) shows that Hemb + is applicable to convergence."}, {"heading": "4.1 Predicting for unobserved degree of parallelism", "text": "In the first scenario, we have gathered convergence results for some values of m and would like to predict the convergence trend for a degree of parallelism not yet observed. This can be simulated by a4. Normally, we are interested in the objective value, but the Hemingway approach is data-driven and therefore sufficiently flexible to handle other metrics such as test classification accuracy, precision, retrieval, etc. 5Diagrams showing the first 100 iterations are provided in Appendix A.leave-one-m-out cross-validation with our data.For example, we forecast the convergence g (i, 128) for m = 128 parallelism using data from m = 1, 2, 4, 8, 16, 32, 64. Figure 5 4 (a) shows that the resulting cross-validated models are well suited for true convergence. Therefore, we are able to estimate the trend of convergence for non-observed values."}, {"heading": "4.2 Forward prediction", "text": "Secondly, we look at predicting the convergence of future iterations. Figures 5 (a) and 5 (b) show the suitability of models to predict 1 and 10 iterations in the past over a time window of 50 iterations. In both cases, predictions become more accurate when i is large enough to provide enough information for modeling. We also applied the Ernest and Hemingway models in combination to predict convergence in the future. Figures 5 (a) and 6 (b) show that our models can capture convergence trends at 1s and 5s in the future."}, {"heading": "5 Related Work", "text": "Similar decoupling of systems and algorithmic efficiencies have been proposed in DimmWitted [34] to study runtimes of optimization algorithms in NUMA machines. Tradeoffs of different access methods and data and model replication techniques have been studied in terms of their statistical and hardware efficiencies. In Hemingway, we also propose decoupling the statistical and hardware execution models, but in addition, we propose an automatic method to select the best configuration in the face of a new algorithm. A number of previous studies have optimized the performance of large-scale machine learning algorithms. On the system side, physical execution motors can choose between row and column layouts."}, {"heading": "6 Challenges", "text": "While our initial experiments are promising in terms of the benefits and insights that can be gained from modeling convergence rates, there are a number of challenges that we are addressing to make this system practicable.Training time. One of the important aspects of any modeling is based on the amount of time it takes to learn the model before it makes useful suggestions to the user. Training resources are closely related to the time it takes to learn a model, which is especially important for cloud computing settings to minimize data collection time."}, {"heading": "7 Conclusion", "text": "In this paper, we have examined how distributed optimization algorithms scale in terms of performance and convergence rate, and demonstrated how choosing the optimal configuration could significantly affect training time. To achieve this, we propose Hemingway, a system that models the convergence rate for distributed optimization algorithms, and we present some of the challenges associated with building such a system."}, {"heading": "A Additional Plots For CoCoA+ Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "et al", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo"], "venue": "TensorFlow: Large-scale machine learning on heterogeneous systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to compose neural networks for question answering", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "arXiv preprint arXiv:1601.01705", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "and N", "author": ["M. Andrychowicz", "M. Denil", "S. Gomez", "M.W. Hoffman", "D. Pfau", "T. Schaul"], "venue": "de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, pages 3981\u20133989", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Designing neural network architectures using reinforcement learning", "author": ["B. Baker", "O. Gupta", "N. Naik", "R. Raskar"], "venue": "arXiv preprint arXiv:1611.02167", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "COMPSTAT", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "C", "author": ["T. Chen", "M. Li", "Y. Li", "M. Lin", "N. Wang", "M. Wang", "T. Xiao", "B. Xu"], "venue": "Zhang, , and Z. Zhang. MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems. In Workshop on Machine Learning Systems (NIPS)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "and N", "author": ["Y. Chen", "M.W. Hoffman", "S.G. Colmenarejo", "M. Denil", "T.P. Lillicrap"], "venue": "de Freitas. Learning to learn for global optimization of black box functions. arXiv preprint arXiv:1611.03824", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "Journal of Machine Learning Research, 13(Jan):165\u2013202", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "RL\u0302 2: Fast reinforcement learning via slow reinforcement learning", "author": ["Y. Duan", "J. Schulman", "X. Chen", "P.L. Bartlett", "I. Sutskever", "P. Abbeel"], "venue": "arXiv preprint arXiv:1611.02779", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, 12:2121\u20132159", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Systemml: Declarative machine learning on mapreduce", "author": ["A. Ghoting", "R. Krishnamurthy", "E. Pednault", "B. Reinwald", "V. Sindhwani", "S. Tatikonda", "Y. Tian", "S. Vaithyanathan"], "venue": "ICDE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Omnivore: An optimizer for multi-device deep learning on cpus and gpus", "author": ["S. Hadjis", "C. Zhang", "I. Mitliagkas", "D. Iter", "C. Re"], "venue": "arXiv preprint arXiv:1606.04487", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "No one (cluster) size fits all: automatic cluster sizing for data-intensive analytics", "author": ["H. Herodotou", "F. Dong", "S. Babu"], "venue": "SOCC", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["M. Jaggi", "V. Smith"], "venue": "Communication-efficient distributed dual coordinate ascent. In NIPS", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "STRADS: A Distributed Framework for Scheduled Model Parallel Machine Learning", "author": ["J.K. Kim", "Q. Ho", "S. Lee", "X. Zheng", "W. Dai", "G.A. Gibson", "E.P. Xing"], "venue": "Eurosys, pages 5:1\u20135:16", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "and A", "author": ["J. Langford", "L. Li"], "venue": "Strehl. Vowpal wabbit online learning project", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "et al", "author": ["M. Li", "D.G. Andersen"], "venue": "Communication efficient distributed machine learning with the parameter server. In NIPS", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient mini-batch training for stochastic optimization", "author": ["M. Li", "T. Zhang", "Y. Chen", "A.J. Smola"], "venue": "KDD, pages 661\u2013670", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["C. Ma", "V. Smith"], "venue": "Adding vs. averaging in distributed primal-dual optimization. In ICML", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["X. Meng", "J. Bradley", "B. Yuvaz", "E. Sparks", "S. Venkataraman", "D. Liu", "J. Freeman", "D. Tsai", "M. Amde", "S. Owen"], "venue": "Mllib: Machine learning in apache spark. Journal of Machine Learning Research, 17(34)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Asynchrony begets momentum", "author": ["I. Mitliagkas", "C. Zhang", "S. Hadjis", "C. Re"], "venue": "with an application to deep learning. arXiv preprint arXiv:1605.09774", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Res: Regularized stochastic bfgs algorithm", "author": ["A. Mokhtari", "A. Ribeiro"], "venue": "IEEE Transactions on Signal Processing, 62(23)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "A linearly-convergent stochastic l-bfgs algorithm", "author": ["P. Moritz", "R. Nishihara", "M.I. Jordan"], "venue": "AISTATS", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, 12:2825\u20132830", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Sdna: stochastic dual newton ascent for empirical risk minimization", "author": ["Z. Qu", "P. Richt\u00e1rik", "M. Tak\u00e1\u010d", "O. Fercoq"], "venue": "arXiv preprint arXiv:1502.02268", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "Advances in Neural Information Processing Systems, pages 693\u2013701", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Journal of Machine Learning Research", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Keystoneml: Optimizing pipelines for large-scale advanced analytics", "author": ["E.R. Sparks", "S. Venkataraman", "T. Kaftan", "M.J. Franklin", "B. Recht"], "venue": "arXiv preprint arXiv:1610.09451", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "A bridging model for parallel computation", "author": ["L.G. Valiant"], "venue": "Communications of the ACM, 33(8):103\u2013111", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1990}, {"title": "Ernest: Efficient Performance Prediction for Large-Scale Advanced Analytics", "author": ["S. Venkataraman", "Z. Yang", "M. Franklin", "B. Recht", "I. Stoica"], "venue": "NSDI", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["T. Yang"], "venue": "NIPS", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing", "author": ["M. Zaharia", "M. Chowdhury", "T. Das", "A. Dave", "J. Ma", "M. McCauley", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "NSDI", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Dimmwitted: A study of main-memory statistical analytics", "author": ["C. Zhang", "C. R\u00e9"], "venue": "PVLDB, 7(12):1283\u20131294", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Splash: User-friendly Programming Interface for Parallelizing Stochastic Algorithms", "author": ["Y. Zhang", "M.I. Jordan"], "venue": "CoRR, abs/1506.07552", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["M.A. Zinkevich", "M. Weimer"], "venue": "Parallelized stochastic gradient descent. In NIPS", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 29, "context": "With growing data sizes and the advent of cloud computing infrastructure [31], distributed machine learning is used in a number of applications like machine translation, computer vision, speech recognition etc.", "startOffset": 73, "endOffset": 77}, {"referenceID": 33, "context": "As a result, recent research has proposed a number of distributed optimization algorithms [35, 14, 10] that handle large input datasets [5] and minimize communication [32, 17, 14] to scale across large clusters.", "startOffset": 90, "endOffset": 102}, {"referenceID": 13, "context": "As a result, recent research has proposed a number of distributed optimization algorithms [35, 14, 10] that handle large input datasets [5] and minimize communication [32, 17, 14] to scale across large clusters.", "startOffset": 90, "endOffset": 102}, {"referenceID": 9, "context": "As a result, recent research has proposed a number of distributed optimization algorithms [35, 14, 10] that handle large input datasets [5] and minimize communication [32, 17, 14] to scale across large clusters.", "startOffset": 90, "endOffset": 102}, {"referenceID": 4, "context": "As a result, recent research has proposed a number of distributed optimization algorithms [35, 14, 10] that handle large input datasets [5] and minimize communication [32, 17, 14] to scale across large clusters.", "startOffset": 136, "endOffset": 139}, {"referenceID": 30, "context": "As a result, recent research has proposed a number of distributed optimization algorithms [35, 14, 10] that handle large input datasets [5] and minimize communication [32, 17, 14] to scale across large clusters.", "startOffset": 167, "endOffset": 179}, {"referenceID": 16, "context": "As a result, recent research has proposed a number of distributed optimization algorithms [35, 14, 10] that handle large input datasets [5] and minimize communication [32, 17, 14] to scale across large clusters.", "startOffset": 167, "endOffset": 179}, {"referenceID": 13, "context": "As a result, recent research has proposed a number of distributed optimization algorithms [35, 14, 10] that handle large input datasets [5] and minimize communication [32, 17, 14] to scale across large clusters.", "startOffset": 167, "endOffset": 179}, {"referenceID": 29, "context": "As the cluster size increases the computation time decreases but the communication time increases and thus choosing an optimal cluster size is important for optimal performance [31].", "startOffset": 177, "endOffset": 181}, {"referenceID": 13, "context": "For example, in CoCoA [14], a communication efficient dual coordinate ascent algorithm, each machine executes a local learning procedure and the resulting dual vectors are then averaged across machines at the end of each iteration.", "startOffset": 22, "endOffset": 26}, {"referenceID": 4, "context": ", first-order methods [5] vs.", "startOffset": 22, "endOffset": 25}, {"referenceID": 24, "context": "second-order methods [26]), it is often hard to predict which algorithm will be the most appropriate for a given cluster setup.", "startOffset": 21, "endOffset": 25}, {"referenceID": 29, "context": "To do this we propose modeling the convergence rate of algorithms as we scale the cluster size and we split our modeling into two parts: based on Ernest [31], we first build a computational model that helps us understand how the time taken per-iteration varies as we increase the scale of computation; we then build a separate model for the convergence rate as we scale the computation and we show how combining these two models can help us determine the optimal configuration.", "startOffset": 153, "endOffset": 157}, {"referenceID": 18, "context": "We propose Hemingway, a prototype implementation and present initial evaluation results from running our system with CoCoA+ [19].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "However having additional choice comes with its own challenges; the performance of machine learning jobs can vary significantly based on the resources chosen and thus a number of recent efforts [13, 31] have focused on recommending the best cluster configuration for a given workload.", "startOffset": 194, "endOffset": 202}, {"referenceID": 29, "context": "However having additional choice comes with its own challenges; the performance of machine learning jobs can vary significantly based on the resources chosen and thus a number of recent efforts [13, 31] have focused on recommending the best cluster configuration for a given workload.", "startOffset": 194, "endOffset": 202}, {"referenceID": 33, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 103, "endOffset": 114}, {"referenceID": 4, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 103, "endOffset": 114}, {"referenceID": 34, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 103, "endOffset": 114}, {"referenceID": 13, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 143, "endOffset": 155}, {"referenceID": 18, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 143, "endOffset": 155}, {"referenceID": 30, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 143, "endOffset": 155}, {"referenceID": 21, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 193, "endOffset": 201}, {"referenceID": 22, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 193, "endOffset": 201}, {"referenceID": 31, "context": "These algorithms are typically iterative and each iteration can be expressed using a bulksynchronous step in a distributed framework like Hadoop MapReduce or Spark [33].", "startOffset": 164, "endOffset": 168}, {"referenceID": 17, "context": "For mini-batch SGD with batch size b, the optimization error after running for T iterations is O(1/ \u221a bT + 1/T ) [18, 8].", "startOffset": 113, "endOffset": 120}, {"referenceID": 7, "context": "For mini-batch SGD with batch size b, the optimization error after running for T iterations is O(1/ \u221a bT + 1/T ) [18, 8].", "startOffset": 113, "endOffset": 120}, {"referenceID": 13, "context": "Recent work in CoCoA [14], CoCoA+ [19] perform local updates using coordinate descent [28] and obtain convergence rates that only degrade with the number of machines rather than the mini-batch size.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "Recent work in CoCoA [14], CoCoA+ [19] perform local updates using coordinate descent [28] and obtain convergence rates that only degrade with the number of machines rather than the mini-batch size.", "startOffset": 34, "endOffset": 38}, {"referenceID": 26, "context": "Recent work in CoCoA [14], CoCoA+ [19] perform local updates using coordinate descent [28] and obtain convergence rates that only degrade with the number of machines rather than the mini-batch size.", "startOffset": 86, "endOffset": 90}, {"referenceID": 33, "context": "Similar rates have also been shown for re-weighted stochastic gradient methods [35].", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "We ran CoCoA [14] on Apache Spark [33] with linear SVM as the loss function.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "We ran CoCoA [14] on Apache Spark [33] with linear SVM as the loss function.", "startOffset": 34, "endOffset": 38}, {"referenceID": 29, "context": "Similar effects have been observed in prior work for larger datasets as well [31].", "startOffset": 77, "endOffset": 81}, {"referenceID": 33, "context": "Figure 1(c) shows the convergence for CoCoA, CoCoA+, Splash [35] and parallel SGD with local updates while using 16 cores.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Such a system can be used along with existing libraries of machine learning algorithms like MLlib [20], Vowpal Wabbit [16], SystemML [11] etc.", "startOffset": 98, "endOffset": 102}, {"referenceID": 15, "context": "Such a system can be used along with existing libraries of machine learning algorithms like MLlib [20], Vowpal Wabbit [16], SystemML [11] etc.", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "Such a system can be used along with existing libraries of machine learning algorithms like MLlib [20], Vowpal Wabbit [16], SystemML [11] etc.", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "Our assumption is that the dataset used by the distributed optimization algorithm is static and that the algorithm is iterative with each iteration expressed as a bulk synchronous paralllel (BSP) [30] job2.", "startOffset": 196, "endOffset": 200}, {"referenceID": 29, "context": "To build the system level model, we propose re-using the approach in Ernest [31], a performance prediction framework that minimizes the time and resources spent in building a performance model.", "startOffset": 76, "endOffset": 80}, {"referenceID": 29, "context": "Results in [31] show that the prediction error for the time taken per-iteration of mini-batch SGD is within 12%, while using samples smaller than 10% of input data.", "startOffset": 11, "endOffset": 15}, {"referenceID": 24, "context": "For example while the computation costs in first order methods typically scale linearly with number of examples, using second order methods like SDNA [26] could incur super-linear computation or communication costs.", "startOffset": 150, "endOffset": 154}, {"referenceID": 23, "context": "We then fit a linear model to log(P (i,m) \u2212 P \u2217) using using LassoCV from scikit-learn [24], where P (i,m) is the primal objective value at iteration i with m parallelism.", "startOffset": 87, "endOffset": 91}, {"referenceID": 32, "context": "Similar decoupling of systems and algorithmic efficiencies had been proposed in DimmWitted [34] for studying running times of optimization algorithms in NUMA machines.", "startOffset": 91, "endOffset": 95}, {"referenceID": 32, "context": "On the systems side, physical execution engines that can choose between row or column storage layouts [34], and choose between algorithms given a fixed iteration budget [29] have been proposed.", "startOffset": 102, "endOffset": 106}, {"referenceID": 27, "context": "On the systems side, physical execution engines that can choose between row or column storage layouts [34], and choose between algorithms given a fixed iteration budget [29] have been proposed.", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "Further systems like STRADS [15] also study the trade-offs between using model parallelism and data parallelism with parameter servers.", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "Recently, distributed deep learning frameworks like Tensorflow [1] and MXNET [6] have implemented various execution strategies [21].", "startOffset": 63, "endOffset": 66}, {"referenceID": 5, "context": "Recently, distributed deep learning frameworks like Tensorflow [1] and MXNET [6] have implemented various execution strategies [21].", "startOffset": 77, "endOffset": 80}, {"referenceID": 20, "context": "Recently, distributed deep learning frameworks like Tensorflow [1] and MXNET [6] have implemented various execution strategies [21].", "startOffset": 127, "endOffset": 131}, {"referenceID": 11, "context": "As a consequence, systems like [25] have been developed to model the cost of each operator and to choose the optimal number of CPUs, GPUs [12] given asynchronous execution.", "startOffset": 138, "endOffset": 142}, {"referenceID": 1, "context": "There has been renewed interest in the machine learning community in \u201cmeta-learning,\u201d where machine learning techniques are used to automatically learn optimal models [2, 4] or optimization algorithms [3, 9, 7].", "startOffset": 167, "endOffset": 173}, {"referenceID": 3, "context": "There has been renewed interest in the machine learning community in \u201cmeta-learning,\u201d where machine learning techniques are used to automatically learn optimal models [2, 4] or optimization algorithms [3, 9, 7].", "startOffset": 167, "endOffset": 173}, {"referenceID": 2, "context": "There has been renewed interest in the machine learning community in \u201cmeta-learning,\u201d where machine learning techniques are used to automatically learn optimal models [2, 4] or optimization algorithms [3, 9, 7].", "startOffset": 201, "endOffset": 210}, {"referenceID": 8, "context": "There has been renewed interest in the machine learning community in \u201cmeta-learning,\u201d where machine learning techniques are used to automatically learn optimal models [2, 4] or optimization algorithms [3, 9, 7].", "startOffset": 201, "endOffset": 210}, {"referenceID": 6, "context": "There has been renewed interest in the machine learning community in \u201cmeta-learning,\u201d where machine learning techniques are used to automatically learn optimal models [2, 4] or optimization algorithms [3, 9, 7].", "startOffset": 201, "endOffset": 210}, {"referenceID": 6, "context": "In particular, [7] attempts to learn an optimizer for black-box functions.", "startOffset": 15, "endOffset": 18}, {"referenceID": 29, "context": "While prior work in Ernest [31] discussed how the system-level model can be trained using a small number of machines and data samples, we plan to investigate if similar approximations can be made for the convergence model.", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "While BSP algorithms have clear synchronization barriers between iterations, the same is not true of asynchronous algorithms such as Hogwild! [27].", "startOffset": 142, "endOffset": 146}], "year": 2017, "abstractText": "Distributed optimization algorithms are widely used in many industrial machine learning applications. However choosing the appropriate algorithm and cluster size is often difficult for users as the performance and convergence rate of optimization algorithms vary with the size of the cluster. In this paper we make the case for an ML-optimizer that can select the appropriate algorithm and cluster size to use for a given problem. To do this we propose building two models: one that captures the system level characteristics of how computation, communication change as we increase cluster sizes and another that captures how convergence rates change with cluster sizes. We present preliminary results from our prototype implementation called Hemingway and discuss some of the challenges involved in developing such a system.", "creator": "LaTeX with hyperref package"}}}