{"id": "1702.05962", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Latent Variable Dialogue Models and their Diversity", "abstract": "We present a dialogue generation model that directly captures the variability in possible responses to a given input, which reduces the `boring output' issue of deterministic dialogue models. Experiments show that our model generates more diverse outputs than baseline models, and also generates more consistently acceptable output than sampling from a deterministic encoder-decoder model.", "histories": [["v1", "Mon, 20 Feb 2017 13:36:23 GMT  (375kb,D)", "http://arxiv.org/abs/1702.05962v1", "Accepted at EACL 2017"]], "COMMENTS": "Accepted at EACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kris cao", "stephen clark"], "accepted": false, "id": "1702.05962"}, "pdf": {"name": "1702.05962.pdf", "metadata": {"source": "CRF", "title": "Latent Variable Dialogue Models and their Diversity", "authors": ["Kris Cao", "Stephen Clark"], "emails": ["sc609}@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "The task of open dialogue generation is an area of active development, with neural sequenceto sequence models dominating the recently published literature (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016b, a; Serban et al., 2016). Most previously published models train to minimize the negative log probability of training data, and then in generation time either search for the output Y that maximizes P (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016) (ML decoding), or sample from the resulting distribution (Serban et al., 2016).One notorious problem with ML decoding is that it tends to generate short, slow responses to inputs, such as \"I don't know how.\" These reactions are common in training data, and can draw on a wide range of inputs."}, {"heading": "2 A Latent Variable Dialogue Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Model Description", "text": "Our task is to model the true probability of an answer Y against an input X. We designate our model distribution by P ao Q (Y | X). We perform a latent Xiv: 170 2.05 962v 1 [cs.C L] 20 February 2017Variable z with a standard Gaussian - i.e. P (z) = N (0, In) - and Factor P (Y | X) as: P (Y | Z, X) P (z) dz (1) To motivate this model, we point out that existing encoder decoder models encode an input X as a single fixed representation. Therefore, all possible responses to X must be stored within the probability distribution P (Y | X) dz, and during decoding it is difficult to unravel these possible responses."}, {"heading": "2.2 Model Implementation", "text": "In the face of an input sentence X and a response Y, we perform two separate bidirectional RNNNs relative to their word embeddings xi and yi. We concatenate the final states of both cells and perform them on a single nonlinear plane to obtain our representations hx and hy of X and Y. We use GRUs (Cho et al., 2014) as a compromise between expressiveness and computational costs. We calculate the mean and variance of Q as: \u00b5 = W\u00b5 [hx hy] + b\u00b5log (\u03a3) = diag (W\u03a3 [hx hy] + b\u03a3) (3), where [a b] denotes the concatenation of a and b, and diag the insertion along the diagonal of a matrix. We take a single sample z from Q using the repair risk trick (Kingma and Welling, 2014), where [a] denotes the concatenation of hx and b, and diag the insertion along the diagonal of a matrix, we assign a single sample of Q and a single state of repair."}, {"heading": "3 Experiments", "text": "We compare our model, DIAL-LV, with three baselines. The first is an encoder-decoder dialog model with ML decoding (DIAL-MLE); the second base model implements the anti-LM decoder from Li et al. (2016a) (DIAL-MMI) on top of the encoder decoder, without length normalization; for these models, we use the beam search with a width of 2 to find the Y set that maximizes the decryption target (either ML or MMI); the last baseline uses the encoder decoder model, but instead uses samples from the decoder to find Y (DIAL-SAMP); we found that naive sampling from the decoder resulted in meaningless jumbled words (either ML or MMI); to solve this, we introduced a temperature parameter that we use as a date digister (the 1, the probability of each word)."}, {"heading": "3.1 Reply statistics", "text": "Previous work (e.g. Li et al. (2016a) used typetoken ratio (TTR) to measure the diversity of the results generated. However, since language follows a zip distribution, TTR is influenced by the length of the answers generated (Mitchell, 2015). Therefore, we use the estimated parameter of a zip distribution that matches our answers as a proxy for the lexical diversity of the results generated, with more diversified results having smaller values. Since ML decoding is known to repeatedly give the same answers, we also give the percentage of unique answers as a rough measure of the sentential diversity compared to lexical diversity. In addition, we present these statistics in the negative logging probability (NLL) predicted by the deterministic encoder decoder model to see which regions of the probability space occupy the answers. We also present these statistics in table 2.We note that DIV generates a higher diversity than other models."}, {"heading": "3.2 Human acceptability judgments", "text": "We also tested whether DIAL-LV could generate a greater number of acceptable responses to a prompt than DIAL-SAMP. We randomly selected 50 prompts from our list of 200 responses and generated 5 random responses to each of the two models. We then asked human commentators 2 to assess how many responses were appropriate, taking into account grammaticality, coherence and relevance. The results are shown in Table 3.Interestingly, although DIAL-LV has a lower NLL value, both models generate roughly the same number of acceptable responses. DIAL-LV also exhibits smaller differences in the number of acceptable responses, suggesting that the results it generates are more consistent than the responses from DIAL-SAMP. Finally, we found that DIAL-LV generates a more diverse output in this scenario than DIAL-SAMP, although its responses are assessed as equally acceptable, suggesting that it succeeds in producing a more coherent, appropriate palette."}, {"heading": "3.3 Sampling from the latent variable space", "text": "Next, we investigated the effect of samples from different regions of latent space. For each input request of the test set, we took 5 uniform samples from shells of radius 0 (which collapses on determinis-2We), 4, 8, 12 and 16 in latent room3, taking samples of P (z) = N (0, I) and then scaling the sample z by the corresponding amount. We then generated a response to the input request using any value of z and measured some statistics of the responses. The results will be presented in Table 4.As expected, samples with a small radius show less diversity in terms of unique results. In addition, we see a consistent trend that samples with a larger radius have a higher NLL value, showing the influence of the previous one in Eqn. 1. However, in the highest radius, we observe the highest NLLs, but also the lowest lexical diversities, indicating that it is possible to combine the words in different ways."}, {"heading": "4 Discussion", "text": "Taken together, our experiments show that ML decoding does not seem to be the best target for generating a diverse dialogue, thus confirming the inadequacy of helplessness as a benchmark for evaluating dialogue models (Liu et al., 2016). In fact, all three models, which show a diversity gain over the vanilla encoder decoder with MLE decoding, instead attempt to sample answers from a less likely region of the reaction space. However, if the response probability is too low, the risk is that it is nonsensical. Therefore, there seems to be a \"Goldilocks\" region of the probability space where the answers are interesting and coherent. Finding ways to focus model samples on this region is therefore a potentially promising area of research for open domain dialogue agents. We also note that our proposed model can be combined with MMI decoding or temperature-based sampling to obtain the benefits of both worlds."}, {"heading": "5 Conclusion", "text": "In this paper, we present a latent variable model for generating responses to input expressions. We examine the diversity of results generated by this model and show that it improves both lexical and sentence diversity, and also produces a more consistent acceptable output as judged by humans, compared to sampling a decoder."}, {"heading": "Acknowledgements", "text": "KC is supported by an EPSRC PhD Award. SC is supported by the ERC Starting Grant DisCoTex (306920) and the ERC Proof of Concept Grant GroundForce (693579)."}, {"heading": "A Model training information", "text": "We implemented all of our models with Keras (Chollet, 2015), which ran on Theano (Theano Development Team, 2016). As vocabulary, we took all of the words that occurred at least 1000 times in the entire corpus. Since it was 4,000 words, we used a two-step hierarchical approach to the full Softmax to speed up model training (Morin and Bengio, 2005), with random clustering. We trained all of our models for 3 epochs with the Adadelta optimizer (pointer, 2012), with default values for the optimization parameters. We used 512-dimensional word embedding and encoders to hide state sizes in all of our models. We used 64 latent latent dimensional latent variables, so the RNN decoder for the DIAL-LV model had hidden state sizes 576. The RNDIN decoder for the AL-M\u00d6LE model also had capacity to hold the 576 covered state sizes."}], "references": [{"title": "Probabilistic generation of weather forecast texts. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics", "author": ["Anja Belz"], "venue": null, "citeRegEx": "Belz.,? \\Q2007\\E", "shortCiteRegEx": "Belz.", "year": 2007}, {"title": "Autoencoding variational Bayes", "author": ["Diederik P. Kingma", "Max Welling"], "venue": "ICLR,", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Semisupervised learning with deep generative models", "author": ["Diederik P. Kingma", "Danilo Jimenez Rezende", "Shakir Mohamed", "Max Welling"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Neural variational inference for text processing", "author": ["Yishu Miao", "Lei Yu", "Phil Blunsom"], "venue": null, "citeRegEx": "Miao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Type-token models: a comparative study", "author": ["David Mitchell"], "venue": "Journal of Quantitative Linguistics,", "citeRegEx": "Mitchell.,? \\Q2015\\E", "shortCiteRegEx": "Mitchell.", "year": 2015}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio"], "venue": "Tenth International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Morin and Bengio.,? \\Q2005\\E", "shortCiteRegEx": "Morin and Bengio.", "year": 2005}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V. Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "venue": "In AAAI,", "citeRegEx": "Serban et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2017}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["Kihyuk Sohn", "Honglak Lee", "Xinchen Yan"], "venue": "In NIPS,", "citeRegEx": "Sohn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 2015}, {"title": "Neural machine translation with reconstruction", "author": ["Zhaopeng Tu", "Yang Liu", "Lifeng Shang", "Xiaohua Liu"], "venue": "In AAAI,", "citeRegEx": "Tu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2017}, {"title": "A neural conversation model", "author": ["Orial Vinyals", "Quoc V. Le"], "venue": null, "citeRegEx": "Vinyals and Le.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "CoRR, abs/1212.5701,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "The task of open-domain dialogue generation is an area of active development, with neural sequenceto-sequence models dominating the recently published literature (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016b,a; Serban et al., 2016).", "startOffset": 162, "endOffset": 245}, {"referenceID": 7, "context": "The task of open-domain dialogue generation is an area of active development, with neural sequenceto-sequence models dominating the recently published literature (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016b,a; Serban et al., 2016).", "startOffset": 162, "endOffset": 245}, {"referenceID": 11, "context": "Most previously published models train to minimise the negative log-likelihood of the training data, and then at generation time either perform beam search to find the output Y which maximises P (Y |input) (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016) (ML decoding), or sample from the resulting distribution (Serban et al.", "startOffset": 206, "endOffset": 269}, {"referenceID": 7, "context": "Most previously published models train to minimise the negative log-likelihood of the training data, and then at generation time either perform beam search to find the output Y which maximises P (Y |input) (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016) (ML decoding), or sample from the resulting distribution (Serban et al.", "startOffset": 206, "endOffset": 269}, {"referenceID": 7, "context": ", 2016) (ML decoding), or sample from the resulting distribution (Serban et al., 2016).", "startOffset": 65, "endOffset": 86}, {"referenceID": 7, "context": "These responses are common in the training data, and can be replies to a wide range of inputs (Li et al., 2016a; Serban et al., 2016).", "startOffset": 94, "endOffset": 133}, {"referenceID": 10, "context": "In addition, shorter responses typically have higher likelihoods, and so wide beam sizes often result in very short responses (Tu et al., 2017; Belz, 2007).", "startOffset": 126, "endOffset": 155}, {"referenceID": 0, "context": "In addition, shorter responses typically have higher likelihoods, and so wide beam sizes often result in very short responses (Tu et al., 2017; Belz, 2007).", "startOffset": 126, "endOffset": 155}, {"referenceID": 0, "context": ", 2017; Belz, 2007). To resolve this problem, Li et al. (2016a) propose instead using maximum mutual information with a length boost as a decoding objective, and report more interesting generated responses.", "startOffset": 8, "endOffset": 64}, {"referenceID": 0, "context": ", 2017; Belz, 2007). To resolve this problem, Li et al. (2016a) propose instead using maximum mutual information with a length boost as a decoding objective, and report more interesting generated responses. Further, natural dialogue is not deterministic; for example, the replies to \u201cWhat\u2019s your name and where do you come from?\u201d will vary from person to person. Li et al. (2016b) have proposed learning representations of personas to account for interperson variation, but there can be variation even among a single person\u2019s responses to certain questions.", "startOffset": 8, "endOffset": 381}, {"referenceID": 7, "context": "Recently, Serban et al. (2017) have introduced latent variables to the dialogue modelling framework, to model the underlying distribution over possible responses directly.", "startOffset": 10, "endOffset": 31}, {"referenceID": 1, "context": "At training time, we follow the variational autoencoder framework (Kingma and Welling, 2014; Kingma et al., 2014; Sohn et al., 2015; Miao et al., 2016) , and approximate the posterior P (z|X,Y ) with a proposal distribution Q(z|X,Y ), which in our case is a diagonal Gaussian whose parameters depend on X and Y .", "startOffset": 66, "endOffset": 151}, {"referenceID": 2, "context": "At training time, we follow the variational autoencoder framework (Kingma and Welling, 2014; Kingma et al., 2014; Sohn et al., 2015; Miao et al., 2016) , and approximate the posterior P (z|X,Y ) with a proposal distribution Q(z|X,Y ), which in our case is a diagonal Gaussian whose parameters depend on X and Y .", "startOffset": 66, "endOffset": 151}, {"referenceID": 9, "context": "At training time, we follow the variational autoencoder framework (Kingma and Welling, 2014; Kingma et al., 2014; Sohn et al., 2015; Miao et al., 2016) , and approximate the posterior P (z|X,Y ) with a proposal distribution Q(z|X,Y ), which in our case is a diagonal Gaussian whose parameters depend on X and Y .", "startOffset": 66, "endOffset": 151}, {"referenceID": 4, "context": "At training time, we follow the variational autoencoder framework (Kingma and Welling, 2014; Kingma et al., 2014; Sohn et al., 2015; Miao et al., 2016) , and approximate the posterior P (z|X,Y ) with a proposal distribution Q(z|X,Y ), which in our case is a diagonal Gaussian whose parameters depend on X and Y .", "startOffset": 66, "endOffset": 151}, {"referenceID": 1, "context": "We take a single sample z from Q using the reparametrization trick (Kingma and Welling, 2014), concatenate hx and z, and initialize the hidden state of the decoder GRU with [hx z].", "startOffset": 67, "endOffset": 93}, {"referenceID": 3, "context": "The second baseline model implements the anti-LM decoder of Li et al. (2016a) (DIAL-MMI) on top of the encoder-decoder, with no length normalization.", "startOffset": 60, "endOffset": 78}, {"referenceID": 5, "context": "However, as language follows a Zipf distribution, TTR is affected by the length of the generated replies (Mitchell, 2015).", "startOffset": 105, "endOffset": 121}, {"referenceID": 3, "context": "Li et al. (2016a)) used typetoken ratio (TTR) to measure the diversity of the generated output.", "startOffset": 0, "endOffset": 18}], "year": 2017, "abstractText": "We present a dialogue generation model that directly captures the variability in possible responses to a given input, which reduces the \u2018boring output\u2019 issue of deterministic dialogue models. Experiments show that our model generates more diverse outputs than baseline models, and also generates more consistently acceptable output than sampling from a deterministic encoder-decoder model.", "creator": "LaTeX with hyperref package"}}}