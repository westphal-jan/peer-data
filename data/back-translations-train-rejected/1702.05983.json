{"id": "1702.05983", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN", "abstract": "Machine learning has been used to detect new malware in recent years, while malware authors have strong motivation to attack such algorithms. Malware authors usually have no access to the detailed structures and parameters of the machine learning models used by malware detection systems, and therefore they can only perform black-box attacks. This paper proposes a generative adversarial network (GAN) based algorithm named MalGAN to generate adversarial malware examples, which are able to bypass black-box machine learning based detection models. MalGAN uses a substitute detector to fit the black-box malware detection system. A generative network is trained to minimize the generated adversarial examples' malicious probabilities predicted by the substitute detector. The superiority of MalGAN over traditional gradient based adversarial example generation algorithms is that MalGAN is able to decrease the detection rate to nearly zero and make the retraining based defensive method against adversarial examples hard to work.", "histories": [["v1", "Mon, 20 Feb 2017 14:32:17 GMT  (139kb,D)", "http://arxiv.org/abs/1702.05983v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["weiwei hu", "ying tan"], "accepted": false, "id": "1702.05983"}, "pdf": {"name": "1702.05983.pdf", "metadata": {"source": "CRF", "title": "Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN", "authors": ["Weiwei Hu", "Ying Tan"], "emails": ["ytan}@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "2 Architecture of MalGAN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Overview", "text": "The architecture of the proposed MalGAN is illustrated in Figure 1.The black-box detector is an external system which adopts machine learning based malware detection algorithms. We assume that the only thing malware authors know about the black-box detector is what kind of features it uses. Malware authors do not know what machine learning algorithm it uses and do not have access to the parameters of the trained model. Malware authors are able to obtain the detection results of their programs from the black-box detector. The entire model includes a generator and a replacement detector, both of which are feed-forward neural networks. The generator and the replacement detector work together to create a machine learning based black-box malware detector.In this paper we generate only contrary examples of binary features, because binary features are widely used that malware detection researchers and are able to perform in high detection accuracy. Here, we take the API function as an example to show how a program presents itself."}, {"heading": "2.2 Generator", "text": "The generator is used to transform a malware feature vector into its opposing version. It takes the concatenation of a malware feature vector m and a noise vector z as input. m is an M-dimensional binary vector. Each element of m corresponds to the presence or absence of a feature. Z is a Z-dimensional vector, where Z is a hyper parameter. z is a random number scanned from an even distribution in the range [0, 1). z causes the generator to generate various opposing examples from a single malware feature vector. The input vector is fed into a multi-layered feed-forward neural network with weights. The output layer of this network has M neurons and the activation function used by the last layer is sigmoid, which limits the output to the range of the network with weights."}, {"heading": "2.3 Substitute Detector", "text": "Since the authors of the malware do not know the detailed structure of the black box detector, the replacement detector is used for the black box detector and provides gradient information to train the generator. The replacement detector is a multi-layered, forward-facing neural network with weights \u03b8d that takes a program feature vector x as input. It classifies the program between benign program and malware. We refer to the predicted probability that x is malware as D\u03b8d (x). The training data of the replacement detector consists of contradictory malware examples from the generator and benign programs from an additional benign dataset collected by malware authors. The ground truth labels of the training data are not used to train the replacement detector. The goal of the replacement detector is to fit on the black box detector. The black box detector detects this training data first and gives out a program that is benign or malware."}, {"heading": "3 Training MalGAN", "text": "In order to train MalGAN malware, the authors should first collect a malware data set and a benign data set. Formula 2 defines the loss function of the replacement detector."}, {"heading": "LD =\u2212 Ex\u2208BBBenign log (1\u2212D\u03b8d(x))", "text": "\u2212 Ex-BBMalware logD\u03b8d (x). (2) BBBenign is the set of programs that are detected as benign by the black box detector, and BBMalware is the set of programs that are detected as malware by the black box detector. To train the replacement detector, LD should be minimized in terms of the weights of the replacement detector. The loss function of the generator is in formula 3.LG = Em SMalware, z Strain form [0.1) logD\u03b8d (G\u03b8g (m, z)). (3) SMalware is the actual malware data set, not the malware labeled by the black box detector. LG is minimized in terms of the weights of the generator. LG's minimization will reduce the predicted likelihood of malware damage and the replacement data set as good.As the replacement detector tries to update the black box detector, the malware will be detected by the BBBenign as the educational malware and the school algorithm."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental Setup", "text": "The dataset used in this paper was crawled by a program for sharing the Website1. We download 180 thousand programs from this website and approximately 30% of them are malware. API functions are used in this paper. A 160-dimensional binary feature vector is constructed for each program, based on 160 system level APIs.To validate the transferability of opposing examples by MalGAN, we have tried several different machine learning algorithms for the black box detector. The classifiers used include random forests (RF), logistic regression (LR), decision trees (vector machines), support vector vector machines (SVM), multi-layer perceptor vector vector vector vector (MLP), and a matching based on ensembles of these classifiers. We have adopted two ways to divide the datasets (LR), vector vector vector vector vector vector machine vector vector (Vector machines), vector systems vector vector (Vector)"}, {"heading": "4.2 Experimental Results", "text": "In fact, it is that we are able to manoeuvre ourselves into a situation in which we see ourselves in a position, in which we are able to assert ourselves that we are in a position we are in, in which we are in, and in which we are in a situation in which we are able to change the world we are in, in which we are in, in which we are in, and in which we are in a situation we are in, in which we are in which we are in, in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are in which we are."}, {"heading": "4.3 Comparison with the Gradient based Algorithm to Generate Adversarial Examples", "text": "This year it is so far that it only takes a few days to reach an agreement."}, {"heading": "4.4 Retraining the Black-Box Detector", "text": "Several defensive algorithms have been proposed to deal with opposing examples. Gu et al. suggested to use autoencoders to purify opposing samples input data [Gu and Rigazio, 2014]. An algorithm called defensive distillation was proposed by Papernot et al. to weaken the effectiveness of opposing perturbations [Papernot et al., 2016d]. Li et al. found that opposing retraining can increase the robustness of machine learning algorithms [Li et al., 2016]. Chen et al. compared these defensive algorithms and concluded that retraining is a very effective way to defend against opposing examples, and is also more robust against repeated attacks. In this section we will analyze the performance of the malware."}, {"heading": "5 Conclusions", "text": "In this work, a novel algorithm called MalGAN was proposed to generate opposing examples from a machine learning-based black box malware detector. Experimental results showed that the generated opposing examples are able to effectively bypass the black box detector, and the probability distribution of opposing examples is controlled by the weights of the generator. Malware authors are able to change the probability distribution frequently by retraining MalGAN, making the black box detector unable to keep pace with it and unable to learn stable patterns from it. Once the black box detector is updated, malware authors can crack it right away, a process that renders machine learning-based malware detection algorithms inoperable."}, {"heading": "Acknowledgments", "text": "This work was supported by the Natural Science Foundation of China (NSFC) under grant number 61375119 and the Beijing Natural Science Foundation under grant number 4162029, and partially supported by the National Key Basic Research Development Plan (973 Plan) Project of China under grant number 2015CB352302."}], "references": [{"title": "In NIPS 2016 Workshop on Adversarial Training", "author": ["Martin Arjovsky", "L\u00e9on Bottou. Towards principled methods for training generative adversarial networks"], "venue": "review for ICLR, volume 2016,", "citeRegEx": "Arjovsky and Bottou. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "and Yevgeniy Vorobeychik", "author": ["Xinyun Chen", "Bo Li"], "venue": "Evaluation of defensive methods for dnns against multiple adversarial evasion models.", "citeRegEx": "Chen et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in neural information processing systems, pages 1486\u20131494,", "citeRegEx": "Denton et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in neural information processing systems", "author": ["Ian Goodfellow", "Jean PougetAbadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio. Generative adversarial nets"], "venue": "pages 2672\u20132680,", "citeRegEx": "Goodfellow et al.. 2014a", "shortCiteRegEx": null, "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al.. 2014b", "shortCiteRegEx": null, "year": 2014}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["Kathrin Grosse", "Nicolas Papernot", "Praveen Manoharan", "Michael Backes", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1606.04435,", "citeRegEx": "Grosse et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["Shixiang Gu", "Luca Rigazio"], "venue": "arXiv preprint arXiv:1412.5068,", "citeRegEx": "Gu and Rigazio. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "pages 470\u2013478", "author": ["Jeremy Z Kolter", "Marcus A Maloof. Learning to detect malicious executables in the wild. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery", "data mining"], "venue": "ACM,", "citeRegEx": "Kolter and Maloof. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "The Journal of Machine Learning Research", "author": ["J Zico Kolter", "Marcus A Maloof. Learning to detect", "classify malicious executables in the wild"], "venue": "7:2721\u20132744,", "citeRegEx": "Kolter and Maloof. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "A general retraining framework for scalable adversarial classification", "author": ["Bo Li", "Yevgeniy Vorobeychik", "Xinyun Chen"], "venue": "arXiv preprint arXiv:1604.02606,", "citeRegEx": "Li et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Delving into transferable adversarial examples and black-box attacks", "author": ["Yanpei Liu", "Xinyun Chen", "Chang Liu", "Dawn Song"], "venue": "arXiv preprint arXiv:1611.02770,", "citeRegEx": "Liu et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional generative adversarial nets", "author": ["Mehdi Mirza", "Simon Osindero"], "venue": "arXiv preprint arXiv:1411.1784,", "citeRegEx": "Mirza and Osindero. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow"], "venue": "arXiv preprint arXiv:1605.07277,", "citeRegEx": "Papernot et al.. 2016a", "shortCiteRegEx": null, "year": 2016}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "Papernot et al.. 2016b", "shortCiteRegEx": null, "year": 2016}, {"title": "The limitations of deep learning in adversarial settings", "author": ["Papernot et al", "2016c] Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami"], "venue": "In Security and Privacy (EuroS&P),", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["Papernot et al", "2016d] Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami"], "venue": "In Security and Privacy (SP),", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen. Improved techniques for training gans"], "venue": "pages 2226\u2013 2234,", "citeRegEx": "Salimans et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "2001", "author": ["Matthew G Schultz", "Eleazar Eskin", "Erez Zadok", "Salvatore J Stolfo. Data mining methods for detection of new malicious executables. In Security", "Privacy"], "venue": "S&P 2001. Proceedings. 2001 IEEE Symposium on, pages 38\u201349. IEEE,", "citeRegEx": "Schultz et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al.. 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 19, "context": "proposed to use DLLs, APIs and strings as features for classification [Schultz et al., 2001], while Kolter et al.", "startOffset": 70, "endOffset": 92}, {"referenceID": 8, "context": "used byte level N-Gram as features [Kolter and Maloof, 2004; Kolter and Maloof, 2006].", "startOffset": 35, "endOffset": 85}, {"referenceID": 9, "context": "used byte level N-Gram as features [Kolter and Maloof, 2004; Kolter and Maloof, 2006].", "startOffset": 35, "endOffset": 85}, {"referenceID": 20, "context": "added imperceptible perturbations to images to maximize a trained neural network\u2019s classification errors, making the network unable to classify the images correctly [Szegedy et al., 2013].", "startOffset": 165, "endOffset": 187}, {"referenceID": 4, "context": "proposed a gradient based algorithm to generate adversarial examples [Goodfellow et al., 2014b].", "startOffset": 69, "endOffset": 95}, {"referenceID": 5, "context": "proposed to use the gradient based approach to generate adversarial Android malware examples [Grosse et al., 2016].", "startOffset": 93, "endOffset": 114}, {"referenceID": 14, "context": "used a substitute neural network to fit the black-box neural network and then generated adversarial examples according to the substitute neural network [Papernot et al., 2016b].", "startOffset": 152, "endOffset": 176}, {"referenceID": 13, "context": "They also used a substitute neural network to attack other machine learning algorithms such as logistic regression, support vector machines, decision trees and nearest neighbors [Papernot et al., 2016a].", "startOffset": 178, "endOffset": 202}, {"referenceID": 11, "context": "performed black-box attacks without a substitute model [Liu et al., 2016], based on the principle that adversarial examples can transfer among different models [Szegedy et al.", "startOffset": 55, "endOffset": 73}, {"referenceID": 20, "context": ", 2016], based on the principle that adversarial examples can transfer among different models [Szegedy et al., 2013].", "startOffset": 94, "endOffset": 116}, {"referenceID": 3, "context": "The learning algorithm of our proposed model is inspired by generative adversarial networks (GAN) [Goodfellow et al., 2014a].", "startOffset": 98, "endOffset": 124}, {"referenceID": 12, "context": "GAN has shown good performance in generating realistic images[Mirza and Osindero, 2014; Denton et al., 2015].", "startOffset": 61, "endOffset": 108}, {"referenceID": 2, "context": "GAN has shown good performance in generating realistic images[Mirza and Osindero, 2014; Denton et al., 2015].", "startOffset": 61, "endOffset": 108}, {"referenceID": 7, "context": "Adam [Kingma and Ba, 2014] was chosen as the optimizer.", "startOffset": 5, "endOffset": 26}, {"referenceID": 17, "context": "How to stabilize the training of GAN have attracted the attention of many researchers [Radford et al., 2015; Salimans et al., 2016; Arjovsky and Bottou, 2017].", "startOffset": 86, "endOffset": 158}, {"referenceID": 18, "context": "How to stabilize the training of GAN have attracted the attention of many researchers [Radford et al., 2015; Salimans et al., 2016; Arjovsky and Bottou, 2017].", "startOffset": 86, "endOffset": 158}, {"referenceID": 0, "context": "How to stabilize the training of GAN have attracted the attention of many researchers [Radford et al., 2015; Salimans et al., 2016; Arjovsky and Bottou, 2017].", "startOffset": 86, "endOffset": 158}, {"referenceID": 5, "context": "modified the traditional gradient based algorithm to generate binary adversarial malware examples [Grosse et al., 2016].", "startOffset": 98, "endOffset": 119}, {"referenceID": 6, "context": "proposed to use autoencoders to map adversarial samples to clean input data [Gu and Rigazio, 2014].", "startOffset": 76, "endOffset": 98}, {"referenceID": 10, "context": "found that adversarial retraining can boost the robustness of machine learning algorithms [Li et al., 2016].", "startOffset": 90, "endOffset": 107}, {"referenceID": 1, "context": "compared these defensive algorithms and concluded that retraining is a very effective way to defend against adversarial examples, and is robust even against repeated attacks [Chen et al., 2016].", "startOffset": 174, "endOffset": 193}], "year": 2017, "abstractText": "Machine learning has been used to detect new malware in recent years, while malware authors have strong motivation to attack such algorithms.Malware authors usually have no access to the detailed structures and parameters of the machine learning models used by malware detection systems, and therefore they can only perform black-box attacks. This paper proposes a generative adversarial network (GAN) based algorithm named MalGAN to generate adversarial malware examples, which are able to bypass black-box machine learning based detection models. MalGAN uses a substitute detector to fit the black-box malware detection system. A generative network is trained to minimize the generated adversarial examples\u2019 malicious probabilities predicted by the substitute detector. The superiority of MalGAN over traditional gradient based adversarial example generation algorithms is that MalGAN is able to decrease the detection rate to nearly zero and make the retraining based defensive method against adversarial examples hard to work.", "creator": "LaTeX with hyperref package"}}}