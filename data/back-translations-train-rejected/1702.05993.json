{"id": "1702.05993", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "An Extended Framework for Marginalized Domain Adaptation", "abstract": "We propose an extended framework for marginalized domain adaptation, aimed at addressing unsupervised, supervised and semi-supervised scenarios. We argue that the denoising principle should be extended to explicitly promote domain-invariant features as well as help the classification task. Therefore we propose to jointly learn the data auto-encoders and the target classifiers. First, in order to make the denoised features domain-invariant, we propose a domain regularization that may be either a domain prediction loss or a maximum mean discrepancy between the source and target data. The noise marginalization in this case is reduced to solving the linear matrix system $AX=B$ which has a closed-form solution. Second, in order to help the classification, we include a class regularization term. Adding this component reduces the learning problem to solving a Sylvester linear matrix equation $AX+BX=C$, for which an efficient iterative procedure exists as well. We did an extensive study to assess how these regularization terms improve the baseline performance in the three domain adaptation scenarios and present experimental results on two image and one text benchmark datasets, conventionally used for validating domain adaptation methods. We report our findings and comparison with state-of-the-art methods.", "histories": [["v1", "Mon, 20 Feb 2017 15:00:13 GMT  (28kb)", "http://arxiv.org/abs/1702.05993v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["gabriela csurka", "boris chidlovski", "stephane clinchant", "sophia michel"], "accepted": false, "id": "1702.05993"}, "pdf": {"name": "1702.05993.pdf", "metadata": {"source": "CRF", "title": "An Extended Framework for Marginalized Domain Adaptation", "authors": ["Gabriela Csurka", "Boris Chidlovski", "St\u00e9phane Clinchant"], "emails": ["Firstname.Lastname@xrce.xerox.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.05 993v 1 [cs.C V] 20 February 20We propose an expanded framework for marginalised domain adaptation aimed at unattended, monitored and semi-monitored scenarios. We argue that the denocialisation principle should be extended to explicitly promote domain invariant features and support the classification task. Therefore, we propose to jointly learn the data auto-encoders and target classifiers. Firstly, in order to make the denoised features domain invariant, we propose a domain adaptation that can mean either a domain prediction loss or a max-imum that there is discrepancy between the source and target data. In this case, the marginalisation is reduced to the solution of the linear matrix system AX = B, which has a closed domain adaptation solution. Secondly, to support classification, we include a class regularization concept."}, {"heading": "1 Introduction", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "2 State of the art", "text": "In fact, most of them will be able to move to a different world in which they are able to live than to another world in which they are able to live."}, {"heading": "3 Domain adaptation by feature denoising", "text": "We define a domain D as the composition of a Feature Space X-IRd and a Label Space Y (3). We define a given task in domain D (3). We define a specific task in domain D (3). We assume that we are working with a source domain Ds, which is represented by the feature matrix Xs and the associated target markers Ys, and that all available target markers Ys, where all available target markers are unlabeled, are unlabeled. In this case, Xtl is empty and the labeled data contains only the described source examples, Xl = X s \u2022 Supervised (SUP) Setting, where few labeled target instancesXtl are available at training times."}, {"heading": "3.3.2 Reducing the MMD between the domain class", "text": "Lc-1-1-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4"}, {"heading": "4 Minimizing the total loss", "text": "In the previous section we described three terms of loss function L. Now we will discuss two main cases of minimizing total losses. First, we will discuss the sequential case in which we first learn W by using only the data without labels (L1 or L1 + \u03b3L3), and then we will learn the classifier Zl or any other classifier. Second, we will describe the common case in which W and Zl are learned jointly by iteratively minimizing the total loss L1 + \u03bbL2 + \u03b3L3. In both cases, we will consider three options for domain regulation L3 and discuss three domain adaptation scenarios, US, SUP and SS.All the mentioned combinations of losses form different models; we will denote them as follows. The sequential methods are followed by a character S followed by the indices of the losses used. For example, if we learn nW with L1 + \u03b3Ld, the method becomes S1D."}, {"heading": "5 Experimental Results", "text": "This year, it has come to the point where it only takes one year for it to come to a conclusion."}, {"heading": "5.3 Comparing domain adaptation methods", "text": "Dre rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the"}, {"heading": "6 Conclusion", "text": "We proposed an expanded domain customization framework, in which state-of-the-art marginalized auto-encoder denoisation is extended to include domain and class regularization terms aimed at addressing uncontrolled, monitored, and semi-monitored scenarios. Domain regularization drives the denosis of source and target data toward domain invariant characteristics. In all cases, the models can be reduced to solving a linear matrix equation or its Sylvester version, for which efficient algorithms exist. We presented the results of an extensive series of experiments on two image and one text benchmark datasets, testing the proposed framework in different settings. We demonstrated that the addition of the new regulatory terms enables baselines to be implemented and the development of the best strategies using two image and one text benchmark datasets, where the adjustment results are relatively compatible and fast."}], "references": [{"title": "Landmarks-based kernelized subspace alignment for unsupervised domain adaptation", "author": ["R. Aljundi", "R. Emonet", "D. Muselet", "M. Sebban"], "venue": "Proc. of CVPR, (IEEE)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised domain adaptation by domain invariant projection", "author": ["M. Baktashmotlagh", "M. Harandi", "B. Lovell", "M. Salzmann"], "venue": "Proc. of ICCV, (IEEE)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Analysis of representations for domain adaptation", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "F. Pereira"], "venue": "NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "EMNLP", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Domain Adaptation with Coupled Subspaces", "author": ["J. Blitzer", "S. Kakade", "D.P. Foster"], "venue": "AISTATS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["K.M. Borgwardt", "A. Gretton", "M.J. Rasch"], "venue": "Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Domain adaptation problems: A dasvm classification technique and a circular validation strategy", "author": ["L. Bruzzone", "M. Marconcini"], "venue": "(PAMI)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Co-training for domain adaptation", "author": ["M. Chen", "K.Q. Weinberger", "J. Blitzer"], "venue": "NIPS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K.Q. Weinberger", "F. Sha"], "venue": "ICML", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Marginalized denoising for link prediction and multi-label learning", "author": ["Z. Chen", "M. Chen", "K.Q. Weinberger", "W. Zhang"], "venue": "AAAI", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Topic modeling using topics from many domains", "author": ["Z. Chen", "B. Liu"], "venue": "lifelong learning and big data. In ICML", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "A marginalized denoising method for link prediction in relational data", "author": ["Z. Chen", "W. Zhang"], "venue": "ICDM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "DLID: Deep learning for domain adaptation by interpolating between domains", "author": ["S. Chopra", "S. Balakrishnan", "R. Gopalan"], "venue": "ICML Workshop (WREPL)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "A domain adaptation regularization for denoising autoencoders", "author": ["S. Clinchant", "G. Csurka", "B. Chidlovskii"], "venue": "Proc. of ACL", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Domain adaptation with a domain specific class means classifier", "author": ["G. Csurka", "B. Chidlovskii", "F. Perronnin"], "venue": "TASK- CV, ECCV workshop", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain Adaptation for Visual Applications: A Comprehensive Survey", "author": ["G. Csurka"], "venue": "CoRR, arXiv:1702.05374", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Frustratingly easy domain adaptation", "author": ["H. Daum\u00e9"], "venue": "CoRR, arXiv:0907.1815", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Domain adaptation for statistical classifiers", "author": ["H. Daume III", "D. Marcu"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "et al", "author": ["J. Donahue"], "venue": "Decaf: A deep convolutional activation feature for generic visual recognition. CoRR, arXiv:1310.1531", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Domain transfer multiple kernel learning", "author": ["L. Duan", "I.W. Tsang", "D. Xu"], "venue": "Transactions of Pattern Recognition and Machine Analyses (PAMI), 34(3):465\u2013479", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Domain adaptation from multiple sources via auxiliary classifiers", "author": ["L. Duan", "I.W. Tsang", "D. Xu", "T.-S. Chua"], "venue": "Proc. of ICML, pages 289\u2013296", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "T", "author": ["N. Farajidavar"], "venue": "deCampos, and J. Kittler. Transductive transfer machines. In Proc. of ACCV, pages 623\u2013639", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised visual domain adaptation using subspace alignment", "author": ["B. Fernando", "A. Habrard", "M. Sebban", "T. Tuytelaars"], "venue": "Proc. of ICCV, (IEEE), pages 2960\u20132967", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Y. Ganin", "V. Lempitsky"], "venue": "Proc. of ICML", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Domain-adversarial training of neural net-  works", "author": ["Y. Ganin"], "venue": "CoRR, arXiv:1505.07818,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proc. of ICML", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Connecting the dots with landmarks: Discriminatively learning domain invariant features for unsupervised domain adaptation", "author": ["B. Gong", "K. Grauman", "F. Sha"], "venue": "ICML", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Domain adaptation for visual recognition", "author": ["R. Gopalan", "R. Li", "V.M. Patel", "R. Chellappa"], "venue": "Foundations and Trends in Computer Graphics and Vision, 8(4)", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient learning of domain-invariant image representations", "author": ["J. Hoffman", "E. Rodner", "J. Donahue", "T. Darrell", "K. Saenko"], "venue": "Proc. of ICLR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Correcting sample selection bias by unlabeled data", "author": ["J. Huang", "A. Smola", "A. Gretton", "K. Borgwardt", "B. Sch\u00f6lkopf"], "venue": "Proc. of NIPS", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep collaborative filtering via marginalized denoising auto-encode", "author": ["S. Li", "J. Kawale", "Y. Fu"], "venue": "CIKM ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning with marginalized corrupted features and labels together", "author": ["Y. Li", "M. Yang", "Z. Xu", "Z. Zhang"], "venue": "Proc. of AAAI, volume arXiv:1602:07332", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning transferable features with deep adaptation networks", "author": ["M. Long", "Y. Cao", "J. Wang", "M.I. Jordan"], "venue": "Proc. of ICML", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Transfer feature learning with joint distribution adaptation", "author": ["M. Long", "J. Wang", "G. Ding", "J. Sun", "P.S. Yu"], "venue": "Proc. of ICCV, (IEEE)", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning with marginalized corrupted features", "author": ["L. v. d. Maaten", "M. Chen", "S. Tyree"], "venue": "In Proc. of ICML,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "J", "author": ["S.J. Pan"], "venue": "T. Tsang, Ivor W.and Kwok, and Q. Yang. Domain adaptation via transfer component analysis. Transactions on Neural Networks", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "Transactions on Knowledge and Data Engineering", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "Proc. of ECCV", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Direct methods for matrix Sylvester and Lyapunov equations", "author": ["D.C. Sorensen", "Y. Zhou"], "venue": "In Journal of Applied Mathematics,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2003}, {"title": "Return of frustratingly easy  domain adaptation", "author": ["B. Sun", "J. Feng", "K. Saenko"], "venue": "Proc. of AAAI", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep domain confusion: Maximizing for domain invariance", "author": ["E. Tzeng", "J. Hoffman", "N. Zhang", "K. Saenko", "T. Darrell"], "venue": "CoRR, arXiv:1412.3474", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "Proc. of ICML", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-source transfer learning with multiview adaboost", "author": ["Z. Xu", "S. Sun"], "venue": "Proc. of NIPS, pages 332\u2013339", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Cross validation framework to choose amongst models and datasets for transfer learning", "author": ["E. Zhong", "W. Fan", "Q. Yang", "O. Verscheure", "J. Ren"], "venue": "Proc. PKDD (ECML)", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Hybrid heterogeneous transfer learning through deep learning", "author": ["J.T. Zhou", "S.J. Pan", "I.W. Tsang", "Y. Yan"], "venue": "Proc. of AAAI", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying learning to rank and domain adaptation: Enabling cross-task document scoring", "author": ["M. Zhou", "K.C. Chang"], "venue": "Proc. of SIGKDD ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "In this paper, we build on the domain adaptation work based on noise marginalization [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 34, "context": "The principle of noise marginalization has been successfully extended to learning with corrupted features [36], link prediction and multi-label learning [10], relational learning [12], collaborative filtering [32] and heterogeneous cross-domain learning [33, 46].", "startOffset": 106, "endOffset": 110}, {"referenceID": 9, "context": "The principle of noise marginalization has been successfully extended to learning with corrupted features [36], link prediction and multi-label learning [10], relational learning [12], collaborative filtering [32] and heterogeneous cross-domain learning [33, 46].", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "The principle of noise marginalization has been successfully extended to learning with corrupted features [36], link prediction and multi-label learning [10], relational learning [12], collaborative filtering [32] and heterogeneous cross-domain learning [33, 46].", "startOffset": 179, "endOffset": 183}, {"referenceID": 30, "context": "The principle of noise marginalization has been successfully extended to learning with corrupted features [36], link prediction and multi-label learning [10], relational learning [12], collaborative filtering [32] and heterogeneous cross-domain learning [33, 46].", "startOffset": 209, "endOffset": 213}, {"referenceID": 31, "context": "The principle of noise marginalization has been successfully extended to learning with corrupted features [36], link prediction and multi-label learning [10], relational learning [12], collaborative filtering [32] and heterogeneous cross-domain learning [33, 46].", "startOffset": 254, "endOffset": 262}, {"referenceID": 44, "context": "The principle of noise marginalization has been successfully extended to learning with corrupted features [36], link prediction and multi-label learning [10], relational learning [12], collaborative filtering [32] and heterogeneous cross-domain learning [33, 46].", "startOffset": 254, "endOffset": 262}, {"referenceID": 24, "context": "Two families of such regularization are considered; one is based on the domain prediction principle, inspired by the adversarial learning of neural networks [25]; the second uses the maximum mean discrepancy (MMD) measure [31].", "startOffset": 157, "endOffset": 161}, {"referenceID": 29, "context": "Two families of such regularization are considered; one is based on the domain prediction principle, inspired by the adversarial learning of neural networks [25]; the second uses the maximum mean discrepancy (MMD) measure [31].", "startOffset": 222, "endOffset": 226}, {"referenceID": 17, "context": "Domain adaptation for text data has been studied for more than a decade, with applications in statistical machine translation, opinion mining, and document ranking [18, 47].", "startOffset": 164, "endOffset": 172}, {"referenceID": 45, "context": "Domain adaptation for text data has been studied for more than a decade, with applications in statistical machine translation, opinion mining, and document ranking [18, 47].", "startOffset": 164, "endOffset": 172}, {"referenceID": 16, "context": "Most effective techniques include feature replication [17], pivot features [4] and finding topic models that are shared between source and target collections [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 3, "context": "Most effective techniques include feature replication [17], pivot features [4] and finding topic models that are shared between source and target collections [11].", "startOffset": 75, "endOffset": 78}, {"referenceID": 10, "context": "Most effective techniques include feature replication [17], pivot features [4] and finding topic models that are shared between source and target collections [11].", "startOffset": 158, "endOffset": 162}, {"referenceID": 27, "context": "A considerable effort to systematize different shallow domain adaptation and transfer learning techniques has been undertaken in [29, 38, 16].", "startOffset": 129, "endOffset": 141}, {"referenceID": 36, "context": "A considerable effort to systematize different shallow domain adaptation and transfer learning techniques has been undertaken in [29, 38, 16].", "startOffset": 129, "endOffset": 141}, {"referenceID": 15, "context": "A considerable effort to systematize different shallow domain adaptation and transfer learning techniques has been undertaken in [29, 38, 16].", "startOffset": 129, "endOffset": 141}, {"referenceID": 42, "context": "The first category aims at correcting sampling bias [44].", "startOffset": 52, "endOffset": 56}, {"referenceID": 7, "context": "The second category is in line with multi-task learning where a common predictor is learned for all domains, which makes it robust to domain shift [8].", "startOffset": 147, "endOffset": 150}, {"referenceID": 35, "context": "The third family seeks to find a common representation for both source and target examples so that the classification task becomes easier [37].", "startOffset": 138, "endOffset": 142}, {"referenceID": 2, "context": "Finally, an important research direction deals with the theory of domain adaptation, namely when adaptation can be effective and guaranteed with generalization bounds [3].", "startOffset": 167, "endOffset": 170}, {"referenceID": 12, "context": "More recently, deep learning has been proposed as a generic solution to domain adaptation and transfer learning problems [13, 26, 34].", "startOffset": 121, "endOffset": 133}, {"referenceID": 25, "context": "More recently, deep learning has been proposed as a generic solution to domain adaptation and transfer learning problems [13, 26, 34].", "startOffset": 121, "endOffset": 133}, {"referenceID": 32, "context": "More recently, deep learning has been proposed as a generic solution to domain adaptation and transfer learning problems [13, 26, 34].", "startOffset": 121, "endOffset": 133}, {"referenceID": 41, "context": "In deep learning, a denoising autoencoder is a one-layer neural network trained to reconstruct input data from partial random corruption [43].", "startOffset": 137, "endOffset": 141}, {"referenceID": 25, "context": "This learned feature representation was applied to domain adaptation [26], where stacked denoising autoencoders (SDA) achieved top performance in sentiment analysis tasks.", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "[9] proposed a variant of SDA where the random corruption is marginalized out.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "05374 sification accuracy comparable with SDAs, with a remarkable reduction of the training time [9].", "startOffset": 97, "endOffset": 100}, {"referenceID": 12, "context": "More recently, deep learning architectures have demonstrated their ability to learn robust features and that good transfer performances could be obtained by just fine-tuning the neural network on the target task [13].", "startOffset": 212, "endOffset": 216}, {"referenceID": 24, "context": "[25] has shown that adding a domain prediction task while learning the deep neural network leads to better domain-invariant feature representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] proposed to add a multi-layer adaptation regularizer, based on a multi-kernel maximum mean discrepancy (MMD).", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "This term is the core element of the marginalized denoising autoencoder (MDA) [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 34, "context": "It can be seen as a generalization of the Marginalized Corrupted Features (MCF) framework [36] with a square loss (the MCF corresponds to the case whenW = Id).", "startOffset": 90, "endOffset": 94}, {"referenceID": 8, "context": "We follow the marginalized framework for optimizing the loss on corrupted data [9, 36], and minimize the loss expectation E[L].", "startOffset": 79, "endOffset": 86}, {"referenceID": 34, "context": "We follow the marginalized framework for optimizing the loss on corrupted data [9, 36], and minimize the loss expectation E[L].", "startOffset": 79, "endOffset": 86}, {"referenceID": 8, "context": "1 Domain Instance Denoising The first term we consider is the loss used by theMarginalized Denoising Autoencoder (MDA) [9].", "startOffset": 119, "endOffset": 122}, {"referenceID": 41, "context": "Its basic idea is to reconstruct the input data from a partial random corruption [43] with a marginalization that yields optimal reconstruction weights in a closed form.", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "[9] showed that by considering the limit case M \u2192 \u221e, the weak law of large numbers allows to rewrite the loss L1 as its expectation and the optimal W can be written as (see Appendix for details):", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "2 Learning with marginalized corrupted features Inspired by the Marginalized Corrupted Features (MCF) approach [36], we propose to marginalize the following loss:", "startOffset": 111, "endOffset": 115}, {"referenceID": 5, "context": "1 Reducing the MMD between domains The minimization of maximum mean discrepancy (MMD) [6] between the source and target domains is the state of art approach widely used in the literature.", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "It is often integrated in feature transformation learning [2, 37] or used as a regularizer for the cross-domain classifier learning [20, 34, 42].", "startOffset": 58, "endOffset": 65}, {"referenceID": 35, "context": "It is often integrated in feature transformation learning [2, 37] or used as a regularizer for the cross-domain classifier learning [20, 34, 42].", "startOffset": 58, "endOffset": 65}, {"referenceID": 19, "context": "It is often integrated in feature transformation learning [2, 37] or used as a regularizer for the cross-domain classifier learning [20, 34, 42].", "startOffset": 132, "endOffset": 144}, {"referenceID": 32, "context": "It is often integrated in feature transformation learning [2, 37] or used as a regularizer for the cross-domain classifier learning [20, 34, 42].", "startOffset": 132, "endOffset": 144}, {"referenceID": 40, "context": "It is often integrated in feature transformation learning [2, 37] or used as a regularizer for the cross-domain classifier learning [20, 34, 42].", "startOffset": 132, "endOffset": 144}, {"referenceID": 33, "context": "If we have labeled source and labeled target examples we can go one step further and modify the MMD to measure the distance between the means (centroids) of corresponding classes in the source and target domains [35].", "startOffset": 212, "endOffset": 216}, {"referenceID": 13, "context": "1), we explore a loss based on the domain classifier [14].", "startOffset": 53, "endOffset": 57}, {"referenceID": 23, "context": "Inspired by [24] who proposed to regularize intermediate layers in a deep learning model with a domain prediction task, [14] combines the domain prediction regularization with the MDA.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "Inspired by [24] who proposed to regularize intermediate layers in a deep learning model with a domain prediction task, [14] combines the domain prediction regularization with the MDA.", "startOffset": 120, "endOffset": 124}, {"referenceID": 38, "context": "The partial derivatives with respect to W can be written as a Sylvester linear matrix equation AW +WB = C, that we solve using the Bartels-Stewart algorithm [40].", "startOffset": 157, "endOffset": 161}, {"referenceID": 37, "context": "Two most popular datasets used to compare visual domain adaptation methods are the Office31 dataset [39] (OFF31) and the Office+Caltech10 [28] (OC10).", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": "For all images, we use the Decaf TF6 features [19] with the full training protocol [27] where all source data is used for training.", "startOffset": 46, "endOffset": 50}, {"referenceID": 26, "context": "For all images, we use the Decaf TF6 features [19] with the full training protocol [27] where all source data is used for training.", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "We adopt the experimental setting of [25] where documents are represented by a bag of unigrams and bi-grams with the 5000 most frequent common words selected and a tf-idf weighting scheme.", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "classifier [15].", "startOffset": 11, "endOffset": 15}, {"referenceID": 43, "context": "All selected parameter values are explained below: Besides, crossvalidation on the source is not the best way to set model parameters for transfer learning and domain adaptation [45].", "startOffset": 178, "endOffset": 182}, {"referenceID": 8, "context": "3) (as in [9]);", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "The second baseline refers to the original MDA method [9] and corresponds to S1 method in our framework.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "5%) in Table 2 outperforms the domain adaptive SVM [7] (70.", "startOffset": 51, "endOffset": 54}, {"referenceID": 20, "context": "3%) and domain adaptation via auxiliary classifiers [21] (84%), and slightly underperforms the more complex JDA [35] (87.", "startOffset": 52, "endOffset": 56}, {"referenceID": 33, "context": "3%) and domain adaptation via auxiliary classifiers [21] (84%), and slightly underperforms the more complex JDA [35] (87.", "startOffset": 112, "endOffset": 116}, {"referenceID": 21, "context": "5%) and TTM [22] (87.", "startOffset": 12, "endOffset": 16}, {"referenceID": 24, "context": "2%, see Table 2) is similar to the state-of-the art results obtained with the DomainAdversarial Neural Networks (DANN) [25], despite the fact that DANN uses a 5 layer stacked MDA where the 5 outputs are concatenated with input to generate 30,000 dimensional features, on which the network is trained.", "startOffset": 119, "endOffset": 123}, {"referenceID": 37, "context": "Concerning the semi-supervised scenario, it is much less used and most papers report results with SURF BOV features and the sampling protocol [39, 28].", "startOffset": 142, "endOffset": 150}, {"referenceID": 22, "context": "6%), SA [23] (53.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "6%), MMDT [30] (52.", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "We report results from [22].", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "We propose an extended framework for marginalized domain adaptation, aimed at addressing unsupervised, supervised and semisupervised scenarios. We argue that the denoising principle should be extended to explicitly promote domain-invariant features as well as help the classification task. Therefore we propose to jointly learn the data auto-encoders and the target classifiers. First, in order to make the denoised features domain-invariant, we propose a domain regularization that may be either a domain prediction loss or a maximum mean discrepancy between the source and target data. The noise marginalization in this case is reduced to solving the linear matrix systemAX = B which has a closed-form solution. Second, in order to help the classification, we include a class regularization term. Adding this component reduces the learning problem to solving a Sylvester linear matrix equation AX +BX = C, for which an efficient iterative procedure exists as well. We did an extensive study to assess how these regularization terms improve the baseline performance in the three domain adaptation scenarios. We present experimental results on two image and one text benchmark datasets, conventionally used for validating domain adaptation methods. We report our findings and comparison with state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}