{"id": "1702.06053", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Learning to Multi-Task by Active Sampling", "abstract": "One of the long-standing challenges in Artificial Intelligence is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for learning behavior in many goal-directed sequential tasks has been in the form of distillation based learning wherein a single student network learns from multiple task-specific teacher networks by mimicking the task-specific policies of the teacher networks. There has also been progress in the form of Progressive Networks which seek to overcome the catastrophic forgetting problem using gating mechanisms. We propose a simple yet efficient Multi-Tasking framework which solves many tasks in an online or active learning setup.", "histories": [["v1", "Mon, 20 Feb 2017 16:31:56 GMT  (1939kb,D)", "http://arxiv.org/abs/1702.06053v1", "9 pages"], ["v2", "Tue, 21 Feb 2017 04:38:43 GMT  (1945kb,D)", "http://arxiv.org/abs/1702.06053v2", "9 pages"], ["v3", "Sat, 25 Feb 2017 08:49:45 GMT  (1981kb,D)", "http://arxiv.org/abs/1702.06053v3", "10 pages + 3 page appendix"], ["v4", "Sun, 21 May 2017 12:47:34 GMT  (8403kb,D)", "http://arxiv.org/abs/1702.06053v4", "11 pages + 30 page appendix"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["sahil sharma", "ashutosh jha", "parikshit hegde", "balaraman ravindran"], "accepted": false, "id": "1702.06053"}, "pdf": {"name": "1702.06053.pdf", "metadata": {"source": "META", "title": "Online Multi-Task Learning Using Biased Sampling", "authors": ["Sahil Sharma", "Balaraman Ravindran"], "emails": ["<ssahil08@gmail.com>,", "<ravi@cse.iitm.ac.in>."], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "2. Background", "text": "In this section we describe the actor-critic algorithm and the A3C algorithm."}, {"heading": "2.1. Actor Critic Algorithms", "text": "In general, reinforcement learning agents are not equipped with a model of the environment (the specification of the underlying MDP) and must either learn optimal control in a model-free manner (b-Q function) or build a model of the environment. One of the model-free methods of learning optimal control in RL consists of two separate components: an actor and a critic. The actor is a parametric function (at-st) that can generally be calculated non-parametrically and outside of politics, we describe a parametrical version of the algorithms here. The actor is a parametrical function (an-st) that maps states in the state to an explicit policy (which represents a probability distribution of actions that can be carried out in the current state) acting according to the RL actor. The basis for improving policy (the objective function of the actor) derives from the stochastic policy gradient (SPG value)."}, {"heading": "2.2. Asynchronous Advantage Actor Critic", "text": "This is because the training data is distributed independently and identically, a problem that has been largely solved by the Asynchronous Advantage Actor Critic (A3C) algorithm (Mnih et al., 2016), by running multiple versions of the actor and critic networks asynchronously and collecting parameter updates in parallel. The asynchronous nature of the algorithm means that the different strands of actors explore different parts of the state and thus the parameter updates are uncorrelated. Parameter updates from different threads are applied to a global parameter vector that ensures that learning is merged across different threads. The algorithm uses the baseline that Q and the previous Q choice is for the most effective (most effective delective) function."}, {"heading": "3. Model Definition", "text": "We demonstrate our method using the LSTM version of the A3C algorithm (Mnih et al., 2016). Figure 1 contains an illustration of our method. To understand how our method works, one must first think about what a naive multitasking agent looks like and why / where it fails. We first describe a naive multitasking base agent A3C (NA3C) and then describe our approach (Biase Sampling A3C - BA3C) as a modification of this baseline."}, {"heading": "3.1. NA3C", "text": "The NA3C multi-tasking agent is a single A3C network which learns to perform k tasks in an-line learning fashion.The training period of an A3C agent can be divided into discrete episodes as it is an on-policy learning algorithm (See (Sutton & Barto, 1998) for a discussion on policy and off-policy reinforcement learning algorithms).In a single-task A3C agent, the training data from the same task is used in each single episode. However, in an on-policy multi-tasking agent based on A3C such as NA3C, at the end of an episode, the agent must decide what task to train on next task A3C agent, the task decision steps as task decision steps and these form the basis of our task step The NA3C agent decides random, the particular task on which it will train next, for 1 episode."}, {"heading": "3.2. BA3C", "text": "Our method is based on the abstract machine learning principle of active learning (Settles, 2010; Prince, 2004; Zhu, 2005). The core hypothesis that drives active learning in the usual machine learning contexts (such as classification problems) is that a machine learning algorithm can perform better with less marked learning examples if it selects the data from which it learns. An active learning algorithm can query an oracle (usually a human annotator) for labels of unmarked data instances. Active learning is important for many modern machine learning methods (particularly deep learning methods access to unmarked data), where access to unmarked data can be easy."}, {"heading": "4. Experimental Setup and Results", "text": "Agents in this work are trained using the LSTM version of the A3C algorithm (Mnih et al., 2016). Two of the hyperparameters we set for BA3C agents are \u03b2 (see (Mnih et al., 2016) for details) and \u03c4, which was introduced in the last section and controls the sharpness of the Softmax operation, which allows us to scan the identity of the next task to be trained. We vote \u03b2for the baseline NA3C agents. For our method, we need to set baseline scores as in the algorithm 2. Since the original A3C publication (Mnih et al., 2016) contains a different metric (human starts) for evaluating performance compared to the raw task scores (for which we want to optimize), we used the published baseline A3C scores as in the algorithm 2. (Sharma et al., 2017) (Table 4) for baseline scores."}, {"heading": "4.1. Evaluation", "text": "In most previous work such as (Parisotto et al., 2015), the arithmetic mean of the performance of the multi-tasking agent for the various tasks is considered to be the performance metric on which the multi-tasking agent is evaluated. Specifically, pam = (k \u2211 i = 1 ai bi) / kwhere p is the performance metric, ai is the score of the multi-tasking agent in task i, bi is the baseline score (of a single tasking agent) in task i and the sub-script am represents the fact that the arithmetic mean is calculated. We argue that this measurement is not robust enough. This is because a multi-tasking agent can achieve pam = 1 by being 6 times better than baseline on a task and is as bad as getting a 0 score in all other tasks. A good multi-tasking agent, which is as good as baseline score for single tasks."}, {"heading": "4.2. Shared Output Head agents", "text": "For this type of multitasking agent, the agent's behavior policy has a constant size of 18, the maximum possible number of actions in the Atari 2600 emulator. These agents need to learn from scratch using only the reward signal, the subset of actions relevant to a given task. If action a is not relevant to task g, then executing one in any state when executing g would not cause any movement at all and is the same as executing a no-op action. We refer to the BA3C version of these agents as BA3CSH and the NA3C version of these agents as NA3CSH. After setting the hyperparameter, we found the best hyperparameters for BA3CSH with \u03b2 = 0.02 and \u03c4 = 0.05. For NA3CSH, the best hyperparameter setting \u03b2 = 0.01. The development of the performance of BA3CSH and NA3CSH agents with training progress is well illustrated in Figure 2. As shown in Figure 2, the BA3CSH agents would be relative to the performance of most of the BA3CSH assignments."}, {"heading": "4.3. Different Output Head agents", "text": "Some work in Multi-Task-Learning (Parisotto et al., 2015) has experimented with baselines, where most layers of the multi-task agent are spread over tasks (to allow learning of a task-agnostic common representation of the different state spaces), but the last layer is task-specific. We also experimented with such an agent, which has an output header that is specific for each task. This agent is known as the BA3CDH Agent. The architectures for BA3CSH and BA3CDH Multi-Task Agent are exactly the same, except for the following difference highlighted below. The only difference in terms of the BA3CSH architecture is that instead of a final 18-dimensional Softmax layer, the 18-Pre-Softmax output vector is projected into k different vector subspaces, where k is the number of tasks that BA3CDH solves neural layers (the projection matrices are comparable to an end of a Wi-Fi)."}, {"heading": "4.4. Importance of the baseline scores", "text": "The crucial part of our method is the use of baseline scores to adequately track the performance of multitasking tasks when deciding on the next task to be trained. In this work, we have used the single-agent performance A3C used in our method (Sharma et al., 2017). In this section, we will show what happens when the baseline scores (defined in step 3, algorithm 2) are changed. Since the hyper parameters were tuned to MT1, the performance of our agents is understandably better on MT1 than on MT2 or MT3. Therefore, we selected the multitasking instances MT2 and MT3 and experimented with doubling the baseline scores used in our method. For example, b5, according to the task of Gopher, a part of MT2 was changed from 9400 to 18800. We would expect that such a change in bi-scores would lead to better performance, as the agent would not stop now, so the baseline scores may not be reached until the baseline is more aggressive in this case."}, {"heading": "4.5. Analysis of Sampling Distribution p", "text": "Figure 4 shows the evolution of the sampling distribution p in terms of training progress for a particular training thread. The sampling distribution p is captured in 1000 training steps and then averaged over 100 such consecutive snapshots to ensure that the curve is even. Graphic shows that certain games that are difficult to train (such as Space Invaders) are sampled more and more, while those that are easy to train, such as Crazy Climber, are sampled less and less."}, {"heading": "5. Conclusion and Future Work", "text": "We propose a simple but efficient method of training multitasking agents who successfully learn to perform online multitasking learning through a form of active learning. The most important finding of our work is that by selecting the task on which they train next, a multitasking agent can choose to focus their resources on tasks they are currently performing poorly. Although we do not claim that our method definitively solves the problem of online multitasking reinforcement learning, we believe that it is an important first step. In the current setup, the probability distribution that determines the next task is adaptable, but not learned per se. A good next step in this direction would be to determine the probability distribution from which the next episode is selected for training using an action-critical meta-controller-based setup. We leave it to future work to explore and experimentally validate such a setup."}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": null, "citeRegEx": "Dharshan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dharshan et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Badia", "Adria Puigdomenech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["Parisotto", "Emilio", "Ba", "Jimmy Lei", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1511.06342,", "citeRegEx": "Parisotto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2015}, {"title": "Does active learning work? a review of the research", "author": ["Prince", "Michael"], "venue": "Journal of engineering education,", "citeRegEx": "Prince and Michael.,? \\Q2004\\E", "shortCiteRegEx": "Prince and Michael.", "year": 2004}, {"title": "Policy distillation", "author": ["Rusu", "Andrei A", "Colmenarejo", "Sergio Gomez", "Gulcehre", "Caglar", "Desjardins", "Guillaume", "Kirkpatrick", "James", "Pascanu", "Razvan", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Hadsell", "Raia"], "venue": "arXiv preprint arXiv:1511.06295,", "citeRegEx": "Rusu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2015}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "4th International Conference on Learning Representations,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Active learning literature survey", "author": ["Settles", "Burr"], "venue": "University of Wisconsin, Madison,", "citeRegEx": "Settles and Burr.,? \\Q2010\\E", "shortCiteRegEx": "Settles and Burr.", "year": 2010}, {"title": "Introduction to reinforcement learning", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton", "Richard S", "McAllester", "David A", "Singh", "Satinder P", "Mansour", "Yishay"], "venue": "In NIPS,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Semi-supervised learning literature survey", "author": ["Zhu", "Xiaojin"], "venue": null, "citeRegEx": "Zhu and Xiaojin.,? \\Q2005\\E", "shortCiteRegEx": "Zhu and Xiaojin.", "year": 2005}], "referenceMentions": [{"referenceID": 7, "context": "Such combinations of DL and RL have resulted in agents which can solve complex visual control problems directly from pixels (Mnih et al., 2015; 2016; Schaul et al., 2015; Lillicrap et al., 2015).", "startOffset": 124, "endOffset": 194}, {"referenceID": 1, "context": "Such combinations of DL and RL have resulted in agents which can solve complex visual control problems directly from pixels (Mnih et al., 2015; 2016; Schaul et al., 2015; Lillicrap et al., 2015).", "startOffset": 124, "endOffset": 194}, {"referenceID": 0, "context": "A usual model trained using a DRL algorithm to solve the game of Sea Quest in the Atari 2600 emulator ALE (Bellemare et al., 2013) cannot play the game of", "startOffset": 106, "endOffset": 130}, {"referenceID": 6, "context": "The Policy Distillation framework (Rusu et al., 2015) and Actor-Mimic Networks (Parisotto et al.", "startOffset": 34, "endOffset": 53}, {"referenceID": 4, "context": ", 2015) and Actor-Mimic Networks (Parisotto et al., 2015) fall into this category of approaches.", "startOffset": 33, "endOffset": 57}, {"referenceID": 10, "context": "The basis for the improvement of the policy (the actor objective function) comes from the stochastic policy gradient (SPG) theorem (Sutton et al., 1999), although actor critic algorithms pre-date the SPG theorem.", "startOffset": 131, "endOffset": 152}, {"referenceID": 3, "context": "This problem was solved to a large extent by the Asynchronous Advantage Actor Critic (A3C) Algorithm (Mnih et al., 2016) by executing multiple versions of the actor and critic networks asynchronously and gathering parameter updates in parallel.", "startOffset": 101, "endOffset": 120}, {"referenceID": 3, "context": "We demonstrate our method with the help of LSTM version of the A3C algorithm (Mnih et al., 2016).", "startOffset": 77, "endOffset": 96}, {"referenceID": 4, "context": "Naive baseline agents such as NA3C have been experimented with in the context of DQNs by works such as (Parisotto et al., 2015) and (Rusu et al.", "startOffset": 103, "endOffset": 127}, {"referenceID": 6, "context": ", 2015) and (Rusu et al., 2015).", "startOffset": 12, "endOffset": 31}, {"referenceID": 6, "context": ", 2016) or the action-value predictions (Rusu et al., 2015; Parisotto et al., 2015) of multiple task-specific experts for being able to solve multiple tasks using a single machine learning agent.", "startOffset": 40, "endOffset": 83}, {"referenceID": 4, "context": ", 2016) or the action-value predictions (Rusu et al., 2015; Parisotto et al., 2015) of multiple task-specific experts for being able to solve multiple tasks using a single machine learning agent.", "startOffset": 40, "endOffset": 83}, {"referenceID": 4, "context": "Previous works (Parisotto et al., 2015; Rusu et al., 2016) have found it hard to train online multi-tasking agents which do not rely on expert supervision and are trained not using supervised learning algorithms but rather directly by maximizing the reward signals (that is, using reinforcement learning objectives).", "startOffset": 15, "endOffset": 58}, {"referenceID": 3, "context": "Agents in this work are trained using the LSTM version of the A3C algorithm (Mnih et al., 2016).", "startOffset": 76, "endOffset": 95}, {"referenceID": 3, "context": "(Mnih et al., 2016) for details) and \u03c4 which was introduced in the last section and controls the sharpness of the softmax operation from which we sample the identity of the next task to be trained on.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Since the original A3C publication (Mnih et al., 2016) contains a different metric (human starts) for evaluation of performance as compared to raw task scores (which we would like to optimize for), we took the published baseline A3C scores from (Sharma et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 4, "context": "In most previous works such as (Parisotto et al., 2015), the arithmetic mean of the performance of the multi-tasking agent on the different tasks is considered as the performance metric based on which the multi-tasking agent is evaluated.", "startOffset": 31, "endOffset": 55}, {"referenceID": 4, "context": "Some works in multi-task learning (Parisotto et al., 2015) have experimented with baselines wherein most layers of the multi-tasking agent are shared across tasks (to enable Table 2.", "startOffset": 34, "endOffset": 58}], "year": 2017, "abstractText": "One of the long-standing challenges in Artificial Intelligence for goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for learning behavior in many goal-directed sequential tasks has been in the form of distillation based learning wherein a single student network learns from multiple task-specific teacher networks by mimicking the task-specific policies of the teacher networks. There has also been progress in the form of Progressive Networks which seek to overcome the catastrophic forgetting problem using gating mechanisms. We propose a simple yet efficient Multi-Tasking framework which solves many tasks in an online or active learning setup.", "creator": "LaTeX with hyperref package"}}}