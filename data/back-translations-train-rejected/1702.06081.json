{"id": "1702.06081", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Learning Non-Discriminatory Predictors", "abstract": "We consider learning a predictor which is non-discriminatory with respect to a \"protected attribute\" according to the notion of \"equalized odds\" proposed by Hardt et al. [2016]. We study the problem of learning such a non-discriminatory predictor from a finite training set, both statistically and computationally. We show that a post-hoc correction approach, as suggested by Hardt et al, can be highly suboptimal, present a nearly-optimal statistical procedure, argue that the associated computational problem is intractable, and suggest a second moment relaxation of the non-discrimination definition for which learning is tractable.", "histories": [["v1", "Mon, 20 Feb 2017 17:52:14 GMT  (99kb,D)", "http://arxiv.org/abs/1702.06081v1", null], ["v2", "Fri, 29 Sep 2017 14:52:10 GMT  (143kb,D)", "http://arxiv.org/abs/1702.06081v2", "28 pages"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["blake woodworth", "suriya gunasekar", "mesrob i ohannessian", "nathan srebro"], "accepted": false, "id": "1702.06081"}, "pdf": {"name": "1702.06081.pdf", "metadata": {"source": "CRF", "title": "Learning Non-Discriminatory Predictors", "authors": ["Blake Woodworth", "Suriya Gunasekar", "Mesrob I. Ohannessian"], "emails": ["blake@ttic.edu", "suriya@ttic.edu", "mesrob@ttic.edu", "nati@ttic.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to move to another world in which they will be able to integrate."}, {"heading": "2 Sub-optimality of Post-training Correction", "text": "In fact, it is in such a way that most of us are able to concentrate on the real world, namely in such a way that they are able to establish themselves in the world, in such a way that they are able to establish themselves in the world, in such a way that they are able to establish themselves in the world, in such a way that they are able to establish themselves in the world. (...) In the real world, in such a way that they are able to establish themselves in the world, in such a way that they are able to live, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, such a world, in such a world, in such a world, such a world, in such a world, such a world, such a world, such a world, such a world, in such a world, in such a world, such a world, in such a world, in such a world, such a world, such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, such a world, in such a world, in such a world, in such a world, in such a world, in such a world, such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, in such a world, such a world, in such a world, in such a world, in such a world"}, {"heading": "3 Detecting Discrimination in Binary Predictors", "text": "In the following sections we will look at the tools for integrating non-discrimination into the supervised learning system. Accurate and non-discriminatory predictors are used in the formulation of learning algorithms. (Precise and non-discriminatory predictors are used in the formulation of learning algorithms.) It is important to address the problems arising from the handling of finite data. Let's start with the generalised probability criteria in the population, but we can also look at finite samples. Let's look at the task of binary classification, where both A, Y and the predictors Y define approximate non-discrimination values in {0, 1}. The definition of population class-related true and false positive rates (Y) = P (Y = 1 | Y = y) and the fact that non-discrimination corresponds to the condition."}, {"heading": "4 Integrated Learning of Non-Discriminatory Binary Predictors", "text": "In Example 1, we have seen that although there is an almost perfect non-discriminatory predictor within the hypothesis class, if we ignore non-discrimination in training with 0-1 loss, then an optimal posthoc correction with (3) results in a bad predictor with no better than random accuracy. Thus, in order to find a predictor that is both almost non-discriminatory and has almost optimal losses for general hypothesis classes, it is necessary to include non-discrimination criteria in the learning process. Ideally, for a hypothesis class H, we would find the optimal non-discriminatory predictor: Y \u0445 = argmin h-H E '(h (X, A), Y) s.t. (h samples) = centric y = {0, 1}. (9) However, as motivated in Section 3, it is impossible to learn 0-discriminating predictors from finite samples."}, {"heading": "4.1 Two step framework for binary predictors", "text": "We partition the training data consisting of n independent samples S = (xi, ai, yi), P (X, A, Y), N = 1 into two subgroups S1 and S2, which may be used in step 1 and step 2, respectively (6), Y (for a predictor h, H and y, a), a), a), b), b), b), b), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c, c), c), c, c), c), c), c), c), c), c), c), c), c), c), c, c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c, c), c), c), c), c, c), c), c), c), c), c), c), c), c), c), c, c), c), c), c), c), c), c, c), c), c), c), c), c), c), c), c), c),"}, {"heading": "4.2 Statistical guarantees", "text": "In this section, we discuss the statistical properties of the estimators Y and Y of step 1 and step 2. We define the following notation to briefly describe the quality of a predictor: Definition 4. Q (L,) = {h: X \u2192 {0, 1}: D (h) \u2264, L (h) \u2264, L (h) \u2264 L) denotes the set of -discriminating binary predictors with loss L. The following theorem shows the statistical learning ability of the hypothesis classes H with reference to the best non-discriminating predictor in H using the two-step framework. In the following results, the notation for both (Y) is denoted by definition 3.For y, a}, let Pya = P (Y = y, A = a) denotes the group result probabilities, and let V C (H) denotes the Vapnik-Chervonenkis dimension of a hypothesis class H.Theorem 1. L.Let = Pya = y = P, A = probabilities of a group result, and V = the group result."}, {"heading": "5 Computational Intractability", "text": "The proposed method for learning non-discriminatory predictors from a sample is statistically optimal, but it is clearly mathematically insoluble for almost any interesting hypothesis Grade level (10) involves minimizing the discrimination problem 0-1. As is typically the case with discriminatory learning problems, we are therefore looking for alternative loss functions and hypotheses classes to find a mathematically feasible forecaster. A natural choice is the hypotheses category of linear predictors with a convex loss function. In this case, we would like to have an efficient algorithm for finding a non-discriminatory predictor that has a convex loss that is about as good as the loss of the best non-discriminating linear predictor. However, even in the case of binary A and Y and without the difficulty of optimizing the 0-1 loss, the non-discriminatory constant is extremely real for the highly discriminating predictors that require a predictor."}, {"heading": "6 Relaxing non-discrimination", "text": "Motivated by the result of the hardship in Section 5, we are now moving to loosen the criterion of equivalence of conditions in general. Previous work by Zafar et al. [2016] deals with a concept of non-discrimination, which amounts to a loosening of the discrimination of equal opportunities. [2016] It is therefore a concept of non-discrimination, which amounts to loosening the discrimination of equal opportunities. [Y = Y = Y = y = y = y, A = 0 = P (Y = Y = Y = y), i.e. the first moments of Y = must3See Daniely [2015] for a description of the problem. They propose to optimize the loss of the ring, which is subject to a limitation of this initial moment. Their work is primarily applied and does not provide any guarantees of learning or non-discrimination for their learning rule, both of which we address in this section for a different relativization."}, {"heading": "7 Conclusion", "text": "In this work, we took the first steps toward a statistical and computational theory of learning non-discriminatory (balanced odds) predictors. We realized that a posthoc correction may not be optimal, and developed a statistically optimal two-step procedure after observing that a direct ERM approach is not enough. Mathematically, working with binary non-discrimination is essentially as difficult as agnostically learnable binary predictors, and therefore we should expect to fall back on relaxation. We took the first step in Section 6, where we considered a second relaxation of non-discrimination leading to comprehensible learning. We hope that this will not be the last word in learning non-discriminatory predictors, and that this work will spark interest in further understanding of our relaxation by proposing other relaxations and studying other computationally efficient practices with verifiable guarantees."}, {"heading": "A Deferred Proofs From Section 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Example 1", "text": "We repeat the example of convenience: Example 1: If the hypothesis class is unlimited (Y = 1 = 1), then for any (0, 1 / 4) constant (0, 1 / 4) there is a distribution D \u2212 \u2212 \u2212 \u2212 \u2212 The optimal non-discriminating predictor prediction with respect to the 0-1 loss hypothesis has a maximum of 2, but b) the post-hoc predictor prediction with respect to the 0-1 loss hypothesis and the same distribution D, a) the optimal non-discriminating predictor prediction with respect to the loss with respect to the hinge loss with a maximum of 4, but b) the post-hoc predictor predictor hypothesis class of Bayes is at least 1. Consider the unlimited hypothesis class of all (possibly randomized) functions from (X, A) to {0, 1} the following distribution over (X, A, Y)."}, {"heading": "A.2 Proof of Example 2", "text": "We affirm the example of convenience: Example 2. Leave H the class of linear predictors with Y = 4b = 4b | for some (2 / 25, 1 / 4). Then there is a distribution D that a) the optimal non-discriminatory predictor in H has a constant predictive value of 1 / 4.2 with respect to the square loss at most 116 + 3 2 + 32, but b) the post-hoc correction of the optimal square loss predictor in H has a constant predictive value of 1 / 4.2. Similarly, for the class H of the sparse linear predictors (0, 1 / 4) there is a distribution D that an optimal non-discriminatory predictor in H with respect to the square loss has a maximum of 2 \u2212 4 2, but b) the post-hoc correction of the optimal square loss regressor in H has again a constant predictor (viratic) loss of 4.1 / 2."}, {"heading": "B Deferred Proofs From Section 3", "text": "Remember the notation \u0441ya (h) = maxy | \u03b3y0 (h) \u2212 \u03b3y1 (h) | and \u0441Sya (h) = maxy | \u03b3Sy0 (h) \u2212 \u03b3Sy1 (h) | from definition 3. To avoid clutter, we sometimes drop the dependence on h for \u03b3ya if h is evident from the context. Also, remember that S = {(xi, yi, ai): i [n]]% Pn (X, Y, A) and Pya = P (Y = y, A = a).The following problem regarding the concentration of \u0421S is applied in several proofs. B.2.Lemma 4. For \u0421\u0430\u043d\u0438\u043d\u0438\u0439 (0, 1 / 2) and a binary predictor h, if nPya > 2 log 16 / \u043c, the following hold applies P (| \u0441\u0430\u0441\u0430\u0441S (h) \u2212 \u0441S (h) | > 2 maxya \u041alog 16 / nya) (36)."}, {"heading": "B.1 Proof of Lemma 1", "text": "Lemma 1 Given n i.i.d. samples S (Y) > 0, if n > 16 maxya log 16 / \u03b4Pya\u03b12, then with a probability of more than 1 \u2212 \u03b4, T fulfills, T (Y, S, \u03b1) = {0, if Y (Y) is 0-discriminatory on population1, if Y (Y) is at least \u03b1-discriminatory on population1. Remember that T (Y) = 1 (Y (Y) > \u03b1n). Select 2 max ya log 16 / \u0441nPya < \u03b1n < \u03b1 \u2212 2 max ya log 16 / \u0441nPya. Then the following results will easily follow from Lemma 4, 1. If Y (Y) is non-discriminatory, i.e., 2 max ya log 16 / \u03b4n) = 0.P (Y) maxi (Y) = 1) = P (T (Y)."}, {"heading": "B.2 Proof of Lemma 4", "text": "Remember that Pya = P (Y = y, A = a, Ai = a), S = (Xi, Yi, Ai): i = Y = y, Ai = a and nSya = a (Yi = y, Ai = a). With slight misuse of notation, we define random variables Sya = [i: Yi = y, Ai = a]. We then have Sya = Sya (h) | Sya = Sya = j Sya h (xj) nSya \u0445 1 nSya \u0445 (Sya, n S ya) with E [Sya | Sya] = Sya \u2212 Sya (Sya \u2212 a) = Sya P (Sya \u2212 a) = Sya Sya P (Sya \u2212 a) P (Sya \u2212 a) P (Sya), Sya (Sya), Sya, Sya (P), Sya (Sya), Sya (Sya, Sya, Sya (P), P (Sya), Sya (Sya), Sya (Sya, Sya, Sya (P), P (Sya), Sya (Sya, Sya, Sya, Sya (P), P (Sya), P (Sya), Sya (Sya, Sya, Sya, Sya (P), P (Sya, Sya), P (Sya, Sya, Sya, Sya, Sya (P), P (Sya, Sya), P (Sya, Sya, Sya (Sya), Sya (Sya, Sya, Sya, Sya, Sya, Sya, Sya, Sya (Sya), P (Sya), P (Sya, Sya (Sya, Sya), Sya (Sya, Sya, Sya, Sya, Sya, Sya, Sya (Sya), P (Sya), P (Sya, Sya (Sya), Sya (Sya, Sya, Sya, Sya, Sya, Sya, Sya (Sya), Sya (Sya), Sya (Sya, Sya, Sya, Sya"}, {"heading": "C Deferred Proofs From Section 4", "text": "We use the notation A \u2264 \u03b4 B to indicate that A \u2264 B is with a probability greater than 1 \u2212 \u03b4. Let us remember the notation \u0394ya (h) = maxy | \u03b3y0 (h) \u2212 \u03b3y1 (h) | and \u0394Sya (h) = maxy | \u03b3Sy0 (h) \u2212 \u03b3Sy1 (h) | by definition 3. To avoid clutter, we sometimes drop the dependence on h for \u03b3ya when h is evident from the context. Finally, in this section C, C1, and C2 denote absolute constants that are not necessarily the same for every occurrence."}, {"heading": "C.1 Proof of Theorem 1", "text": "Theorem 1: Let us n = 1: 1: 1: 2: 1: 2: 2: 2: 2: 2: 2: 2: 2: 3: 3: 3: 3: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 5: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 5: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 4: 5: 5: 5: 5: 5: 5: 5: 5"}, {"heading": "C.2 Proof of Lemma 2 and Lemma 3", "text": "Lemma 2 Under the conditions of theorem 1, w.p greater than 1 \u2212 \u03b4, Y \u00b2 (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction) (without distinction)."}, {"heading": "C.3 Supporting Lemmas for Proof of Theorem 1", "text": "Lemma 5: Let HS1\u03b1n = (1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5),"}, {"heading": "C.4 Proof of Theorem 2", "text": "The boundary distribution over (A, Y) is p = mina, y P (A = hi = a, Y = y = 1. Since the definition of fairness (1) is invariable in order to exclude the label of A \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 n = 0.5 errors, it is assumed without loss of generality that p = A = 1, Y = 1. For \u03b1 (0, 1 / 2), the distribution D over (X, A = 0, Y) is more than 1,..., n P (X1 = y = y) = 1 \u2212 P (0, Y = 1 \u2212 \u03b1 P (0, Y = 0, 1), the distribution D over (1) = 1 for i = 2, 3,..., n P (Xi = 1 | Y = 1) \u00b7 0 = 1 for i = 2, 3..., n P (Xi = 1) = 1, 1, hi = 1, 1 = 1, 1, 1 = 1."}, {"heading": "D Proof of Theorem 3", "text": "We assume that an algorithm that takes as its starting point a hypothesis of class H, a problem of distribution D over (X, A, Y, Y) is (X, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D,"}, {"heading": "D.1 Deferred proofs", "text": "The Evidence for Lemma 7. Definition h (X, A) = Sign (1 w) = Sign (PD) = Sign (PD) (PD) (PD) (PD) (1, 0) = Sign (PD) = Sign (PD) (PD) = Sign (PD) = Sign (PD) = Sign (PD) = Sign (PD) = Sign (PD) = Sign (PD) = Sign h = Sign (PD) = Sign (PD) = (PD) = Sign h = Sign (PD) (PD) = (PD) (PD) (PD) = Sign (PD) (PD) = (PD) (PD) = (PD) (PD = PD) (PD) = (PD) (PD) = (PD) (PD) = (PD) (PD) (PD = PD) (PD) (PD) = (PD) (PD) (PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD) = PD (PD)) = PD (PD (PD) = PD (PD) = PD (PD) = PD (PD (PD) = PD (PD) = PD (PD) = PD (PD (PD) = PD)) (PD) (PD (PD (PD)) = PD (PD (PD) = PD (PD))) (PD (PD (PD) = PD) (PD) (PD (PD)) (PD (PD (PD (PD))) = PD) (PD (PD (PD (PD)) = PD (PD (PD)))"}, {"heading": "E Proofs for Section 6 - Relaxing non-discrimination", "text": "We use \u03a3 \u00b7 to denote covariances involving a vector, and we reserve \u03c3 \u00b7 for scalar covariances."}, {"heading": "E.1 Proof of Theorem 4", "text": "Let us first show the second assertion. Let R be any linear problem. By linearity, it follows that (R, A, Y) are common to Gaussian. Through the conditional formula of covariance, we have: \u03c3RA | Y = \u03c3RY \u03c3Y A / \u03c32Y. If R meets the balanced correlations, then the right side here is 0. It follows that R and A are unconditionally correlated to Y. However, since they are common to Gaussian, they are also conditionally independent to Y. Therefore, R also meets the balanced probabilities of non-discrimination. The inverse also applies: If R meets the balanced probabilities, then R and A are uncorrelated to Y, and therefore are satisfied with egalized correlations. Now let us go back to the main claim, without loss of universality, that all variables are centered."}, {"heading": "E.3 Proof of Theorem 5", "text": "Let us start with R? as: R? = w? T [X A] = w-T [X A] \u2212 \u03b1TvT\u03a3 \u2212 1 [X A], [X A] = R-TvT\u03a3 \u2212 1 [X A], [X A] [X A] [X A]] Next, let us remember that v = \u03a3 [XA], A \u2212 \u03a3 [XA], Y [Y] = [0dim (A) \u00d7 dim (X) Idim (A)] [X A] = [X A] \u03a3 [X A] \u2212 1 [X A], [X-dim (A) \u00d7 dim (X) IdiA], we TYA \u2212 1 [X A], [X A] \u2212 1 [X A], [X A] \u2212 1 [X A], [X A], RV [A], TYA \u2212 1 [X], [X] TYA], [X] TYA], [X] A], [X] A], [X] A], [X] A], [X] A] A], [X] A], [X] A], [X] A] A], [X] A], [X] A] A], [X] A] A], [X [X] A] A], [X] A], [X] A] A], A] A [X [X, A] A] A], A] A [X [X], A] A [X], A] A [X, A] A] A [X, A] A]"}], "references": [{"title": "Introduction to statistical learning theory", "author": ["Olivier Bousquet", "St\u00e9phane Boucheron", "G\u00e1bor Lugosi"], "venue": "In Advanced lectures on machine learning,", "citeRegEx": "Bousquet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bousquet et al\\.", "year": 2004}, {"title": "Complexity theoretic limitations on learning halfspaces", "author": ["Amit Daniely"], "venue": "arXiv preprint arXiv:1505.05800,", "citeRegEx": "Daniely.,? \\Q2015\\E", "shortCiteRegEx": "Daniely.", "year": 2015}, {"title": "From average case complexity to improper learning complexity", "author": ["Amit Daniely", "Nati Linial", "Shai Shalev-Shwartz"], "venue": "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Daniely et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2014}, {"title": "Equality of opportunity in supervised learning", "author": ["Moritz Hardt", "Eric Price", "Nathan Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hardt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2016}, {"title": "Big data: A report on algorithmic systems, opportunity, and civil rights. 2016", "author": ["White House"], "venue": "URL https://obamawhitehouse.archives.gov/sites/default/files/microsites/", "citeRegEx": "House.,? \\Q2016\\E", "shortCiteRegEx": "House.", "year": 2016}, {"title": "Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment", "author": ["Muhammad Bilal Zafar", "Isabel Valera", "Manuel Gomez-Rodriguez", "Krishna P. Gummadi"], "venue": "CoRR, abs/1610.08452,", "citeRegEx": "Zafar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zafar et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "We consider learning a predictor which is non-discriminatory with respect to a \u201cprotected attribute\u201d according to the notion of \u201cequalized odds\u201d proposed by Hardt et al. [2016]. We study the problem of learning such a non-discriminatory predictor from a finite training set, both statistically and computationally.", "startOffset": 157, "endOffset": 177}, {"referenceID": 3, "context": "The particular notion of non-discrimination we consider here is \u201cequalized odds\u201d, recently presented and studied by Hardt et al. [2016]:", "startOffset": 116, "endOffset": 136}, {"referenceID": 3, "context": "Informally, we require that even if the correct label Y provides information about the protected attribute A, if we already know Y , the prediction does not provide any additional information about See Hardt et al. [2016] for a discussion on why it might be necessary for a non-discriminatory predictor to use A", "startOffset": 202, "endOffset": 222}, {"referenceID": 3, "context": "See Hardt et al. [2016] for further discussion of the definition, its implications, and comparisons to alternative notions.", "startOffset": 4, "endOffset": 24}, {"referenceID": 3, "context": "One possible approach to learning a non-discriminative predictor is post hoc correction [Hardt et al., 2016]: first learn a good predictor ignoring non-discrimination, i.", "startOffset": 88, "endOffset": 108}, {"referenceID": 3, "context": "When the protected attribute A and the target Y are both binary, the post hoc correction algorithm proposed by Hardt et al. [2016] can be applied to a binary or real-valued predictor \u0176 \u2208 H, deriving a randomized binary predictor that is non-discriminatory.", "startOffset": 111, "endOffset": 131}, {"referenceID": 3, "context": "1 Hardt et al. [2016]] A predictor \u1ef8 is derived from a random variable R and protected attribute A if it is a possibly randomized function of (R,A) alone.", "startOffset": 2, "endOffset": 22}, {"referenceID": 3, "context": "For binary classification problems, the optimal post hoc correction \u1ef8 for a binary or real valued predictor \u0176 \u2208 R is a straightforward ternary optimization problem: it is simply the nondiscriminatory, derived, binary predictor that minimizes the expectation of loss ` (Hardt et al. [2016]): \u1ef8 = argmin f :R\u00d7{0,1}7\u2192{0,1} E ` ( f(\u0176 , A), Y )", "startOffset": 269, "endOffset": 289}, {"referenceID": 3, "context": "Indeed, Hardt et al. [2016] show that when the target Y is binary, if we can first find a predictor R that is exactly or nearly Bayes optimal for the squared loss over an unconstrained hypothesis class, then applying the post hoc correction (3) using the 0-1 loss (i.", "startOffset": 8, "endOffset": 28}, {"referenceID": 3, "context": "The above optimization problem is a finite sample adaptation of the post hoc correction in (3) proposed by Hardt et al. [2016]. As with the post hoc correction on the population (3), estimating a predictor \u1ef8 \u2208 P(\u0176 ) derived from (\u0176 , A) is simply optimization over the following four parameters that completely specify \u1ef8 ,", "startOffset": 107, "endOffset": 127}, {"referenceID": 1, "context": "Unfortunately, using a result by Daniely [2015] even this is computationally intractable: Theorem 3.", "startOffset": 33, "endOffset": 48}, {"referenceID": 1, "context": "Unfortunately, using a result by Daniely [2015] even this is computationally intractable: Theorem 3. Let L\u2217 be the loss of the optimal linear predictor whose sign is non-discriminatory. Subject to the assumption that refuting random K-XOR formulas is computationally hard,3 the learning problem of finding a possibly randomized function f such that Lhinge(f) \u2264 L\u2217+ such that sign(f) is \u03b1-discriminatory requires exponential time in the worst case for < 1 8 and \u03b1 < 1 8 . The proof goes through a reduction from the hardness of improper, agnostic PAC learning of Halfspaces. Given a distribution D over (X,Y ) and the knowledge that there is a linear predictor which achieves 0-1 loss L\u2217 on D, we construct a new distribution D\u0303 over (X\u0303, \u00c3, \u1ef8 ) such that an approximately non-discriminatory predictor with small hinge loss can be used to make accurate predictions on D, even if it is not a linear function. The distribution D\u0303 is identical to the original distribution D when conditioned on \u00c3 = 1, and is supported on only two points conditioned on \u00c3 = 0. The probabilities of the two points are constructed so that satisfying non-discrimination requires making accurate predictions on the \u00c3 = 1 population, and thus on D. In particular, for parameters , \u03b1 < 1 8 , the predictor will have 0-1 loss at most 15 16L \u2217 + 47 128 on D, which is bounded away from 12 when L \u2217 < 1 10 . Since Daniely [2015] prove that finding a predictor with accuracy bounded away from 1 2 is hard in general, we conclude that the learning problem is computationally hard.", "startOffset": 33, "endOffset": 1399}, {"referenceID": 4, "context": "Previous work by Zafar et al. [2016] addresses a notion of non-discrimination which amounts to relaxing the equalized odds constraint that P(\u0176 = \u0177 | Y = y,A = 0) = P(\u0176 = \u0177 | Y = y,A = 1) to the constraint that E[\u0176 | Y = y,A = 0] = E[\u0176 | Y = y,A = 1], i.", "startOffset": 17, "endOffset": 37}, {"referenceID": 1, "context": "the first moments of \u0176 must See Daniely [2015] for a description of the problem.", "startOffset": 32, "endOffset": 47}, {"referenceID": 0, "context": "Using this in standard VC dimension uniform bound [Bousquet et al., 2004], we have the following with high probability,", "startOffset": 50, "endOffset": 73}, {"referenceID": 0, "context": "Using (42) and the standard VC dimension uniform bound [Bousquet et al., 2004], the following holds with high probability for absolute constants C1 and C2,", "startOffset": 55, "endOffset": 78}, {"referenceID": 1, "context": "We will show that such an algorithm can be used to improperly weakly learn Halfspace which, subject to the complexity assumption that refuting random K-XOR formulas is hard, was shown to be computationally hard by Daniely [2015]. We conclude that A must be computationally hard to compute.", "startOffset": 214, "endOffset": 229}, {"referenceID": 1, "context": "3 from Daniely [2015] proves that there is no algorithm running in time polynomial in the dimension d that can return a predictor achieving 0-1 error \u2264 12 \u2212 d \u2212c with high probability", "startOffset": 7, "endOffset": 22}, {"referenceID": 1, "context": "3 in Daniely [2015] involves a distribution D that is supported on the unit hypercube in Rd, thus the predictor h\u2217 \u2016w\u2217\u20162 \u221a d \u2208 [\u22121, 1] with probability 1, and has the same 0-1 loss as h\u2217.", "startOffset": 5, "endOffset": 20}], "year": 2017, "abstractText": "We consider learning a predictor which is non-discriminatory with respect to a \u201cprotected attribute\u201d according to the notion of \u201cequalized odds\u201d proposed by Hardt et al. [2016]. We study the problem of learning such a non-discriminatory predictor from a finite training set, both statistically and computationally. We show that a post-hoc correction approach, as suggested by Hardt et al, can be highly suboptimal, present a nearly-optimal statistical procedure, argue that the associated computational problem is intractable, and suggest a second moment relaxation of the non-discrimination definition for which learning is tractable.", "creator": "LaTeX with hyperref package"}}}