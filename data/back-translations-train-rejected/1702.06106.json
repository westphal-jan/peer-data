{"id": "1702.06106", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "An Attention-Based Deep Net for Learning to Rank", "abstract": "In information retrieval, learning to rank constructs a machine-based ranking model which given a query, sorts the search results by their degree of relevance or importance to the query. Neural networks have been successfully applied to this problem, and in this paper, we propose an attention-based deep neural network which better incorporates different embeddings of the queries and search results with an attention-based mechanism. This model also applies a decoder mechanism to learn the ranks of the search results in a listwise fashion. The embeddings are trained with convolutional neural networks or the word2vec model. We demonstrate the performance of this model with image retrieval and text querying data sets.", "histories": [["v1", "Mon, 20 Feb 2017 18:47:25 GMT  (171kb,D)", "https://arxiv.org/abs/1702.06106v1", null], ["v2", "Mon, 15 May 2017 23:57:57 GMT  (452kb,D)", "http://arxiv.org/abs/1702.06106v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["baiyang wang", "diego klabjan"], "accepted": false, "id": "1702.06106"}, "pdf": {"name": "1702.06106.pdf", "metadata": {"source": "META", "title": "An Attention-Based Deep Net for Learning to Rank", "authors": ["Baiyang Wang", "Diego Klabjan"], "emails": ["BAIYANG@U.NORTHWESTERN.EDU", "D-KLABJAN@NORTHWESTERN.EDU"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of us are able to set out in search of new paths to follow."}, {"heading": "2. Literature Review", "text": "To begin with, the ranking probability is defined as: P (0) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (2) x (3) x (3) x (3) x (3) x (3) x (3) x (4) x (4) x (4) x (4) x (5) x (5) x (5) x (5) x (5) x (5) x (5) x (5) x (5) x (5) x) x (5) x (5) x (5) x (5) x (5) x (5) x (5) x (5) x (5) x) x (5) x."}, {"heading": "3. Model and Algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Introduction to the Model", "text": "Both queries and search results can be embedded with neural networks. Given an input vector x0 representing a query or search result, we call the l layer in a neural network xl Rdl, l = 0, 1,.., L. However, we have the following ratio xl + 1 = f (Wlxl + bl), l = 0, 1,.., L \u2212 1. (4) where Wl is a dl + 1 \u00d7 dl weight matrix, bl Rdl + 1 is the bias, and f is a nonlinear activation function. If the target is classification with C categories, then (P (y = 1),.., P (y = C)) = softmax (WLxL + bL), (5) where y is a class indicator and softmax (u) is another activation function."}, {"heading": "3.2. Model Construction", "text": "We assume that the results will be shown in order r 1."}, {"heading": "3.3. Model Calibration", "text": "To calibrate the parameters of this model, we apply the stochastic gradient descend algorithm (Reddy, 1977), which maintains a number of paths at each iteration, i.e. a subsample of the training data. To classify the search results for the test data, we apply the beam search algorithm (Reddy, 1977), which maintains a number of paths, i.e. sequences of r-1,..., r-t at state t, which has the highest log probability. Other paths are trimmed at each state t, and finally the path with the highest log probability is chosen.Alternatively, we can consider the hinge loss function to replace the Softmax function. Compare a potentially selected result r-t with another unranked result rt-t, with rt-1,."}, {"heading": "4. Data Studies", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. The MNIST Data Set", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4.2. The CIFAR-10 Data Set", "text": "The CIFAR-10 dataset contains images from 10 different classes. There are 50,000 training images and 10,000 test images. Again, we took each image as a query and randomly generated 30 search results with 5 images of the same class and 25 images of different classes. The order is imposed in the same way as MNIST and based on different classes. We use 5 Convolutionary Neural Networks in the same way as MNIST based on the one in Table 5.We apply a batch size of 50, a learning rate of 0.0005 and a fixed number of 20 epochs to train AttRN. The error rates and standard deviations are in Table 6. AttRNHL again achieves the best performance, while AttRN-SM proves to be unsuitable for this dataset. Table 7 also shows that replacing the pooling functions from \"medium\" to \"maximum\" again produces very small changes in error rates, which means that our model is quite robust.Again, we show three good cases of retrieval and three worse images can be considered as the actual ones in each of the HL-4."}, {"heading": "4.3. The 20 Newsgroups Data Set", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves if they do not. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "5. Conclusion", "text": "In this paper, we have proposed a new neural network for learning-to-rank problems that uses the attention mechanism to include different embedding of search queries and results, and classifies search results according to a list-by-list approach. Data experiments show that our model brings improvements over the most modern techniques. In the future, it would be interesting to consider improving the RNN structure in the attention mechanism and adapting the embedding part of the neural network to this problem."}], "references": [{"title": "Mining text data", "author": ["C. Aggarwal", "Zhai", "C. (eds"], "venue": null, "citeRegEx": "Aggarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Aggarwal et al\\.", "year": 2012}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Latent dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Learning to rank using gradient descent", "author": ["C. Burges", "T. Shaked", "E. Renshaw", "A. Lazier", "M. Deeds", "N. Hamilton", "G. Hullender"], "venue": "In Proceedings of the 22th International Conference on Machine Learning (ICML", "citeRegEx": "Burges et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Burges et al\\.", "year": 2005}, {"title": "Learning to rank with nonsmooth cost functions", "author": ["C. Burges", "R. Regno", "Q. Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Burges et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Burges et al\\.", "year": 2006}, {"title": "Learning to rank: from pairwise approach to listwise approach", "author": ["Z. Cao", "T. Qin", "T. Liu", "M. Tsai", "H. Li"], "venue": "In Proceedings of the 24th International Conference on Machine Learning (ICML", "citeRegEx": "Cao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2007}, {"title": "An online algorithm for large scale image similarity learning", "author": ["G. Chechik", "V. Sharma", "U. Shalit", "S. Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Chechik et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chechik et al\\.", "year": 2009}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["K. Cho", "A. Courville", "Y. Bengio"], "venue": "IEEE Transactions on Multimedia,", "citeRegEx": "Cho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Convolutional neural network committees for handwritten character classification", "author": ["Ciresan", "Dan Claudiu", "Meier", "Ueli", "Gambardella", "Luca Maria", "Schmidhuber", "J\u00fcrgen"], "venue": "In ICDAR,", "citeRegEx": "Ciresan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2011}, {"title": "Gaussian lda for topic models with word embeddings", "author": ["Das", "Rajarshi", "Zaheer", "Manzil", "Dyer", "Chris"], "venue": "In The 2015 Conference of the Association for Computational Linguistics", "citeRegEx": "Das et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Das et al\\.", "year": 2015}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning to rank for information retrieval", "author": ["T. Liu"], "venue": null, "citeRegEx": "Liu,? \\Q2011\\E", "shortCiteRegEx": "Liu", "year": 2011}, {"title": "Better digit recognition with a committee of simple neural nets", "author": ["Meier", "Ueli", "Ciresan", "Dan Claudiu", "Gambardella", "Luca Maria", "Schmidhuber", "J\u00fcrgen"], "venue": "In ICDAR,", "citeRegEx": "Meier et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Meier et al\\.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Speech understanding systems: summary of results of the five-year research effort at carnegie-mellon university", "author": ["R. Reddy"], "venue": "Technical Report,", "citeRegEx": "Reddy,? \\Q1977\\E", "shortCiteRegEx": "Reddy", "year": 1977}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["A. Severyn", "A. Moschitti"], "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information", "citeRegEx": "Severyn and Moschitti,? \\Q2015\\E", "shortCiteRegEx": "Severyn and Moschitti", "year": 2015}, {"title": "Adapting deep ranknet for personalized search", "author": ["Y. Song", "H. Wang", "X. He"], "venue": "In The 7th ACM WSDM Conference,", "citeRegEx": "Song et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Song et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Frank: a ranking method with fidelity loss", "author": ["M. Tsai", "T. Liu", "T. Qin", "H. Chen", "W. Ma"], "venue": "In The 30th Annual International ACM SIGIR Conference,", "citeRegEx": "Tsai et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Tsai et al\\.", "year": 2007}, {"title": "Deep learning for content-based image retrieval: a comprehensive study", "author": ["J. Wan", "D. Wang", "S. Hoi", "P. Wu", "J. Zhu", "Y. Zhang", "J. Li"], "venue": "In Proceedings of the 22nd ACM international conference on Multimedia,", "citeRegEx": "Wan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2014}, {"title": "Personalized ranking model adaptation for web search", "author": ["H. Wang", "X. He", "Chang", "M.-W", "Y. Song", "R. White", "W. Chu"], "venue": "In The 36th Annual ACM SIGIR Conference (SIGIR\u20192013),", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Learning fine-grained image similarity with deep ranking", "author": ["J. Wang", "Y. Song", "T. Leung", "C. Rosenberg", "J. Philbin", "B. Chen", "Y. Wu"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Online multimodal deep similarity learning with application to image retrieval", "author": ["P. Wu", "S. Hoi", "H. Xia", "P. Zhao", "D. Wang", "C. Miao"], "venue": "In Proceedings of the 21st ACM international conference on Multimedia, Barcelona,", "citeRegEx": "Wu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "In learning to rank, there are typically three approaches: the pointwise, pairwise, and listwise approaches (Liu, 2011).", "startOffset": 108, "endOffset": 119}, {"referenceID": 3, "context": "For instance, the RankNet (Burges et al., 2005) applies a neural network to calculate a probability for any search result being more relevant compared to another.", "startOffset": 26, "endOffset": 47}, {"referenceID": 8, "context": "It has already been observed that for training images, applying a committee of convolutional neural nets improves digit and character recognition (Ciresan et al., 2011; Meier et al., 2011).", "startOffset": 146, "endOffset": 188}, {"referenceID": 12, "context": "It has already been observed that for training images, applying a committee of convolutional neural nets improves digit and character recognition (Ciresan et al., 2011; Meier et al., 2011).", "startOffset": 146, "endOffset": 188}, {"referenceID": 2, "context": "For training text data, combining different techniques such as tf-idf, latent Dirichlet allocation (LDA) (Blei et al., 2003), or word2vec (Mikolov et al.", "startOffset": 105, "endOffset": 124}, {"referenceID": 13, "context": ", 2003), or word2vec (Mikolov et al., 2013), has also been explored by Das et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 2, "context": "For training text data, combining different techniques such as tf-idf, latent Dirichlet allocation (LDA) (Blei et al., 2003), or word2vec (Mikolov et al., 2013), has also been explored by Das et al. (2015). This is due to the fact that it is relatively hard to judge different models a priori.", "startOffset": 106, "endOffset": 206}, {"referenceID": 1, "context": "For learning to rank, we propose the application of the attention mechanism (Bahdanau et al., 2015; Cho et al., 2015), which is demonstrated to be successful in focusing on different aspects of the input so that it can incorporate distinct features.", "startOffset": 76, "endOffset": 117}, {"referenceID": 7, "context": "For learning to rank, we propose the application of the attention mechanism (Bahdanau et al., 2015; Cho et al., 2015), which is demonstrated to be successful in focusing on different aspects of the input so that it can incorporate distinct features.", "startOffset": 76, "endOffset": 117}, {"referenceID": 17, "context": "A deeper application of the RankNet can be found in (Song et al., 2014), where a five-layer RankNet is proposed, and each data example is weighed differently for each user in order to adapt to personalized search.", "startOffset": 52, "endOffset": 71}, {"referenceID": 4, "context": "For instance, the LambdaRank (Burges et al., 2006) speeds up the RankNet by altering the cost function according to the change in NDCG caused by swapping search results.", "startOffset": 29, "endOffset": 50}, {"referenceID": 19, "context": "the FRank (Tsai et al., 2007) applies the fidelity loss on the ranking probabilities from the RankNet.", "startOffset": 10, "endOffset": 29}, {"referenceID": 5, "context": "The ListNet (Cao et al., 2007) modifies the RankNet with permutation probabilities, providing a listwise approach.", "startOffset": 12, "endOffset": 30}, {"referenceID": 11, "context": "More models can be found in the summary of (Liu, 2011).", "startOffset": 43, "endOffset": 54}, {"referenceID": 6, "context": "(2014) applied deep convolutional neural nets together with the OASIS algorithm (Chechik et al., 2009) for similarity learning.", "startOffset": 80, "endOffset": 102}, {"referenceID": 19, "context": "For image querying, Wan et al. (2014) applied deep convolutional neural nets together with the OASIS algorithm (Chechik et al.", "startOffset": 20, "endOffset": 38}, {"referenceID": 1, "context": "For instance, Bahdanau et al. (2015) applied it to neural machine translation with a bidirectional recurrent neural network.", "startOffset": 14, "endOffset": 37}, {"referenceID": 1, "context": "For instance, Bahdanau et al. (2015) applied it to neural machine translation with a bidirectional recurrent neural network. Cho et al. (2015) further applied it to image caption and video description generation with convolutional neural nets.", "startOffset": 14, "endOffset": 143}, {"referenceID": 1, "context": "For instance, Bahdanau et al. (2015) applied it to neural machine translation with a bidirectional recurrent neural network. Cho et al. (2015) further applied it to image caption and video description generation with convolutional neural nets. Vinyals et al. (2015) applied it for solving combinatorial problems with the sequence-to-sequence paradigm.", "startOffset": 14, "endOffset": 266}, {"referenceID": 10, "context": "For images, convolutional neural nets (CNNs) (LeCun et al., 1998) are more suitable, in which each node only takes information from neighborhoods of the previous layer.", "startOffset": 45, "endOffset": 65}, {"referenceID": 15, "context": "To rank search results for the testing data, we apply the beam search algorithm (Reddy, 1977), which keeps a number of paths, i.", "startOffset": 80, "endOffset": 93}, {"referenceID": 6, "context": ", r\u0303t\u22121}, we apply the following hinge loss function, similar to Chechik et al. (2009)", "startOffset": 65, "endOffset": 87}, {"referenceID": 18, "context": "For this data set, we pretrain 5 convolutional neural networks based on different L regularization rates for all layers and Dropout (Srivastava et al., 2014) regularization for fully connected (FC) layers with respect to the values shown in Table 1.", "startOffset": 132, "endOffset": 157}, {"referenceID": 6, "context": "We compare our algorithm against OASIS (Chechik et al., 2009), in the setting of image retrieval problems (Wan et al.", "startOffset": 39, "endOffset": 61}, {"referenceID": 20, "context": ", 2009), in the setting of image retrieval problems (Wan et al., 2014; Wang et al., 2014; Wu et al., 2013).", "startOffset": 52, "endOffset": 106}, {"referenceID": 22, "context": ", 2009), in the setting of image retrieval problems (Wan et al., 2014; Wang et al., 2014; Wu et al., 2013).", "startOffset": 52, "endOffset": 106}, {"referenceID": 23, "context": ", 2009), in the setting of image retrieval problems (Wan et al., 2014; Wang et al., 2014; Wu et al., 2013).", "startOffset": 52, "endOffset": 106}, {"referenceID": 11, "context": "To compare the results, we apply MAP and NDCGp (Liu, 2011) as the criteria, and calculate the standard deviations averaged over five runs for each model.", "startOffset": 47, "endOffset": 58}, {"referenceID": 13, "context": "We pretrain the documents with three different word2vectype (Mikolov et al., 2013) models: CBOW, skip-gram, and GLOVE (Pennington et al.", "startOffset": 60, "endOffset": 82}, {"referenceID": 14, "context": ", 2013) models: CBOW, skip-gram, and GLOVE (Pennington et al., 2014).", "startOffset": 43, "endOffset": 68}, {"referenceID": 3, "context": "We compare our model against RankNet, which is commonly applied to information retrieval problems (Burges et al., 2005; Song et al., 2014; Wang et al., 2013).", "startOffset": 98, "endOffset": 157}, {"referenceID": 17, "context": "We compare our model against RankNet, which is commonly applied to information retrieval problems (Burges et al., 2005; Song et al., 2014; Wang et al., 2013).", "startOffset": 98, "endOffset": 157}, {"referenceID": 21, "context": "We compare our model against RankNet, which is commonly applied to information retrieval problems (Burges et al., 2005; Song et al., 2014; Wang et al., 2013).", "startOffset": 98, "endOffset": 157}], "year": 2017, "abstractText": "In information retrieval, learning to rank constructs a machine-based ranking model which given a query, sorts the search results by their degree of relevance or importance to the query. Neural networks have been successfully applied to this problem, and in this paper, we propose an attention-based deep neural network which better incorporates different embeddings of the queries and search results with an attention-based mechanism. This model also applies a decoder mechanism to learn the ranks of the search results in a listwise fashion. The embeddings are trained with convolutional neural networks or the word2vec model. We demonstrate the performance of this model with image retrieval and text querying data sets.", "creator": "LaTeX with hyperref package"}}}