{"id": "1702.06120", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "On the Consistency of $k$-means++ algorithm", "abstract": "We prove in this paper that the expected value of the objective function of the $k$-means++ algorithm for samples converges to population expected value. As $k$-means++, for samples, provides with constant factor approximation for $k$-means objectives, such an approximation can be achieved for the population with increase of the sample size.", "histories": [["v1", "Mon, 20 Feb 2017 10:09:02 GMT  (303kb,D)", "http://arxiv.org/abs/1702.06120v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mieczys{\\l}aw a k{\\l}opotek"], "accepted": false, "id": "1702.06120"}, "pdf": {"name": "1702.06120.pdf", "metadata": {"source": "CRF", "title": "On the Consistency of k-means++ algorithm", "authors": ["Mieczys\u0142aw A. K\u0142opotek"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is not the case that we are in a position to set out to find a solution that is capable, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position to move, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position, that it is in a position to move, that it is in a position to change the world."}], "references": [{"title": "k-means++: the advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proc. of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Approximate k-means++ in sublinear time", "author": ["Olivier Bachem", "Mario Lucic", "S. Hamed Hassani", "Andreas Krause"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Approximate clustering without the approximation", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Sampling within k-means algorithm to cluster large datasets", "author": ["Jeremy Bejarano", "Koushiki Bose", "Tyler Brannan", "Anita Thomas", "Kofi Adragni", "Nagaraj K. Neerchal"], "venue": "Technical Report HPCF-2011-12,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J. MacQueen"], "venue": "In Proc. Fifth Berkeley Symp. on Math. Statist. and Prob.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1967}, {"title": "The planar k-means problem is np-hard", "author": ["Meena Mahajan", "Prajakta Nimbhorkar", "Kasturi Varadarajan"], "venue": "In Proceedings of the 3rd International Workshop on Algorithms and Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Strong consistency of k\u2013means clustering", "author": ["David Pollard"], "venue": "Ann. Statist.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1981}, {"title": "Strong consistency of reduced k-means clustering", "author": ["Yoshikazu Terada"], "venue": "Scand. J. Stat.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Both are described in detail in the paper [3] by Arthur and Vassilvitskii.", "startOffset": 42, "endOffset": 45}, {"referenceID": 3, "context": "[6] on k-means accelerating via subsampling.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "For these reasons Pollard [9] investigated already in 1981 the \u201cin the limit\u201d behaviour of k-means, while MacQueen [7] considered it already in 1967.", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "For these reasons Pollard [9] investigated already in 1981 the \u201cin the limit\u201d behaviour of k-means, while MacQueen [7] considered it already in 1967.", "startOffset": 115, "endOffset": 118}, {"referenceID": 0, "context": "So far, only for k-means++ we have this kind of approximation provided by Arthur and Vassilvitskii [3] .", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "2 Previous work Arthur and Vassilvitskii [3] proved the following property of their k-means++ algorithm.", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "[3] The expected value of the partition cost, computed according to equation (1), is delimited in case of k-means++ algorithm by the inequality E(J) \u2264 8(ln k + 2)Jopt (2) where Jopt denotes the optimal value of the partition cost.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Note that the proof in [3] refers to the initialisation step of the algorithm only so that subsequently we assume that k-means++ clustering consists solely of this initialisation.", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "There exist a number of other constant factor approximation algorithms for k-means objective beside [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 0, "context": "Still most of them rely on the basic approach of [3].", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "Other approaches, like that of [5], assume that all the near optimal partitions lie close to the optimal solution without actually proving it.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "All this research attempts to annihilate the fundamental problem behind the k-means-random, namely, as [3] states: \u201dIt is the speed and simplicity 4", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "Pollard [9] proved an interesting property of a hypothetical k-means-ideal algorithm that would produce a partition yielding Jopt for finite sample.", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": "MacQueen [7] considered a k-means-ideal algorithm under the settings where there exists no unique clustering reaching the minimum of the cost function for the population.", "startOffset": 9, "endOffset": 12}, {"referenceID": 7, "context": "Terada [11] considered a version of k-means called \u201dreduced\u201d k-means which seeks a lower dimensional subspace in which k-means can be applied.", "startOffset": 7, "endOffset": 11}, {"referenceID": 6, "context": "The problem of the k-means-ideal algorithm is that it is NP -hard [9], even in 2D [8]).", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "The problem of the k-means-ideal algorithm is that it is NP -hard [9], even in 2D [8]).", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "[4], hence it is with studying.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "We prove in this paper that the expected value of the objective function of the k-means++ algorithm for samples converges to population expected value. As k-means++, for samples, provides with constant factor approximation for k-means objectives, such an approximation can be achieved for the population with increase of the sample size. This result is of potential practical relevance when one is considering using subsampling when clustering large data sets (large data bases).", "creator": "LaTeX with hyperref package"}}}