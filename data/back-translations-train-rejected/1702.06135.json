{"id": "1702.06135", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Enabling Multi-Source Neural Machine Translation By Concatenating Source Sentences In Multiple Languages", "abstract": "In this paper, we propose a novel and elegant solution to \"Multi-Source Neural Machine Translation\" (MSNMT) which only relies on preprocessing a N-way multilingual corpus without modifying the Neural Machine Translation (NMT) architecture or training procedure. We simply concatenate the source sentences to form a single long multi-source input sentence while keeping the target side sentence as it is and train an NMT system using this augmented corpus. We evaluate our method in a low resource, general domain setting and show its effectiveness (+2 BLEU using 2 source languages and +6 BLEU using 5 source languages) along with some insights on how the NMT system leverages multilingual information in such a scenario by visualizing attention.", "histories": [["v1", "Mon, 20 Feb 2017 19:00:06 GMT  (159kb,D)", "https://arxiv.org/abs/1702.06135v1", "Work in progress. Augmented manuscripts to follow"], ["v2", "Tue, 7 Mar 2017 08:25:29 GMT  (198kb,D)", "http://arxiv.org/abs/1702.06135v2", "Added results for IWSLT corpus setting along with some typo corrections, additional references and statistics"], ["v3", "Mon, 3 Apr 2017 11:37:41 GMT  (232kb,D)", "http://arxiv.org/abs/1702.06135v3", "Added results for IWSLT corpus setting along with some typo corrections, additional references and statistics"]], "COMMENTS": "Work in progress. Augmented manuscripts to follow", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["raj dabre", "fabien cromieres", "sadao kurohashi"], "accepted": false, "id": "1702.06135"}, "pdf": {"name": "1702.06135.pdf", "metadata": {"source": "CRF", "title": "Enabling Multi-Source Neural Machine Translation By Concatenating Source Sentences In Multiple Languages", "authors": ["Raj Dabre", "Fabien Cromieres", "Sadao Kurohashi"], "emails": ["raj@nlp.ist.i.kyoto-u.ac.jp,", "fabien@pa.jst.jp,", "kuro@i.kyoto-u.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "It is not as if it is a way of dealing with word adaptations, translation rules, and complicated decoding algorithms that are a characteristic feature of phrase-based machine translation (PBSMT et al). However, NMT is reported to work better than PBSMT only when there is a plethora of parallel sentences. In a low resource scenario, Vanilla NMT is either inferior to or comparable to PBSMT (Zoph et al). Multilingual NMT languages have shown to be quite effective in a variety of settings such as transfer learning (Zoph et al., 2016), where a model of trained, resource-rich language pairs is used to initialize the parameters for a model designed for a resource-poor pair."}, {"heading": "2 Related Work", "text": "One of the first studies on multi-source MT (Och and Ney, 2001) was conducted to see how word-based SMT systems would benefit from multiple source languages. Although effective, it suffered from a number of constraints that classic word and phrase-based SMT systems have, including the inability to perform end-to-end training. In the context of NMT, work on multi-encoder multi-source NMT (Zoph and Knight, 2016) led for the first time to an end-to-end approach that focused on the use of French and German as source languages to translate into English. However, their method led to models with significantly larger parameter ranges, and they did not examine the effects of using more than two source languages. Multi-source ensembling that requires the use of a multi-way NMT model (Firat et al., 2016b) is a single-source-to-end approach to mash model of a single-end, however, and MT requires a very large end-to-end-mash model of a single-end-to-source MT."}, {"heading": "3 Overview of Our Method", "text": "Refer to Figure 1 for an overview of our method, which reads as follows: \u2022 For each target sentence, link the corresponding source sentences leading to a parallel corpus, with the source sentence being a very long sentence that conveys the same meaning in multiple languages. An example line in such a corpus would be: Source: \"Hello Bonjour Namaskar Kamusta Hello\" and Target: \"konnichiwa.\" The 5 source languages are English, French, Marathi, Filipino and Luuxembourgish, while the target language is Japanese. In this example, each source sentence is a word that conveys \"Hello\" in different languages. We romanize the words Marathi and Japanese for readability. \u2022 Use word segmentation on the source and target sentences, Byte Pair Encoding (BPE) 3 (Sennrich et al., 2016a) in our case, to overcome the data sparseness and eliminate the unknown word segmentation to use a T as a training model."}, {"heading": "4 Experimental Settings", "text": "All of our experiments were carried out using an encoder decoder NMT system, paying attention to the various baselines and multi-source experiments. To provide an infinite vocabulary and reduce data sparseness, we use the Byte Pair Encoding (BPE) -based vocabulary segmentation approach (Sennrich et al., 2016b). However, we do make a slight modification of the original code, where instead of setting the number of merge operations manually, we set a desired vocabulary size and the BPE learning process is automatically terminated after having learned enough rules to maintain the predefined vocabulary size. We prefer this approach because it allows us to learn a minimal model, and it is similar to the way Google's NMT system (Wu et al., 2016) 3The BPE model is only used on the training set.Word model Piece (2012) based on the values of our NMT models (We use our NMT models)."}, {"heading": "4.1 Languages and Corpora Settings", "text": "This year it has come to the point where it will be able to put itself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "4.2 NMT Systems and Model Settings", "text": "This year it's gotten to the point that it will be able to retaliate, \"he said.\" We're able to retaliate, \"he said.\" We're able to retaliate, \"he said."}, {"heading": "4.3 Analysis", "text": "This year, we will be able to look for a solution that is able to find a solution that is able to find a solution, \"he said.\" We must be able to find a solution, \"he said.\" We must strive for a solution that is able to find a solution, \"he said.\" We must strive for a solution that is able to find a solution, \"he said."}, {"heading": "4.4 Studying multi-source attention", "text": "To understand whether or not our multilingual NMT approach favors certain languages over others, we extracted a subset of 50 random sentences from the test set and obtained visualizations for the attention vectors. Note that the words of the target sentence in Hindi are arranged from top to bottom along the lines where the words of the multisource sentence are distributed across the columns from right to left. Note that the source languages get the following order: Bengali, English, Marathi, Tamil, Telugu. The most interesting thing to see is that the attention mechanism is focused on each language, but with different focus. Bengali, Marathi and Telugu are the three languages that get the most attention when English and Tamil receive little attention. This clearly reflects how, when either Bengali or Telugu are combined with Marathi, there seems to be significant gains in the EU BLL system, while we gave predominantly to English by observing the five languages in the MMT system."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed and evaluated a simple approach to \"Multi-Source Neural Machine Translation\" without modifying the NMT system architecture in a resource-poor and resource-rich environment using the ILCI, IWSLT and UN corporations. We compared our approach with two other approaches previously proposed, showing that it is highly effective, domain-independent and language-independent, and that the benefits are significant. Furthermore, by visualizing attention, we observed that NMT focuses on some languages by practically ignoring others, suggesting that language relations are one of the aspects that should be taken into account in a multilingual MT scenario. In the future, we plan to conduct a comprehensive study of the phenomenon of language relations by considering even more languages in resource-rich and resource-poor scenarios."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR 2015). International Conference", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "The iwslt 2015 evaluation campaign", "author": ["M Cettolo", "J Niehues", "S St\u00fcker", "L Bentivogli", "R Cattoni", "M Federico."], "venue": "Proceedings of the Twelfth International Workshop on Spoken Language Translation (IWSLT).", "citeRegEx": "Cettolo et al\\.,? 2015", "shortCiteRegEx": "Cettolo et al\\.", "year": 2015}, {"title": "Wit: Web inventory of transcribed and translated talks", "author": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."], "venue": "Proceedings of the 16 Conference of the European Association for Machine Translation (EAMT). Trento, Italy, pages 261\u2013268.", "citeRegEx": "Cettolo et al\\.,? 2012", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "\u00c7alar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A massively parallel corpus: the bible in 100 languages", "author": ["Christos Christodouloupoulos", "Mark Steedman."], "venue": "Language Resources and Evaluation 49(2):375\u2013395. https://doi.org/10.1007/s10579014-9287-y.", "citeRegEx": "Christodouloupoulos and Steedman.,? 2015", "shortCiteRegEx": "Christodouloupoulos and Steedman.", "year": 2015}, {"title": "Kyoto university participation to wat 2016", "author": ["Fabien Cromieres", "Chenhui Chu", "Toshiaki Nakazawa", "Sadao Kurohashi."], "venue": "Proceedings of the 3rd Workshop on Asian Translation (WAT2016). The COLING 2016 Organiz-", "citeRegEx": "Cromieres et al\\.,? 2016", "shortCiteRegEx": "Cromieres et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the", "citeRegEx": "Firat et al\\.,? 2016a", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Zero-resource translation with multi-lingual neural machine translation", "author": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T. Yarman-Vural", "Kyunghyun Cho."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natu-", "citeRegEx": "Firat et al\\.,? 2016b", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Ensemble learning for multi-source neural machine translation", "author": ["Ekaterina Garmash", "Christof Monz."], "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Orga-", "citeRegEx": "Garmash and Monz.,? 2016", "shortCiteRegEx": "Garmash and Monz.", "year": 2016}, {"title": "The tdil program and the indian langauge corpora intitiative (ilci)", "author": ["Girish Nath Jha."], "venue": "Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias,", "citeRegEx": "Jha.,? 2010", "shortCiteRegEx": "Jha.", "year": 2010}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Conference Proceedings: the tenth Machine Translation Summit. AAMT, AAMT, Phuket, Thailand, pages 79\u201386. http://mt-archive.info/MTS-2005-Koehn.pdf.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Statistical multi-source translation", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Proceedings of MT Summit. volume 8, pages 253\u2013258.", "citeRegEx": "Och and Ney.,? 2001", "shortCiteRegEx": "Och and Ney.", "year": 2001}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Asso-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Japanese and korean voice search", "author": ["Mike Schuster", "Kaisuke Nakajima."], "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2012, Kyoto, Japan, March 25-30, 2012. pages 5149\u20135152.", "citeRegEx": "Schuster and Nakajima.,? 2012", "shortCiteRegEx": "Schuster and Nakajima.", "year": 2012}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12,", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proceedings of the 27th", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "The united nations parallel corpus v1.0", "author": ["Micha Ziemski", "Marcin Junczys-Dowmunt", "Bruno Pouliquen"], "venue": null, "citeRegEx": "Ziemski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ziemski et al\\.", "year": 2016}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}, {"title": "Transfer learning for low-resource neural machine translation", "author": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin,", "citeRegEx": "Zoph et al\\.,? 2016", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Neural machine translation (NMT) (Bahdanau et al., 2015; Cho et al., 2014; Sutskever et al., 2014) enables training an end-to-end system with-", "startOffset": 33, "endOffset": 98}, {"referenceID": 3, "context": "Neural machine translation (NMT) (Bahdanau et al., 2015; Cho et al., 2014; Sutskever et al., 2014) enables training an end-to-end system with-", "startOffset": 33, "endOffset": 98}, {"referenceID": 16, "context": "Neural machine translation (NMT) (Bahdanau et al., 2015; Cho et al., 2014; Sutskever et al., 2014) enables training an end-to-end system with-", "startOffset": 33, "endOffset": 98}, {"referenceID": 19, "context": "In a low resource scenario, vanilla NMT is either worse than or comparable to PBSMT (Zoph et al., 2016).", "startOffset": 84, "endOffset": 103}, {"referenceID": 19, "context": "ing (Zoph et al., 2016) where a model trained on a resource rich language pair is used to initialize", "startOffset": 4, "endOffset": 23}, {"referenceID": 6, "context": "the parameters for a model that is to be trained for a resource poor pair, Multilingual-Multiway NMT (Firat et al., 2016a) where multiple language pairs are learned simultaneously with separate encoders and decoders for each source and target lan-", "startOffset": 101, "endOffset": 122}, {"referenceID": 17, "context": "ist N-way corpora (ordered from largest to smallest according to number of lines of corpora) like United Nations (Ziemski et al., 2016), Europarl (Koehn, 2005), Ted Talks (Cettolo et al.", "startOffset": 113, "endOffset": 135}, {"referenceID": 10, "context": ", 2016), Europarl (Koehn, 2005), Ted Talks (Cettolo et al.", "startOffset": 18, "endOffset": 31}, {"referenceID": 2, "context": ", 2016), Europarl (Koehn, 2005), Ted Talks (Cettolo et al., 2012) ILCI (Jha, 2010) and Bible (Christodouloupoulos", "startOffset": 43, "endOffset": 65}, {"referenceID": 9, "context": ", 2012) ILCI (Jha, 2010) and Bible (Christodouloupoulos", "startOffset": 13, "endOffset": 24}, {"referenceID": 18, "context": "encoder (Zoph and Knight, 2016) and multi-", "startOffset": 8, "endOffset": 31}, {"referenceID": 8, "context": "source ensembling (Garmash and Monz, 2016; Firat et al., 2016b).", "startOffset": 18, "endOffset": 63}, {"referenceID": 7, "context": "source ensembling (Garmash and Monz, 2016; Firat et al., 2016b).", "startOffset": 18, "endOffset": 63}, {"referenceID": 18, "context": "against two existing methods (Zoph and Knight, 2016; Firat et al., 2016b) for MSNMT.", "startOffset": 29, "endOffset": 73}, {"referenceID": 7, "context": "against two existing methods (Zoph and Knight, 2016; Firat et al., 2016b) for MSNMT.", "startOffset": 29, "endOffset": 73}, {"referenceID": 11, "context": "One of the first studies on multi-source MT (Och and Ney, 2001) was done to see how word based SMT systems would benefit from multiple source", "startOffset": 44, "endOffset": 63}, {"referenceID": 18, "context": "NMT (Zoph and Knight, 2016) is the first of its kind end-to-end approach which focused on utilizing French and German as source languages to translate to English.", "startOffset": 4, "endOffset": 27}, {"referenceID": 7, "context": "Multi-source ensembling using a multilingual multi-way NMT model (Firat et al., 2016b) is an end-to-end approach but requires training a very large and complex NMT model.", "startOffset": 65, "endOffset": 86}, {"referenceID": 8, "context": "which uses separately trained single source models (Garmash and Monz, 2016) is comparatively simpler in the sense that one does not need to train additional NMT models but the approach is not truly end-to-end since it needs an ensemble func-", "startOffset": 51, "endOffset": 75}, {"referenceID": 14, "context": "(Sennrich et al., 2016a) in our case, to overcome data sparsity and eliminate the unknown word rate.", "startOffset": 0, "endOffset": 24}, {"referenceID": 15, "context": "reduce data sparsity we use the Byte Pair Encoding (BPE) based word segmentation approach (Sennrich et al., 2016b).", "startOffset": 90, "endOffset": 114}, {"referenceID": 13, "context": "works with the Word Piece Model (WPM) (Schuster and Nakajima, 2012).", "startOffset": 38, "endOffset": 67}, {"referenceID": 12, "context": "using the standard BLEU (Papineni et al., 2002) metric4 on the translations of the test set.", "startOffset": 24, "endOffset": 47}, {"referenceID": 9, "context": "All of our experiments were performed using the publicly available ILCI5 (Jha, 2010), United Nations8 (Ziemski et al.", "startOffset": 73, "endOffset": 84}, {"referenceID": 17, "context": "All of our experiments were performed using the publicly available ILCI5 (Jha, 2010), United Nations8 (Ziemski et al., 2016) and IWSLT9 (Cettolo", "startOffset": 102, "endOffset": 124}, {"referenceID": 5, "context": "For training various NMT systems, we used the open source KyotoNMT system11 (Cromieres et al., 2016).", "startOffset": 76, "endOffset": 100}, {"referenceID": 0, "context": "KyotoNMT implements an Attention based Encoder-Decoder (Bahdanau et al., 2015) with slight modifications to the training procedure.", "startOffset": 55, "endOffset": 78}, {"referenceID": 18, "context": "Since the NMT model architecture used in (Zoph and Knight, 2016) is different from the one in KyotoNMT the multi encoder implementation is not identical (but is equivalent) to the one in", "startOffset": 41, "endOffset": 64}, {"referenceID": 18, "context": "\u2022 N source to one target using the multi encoder multi source approach (Zoph and Knight, 2016).", "startOffset": 71, "endOffset": 94}, {"referenceID": 7, "context": "ensembling approach that late averages (Firat et al., 2016b) N one source to one target models12.", "startOffset": 39, "endOffset": 60}, {"referenceID": 12, "context": "01 \u2022 Choosing the best model: Evaluate the model on the development set and select the one with the best BLEU (Papineni et al., 2002) after reversing the BPE segmentation on the output of the NMT model.", "startOffset": 110, "endOffset": 133}, {"referenceID": 17, "context": "score for Spanish-English was around 9 BLEU points higher than for French-English which is consistent with the observations in the original work concerning the construction of the UN corpus (Ziemski et al., 2016).", "startOffset": 190, "endOffset": 212}], "year": 2017, "abstractText": "In this paper, we propose a novel and elegant solution to \u201cMulti-Source Neural Machine Translation\u201d (MSNMT) which only relies on preprocessing a N-way multilingual corpus without modifying the Neural Machine Translation (NMT) architecture or training procedure. We simply concatenate the source sentences to form a single long multi-source input sentence while keeping the target side sentence as it is and train an NMT system using this preprocessed corpus. We evaluate our method in resource poor as well as resource rich settings and show its effectiveness (up to 4 BLEU using 2 source languages and up to 6 BLEU using 5 source languages) by comparing against existing methods for MSNMT. We also provide some insights on how the NMT system leverages multilingual information in such a scenario by visualizing attention.", "creator": "LaTeX with hyperref package"}}}