{"id": "1702.06186", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2017", "title": "Survey of reasoning using Neural networks", "abstract": "Reason and inference require process as well as memory skills by humans. Neural networks are able to process tasks like image recognition (better than humans) but in memory aspects are still limited (by attention mechanism, size). Recurrent Neural Network (RNN) and it's modified version LSTM are able to solve small memory contexts, but as context becomes larger than a threshold, it is difficult to use them. The Solution is to use large external memory. Still, it poses many challenges like, how to train neural networks for discrete memory representation, how to describe long term dependencies in sequential data etc. Most prominent neural architectures for such tasks are Memory networks: inference components combined with long term memory and Neural Turing Machines: neural networks using external memory resources. Also, additional techniques like attention mechanism, end to end gradient descent on discrete memory representation are needed to support these solutions. Preliminary results of above neural architectures on simple algorithms (sorting, copying) and Question Answering (based on story, dialogs) application are comparable with the state of the art. In this paper, I explain these architectures (in general), the additional techniques used and the results of their application.", "histories": [["v1", "Tue, 14 Feb 2017 17:24:04 GMT  (419kb,D)", "http://arxiv.org/abs/1702.06186v1", "12 pages"], ["v2", "Thu, 2 Mar 2017 09:36:24 GMT  (419kb,D)", "http://arxiv.org/abs/1702.06186v2", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["amit sahu"], "accepted": false, "id": "1702.06186"}, "pdf": {"name": "1702.06186.pdf", "metadata": {"source": "CRF", "title": "Seminar in Collaborative Intelligence: Reasoning using Neural Networks", "authors": ["Amit Sahu"], "emails": ["asahu@rhrk.uni-kl.de"], "sections": [{"heading": null, "text": "Keywords: Neural Networks, Turing Machine, RNN, LSTM, Gradient Descent, Sorting, Discrete Memory, External Memory, Long Term Memory"}, {"heading": "1 Introduction", "text": "Most machine learning models are unable to read and write easily to obtain a (large) component, and conclude that they use a small portion of that large memory to answer questions from a set of facts in a story. In principle, they can be solved by a language modeler, such as a recursive neural network (RNN) ([1]), but their memory is too small and not compartmented enough to remember the required facts. Even in simple memory tasks such as copying a sequence you have just seen, the problems they face are well known."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 RNN", "text": "Recurring neural networks are neural networks with a loop on a hidden node, i.e. the output of the hidden node is returned to the hidden node along with the input on the next timestamp. Thus, the output of the hidden node behaves like a dynamic state, the development of which depends on both the input and the current state (output of the hidden node on the previous timestamp).By unfolding RNN over time, one can see that the context (dynamic state) from an earlier timestamp could affect the behavior of the network at a later timestamp. RNN avoids the problem of \"disappearing and exploding gradients.\" Since the gradient propagates backwards over timestamps, it is multiplied by wlh (t) (weight of the loop connection), depending on its value it disappears (wlh (< 1) or wlt (1) > (1)."}, {"heading": "2.2 LSTM", "text": "In order to solve the problem of the \"disappearing and exploding gradient,\" another architecture called Long Short-Term Memory (LSTM) [2] was developed, which solves the problem by embedding perfect integrators [9] for storage in the network. This is implemented with a complex structure of gates, as shown in Figure 1. For understanding, we will take a simple perfect integrator x (t + 1) = x (t) + i (t), where i (t) is input and x (t) storage. Since the emphasis on x (t) here is identity, the gradient does not disappear or explode. If we now add a mechanism to select when integrator takes over input, i.e. a programmable gate depending on the context: x (t + 1) = x (t) + g (context) i (t), we can now selectively store information for an indefinite period of time. Goals in a similar sense are used in LSTM to enable this."}, {"heading": "3 Approaches", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Neural Turing Machines", "text": "The answer to the question of whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is, whether it is, whether it is a question, whether it is a question, whether it is a question, whether it is, whether it is, whether it is a question, whether it is, whether it is a question, whether it is, whether it is a question, whether it is, whether it is a question, whether it is, whether it is, whether it is a question, whether it is, whether it is a question, whether it is a question, whether it is, whether it is, whether it is a question, whether it is, whether it is, whether it is, whether it is a question, whether it is, whether it is a question, whether it is, whether it is a question, whether it is a question, whether it is a question, whether it is, whether it"}, {"heading": "3.2 Memory Networks", "text": "A memory network [7] consists of one memory m and four components: I: Input Feature Map - converts input into internal features G: Generalization - updates old memory (state) according to the new input O: Output Feature Map - produces output into feature representation Space based on the new input and the current memory status R: Response - converts output into the desired format flow of the model: 1. Convert Input x into internal input Representation I (x) 2. Update Memory m with G 3. Compute Output Features o with O 4. Decode Output Features o to give the final responseA MemNN implementation for text When neural networks are used as components of a memory network (defined above), it is referred to as Memory Neural Network (MemNN). Basic Model Four components of MemNN are defined as follows: I: Sentences x (question or statement of a factor) x are emulated as vectors (G) (no new memory I) are created."}, {"heading": "3.3 End-To-End Memory Networks", "text": "This model [8] takes discrete input representations x1,.., xn to store them in memory, a query q, and gives an answer a. Each of the xi, q, and a contains symbols from a dictionary with vocabulary V. The model converts x (up to a fixed buffer size) and q into a continuous representation. This representation is processed over several hops to output a. Since all of these representations are continuous, we can use input x and query q for training. Single Layer In a single layer case, the model implements a single memory jump operation. Structure and flow of the single layer model is as follows: Input memory representation: By embedding matrices A, B (of size d \u00b7 V) we convert input x and query q into the same continuous space of dimension d. Transformed input is memory vectors {mi} and transformed query is u. In the embedding space we calculate the similarity between the product and an inner similarity with mi."}, {"heading": "4 Experiment and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Neural Turing Machine", "text": "The aim of the experiments was to observe the problem solving and learning of compact internal programs through the NTM architecture. Such solution programs could generalize far beyond the training data. For example, networks trained to copy sequences of length 20 were tested for sequences of length 100. Three architectures are compared in experiments: - NTM with a feed controller - NTM with an LSTM controller - Standard LSTM network tasks were episodic and therefore the dynamic state (previous hidden state) was reset at the beginning of each input sequence (to learn bias vector); all tasks were monitored by learning problems; all networks had logistic sigmoid output coatings and were trained with cross-entropy objective function. Sequence prediction errors are reset in bits-per-sequence.Copy The task was to copy a sequence of binary vectors followed by a delimium."}, {"heading": "4.2 Memory Networks", "text": "It consists of 14M statements, stored as (subject, relation, object) triples. It was a combination of pseudo-marked QA pairs consisting of a question and an associated triple and 35M pair of paraphrased questions.In the experiment, the answers of the top candidates were re-evaluated and the results measured using F1 scores across the test set. Systems developed according to the architecture specified in [13] and [14] were tested and compared on the same basis as in Table 1. The results show the viability of MemNs in the large QA response, but the search is slow for which method enhancements such as word hashing and cluster hashing were used. Simulated world QA simple simulation of 4 characters, 3 objects and 5 spaces - where the characters move, pick up and drop zero objects, based on the approach of [15]. This simulation was generated in text form statements and QA sample is shown in the NK questions that were generated."}, {"heading": "4.3 End-To-End Memory Networks", "text": "Synthetic Question and Answering Experiments Experiemnts were performed on synthetic QA tasks defined in [12]. A QA task consists of a set of instructions, a question and a corresponding answer. Answers are available at the time of training and are predicted at the time of testing. There are 20 different types of tasks that require different forms of reasoning and completion, and only a subset of reserved instructions is relevant to answering the task. This information is not provided to the model both during training and during testing. Three models (baselines) are compared to this approach (abbreviated MemN2N): - MemNN: The tightly monitored AM + NG + NL Memory Network approach proposed in [12]. It uses supporting facts (strong monitoring), n-gram modeling NG, non-linear layers NL, and an adaptive number of hops AM per query."}, {"heading": "5 Conclusion", "text": "NTMs enrich the capabilities of recurrent networks most profoundly through the use of attention mechanisms, memory writing, and large addressable memory. However, the results of NTMs are presented only on the basis of simple copy and sort tasks as described in Section 4.1. Results from MemNN and MemN2N are compared in Table 3, which suggest that MemNN works best for strong monitoring (when facts are known during training), with the lowest margin of error. However, in the case of weak monitoring, MemN2N is better. In all experiments (Tables 2, 3, Fig. 2, 4) it has been consistently observed that these new architectures perform better than RNN, LSTM for tasks that require a large amount of memory searching for conclusions. MemN2N has also been applied in many situations, such as dialogues in a restaurant environment, QS based on a history, targeted dialogues, etc. These research results suggest the importance of neural networks in reasoning tasks."}], "references": [{"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "J. Interspeech. 1045\u20131048", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "Schmidhuber J."], "venue": "J. Neural Computation. 9(8), 1735\u20131780", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint:1410.4615", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "On the computational power of neural nets", "author": ["H.T. Siegelmann", "E.D. Sontag"], "venue": "Journal of computer and system sciences. 50(1), 132\u2013150", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Neural Turing Machine", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint:1410.5401v2", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "The problem of rapid variable creation", "author": ["R.F. Hadley"], "venue": "J. Neural computation. 21(2), 510\u2013532", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Memory Networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "C. International Conference on Learning Representations. arXiv:1410.3916", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "End-To-End Memory Networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "J. Advances in Neural Information Processing Systems. 28, 2440\u20132448", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Continuous attractors and oculomotor control", "author": ["H.S. Seung"], "venue": "J. Neural Networks. 11(7), 1253\u20131258", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Supervised sequence labelling with recurrent neural networks", "author": ["A. Graves"], "venue": "Springer, vol.385", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Neural Networks and physical systems with emergent collective computational abilities", "author": ["J.J. Hopfield"], "venue": "J. Proceedings of the national academy of sciences. 79(8), 2554\u20132558", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1982}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv:1502.05698", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Paraphrase-driven learning for open question answering", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "J. Association for Computational linguistics. 1608\u20131618", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Open question answering with weakly supervised embedding models", "author": ["A. Bordes", "J. Weston", "N. Usunier"], "venue": "C. ECML-PKDD.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards understanding situated natural language", "author": ["A. Bordes", "N. Usunier", "R. Collobert", "J. Weston"], "venue": "C. AISTATS.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "In principle they can be solved by a language modeller such as a recurrent neural network (RNN) ([1]; [2]), but their memory is too small and not compartmentalized enough to remember the required facts accurately.", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "In principle they can be solved by a language modeller such as a recurrent neural network (RNN) ([1]; [2]), but their memory is too small and not compartmentalized enough to remember the required facts accurately.", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "Even in simple memorization task like copying a just seen sequence RNNs are known to have problems [3].", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "RNNs are turing complete [4] and therefore have the capacity to simulate arbitrary procedures but in practice they are not able to.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "In Neural Turing Machine (NTM) [5], these problems are attempted in analogy to Turing\u2019s enrichment of finite-state machine by an infinte memory tape.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "NTM work like a working memory by solving tasks that require the application of approximate rules to \u201crapidly-created variables \u201d[6] and by using an attentional process to read from and write to memory selectively.", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "In Memory Networks [7], the idea is to combine successful machine learning strategies with memory component that can be read and written to.", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "End-to-end memory networks (MemN2N) [8] extends on memory networks by removing problem in backpropagation and requirement of strong supervision at each layer.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "To solve the problem of \u201cvanishing and exploding gradient \u201d, another architecture called Long Short-Term Memory (LSTM) [2] was developed.", "startOffset": 119, "endOffset": 122}, {"referenceID": 8, "context": "It solves the problem by embedding perfect integrators[9] for memory storage in the network.", "startOffset": 54, "endOffset": 57}, {"referenceID": 9, "context": "A LSTM block (adapted from [10])", "startOffset": 27, "endOffset": 31}, {"referenceID": 4, "context": "Architecture of Neural Turing Machine (NTM)[5] contains mainly: a neural network controller and a memory bank.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "For erase operation, the model defines an erase vector et whose M elements lie in [0,1].", "startOffset": 82, "endOffset": 87}, {"referenceID": 10, "context": "Two types of addressing mechanism that complement each other are used: \u2013 Content-based addressing: focusses attention on memory locations related to values emitted by controller [11].", "startOffset": 178, "endOffset": 182}, {"referenceID": 0, "context": "a scalar interpolation gate gt (gt \u2208 [0, 1]) is used to have weighted focus on the content weighting w t and/or the weighting from previous timestep wt\u22121: w t = gtwt + (1\u2212 gt)wt\u22121 (6)", "startOffset": 37, "endOffset": 43}, {"referenceID": 6, "context": "A memory network[7] consists of a memory m and four components: I: input feature map - converts input to internal features G: generalization - updates old memories (state) according to the new input O: output feature map - produces output in feature representation space based on the new input and the current memory state R: response - converts output into desired format", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "This model[8] takes discrete input representations x1, .", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "Copy Learning Curves (adapted from [5])", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "(adapted from [5])", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "Input was a collection of random binary vectors with priority from the range [-1,1].", "startOffset": 77, "endOffset": 83}, {"referenceID": 4, "context": "(adapted from [5])", "startOffset": 14, "endOffset": 17}, {"referenceID": 12, "context": "Results on the large-scale QA task of [13](adapted from [7]).", "startOffset": 38, "endOffset": 42}, {"referenceID": 6, "context": "Results on the large-scale QA task of [13](adapted from [7]).", "startOffset": 56, "endOffset": 59}, {"referenceID": 12, "context": "Method F1 Adapted from [13] 0.", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "54 Adapted from [14] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "Large-scale QA Experiments were performed on QA dataset introduced in [13].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "Systems developed following architecture given in [13] and [14], were tested on the same and compared as shown in Table 1.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "Systems developed following architecture given in [13] and [14], were tested on the same and compared as shown in Table 1.", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "Simulated World QA A simple simulation of 4 characters, 3 objects and 5 rooms - where characters move around, pick up and drop objects, based on the approach of [15] was built.", "startOffset": 161, "endOffset": 165}, {"referenceID": 6, "context": "Test accuracy on the simulation QA task (adapted from [7]).", "startOffset": 54, "endOffset": 57}, {"referenceID": 11, "context": "Synthetic Question and Answering Experiments Experiemnts were performed on synthetic QA tasks defined in [12].", "startOffset": 105, "endOffset": 109}, {"referenceID": 11, "context": "\u2013 MemNN: The strongly supervised AM+NG+NL Memory Network approach, proposed in [12].", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "(adapted from [8])", "startOffset": 14, "endOffset": 17}], "year": 2017, "abstractText": "Reason and inference require process as well as memory skills by humans. Neural networks are able to process tasks like image recognition (better than humans) but in memory aspects are still limited (by attention mechanism, size). Recurrent Neural Network (RNN) and it\u2019s modified version LSTM are able to solve small memory contexts, but as context becomes larger than a threshold, it is difficult to use them. The Solution is to use large external memory. Still, it poses many challenges like, how to train neural networks for discrete memory representation, how to describe long term dependencies in sequential data etc. Most prominent neural architectures for such tasks are Memory networks: inference components combined with long term memory and Neural Turing Machines: neural networks using external memory resources. Also, additional techniques like attention mechanism, end to end gradient descent on discrete memory representation are needed to support these solutions. Preliminary results of above neural architectures on simple algorithms (sorting, copying) and Question Answering (based on story, dialogs) application are comparable with the state of the art. In this paper, I explain these architectures (in general), the additional techniques used and the results of their application.", "creator": "LaTeX with hyperref package"}}}