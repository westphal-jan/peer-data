{"id": "1702.06199", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "The Dialog State Tracking Challenge with Bayesian Approach", "abstract": "Generative model has been one of the most common approaches for solving the Dialog State Tracking Problem with the capabilities to model the dialog hypotheses in an explicit manner. The most important task in such Bayesian networks models is constructing the most reliable user models by learning and reflecting the training data into the probability distribution of user actions conditional on networks states. This paper provides an overall picture of the learning process in a Bayesian framework with an emphasize on the state-of-the-art theoretical analyses of the Expectation Maximization learning algorithm.", "histories": [["v1", "Mon, 20 Feb 2017 22:43:54 GMT  (357kb,D)", "http://arxiv.org/abs/1702.06199v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["quan nguyen"], "accepted": false, "id": "1702.06199"}, "pdf": {"name": "1702.06199.pdf", "metadata": {"source": "CRF", "title": "The Dialog State Tracking Challenge with Bayesian Approach", "authors": ["Quan Nguyen"], "emails": ["5QNGUYEN@INFORMATIK.UNI-HAMBURG.COM"], "sections": [{"heading": null, "text": "c \u00a9 Q. Nguyen.ar Xiv: 170 2.06 199v 1 [cs.A I] 2 0Fe b20 17"}, {"heading": "1. Introduction", "text": "One main reason for this is that for a long time there have been no effective dialogue models that could match the speech signals to some correct hypotheses about what the speakers intend to do. \u2022 Figure 1 illustrates an effective model proposed by Williams et al. (2016). \u2022 Basically, any dialogue model must be able to accomplish the following three tasks: \u2022 Understanding the meaning of the users \"expressions in light of the current state of the dialogue. \u2022 Understanding the changes in the states of dialogue in light of the importance of the users\" expressions. \u2022 Appropriate measures based on the new states of the dialogue. It can be observed that the model in Figure 1 has dedicated modules to meet all these requirements."}, {"heading": "2. The Dialog State Tracking Challenge", "text": "The main idea behind this test frame in contest format is that for the same training and test data, different trackers based on different models compete with each other to find the best model (i.e. the highest performance score) in different scenarios and test systems (i.e. an erroneous distribution between train and test data, changes in user objectives, open versus closed dictionary, etc. (2016). Figure 2 shows the typical output of a tracker's results. A number of different (contradictory and complementary) states are maintained and evaluated in the tracker. These values are effectively constructed from not just a single talking phase, but from several phases. As the dialog progresses, it is an expected behavior that the DM will gradually update the values of the states and the most likely states, making the most likely states increasingly apparent. In this setting, although the output of the ASR model is not very accurate, the system is able to eliminate multiple outputs and selectively select the most dynamic states."}, {"heading": "3. The Bayesian Method", "text": "In generative models, the probability conclusion of observation is formulated as a stochastic sequence generation mechanism consisting of several latent variables. A simple but effective model is the Hidden Markov model, in which the observation and transition states are determined by the Markov property, in which the probability of encountering the current state or observation depends only on the immediately preceding state. Figure 3 illustrates the graphical model of a simple Hidden Markov model (HMM). The sequence of hidden states and observations is called the Markov chain. There are two main objectives of such a model: the first is to find the most likely hidden states that lead to a given sequence of observations, the second is to construct the best transition and observation probability from a randomly initialized setting. The accuracy of the former object depends entirely on the accuracy of the latter object. Therefore, many systems place a strong emphasis on the evaluation of this signal and the estimation of the probabilities generated in the MM context for the observation. \""}, {"heading": "4. Expectation Maximization Algorithm", "text": "The expectation maximization algorithm (EM) is one of the most commonly used optimization algorithms Syed and Williams (2008).The basic idea of EM is to find an appropriate probability distribution of the latent variables and based on this distribution the parameters are repeatedly estimated with better values than the previous ones.The objective function of EM is the protocol probability function of the observed data."}, {"heading": "4.1. Jensen Inequality", "text": "In general, the Jensen inequality states that if a function is convex, the function of expectation is always smaller or equal to the expectation of this function Borman (2004).f (E [x]) \u2264 E [f (x)] (1) The same property can be given for a concave function with inverse inequality (greater or equal instead of less or equal).This strict evaluation (smaller or greater) of two terms: expected value of a function and the value of this function to the expected value of its domain can be given by the convexity of the function Syed and Williams (2008) Figure 4 illustrates the relative comparison between the two quantities in a parabolic convex function. It is observable (and mathematically provable) that the equality of Jensen inequality applies only if the variable is identical to its expected value."}, {"heading": "4.2. Expectation Maximization Algorithm", "text": "Since the distribution of latent variables is unknown in most cases, directly maximizing the objective function by traversing all possible configurations of hidden states is simply unfeasible. A better method would be to find a strict function with lower limits and instead maximize the function with lower limits. The most important property that this method must possess is the convergence of the final state. In other words, it is absolutely necessary that the new parameters are strictly better than the previous one after each optimization step. Figure 5 shows an optimization step in the EM algorithm. Note that when the algorithm moves from the current parameter envin to the new state, it is absolutely necessary that the actual probability function L rises with lower limits Ictn after increasing the function. Figure 5 shows an optimization step in the EM algorithm."}, {"heading": "4.3. Forward-Backward Algorithm", "text": "In the process of finding the optimal parameters in EU policy, the probability of obtaining a subset of events or whole events is very high."}, {"heading": "5. Empirical results", "text": "The empirical performance of the EM algorithm in comparison to the other two transcribed dialog methods can be found in Figure 7 and Figure 8. In general, EM works better than automatic transcribed protocols, but worse than manually transcribed protocols. The learning curve shown in Figure 7 indicates a monotonously increasing relationship between the performance of an algorithm and the number of dialogs in the training set. The justification is obvious: the more data in the training set, the closer to the estimated model to the optimal setting. The exact log probability value of each method can be found in Figure 8. The discrepancy between manual and automatic transcribed protocols can be explained by the faultiness of the ASR module. Since ASR is not optimal, a system without a dialog manager performs worse than a system with optimization steps such as the Bayean method. For the same reason, the manually transcribed method eliminates all possible errors in the field model, and thus achieves the best possible result in the three below."}, {"heading": "6. Conclusion and Discussion", "text": "The convergence of the EM algorithm has been proven in Collins et al (1997). Further evidence can be found in Yihua Chen (2010). However, the gradual optimization in EM is only as good as the gradient parentage, which makes it susceptible to saddle points Collins (1997). It should be noted by gradient parentage, we are referring to the optimization performed with the original probability function by calculating the derivative of the log likelihood function and adding the derivatives to the parameters, similar to the backpropagation learning algorithm in neural networks. The inherent weakness of the generative model is necessary to model the previous distribution of latent variables p (\u03b8). While modelling this prior distribution could be advantageous in the sense that it tells us how the latent variables are spanned in their domain space, we can hardly have enough data and computational resources to estimate this distribution accurately."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Generative model has been one of the most common approaches for solving the Dialog State Tracking Problem with the capabilities to model the dialog hypotheses in an explicit manner. The most important task in such Bayesian networks models is constructing the most reliable user models by learning and reflecting the training data into the probability distribution of user actions conditional on networks\u2019 states. This paper provides an overall picture of the learning process in a Bayesian framework with an emphasize on the state-of-the-art theoretical analyses of the Expectation Maximization learning algorithm.", "creator": "LaTeX with hyperref package"}}}