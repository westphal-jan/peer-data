{"id": "1702.06230", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Beating the World's Best at Super Smash Bros. with Deep Reinforcement Learning", "abstract": "There has been a recent explosion in the capabilities of game-playing artificial intelligence. Many classes of RL tasks, from Atari games to motor control to board games, are now solvable by fairly generic algorithms, based on deep learning, that learn to play from experience with minimal knowledge of the specific domain of interest. In this work, we will investigate the performance of these methods on Super Smash Bros. Melee (SSBM), a popular console fighting game. The SSBM environment has complex dynamics and partial observability, making it challenging for human and machine alike. The multi-player aspect poses an additional challenge, as the vast majority of recent advances in RL have focused on single-agent environments. Nonetheless, we will show that it is possible to train agents that are competitive against and even surpass human professionals, a new result for the multi-player video game setting.", "histories": [["v1", "Tue, 21 Feb 2017 01:06:11 GMT  (152kb,D)", "http://arxiv.org/abs/1702.06230v1", "Submitted to IJCAI 2017"], ["v2", "Thu, 2 Mar 2017 01:54:33 GMT  (152kb,D)", "http://arxiv.org/abs/1702.06230v2", "Submitted to IJCAI 2017"], ["v3", "Mon, 8 May 2017 15:03:25 GMT  (152kb,D)", "http://arxiv.org/abs/1702.06230v3", null]], "COMMENTS": "Submitted to IJCAI 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["vlad firoiu", "william f whitney", "joshua b tenenbaum"], "accepted": false, "id": "1702.06230"}, "pdf": {"name": "1702.06230.pdf", "metadata": {"source": "CRF", "title": "Beating the World\u2019s Best at Super Smash Bros. Melee with Deep Reinforcement Learning", "authors": ["Vlad Firoiu", "William F. Whitney", "Joshua B. Tenenbaum"], "emails": ["vladfi1@mit.edu", "wwhitney@cs.nyu.edu", "jbt@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "In recent years, there has been a kind of renaissance of neural network models in the field of AI and machine learning. Partly driven by hardware advances in the GPUs that accelerate their formation, the first breakthroughs came in 2012, when revolutionary architectures achieved record performance in image classification [Krizhevsky et al., 2012]. Today, the technique is known as deep learning because of its use of many layers, which increasingly build abstract representations from raw data. In this paper, we focus not on seeing, but on gaming. Neural networks were already used in the early 1990s to play expert-level backgammon [Tesauro, 1995]. More recently, there have been breakthroughs in learning to play various video games [Mnih et al., 2013]. Even the ancient board game Go, which long thwarted attempts by AI researchers to build programs at the human level, fell into a 2016 neural combination of neural networks [Mnih et al., Silver Tree et al]."}, {"heading": "2 The SSBM Environment", "text": "We focus on Super Smash Bros. Melee (SSBM), a fast-paced multiplayer combat game released in 2001 for Nintendo Gamecube. SSBM has steadily grown in popularity over its 15-year history and now has an active tournament and professional scene. The metagame is constantly evolving as new mechanics are discovered and refined, and top players push each other to ever higher levels of skill. From an RL perspective, the SSBM environment presents several challenges - large and only partially observable states, complex transition dynamics and delayed rewards. In addition, there is great variety in the environment, with 26 unique characters and a variety of different stages. Partial observability stems from the limitations of human response time along with multiple frames of built-in input delays that force players to anticipate their opponent's actions in advance. In addition, a multiplayer game adds a whole new dimension of complexity - absolute success is not a one that must be measured, but relative."}, {"heading": "2.1 State, Action, Reward", "text": "Many previous applications from deep RL to video games have used raw pixels as observations. Partly for pragmatic reasons, we instead use features that are read from the game's memory on each frame, consisting of each player's position, speed and state of action, along with several other values. This al-ar Xiv: 170 230v 1 [cs.L G] 21 February 2017 allows us to focus exclusively on the RL challenge of playing SSBM rather than on perception. In any case, the game characteristics are easily derived from the pixels; as deep networks we are known to respond fairly well to visual tasks, we have good reason to believe that pixel-based models would work similarly. Pixel-based networks would also be better able to deal with projectiles that we currently do not know how to read from the game's memory. The game runs natively to 60 frames per second, which we reduce to 30 by skipping each other frame."}, {"heading": "3 Methods", "text": "We used two main classes of model-free RL algorithms: Qlearning and political gradients. Although by default, we follow with a brief review of these techniques. From now on, we will use s to denote states, a to denote actions, and r to denote rewards, all three of which can optionally be indexed by a time step. Capital letters stand for random variables."}, {"heading": "3.1 Q-learning", "text": "In Q-Learning, we try to learn a function that maps pairs of state actions to expected future rewards: Q\u03c0 (st, at) = E [Rt + \u03bbRt + 1 + \u03bb2Rt + 2 + \u00b7 \u00b7] (1) We assume that all future actions (on which the Ri is implicitly dependent) will be taken in accordance with policy guidelines. In practice, we estimate the RHS from a single sampled plot line and also shorten the sum to reduce the deviation of the estimate (at the expense of introducing bias). Our objective function will be: L = (Q, at) \u2212 [rt + \u03bbrt + \u00b7 \u00b7 \u00b7 nQ (st + n, at + n)] 2) With Q, we approach the neural network, use (batched) stochastic descent onL to learn the parameters. Note that the second (subtracted) Q in the target is considered a constant in terms of gradient."}, {"heading": "3.2 Policy Gradient Methods", "text": "The REINFORCE [Williams, 1992] learning rule is only the prototype example: \u0394\u03b8 = \u03b1 (R \u2212 b) reward (s, a) (4) Here R is the sampled future reward (possibly shortened as described above), b is a reward, and \u03b1 is the learning rate. Intuitively, this increases the likelihood that we will take measures that are better than the baseline, and vice versa. It can be shown that, in anticipation, we maximize the expected discounted rewards averaged across all states. The Actor Critic algorithm is an extension of the REINFORCE component that replaces the baseline b with a parameterized function of the state known as the Critic. This Critic tries to predict the expected reward from a similar Q function."}, {"heading": "3.3 Training", "text": "Although we are 15 years old, SSBM is not trivial to emulate the problem of the INCE network. Empirically, we have found that while a modern CPU can achieve framerate of about 5x real-time, the people typically found on servers can only achieve 1-2x. This is relatively slow compared to the performance-oriented Atari Learning Environment, which can run Atari games over a hundred times faster than real-time. This means that experience generation (stateaction reward sequences) is a major bottleneck. We remedynamize this by running many different emulators in parallel, typically 50 or more per experiment. 1The many parallel agents periodically send their experiences to a trainer who maintains a circular queue of recent experiences."}, {"heading": "4 Results", "text": "Unless otherwise stated, all agents, human or AI, played as Captain Falcon on stage Battlefield 2. We chose Captain Falcon because he is one of the most popular characters and because he has no projectiles (which is missing from our state representation). Using just one character and one stage greatly simplifies the environment and allows you to compare learning curves and scores directly."}, {"heading": "4.1 In-game AI", "text": "We started testing the RL algorithms against the in-game AI. After setting the parameters accordingly, both Q-learner and Actor-Critic proved to be able to defeat this AI at its highest difficulty level, and achieved similar average reward levels within one day. For each algorithm, we found little variation between experiments with different initializations. However, the two algorithms found qualitatively different strategies from each other. Actor-Critic followed a standard strategy of attack and counter-attack, similar to the way humans play. Q-Learner, on the other hand, would consistently find the intuitive strategy of getting the in-game AI to kill itself. This multi-step tactic is quite impressive; it involves moving to the edge of the stage and allowing the enemy to attempt a 2-attack string from which the first hit (leading to a small negative reward) hits the universe, while the second one misses the big one (leading to the self-killing)."}, {"heading": "4.2 Self-play", "text": "This is due to the quality of their opponent - the in-game trained AI has a very specific (and exploitable for the Q-learner) strategy that does not reflect how experienced players actually play. Without ever playing against an opponent on a human level, it is not surprising that the trained agents themselves are below human level.By switching player structures in state representation, we can have a networking game as Player 1 or 2 that allows him to train against old versions of himself in a similar way to AlphaGo [Silver et al., 2016].After a week of self-training by an actor-critic, our network showed a very strong game, similar to an expert (if repetitive) human. The author, himself a mid-level player, was hard pressed to defeat this AI by conventional tactics. After another week of training, we brought a copy of the network to two major tournaments, where it behaves favorably against all professional players willing to engage with this strategy."}, {"heading": "4.3 Agent Diversity", "text": "The simple solution to playing multiple characters is to use a different network for each character. We did this for the six most popular competitive characters (Fox, Falco, Sheik, Marth, Peach and Captain Falcon) and had the networks train against each other for several days. Results were quite good, with the networks being difficult for the author to play against. Also, these networks did not exhibit the strange behavior of the former Falcon robot. We suspect that this is due to the additional uncertainty in the environment caused by training against different foes. These six networks then became foes against which future networks should be trained, which is a concrete measure of performance measurement. Empirically, none of these future attempts was able to find degenerated counterstrategies to the benchmark networks, so we cautiously explain that weakness was resolved."}, {"heading": "4.4 Character Transfer", "text": "When we train a network to play as a new character, we found it more efficient to initialize from an already trained network than from scratch. We can measure this by the time it takes to achieve an average reward against the scale of the agents. By measuring this, Transfer provides a significant acceleration of training, especially for similar character pairs like Fox and Falco. Overall, these results are not surprising, since many basic tactics (stay on stage, attack in the direction of the enemy, dodge or shield when the enemy attacks) are universal for all characters. Data also show the overall ease of playing each character - Peach, Fox and Falco - all trained quite quickly, while Captain Falcon was significantly slower than the rest. To some extent, this corresponds to the consensus of the SSBM community, which ranks the characters (in decreasing order of strength) as: Fox, Falco, Marth, Sheik C."}, {"heading": "5 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Actor-Critic vs Q-Learning", "text": "It could be argued that learning the Q function per se is more difficult than learning a policy. Technically, this is true in the sense that you can directly get a policy from Q\u03c0 that performs as well as \u03c0 (by playing greedily), but from \u03c0 it is not easy to get Q\u03c0 (that is the whole challenge of Q-Learning). However, we found that Q-learners perform reasonably well against fixed opponents, such as in-game AI and benchmark networks. This leads us to believe that the problem is the non-stationary nature of the game against agents who are also trained. In this scenario, the Q function must keep pace not only with policy repetition, but also with the changes of the opponent."}, {"heading": "5.2 Exploration vs Exploitation", "text": "Our most important method of quantitative measurement of an agent's propensity to explore various actions is the average entropy of his policies. In Q networks, this is directly controlled by the temperature parameter. To actor critics, the entropy factor nudges the policy gradient's direction toward randomness during the training. However, if you look only at the mean entropy across many states, this can be misleading. Typically, the minimum entropy quickly falls below 0.5 while the average remains above 3. In many cases, we found that these seemingly highly entropic agents actually play very repetitively, suggesting that at most levels, the measures taken are largely irrelevant to the agent's performance. In fact, once an attack is initiated in the SSBM, it generally cannot be stopped during its duration, which can take on the order of seconds. A more principled approach to research would attempt to quantify the agent's uncertainty and to examine the unsafe measures over the actions."}, {"heading": "5.3 Action Delay", "text": "The main criticism of our agents is that they play with unrealistic response speed: 2 frames (33ms) compared to over 200ms for humans. To be fair, Captain Falcon is perhaps the least equipped of the popular characters to take advantage of this response speed, with attacks requiring many frames to become active (in the order of 15 frames, or 250ms). Many other characters have attacks that become active half the time, or even immediately (on the next frame) - this was an additional reason for using C. Falcon Initial.The problem of reaction time highlights a big difference between these neural network-based agents and humans. The neural network is effectively cloned to request an action and then destroyed in each frame. While the cloning and destruction does not really take place, this perspective puts the network in stark contrast to the people who have a memory and constantly respond to their sensory experiences and internal thoughts."}, {"heading": "6 Conclusions", "text": "The aim of this work is threefold: we introduce a new environment into the amplification learning community, Super Smash Bros Melee, a competitive multiplayer game that offers a variety of levels and characters; we analyze the difficulties of adapting traditional amplification learning algorithms, typically based on a stationary Markov decision-making process, to multiplayer games where the opponent can learn for himself; and finally, we demonstrate an agent based on deep learning that sets the state of the art in this environment and surpasses the abilities of ten high-level human players."}], "references": [{"title": "and Remi Munos", "author": ["Marc G. Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton"], "venue": "Unifying count-based exploration and intrinsic motivation,", "citeRegEx": "Bellemare et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Wojciech Zaremba", "author": ["Greg Brockman", "Vicki Cheung", "Ludwig Pettersson", "Jonas Schneider", "John Schulman", "Jie Tang"], "venue": "Openai gym,", "citeRegEx": "Brockman et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Advances in neural information processing systems", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks"], "venue": "pages 1097\u20131105,", "citeRegEx": "Krizhevsky et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "Mnih et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Pascanu et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I. Jordan", "Pieter Abbeel"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Schulman et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "PA", "author": ["Jonathan R Shewchuk. An introduction to the conjugate gradient method without the agonizing pain. Technical report", "Pittsburgh"], "venue": "USA,", "citeRegEx": "Shewchuk. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis"], "venue": "search. Nature,", "citeRegEx": "Sutskever et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2016}, {"title": "38(3):58\u201368", "author": ["Gerald Tesauro. Temporal difference learning", "td-gammon. Commun. ACM"], "venue": "March", "citeRegEx": "Tesauro. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "CoRR", "author": ["Hado van Hasselt", "Arthur Guez", "David Silver. Deep reinforcement learning with double q-learning"], "venue": "abs/1509.06461,", "citeRegEx": "van Hasselt et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Machine learning", "author": ["Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning"], "venue": "8(3-4):229\u2013256,", "citeRegEx": "Williams. 1992", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 2, "context": "Driven in part by hardware advances in the GPUs that accelerate their training, the first breakthroughs came in 2012 when convolutional architectures were able to achieve record performance on image classification [Krizhevsky et al., 2012].", "startOffset": 214, "endOffset": 239}, {"referenceID": 8, "context": "As far back as the early 90\u2019s, neural networks were used to reach expert-level play on Backgammon [Tesauro, 1995].", "startOffset": 98, "endOffset": 113}, {"referenceID": 3, "context": "More recently, there have been breakthroughs on learning to play various video games [Mnih et al., 2013].", "startOffset": 85, "endOffset": 104}, {"referenceID": 3, "context": "DeepMind\u2019s original work using deep Qnetworks (abbreviated DQN) on Atari games employed a slightly different algorithm based on the Bellman equation [Mnih et al., 2013]:", "startOffset": 149, "endOffset": 168}, {"referenceID": 9, "context": "There exist techniques such as the doubleDQN [van Hasselt et al., 2015] to alleviate this effect, which warrant further exploration.", "startOffset": 45, "endOffset": 71}, {"referenceID": 10, "context": "The REINFORCE [Williams, 1992] learning rule is the prototypical example:", "startOffset": 14, "endOffset": 30}, {"referenceID": 5, "context": "This approach, known as Trust Region Policy Optimization [Schulman et al., 2015], constrains each gradient step so that the change in policy is bounded.", "startOffset": 57, "endOffset": 80}, {"referenceID": 6, "context": "We thus resort to the Conjugate Gradient method [Shewchuk, 1994], which only requires the ability to take matrix-vector products withH .", "startOffset": 48, "endOffset": 64}, {"referenceID": 1, "context": "OpenAI has released Gym and Universe, which provide a uniform RL interface to a collection of various environments (such as Atari) [Brockman et al., 2016].", "startOffset": 131, "endOffset": 154}, {"referenceID": 0, "context": "Even measuring how much an state/action has been explored can be quite difficult - once this is known, bandit algorithms such as UCB may be applied [Bellemare et al., 2016].", "startOffset": 148, "endOffset": 172}, {"referenceID": 4, "context": "Unfortunately these network are known to be difficult to train [Pascanu et al., 2013], and we were not able to train a competent recurrent agent.", "startOffset": 63, "endOffset": 85}], "year": 2017, "abstractText": "There has been a recent explosion in the capabilities of game-playing artificial intelligence. Many classes of RL tasks, from Atari games to motor control to board games, are now solvable by fairly generic algorithms, based on deep learning, that learn to play from experience with minimal knowledge of the specific domain of interest. In this work, we will investigate the performance of these methods on Super Smash Bros. Melee (SSBM), a popular console fighting game. The SSBM environment has complex dynamics and partial observability, making it challenging for human and machine alike. The multi-player aspect poses an additional challenge, as the vast majority of recent advances in RL have focused on single-agent environments. Nonetheless, we will show that it is possible to train agents that are competitive against and even surpass human professionals, a new result for the multi-player video game setting.", "creator": "LaTeX with hyperref package"}}}