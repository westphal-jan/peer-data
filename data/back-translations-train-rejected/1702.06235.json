{"id": "1702.06235", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Learning to generate one-sentence biographies from Wikidata", "abstract": "We investigate the generation of one-sentence Wikipedia biographies from facts derived from Wikidata slot-value pairs. We train a recurrent neural network sequence-to-sequence model with attention to select facts and generate textual summaries. Our model incorporates a novel secondary objective that helps ensure it generates sentences that contain the input facts. The model achieves a BLEU score of 41, improving significantly upon the vanilla sequence-to-sequence model and scoring roughly twice that of a simple template baseline. Human preference evaluation suggests the model is nearly as good as the Wikipedia reference. Manual analysis explores content selection, suggesting the model can trade the ability to infer knowledge against the risk of hallucinating incorrect information.", "histories": [["v1", "Tue, 21 Feb 2017 01:30:59 GMT  (64kb,D)", "http://arxiv.org/abs/1702.06235v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["andrew chisholm", "will radford", "ben hachey"], "accepted": false, "id": "1702.06235"}, "pdf": {"name": "1702.06235.pdf", "metadata": {"source": "CRF", "title": "Learning to generate one-sentence biographies from Wikidata", "authors": ["Andrew Chisholm", "Will Radford", "Ben Hachey"], "emails": ["andy.chisholm.89@gmail.com", "wradford@hugo.ai", "bhachey@hugo.ai"], "sections": [{"heading": "1 Introduction", "text": "Despite massive efforts, Wikipedia and other collaborative knowledge databases (KBs) have coverage and quality problems. Popular topics are treated in great detail, but there is a long tail of technical topics with little or no text. Other texts can be wrong, whether by accident or vandalism. We report on the task of generating textual summaries for people by mapping slot value facts on onesentence encyclopedic biographies. Besides initializing stub articles with only structured data, the resulting model could be used to improve the consistency and accuracy of existing articles. Figure 1 shows a Wikidata entry for Mathias Tuomi, with fact keys and values smoothed into a sequence, and the first sentence from his Wikipedia article. Some values are in the text, others are missing (e.g. male) or expressed differently (e.g. data)."}, {"heading": "2 Background", "text": "They have focused on problems that are sequences of the same entities, such as translating a sequence of words from one language into another, different work that is able to use these models by forcing different structures into sequences, such as flattening trees for parsing (Vinyals et al., 2016). They have also been successful in knowledge-to-text tasks and lengths via byte input (Gillick et al., 2016) and flattening logical forms for semantic parsing (Xiao et al., 2016). RNN models have also been used successfully in knowledge-to-text systems, e.g. to generalize conversation reactions (Vinyals and Le, 2015), abstract summaries of summaries (Rush et al et al)."}, {"heading": "3 Task and Data", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "3.1 Task complexity", "text": "Wikipedia's first sentences show a relatively narrow domain of language compared to other generations of tasks such as translation. Therefore, it is not clear how complex the generational task is, and we first try to describe it using perplexity. We train both RNN models until the DEV perplexity stops improving. Our basic sequence-to-sequence model (S2S + AE) achieves perplexity from 2.82 on TRAIN and 2.92 on DEV after 15,000 stacks of stochastic gradient drop. The autoencoding sequence sequence model (S2S + AE) takes longer to fit, but achieves a lower minimum perplexity of 2.39 on TRAIN and 2.51 on DEV after 25,000 stacks. To increase the perplexity numbers and understand the complexity of sentence biographies, we train a benchmark language model and rate perplexity values on DEV."}, {"heading": "4 Model", "text": "We model the task as a sequence-to-sequence learning problem. In this setting, a variable length input sequence of facts is encoded by a multi-layered RNN into a distributed representation of fixed length. This input representation is then fed into a separate decoder network, which estimates a distribution via tokens as output. During the training, parameters for both the encoder and the decoder network are optimized to maximize the likelihood of a summary sequence taking into account an observed fact sequence. Our setting differs from the translation task in that the input is a sequence representation of structured data and not the natural human language. As described in Section 3 above, we map Wikidata facts to a sequence of tokens serving as input into the model, as illustrated in Figure 2 above. Experiments below show that for an end-to-sequence learning, this needs to be well summarized in order to facilitate both our summary learning and to generate this content here."}, {"heading": "4.1 Sequence-to-sequence model (S2S)", "text": "To generate language, we seed the decoder network with the output of the encoder and a dedicated GO token, and then greedily generate symbols, taking the most likely output token of the decoder at each step until an EOS token is produced. This approach follows (Sutskever et al., 2014), which demonstrates a larger model with greedy sequence inference, comparable to bar search. In contrast to translation, we expect good performance in the aggregation task, where output sum sequences tend to be well structured and often formulaic. In addition, we expect a partially shared language across input and output. To take advantage of this, we use a bound embedding space that allows both the encoder and decoder networks to exchange information about word meanings between fact values and output tokens. Our model uses a 3-layer stacked Unified Recurrent RNN to match both the encoder number 1K and the number 1K for the first coding of a code."}, {"heading": "4.2 S2S with autoencoding (S2S+AE)", "text": "A challenge for vanilla sequence-to-sequence models in this setting is the lack of a mechanism that forces output sequences to express only the facts present in the data. Given an oracle for fact extraction, we could compare the facts expressed in the output sequence with those of input and adjust the loss for each instance accordingly. While a pure forward model is only required to generate text sequences predicted by the facts, an automatic encoding model is additionally limited in generating text for the input facts. At the place of this ideal setting, we introduce a second sequence-to-sequence model that runs backwards - the text output sequence of the forward model is encoded into facts. This closed-loop model is detailed in Figure 3. The resulting network is trained end-to-end to be able to use both the input-to-output sequence sequence sequence loss (L to L) and the input-to-forward loss (L to L) while the resulting network is also encoded in x-to-forward-to-loss."}, {"heading": "5 Experimental methodology", "text": "The evaluation suite includes standard baselines for comparison, automated metrics for learning, human judgment for evaluation, and detailed analyses for diagnostics. While each is useful in its own right, its combination provides a comprehensive analysis of a complex problem area."}, {"heading": "5.1 Benchmarks", "text": "WIKI We use the first sentence from Wikipedia both as a gold standard reference for evaluating generated sentences and as an upper limit in judging human preferences. BASE template-based systems are strong baseline, especially in human assessment. While the output may be stilted, the corresponding consistency can be an advantage when consistency is important. We induce common patterns from the TRAIN set by replacing complete matches of values by their slot and randomly selecting them on ties. Multiple non-factual symbols are grouped into a single symbol. A small selection of the most common patterns has been manually examined to produce templates, roughly expressed as: TITLE, known as GIVE NAME (born DATE OF BIRTH in PLACE OF BIRTH; died DATE OF DEATH in PLACE OF DEATH) is a POSITION HELOCCUCCUTION consisting of some background data and SHIP."}, {"heading": "5.2 Metrics", "text": "BLEU We also report BLEU n-gram overlaps with respect to the reference Wikipedia summary. With a large dev / test set (10,000 sentences here), BLEU is a reasonable assessment of the generated content. However, there is no indication of shapeliness or legibility. Therefore, we complement BLEU with an evaluation of human preferences. Human preference We use crowdsourced judgments to evaluate the relative quality of the generated sentences and the first sentence of the reference Wikipedia. We obtain paired judgments that show CrowdFlower2 results from two different systems and require everyone to state their binary preferences. System names are anonymized and pseudo-randomly ranked. We request 3 judgments and dynamically increase them until we achieve a match of at least 70% or a maximum of 5 judgments. We use CrowdFlower2 to provide judgments at the cost of $31 for all 6 pair combinations via 822http: / www.crowd.com to collect 0.5% of each assignment for each flower.com task."}, {"heading": "5.3 Analysis of content selection", "text": "After all, no system is perfect, and it can be difficult to understand the inherent difficulty of the problem area and the limitations of a system. Due to the limitations of the evaluation metrics mentioned above, we suggest that manual annotations are important and still necessary for qualitative analysis to guide system improvement. Structured data in knowledge-to-text tasks, if we can identify factual expressions in the text, allow us to identify cases where facts have been omitted, misstated or otherwise expressed."}, {"heading": "6 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Comparison against Wikipedia reference", "text": "Table 3 shows BLEU values that have calculated more than 10,000 units collected from DEV and TEST using the Wikipedia set as a single reference, using uniform weights for 1-4 grams, and padding sets with less than 4 tokens. Values are similar in DEV and TEST, suggesting that the samples are of comparable difficulty. We evaluate significance by bootstrapped resampling with 1,000 samples. Each system result is outside the 95% confidence intervals of other systems. BASE has reasonable values of 21, with S2S being about 32 higher, indicating that the model is at least capable of producing closer text than the baseline. S2S + AE values are even higher at about 41, which is roughly double the baseline values, indicating that the autoencoder model is actually capable of producing better text."}, {"heading": "6.2 Human preference evaluation", "text": "Table 4 shows the results of our human evaluation compared to 82 units sampled from TEST. For each system pair, we show the percentage of units where mass A is preferred over B. Significant differences are commented on with * and * * for p-values < 0.05 and 0.01 with a disposable \u03c72 test. WIKI is uniformly preferred to all systems, as is appropriate for an upper limit. S2S is the least preferred model with respect to WIKI. Finally, while the S2S + AE model is preferred over the BASE and S2S models with a greater distance to the latter, these results show that without automatic coding, the sequence-to-sequence model is less effective than a template-based system. Finally, WIKI is more preferred than S2S + AE, but the distributions do not differ significantly, which we interpret as proof that the model is capable of coding from a human point of view, but requires automated coding."}, {"heading": "7 Analysis", "text": "While the results presented above are encouraging and indicate good performance of the model, they are not diagnostic in the sense that they can provide deeper insights into the strengths and weaknesses of the model. While inspection and manual analysis are still required, we also use the structured factual data inherent in our task to perform both quantitative and qualitative analyses."}, {"heading": "7.1 Fact Count", "text": "Figure 4 shows the impact of inputs on generation performance. As more inputs provide more information for the model to work with, longer inputs are also rarer and more complex to encode. Interestingly, we observe that the S2S + AE model maintains performance for more complex inputs while S2S performance decreases."}, {"heading": "7.2 Example generated text", "text": "Table 5 shows some DEV units and their summaries. The model learns interesting mappings: between numerical and string data and country demonyms. The model also demonstrates the ability to work around marginal cases where templates fail, such as removing parentheses (e.g. (actor)) and printing the name Robert if the input is Bob. The results also suggest that the model can draw conclusions about several facts to improve generating precision, such as describing a unit as English rather than British information about citizenship and place of birth. Unfortunately, the model can also include unsubstantiated facts in the text (e.g. jazz drummers)."}, {"heading": "7.3 Content selection and hallucination", "text": "We know what we have to do to save the world, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "8 Discussion and future work", "text": "Our experiments show that RNNs can generate biographical summaries from structured data, and that a secondary autoencoding target is able to account for some of the information differences between input facts and target output sets. In the future, we will investigate whether the results could be improved by explicitly modelling facts and conditioning generation and autoencoding losses on slots. We expect this to benefit the generation of diverse and noisy slot schemes such as Wikipedia Infoboxes. Another natural extension is to investigate the performance of the network running backwards from the summary text back to the facts. We plan to isolate the performance of the S2S + AE backward model by deriving facts and comparing it to standardized relation extraction systems. Finally, similar RNN models have been applied extensively to language translation tasks. We plan to investigate whether a common model of machine-wide translation and low-density translation can contribute to the use of higher-density, higher-density languages."}, {"heading": "9 Conclusion", "text": "We present a neural model for mapping between structured and unstructured data, focusing on creating Wikipedia biographical summary sentences from slot-value pairs of Wikidata. We introduce automatic sequence-to-sequence RNN encoding, which improves basic models by collectively learning to generate text and reconstruct facts. Our analysis of the task suggests that evaluation is difficult in this area. Instead of a single score, we analyze statistical metrics, human preference assessments, and manual annotations to characterize the task and understand system performance. In evaluating human preferences, our best model performs better than template baselines and becomes the gold standard Wikipedia reference 40% of the time.Code and data are available at https: / / github.com / andychisholm / mimo."}, {"heading": "Acknowledgments", "text": "This work was supported by a Google Faculty Research Award (Chisholm) and an Australian Research Council Discovery Early Career Researchers Award (DE120102900, Hachey). Many thanks to the reviewers for revealing comments and suggestions and to Glen Pink, Kellie Webster, Art Harol and Bo Han for feedback at various stages."}], "references": [{"title": "A simple domain-independent probabilistic approach to generation", "author": ["Gabor Angeli", "Percy Liang", "Dan Klein."], "venue": "Conference on Empirical Methods in Natural Language Processing, pages 502\u2013512.", "citeRegEx": "Angeli et al\\.,? 2010", "shortCiteRegEx": "Angeli et al\\.", "year": 2010}, {"title": "Unsupervised discovery of biographical structure from text", "author": ["David Bamman", "Noah A. Smith."], "venue": "Transactions of the Association for Computational Linguistics, 2:363\u2013376.", "citeRegEx": "Bamman and Smith.,? 2014", "shortCiteRegEx": "Bamman and Smith.", "year": 2014}, {"title": "An unsupervised approach to biography production using Wikipedia", "author": ["Fadi Biadsy", "Julia Hirschberg", "Elena Filatova."], "venue": "Annual Meeting of the Association for Computational Linguistics, pages 807\u2013815.", "citeRegEx": "Biadsy et al\\.,? 2008", "shortCiteRegEx": "Biadsy et al\\.", "year": 2008}, {"title": "Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit", "author": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "O\u2019Reilly Media.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Columbia University at DUC 2004", "author": ["Sasha Blair-Goldensohn", "David Evans", "Vasileios Hatzivassiloglou", "Kathleen McKeown", "Ani Nenkova", "Rebecca Passonneau", "Barry Schiffman", "Andrew Schlaikjer", "Advaith Siddharthan", "Sergey Siegelman."], "venue": "In", "citeRegEx": "Blair.Goldensohn et al\\.,? 2004", "shortCiteRegEx": "Blair.Goldensohn et al\\.", "year": 2004}, {"title": "Summarizing entity descriptions for effective and efficient human-centered entity linking", "author": ["Gong Cheng", "Danyun Xu", "Yuzhong Qu."], "venue": "International Conference on World Wide Web, pages 184\u2013 194.", "citeRegEx": "Cheng et al\\.,? 2015", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Semisupervised sequence learning", "author": ["Andrew M. Dai", "Quoc V. Le."], "venue": "Annual Conference on Neural Information Processing Systems, pages 3079\u20133087.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Statistical acquisition of content selection rules for natural language generation", "author": ["Pablo Ariel Duboue", "Kathleen R McKeown."], "venue": "Conference on Empirical Methods in Natural Language Processing, pages 121\u2013128.", "citeRegEx": "Duboue and McKeown.,? 2003", "shortCiteRegEx": "Duboue and McKeown.", "year": 2003}, {"title": "Generating natural language from linked data: Unsupervised template extraction", "author": ["Daniel Duma", "Ewan Klein."], "venue": "International Conference on Computational Semantics, pages 83\u201394.", "citeRegEx": "Duma and Klein.,? 2013", "shortCiteRegEx": "Duma and Klein.", "year": 2013}, {"title": "Structural, transitive and latent models for biographic fact extraction", "author": ["Nikesh Garera", "David Yarowsky."], "venue": "Conference of the European Chapter of the Association for Computational Linguistics, pages 300\u2013308.", "citeRegEx": "Garera and Yarowsky.,? 2009", "shortCiteRegEx": "Garera and Yarowsky.", "year": 2009}, {"title": "Multilingual language processing from bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics, pages 1296\u20131306.", "citeRegEx": "Gillick et al\\.,? 2016", "shortCiteRegEx": "Gillick et al\\.", "year": 2016}, {"title": "Surface realisation from knowledge-bases", "author": ["Bikash Gyawali", "Claire Gardent."], "venue": "Annual Meeting of the Association for Computational Linguistics, pages 424\u2013434.", "citeRegEx": "Gyawali and Gardent.,? 2014", "shortCiteRegEx": "Gyawali and Gardent.", "year": 2014}, {"title": "Exploiting knowledge base to generate responses for natural language dialog listening agents", "author": ["Sangdo Han", "Jeesoo Bang", "Seonghan Ryu", "Gary Geunbae Lee."], "venue": "Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Han et al\\.,? 2015", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "KenLM: Faster and smaller language model queries", "author": ["Kenneth Heafield."], "venue": "Workshop on Statistical Machine Translation, pages 187\u2013197.", "citeRegEx": "Heafield.,? 2011", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "A statistical NLG framework for aggregated planning and realization", "author": ["Ravi Kondadadi", "Blake Howald", "Frank Schilder."], "venue": "Annual Meeting of the", "citeRegEx": "Kondadadi et al\\.,? 2013", "shortCiteRegEx": "Kondadadi et al\\.", "year": 2013}, {"title": "Conceptto-text generation via discriminative reranking", "author": ["Ioannis Konstas", "Mirella Lapata."], "venue": "Annual Meeting of the Association for Computational Linguistics, pages 369\u2013378.", "citeRegEx": "Konstas and Lapata.,? 2012", "shortCiteRegEx": "Konstas and Lapata.", "year": 2012}, {"title": "Neural text generation from structured data with application to the biography domain", "author": ["R\u00e9mi Lebret", "David Grangier", "Michael Auli."], "venue": "Conference on Empirical Methods in Natural Language Processing, pages 1203\u20131213.", "citeRegEx": "Lebret et al\\.,? 2016", "shortCiteRegEx": "Lebret et al\\.", "year": 2016}, {"title": "Automatic evaluation of summaries using n-gram cooccurrence statistics", "author": ["Chin-Yew Lin", "Eduard Hovy."], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics, pages 71\u201378.", "citeRegEx": "Lin and Hovy.,? 2003", "shortCiteRegEx": "Lin and Hovy.", "year": 2003}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "What to talk about and how? Selective generation using LSTMs with coarse-to-fine alignment", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Mei et al\\.,? 2015", "shortCiteRegEx": "Mei et al\\.", "year": 2015}, {"title": "Evaluating content selection in summarization: The pyramid method", "author": ["Ani Nenkova", "Rebecca Passonneau."], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics, pages 145\u2013152.", "citeRegEx": "Nenkova and Passonneau.,? 2004", "shortCiteRegEx": "Nenkova and Passonneau.", "year": 2004}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Annual Meeting on Association for Computational Linguistics, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Expressing OWL axioms by english sentences: Dubious in theory, feasible in practice", "author": ["Richard Power", "Allan Third."], "venue": "International Conference on Computational Linguistics, pages 1006\u2013 1013.", "citeRegEx": "Power and Third.,? 2010", "shortCiteRegEx": "Power and Third.", "year": 2010}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."], "venue": "Conference on Empirical Methods in Natural Language Processing, pages 379\u2013389.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Producing biographical summaries: Combining linguistic knowledge with corpus statistics", "author": ["Barry Schiffman", "Inderjeet Mani", "Kristian Concepcion."], "venue": "Annual Meeting of the Association for Computational Linguistics, pages 458\u2013465.", "citeRegEx": "Schiffman et al\\.,? 2001", "shortCiteRegEx": "Schiffman et al\\.", "year": 2001}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Annual Conference on Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc V. Le."], "venue": "ICML Deep Learning Workshop.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Annual Conference on Neural Information Processing Systems, pages 2755\u20132763.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrk\u0161i\u0107", "PeiHao Su", "David Vandyke", "Steve Young."], "venue": "Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Sequence-based structured prediction for semantic parsing", "author": ["Chunyang Xiao", "Marc Dymetman", "Claire Gardent."], "venue": "Annual Meeting of the Association for Computational Linguistics, pages 1341\u2013 1350.", "citeRegEx": "Xiao et al\\.,? 2016", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "Summarizing highly structured documents for effective search interaction", "author": ["Lanbo Zhang", "Yi Zhang", "Yunfei Chen."], "venue": "International Conference on Research and Development in Information Retrieval, pages 145\u2013154.", "citeRegEx": "Zhang et al\\.,? 2012", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 25, "context": "We treat this knowlege-to-text task like translation, using a recurrent neural network (RNN) sequence-to-sequence model (Sutskever et al., 2014) that learns to select and realise the most", "startOffset": 120, "endOffset": 144}, {"referenceID": 25, "context": "RNN sequence-to-sequence models (Sutskever et al., 2014) have driven various recent advances in natural language understanding.", "startOffset": 32, "endOffset": 56}, {"referenceID": 27, "context": ", flattening trees for parsing (Vinyals et al., 2015), predicting span types and lengths over byte input (Gillick et al.", "startOffset": 31, "endOffset": 53}, {"referenceID": 10, "context": ", 2015), predicting span types and lengths over byte input (Gillick et al., 2016) or flattening logical forms for semantic parsing (Xiao et al.", "startOffset": 59, "endOffset": 81}, {"referenceID": 29, "context": ", 2016) or flattening logical forms for semantic parsing (Xiao et al., 2016).", "startOffset": 57, "endOffset": 76}, {"referenceID": 26, "context": ", generating conversational responses (Vinyals and Le, 2015), abstractive summarisation (Rush et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 23, "context": ", generating conversational responses (Vinyals and Le, 2015), abstractive summarisation (Rush et al., 2015).", "startOffset": 88, "endOffset": 107}, {"referenceID": 28, "context": "that completely expresses a set of facts: restaurant recommendation text from dialogue acts (Wen et al., 2015), weather reports from sensor data and sports commentary from on-field events (Mei et al.", "startOffset": 92, "endOffset": 110}, {"referenceID": 19, "context": ", 2015), weather reports from sensor data and sports commentary from on-field events (Mei et al., 2015).", "startOffset": 85, "endOffset": 103}, {"referenceID": 6, "context": "Recent work explores unsupervised autoencoding objectives in sequence-to-sequence models, improving both text classification as a pretraining step (Dai and Le, 2015) and translation as a multitask objective (Luong et al.", "startOffset": 147, "endOffset": 165}, {"referenceID": 18, "context": "Recent work explores unsupervised autoencoding objectives in sequence-to-sequence models, improving both text classification as a pretraining step (Dai and Le, 2015) and translation as a multitask objective (Luong et al., 2016).", "startOffset": 207, "endOffset": 227}, {"referenceID": 24, "context": "Biographic summarisation has been extensively researched and is often approached as a sequence of subtasks (Schiffman et al., 2001).", "startOffset": 107, "endOffset": 131}, {"referenceID": 4, "context": "A version of the task was featured in the Document Understanding Conference in 2004 (Blair-Goldensohn et al., 2004) and other work learns policies for content selection without generating text (Duboue and McKeown, 2003; Zhang et al.", "startOffset": 84, "endOffset": 115}, {"referenceID": 7, "context": ", 2004) and other work learns policies for content selection without generating text (Duboue and McKeown, 2003; Zhang et al., 2012; Cheng et al., 2015).", "startOffset": 85, "endOffset": 151}, {"referenceID": 30, "context": ", 2004) and other work learns policies for content selection without generating text (Duboue and McKeown, 2003; Zhang et al., 2012; Cheng et al., 2015).", "startOffset": 85, "endOffset": 151}, {"referenceID": 5, "context": ", 2004) and other work learns policies for content selection without generating text (Duboue and McKeown, 2003; Zhang et al., 2012; Cheng et al., 2015).", "startOffset": 85, "endOffset": 151}, {"referenceID": 12, "context": "Generating textual templates that are filled by structured data is a common approach and has been used for conversational text (Han et al., 2015) and biographical text generation (Duma and Klein, 2013).", "startOffset": 127, "endOffset": 145}, {"referenceID": 8, "context": ", 2015) and biographical text generation (Duma and Klein, 2013).", "startOffset": 41, "endOffset": 63}, {"referenceID": 2, "context": "Wikipedia has also been a popular resource for studying biography, including sentence harvesting and ordering (Biadsy et al., 2008), unsupervised discovery of distinct sequences of life events (Bamman and Smith, 2014) and fact extraction from text (Garera and Yarowsky, 2009).", "startOffset": 110, "endOffset": 131}, {"referenceID": 1, "context": ", 2008), unsupervised discovery of distinct sequences of life events (Bamman and Smith, 2014) and fact extraction from text (Garera and Yarowsky, 2009).", "startOffset": 69, "endOffset": 93}, {"referenceID": 9, "context": ", 2008), unsupervised discovery of distinct sequences of life events (Bamman and Smith, 2014) and fact extraction from text (Garera and Yarowsky, 2009).", "startOffset": 124, "endOffset": 151}, {"referenceID": 14, "context": "There has also been substantial work in generating from other structured KBs using template induction (Kondadadi et al., 2013), semantic web techniques (Power and Third, 2010), tree adjoining grammars (Gyawali and Gardent, 2014), probabilistic context free grammars (Konstas and Lapata, 2012) and probabilistic models that jointly select and realise content (Angeli et al.", "startOffset": 102, "endOffset": 126}, {"referenceID": 22, "context": ", 2013), semantic web techniques (Power and Third, 2010), tree adjoining grammars (Gyawali and Gardent, 2014), probabilistic context free grammars (Konstas and Lapata, 2012) and probabilistic models that jointly select and realise content (Angeli et al.", "startOffset": 33, "endOffset": 56}, {"referenceID": 11, "context": ", 2013), semantic web techniques (Power and Third, 2010), tree adjoining grammars (Gyawali and Gardent, 2014), probabilistic context free grammars (Konstas and Lapata, 2012) and probabilistic models that jointly select and realise content (Angeli et al.", "startOffset": 82, "endOffset": 109}, {"referenceID": 15, "context": ", 2013), semantic web techniques (Power and Third, 2010), tree adjoining grammars (Gyawali and Gardent, 2014), probabilistic context free grammars (Konstas and Lapata, 2012) and probabilistic models that jointly select and realise content (Angeli et al.", "startOffset": 147, "endOffset": 173}, {"referenceID": 0, "context": ", 2013), semantic web techniques (Power and Third, 2010), tree adjoining grammars (Gyawali and Gardent, 2014), probabilistic context free grammars (Konstas and Lapata, 2012) and probabilistic models that jointly select and realise content (Angeli et al., 2010).", "startOffset": 239, "endOffset": 260}, {"referenceID": 21, "context": "(2016) report BLEU scores (Papineni et al., 2002) which calculate the n-gram overlap between text produced by the system with respect to a human-written reference.", "startOffset": 26, "endOffset": 49}, {"referenceID": 17, "context": "Summarisation evaluations have concentrated on the content that is included in the summary, with semantic content typically extracted manually for comparison (Lin and Hovy, 2003; Nenkova and Passonneau, 2004).", "startOffset": 158, "endOffset": 208}, {"referenceID": 20, "context": "Summarisation evaluations have concentrated on the content that is included in the summary, with semantic content typically extracted manually for comparison (Lin and Hovy, 2003; Nenkova and Passonneau, 2004).", "startOffset": 158, "endOffset": 208}, {"referenceID": 15, "context": "Lebret et al. (2016) report BLEU scores (Papineni et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "Our final system comparison follows Kondadadi et al. (2013) in running a crowd task to collect pairwise preferences for evaluating and", "startOffset": 36, "endOffset": 60}, {"referenceID": 3, "context": "We then use sitelinks to identify each entity\u2019s Wikipedia article text and NLTK (Bird et al., 2009) to tokenize and extract the lower-cased first sentence.", "startOffset": 80, "endOffset": 99}, {"referenceID": 13, "context": "(2016), we build Kneser-Ney smoothed 5-gram language models using the KenLM toolkit (Heafield, 2011).", "startOffset": 84, "endOffset": 100}, {"referenceID": 15, "context": "Following Lebret et al. (2016), we build Kneser-Ney smoothed 5-gram language models using the KenLM toolkit (Heafield, 2011).", "startOffset": 10, "endOffset": 31}, {"referenceID": 16, "context": "TITLE indicates templating of entity names only, while FULL indicates templating of all fact values by token index as described in Lebret et al. (2016). This shows that templating is an effective way to reduce the sparsity of a task, and that titles account for a large component of this.", "startOffset": 131, "endOffset": 152}, {"referenceID": 16, "context": "Although Lebret et al. (2016) evaluate on a different dataset, we are able to draw some comparisons given the similarity of our task.", "startOffset": 9, "endOffset": 30}, {"referenceID": 27, "context": "The decoder network includes an attention mechanism (Vinyals et al., 2015) to help facilitate accurate content selection.", "startOffset": 52, "endOffset": 74}, {"referenceID": 25, "context": "This approach follows (Sutskever et al., 2014) who demonstrate a", "startOffset": 22, "endOffset": 46}], "year": 2017, "abstractText": "We investigate the generation of onesentence Wikipedia biographies from facts derived from Wikidata slot-value pairs. We train a recurrent neural network sequence-to-sequence model with attention to select facts and generate textual summaries. Our model incorporates a novel secondary objective that helps ensure it generates sentences that contain the input facts. The model achieves a BLEU score of 41, improving significantly upon the vanilla sequence-to-sequence model and scoring roughly twice that of a simple template baseline. Human preference evaluation suggests the model is nearly as good as the Wikipedia reference. Manual analysis explores content selection, suggesting the model can trade the ability to infer knowledge against the risk of hallucinating incorrect information.", "creator": "LaTeX with hyperref package"}}}