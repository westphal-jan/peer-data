{"id": "1702.06239", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Reinforcement Learning Based Argument Component Detection", "abstract": "Argument component detection (ACD) is an important sub-task in argumentation mining. ACD aims at detecting and classifying different argument components in natural language texts. Historical annotations (HAs) are important features the human annotators consider when they manually perform the ACD task. However, HAs are largely ignored by existing automatic ACD techniques. Reinforcement learning (RL) has proven to be an effective method for using HAs in some natural language processing tasks. In this work, we propose a RL-based ACD technique, and evaluate its performance on two well-annotated corpora. Results suggest that, in terms of classification accuracy, HAs-augmented RL outperforms plain RL by at most 17.85%, and outperforms the state-of-the-art supervised learning algorithm by at most 11.94%.", "histories": [["v1", "Tue, 21 Feb 2017 02:18:38 GMT  (329kb,D)", "http://arxiv.org/abs/1702.06239v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yang gao", "hao wang", "chen zhang", "wei wang"], "accepted": false, "id": "1702.06239"}, "pdf": {"name": "1702.06239.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning Based Argument Component Detection", "authors": ["Yang Gao", "Hao Wang", "Chen Zhang", "Wei Wang"], "emails": ["wangwei2014}@iscas.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "2 Related Work", "text": "Most existing automatic ACD methods report that the task of classification has only been partially accomplished (although their emphasis is mostly on designing useful features for displaying clauses and selecting suitable SL-based classifiers). Widespread features include structural, lexical, syntactic, and contextual features, but popular classifiers also include SVM, naive bayes, decision-making structures, and random forests. For well-structured documents that use these SL classifiers and conventional features, this results in relatively good performance: for example, SVM achieves Macro-F1 in companies consisting of compelling essays [Stab and Gurevych, 2014a)."}, {"heading": "3 Formulating ACD as a Sequential Decision Making Problem", "text": "In this work, we formally consider ACD as episodic MDPs. An episodic MDP is a tuple (S, A, P, R, T). S is the set of states; a state is a representation of the current status of the problem at hand. A is the set of actions that performs an action that is transferred to a new state and receives a numerical reward. R (s, a), where R: S \u00d7 A \u2192 R is the reward function. P (s, a) is the transitional function: There is the probability of moving from a state to a new state by performing an action. T S is the set of terminal states: When the agent is transferred to a state, the current episode ends. The components of our MDP-based ACD formulation are as follows."}, {"heading": "4 RL-based ACD Framework", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "5 Datasets", "text": "This year, we will be able to look for a solution that is capable of finding a solution, that is able to find a solution that enables us to find a solution, that is able to find a solution that enables us to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution that enables us to find a solution that is able to find a solution, that is able to find a solution that is able to find a solution. \""}, {"heading": "6 Experimental Settings and Results", "text": "In this section, we designate an HAs combination with type L window size i and type C window size j as a pair (i, j). In each HAs combination setting, we used a 10-fold cross validation, ensuring that clauses from the same document are not distributed over the train and test sets; in addition, we repeated the cross validation 10 times, giving a total of 100 folds. All the results presented are averages over the 100 folds. We left the significance level at 0.05. With respect to conventional linguistic features (see section 3), we used exactly the same features as in [Stab and Gurevych, 2014b]. For model selection and hyperparameter tuning, we randomly stitched 25% documents (from both corpora) and performed a 5-fold cross validation."}, {"heading": "6.1 Baselines", "text": "We select SVM and SVM-HMM as our baselines because these two algorithms are among the most widely used and powerful algorithms to build ACD tools (see Section 2). In terms of algorithm implementations, we used LIBSVM [Chang and Lin, 2011] for SVM and a revision of SVMstruct [Joachims et al., 2009] for SVM-HMM. Although SVM-HMM considers the performance of SVM and SVM as type C HAs, it ignores type L HAs and does not explicitly consider HAs. For these reasons, and also to ensure the fairness of the comparison between the SL and RL-based ACD tools, we test the performance of SVM and SVM-HMM using the HAs-enhanced properties. We try all HAs combinations from (0.0) to (9.5) in order to ensure the fairness of the comparison between the SL and RL-based ACD tools, we test the performance of SVM and SVM-HMM using the HAs-enhanced properties. We try to find all SVM combinations from (SVM) to (SVM) in both baseline algorithms that SVM and HMM do not result from naive HM decisions, and HM HMM-HMM attributes, and HMM-HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HMM HAS HMM HMM HMM HMM HMM HMM HMM H"}, {"heading": "6.2 Results", "text": "First, we study the effects of HAs on RL-based ACD. The performance of RL-based ACD using different HAs combinations is shown in Fig. 3. We can see that in both companies the worst performance is achieved when no HAs are used, and the performance increases almost linearly with the growth of type L and type C window sizes. To assess the importance of improvement, we run tests between performances (0, 0) (no HAs are used), (type L is used at maximum and no type C is used), (type C is used at maximum and no type L is used) and (both types of HAs are fully utilized); results indicate that the performance on (0, 0, 0) is significantly worse than the performance on (0, 0, 0)."}, {"heading": "6.3 Discussion and Error Analysis", "text": "To get more insight into how the use of HAs improves the performance of RL, we look at the confusion matrices of each algorithm and manually examine some misclassified cases. In both companies, we find the biggest source of error is the misclassification of premises and claims: for example, in the essay Corpus, 607 of 1506 claims are misclassified as premises of SVM-HMM; in Hotel Corpus, 88 of 180 premises are classified as such."}, {"heading": "7 Conclusion", "text": "In this paper, we propose an RL-based ACD technique and examine the influence of HA in it. Empirical results on two corporations suggest that the use of HA can significantly improve the performance of RL-based ACD and that the performance of HA augmented RL is significantly better than that of modern SL-based ACD techniques. To our knowledge, this is the first work to systematically examine the influence of HA and the applicability of RL in the ACD task. Future work will include investigating the influence of some other contextual information, such as linguistic features of surrounding clauses, in SL and RL-based ACD methods, as well as examining the applicability of RL to some other subtasks in argumentation mining, such as predicting reasoning relationships."}], "references": [{"title": "et al", "author": ["Y. Altun", "I. Tsochantaridis", "T. Hofmann"], "venue": "Hidden Markov support vector machines. In Proc. of ICML", "citeRegEx": "Altun et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "JMLR, 3", "citeRegEx": "Blei et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2(3)", "citeRegEx": "Chang and Lin. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast prediction with SVM models containing RBF kernels", "author": ["M. Claesen", "F. De Smet", "J. Suykens", "B. De Moor"], "venue": "arXiv preprint arXiv:1403.0736", "citeRegEx": "Claesen et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "On the role of discourse markers for discriminating claims and premises in argumentative discourse", "author": ["J. Eckle-Kohler", "R. Kluge", "I. Gurevych"], "venue": "Proc. of EMNLP", "citeRegEx": "Eckle.Kohler et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Measuring nominal scale agreement among many raters", "author": ["J.L. Fleiss"], "venue": "Psychological Bulletin, 76(5):378\u2013382", "citeRegEx": "Fleiss. 1971", "shortCiteRegEx": null, "year": 1971}, {"title": "Incremental least-squares temporal difference learning", "author": ["A. Geramifard", "M. Bowling", "R.S. Sutton"], "venue": "Proc. of AAAI", "citeRegEx": "Geramifard et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Exploiting debate portals for semisupervised argumentation mining in user-generated web discourse", "author": ["I. Habernal", "I. Gurevych"], "venue": "Proc. of EMNLP", "citeRegEx": "Habernal and Gurevych. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Argumentation mining on the web from information seeking perspective", "author": ["I. Habernal", "J. Eckle-Kohler", "I. Gurevych"], "venue": "Proc. of the Workshop on Frontiers and Connections between Argumentation Theory and Natural Language Processing", "citeRegEx": "Habernal et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The WEKA data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD explorations newsletter, 11(1):10\u201318", "citeRegEx": "Hall et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Cutting-plane training of structural SVMs", "author": ["T. Joachims", "T. Finley", "C.-N. Yu"], "venue": "Machine Learning, 77(1)", "citeRegEx": "Joachims et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "JMLR", "author": ["Michail G Lagoudakis", "Ronald Parr. Least-squares policy iteration"], "venue": "4:1107\u20131149,", "citeRegEx": "Lagoudakis and Parr. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "The measurement of observer agreement for categorical data", "author": ["J.R. Landis", "G.G. Koch"], "venue": "Biometrics, 33(1):159\u2013174", "citeRegEx": "Landis and Koch. 1977", "shortCiteRegEx": null, "year": 1977}, {"title": "Mining arguments from 19th century philosophical texts using topic based modelling", "author": ["J. Lawrence", "C. Ree", "A. Colin", "S. MacAlister", "A. Ravenscroft", "D. Bourget"], "venue": "Proc. of Workshop on Argumentation Mining", "citeRegEx": "Lawrence et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Context dependent claim detection", "author": ["R. Levy", "Y. Bilu", "D. Hershcovich", "E. Aharoni", "N. Slonim"], "venue": "Proc. of COLING", "citeRegEx": "Levy et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Argumentation mining: State of the art and emerging trends", "author": ["M. Lippi", "P. Torroni"], "venue": "ACM Transactions on Internet Technology", "citeRegEx": "Lippi and Torroni. 2015a", "shortCiteRegEx": null, "year": 2015}, {"title": "Context-independent claim detection for argument mining", "author": ["M. Lippi", "P. Torroni"], "venue": "Proc. of IJCAI", "citeRegEx": "Lippi and Torroni. 2015b", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533", "citeRegEx": "Mnih et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Extracting argument and domain words for identifying argument components in texts", "author": ["H.V. Nguyen", "D.J. Litman"], "venue": "Proc. of Workshop on Argumentation Mining", "citeRegEx": "Nguyen and Litman. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Kernelbased reinforcement learning", "author": ["D. Ormoneit", "\u015a. Sen"], "venue": "MACH LEARN, 49(23):161\u2013178", "citeRegEx": "Ormoneit and Sen. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Argumentation mining: the detection", "author": ["R.M. Palau", "M.-F. Moens"], "venue": "classification and structure of arguments in text. In Proc. of ICAIL", "citeRegEx": "Palau and Moens. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Sample efficient on-line learning of optimal dialogue policies with Kalman temporal differences", "author": ["O. Pietquin", "M. Geist", "S. Chandramohan"], "venue": "Proc. of IJCAI", "citeRegEx": "Pietquin et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Fear the REAPER: A system for automatic multidocument summarization with reinforcement learning", "author": ["C. Rioux", "S.A. Hasan", "Y. Chali"], "venue": "Proc. of EMNLP", "citeRegEx": "Rioux et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Framework of automatic text summarization using reinforcement learning", "author": ["S. Ryang", "T. Abekawa"], "venue": "Proc. of EMNLP", "citeRegEx": "Ryang and Abekawa. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Annotating argument components and relations in persuasive essays", "author": ["C. Stab", "I. Gurevych"], "venue": "Proc. of COLING", "citeRegEx": "Stab and Gurevych. 2014a", "shortCiteRegEx": null, "year": 2014}, {"title": "Identifying argumentative discourse structures in persuasive essays", "author": ["C. Stab", "I. Gurevych"], "venue": "Proc. of EMNLP", "citeRegEx": "Stab and Gurevych. 2014b", "shortCiteRegEx": null, "year": 2014}, {"title": "Parsing argumentation structures in persuasive essays", "author": ["C. Stab", "I. Gurevych"], "venue": "arXiv preprint, arXiv:1604.07370", "citeRegEx": "Stab and Gurevych. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesv\u00e1ri", "E. Wiewiora"], "venue": "Proc. of ICML. ACM", "citeRegEx": "Sutton et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Kernelized value function approximation for reinforcement learning", "author": ["G. Taylor", "R. Parr"], "venue": "Proc. of ICML, pages 1017\u20131024. ACM", "citeRegEx": "Taylor and Parr. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "A review corpus for argumentation analysis", "author": ["H. Wachsmuth", "M. Trenkmann", "B. Stein", "G. Engels", "T. Palakarska"], "venue": "Computational Linguistics and Intelligent Text Processing, pages 115\u2013127. Springer", "citeRegEx": "Wachsmuth et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Sentiment flow\u2013a general model of web review argumentation", "author": ["Henning Wachsmuth", "Johannes Kiesel", "Benno Stein"], "venue": "Proc. of EMNLP,", "citeRegEx": "Wachsmuth et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Partially observable Markov decision processes for spoken dialog systems", "author": ["J.D. Williams", "S. Young"], "venue": "Computer Speech & Language, 21(2)", "citeRegEx": "Williams and Young. 2007", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 15, "context": "[Lippi and Torroni, 2015a]).", "startOffset": 0, "endOffset": 26}, {"referenceID": 4, "context": "An argument is a basic unit people use to persuade their audiences to accept a particular state of affairs [Eckle-Kohler et al., 2015], and it usually consists of a claim and some premises offered in support of the claim.", "startOffset": 107, "endOffset": 134}, {"referenceID": 24, "context": "To obtain the contextual information, human annotators usually need to read and label a document for multiple rounds [Stab and Gurevych, 2014a].", "startOffset": 117, "endOffset": 143}, {"referenceID": 22, "context": "text summarisation [Rioux et al., 2014] and dialogue generation [Pietquin et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 21, "context": ", 2014] and dialogue generation [Pietquin et al., 2011].", "startOffset": 32, "endOffset": 55}, {"referenceID": 24, "context": "741 macro-F1 in corpora consisting of persuasive essays [Stab and Gurevych, 2014a; Stab and Gurevych, 2014b] and legal documents [Palau and Moens, 2009], resp.", "startOffset": 56, "endOffset": 108}, {"referenceID": 25, "context": "741 macro-F1 in corpora consisting of persuasive essays [Stab and Gurevych, 2014a; Stab and Gurevych, 2014b] and legal documents [Palau and Moens, 2009], resp.", "startOffset": 56, "endOffset": 108}, {"referenceID": 20, "context": "741 macro-F1 in corpora consisting of persuasive essays [Stab and Gurevych, 2014a; Stab and Gurevych, 2014b] and legal documents [Palau and Moens, 2009], resp.", "startOffset": 129, "endOffset": 152}, {"referenceID": 14, "context": "Wikipedia articles, these methods have significantly poorer performances: [Levy et al., 2014; Lippi and Torroni, 2015b] report that in the task for detecting claims from Wikipedia articles, only around .", "startOffset": 74, "endOffset": 119}, {"referenceID": 16, "context": "Wikipedia articles, these methods have significantly poorer performances: [Levy et al., 2014; Lippi and Torroni, 2015b] report that in the task for detecting claims from Wikipedia articles, only around .", "startOffset": 74, "endOffset": 119}, {"referenceID": 13, "context": "[Lawrence et al., 2014] as-", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "sume that clauses belonging to the same argument are likely to share the same topic; thus, they employ the LDA-based topic modelling technique [Blei et al., 2003] to extract each clause\u2019s topics and use these topics as features.", "startOffset": 143, "endOffset": 162}, {"referenceID": 18, "context": "[Nguyen and Litman, 2015] use LDA to extract the argument words (i.", "startOffset": 0, "endOffset": 25}, {"referenceID": 24, "context": "However, these features have only been tested on small corpora constructed from well-structured documents (the former is tested on documents obtained from a 19th century philosophical book, while the later is tested on the persuasive essay corpus proposed in [Stab and Gurevych, 2014a]), and the computational expense of LDA is high.", "startOffset": 259, "endOffset": 285}, {"referenceID": 7, "context": "In [Habernal and Gurevych, 2015], ACD is modelled as a sequence tagging problem, and SVM-HMM [Altun et al.", "startOffset": 3, "endOffset": 32}, {"referenceID": 0, "context": "In [Habernal and Gurevych, 2015], ACD is modelled as a sequence tagging problem, and SVM-HMM [Altun et al., 2003] is used to solve this problem; SVM-HMM implicitly considers type-C HAs during the labelling process.", "startOffset": 93, "endOffset": 113}, {"referenceID": 23, "context": "In RL-based text summarisation techniques [Ryang and Abekawa, 2012; Rioux et al., 2014], annotations of all preceding sentences, made in the current round of scanning, are included in the feature vector; in RL-based dialogue generation systems, e.", "startOffset": 42, "endOffset": 87}, {"referenceID": 22, "context": "In RL-based text summarisation techniques [Ryang and Abekawa, 2012; Rioux et al., 2014], annotations of all preceding sentences, made in the current round of scanning, are included in the feature vector; in RL-based dialogue generation systems, e.", "startOffset": 42, "endOffset": 87}, {"referenceID": 21, "context": "[Pietquin et al., 2011; Williams and Young, 2007], the full history of dialogue acts in the current dialogue is used in the state representation.", "startOffset": 0, "endOffset": 49}, {"referenceID": 31, "context": "[Pietquin et al., 2011; Williams and Young, 2007], the full history of dialogue acts in the current dialogue is used in the state representation.", "startOffset": 0, "endOffset": 49}, {"referenceID": 14, "context": "As for other forms of contextual information, in [Levy et al., 2014], the topic of a document is used to build features to identify claims.", "startOffset": 49, "endOffset": 68}, {"referenceID": 16, "context": "However, the importance of the topic information is questionable, as [Lippi and Torroni, 2015b] report that similar performances can be obtained without using the topic information.", "startOffset": 69, "endOffset": 95}, {"referenceID": 15, "context": "[Lippi and Torroni, 2015a]), we need to select RL algorithms with strong generalisation capabilities, so as to learn the optimal policies with limited amount of training data.", "startOffset": 0, "endOffset": 26}, {"referenceID": 11, "context": "To strike a trade-off between the above two factors, we decide to use the least square policy iteration (LSPI) [Lagoudakis and Parr, 2003] algorithm to solve our MDPbased ACD.", "startOffset": 111, "endOffset": 138}, {"referenceID": 6, "context": "[Geramifard et al., 2006; Sutton et al., 2009]).", "startOffset": 0, "endOffset": 46}, {"referenceID": 27, "context": "[Geramifard et al., 2006; Sutton et al., 2009]).", "startOffset": 0, "endOffset": 46}, {"referenceID": 11, "context": "As such, the complexity of each episode (line 6) using LSPI is O(N), and the complexity for obtaining the final policy (line 9) is O(N) [Lagoudakis and Parr, 2003]; thus, the overall complexity in the training phase is O(KMN +N).", "startOffset": 136, "endOffset": 163}, {"referenceID": 2, "context": "2), we select SVM with RBF kernel as the classifier, and its complexity in the training phase is between O(N(KM)) and O(N(KM)) (using LIBSVM [Chang and Lin, 2011]).", "startOffset": 141, "endOffset": 162}, {"referenceID": 0, "context": "As for SVM-HMM [Altun et al., 2003], its complexity is no cheaper than standard SVM.", "startOffset": 15, "endOffset": 35}, {"referenceID": 3, "context": "In the test phase, the complexity for computing the annotation for one clause is O(N \u00b7 |A|) when using LSPI, but is approximately O(N) [Claesen et al., 2014] when using SVM and SVM-HMM (with RBF kernel).", "startOffset": 135, "endOffset": 157}, {"referenceID": 7, "context": "When selecting corpora for testing our methods, we primarily consider the labelling quality of the corpora, because the corpora\u2019s quality heavily influences the quality of the ACD tools trained on them [Habernal and Gurevych, 2015].", "startOffset": 202, "endOffset": 231}, {"referenceID": 5, "context": "Fleiss\u2019 kappa [Fleiss, 1971] is among the most widely used IRA metrics, because it can compute the agreement between two or more raters, and it considers the possibility of the agreement occurring by chance, thus giving more \u201crobust\u201d measure than simple percentage agreement.", "startOffset": 14, "endOffset": 28}, {"referenceID": 29, "context": "1 We randomly sampled 200 hotel reviews of appropriate length (50 - 200 words) in the hotel review dataset provided by [Wachsmuth et al., 2014].", "startOffset": 119, "endOffset": 143}, {"referenceID": 30, "context": "Similar to [Wachsmuth et al., 2015], we viewed each sub-sentence as a clause.", "startOffset": 11, "endOffset": 35}, {"referenceID": 12, "context": "6, suggesting that the agreement is substantial [Landis and Koch, 1977].", "startOffset": 48, "endOffset": 71}, {"referenceID": 26, "context": "Another corpus we used to test our approach is the persuasive essays corpus proposed in [Stab and Gurevych, 2016].", "startOffset": 88, "endOffset": 113}, {"referenceID": 14, "context": "the one in [Levy et al., 2014], have much lower IRA scores (.", "startOffset": 11, "endOffset": 30}, {"referenceID": 20, "context": "39); the legal texts corpus proposed in [Palau and Moens, 2009] is not publicly available, and the web texts corpus proposed in [Habernal et al.", "startOffset": 40, "endOffset": 63}, {"referenceID": 8, "context": "39); the legal texts corpus proposed in [Palau and Moens, 2009] is not publicly available, and the web texts corpus proposed in [Habernal et al., 2014] has relatively low IRA (below .", "startOffset": 128, "endOffset": 151}, {"referenceID": 25, "context": "3), we used exactly the same features to those in [Stab and Gurevych, 2014b].", "startOffset": 50, "endOffset": 76}, {"referenceID": 2, "context": "As for the algorithm implementations, we used LIBSVM [Chang and Lin, 2011] for SVM and a revision of SVM [Joachims et al.", "startOffset": 53, "endOffset": 74}, {"referenceID": 10, "context": "As for the algorithm implementations, we used LIBSVM [Chang and Lin, 2011] for SVM and a revision of SVM [Joachims et al., 2009] for SVM-HMM.", "startOffset": 105, "endOffset": 128}, {"referenceID": 9, "context": "rithms (J48 decision tree, naive Bayes and random forest provided in WEKA [Hall et al., 2009]), and we make similar observations.", "startOffset": 74, "endOffset": 93}, {"referenceID": 28, "context": "the kernel-based RL algorithms [Taylor and Parr, 2009; Ormoneit and Sen, 2002] and the recently proposed deep RL [Mnih et al.", "startOffset": 31, "endOffset": 78}, {"referenceID": 19, "context": "the kernel-based RL algorithms [Taylor and Parr, 2009; Ormoneit and Sen, 2002] and the recently proposed deep RL [Mnih et al.", "startOffset": 31, "endOffset": 78}, {"referenceID": 17, "context": "the kernel-based RL algorithms [Taylor and Parr, 2009; Ormoneit and Sen, 2002] and the recently proposed deep RL [Mnih et al., 2015], the performance of RL-based ACD can be substantially improved (at the price of higher computational complexity though).", "startOffset": 113, "endOffset": 132}, {"referenceID": 25, "context": "Similar observations are also reported in [Stab and Gurevych, 2014b], and we believe that ignoring contextual information (including HAs) is a major factor leading to these misclassifications, as illustrated in Example 1.", "startOffset": 42, "endOffset": 68}], "year": 2017, "abstractText": "Argument component detection (ACD) is an important sub-task in argumentation mining. ACD aims at detecting and classifying different argument components in natural language texts. Historical annotations (HAs) are important features the human annotators consider when they manually perform the ACD task. However, HAs are largely ignored by existing automatic ACD techniques. Reinforcement learning (RL) has proven to be an effective method for using HAs in some natural language processing tasks. In this work, we propose a RL-based ACD technique, and evaluate its performance on two well-annotated corpora. Results suggest that, in terms of classification accuracy, HAsaugmented RL outperforms plain RL by at most 17.85%, and outperforms the state-of-the-art supervised learning algorithm by at most 11.94%.", "creator": "LaTeX with hyperref package"}}}