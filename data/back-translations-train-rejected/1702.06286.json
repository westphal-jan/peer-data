{"id": "1702.06286", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection", "abstract": "Sound events often occur in unstructured environments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks (CNN) are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural networks (RNNs) are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a Convolutional Recurrent Neural Network (CRNN) and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events.", "histories": [["v1", "Tue, 21 Feb 2017 07:37:59 GMT  (2893kb,D)", "http://arxiv.org/abs/1702.06286v1", "Accepted for IEEE Transactions on Audio, Speech and Language Processing, Special Issue on Sound Scene and Event Analysis"]], "COMMENTS": "Accepted for IEEE Transactions on Audio, Speech and Language Processing, Special Issue on Sound Scene and Event Analysis", "reviews": [], "SUBJECTS": "cs.LG cs.SD", "authors": ["emre \\c{c}ak{\\i}r", "giambattista parascandolo", "toni heittola", "heikki huttunen", "tuomas virtanen"], "accepted": false, "id": "1702.06286"}, "pdf": {"name": "1702.06286.pdf", "metadata": {"source": "CRF", "title": "Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection", "authors": ["Emre \u00c7ak\u0131r", "Giambattista Parascandolo", "Toni Heittola", "Heikki Huttunen", "Tuomas Virtanen"], "emails": ["emre.cakir@tut.fi."], "sections": [{"heading": null, "text": "This year, it is so far that it is only a matter of time before it will be so far, until it is so far."}, {"heading": "II. METHOD", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Problem formulation", "text": "The aim of the polyphonic SED is to locate and label the sound event classes contained in a polyphonic audio signal in time. Polyphonic SED can be formulated in two phases: sound representation and classification. In the sound representation stage, the sound characteristics (such as energetics and frequency coefficients (MFCC) for each timeframe t are extracted in the audio signal to obtain a character vector xt, where F-N is the number of characteristics per frame. In the classification stage, the task is to determine the probabilities p (yt) | xt) for the event classes k = 1, 2,."}, {"heading": "B. Proposed Method", "text": "The CRNN proposal in this paper, illustrated in Fig. 2, consists of four parts: (1) at the top of the architecture, a time frequency representation of the data (a context window of the last convolution layer is stacked along the frequency axis and fed to Lr-N recurrent layers; (3) a single feedback layer with sigmoid activation reads the last recurrent layers and estimates the event activity probabilities for each frame; and (4) event activity probabilities are binarized using a constant to obtain event activity predictions. In this structure, the revolutionary layers act as extractors that integrate recurrent properties over time, and finally the upstream layer produces the activity probabilities for each class."}, {"heading": "III. EVALUATION", "text": "To test the proposed method, we conduct a series of experiments on four different datasets. We evaluate the results by comparing the system output with the annotated references. As we approach the task as scene independent, we train a single model on each dataset, regardless of the presence of different acoustic scenes."}, {"heading": "A. Datasets and Settings", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "B. Evaluation Metrics", "text": "The segment lengths used in this work are (1): a single timeframe (40 ms in this work) and (2): a one-second segment. The segment length for each metric is commented with the subscript (e.g., F1frm and F11sec). Segment-based F1 score calculated in a single timeframe (F1frm) is used as the primary evaluation \u2022 For each segment in the test group, intermediate statistics, i.e., the number of true positive (TP), false positive (FP) and false negative (FN) entries are calculated as follows. If an event is detected in one of the frames within a segment and it is also present in the same segment of the commented data, this event is considered a TP. \u2022 is not detected in each of the frames within a segment."}, {"heading": "C. Baselines", "text": "For this paper, we compare the proposed method with two newer approaches: the Gaussian mixing model (GMM) of [38] and the feedback neural mesh model (FNN) of [15]. GMM was chosen as the basic method because it is an established generative modeling method used in many sound detection tasks [12], [13], [16] and [17]. Parallel to the recent wave of deep learning techniques in pattern recognition, FNs have been shown to greatly exceed GMM-based methods. Furthermore, this FNN architecture represents a simple, deep learning method used as a baseline for more complex architectures such as CNN, RNN.GMM. The first baseline system is based on a binary frame classification approach that uses a layer of sound detection methods for each layer."}, {"heading": "D. Experiments set-up", "text": "In recent years, it has become apparent that the number of people who are able to survive on their own has multiplied in all areas in which they are able to hold their own. (...) In recent years, the number of people who are able to climb has multiplied. (...) In recent years, the number of people who are able to climb has multiplied. (...) The number of people who are able to climb has multiplied. (...) In recent years, the number of people who are able to climb has increased. (...) The number of people who are able to climb. (...) The number of people who are able to climb. (...) The number of people who are able to climb has increased. (...) The number of people who are able to climb. (...) The number of people who are able to climb. (...) The number of people who are able to climb. (...) The number of people who are able to climb. (...) The number of people who are able to climb."}, {"heading": "IV. RESULTS", "text": "In this section, we present results for all data sets and experiments described in Section III. Unless otherwise specified, we perform each neural network-based experiment ten times with different random seeds (five times for TUT-SED 2009) to reflect the effects of the initialization of random weight. In this section, we provide the mean and standard deviation of these experiments; the method with the best performance is presented in bold in the tables of this section; the methods whose best performance is below the ten runs within a standard deviation from the method with the best performance are also highlighted in bold; the main results with the best performance (based on the validation data) CRNN, CNN, RNN and the GMM and FNN baselines are presented in Table II. The results are calculated according to the description in Section III-B, taking into account each instance of events independently of the class in the same way as the NCRNA base data and NAs baselines are presented in CNA consistent way."}, {"heading": "A. TUT Sound Events Synthetic 2016", "text": "Considering the number of parameters used for each method (see Table I), the performance of CRNN indicates an architectural advantage over CNN and RNN methods. All of the four deep-seated methods, which relate to the basics and the temporal sequences, exceed the principles of GMF. (5) The way in which the individual methods are applied is not comparable. (5) The way in which the individual groups present their results is very different. (5) The way in which the individual groups present their results is very different. (5) The way in which the individual groups present their results is very different. (5) The way in which the individual groups are proceeded is very different. (5) The way in which the way the the the the way the way is presented is very different. (5)"}, {"heading": "B. TUT-SED 2009", "text": "In order to allow direct comparison, we have calculated all key figures used in this table in the same way. First-published systems were scene-dependent, with information about the scene being made available to the system and separate event models being trained for each scene [14], [24], [25]. Recent work [11], [15] and the current study consist of scene-independent systems. Methods [24], [25] are based on HMM and use either multiple Viterbi decryption stages or NMF pre-processing to perform polyphonic SED processes. In contrast, the use of NMF in [14] does not show explicit class models, but introduces coupled NMF of spectral representation and significant CRM pre-processing to improve the polyphonic SED method [further phasaric methods]."}, {"heading": "C. TUT-SED 2016", "text": "The CRNN and RNN architectures achieve the best results in terms of Framewise F1. CRNN outperforms all other architectures in terms of ER-Framewise and 1-second blocks. While the FNN achieves better results on the 1-second block F1, this is at the expense of a very large 1-second block ER.For all architectures analyzed, the overall results on this data set are quite low compared to the other data sets. This is most likely due to the fact that TUT-SED 2016 is very small and the noise events are sparse (i.e. a large part of the data is silent).If we look at class-wise results (unfortunately not available due to space constraints), we noticed a significant performance difference between the classes that are most strongly represented in the data set (e.g. birdsong and passing car, F1frm around 50%) and the least represented classes (e.g. closet and object mapping, other data systems may be applied to smaller ones)."}, {"heading": "D. CHiME-Home", "text": "The results obtained on CHiME-Home are reproduced in Table VI. For all three of our architectures, there is a significant antiprovement compared to the previous results reported on the same dataset at the DCASE2016 challenge, with new state-of-the-art results being set. After the first series of experiments, CNN achieved slightly better results compared to the CRNN. The CRNN and CNN architecture used are almost identical, with the only exception of the last recurring (GRU) layer in the CRNN, which is replaced by a fully connected layer, followed by the batch normalization. To test whether the improvement in results is due to the absence of recurring connections or the presence of batch normalization, we are again running the same CNN experiments that remove the normalization layer."}, {"heading": "V. CONCLUSIONS", "text": "In this paper, we proposed applying a CRNN - a combination of CNN and RNN, two complementary classification methods - to a polyphonic SED task. Method11first extracts higher characteristics through multiple convolutionary layers (with small filters covering both time and frequency) and pooling in the frequency domain; these characteristics are then passed on to recurring layers, whose characteristics are in turn used to obtain event activity through a feedback layer fully connected. CRNN demonstrates the ability to model local translation invariant filters and RNN ability to model short-term and long-term dependencies. Evaluation results across four datasets show a clear improvement in performance for the proposed CRNN method compared to CNN, RNN and other established methods in polyphonic SED.Despite the improvement in performance, we identify a limitation of temporal dependence in a single classification."}], "references": [{"title": "Reliable detection of audio events in highly noisy environments", "author": ["P. Foggia", "N. Petkov", "A. Saggese", "N. Strisciuglio", "M. Vento"], "venue": "Pattern Recognition Letters, vol. 65, pp. 22\u201328, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic monitoring and localization for social care", "author": ["S. Goetze", "J. Schroder", "S. Gerlach", "D. Hollosi", "J.-E. Appell", "F. Wallhoff"], "venue": "Journal of Computing Science and Engineering, vol. 6, no. 1, pp. 40\u201350, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature learning with deep scattering for urban sound analysis", "author": ["J. Salamon", "J.P. Bello"], "venue": "2015 23rd European Signal Processing Conference (EUSIPCO). IEEE, 2015, pp. 724\u2013728.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Audio-based multimedia event detection using deep recurrent neural networks", "author": ["Y. Wang", "L. Neves", "F. Metze"], "venue": "2016 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 2742\u20132746.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Acoustic event detection for multiple overlapping similar sources", "author": ["D. Stowell", "D. Clayton"], "venue": "2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2015, pp. 1\u20135.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Sound event recognition in unstructured environments using spectrogram image processing", "author": ["J.W. Dennis"], "venue": "Nanyang Technological University, Singapore, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Recognition of acoustic events using deep neural networks", "author": ["O. Gencoglu", "T. Virtanen", "H. Huttunen"], "venue": "Proc. European Signal Processing Conference (EUSIPCO), 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust sound event recognition using convolutional neural networks", "author": ["H. Zhang", "I. McLoughlin", "Y. Song"], "venue": "2015 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 559\u2013563.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust audio event recognition with 1-max pooling convolutional neural networks", "author": ["H. Phan", "L. Hertel", "M. Maass", "A. Mertins"], "venue": "Interspeech, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Environmental sound classification with convolutional neural networks", "author": ["K.J. Piczak"], "venue": "Int. Workshop on Machine Learning for Signal Processing (MLSP), 2015, pp. 1\u20136.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural networks for polyphonic sound event detection in real life recordings", "author": ["G. Parascandolo", "H. Huttunen", "T. Virtanen"], "venue": "2016 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 6440\u20136444.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "A flexible framework for key audio effects detection and auditory context inference", "author": ["L.-H. Cai", "L. Lu", "A. Hanjalic", "H.-J. Zhang", "L.-H. Cai"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing, vol. 14, no. 3, pp. 1026\u20131039, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Acoustic event detection in real life recordings", "author": ["A. Mesaros", "T. Heittola", "A. Eronen", "T. Virtanen"], "venue": "Proc. European Signal Processing Conference (EUSIPCO), 2010, pp. 1267\u20131271.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Sound event detection in real life recordings using coupled matrix factorization of spectral representations and class activity annotations", "author": ["A. Mesaros", "O. Dikmen", "T. Heittola", "T. Virtanen"], "venue": "2015 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 151\u2013155.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Polyphonic sound event detection using multilabel deep neural networks", "author": ["E. Cakir", "T. Heittola", "H. Huttunen", "T. Virtanen"], "venue": "Int. Joint Conf. on Neural Networks (IJCNN), 2015, pp. 1\u20137.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Filterbank learning for deep neural network based polyphonic sound event detection", "author": ["E. Cakir", "E. Ozan", "T. Virtanen"], "venue": "Int. Joint Conf. on Neural Networks (IJCNN), 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "2013 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 6645\u20136649.  12", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "2015 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4580\u20134584.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3128\u20133137.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Context-dependent sound event detection", "author": ["T. Heittola", "A. Mesaros", "A. Eronen", "T. Virtanen"], "venue": "EURASIP Journal on Audio, Speech, and Music Processing, vol. 2013, no. 1, p. 1, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Supervised model training for overlapping sound events based on unsupervised source separation", "author": ["T. Heittola", "A. Mesaros", "T. Virtanen", "M. Gabbouj"], "venue": "Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2013, pp. 8677\u20138681.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Sound event detection using non-negative dictionaries learned from annotated overlapping events", "author": ["O. Dikmen", "A. Mesaros"], "venue": "2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, 2013, pp. 1\u20134.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1998}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "Proceedings of The 33rd International Conference on Machine Learning, 2016, pp. 173\u2013182.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning the speech front-end with raw waveform cldnns", "author": ["T.N. Sainath", "R.J. Weiss", "A. Senior", "K.W. Wilson", "O. Vinyals"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional recurrent neural networks for music classification", "author": ["K. Choi", "G. Fazekas", "M. Sandler", "K. Cho"], "venue": "arXiv preprint arXiv:1609.04243, 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "An end-to-end neural network for polyphonic piano music transcription", "author": ["S. Sigtia", "E. Benetos", "S. Dixon"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 5, pp. 927\u2013939, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1997}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8), 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting.", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Y. Gal"], "venue": "Advances in neural information processing systems, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proceedings of The 32nd International Conference on Machine Learning, 2015, pp. 448\u2013456.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Audio context recognition using audio event histograms", "author": ["T. Heittola", "A. Mesaros", "A. Eronen", "T. Virtanen"], "venue": "Proc. of the 18th European Signal Processing Conference (EUSIPCO), 2010, pp. 1272\u20131276.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "TUT database for acoustic scene classification and sound event detection", "author": ["A. Mesaros", "T. Heittola", "T. Virtanen"], "venue": "24th European Signal Processing Conference (EUSIPCO), 2016.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Chime-home: A dataset for sound source recognition in a domestic environment", "author": ["P. Foster", "S. Sigtia", "S. Krstulovic", "J. Barker", "M.D. Plumbley"], "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). IEEE, 2015, pp. 1\u20135.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "DCASE2016 challenge - audio tagging", "author": ["T. Heittola"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Metrics for polyphonic sound event detection", "author": ["A. Mesaros", "T. Heittola", "T. Virtanen"], "venue": "Applied Sciences, vol. 6, no. 6, p. 162, 2016.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Apples-to-apples in cross-validation studies: pitfalls in classifier performance measurement", "author": ["G. Forman", "M. Scholz"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 12, no. 1, pp. 49\u201357, 2010.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic context detection based on hierarchical audio models", "author": ["W.-H. Cheng", "W.-T. Chu", "J.-L. Wu"], "venue": "Proceedings of the 5th ACM  SIGMM international workshop on Multimedia information retrieval, 2003, pp. 109\u2013115.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2003}, {"title": "2016) DCASE2016 baseline system", "author": ["T. Heittola", "A. Mesaros", "T. Virtanen"], "venue": "https://github.com/TUT-ARG/", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE Int. Conf. on Computer Vision, 2015, pp. 1026\u2013 1034.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "International conference on learning representations, 2015.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "librosa: Audio and music signal analysis in python", "author": ["B. McFee", "C. Raffel", "D. Liang", "D.P. Ellis", "M. McVicar", "E. Battenberg", "O. Nieto"], "venue": "Proceedings of the 14th Python in Science Conference, 2015.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: A python framework for fast computation of mathematical expressions", "author": ["T.T.D. Team", "R. Al-Rfou", "G. Alain", "A. Almahairi", "C. Angermueller", "D. Bahdanau", "N. Ballas", "F. Bastien", "J. Bayer", "A. Belikov"], "venue": "arXiv preprint arXiv:1605.02688, 2016.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "CQT-based convolutional neural networks for audio scene classification and domestic audio tagging", "author": ["T. Lidy", "A. Schindler"], "venue": "DCASE2016 Challenge, Tech. Rep., September 2016.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Domestic audio tagging with convolutional neural networks", "author": ["E. Cakir", "T. Heittola", "T. Virtanen"], "venue": "DCASE2016 Challenge, Tech. Rep., September 2016.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminative training of GMM parameters for audio scene classification", "author": ["S. Yun", "S. Kim", "S. Moon", "J. Cho", "T. Kim"], "venue": "DCASE2016 Challenge, Tech. Rep., September 2016.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "ICLR Workshop, 2014.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning of representations for unsupervised and transfer learning.", "author": ["Y. Bengio"], "venue": "ICML Unsupervised and Transfer Learning,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "SED has been drawing a surging amount of interest in recent years with applications including audio surveillance [1], healthcare monitoring [2], urban sound analysis [3], multimedia event detection [4] and bird call detection [5].", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "SED has been drawing a surging amount of interest in recent years with applications including audio surveillance [1], healthcare monitoring [2], urban sound analysis [3], multimedia event detection [4] and bird call detection [5].", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "SED has been drawing a surging amount of interest in recent years with applications including audio surveillance [1], healthcare monitoring [2], urban sound analysis [3], multimedia event detection [4] and bird call detection [5].", "startOffset": 166, "endOffset": 169}, {"referenceID": 3, "context": "SED has been drawing a surging amount of interest in recent years with applications including audio surveillance [1], healthcare monitoring [2], urban sound analysis [3], multimedia event detection [4] and bird call detection [5].", "startOffset": 198, "endOffset": 201}, {"referenceID": 4, "context": "SED has been drawing a surging amount of interest in recent years with applications including audio surveillance [1], healthcare monitoring [2], urban sound analysis [3], multimedia event detection [4] and bird call detection [5].", "startOffset": 226, "endOffset": 229}, {"referenceID": 5, "context": "Factors such as environmental noise and overlapping sources are present in the unstructured environments and they may introduce a high degree of variation among the sound events from the same sound event class [6].", "startOffset": 210, "endOffset": 213}, {"referenceID": 0, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 104, "endOffset": 107}, {"referenceID": 7, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 150, "endOffset": 153}, {"referenceID": 9, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 155, "endOffset": 159}, {"referenceID": 3, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 196, "endOffset": 199}, {"referenceID": 10, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 201, "endOffset": 205}, {"referenceID": 11, "context": "model (HMM) based modeling\u2014an established method that has been widely used in automatic speech recognition\u2014has been proposed to model individual sound events with Gaussian mixtures and detect each event through HMM states using Viterbi algorithm [12], [13].", "startOffset": 246, "endOffset": 250}, {"referenceID": 12, "context": "model (HMM) based modeling\u2014an established method that has been widely used in automatic speech recognition\u2014has been proposed to model individual sound events with Gaussian mixtures and detect each event through HMM states using Viterbi algorithm [12], [13].", "startOffset": 252, "endOffset": 256}, {"referenceID": 13, "context": "Non-negative matrix factorization (NMF) based source separation [14] and deep learning based methods (such as feedforward neural networks (FNN) [15], CNN [16] and RNN [11]) have been shown to perform significantly better compared to established methods such as GMM-HMM for polyphonic SED.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "Non-negative matrix factorization (NMF) based source separation [14] and deep learning based methods (such as feedforward neural networks (FNN) [15], CNN [16] and RNN [11]) have been shown to perform significantly better compared to established methods such as GMM-HMM for polyphonic SED.", "startOffset": 144, "endOffset": 148}, {"referenceID": 15, "context": "Non-negative matrix factorization (NMF) based source separation [14] and deep learning based methods (such as feedforward neural networks (FNN) [15], CNN [16] and RNN [11]) have been shown to perform significantly better compared to established methods such as GMM-HMM for polyphonic SED.", "startOffset": 154, "endOffset": 158}, {"referenceID": 10, "context": "Non-negative matrix factorization (NMF) based source separation [14] and deep learning based methods (such as feedforward neural networks (FNN) [15], CNN [16] and RNN [11]) have been shown to perform significantly better compared to established methods such as GMM-HMM for polyphonic SED.", "startOffset": 167, "endOffset": 171}, {"referenceID": 16, "context": "Deep neural networks [17] have recently achieved remarkable success in several domains such as image recognition [18], [19], speech recognition [20], [21], machine translation [22], even integrating multiple data modalities such as image and text in image captioning [23].", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "Deep neural networks [17] have recently achieved remarkable success in several domains such as image recognition [18], [19], speech recognition [20], [21], machine translation [22], even integrating multiple data modalities such as image and text in image captioning [23].", "startOffset": 113, "endOffset": 117}, {"referenceID": 18, "context": "Deep neural networks [17] have recently achieved remarkable success in several domains such as image recognition [18], [19], speech recognition [20], [21], machine translation [22], even integrating multiple data modalities such as image and text in image captioning [23].", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "Deep neural networks [17] have recently achieved remarkable success in several domains such as image recognition [18], [19], speech recognition [20], [21], machine translation [22], even integrating multiple data modalities such as image and text in image captioning [23].", "startOffset": 144, "endOffset": 148}, {"referenceID": 20, "context": "Deep neural networks [17] have recently achieved remarkable success in several domains such as image recognition [18], [19], speech recognition [20], [21], machine translation [22], even integrating multiple data modalities such as image and text in image captioning [23].", "startOffset": 150, "endOffset": 154}, {"referenceID": 21, "context": "Deep neural networks [17] have recently achieved remarkable success in several domains such as image recognition [18], [19], speech recognition [20], [21], machine translation [22], even integrating multiple data modalities such as image and text in image captioning [23].", "startOffset": 176, "endOffset": 180}, {"referenceID": 22, "context": "Deep neural networks [17] have recently achieved remarkable success in several domains such as image recognition [18], [19], speech recognition [20], [21], machine translation [22], even integrating multiple data modalities such as image and text in image captioning [23].", "startOffset": 267, "endOffset": 271}, {"referenceID": 6, "context": "Feedforward neural networks have been used in monophonic [7] and polyphonic SED in real-life environments [15] by processing concatenated input frames from a small time window of the spectrogram.", "startOffset": 57, "endOffset": 60}, {"referenceID": 14, "context": "Feedforward neural networks have been used in monophonic [7] and polyphonic SED in real-life environments [15] by processing concatenated input frames from a small time window of the spectrogram.", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "This simple architecture\u2014while vastly improving over established approaches such as GMMHMMs [24] and NMF source separation based SED [25], [26]\u2014presents two major shortcomings: (1) it lacks both time and frequency invariance\u2014due to the fixed connections between the input and the hidden units\u2014which would allow to model small variations in the events; (2) temporal context is restricted to short time windows, preventing effective modeling of typically longer events (e.", "startOffset": 92, "endOffset": 96}, {"referenceID": 24, "context": "This simple architecture\u2014while vastly improving over established approaches such as GMMHMMs [24] and NMF source separation based SED [25], [26]\u2014presents two major shortcomings: (1) it lacks both time and frequency invariance\u2014due to the fixed connections between the input and the hidden units\u2014which would allow to model small variations in the events; (2) temporal context is restricted to short time windows, preventing effective modeling of typically longer events (e.", "startOffset": 133, "endOffset": 137}, {"referenceID": 25, "context": "This simple architecture\u2014while vastly improving over established approaches such as GMMHMMs [24] and NMF source separation based SED [25], [26]\u2014presents two major shortcomings: (1) it lacks both time and frequency invariance\u2014due to the fixed connections between the input and the hidden units\u2014which would allow to model small variations in the events; (2) temporal context is restricted to short time windows, preventing effective modeling of typically longer events (e.", "startOffset": 139, "endOffset": 143}, {"referenceID": 26, "context": "CNNs [27] can address the former limitation by learning filters that are shifted in both time and frequency [8], lacking", "startOffset": 5, "endOffset": 9}, {"referenceID": 7, "context": "CNNs [27] can address the former limitation by learning filters that are shifted in both time and frequency [8], lacking", "startOffset": 108, "endOffset": 111}, {"referenceID": 19, "context": "Recurrent neural networks (RNNs), which have been successfully applied to automatic speech recognition (ASR) [20] and polyphonic SED [11], solve the latter shortcoming by integrating information from the earlier time windows, presenting a theoretically unlimited context information.", "startOffset": 109, "endOffset": 113}, {"referenceID": 10, "context": "Recurrent neural networks (RNNs), which have been successfully applied to automatic speech recognition (ASR) [20] and polyphonic SED [11], solve the latter shortcoming by integrating information from the earlier time windows, presenting a theoretically unlimited context information.", "startOffset": 133, "endOffset": 137}, {"referenceID": 20, "context": "Similar approaches combining CNNs and RNNs have been presented recently in ASR [21], [28], [29] and music classification [30].", "startOffset": 79, "endOffset": 83}, {"referenceID": 27, "context": "Similar approaches combining CNNs and RNNs have been presented recently in ASR [21], [28], [29] and music classification [30].", "startOffset": 85, "endOffset": 89}, {"referenceID": 28, "context": "Similar approaches combining CNNs and RNNs have been presented recently in ASR [21], [28], [29] and music classification [30].", "startOffset": 91, "endOffset": 95}, {"referenceID": 29, "context": "Similar approaches combining CNNs and RNNs have been presented recently in ASR [21], [28], [29] and music classification [30].", "startOffset": 121, "endOffset": 125}, {"referenceID": 3, "context": "This approach integrates the strengths of both CNNs and RNNs, which have shown excellent performance in acoustic pattern recognition applications [4], [8], [9], [10], while overcoming their individual weaknesses.", "startOffset": 146, "endOffset": 149}, {"referenceID": 7, "context": "This approach integrates the strengths of both CNNs and RNNs, which have shown excellent performance in acoustic pattern recognition applications [4], [8], [9], [10], while overcoming their individual weaknesses.", "startOffset": 151, "endOffset": 154}, {"referenceID": 8, "context": "This approach integrates the strengths of both CNNs and RNNs, which have shown excellent performance in acoustic pattern recognition applications [4], [8], [9], [10], while overcoming their individual weaknesses.", "startOffset": 156, "endOffset": 159}, {"referenceID": 9, "context": "This approach integrates the strengths of both CNNs and RNNs, which have shown excellent performance in acoustic pattern recognition applications [4], [8], [9], [10], while overcoming their individual weaknesses.", "startOffset": 161, "endOffset": 165}, {"referenceID": 20, "context": "does not shrink) by computing the max pooling operation in the frequency dimension only\u2014as done in [21], [31]\u2014and by zero-padding the inputs to the convolutional layers (also known as same convolution).", "startOffset": 99, "endOffset": 103}, {"referenceID": 30, "context": "does not shrink) by computing the max pooling operation in the frequency dimension only\u2014as done in [21], [31]\u2014and by zero-padding the inputs to the convolutional layers (also known as same convolution).", "startOffset": 105, "endOffset": 109}, {"referenceID": 31, "context": "The function F , which can represent a long short term memory (LSTM) unit [32] or gated recurrent unit (GRU) [33], has two", "startOffset": 74, "endOffset": 78}, {"referenceID": 32, "context": "The function F , which can represent a long short term memory (LSTM) unit [32] or gated recurrent unit (GRU) [33], has two", "startOffset": 109, "endOffset": 113}, {"referenceID": 33, "context": "Regularization: In order to reduce overfitting, we experimented with dropout [34] regularization in the network, which has proven to be extremely effective in several deep learning applications [18].", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "Regularization: In order to reduce overfitting, we experimented with dropout [34] regularization in the network, which has proven to be extremely effective in several deep learning applications [18].", "startOffset": 194, "endOffset": 198}, {"referenceID": 33, "context": "This reduces units co-adaptation, approximates model averaging [34], and can be seen as a form of data augmentation without domain knowledge.", "startOffset": 63, "endOffset": 67}, {"referenceID": 34, "context": "For the recurrent layers we adopted the dropout proposed in [35], where the choice of dropped units is kept constant along a sequence.", "startOffset": 60, "endOffset": 64}, {"referenceID": 35, "context": "To speed up the training phase we train our networks with batch normalization layers [36] after every convolutional or fully connected layer.", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "with the network presented in [21] for speech recognition.", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "(v) We use much longer sequences, up to thousands of steps, compared to 20 steps in [21].", "startOffset": 84, "endOffset": 88}, {"referenceID": 36, "context": "b) TUT Sound Events 2009 (TUT-SED 2009): This dataset, first presented in [37], consists of 8 to 14 binaural recordings from 10 real-life scenes.", "startOffset": 74, "endOffset": 78}, {"referenceID": 37, "context": "2016): This dataset consists of recordings from two real-life scenes: residential area and home [38].", "startOffset": 96, "endOffset": 100}, {"referenceID": 37, "context": "The four-fold cross-validation setup published along with the dataset [38] is used in the evaluations.", "startOffset": 70, "endOffset": 74}, {"referenceID": 38, "context": "d) CHiME-Home: CHiME-Home dataset [39] consists of 4-second audio chunks from home environments.", "startOffset": 34, "endOffset": 38}, {"referenceID": 39, "context": "Home as it is used in audio tagging task in DCASE2016 challenge [40], namely 1946 chunks for development (in four folds) and 846 chunks for evaluation.", "startOffset": 64, "endOffset": 68}, {"referenceID": 40, "context": "Segment-based F1 score calculated in a single time frame (F1frm) is used as the primary evaluation metric [41].", "startOffset": 106, "endOffset": 110}, {"referenceID": 41, "context": "This calculation method is referred to as micro-averaging, and is the recommended method for evaluation of classifier [42].", "startOffset": 118, "endOffset": 122}, {"referenceID": 40, "context": "More detailed and visualized explanation of segment-based F1 score in multi label setting can be found in [41].", "startOffset": 106, "endOffset": 110}, {"referenceID": 40, "context": "The second evaluation metric is segment-based error rate as proposed in [41].", "startOffset": 72, "endOffset": 76}, {"referenceID": 40, "context": ", the number of substitutions (s), insertions (i), deletions (d) and active classes from annotations (a) are calculated per segment as explained in detail in [41].", "startOffset": 158, "endOffset": 162}, {"referenceID": 10, "context": "The main metric used in previous works [11], [14], [15] on TUT-SED 2009 dataset differs from the F1 score calculation used in this paper.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "The main metric used in previous works [11], [14], [15] on TUT-SED 2009 dataset differs from the F1 score calculation used in this paper.", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "The main metric used in previous works [11], [14], [15] on TUT-SED 2009 dataset differs from the F1 score calculation used in this paper.", "startOffset": 51, "endOffset": 55}, {"referenceID": 37, "context": "For this work, we compare the proposed method with two recent approaches: the Gaussian mixture model (GMM) of [38] and the feedforward neural network model (FNN) from [15].", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "For this work, we compare the proposed method with two recent approaches: the Gaussian mixture model (GMM) of [38] and the feedforward neural network model (FNN) from [15].", "startOffset": 167, "endOffset": 171}, {"referenceID": 11, "context": "GMM has been chosen as a baseline method since it is an established generative modeling method used in many sound recognition tasks [12], [13], [43].", "startOffset": 132, "endOffset": 136}, {"referenceID": 12, "context": "GMM has been chosen as a baseline method since it is an established generative modeling method used in many sound recognition tasks [12], [13], [43].", "startOffset": 138, "endOffset": 142}, {"referenceID": 42, "context": "GMM has been chosen as a baseline method since it is an established generative modeling method used in many sound recognition tasks [12], [13], [43].", "startOffset": 144, "endOffset": 148}, {"referenceID": 14, "context": "GMM based methods in SED [15].", "startOffset": 25, "endOffset": 29}, {"referenceID": 37, "context": "GMM: The first baseline system is based on a binary frame-classification approach, where for each sound event class a binary classifier is set up [38].", "startOffset": 146, "endOffset": 150}, {"referenceID": 43, "context": "The system is used as a baseline in the DCASE2016 challenge [44], however, in this study the system is used as scene-independent to match the setting of the other methods presented.", "startOffset": 60, "endOffset": 64}, {"referenceID": 14, "context": "FNN: The second baseline system is a deep multi-label FNN with temporal context [15].", "startOffset": 80, "endOffset": 84}, {"referenceID": 14, "context": "The sliding window post-processing of the event activity probabilities in [15] has not been implemented for the baseline experiments in order to make a fair comparison based on classifier architecture for different deep learning", "startOffset": 74, "endOffset": 78}, {"referenceID": 37, "context": "GMM [38] 40.", "startOffset": 4, "endOffset": 8}, {"referenceID": 14, "context": "FNN [15] 49.", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "We opted for this setting as it was recently used with very good performance in several works on SED [11], [15].", "startOffset": 101, "endOffset": 105}, {"referenceID": 14, "context": "We opted for this setting as it was recently used with very good performance in several works on SED [11], [15].", "startOffset": 107, "endOffset": 111}, {"referenceID": 44, "context": "The weights are initialized according to the scheme proposed in [46].", "startOffset": 64, "endOffset": 68}, {"referenceID": 45, "context": "Binary cross-entropy is set as the loss function, and all networks are trained with Adam [47] as gradient descent optimizer, with the default parameters proposed in the original paper.", "startOffset": 89, "endOffset": 93}, {"referenceID": 46, "context": "For feature extraction, the Python library Librosa [48] has been used in this work.", "startOffset": 51, "endOffset": 55}, {"referenceID": 47, "context": "2) as backend [50].", "startOffset": 14, "endOffset": 18}, {"referenceID": 48, "context": "As claimed in [51], this may be due to the capability of deep learning methods to use different subsets of hidden units to model different sound events simultaneously.", "startOffset": 14, "endOffset": 18}, {"referenceID": 30, "context": "This is consistent with the results presented in [31] on a similar task.", "startOffset": 49, "endOffset": 53}, {"referenceID": 13, "context": "formation about the scene is provided to the system and separate event models are trained for each scene [14], [24], [25].", "startOffset": 105, "endOffset": 109}, {"referenceID": 23, "context": "formation about the scene is provided to the system and separate event models are trained for each scene [14], [24], [25].", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "formation about the scene is provided to the system and separate event models are trained for each scene [14], [24], [25].", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "More recent work [11], [15], as well as the current study,", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "More recent work [11], [15], as well as the current study,", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "HMM multiple Viterbi decoding? [24] 20.", "startOffset": 31, "endOffset": 35}, {"referenceID": 24, "context": "4 NMF-HMM? [25] 36.", "startOffset": 11, "endOffset": 15}, {"referenceID": 24, "context": "7 NMF-HMM + stream elimination? [25] 44.", "startOffset": 32, "endOffset": 36}, {"referenceID": 37, "context": "9 GMM? [38] 34.", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "Coupled NMF? [14] 57.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "FNN [15] 63.", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "BLSTM [11] 64.", "startOffset": 6, "endOffset": 10}, {"referenceID": 23, "context": "Methods [24], [25] are HMM based, using either multiple Viterbi decoding stages or NMF pre-processing to do polyphonic SED.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "Methods [24], [25] are HMM based, using either multiple Viterbi decoding stages or NMF pre-processing to do polyphonic SED.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "In contrast, the use of NMF in [14] does not build explicit class models, but performs coupled NMF of spectral representation and event activity annotations to build dictionaries.", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "CRNN has significantly higher performance than previous methods [14], [24], [25], [38], and it still shows considerable improvement over other neural network approaches.", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "CRNN has significantly higher performance than previous methods [14], [24], [25], [38], and it still shows considerable improvement over other neural network approaches.", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "CRNN has significantly higher performance than previous methods [14], [24], [25], [38], and it still shows considerable improvement over other neural network approaches.", "startOffset": 76, "endOffset": 80}, {"referenceID": 37, "context": "CRNN has significantly higher performance than previous methods [14], [24], [25], [38], and it still shows considerable improvement over other neural network approaches.", "startOffset": 82, "endOffset": 86}, {"referenceID": 49, "context": "[52] 17.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[53] 17.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[54] 17.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "More specifically, we use the technique described in [55] to visualize what kind of patterns in the input data different neurons in the convolutional layers are looking for.", "startOffset": 53, "endOffset": 57}, {"referenceID": 53, "context": "Transfer learning [56], [57] could be potentially applied with success in this setting: by first training a CRNN on a large dataset (such as TUT-SED Synthetic 2016), the last", "startOffset": 18, "endOffset": 22}], "year": 2017, "abstractText": "Sound events often occur in unstructured environments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks (CNN) are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural networks (RNNs) are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a Convolutional Recurrent Neural Network (CRNN) and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events.", "creator": "LaTeX with hyperref package"}}}