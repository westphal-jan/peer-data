{"id": "1702.06329", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Towards a Common Implementation of Reinforcement Learning for Multiple Robotic Tasks", "abstract": "Mobile robots are increasingly being employed for performing complex tasks in dynamic environments. Reinforcement learning (RL) methods are recognized to be promising for specifying such tasks in a relatively simple manner. However, the strong dependency between the learning method and the task to learn is a well-known problem that restricts practical implementations of RL in robotics, often requiring major modifications of parameters and adding other techniques for each particular task. In this paper we present a practical core implementation of RL which enables the learning process for multiple robotic tasks with minimal per-task tuning or none. Based on value iteration methods, this implementation includes a novel approach for action selection, called Q-biased softmax regression (QBIASSR), which avoids poor performance of the learning process when the robot reaches new unexplored states. Our approach takes advantage of the structure of the state space by attending the physical variables involved (e.g., distances to obstacles, X,Y,{\\theta} pose, etc.), thus experienced sets of states may favor the decision-making process of unexplored or rarely-explored states. This improvement has a relevant role in reducing the tuning of the algorithm for particular tasks. Experiments with real and simulated robots, performed with the software framework also introduced here, show that our implementation is effectively able to learn different robotic tasks without tuning the learning method. Results also suggest that the combination of true online SARSA({\\lambda}) with QBIASSR can outperform the existing RL core algorithms in low-dimensional robotic tasks.", "histories": [["v1", "Tue, 21 Feb 2017 11:07:27 GMT  (747kb,D)", "http://arxiv.org/abs/1702.06329v1", "15 pages, 10 figures, 7 tables. To be published in a scientific journal"]], "COMMENTS": "15 pages, 10 figures, 7 tables. To be published in a scientific journal", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.RO", "authors": ["angel mart\\'inez-tenor", "juan antonio fern\\'andez-madrigal", "ana cruz-mart\\'in", "javier gonz\\'alez-jim\\'enez"], "accepted": false, "id": "1702.06329"}, "pdf": {"name": "1702.06329.pdf", "metadata": {"source": "CRF", "title": "Towards a Common Implementation of Reinforcement Learning for Multiple Robotic Tasks", "authors": ["Angel Mart\u0301\u0131nez-Tenor", "Juan Antonio Fern\u00e1ndez-Madrigal", "Ana Cruz-Mart\u0301\u0131n", "Javier Gonz\u00e1lez-Jim\u00e9nez"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which"}, {"heading": "2 Background and related work", "text": "In this section we offer a brief summary of the most important RL algorithms related to this work and their application to robotics."}, {"heading": "2.1 Reinforcement learning", "text": "The concept of autonomous learning in robotics goes back to Alan Turing's idea of having robots that learn in a similar way to children (Turing 1950), which evolved into the modern concept of developmental robotics (Lungarella, Metta, Pfeifer & Sandini 2003), a paradigm that is still emerging today, analogous to that of mental development in human psychology. In the field of autonomous learning, mechanisms of decision-making under uncertainty are an important and increasingly popular approach. They represent cognitive processes that are able to select a sequence of actions that should lead to a specific outcome. Markov decision-making processes (MDP) (Murphy 2002) are the most commonly used paradigms in this sense. They are dynamic Bayesian networks whose nodes define the system states and whose actions, together with the associated rewards and probabilities of occurrence, define a MDP is formally a tuple (S, A, T, where actions are a series of S)."}, {"heading": "8 end", "text": "The success of Q-learning (in pseudocode in algorithm 1 (s), a) (4) (often necessary in most RL steps) depends on the exact choice of parameters \u03b1 and \u03b3, along with a set of appropriate rewards R (s, a, s) that define the task to be learned, and an action selection strategy, the latter pointing us to the exploitation exploration dilemma, which consists in deciding whether the agent should use his current learned policy or explore other measures at each learning step. A practical upgrade of Q-Learning came with SARSA (Rummery & Niranjan 1994), which updates the Q values for the action that the agent will perform in the next learning step instead of the optimal action that has been learned so far. Hence a, the action to perform in the next step, must also be selected before updating Q (s, a). The TD error in this case is: It is k + k = Q\u2212 Qa."}, {"heading": "2.2 Reinforcement learning in robotics", "text": "Today, the effectiveness of RL algorithms has been proven in many different areas, such as game theory, control technology, statistics, or even robotics, when using toy models or very low-realistic robot simulators (Sutton & Barto 1998), (Kaelbling et al. 1996), (Wiering & Van Otterlo 2012), and (Kober et al. 2013).RL researchers can also resort to more advanced techniques to overcome the constraints arising from the practical aspects of the RL (Kober et al. 2013), such as the Curse of Dimensionality (Bellman 1957).However, when RL is applied to precise, physically realistic simulators or real robots, we need to address additional problems in this area, such as the Curse of Real Samples (Kober et al. 2013), which hinders the execution of RL-based methods, often rendering them impracticable."}, {"heading": "3 Reducing the task depen-", "text": "The RL-ROBOT software framework, which is described in depth in Section 4, was used together with the very realistic V-REP simulator (Rohmer, Singh Freese 2013) to compare these methods. The RL-Q problem, defined as a series of states, actions and rewards, looks abstract and task-independent; however, solving a task in an RL approach (i.e. approaching an almost optimal value function) depends on the structure of the unknown transition matrix, that is, on how easy it is to research all interesting states often enough. Therefore, our efforts to reduce dependence on the respective task focus on accelerating the estimation of Q values for relevant state actions, which are sometimes difficult to explore, to explore in this task. First, we have conducted a series of experiments to find a learning method among the existing RL techniques that has good properties when we are confronted with other REL-related tasks, the RL-BOT framework was used in the very deep section."}, {"heading": "4 Implementation: the RL-", "text": "The idea behind it is that people in the region, not only in the United States, but also in other parts of the world, come up with the idea of changing themselves and their environment - in the way in which they do what they want to do, and in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in the way in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do not do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do not, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, in which they do it, they do it, in which they do it, they do it, they do it, they do it, they do it, they do it, they do it, they do it, they do it, they do it, they do it, they do it, they do it, they do it, they do it, they do it, they do it,"}, {"heading": "5 Experiments and discussion", "text": "The experiments in this work were performed in RL-ROBOT with both simulated and real robots. Periods of up to 60 minutes were used in simulated experiments to be reproducible on real robots. A learning curve showing the evolution of the average reward achieved over time (in steps) has proven to be both a stable measure of learning process performance and a reliable indicator for comparing different techniques. For episodic tasks, the evolution of the average reward of the last episode was used as an indicator instead of the classical development per episode, as the number of episodes required was low. Results include the average learning curve of several repetitions as well as a variance analysis and a post-hoc tukey test (Tukey 1949) for each task to assess the significance of the conclusions."}, {"heading": "5.1 Sample-modeled and V-REP simulated experiments", "text": "Sample-modeled experiments are based on Markovian models based on T (s, a, s) and R (s, s), which include real-world tasks in the V-REP simulation. On the contrary, in realistic simulations, each learning step was performed in one second of simulation directly in V-REP. The total time of each learning process ranges from 1 to 60 minutes after the task. Preliminary tests performed in Section 3 are essential to obtain a basic algorithm valid for the existing tasks, which led to TOSL, as explained above. Both examples of modeled and V-REP simulations were used with multiple tasks."}, {"heading": "5.2 Real robot experiments", "text": "The next experiments were performed with the Giraff robot shown in Figure 8, which is equipped with two differential drives and two rollers and a Hokuyo laser rangefinder. The same tasks designed for the simulated mobile robot can be learned from Giraff by specifying only the physical parameters of the task. In this case, RL-ROBOT starts the ROS node, giving access to the real sensors and actuators. Giraff must learn the task in a hexagonal scenario of 1.5x1m. Up to 20 learning processes of 30 minutes each were performed for TOSL + QBIASSR, TOSL + SR and Q + SR. The results shown in Figure 9 underscore the differences between the evaluated methods. The learning curve of TOSL + QBIASSR shows a continuous decrease (collisions) during the first 3 minutes (180 steps), whereupon QOSR has learned to perform the task significantly better than the average of QASS + learning curve."}, {"heading": "5.3 Reduced eligibility traces", "text": "A comparative test between the reduced version of the Authorization Certificates (ETS) proposed in Section 3 and the standard ETS was also carried out in a random sample simulation to verify whether the reduced ETS used in this thesis has a negligible effect on the learning process, a conclusion clearly illustrated in Figure 10."}, {"heading": "5.4 Computational cost", "text": "Finally, a final set of tests was performed to measure the computational cost of the algorithms used in this thesis. To this end, the tests were performed in random modeled simulation for the walking 1K task. Various RL implementations were tested, with 30 learning processes of 3,600 steps each performed. Measuring the choice to assess the computational impact of the learning algorithms is the average CPU time consumed in each learning step. Results obtained with RL-ROBOT in an IntelCore i5-4460 processor running on 64 bits of Ubuntu are shown in Table 3. Execution times achieved for TOSL using the reduced ET discussed in Section 3 are up to 17 times faster than the standard ET implementation, without affecting the performance of the learning algorithm; they are even of the same magnitude as the reference times of Q-Learning and SARL, which we consider negligible for Rtitude applications."}, {"heading": "6 Conclusions and future work", "text": "This year, it has come to the point that there is only one occasion when there is a scandal before there is a scandal."}], "references": [{"title": "Autonomous helicopter aerobatics through apprenticeship learning", "author": ["P. Abbeel", "A. Coates", "A.Y. Ng"], "venue": "The International Journal of Robotics Research", "citeRegEx": "Abbeel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2010}, {"title": "Transfer learning for reinforcement learning on a physical robot, in \u2018Ninth International Conference on Autonomous Agents and Multiagent Systems-Adaptive Learning Agents Workshop (AAMAS-ALA)", "author": ["S. Barrett", "M.E. Taylor", "P. Stone"], "venue": null, "citeRegEx": "Barrett et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Barrett et al\\.", "year": 2010}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["A.G. Barto", "S. Mahadevan"], "venue": "Discrete Event Dynamic Systems", "citeRegEx": "Barto and Mahadevan,? \\Q2003\\E", "shortCiteRegEx": "Barto and Mahadevan", "year": 2003}, {"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["A.G. Barto", "R.S. Sutton", "C.W. Anderson"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics", "citeRegEx": "Barto et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1983}, {"title": "Dynamic programming and Lagrange multipliers", "author": ["R. Bellman"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "Bellman,? \\Q1956\\E", "shortCiteRegEx": "Bellman", "year": 1956}, {"title": "Transferring knowledge as heuristics in reinforcement learning: A case-based approach", "author": ["R.A. Bianchi", "L.A. Celiberto", "P.E. Santos", "J.P. Matsuura", "R.L. de Mantaras"], "venue": "Artificial Intelligence", "citeRegEx": "Bianchi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bianchi et al\\.", "year": 2015}, {"title": "Hierarchical reinforcement learning and decision making", "author": ["M.M. Botvinick"], "venue": "Current Opinion in Neurobiology 22(6),", "citeRegEx": "Botvinick,? \\Q2012\\E", "shortCiteRegEx": "Botvinick", "year": 2012}, {"title": "OpenAI gym", "author": ["G. Brockman", "V. Cheung", "L. Pettersson", "J. Schneider", "J. Schulman", "J. Tang", "W. Zaremba"], "venue": null, "citeRegEx": "Brockman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Brockman et al\\.", "year": 2016}, {"title": "Multiple model q-learning for stochastic asynchronous rewards", "author": ["J.S. Campbell", "S.N. Givigi", "H.M. Schwartz"], "venue": "Journal of Intelligent & Robotic Systems", "citeRegEx": "Campbell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Campbell et al\\.", "year": 2016}, {"title": "Modelfree reinforcement learning with continuous action in practice, in \u20182012", "author": ["T. Degris", "P.M. Pilarski", "R.S. Sutton"], "venue": "American Control Conference (ACC)\u2019,", "citeRegEx": "Degris et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Degris et al\\.", "year": 2012}, {"title": "Gaussian processes for data-efficient learning in robotics and control", "author": ["M.P. Deisenroth", "D. Fox", "C.E. Rasmussen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Deisenroth et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2015}, {"title": "A comprehensive survey on safe reinforcement learning", "author": ["J. Gar\u0107\u0131a", "F. Fern\u00e1ndez"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Gar\u0107\u0131a and Fern\u00e1ndez,? \\Q2015\\E", "shortCiteRegEx": "Gar\u0107\u0131a and Fern\u00e1ndez", "year": 2015}, {"title": "Q-Learning for Robot Control, PhD thesis, Research School of Information Sciences and Engineering. Department of Systems Engineering and The Australian National University", "author": ["C. Gaskett"], "venue": null, "citeRegEx": "Gaskett,? \\Q2002\\E", "shortCiteRegEx": "Gaskett", "year": 2002}, {"title": "Rlpy: A value-function-based reinforcement learning framework for education and research", "author": ["A. Geramifard", "C. Dann", "R.H. Klein", "W. Dabney", "J.P. How"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Geramifard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Geramifard et al\\.", "year": 2015}, {"title": "Learning exploration strategies in model-based reinforcement learning", "author": ["T. Hester", "M. Lopes", "P. Stone"], "venue": "in \u2018Proceedings of the 2013 International Conference on Autonomous Agents and Multiagent Systems\u2019,", "citeRegEx": "Hester et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hester et al\\.", "year": 2013}, {"title": "Rtmba: A real-time model-based reinforcement learning architecture for robot control, in \u20182012", "author": ["T. Hester", "M. Quinlan", "P. Stone"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),\u2019,", "citeRegEx": "Hester et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hester et al\\.", "year": 2012}, {"title": "Texplore: real-time sample-efficient reinforcement learning for robots", "author": ["T. Hester", "P. Stone"], "venue": "Machine learning", "citeRegEx": "Hester and Stone,? \\Q2013\\E", "shortCiteRegEx": "Hester and Stone", "year": 2013}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "Kaelbling et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "Brain function and adaptive systems: a heterostatic theory, Technical report, DTIC Document", "author": ["A.H. Klopf"], "venue": null, "citeRegEx": "Klopf,? \\Q1972\\E", "shortCiteRegEx": "Klopf", "year": 1972}, {"title": "Learning motor skills: from algorithms to robot experiments", "author": ["J. Kober"], "venue": "it-Information Technology", "citeRegEx": "Kober,? \\Q2014\\E", "shortCiteRegEx": "Kober", "year": 2014}, {"title": "Reinforcement learning in robotics: A survey", "author": ["J. Kober", "J.A. Bagnell", "J. Peters"], "venue": "The International Journal of Robotics Research", "citeRegEx": "Kober et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kober et al\\.", "year": 2013}, {"title": "Reinforcement learning in robotics: Applications and real-world challenges", "author": ["P. Kormushev", "S. Calinon", "D.G. Caldwell"], "venue": "Robotics", "citeRegEx": "Kormushev et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kormushev et al\\.", "year": 2013}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": null, "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Reinforcement Learning for Robots Using Neural Networks, PhD thesis, Carnegie Mellon University. UMI Order GAX93-22750", "author": ["Lin", "L.-J"], "venue": null, "citeRegEx": "Lin and L..J.,? \\Q1992\\E", "shortCiteRegEx": "Lin and L..J.", "year": 1992}, {"title": "Exploration in model-based reinforcement learning by empirically estimating learning progress, in \u2018Advances in Neural Information", "author": ["M. Lopes", "T. Lang", "M. Toussaint", "Oudeyer", "P.-Y"], "venue": "Processing Systems\u2019,", "citeRegEx": "Lopes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lopes et al\\.", "year": 2012}, {"title": "Developmental robotics: a survey", "author": ["M. Lungarella", "G. Metta", "R. Pfeifer", "G. Sandini"], "venue": "Connection Science", "citeRegEx": "Lungarella et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lungarella et al\\.", "year": 2003}, {"title": "RL-ROBOT: Reinforcement learning framework for robotics", "author": ["A. Mart\u0301\u0131nez-Tenor"], "venue": null, "citeRegEx": "Mart\u0301\u0131nez.Tenor,? \\Q2016\\E", "shortCiteRegEx": "Mart\u0301\u0131nez.Tenor", "year": 2016}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Prioritized sweeping: Reinforcement learning with less data and less time", "author": ["A.W. Moore", "C.G. Atkeson"], "venue": "Machine Learning", "citeRegEx": "Moore and Atkeson,? \\Q1993\\E", "shortCiteRegEx": "Moore and Atkeson", "year": 1993}, {"title": "Dynamic bayesian networks: representation, inference and learning, PhD thesis, University of California, Berkeley", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "Murphy,? \\Q2002\\E", "shortCiteRegEx": "Murphy", "year": 2002}, {"title": "Real-world reinforcement learning for autonomous humanoid robot docking", "author": ["N. Navarro-Guerrero", "C. Weber", "P. Schroeter", "S. Wermter"], "venue": "Robotics and Autonomous Systems", "citeRegEx": "Navarro.Guerrero et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Navarro.Guerrero et al\\.", "year": 2012}, {"title": "Autonomous inverted helicopter flight via reinforcement learning, in \u2018Experimental Robotics IX", "author": ["A.Y. Ng", "A. Coates", "M. Diel", "V. Ganapathi", "J. Schulte", "B. Tse", "E. Berger", "E. Liang"], "venue": null, "citeRegEx": "Ng et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2006}, {"title": "Deep exploration via bootstrapped dqn", "author": ["I. Osband", "C. Blundell", "A. Pritzel", "B. Van Roy"], "venue": null, "citeRegEx": "Osband et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2016}, {"title": "Safe exploration techniques for reinforcement learning\u2013an overview, in \u2018International Workshop on Modelling and Simulation for Autonomous Systems", "author": ["M. Pecka", "T. Svoboda"], "venue": null, "citeRegEx": "Pecka and Svoboda,? \\Q2014\\E", "shortCiteRegEx": "Pecka and Svoboda", "year": 2014}, {"title": "Reinforcement learning for humanoid robotics", "author": ["J. Peters", "S. Vijayakumar", "S. Schaal"], "venue": "in \u2018Proceedings of the third IEEE-RAS International Conference on Humanoid Robots\u2019,", "citeRegEx": "Peters et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2003}, {"title": "ROS: an open-source robot operating system, in \u2018ICRA Workshop on Open Source Software", "author": ["M. Quigley", "K. Conley", "B.P. Gerkey", "J. Faust", "T. Foote", "J. Leibs", "R. Wheeler", "A.Y. Ng"], "venue": null, "citeRegEx": "Quigley et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quigley et al\\.", "year": 2009}, {"title": "V-rep: A versatile and scalable robot simulation framework, in \u20182013", "author": ["E. Rohmer", "S.P.N. Singh", "M. Freese"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems\u2019,", "citeRegEx": "Rohmer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rohmer et al\\.", "year": 2013}, {"title": "On-line Q-learning using connectionist systems, CUED/FINFENG/TR", "author": ["G.A. Rummery", "M. Niranjan"], "venue": null, "citeRegEx": "Rummery and Niranjan,? \\Q1994\\E", "shortCiteRegEx": "Rummery and Niranjan", "year": 1994}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": null, "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "A reinforcement learning method for maximizing undiscounted rewards", "author": ["A. Schwartz"], "venue": "in \u2018Proceedings of the tenth International Conference on Machine Learning\u2019,", "citeRegEx": "Schwartz,? \\Q1993\\E", "shortCiteRegEx": "Schwartz", "year": 1993}, {"title": "Towards deep developmental learning", "author": ["O. Sigaud", "A. Droniou"], "venue": "IEEE Transactions on Cognitive and Developmental Systems", "citeRegEx": "Sigaud and Droniou,? \\Q2016\\E", "shortCiteRegEx": "Sigaud and Droniou", "year": 2016}, {"title": "Reinforcement learning with replacing eligibility traces", "author": ["S.P. Singh", "R.S. Sutton"], "venue": "Machine Learning", "citeRegEx": "Singh and Sutton,? \\Q1996\\E", "shortCiteRegEx": "Singh and Sutton", "year": 1996}, {"title": "Effective reinforcement learning for mobile robots, in \u20182002", "author": ["W.D. Smart", "L.P. Kaelbling"], "venue": "IEEE International Conference on Robotics and Automation, (ICRA)\u2019,", "citeRegEx": "Smart and Kaelbling,? \\Q2002\\E", "shortCiteRegEx": "Smart and Kaelbling", "year": 2002}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["B.C. Stadie", "S. Levine", "P. Abbeel"], "venue": null, "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "Dyna-style planning with linear function approximation and prioritized sweeping", "author": ["R.S. Sutton", "C. Szepesv\u00e1ri", "A. Geramifard", "M.P. Bowling"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2012}, {"title": "Algorithms for reinforcement learning", "author": ["C. Szepesv\u00e1ri"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning", "citeRegEx": "Szepesv\u00e1ri,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri", "year": 2010}, {"title": "RL-Glue: Languageindependent software for reinforcement-learning experiments", "author": ["B. Tanner", "A. White"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Tanner and White,? \\Q2009\\E", "shortCiteRegEx": "Tanner and White", "year": 2009}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Taylor and Stone,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Stone", "year": 2009}, {"title": "Efficient exploration in reinforcement learning", "author": ["S.B. Thrun"], "venue": "Technical report,", "citeRegEx": "Thrun,? \\Q1992\\E", "shortCiteRegEx": "Thrun", "year": 1992}, {"title": "Value-difference based exploration: Adaptive control between epsilon-greedy and softmax., in \u2018KI", "author": ["M. Tokic", "G. Palm"], "venue": null, "citeRegEx": "Tokic and Palm,? \\Q2011\\E", "shortCiteRegEx": "Tokic and Palm", "year": 2011}, {"title": "Comparing individual means in the analysis of variance", "author": ["J.W. Tukey"], "venue": "Biometrics pp", "citeRegEx": "Tukey,? \\Q1949\\E", "shortCiteRegEx": "Tukey", "year": 1949}, {"title": "Computing machinery and intelligence", "author": ["A.M. Turing"], "venue": "Mind 59(236),", "citeRegEx": "Turing,? \\Q1950\\E", "shortCiteRegEx": "Turing", "year": 1950}, {"title": "True online temporal-difference learning", "author": ["H. van Seijen", "A.R. Mahmood", "P.M. Pilarski", "M.C. Machado", "R.S. Sutton"], "venue": null, "citeRegEx": "Seijen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Seijen et al\\.", "year": 2015}, {"title": "Learning on real robots from experience and simple user feedback", "author": ["P.Q. Vidal", "R.I. Rod\u0155\u0131guez", "M.A.R. Gonz\u00e1lez", "C.V. Regueiro"], "venue": "JoPha: Journal of Physical Agents", "citeRegEx": "Vidal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vidal et al\\.", "year": 2013}, {"title": "Learning from delayed rewards, PhD thesis, King\u2019s College, Cambridge", "author": ["Watkins", "C.J.C. H"], "venue": null, "citeRegEx": "Watkins and H.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and H.", "year": 1989}, {"title": "Q-learning behavior on autonomous navigation of physical robot, in \u20188th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)", "author": ["H. Wicaksono"], "venue": null, "citeRegEx": "Wicaksono,? \\Q2011\\E", "shortCiteRegEx": "Wicaksono", "year": 2011}, {"title": "Explorations in efficient reinforcement learning", "author": ["M.A. Wiering"], "venue": "PhD thesis,", "citeRegEx": "Wiering,? \\Q1999\\E", "shortCiteRegEx": "Wiering", "year": 1999}, {"title": "Reinforcement learning: State-of-the-art\u2019, Adaptation, Learning, and Optimization", "author": ["M. Wiering", "M. Van Otterlo"], "venue": null, "citeRegEx": "Wiering and Otterlo,? \\Q2012\\E", "shortCiteRegEx": "Wiering and Otterlo", "year": 2012}, {"title": "Reinforcement learning algorithms for robotic navigation in dynamic environments", "author": ["G. Yen", "T. Hickey"], "venue": "in \u2018Proceedings of the 2002 International Joint Conference on Neural Networks, 2002 (IJCNN)\u2019,", "citeRegEx": "Yen and Hickey,? \\Q2002\\E", "shortCiteRegEx": "Yen and Hickey", "year": 2002}, {"title": "Towards vision-based deep reinforcement learning for robotic motion control", "author": ["F. Zhang", "J. Leitner", "M. Milford", "B. Upcroft", "P. Corke"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 52, "context": "The autonomous learning concept in robotics dates back to Alan Turing\u2019s idea of having robots that learn in a way similar to children (Turing 1950).", "startOffset": 134, "endOffset": 147}, {"referenceID": 30, "context": "Markov decision processes (MDP) (Murphy 2002) are the most exploited paradigm in this sense.", "startOffset": 32, "endOffset": 45}, {"referenceID": 4, "context": "With those definitions, Bellman equations (Bellman 1956) can be used recursively for improving an arbitrary initial policy until it converges to the optimal one (the one with greatest value).", "startOffset": 42, "endOffset": 56}, {"referenceID": 18, "context": "Another mechanism widely used for improving RL is eligibility traces (ET) (Klopf 1972), (Kaelbling, Littman & Moore 1996), (Sutton & Barto 1998) and (Lin 1992), and its variant replacing traces (Singh & Sutton 1996).", "startOffset": 74, "endOffset": 86}, {"referenceID": 40, "context": "So far the algorithms mentioned are derived from the classical value iteration, but there are other effective TDbased algorithms, such as actor-critic methods (Barto, Sutton & Anderson 1983), R-learning (Schwartz 1993), and those based on policy iteration.", "startOffset": 203, "endOffset": 218}, {"referenceID": 17, "context": "(Kaelbling et al. 1996) and (Wiering & Van Otterlo 2012).", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "The problem worsens when trying to solve different tasks (Kaelbling et al. 1996), (Sutton & Barto 1998), (Szepesv\u00e1ri 2010) and (Wiering & Van Otterlo 2012).", "startOffset": 57, "endOffset": 80}, {"referenceID": 46, "context": "1996), (Sutton & Barto 1998), (Szepesv\u00e1ri 2010) and (Wiering & Van Otterlo 2012).", "startOffset": 30, "endOffset": 47}, {"referenceID": 49, "context": "An adequate approach to the exploration-exploitation dilemma (Thrun 1992), (Wiering 1999), (Tokic & Palm 2011), (Lopes, Lang, Toussaint & Oudeyer 2012), (Hester, Lopes & Stone 2013) and (Pecka & Svoboda 2014) is also proved to be essential for the performance of the learning process, especially for practical applications.", "startOffset": 61, "endOffset": 73}, {"referenceID": 57, "context": "An adequate approach to the exploration-exploitation dilemma (Thrun 1992), (Wiering 1999), (Tokic & Palm 2011), (Lopes, Lang, Toussaint & Oudeyer 2012), (Hester, Lopes & Stone 2013) and (Pecka & Svoboda 2014) is also proved to be essential for the performance of the learning process, especially for practical applications.", "startOffset": 75, "endOffset": 89}, {"referenceID": 17, "context": "Today the efficacy of RL algorithms has been demonstrated in many different fields, such as game theory, control engineering, statistics, or even robotics when toy models or very low-realistic robot simulators are used (Sutton & Barto 1998), (Kaelbling et al. 1996), (Wiering & Van Otterlo 2012) and (Kober et al.", "startOffset": 242, "endOffset": 265}, {"referenceID": 20, "context": "1996), (Wiering & Van Otterlo 2012) and (Kober et al. 2013).", "startOffset": 40, "endOffset": 59}, {"referenceID": 20, "context": "However, when RL is applied to accurate physical realistic simulators or to real robots, we must face additional issues that are specific of this area, such as the curse of real-world samples (Kober et al. 2013), which obstruct the execution of RL-based methods, often making them unviable.", "startOffset": 192, "endOffset": 211}, {"referenceID": 12, "context": "Despite these limitations, many works have implemented RL-based methods for learning complex but particular robotic tasks, overcoming some of these problems (Lin 1992), (Smart & Kaelbling 2002), (Gaskett 2002), (Abbeel, Coates & Ng 2010), (Yen & Hickey 2002), (Peters, Vijayakumar & Schaal 2003), (Ng, Coates, Diel, Ganapathi, Schulte, Tse, Berger & Liang 2006), (Wicaksono 2011), (Degris, Pilarski & Sutton 2012), (Hester, Quinlan & Stone 2012), (Navarro-Guerrero, Weber, Schroeter & Wermter 2012), (Hester & Stone 2013), (Kober et al.", "startOffset": 195, "endOffset": 209}, {"referenceID": 56, "context": "Despite these limitations, many works have implemented RL-based methods for learning complex but particular robotic tasks, overcoming some of these problems (Lin 1992), (Smart & Kaelbling 2002), (Gaskett 2002), (Abbeel, Coates & Ng 2010), (Yen & Hickey 2002), (Peters, Vijayakumar & Schaal 2003), (Ng, Coates, Diel, Ganapathi, Schulte, Tse, Berger & Liang 2006), (Wicaksono 2011), (Degris, Pilarski & Sutton 2012), (Hester, Quinlan & Stone 2012), (Navarro-Guerrero, Weber, Schroeter & Wermter 2012), (Hester & Stone 2013), (Kober et al.", "startOffset": 363, "endOffset": 379}, {"referenceID": 20, "context": "Despite these limitations, many works have implemented RL-based methods for learning complex but particular robotic tasks, overcoming some of these problems (Lin 1992), (Smart & Kaelbling 2002), (Gaskett 2002), (Abbeel, Coates & Ng 2010), (Yen & Hickey 2002), (Peters, Vijayakumar & Schaal 2003), (Ng, Coates, Diel, Ganapathi, Schulte, Tse, Berger & Liang 2006), (Wicaksono 2011), (Degris, Pilarski & Sutton 2012), (Hester, Quinlan & Stone 2012), (Navarro-Guerrero, Weber, Schroeter & Wermter 2012), (Hester & Stone 2013), (Kober et al. 2013), (Kormushev, Calinon & Caldwell 2013), (Vidal, Rod\u0155\u0131guez, Gonz\u00e1lez & Regueiro 2013), (Kober 2014), (Deisenroth, Fox & Rasmussen 2015) and (Gar\u0107\u0131a & Fern\u00e1ndez 2015).", "startOffset": 523, "endOffset": 542}, {"referenceID": 19, "context": "2013), (Kormushev, Calinon & Caldwell 2013), (Vidal, Rod\u0155\u0131guez, Gonz\u00e1lez & Regueiro 2013), (Kober 2014), (Deisenroth, Fox & Rasmussen 2015) and (Gar\u0107\u0131a & Fern\u00e1ndez 2015).", "startOffset": 91, "endOffset": 103}, {"referenceID": 20, "context": "The main conclusion to be drawn from this is that the complexity of RL in robotics requires the learning method to be complemented through advanced representation, prior knowledge, and models in order to make the problem tractable (Kober et al. 2013); this biases the learning process with knowledge from the task, resulting in a task-dependent learning method.", "startOffset": 231, "endOffset": 250}, {"referenceID": 19, "context": "Although some studies have searched for techniques adaptable to multiple tasks (Kober 2014), they usually rely on closely related ones, sometimes leading to poor performance.", "startOffset": 79, "endOffset": 91}, {"referenceID": 26, "context": "RL-ROBOT has been released as an open source project, Python PEP8 style, available on Github (Mart\u0301\u0131nez-Tenor 2016).", "startOffset": 93, "endOffset": 115}, {"referenceID": 51, "context": "The results include the average learning curve of several repetitions, along with an analysis of variance and a post-hoc Tukey test (Tukey 1949) for each task in order to assess the significance of the conclusions.", "startOffset": 132, "endOffset": 144}, {"referenceID": 6, "context": "A suitable generalization exploiting the basis of QBIASSR could be hierarchical RL (Barto & Mahadevan 2003) and (Botvinick 2012), where complex tasks could be formed by adding more input variables to existing tasks.", "startOffset": 112, "endOffset": 128}], "year": 2017, "abstractText": "Mobile robots are increasingly being employed for performing complex tasks in dynamic environments. Reinforcement learning (RL) methods are recognized to be promising for specifying such tasks in a relatively simple manner. However, the strong dependency between the learning method and the task to learn is a well-known problem that restricts practical implementations of RL in robotics, often requiring major modifications of parameters and adding other techniques for each particular task. In this paper we present a practical core implementation of RL which enables the learning process for multiple robotic tasks with minimal per-task tuning or none. Based on value iteration methods, this implementation includes a novel approach for action selection, called Q-biased softmax regression (QBIASSR), which avoids poor performance of the learning process when the robot reaches new unexplored states. Our approach takes advantage of the structure of the state space by attending the physical variables involved (e.g., distances to obstacles, X ,Y , \u03b8 pose, etc.), thus experienced sets of states may favor the decision-making process of unexplored or rarely-explored states. This improvement has a relevant role in reducing the tuning of the algorithm for particular tasks. Experiments with real and simulated robots, performed with the software framework also introduced here, show that our implementation is effectively able to learn different robotic tasks without tuning the learning method. Results also suggest that the combination of true online SARSA(\u03bb) (TOSL) with QBIASSR can outperform the existing RL core algorithms in low-dimensional robotic tasks.", "creator": "LaTeX with hyperref package"}}}