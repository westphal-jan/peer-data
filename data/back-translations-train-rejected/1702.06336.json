{"id": "1702.06336", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Hybrid Dialog State Tracker with ASR Features", "abstract": "This paper presents a hybrid dialog state tracker enhanced by trainable Spoken Language Understanding (SLU) for slot-filling dialog systems. Our architecture is inspired by previously proposed neural-network-based belief-tracking systems. In addition, we extended some parts of our modular architecture with differentiable rules to allow end-to-end training. We hypothesize that these rules allow our tracker to generalize better than pure machine-learning based systems. For evaluation, we used the Dialog State Tracking Challenge (DSTC) 2 dataset - a popular belief tracking testbed with dialogs from restaurant information system. To our knowledge, our hybrid tracker sets a new state-of-the-art result in three out of four categories within the DSTC2.", "histories": [["v1", "Tue, 21 Feb 2017 11:34:14 GMT  (337kb,D)", "http://arxiv.org/abs/1702.06336v1", "Accepted to EACL 2017"]], "COMMENTS": "Accepted to EACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["miroslav vodol\\'an", "rudolf kadlec", "jan kleindienst"], "accepted": false, "id": "1702.06336"}, "pdf": {"name": "1702.06336.pdf", "metadata": {"source": "CRF", "title": "Hybrid Dialog State Tracker with ASR Features", "authors": ["Miroslav Vodol\u00e1n", "Rudolf Kadlec", "Jan Kleindienst"], "emails": ["jankle}@cz.ibm.com", "vodolan@ufal.mff.cuni.cz"], "sections": [{"heading": "1 Introduction", "text": "A faith-state tracker is an important component of dialog systems whose responsibility is to predict the user's goals based on the history of the dialog. Faith-state tracker has been extensively investigated in the Dialog State Tracking Challenge (DSTC) series (Williams et al., 2016) by providing common test environments for different tracking approaches. The DSTC abstracts the subsystems from end-to-end dialog systems and focuses only on dialog-state tracking by issuing data sets of automatic speech recognition (ASR) and language understanding (SLU) with reference transcriptions, along with annotations at the level of dialog files and user attainment on slot fill tasks where the dialog system attempts to fill predefined slots with values from a known ontology (e.g. moderate value for apricerange slot). In this work, we are improving the state of the D2 D4 architecture by successfully integrating two ST4al ideas (D2)."}, {"heading": "2 Hybrid dialog state tracker model", "text": "The tracker works separately on the probability distribution for each slot. Each round, the tracker generates these distributions to reflect the user's goals based on the last action of the machine, the observed user actions, the probability distributions from the previous round and an internal hidden state. The probability distribution hst [v] is a distribution of all possible values v from the domain of the sot1. For historical reasons, we have adopted the hand-coded rule term used throughout the faith community. From another point of view, our rules can be considered linear combination techniques. ar Xiv: 170 2.06 336v 1 [cs.C L] 21 Feb 2017s in the turn dialog. The common faith state is represented by a probability distribution across the individual slot domains. In the following notation, a user action is designated which is transformed into a probability distribution of informed values before processing."}, {"heading": "2.1 Rule-based part", "text": "The rules-based part of our tracker, inspired by Vodola \u0301 n et al. (2015), is specified by a function R (hst \u2212 1, u s t, a) = h s t, which is a function of a slot value probability distribution hst \u2212 1 in the previous turn, the output of a traceable SLU, and transition coefficients a, which determine how the new faith is calculated. The first equation specifies the faith update rule for the probability assigned to the slot value vi: hst [vi] = h s \u2212 1 [vj] \u2212 1 [vj] to other slot values in hst. This is calculated as: h-st [vi] = vh-vst \u2212 1 [vj] \u00b7 avivj (1), where h [vi] expresses how much probability is transferred from hst \u2212 1 [vj] \u2212 1 [vj] to other slot values in hst. This is called as: h-st [vi] = vst-vst \u2212 1 function \u2212 vj-j-vj-vi (efficiency in vj-vst), where vi-vj-vj-efficiency is called (vj-vj-j-vi)."}, {"heading": "2.2 Machine-learned part", "text": "It is not in such a way as if it were about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about which it is about a way in which it is about a way in which it is about which it is about a way in which it is about a way in which it is about a way in which it is not in which it is not in which it is about which it is not in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is not in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it"}, {"heading": "3 Evaluation", "text": "From each dialog in the dstc2 train data (1612 dialogs), we extracted training samples for the Food, pricerange and area slots and used them all to train each tracker. The development data dstc2 dev (506 dialogs) was used to select the ft and fv features. We took the 2000 most common ft features and the 100 most common fv features and the 100 most common fv features. The costs we optimized consisted of tracking costs that are calculated as cross entropy between a belief state and a target rating, and an SLU contribution that includes a cross entropy between the output of the SLU ust and a semantic annotation. We did not use regulation on model parameters. We trained the model for 30 epochs of SGD with the AdaDelta (pointer, 2012) weighted date rule and batch size fully rolled out on 16 dialogues."}, {"heading": "4 Lessons learned", "text": "Originally, we designed the special SLU unit M with sigmoid activation inspired by the architecture of (Henderson et al., 2014b), but we found it difficult to train it because the gradients were poorly distributed through this layer, making its output resemble precursors of ontology values rather than the likelihood of determining an ontology value based on appropriate ASR hypotheses, as the network hierarchy suggests. The problem resulted in the inability to learn alternative formulations of ontology values, which are often present in the training data. One such example may be \"Asian food,\" which appears 16 times in the training data as part of the best ASR hypothesis, while 13 times it really informs about the \"Asian oriental\" ontology value. Measurements on dstc2 dev have shown 4validation to find the weights alone. That the SLU was unable to detect this alias at any time."}, {"heading": "5 Conclusion", "text": "We have presented a fully traceable belief tracker, whose modular architecture is complemented by differentiated rules. Our tracker's modular architecture outperforms other approaches in almost all common DSTC categories without major modifications, making our tracker successful in a variety of input feature settings."}, {"heading": "Acknowledgments", "text": "This work was supported by the GAUK scholarship 1170516 of Charles University in Prague."}], "references": [{"title": "The second dialog state tracking challenge", "author": ["Blaise Thomson", "Jason D. Williams"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dia-", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Knowledge-based dialog state tracking", "author": ["Kadlec et al.2014] Rudolf Kadlec", "Miroslav Vodolan", "Jindrich Libovicky", "Jan Macek", "Jan Kleindienst"], "venue": "In Spoken Language Technology Workshop (SLT),", "citeRegEx": "Kadlec et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2014}, {"title": "Task lineages: Dialog state tracking for flexible interaction", "author": ["Lee", "Stent2016] Sungjin Lee", "Amanda Stent"], "venue": "In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Optimizing generative dialog state tracker via cascading gradient descent", "author": ["Lee et al.2014] Byung-Jun Lee", "Woosang Lim", "Daejoong Kim", "Kee-Eung Kim"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dia-", "citeRegEx": "Lee et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2014}, {"title": "Neural belief tracker: Data-driven dialogue state tracking. arXiv preprint arXiv:1606.03777", "author": ["Mrk\u0161i\u0107 et al.2016] Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Tsung-Hsien Wen", "Blaise Thomson", "Steve Young"], "venue": null, "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2016}, {"title": "Markovian discriminative modeling for dialog state tracking", "author": ["Ren et al.2014] Hang Ren", "Weiqun Xu", "Yonghong Yan"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),", "citeRegEx": "Ren et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2014}, {"title": "Comparative error analysis of dialog state tracking", "author": ["Ronnie Smith"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),", "citeRegEx": "Smith.,? \\Q2014\\E", "shortCiteRegEx": "Smith.", "year": 2014}, {"title": "The sjtu system for dialog state tracking challenge", "author": ["Sun et al.2014] Kai Sun", "Lu Chen", "Su Zhu", "Kai Yu"], "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),", "citeRegEx": "Sun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Hybrid dialog state tracker", "author": ["Rudolf Kadlec", "Jan Kleindienst"], "venue": "In Proceedings of NIPS 2015 Workshop on Machine Learning for Spoken Language Understanding and Interaction,", "citeRegEx": "Vodol\u00e1n et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vodol\u00e1n et al\\.", "year": 2015}, {"title": "The dialog state tracking challenge series: A review", "author": ["Antoine Raux", "Matthew Henderson"], "venue": "Dialogue & Discourse,", "citeRegEx": "Williams et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2016}, {"title": "Web-style ranking and slu combination for dialog state tracking", "author": ["Jason D. Williams"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),", "citeRegEx": "Williams.,? \\Q2014\\E", "shortCiteRegEx": "Williams.", "year": 2014}, {"title": "Constrained Markov Bayesian Polynomial for Efficient Dialogue State Tracking", "author": ["Yu et al.2015] Kai Yu", "Kai Sun", "Lu Chen", "Su Zhu"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "Belief-state tracking was extensively studied in the Dialog State Tracking Challenge (DSTC) series (Williams et al., 2016) by providing shared testbed for various tracking approaches.", "startOffset": 99, "endOffset": 122}, {"referenceID": 0, "context": "In this work we improve state-of-the-art results on DSTC2 (Henderson et al., 2014a) by combining two central ideas previously proposed in different successful models: 1) machine learning core with hand-coded1 rules, an idea already explored by Yu et al. (2015) and Vodol\u00e1n et al.", "startOffset": 59, "endOffset": 261}, {"referenceID": 0, "context": "In this work we improve state-of-the-art results on DSTC2 (Henderson et al., 2014a) by combining two central ideas previously proposed in different successful models: 1) machine learning core with hand-coded1 rules, an idea already explored by Yu et al. (2015) and Vodol\u00e1n et al. (2015) with 2) a complex neural network based architecture that processes ASR features proposed by Henderson et al.", "startOffset": 59, "endOffset": 287}, {"referenceID": 0, "context": "In this work we improve state-of-the-art results on DSTC2 (Henderson et al., 2014a) by combining two central ideas previously proposed in different successful models: 1) machine learning core with hand-coded1 rules, an idea already explored by Yu et al. (2015) and Vodol\u00e1n et al. (2015) with 2) a complex neural network based architecture that processes ASR features proposed by Henderson et al. (2014b). Their network consist of two main units.", "startOffset": 59, "endOffset": 404}, {"referenceID": 0, "context": "When compared to Henderson et al. (2014b) that inspired our work: 1) our model does not require auto-encoder pre-training and shared initial training on all slots which makes the training easier; 2) our approach combines a rule-based core of the tracker and RNNs while their model used only RNNs; 3) we use different NN architecture to process SLU features.", "startOffset": 17, "endOffset": 42}, {"referenceID": 12, "context": "To make our system comparable to the best-performing tracker (Williams, 2014) we also included features from batch ASR (recognition hypotheses and the unigram word-confusion matrix).", "startOffset": 61, "endOffset": 77}, {"referenceID": 0, "context": "The same approach is used in Henderson et al. (2014b). To make our system comparable to the best-performing tracker (Williams, 2014) we also included features from batch ASR (recognition hypotheses and the unigram word-confusion matrix).", "startOffset": 29, "endOffset": 54}, {"referenceID": 0, "context": "The same approach is used in Henderson et al. (2014b). To make our system comparable to the best-performing tracker (Williams, 2014) we also included features from batch ASR (recognition hypotheses and the unigram word-confusion matrix). The batch ASR hypotheses are encoded in the same way as hypotheses from the regular ASR. The confusion matrix information is encoded as weighted unigrams. The last part of the turn features encodes machine-action dialog acts. We are using trigram-like encoding dialogact-slot-value with weight 1.0. The other features are value features fvi created from turn features, which contain occurrence of vi, by replacing occurrence of the value vi and slot name s by a common tag (inform-food-italian\u2192 inform-<slot>-<value>). This technique is called delexicalization by Henderson et al. (2014b).", "startOffset": 29, "endOffset": 827}, {"referenceID": 10, "context": "The rule-based part of our tracker, inspired by Vodol\u00e1n et al. (2015), is specified by a function R(ht\u22121, u s t , a) = h s t , which is a function of a slot\u2013 value probability distribution ht\u22121 in the previous turn, the output ut of a trainable SLU and of transition coefficients a which control how the new belief ht is computed.", "startOffset": 48, "endOffset": 70}, {"referenceID": 3, "context": "These coefficients were modelled by a so called durability function in Kadlec et al. (2014).", "startOffset": 71, "endOffset": 92}, {"referenceID": 10, "context": "The machine-learned part modulates behavior of the rule-based part R by transition coefficients avivj that control the amount of probability which is transferred from ht\u22121[vj ] to h s t [vi] as in Vodol\u00e1n et al. (2015). However, our computation of the coefficients involves two different functions:", "startOffset": 197, "endOffset": 219}, {"referenceID": 0, "context": "The SLU part of the tracker shown in Figure 2 is inspired by an architecture, proposed in Henderson et al. (2014b), consisting of two separate units.", "startOffset": 90, "endOffset": 115}, {"referenceID": 14, "context": "We trained the model for 30 epochs by SGD with the AdaDelta (Zeiler, 2012) weightupdate rule and batch size 16 on fully unrolled dialogs.", "startOffset": 60, "endOffset": 74}, {"referenceID": 0, "context": "This setup was proposed in Henderson et al. (2014a) where an ensemble was trained from all DSTC2 submissions.", "startOffset": 27, "endOffset": 52}, {"referenceID": 6, "context": "416 Neural Belief Tracker (Mrk\u0161i\u0107 et al., 2016) \u221a .", "startOffset": 26, "endOffset": 47}, {"referenceID": 3, "context": "406 Knowledge-based tracker (Kadlec et al., 2014) .", "startOffset": 28, "endOffset": 49}, {"referenceID": 0, "context": "721 Henderson et al. (2014b) .", "startOffset": 4, "endOffset": 29}, {"referenceID": 6, "context": "433 Smith (2014) .", "startOffset": 4, "endOffset": 17}, {"referenceID": 4, "context": "452 Lee et al. (2014) .", "startOffset": 4, "endOffset": 22}], "year": 2017, "abstractText": "This paper presents a hybrid dialog state tracker enhanced by trainable Spoken Language Understanding (SLU) for slotfilling dialog systems. Our architecture is inspired by previously proposed neuralnetwork-based belief-tracking systems. In addition we extended some parts of our modular architecture with differentiable rules to allow end-to-end training. We hypothesize that these rules allow our tracker to generalize better than pure machinelearning based systems. For evaluation we used the Dialog State Tracking Challenge (DSTC) 2 dataset a popular belief tracking testbed with dialogs from restaurant information system. To our knowledge, our hybrid tracker sets a new stateof-the-art result in three out of four categories within the DSTC2.", "creator": "LaTeX with hyperref package"}}}