{"id": "1702.06378", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Multitask Learning with CTC and Segmental CRF for Speech Recognition", "abstract": "Segmental conditional random fields (SCRFs) and connectionist temporal classification (CTC) are two sequence labeling objectives used for end-to-end training of speech recognition models. Both models define the transcription probability by marginalizing decisions about latent segmentation alternatives to derive a sequence probability: the former uses a globally normalized joint model of segment labels and durations, and the latter classifies each frame as either an output symbol or a \"continuation\" of the previous label. In this paper, we train a recognition model by optimizing an interpolation between the SCRF and CTC losses, where the same recurrent neural network (RNN) encoder used for feature extraction for both outputs. We find that this multi-task objective improves recognition accuracy when decoding with either the SCRF or CTC models. Additionally, we show that CTC can also be used to pretrain the RNN encoder, which improves the convergence rate when learning the joint model.", "histories": [["v1", "Tue, 21 Feb 2017 13:39:35 GMT  (144kb,D)", "http://arxiv.org/abs/1702.06378v1", "5 pages, 2 figures, submitted to Interspeech 2017"], ["v2", "Mon, 6 Mar 2017 02:40:45 GMT  (144kb,D)", "http://arxiv.org/abs/1702.06378v2", "5 pages, 2 figures, submitted to Interspeech 2017"], ["v3", "Thu, 23 Mar 2017 20:42:54 GMT  (144kb,D)", "http://arxiv.org/abs/1702.06378v3", "5 pages, 2 figures, submitted to Interspeech 2017"], ["v4", "Mon, 5 Jun 2017 18:19:34 GMT  (144kb,D)", "http://arxiv.org/abs/1702.06378v4", "5 pages, 2 figures, camera ready version at Interspeech 2017"]], "COMMENTS": "5 pages, 2 figures, submitted to Interspeech 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["liang lu", "lingpeng kong", "chris dyer", "noah a smith"], "accepted": false, "id": "1702.06378"}, "pdf": {"name": "1702.06378.pdf", "metadata": {"source": "CRF", "title": "Multi-task Learning with CTC and Segmental CRF for Speech Recognition", "authors": ["Liang Lu", "Lingpeng Kong", "Chris Dyer", "Noah A. Smith"], "emails": ["llu@ttic.edu,", "lingpenk@cs.cmu.edu,", "cdyer@google.com,", "nasmith@cs.washington.edu"], "sections": [{"heading": "1. Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2. Segmental Conditional Random Fields", "text": "SCRF is a variant of the linear chain model CRF, where each output character corresponds to a segment of input marks instead of a single input instance. In the context of speech recognition, for a sequence of input vectors of T-frames X = {x1, \u00b7 \u00b7 \u00b7, xT} and their corresponding sequence of output marks Xiv: 170 2.06 378v 1 [cs.C L] 21 Feb 2017y = {y1, \u00b7 \u00b7 \u00b7, yJ} the linear chain CRF defines the sequence level of conditional probabilities asP (y | X) = 1 Z (X)."}, {"heading": "2.1. Feature Function and Acoustic Embedding", "text": "SRNN uses an RNN to learn segment level and create a hidden state in which we simply hide the hidden vector. Given the input sequence X = > cost of multiple memory function X = {x1, \u00b7 \u00b7 \u00b7 \u00b7, xT}, and we need to embed vector x-j in Eq. (4) Matches the segment ej = < sj, nj >. Since the segment boundaries are known, it is straightforward to use an RNN to designate the segment in a vector ashsj hsj + 1... hnj = RNN (h0, xsj) RNN (hsj) RNN (hsj + 1)... RNN (hnj \u2212 1, xnj) where h0 denotes the initial hidden state that is initialized to be zero. RNN (\u00b7) denotes the non-linear recurence operation that is used in a previous RNN and the input VN."}, {"heading": "2.2. Loss Function", "text": "In this case, we cannot train the model directly by maximizing the conditional probability as equality. (2) However, the problem can be solved by marginalizing the segmentation variable asLscrf = \u2212 logP (y | X) = \u2212 logP (y, E | X) = \u2212 log \u2211 E-jexp f (yj, ej, x-j). (10) where Z (X, y) denotes the sum of all possible segments when only y is observed. To simplify the notations, the objective function Lscrf is defined with only one training expression. However, the number of possible segmentations is exponential with the length of X, making the naive calculation of both Z (X, y) and Z (X) impractical. To solve this problem, dynamic programming can be reduced to the length of T (2)."}, {"heading": "3. Connectionist Temporal Classification", "text": "CTC also directly calculates the conditional probability of P (y | X), the main difference from SCRF being that it normalizes the probable distribution at the pro-frame level. To solve the problem of length mismatch between input and output sequences, CTC allows repetitions of output labels and introduces a special blank token (\u2212) that represents the probability of not issuing a valid label at a given time. Conditional probability is obtained by adding all probabilities of all paths matching y after the repeated labels have been merged and the blank tokens removed, e.g. P (y | X) P (\u03c0 | X), (11), where \u0435 (y) denotes the set of all possible paths corresponding to y after repetitions of labels and blank brand inserts."}, {"heading": "4. Joint Training Loss", "text": "We can simply interpolate the CTC and SCRF loss functions asL = \u03bbLctc + (1 \u2212 \u03bb) Lscrf, (14), with the total weight of the interpolation being \u03bb [0, 1]. In this paper, we focus on short-term memory RNN (LSTM) [23] for feature extraction. Other types of neural architecture, such as the Convolutionary Neural Network (CNN) or combinations of CNN and RNN, can be considered for future work."}, {"heading": "5. Experiments", "text": "Our experiments were conducted on the TIMIT database, and both the SRNN and CTC models were implemented with the DyNet toolkit [24]. We followed the standard protocol of the TIMIT dataset, and our experiments were based on the Kaldi recipe [25]. We used the core test set as our assessment set, which contains 192 statements. Our models were trained with 48 phonemes and their predictions were converted into 39 phonemes before the assessment. The dimension of uj was set to 64, and the dimension of w in Equation (4) is also 64. We set the initial SGD learning rate to 0.1, and we decreased the learning rate exponentially by 0.75 when the validation error stopped falling."}, {"heading": "5.1. Baseline Results", "text": "Table 1 shows the basic results of SRNN and CTC models with two different types of characteristics: The FBANK characteristics are 120-dimensional with delta and delta delta coefficients, and the fMLLR characteristics are 40-dimensional derived from a Kaldi base system. We used 3-layer bidirectional LSTMs for feature extraction, and we used the greedily best path decoding algorithm for both models. Our SRNN and CTC achieved a comparable telephone error rate (PER) for both types of offfeatures. However, in the CTC system, Graves et al. [28] achieved a better result with approximately the same size of the neural network (3 hidden layers with 250 hidden units of bidirectional LSTMs) than ours (18.6% vs. 19.9%). Aside from the implementation difference in using different code bases, our decoding algorithm has the best gradient applied to the search algorithms [28]."}, {"heading": "5.2. Multi-task Learning Results", "text": "Table 2 shows the results of multi-task learning for CTC and SRNN using the interpolated loss as Eq. (14). We only show results of using LSTMs with 250 dimensional hidden states. Interpolation weight was set at 0.5. In our experiments, the adjustment of interpolation weight did not further improve detection accuracy. Table 2 shows that multi-task learning improves detection accuracy of both SRNN and CTC acoustic models, possibly due to the regulating effect of joint training loss. Improving the FBANK function is relatively much greater than the features of fMLLR. Especially in multi-task learning, the detection accuracy of our CTC system with the best path coding is comparable to the results obtained by Graves et al. [28] with beamline decoding is one of the main components of the SCF's high computing costs."}, {"heading": "6. Conclusion", "text": "In this paper, we examined multi-task learning with CTC and SCRF for speech recognition. Using an RNN encoder for feature extraction, both CTC and SCRF can be trained end-to-end, and the two models can be trained together by interpolating the two loss functions. Using experiments on the TIMIT dataset, the multi-task learning approach improved the detection accuracy of both CTC and SCRF acoustic models. We also demonstrated that CTC can be used to pre-train the RNN encoder and accelerate the training of the common model. In the future, we will explore the multi-task learning approach for larger speech recognition tasks, where the CTC pre-training approach may be helpful in overcoming the problem of high computing costs."}, {"heading": "7. Acknowledgements", "text": "We thank NVIDIA Corporation for donating a Titan X GPU."}, {"heading": "8. References", "text": "[1] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury L., 2012. [2] F. Seide, G. Li, and D. Yu, \"Conversational speech transcription using context-dependent deep neural networks,\" in Interspeech, 2011, pp. 437-440. [3] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig, Achieving human parity in talking speech recognition, \"arXiv preprint arXiv: 1610.05256, 2016."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Conversational speech transcription using context-dependent deep neural networks.", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "in Interspeech,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Achieving human parity in conversational speech recognition", "author": ["W. Xiong", "J. Droppo", "X. Huang", "F. Seide", "M. Seltzer", "A. Stolcke", "D. Yu", "G. Zweig"], "venue": "arXiv preprint arXiv:1610.05256, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition", "author": ["H. Soltau", "H. Liao", "H. Sak"], "venue": "arXiv preprint arXiv:1610.09975, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "Proc. ICML, 2014, pp. 1764\u2013 1772.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Speech: Scaling up end-to-end speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger"], "venue": "arXiv preprint arXiv:1412.5567, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning acoustic frame labeling for speech recognition with recurrent neural networks", "author": ["H. Sak", "A. Senior", "K. Rao", "O. Irsoy", "A. Graves", "F. Beaufays", "J. Schalkwyk"], "venue": "Proc. ICASSP. IEEE, 2015, pp. 4280\u20134284.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Y. Miao", "M. Gowayyed", "F. Metze"], "venue": "Proc. ASRU. IEEE, 2015, pp. 167\u2013174.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Attention-based models for speech recognition", "author": ["J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 577\u2013 585.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "A study of the recurrent neural network encoder-decoder for large vocabulary speech recognition", "author": ["L. Lu", "X. Zhang", "K. Cho", "S. Renals"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["W. Chan", "N. Jaitly", "Q. Le", "O. Vinyals"], "venue": "Proc. ICASSP. IEEE, 2016, pp. 4960\u2013 4964.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep segmental neural networks for speech recognition.", "author": ["O. Abdel-Hamid", "L. Deng", "D. Yu", "H. Jiang"], "venue": "in Proc. INTER- SPEECH,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Segmental recurrent neural networks for end-to-end speech recognition", "author": ["L. Lu", "L. Kong", "C. Dyer", "N. Smith", "S. Renals"], "venue": "Proc. INTERSPEECH, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["H. Sak", "A. Senior", "K. Rao", "F. Beaufays"], "venue": "Proc. INTERSPEECH, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Lower frame rate neural network acoustic models", "author": ["G. Pundak", "T.N. Sainath"], "venue": "Proc. INTERSPEECH, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint ctc-attention based endto-end speech recognition using multi-task learning", "author": ["S. Kim", "T. Hori", "S. Watanabe"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Error back propagation for sequence training of context-dependent deep networks for conversational speech transcription", "author": ["H. Su", "G. Li", "D. Yu", "F. Seide"], "venue": "Proc. ICASSP. IEEE, 2013, pp. 6664\u20136668.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Speech recognitionwith segmental conditional random fields: A summary of the jhu clsp 2010 summer workshop", "author": ["G. Zweig", "P. Nguyen", "D. Van Compernolle", "K. Demuynck", "L. Atlas", "P. Clark", "G. Sell", "M. Wang", "F. Sha", "H. Hermansky"], "venue": "Proc. ICASSP. IEEE, 2011, pp. 5044\u20135047.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Conditional random fields in speech, audio, and language processing", "author": ["E. Fosler-Lussier", "Y. He", "P. Jyothi", "R. Prabhavalkar"], "venue": "Proceedings of the IEEE, vol. 101, no. 5, pp. 1054\u20131075, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Segmental recurrent neural networks", "author": ["L. Kong", "C. Dyer", "N.A. Smith"], "venue": "Proc. ICLR, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-markov conditional random fields for information extraction.", "author": ["S. Sarawagi", "W.W. Cohen"], "venue": "in Proc. NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proc. ICML. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "DyNet: The Dynamic Neural Network Toolkit", "author": ["G. Neubig", "C. Dyer", "Y. Goldberg", "A. Matthews", "W. Ammar", "A. Anastasopoulos", "M. Ballesteros", "D. Chiang", "D. Clothiaux", "T. Cohn"], "venue": "arXiv preprint arXiv:1701.03980, 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motl\u0131cek", "Y. Qian", "P. Schwarz", "J. Silovsk\u00fd", "G. Semmer", "K. Vesel\u00fd"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1929}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-R. Mohamed", "G. Hinton"], "venue": "Proc. ICASSP. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "State-of-the-art speech recognition accuracy has been significantly improved over the past few years since the application of deep neural networks [1, 2].", "startOffset": 147, "endOffset": 153}, {"referenceID": 1, "context": "State-of-the-art speech recognition accuracy has been significantly improved over the past few years since the application of deep neural networks [1, 2].", "startOffset": 147, "endOffset": 153}, {"referenceID": 2, "context": "Recently, it has been shown that with the application of both neural network acoustic model and language model, the automatic speech recognizer has approached the human-level accuracy on switchboard conversational speech recognition benchmark using around 2000 hours of transcribed data [3].", "startOffset": 287, "endOffset": 290}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].", "startOffset": 46, "endOffset": 58}, {"referenceID": 5, "context": ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].", "startOffset": 46, "endOffset": 58}, {"referenceID": 6, "context": ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].", "startOffset": 46, "endOffset": 58}, {"referenceID": 7, "context": ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].", "startOffset": 46, "endOffset": 58}, {"referenceID": 8, "context": ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].", "startOffset": 102, "endOffset": 113}, {"referenceID": 9, "context": ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].", "startOffset": 102, "endOffset": 113}, {"referenceID": 10, "context": ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].", "startOffset": 102, "endOffset": 113}, {"referenceID": 11, "context": ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].", "startOffset": 176, "endOffset": 184}, {"referenceID": 12, "context": ", connectionist temporal classification (CTC) [5, 6, 7, 8], sequence-to-sequence with attention model [9, 10, 11], and neural network segmental conditional random field (SCRF) [12, 13].", "startOffset": 176, "endOffset": 184}, {"referenceID": 13, "context": "Though CTC has been shown to outperform HMM systems[14], the improvement is based on the use of context dependent phone targets and very large amount of training data.", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "When the training data is less abundant, it has been shown that the accuracy of CTC systems degrade significantly [15].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "[16] proposed a multitask learning approach to train a joint attention and CTC model using a shared encoder.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "The key difference of our study from [16] is that the two loss functions of CTC and attention model are locally normalized for each output token and they are both trained using the cross entropy criterion.", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Similar to the observations in [16], we demonstrate that the joint training approach improves the recognition accuracies of both CTC and SCRF acoustic models.", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "Most of conventional approaches for SCRF-based acoustic models use manually defined feature function \u03a6(\u00b7), where the features and segment boundary information are provided by an auxiliary system [18, 19].", "startOffset": 195, "endOffset": 203}, {"referenceID": 18, "context": "Most of conventional approaches for SCRF-based acoustic models use manually defined feature function \u03a6(\u00b7), where the features and segment boundary information are provided by an auxiliary system [18, 19].", "startOffset": 195, "endOffset": 203}, {"referenceID": 19, "context": "In [20, 13], we proposed an end-to-end training approach for SCRFs, where \u03a6(\u00b7) was defined with neural networks, and the segmental level features were leaned by RNNs.", "startOffset": 3, "endOffset": 11}, {"referenceID": 12, "context": "In [20, 13], we proposed an end-to-end training approach for SCRFs, where \u03a6(\u00b7) was defined with neural networks, and the segmental level features were leaned by RNNs.", "startOffset": 3, "endOffset": 11}, {"referenceID": 12, "context": "Given the recurrent hidden states, the embedding vector can be simply defined as x\u0304j = hnj as in our previous work [13].", "startOffset": 115, "endOffset": 119}, {"referenceID": 20, "context": "To address this problem, a dynamic programming algorithm can be applied, which can reduce the computational complexity to O(T 2 \u00b7 |Y|) [21].", "startOffset": 135, "endOffset": 139}, {"referenceID": 12, "context": "The readers are referred to [13] for further details including the decoding algorithm.", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": "More details regarding the computation of the loss and the backpropagation algorithm to train CTC models can be found in [22].", "startOffset": 121, "endOffset": 125}, {"referenceID": 0, "context": "where \u03bb \u2208 [0, 1] is the interpolation weight.", "startOffset": 10, "endOffset": 16}, {"referenceID": 22, "context": "In this work, we focus on the RNN with long short-term memory (LSTM) [23] units for feature extraction.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "Our experiments were performed on the TIMIT database, and both the SRNN and CTC models were implemented using the DyNet toolkit [24].", "startOffset": 128, "endOffset": 132}, {"referenceID": 24, "context": "We followed the standard protocol of the TIMIT dataset, and our experiments were based on the Kaldi recipe [25].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "We also subsampled the acoustic sequence by a factor of 4 using the hierarchical RNN as in [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 25, "context": "Our models were trained with dropout regularization [26], using an specific implementation for recurrent networks [27].", "startOffset": 52, "endOffset": 56}, {"referenceID": 26, "context": "Our models were trained with dropout regularization [26], using an specific implementation for recurrent networks [27].", "startOffset": 114, "endOffset": 118}, {"referenceID": 27, "context": "[28] obtained better result using about the same size of neural network (3 hidden layers with 250 hidden units of bidirectional LSTMs) than ours (18.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] applied the prefix decoding with beam search, which may have lower search error than our best path decoding algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] with beam search decoding.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Segmental conditional random fields (SCRFs) and connectionist temporal classification (CTC) are two sequence labeling objectives used for end-to-end training of speech recognition models. Both models define the transcription probability by marginalizing decisions about latent segmentation alternatives to derive a sequence probability: the former uses a globally normalized joint model of segment labels and durations, and the latter classifies each frame as either an output symbol or a \u201ccontinuation\u201d of the previous label. In this paper, we train a recognition model by optimizing an interpolation between the SCRF and CTC losses, where the same recurrent neural network (RNN) encoder used for feature extraction for both outputs. We find that this multi-task objective improves recognition accuracy when decoding with either the SCRF or CTC models. Additionally, we show that CTC can also be used to pretrain the RNN encoder, which improves the convergence rate when learning the joint model.", "creator": "LaTeX with hyperref package"}}}