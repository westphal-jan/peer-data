{"id": "1702.06392", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "A GPU-Outperforming FPGA Accelerator Architecture for Binary Convolutional Neural Networks", "abstract": "FPGA-based hardware accelerators for convolutional neural networks (CNNs) have obtained great attentions due to their higher energy efficiency than GPUs. However, it is challenging for FPGA-based solutions to achieve a higher throughput than GPU counterparts. In this paper, we demonstrate that FPGA acceleration can be a superior solution in terms of both throughput and energy efficiency when a CNN is trained with binary constraints on weights and activations. Specifically, we propose an optimized accelerator architecture tailored for bitwise convolution and normalization that features massive spatial parallelism with deep pipelines stages. Experiment results show that the proposed architecture is 8.3x faster and 75x more energy-efficient than a Titan X GPU for processing online individual requests (in small batch size). For processing static data (in large batch size), the proposed solution is on a par with a Titan X GPU in terms of throughput while delivering 9.5x higher energy efficiency.", "histories": [["v1", "Mon, 20 Feb 2017 05:21:34 GMT  (954kb)", "http://arxiv.org/abs/1702.06392v1", null], ["v2", "Thu, 8 Jun 2017 16:09:55 GMT  (998kb)", "http://arxiv.org/abs/1702.06392v2", null]], "reviews": [], "SUBJECTS": "cs.DC cs.AR cs.CV cs.LG", "authors": ["yixing li", "zichuan liu", "kai xu", "hao yu", "fengbo ren"], "accepted": false, "id": "1702.06392"}, "pdf": {"name": "1702.06392.pdf", "metadata": {"source": "CRF", "title": "A 7.663-TOPS 8.2-W Energy-efficient FPGA Accelerator for Binary Convolutional Neural Networks", "authors": ["Yixing Li", "Zichuan Liu", "Kai Xu", "Hao Yu", "Fengbo Ren"], "emails": ["yixingli@asu.edu", "zliu016@e.ntu.edu.sg", "kaixu@asu.edu", "haoyu@ntu.edu.sg", "renfengbo@asu.edu"], "sections": [{"heading": null, "text": "Networks (CNNs) have attracted a lot of attention due to their higher energy efficiency than GPUs. However, it is difficult for FPGAbased solutions to achieve higher throughput than GPU counterparts. In this paper, we show that FPGA acceleration can be a superior solution in terms of both throughput and energy efficiency when training a CNN with binary weight and activation limitations. In particular, we propose an optimized accelerator architecture that is tailored to bitwise folding and normalization and has massive spatial parallelism with deep pipeline levels. Experimental results show that the proposed architecture is 8.3x faster and 75x more energy efficient than a Titan X GPU for processing individual online requests (in small batches). For processing static data (in large batches), the proposed solution of a Titan X GPU in terms of throughput and provides 9.5x more energy efficiency."}, {"heading": "1. INTRODUCTION", "text": "These methods include presetting CNNs to develop more efficient hardware acceleration solutions for real-time driving applications. Several methods have been proposed to reduce compilation complexity and storage space by reducing the redundancy of CNN models. These methods include presetting CNNs to develop more efficient hardware acceleration solutions for real-time driving applications."}, {"heading": "2. BACKGROUND & MOTIVATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Binary CNN (BCNN)", "text": "A CNN is a trained neural network model with high-grade properties extracted from the input image. [13] A typical CNN model contains revolutionary, pooling, and fully connected layers. The first layers typically capture regional information such as edges and curves, and the last layers interpret these low-threshold features into high-grade abstractions with the posterior probability assigned to classification. A BCNN is a CNN that is trained with binary constraints, resulting in binary weights and activations and a huge reduction in computational complexity."}, {"heading": "2.1.1 Convolution", "text": "The folding layer is the core layer of a BCNN. As typical CNN, entering each folding layer is a 3D function board with a size of as shown in Figure 1. Each filter has a size of, \""}, {"heading": "2.1.2 Pooling", "text": "The pooling layer performs subsampling over a K \u00d7 K contiguous region on the output feature map of the sinuous layers, which bundles sensitive information that is classified and considered insensitive. There are two types of pooling methods commonly used in BCNNs. One is maxpooling, which takes the maximum value of the pooling region, and the other is average-pooling, which takes the mean value of the pooling region."}, {"heading": "2.1.3 Normalization", "text": "Normalization is a powerful technique that stabilizes and accelerates the training process [11]. In the follow-up phase, normalization is also required to match the training process. Statistical reference values are counted throughout the training set as = 2 + \u03b3 +. (2) where the mean and 2 is the variance with a very small constant to ensure a denominator that is not zero. Note that and the normalized values scale and shift. Since, 2,, and all constants are in the inference level, it allows the possibility to reduce the computational complexity of normalization."}, {"heading": "2.1.4 Nonlinear function (Binarization)", "text": "Nonlinear function is an elementary operation performed on each neuron after normalization in sinuous layers and fully connected layers [13]. Since the weights and activations are limited to either + 1 or -1, the nonlinear function of the BCNN is defined as an adapted drawing function, also called a binary function, defined as () = {1, (3)."}, {"heading": "2.2 A BCNN on CIFAR-10", "text": "The overall architecture of BCNN is shown in Table 1 [9]. It requires an RGB image with a size of 3 x 32 x 32 as input of the first layer. For each layer, the filter size is specified as 3 x 3 with a step speed of 1 pixel and zero padding of 1 pixel. Filter information of each layer of the FPUT layer in Table 1 is specified as the WID layer in which all layers are traversed. Max pooling is shown via a 2 x 2 window with step 2 followed by the conversion layer 2, 4 and 6. The last three layers are fully connected layers. Normalization is applied to all layers followed by binarization except the last layer. Figure 2 shows the accuracy comparison between the BCNN and a reduced precision CNN with the same configuration in Table 1. It is shown that simply quantifying network parameters below 10 bits in the post-training phase will cause significant accuracy losses."}, {"heading": "3. Algorithm Reformulation for FPGA Mapping", "text": "In order to achieve the best quality implementation results on an FPGA, we reformulate the BNN model into our BCNN model in order to further improve the hardware friendliness of the BNN model [9]."}, {"heading": "3.1 Binary-encoded Convolution", "text": "When training a BCNN in [9], the weights and activations are limited to either + 1 or -1. For efficient FPGA imaging, we encode + 1 and -1 as 1 and 0 in our design. In this way, only 1-bit word length is needed to store a weight value or an activation value. In addition, the folding process in the plane is simplified into an XNOR point product of the input card. (4) Equation (4) shows that we are summarizing 1s and 0s, which are different from the original BCNN values, which are summarized as -1s and + 1s as in Equation (1)."}, {"heading": "3.2 Comparator-based Normalization", "text": "Therefore, we can combine the binarization (Equation (3), normalization (Equation (2)), and equation (5) and simplify them into a modified drawing function defined as (,) = {1, (,). Then we round up the closest integer for the hardware implementation.The effect of this reformulation on the hardware implementation is that we now only need a LUTs-based comparator to implement both the normalization and binarization functions. Furthermore, we only need to store one constant for each baseline value, rather than a set of parameters, 2 and."}, {"heading": "4. Architecture Design and Optimization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Architecture Overview", "text": "The binary nature of the BCNN allows us to map all weights, function boards and reference values (for normalization) in a single FPGA to the onchip block RAMs (BRAMs), eliminating any latency of the DRAM access and reducing the system's power consumption compared to the existing work based on off-chip memory [1] [3] [12]. Figure 3 shows the overall architecture of the proposed BCNN accelerator. The binary Constitutional Kernel in each layer is followed by a standard Binary (NB) kernel with or without Maxpooling (MP) kernel. All cores are highly parallelized with an optimized number of processing elements (PEs) and operate in a single statement with multiple data (SIMD). A streaming architecture is made possible by using double buffering memory channels to handle the data flow between adjacent layers."}, {"heading": "4.2 Loop Unrolling", "text": "Note that the three nested loops in (1), which accumulate the XNOR output values along the three dimensions of a folding filter, have loop-transmitted data dependencies; the processing of data-dependent loops is the same as the architectural unfolding that will improve throughput by increasing the level of temporal parallelism; thus, more hardware resources are traded with reduced loop latency; the unfolding factor is a critical architectural parameter in our design called UF. UF has a maximum value of in each layer. In other words, the calculation of pixel values along the three dimensions of an output function card has no loop-transmitted data dependency; the unfolding of independent loops corresponds to the creation of spatial parallelism in the architecture to improve throughput. In our design, we fully unroll these independent loops to maximize throughput, as we call them the non-mass factor in the PGA."}, {"heading": "4.3 Pipelining", "text": "In the proposed architecture, deep pipelining is used to further improve time parallelism and maximize system throughput. Note that the waiting time for the next data to be fed in is the reverse throughput, referred to in this essay as the initial interval. If there is a loop in the data path, the minimum initial interval is limited by the latency of the physical loop in the hardware. Pipelining allows us to feed in the next data whenever possible with a shorter waiting time. In the case of full pipelining (= 1), we can feed in new data at each cycle."}, {"heading": "4.4 Throughput Modeling", "text": "In this context, it should be noted that this is a pure course of action, which is only an approach, which is an approach, which is not an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, which is an approach, and which is an approach, and which is an approach, and which is an approach,"}, {"heading": "5. FPGA Implementation", "text": "In this section we specify our strategy to map different computing units in order to optimize the use of FPGA resources."}, {"heading": "5.1 PE Unit", "text": "The block diagram of a PE is in Figure 4. A PE unit performs the calculation of the XNOR point product of a weight vector and a feature map vector from the previous layer. The vectors are fed into an array of 2-input XNOR gates, followed by a parallel bit count logic for accumulation. Since both the XNOR gates and the bit count logic use binary values as input, the PEs can be efficiently implemented with the abundant LUT resources. This is the key to enabling massive computational parallelism on an FPGA. Note that the number of XNOR gates in each PE is the same as the evolution factor UF of the current layer. By accumulating the PE output, a pixel value of the output feature map can be calculated by the bitcount logic."}, {"heading": "5.2 Computing Kernels", "text": "Figure 5 shows the architecture of the revolution core followed by the MP and NB kernel. Each revolution core has a series of PEs implemented by LUTs, followed by a series of accumulators implemented by DSP48 sections. The number of PEs and DSP sections corresponds to the spatial parallelism factor P. Each revolution core calculates the P pixel values of the output feature map in parallel. Besides the weight arrays, only intermediate results of the accumulator outputs (bit count results) are stored in BRAMs within a single feature map. Feature maps are mapped to distributed RAMs. At revolution levels 1, 3 and 5 without max pooling, the outputs of the accumulators are directly connected to the NB cores."}, {"heading": "5.3 Memory", "text": "To read and write a large number of bits in the same clock cycle, we need to partition and redesign the storage arrays in the BCNN model. Essentially, the partition splits a large data array into smaller ones to accommodate multiple BRAMs for parallel access. Essentially, the transformation redefines the depth and width of a single BRAM by grouping several words into a wider one. In our design, weight and arrays are mapped to BRAMs and distributed accordingly to RAMs (registers). As the maximum word length of a BRAM in a Virtex-7 FPGA is limited to 32 bits, we first reshape the weight array by 32 BRAMs and then redistribute the weight arrays to multiple BRAMs to ensure sufficient memory bandwidth for the required system throughput."}, {"heading": "6. Experiment Results", "text": "We implemented the proposed accelerator architecture for the BCNN in [9] using the parameters in Table 2. Looking at Equation (10), we adjust the parameters of UF and P so that each layer is approximately the same and it is assumed that in each layer (= 1) a complete pipeline is plotted. In particular, the calculation along FW and FD is performed in full."}, {"heading": "6.1 Design Environment", "text": "For this work, we use C language to describe the accelerator architecture and Vivado HLS to generate RTL codes. Vivado Design Suite is used to map the design to a Xilinx Virtex-7 XC7VX690 FPGA. Execution time in terms of clock cycles is reported by Vivado HLS and system frequency is reported by Vivado Design Suite. We notice a large discrepancy in the use of LUTs between the synthesis reports in Vivado HLS and Vivado Design Suite. Resource consumption and power consumption are reported in the Vivado Design Suite after implementation."}, {"heading": "6.2 Implementation results", "text": "As shown in Table 2, the real execution time is very narrowly defined by the synthesis report for each layer. The bottleneck layer is layer 6. With a maximum system frequency of 90 MHz, the throughput of our BCNN with the FPGA accelerator is 6218 fps. The highest accuracy is 87.8%, with an accuracy of only 0.3% compared to its counterpart Theano.We use the IP cores generated by Vivado HLS to implement our design layer in the Vivado Design Suite. For each layer, it includes the initialization of the function board from the previous layer and all the calculation in the current layer. This scheme can ensure that we capture the resource utilization of each layer once across the entire network. The overhead introduced by initialization can be foggy. Table 3 shows the resource utilization for the entire network."}, {"heading": "7. Conclusion", "text": "In this paper, we propose an optimized accelerator architecture that is tailored to BCNNs and has shown for the first time that the FPGA-accelerated BCNN solution can significantly outperform a Titan X GPU in both throughput and energy efficiency when processing accurate image classification tasks. The proposed FPGA-accelerated BCNN is 8.3x faster and 75x more energy efficient than a Titan X GPU in processing single online queries (in small batch sizes). For processing static data (in large batch sizes), the proposed solution is on par with a Titan X GPU in terms of throughput at 9.5x higher energy efficiency. Therefore, BCNNs are ideal for efficient hardware implementations on FPGAs regardless of workload. Bitwise operations in a BCNN allow efficient hardware imaging of folding cores on LUTs. Architectural unfolding, parallelism, and parallelism are key to high computing architecture."}, {"heading": "8. REFERENCE", "text": "[1] Zhang, C., P., Sun, G., Guan, Y., Xiao, B., & Cong, J.2015. Optimizing fpga-based accelerator design for deep convolutional neural networks. In Proceedings of the 2015 ACM / SIGDA International Symposium on FieldProgrammable Gate Arrays, 161-170. [2] Krizhevsky, A., Sutskever, I., Akselrod, G. E., & LeCun, Y. 2011. Neuflow: A runtime reconfigurable dataflow processor for vision. In Cvpr 2011 Workshops, 109- 116."}], "references": [{"title": "Optimizing fpga-based accelerator design for deep convolutional neural networks", "author": ["C. Zhang", "P. Li", "G. Sun", "Y. Guan", "B. Xiao", "J. Cong"], "venue": "In Proceedings of the 2015 ACM/SIGDA International Symposium on Field- Programmable Gate Arrays,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Neuflow: A runtime reconfigurable dataflow processor for vision", "author": ["C. Farabet", "B. Martini", "B. Corda", "P. Akselrod", "E. Culurciello", "Y. LeCun"], "venue": "In Cvpr 2011 Workshops,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Throughput- Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks", "author": ["N. Suda", "V. Chandra", "G. Dasika", "A. Mohanty", "Y. Ma", "S. Vrudhula", "J.S. Seo", "Y. Cao"], "venue": "In Proceedings of the 2016 ACM/SIGDA International Symposium on Field- Programmable Gate Arrays,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.P. David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Resiliency of Deep Neural Networks under Quantization", "author": ["W. Sung", "S. Shin", "K. Hwang"], "venue": "arXiv preprint arXiv:1511.06488", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Training Binary Multilayer Neural Networks for Image Classification using Expectation Backpropagation", "author": ["Z. Cheng", "D. Soudry", "Z. Mao", "Z. Lan"], "venue": "arXiv preprint arXiv:1503.03562", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Bitwise neural networks. arXiv preprint arXiv:1601.06071", "author": ["M. Kim", "P. Smaragdis"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["M. Courbariaux", "Y. Bengio"], "venue": "arXiv preprint arXiv:1602.02830", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "arXiv preprint arXiv:1603.05279", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Going deeper with embedded fpga platform for convolutional neural network", "author": ["J. Qiu", "J. Wang", "S. Yao", "K. Guo", "B. Li", "E. Zhou", "J. Yu", "T. Tang", "N. Xu", "S. Song", "Y. Wang"], "venue": "In Proceedings of the 2016 ACM/SIGDA  International Symposium on Field-Programmable Gate Arrays,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Structured Pruning of deep convolutional neural networks", "author": ["S. Anwar", "K. Hwang", "W. Sung"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "INTRODUCTION Convolutional neural network (CNN) has become a popular machine learning engine for many image-related data analytics [13] [15-16], such as image classification, face detection, object tracking, etc.", "startOffset": 136, "endOffset": 143}, {"referenceID": 13, "context": "INTRODUCTION Convolutional neural network (CNN) has become a popular machine learning engine for many image-related data analytics [13] [15-16], such as image classification, face detection, object tracking, etc.", "startOffset": 136, "endOffset": 143}, {"referenceID": 15, "context": "These methods include pruning [18], reduced-precision CNNs [4], and binary CNNs (BCNNs) [9].", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "These methods include pruning [18], reduced-precision CNNs [4], and binary CNNs (BCNNs) [9].", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "These methods include pruning [18], reduced-precision CNNs [4], and binary CNNs (BCNNs) [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 15, "context": "The pruning technique [18] prunes the \u201cuseless\u201d weights of a trained network based on sensitivity analysis, which can effectively reduce the CNN weight count (usually referred to as network size) for a ten-class classification problem by 75% [18].", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "The pruning technique [18] prunes the \u201cuseless\u201d weights of a trained network based on sensitivity analysis, which can effectively reduce the CNN weight count (usually referred to as network size) for a ten-class classification problem by 75% [18].", "startOffset": 242, "endOffset": 246}, {"referenceID": 3, "context": "[4] demonstrates that reducing the numerical precision of a CNN from 32 to 16 bits has very limited impact on classification accuracy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "However, a numerical precision below 8 bits resulted from quantization in the post-training stage often suffers from unacceptable accuracy drop [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 4, "context": "BinaryConnect [5] and the work in [6] demonstrate the successful use of binary and ternary (-1, 0, +1) weights in CNN, respectively.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "BinaryConnect [5] and the work in [6] demonstrate the successful use of binary and ternary (-1, 0, +1) weights in CNN, respectively.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "As one step forward, EBP [7], Bitwise DNNs [8], and the BCNN in [9] successfully exploit both binary weights and activations.", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "As one step forward, EBP [7], Bitwise DNNs [8], and the BCNN in [9] successfully exploit both binary weights and activations.", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "As one step forward, EBP [7], Bitwise DNNs [8], and the BCNN in [9] successfully exploit both binary weights and activations.", "startOffset": 64, "endOffset": 67}, {"referenceID": 8, "context": "In particular, the BCNN in [9] shows a 0.", "startOffset": 27, "endOffset": 30}, {"referenceID": 14, "context": "96% classification error rate on the MNIST database [17], which is comparable to a full-precision state-of-theart CNN.", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "Early research effort [9] shows that GPU can get 7x speedup using a binary kernel for MNIST classification task on a binary multilayer perceptron (MLP).", "startOffset": 22, "endOffset": 25}, {"referenceID": 10, "context": "3 Normalization Normalization is a powerful technique that stabilizes and accelerates the training process [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 8, "context": "2 A BCNN on CIFAR-10 The overall architecture of BCNN is shown in Table 1 [9].", "startOffset": 74, "endOffset": 77}, {"referenceID": 8, "context": "Algorithm Reformulation for FPGA Mapping For the best quality of implementation results on an FPGA, we reformulate BNN model to our BCNN model to further improve the hardware-friendliness of the BNN model [9].", "startOffset": 205, "endOffset": 208}, {"referenceID": 8, "context": "1 Binary-encoded Convolution When training a BCNN in [9], the weights and activations are constrained to either +1 or -1.", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "This eliminates any DRAM access latency and dramatically reduces the energy consumption of the system comparing to the existing work relying on off-chip storage [1] [3] [12].", "startOffset": 161, "endOffset": 164}, {"referenceID": 2, "context": "This eliminates any DRAM access latency and dramatically reduces the energy consumption of the system comparing to the existing work relying on off-chip storage [1] [3] [12].", "startOffset": 165, "endOffset": 168}, {"referenceID": 11, "context": "This eliminates any DRAM access latency and dramatically reduces the energy consumption of the system comparing to the existing work relying on off-chip storage [1] [3] [12].", "startOffset": 169, "endOffset": 173}, {"referenceID": 8, "context": "Experiment Results We implemented the proposed accelerator architecture for the BCNN in [9] using the architectural parameters shown in Table 2.", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "For GPU-based ones, the baseline kernel is non-optimized one for floating-point and the XNOR kernel is optimized for BCNN [9].", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": "REFERENCE [1] Zhang, C.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "[2] Krizhevsky, A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Farabet, C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Suda, N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Courbariaux, M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Sung, W.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] [1] [12] [4] Our work", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[3] [1] [12] [4] Our work", "startOffset": 4, "endOffset": 7}, {"referenceID": 11, "context": "[3] [1] [12] [4] Our work", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "[3] [1] [12] [4] Our work", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "[7] Cheng, Z.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Kim, M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Courbariaux, M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Rastegari, M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Ioffe, S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Qiu, J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] Krizhevsky, A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] Simonyan, K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] Anwar, S.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "FPGA-based hardware accelerators for convolutional neural networks (CNNs) have obtained great attentions due to their higher energy efficiency than GPUs. However, it is challenging for FPGAbased solutions to achieve a higher throughput than GPU counterparts. In this paper, we demonstrate that FPGA acceleration can be a superior solution in terms of both throughput and energy efficiency when a CNN is trained with binary constraints on weights and activations. Specifically, we propose an optimized accelerator architecture tailored for bitwise convolution and normalization that features massive spatial parallelism with deep pipelines stages. Experiment results show that the proposed architecture is 8.3x faster and 75x more energy-efficient than a Titan X GPU for processing online individual requests (in small batch size). For processing static data (in large batch size), the proposed solution is on a par with a Titan X GPU in terms of throughput while delivering 9.5x higher energy efficiency.", "creator": "Microsoft\u00ae Word 2016"}}}