{"id": "1702.06404", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Delving Deeper into MOOC Student Dropout Prediction", "abstract": "In order to obtain reliable accuracy estimates for automatic MOOC dropout predictors, it is important to train and test them in a manner consistent with how they will be used in practice. Yet most prior research on MOOC dropout prediction has measured test accuracy on the same course used for training the classifier, which can lead to overly optimistic accuracy estimates. In order to understand better how accuracy is affected by the training+testing regime, we compared the accuracy of a standard dropout prediction architecture (clickstream features + logistic regression) across 4 different training paradigms. Results suggest that (1) training and testing on the same course (\"post-hoc\") can overestimate accuracy by several percentage points; (2) dropout classifiers trained on proxy labels based on students' persistence are surprisingly competitive with post-hoc training (87.33% versus 90.20% AUC averaged over 8 weeks of 40 HarvardX MOOCs); and (3) classifier performance does not vary significantly with the academic discipline. Finally, we also research new dropout prediction architectures based on deep, fully-connected, feed-forward neural networks and find that (4) networks with as many as 5 hidden layers can statistically significantly increase test accuracy over that of logistic regression.", "histories": [["v1", "Tue, 21 Feb 2017 14:35:55 GMT  (248kb,D)", "http://arxiv.org/abs/1702.06404v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CY", "authors": ["jacob whitehill", "kiran mohan", "daniel seaton", "yigal rosen", "dustin tingley"], "accepted": false, "id": "1702.06404"}, "pdf": {"name": "1702.06404.pdf", "metadata": {"source": "CRF", "title": "Delving Deeper into MOOC Student Dropout Prediction", "authors": ["Jacob Whitehill", "Kiran Mohan", "Daniel Seaton", "Dustin Tingley"], "emails": ["jrwhitehill@wpi.edu", "kmohan@wpi.edu", "seaton@harvard.edu", "rosen@harvard.edu", "dtingley@gov.harvard.edu"], "sections": [{"heading": "INTRODUCTION AND RELATED WORK", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "DATASET", "text": "The experiments and analyses in this paper are based on data from 40 MOOCs from HarvardX - see Table 2, which lists the codename of each course; the year and semester (T1, T2 and T3 are spring, summer and autumn respectively in the Northern Hemisphere) in which the course was offered; the academic discipline; the number of registered participants; and the number of certified participants. Each MOOC has a start date (which we call T0%) when the first materials - such as lecture videos, discussion forums, etc. - are published, and an end date when certificates are issued."}, {"heading": "List of MOOCs Course Year Field #Part. #Cert.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1368.1x 2014T3 SocialSci 2335 387", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Target labels", "text": "The binary target labels used for training and evaluation were whether (1) or not (0) each student earned enough points to receive a certificate during the MOOC. Grade thresholds for certification differed between MOOCs, but typically were about 70%. Note that as of the end of 2015, some HarvardX MOOCs implemented a policy whereby only students who paid a fee to verify their identity could officially acquire a certificate. In these courses, we still considered the target label for a student to be 1 as long as their score exceeded the verification threshold - in other words, we ignored the fact that the student was paying money or not to be identified."}, {"heading": "Features", "text": "The features used in the MOOC dropout prediction are clickstream features calculated from the clickstream log, which contains all interaction events between each student and the MOOC course software, including answers to quizzes, play / pause / rewind lecture videos, reading and writing to the discussion form (the events, not the actual text), and more. The features we have selected as the basis for automatic MOOC dropout detection are similar to previous approaches (e.g. [2]) and can be generalized to a variety of MOOCs across academic disciplines; they are listed in Table 3."}, {"heading": "Feature extraction", "text": "All characteristics have been extracted from two different database tables - person course and person course day - which are automatically updated daily by the edx2bigquery Data Management Framework [4] used for MITx and HarvardX MOOCs. Characteristics of the person course consist of the self-reported highest educational level (LoE), year of birth (YoB), sex and continent (Africa, Europe, etc.). Each of these variables has been converted into a vector of binary dummy variables: \u2022 Age: dummy variables based on the approximate age of the students (calculated as 2012 \u2212 YoB) in the following areas: < 10, 10 \u2212 15, 15 \u2212 20 \u2212 25, 25 \u2212 30 \u2212 35 \u2212 40, 40 \u2212 45 \u2212 50 \u2212 50, 50 \u2212 55 \u2212 60 years, and zero (no answer)."}, {"heading": "TRAINING PARADIGMS", "text": "We compared several different paradigms for the training of automatic MOOC dropout predictors: 1. Train on the same course (post-hoc): In predicting which students will drop out of course c, we have to deal with characteristics and target labels from the exact same course; 2. That a new MOOC will emerge with the exact distribution of students (demographics, prior knowledge, etc.); and 3. That we will be able to go back in time until the MOOC starts for the first time; 3. That a new MOOC will emerge with the exact distribution of students; 4. That we will be able to conduct the experiments with separate individuals; 5-6. May; 6-7. June; 7-8.-8.-8.-8.-8.-9. June; 8-9. June; 8-9. September; 9-9. September; 9-10. September; 9-10."}, {"heading": "CLASSIFICATION ARCHITECTURE", "text": "The classification architecture for the detectors was logistic regression with L2 regularization, which corresponds to a two-layer neural network. In estimating the parameters of the logistic regression models, we varied the strength of the regularization parameters using the theorem C: 10 \u2212 4, 10 \u2212 3, 10 \u2212 2, 10 \u2212 1, 100, 101, 102}. The predictive accuracy of the test theorem did not change significantly (relative to the default value of C = 1) compared to this theorem, and we therefore report all experimental results for C: 1. We used the open source scar and numpy packages for the training."}, {"heading": "Baseline approaches", "text": "To assess how much \"added value\" machine learning approaches bring to the MOOC drop-out prediction using detailed clickstream information, we compared the architectures described above with two simple basic euristics. Specifically, Baseline 1 uses only demographic information - consisting of self-reported year of birth, continent of origin (Africa, North America, etc.), educational level (primary / primary, high / secondary, college, etc.), and gender - to make predictions; this information is available to each student as soon as they sign up for the course, so no clickstream data is required. As with the other approaches, logistic firsthand regression has been used for classification training; we trained a separate logistic regression classifier for each course; we also compared it with an even simpler baseline 2, which does not require machine learning at all; rather, the predictor makes predictions based on the number of days the last student has been interacting with the course since the last one."}, {"heading": "Normalization", "text": "For Train on the Same Course (post hoc), Train on Other Course from the Same Field, and Train on Many Other Courses, both training and test sets were normalized by subtracting each characteristic value by the characteristic mean and dividing it by the characteristic standard deviation, with the mean and standard deviation calculated only by the training data. This normalization is important to ensure that the L2 regularization affects all characteristics in a similar way. In the case of Train with proxy labels (in situ), we used a different type of normalization to ensure that characteristic values are comparable, even if they are extracted over time frames of varying lengths (see Figure 1)."}, {"heading": "Accuracy metric", "text": "To measure the accuracy of the dropout classifiers, we use the Area Under the receiver operating characteristicsCurve (AUC) metric. The Receiver Operating Characteristics (ROC) curve itself indicates the true positive rate versus the false positive rate of the trained classifier. The AUC is the integral of the ROC curve over the interval [0, 1] and answers the following question: If you take into account a randomly selected student who leaves the MOOC and a randomly selected student who is certified in the MOOC, what is the probability that the classifier can correctly distinguish the two students? A useless classifier who simply flips a coin has an AUC of 0.5, while a perfect classifier has an AUC of 1. (A classifier with an AUC of 0 always makes the wrong distinction.) Importantly, the dropout accuracy as rated by the AUC statistics is not influenced by the total percentage of a particular student within a course that is very high in the rule."}, {"heading": "When to Measure Accuracy", "text": "In order to obtain reliable estimates of the accuracy of a MOOC dropout detector, it is important to take into account the period during which the accuracy is calculated. To achieve the goal of automatic intervention, it is more useful to be able to predict early in the course, rather than later, which students will ultimately drop out. Another point to take into account is that some students have already accumulated enough points to earn a certificate just before the end of the MOOC. A dropout detector that predicts that these students will not drop out does not so much \"predict\" the future performance of these students as it reports what they have already achieved; therefore, accuracy statistics calculated about such students may overestimate the performance of the predictor. For both reasons, we have decided to increase the accuracy of every week of each MOOC between the start date of the course (T0% when lessons begin) and the earliest date by which students may have accumulated enough points to earn a certificate (100%)."}, {"heading": "RESULTS", "text": "The horizontal axis indices of the week in the MOOC, where week 0 is defined for each course to be T100%. (Week \u2212 3 equals 3 weeks before T100%, etc.) Each data point is represented with error bars corresponding to the standard error of the mean. As the length of the courses varied, the accuracy statistics for each week w were calculated using only those MOOCs for which data were available during the week. As shown in the chart, the most accurate prediction paradigm was Train on the same course (post-hoc), which was the prevailing training paradigm used in most previous prediction research. It achieved an accuracy (averaged over every 8 weeks, and all MOOCs within each week) of 90.20%. Perhaps more surprisingly, the second most accurate approach was Train a proxy label (in u). This approach does not require even higher accuracy (between the MOOCs offered before or during the courses) were the students qualified?"}, {"heading": "COMPARISON TO BASELINE HEURISTICS", "text": "Baseline 1, whose predictions are based solely on each student's self-collected demographic data, achieved an average predictive accuracy of 58.85%, indicating that only a small amount of information about dropouts is contained in the demographic. (Note that we also tried a demographic classifier whose parameters were averaged across many courses, and not just for the one course itself; the accuracy of this approach was even lower.) Baseline 2, whose predictions are based solely on the number of days since the student last interacted with the course, performed remarkably well: it achieved an average predictive accuracy of 82.45%, confirming previous results [22, 14] that this variable is highly important for predicting. Nevertheless, Baseline 2 was still significantly less accurate than Train the same course (post-hoc) or Train Train Train using proxy labels (in situ), suggesting that the use of a more detailed clickstream improves significantly."}, {"heading": "ACCURACY ACROSS ACADEMIC FIELDS", "text": "To investigate whether the accuracy of the drop-out prediction for MOOCs in a particular academic area - STEM, humanities, social sciences or health sciences - tended to be higher, we implemented a linear model of mixed effects in which the accuracy of the drop-out prediction for the train on the same course (posthoc) was estimated using week w and field f as fixed effects and the course as a random effect. Results: Week w was statistically significant (\u03c72 (1) = 184.77, p < 0.0001) correlated to the accuracy of the drop-out prediction: for each week later in the course (i.e. closer to T100%), an estimated increase in prediction accuracy of 1.53% resulted. The difference in average accuracy varied only slightly depending on the field: prediction accuracy tended to be slightly lower for STEM courses and slightly higher for humanities courses (i.e. closer to T53%). However, the difference in average accuracy was between these statistical ranges \u2212 2.919% \u2212 which is significant for the series only."}, {"heading": "DEEPER PREDICTION ARCHITECTURES", "text": "In fact, one sees oneself in a position to stir and stir the aforementioned cerebrospinal, cerebrospinal and cerebrospinal vascularserwtcsrteeSn."}, {"heading": "Results", "text": "The results of maximizing both w and (separately after specifying w * = 5) h are shown in Figure 4. For a network with h = 1 hidden layer, the accuracy at w = 5 was 97.43%. While maintaining width at w *, the highest accuracy - 97.55% - was achieved for h = 5. For comparison, the logistical regression - which corresponds to a neural network with h = 0 hidden layers - reached an accuracy of 97.20%. The difference between these accuracies - 97.55 \u2212 97.20 = 0.35% - was statistically significant (t (23738) = 78.47, p < 0.00001, double-tailed). For comparison: The accuracy difference between the first and second place finishers in the 2015 KDD Cup on MOOC drop-out prediction was 90.92 \u2212 90.89 = 0.03% [13]. Training costs: On GSE2x, whose training set contains the best net seconds of net use of 2 (net example)."}, {"heading": "SUMMARY AND CONCLUSIONS", "text": "In this paper, we examined the practical level of accuracy that can be achieved with an automatic MOOC dropout predictor, since detectors must be trained before they can be used, using training data collected prior to their first use. Specifically, we compared several different training paradigms that are widely used in MOOC dropout prediction literature - e.g., we train on the same course (post-hoc), train on another course from the same academic discipline, and train on many different courses - on 40 different MOOCs covering a variety of disciplines, including humanities, social sciences, health sciences, and STEM. The results suggest that the accuracy of classifiers trained on data collected after completion of a course - and therefore useless in the MOOOC itself - is several percentage points higher than that of classifiers trained on other MOOCs."}], "references": [{"title": "Predicting student retention in massive open online courses using hidden markov models", "author": ["G. Balakrishnan", "D. Coetzee"], "venue": "Technical report, UC Berkeley,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Transfer learning for predictive models in massive open online courses", "author": ["S. Boyer", "K. Veeramachaneni"], "venue": "In International Conference on Artificial Intelligence in Education, pages 54\u201363. Springer,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Net2net: Accelerating learning via knowledge transfer", "author": ["T. Chen", "I.J. Goodfellow", "J. Shlens"], "venue": "CoRR, abs/1511.05641,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "MITx/HarvardX edx2bigquery", "author": ["I. Chuang", "G. Lopez"], "venue": "https://github.com/mitodl/edx2bigquery,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Probabilistic use cases: Discovering behavioral patterns for predicting certification", "author": ["C. Coleman", "D. Seaton", "I. Chuang"], "venue": "In Learning at Scale,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Combining click-stream data with nlp tools to better understand mooc completion", "author": ["S. Crossley", "L. Paquette", "M. Dascalu", "D.S. McNamara", "R.S. Baker"], "venue": "In Proceedings of the Sixth International Conference on Learning Analytics & Knowledge, pages 6\u201314. ACM,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Temporal models for predicting student dropout in massive open online courses", "author": ["M. Fei", "D.-Y. Yeung"], "venue": "In 2015 IEEE International Conference on Data Mining Workshop (ICDMW), pages 256\u2013263. IEEE,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Case study: using moocs for conventional college coursework", "author": ["R. Firmin", "E. Schiorring", "J. Whitmer", "T. Willett", "E.D. Collins", "S. Sujitparapitaya"], "venue": "Distance Education, 35(2):178\u2013201,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout prediction in MOOCs using learner activity features", "author": ["S. Halawa", "D. Greene", "J. Mitchell"], "venue": "In European MOOC Summit,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Benjamin, I", "author": ["J. He", "J. Bailey"], "venue": "Rubinstein, and R. Zhang. Identifying at-risk students in massive open online courses. In AAAI,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Harvardx and mitx: The first year of open online courses, fall 2012-summer 2013", "author": ["A.D. Ho", "J. Reich", "S.O. Nesterko", "D.T. Seaton", "T. Mullaney", "J. Waldo", "I. Chuang"], "venue": "Ho, AD, Reich, J., Nesterko, S., Seaton, DT, Mullaney, T., Waldo, J., & Chuang, I.(2014). HarvardX and MITx: The first year of open online courses (HarvardX and MITx Working Paper No. 1),", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "and D", "author": ["S. Jiang", "A. Williams", "K. Schenke", "M. Warschauer"], "venue": "O\u2019Dowd. Predicting MOOC performance with week 1 behavior. In Educational Data Mining,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Organizers", "author": ["KDD Cu"], "venue": "http://kddcup2015.com/submission-rank.html,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Attrition and achievement gaps in online learning", "author": ["R. Kizilcec", "S. Halawa"], "venue": "In Learning at Scale,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting MOOC dropout over weeks using machine learning methods", "author": ["M. Kloft", "F. Stiehler", "Z. Zheng", "N. Pinkwart"], "venue": "In Proceedings of the EMNLP 2014 Workshop on Analysis of Large Scale Social Interaction in MOOCs, pages 60\u201365,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning is not a spectator sport: Doing is better than watching for learning from a mooc", "author": ["K.R. Koedinger", "J. Kim", "J.Z. Jia", "E.A. McLaughlin", "N.L. Bier"], "venue": "In Proceedings of the Second (2015) ACM Conference on Learning@ Scale, pages 111\u2013120. ACM,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Forecasting student achievement in moocs with natural language processing", "author": ["C. Robinson", "M. Yeomans", "J. Reich", "C. Hulleman", "H. Gehlbach"], "venue": "In Proceedings of the Sixth International Conference on Learning Analytics & Knowledge, pages 383\u2013387. ACM,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Social factors that contribute to attrition in moocs", "author": ["C.P. Ros\u00e9", "R. Carlson", "D. Yang", "M. Wen", "L. Resnick", "P. Goldman", "J. Sherer"], "venue": "In Proceedings of the first ACM conference on Learning@ scale conference, pages 197\u2013198. ACM,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "The impact on retention of interventions to support distance learning students", "author": ["O. Simpson"], "venue": "Open Learning: The Journal of Open, Distance and e-Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Mass attrition: An analysis of drop out from a principles of microeconomics MOOC", "author": ["R. Stein", "G. Allione"], "venue": "PIER Working Paper, 14(031),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "K", "author": ["C. Taylor"], "venue": "Veeramachaneni, and U.-M. O\u2019Reilly. Likely to stop? Predicting stopout in massive open online courses. arXiv,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond prediction: Toward automatic intervention to reduce mooc student stopout", "author": ["J. Whitehill", "J. Williams", "G. Lopez", "C. Coleman", "J. Reich"], "venue": "In Educational Data Mining,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "The impact of instructor interaction in massive open online course (mooc) discussion forums", "author": ["T. Wong", "D. Lichtenstein", "J. Whitehill", "G. Lopez", "I. Chuang", "A. Ho"], "venue": "In American Educational Research Association workshop,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Temporal predication of dropouts in moocs: Reaching the low hanging fruit through stacking generalization", "author": ["W. Xing", "X. Chen", "J. Stein", "M. Marcinkowski"], "venue": "Computers in Human Behavior, 58:119\u2013129,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Turn on, tune in, drop out\u201d: Anticipating student dropouts in massive open online courses", "author": ["D. Yang", "T. Sinha", "D. Adamson", "C.P. Rose"], "venue": "In NIPS Workshop on Data-Driven Education,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Early prediction of student dropout and performance in moocs using higher granularity temporal information", "author": ["C. Ye", "G. Biswas"], "venue": "Journal of Learning Analytics, 1(3),", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 21, "context": ", email reminders [22]) may incur very little cost once the core infrastructure for con-", "startOffset": 18, "endOffset": 22}, {"referenceID": 18, "context": "1145/1235 ducting them is in place, other types of interventions \u2013 such as a course staff-member actively reaching out to a learner to ask her/him how she/he is doing [19, 8], or an instructor responding substantively within a discussion forum to a comment the student had written [23] \u2013 may have high variable costs and be very time-intensive.", "startOffset": 167, "endOffset": 174}, {"referenceID": 7, "context": "1145/1235 ducting them is in place, other types of interventions \u2013 such as a course staff-member actively reaching out to a learner to ask her/him how she/he is doing [19, 8], or an instructor responding substantively within a discussion forum to a comment the student had written [23] \u2013 may have high variable costs and be very time-intensive.", "startOffset": 167, "endOffset": 174}, {"referenceID": 22, "context": "1145/1235 ducting them is in place, other types of interventions \u2013 such as a course staff-member actively reaching out to a learner to ask her/him how she/he is doing [19, 8], or an instructor responding substantively within a discussion forum to a comment the student had written [23] \u2013 may have high variable costs and be very time-intensive.", "startOffset": 281, "endOffset": 285}, {"referenceID": 0, "context": "Balakrishnan & Coetzee [1] 1 Clickstream HMM + SVM Same course", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "Boyer & Veeramachaneni [2] 3 Clickstream TL+LR Different offering In-situ", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "[5] 1 Clickstream LDA+LR Same course", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] 1 Clickstream; NLP DFA Same course", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Fei & Yeung [7] 2 Clickstream RNN Same course", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "[10] 2 Clickstream Smoothed LR Different offering", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] 1 Social network; grades LR Same course", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14, 9] 20 Clickstream LR Different course Same course", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "[14, 9] 20 Clickstream LR Different course Same course", "startOffset": 0, "endOffset": 7}, {"referenceID": 14, "context": "[15] 1 Clickstream SVM Same course", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] 1 Clickstream; grades LR Same course", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] 1 Survey; NLP LR Same course", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25, 18] 1 Forum; social network SA Same course", "startOffset": 0, "endOffset": 8}, {"referenceID": 17, "context": "[25, 18] 1 Forum; social network SA Same course", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "Stein & Allione [20] 1 Clickstream; survey SA Same course", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "[21] 1 Clickstream LR Same course", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] 10 Clickstream LR Different course", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] 1 Clickstream; social network PCA+{BN,DT} Same course", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Ye & Biswas [26] 1 Clickstream LR Same course", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "also explore a novel scheme (similar to the in-situ approach proposed by [2]) for training a classifier while a course is ongoing using proxy labels and show that its performance is surprisingly good even compared to the overly optimistic post-hoc training paradigm.", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": ", [2]) and can generalize to a wide variety of MOOCs across academic disciplines; they are listed in Table 3.", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "All features were extracted from two different database tables \u2013 person course and person course day \u2013 that are automatically updated daily by the edx2bigquery data management framework [4] that is used for MITx and HarvardX MOOCs.", "startOffset": 186, "endOffset": 189}, {"referenceID": 10, "context": ", comedian Stephen Colbert talking on television about MOOCs [11]) cause students to behave differently during the later incarnation of the course.", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "In addition, we also explore a novel training approach, similar to the in-situ training method proposed by [2], based on using proxy labels:", "startOffset": 107, "endOffset": 110}, {"referenceID": 21, "context": "In prior research on dropout detection [22, 14], this variable alone has shown to be highly predictive of dropout.", "startOffset": 39, "endOffset": 47}, {"referenceID": 13, "context": "In prior research on dropout detection [22, 14], this variable alone has shown to be highly predictive of dropout.", "startOffset": 39, "endOffset": 47}, {"referenceID": 0, "context": "The AUC is the integral of the ROC curve over the interval [0, 1] and answers the following question: Given one randomly chosen student who drops out from the MOOC and one randomly chosen student who certifies in the MOOC, what is the probability that the classifier can correctly distinguish the two students? A useless classifier that simply flips a coin will have an AUC of 0.", "startOffset": 59, "endOffset": 65}, {"referenceID": 21, "context": "45%, which corroborates previous findings [22, 14] that this variable is highly salient for prediction.", "startOffset": 42, "endOffset": 50}, {"referenceID": 13, "context": "45%, which corroborates previous findings [22, 14] that this variable is highly salient for prediction.", "startOffset": 42, "endOffset": 50}, {"referenceID": 6, "context": "Only a handful [7, 5, 1, 24] of approaches have considered deeper learning architectures that can capitalize on nonlinear feature representations and interactions between features.", "startOffset": 15, "endOffset": 28}, {"referenceID": 4, "context": "Only a handful [7, 5, 1, 24] of approaches have considered deeper learning architectures that can capitalize on nonlinear feature representations and interactions between features.", "startOffset": 15, "endOffset": 28}, {"referenceID": 0, "context": "Only a handful [7, 5, 1, 24] of approaches have considered deeper learning architectures that can capitalize on nonlinear feature representations and interactions between features.", "startOffset": 15, "endOffset": 28}, {"referenceID": 23, "context": "Only a handful [7, 5, 1, 24] of approaches have considered deeper learning architectures that can capitalize on nonlinear feature representations and interactions between features.", "startOffset": 15, "endOffset": 28}, {"referenceID": 2, "context": "cently developed methodology for neural network training called Net2Net [3].", "startOffset": 72, "endOffset": 75}, {"referenceID": 12, "context": "03% [13].", "startOffset": 4, "endOffset": 8}, {"referenceID": 1, "context": "In addition, we have explored a training approach, similar to in-situ training [2], based on the idea of proxy labels \u2013 labels that approximate the quantity of interest (dropout versus certification) but that can be collected", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "Using an iterative training strategy [3], the total training time to create such a network is modest and amenable to large-scale implementation over many (hundreds) of MOOCs at regular training intervals (e.", "startOffset": 37, "endOffset": 40}], "year": 2017, "abstractText": "In order to obtain reliable accuracy estimates for automatic MOOC dropout predictors, it is important to train and test them in a manner consistent with how they will be used in practice. Yet most prior research on MOOC dropout prediction has measured test accuracy on the same course used for training the classifier, which can lead to overly optimistic accuracy estimates. In order to understand better how accuracy is affected by the training+testing regime, we compared the accuracy of a standard dropout prediction architecture (clickstream features + logistic regression) across 4 different training paradigms. Results suggest that (1) training and testing on the same course (\u201cpost-hoc\u201d) can overestimate accuracy by several percentage points; (2) dropout classifiers trained on proxy labels based on students\u2019 persistence are surprisingly competitive with post-hoc training (87.33% versus 90.20% AUC averaged over 8 weeks of 40 HarvardX MOOCs); and (3) classifier performance does not vary significantly with the academic discipline. Finally, we also research new dropout prediction architectures based on deep, fully-connected, feed-forward neural networks and find that (4) networks with as many as 5 hidden layers can statistically significantly increase test accuracy over that of logistic regression. INTRODUCTION AND RELATED WORK As the number and diversity of massive open online courses (MOOCs) continues to grow, researchers, teachers, and educational technologists are starting to explore innovative ways of helping more students who participate in these courses to persist longer and learn more. While some interventions (e.g., email reminders [22]) may incur very little cost once the core infrastructure for conACM ISBN 978-1-4503-2138-9. DOI: 10.1145/1235 ducting them is in place, other types of interventions \u2013 such as a course staff-member actively reaching out to a learner to ask her/him how she/he is doing [19, 8], or an instructor responding substantively within a discussion forum to a comment the student had written [23] \u2013 may have high variable costs and be very time-intensive. An automatic detector that could predict automatically \u2013 based on demographics or by analyzing subtle cues in a learner\u2019s interaction history with the MOOC courseware \u2013 which learners are in danger of \u201cdropping out\u201d from the MOOC and which will likely succeed, would be a valuable tool for making smart decisions about who receives a particular intervention. Related work: Within the fields of learning analytics and educational data mining, the possibility of creating automatic MOOC dropout detectors has generated considerable interest within the past few years. Existing approaches vary across several dimensions including the features used for classification, the architecture used for training and testing, and the training set used to optimize the classifier parameters \u2013 see Table 1 for a synopsis of prior work. The most popular feature representations include clickstream logs, natural language processing (NLP) of discussion forum content, and social network metrics. In terms of architecture, most prior approaches have used generalized linear models (including logistic regression and linear SVMs), survival analysis (e.g., Cox proportional hazard model), and logistic regression. A third dimension of variability is the training setting that describes the source of the training data \u2013 e.g., the same course, a prior instance of the same course, or a different course altogether \u2013 relative to the target use of the classifier once it has been trained: most research on MOOC dropout prediction to-date has focused on training and testing on data sampled from the same MOOC. When designing and implementing educational interventions that depend on automatic MOOC dropout predictors to predict which students are in danger of failing, it is important to know how accurate such predictors are. In order to obtain reliable accuracy estimates, it ar X iv :1 70 2. 06 40 4v 1 [ cs .A I] 2 1 Fe b 20 17 Survey of Prior Research on MOOC Dropout Prediction Study #MOOCs Features Architecture Training setting Balakrishnan & Coetzee [1] 1 Clickstream HMM + SVM Same course Boyer & Veeramachaneni [2] 3 Clickstream TL+LR Different offering In-situ Coleman et al. [5] 1 Clickstream LDA+LR Same course Crossley et al. [6] 1 Clickstream; NLP DFA Same course Fei & Yeung [7] 2 Clickstream RNN Same course He et al. [10] 2 Clickstream Smoothed LR Different offering Jiang et al. [12] 1 Social network; grades LR Same course Kizilcec et al. [14, 9] 20 Clickstream LR Different course Same course Kloft et al. [15] 1 Clickstream SVM Same course Koedinger et al. [16] 1 Clickstream; grades LR Same course Robinson et al. [17] 1 Survey; NLP LR Same course Rose et al. [25, 18] 1 Forum; social network SA Same course Stein & Allione [20] 1 Clickstream; survey SA Same course Taylor et al. [21] 1 Clickstream LR Same course Whitehill et al. [22] 10 Clickstream LR Different course Xing et al. [24] 1 Clickstream; social network PCA+{BN,DT} Same course Ye & Biswas [26] 1 Clickstream LR Same course Our paper 40 Clickstream {LR, DNN} Same course In-situ", "creator": "LaTeX with hyperref package"}}}