{"id": "1702.06506", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "PixelNet: Representation of the pixels, by the pixels, and for the pixels", "abstract": "We explore design principles for general pixel-level prediction problems, from low-level edge detection to mid-level surface normal estimation to high-level semantic segmentation. Convolutional predictors, such as the fully-convolutional network (FCN), have achieved remarkable success by exploiting the spatial redundancy of neighboring pixels through convolutional processing. Though computationally efficient, we point out that such approaches are not statistically efficient during learning precisely because spatial redundancy limits the information learned from neighboring pixels. We demonstrate that stratified sampling of pixels allows one to (1) add diversity during batch updates, speeding up learning; (2) explore complex nonlinear predictors, improving accuracy; and (3) efficiently train state-of-the-art models tabula rasa (i.e., \"from scratch\") for diverse pixel-labeling tasks. Our single architecture produces state-of-the-art results for semantic segmentation on PASCAL-Context dataset, surface normal estimation on NYUDv2 depth dataset, and edge detection on BSDS.", "histories": [["v1", "Tue, 21 Feb 2017 18:20:30 GMT  (8360kb,D)", "http://arxiv.org/abs/1702.06506v1", "Project Page:this http URLarXiv admin note: substantial text overlap witharXiv:1609.06694"]], "COMMENTS": "Project Page:this http URLarXiv admin note: substantial text overlap witharXiv:1609.06694", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.RO", "authors": ["aayush bansal", "xinlei chen", "bryan russell", "abhinav gupta", "deva ramanan"], "accepted": false, "id": "1702.06506"}, "pdf": {"name": "1702.06506.pdf", "metadata": {"source": "CRF", "title": "PixelNet: Representation of the pixels, by the pixels, and for the pixels", "authors": ["Aayush Bansal", "Xinlei Chen", "Bryan Russell", "Abhinav Gupta", "Deva Ramanan"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "In fact, the fact is that most of them will be able to demonstrate that they are able, that they are able to achieve their goals, and that they are able to achieve their goals."}, {"heading": "2. Background", "text": "In this section, we review the related work by using a uniform notation used to describe our architecture. We address the pixel-by-pixel predictable problem where faced with an input image X, we try to predict outputs Y. For pixel location p, the output may include binary responses to a particular column (e.g., edge detection), multi-level prediction functions (e.g., semantic segmentation), or real-rated responses to a particular column. There is a rich prediction of this prediction problem using hand-crafted features (representative examples are [3, 14, 21, 38, 80, 87, 96]). Constitutional predictions: We research spatial invariant predictors f. p (X) that are end-to-end tractive parameters."}, {"heading": "3. PixelNet", "text": "It is about the way people in the individual countries of the world deal with their different views and views, how they behave in the individual countries of the world. (...) It is about the question of how they are to behave in the individual countries of the world. (...) It is about the question of how they are to behave in the individual countries of the world. (...) It is about the question of how they are to behave in the individual countries of the world. (...) It is about the question of how they are to behave in the individual countries of the world. (...) It is about the question of how they are to behave in the individual countries of the world. (...) It is about the question of how they are to behave in the individual countries of the world. (...) World of the world, world of the world, world of the world, world of the world, world of the world, world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world, the world of the world, the world of the world, the world of the world, the world, the world of the world, the world, the world of the world, the world, the world of the world, the world, the world of the world, the world, the world of the world, the world, the world of the world, the world, the world of the world, the world, the world, the world of the world, the world, the world, the world of the world, the world, the world of the world, the world of the world, the world of the world, the world, the world of the world, the world, the world of the world, the world, the world of the world, the world, the world of the world, the world of the world, the world, the world, the world of the world, the world of the world, the world of the world, the world, the world of the world of the world, the world of the world, the world of the world, the world of the world, the world of the way, the way it is about the way it is about the way it is about"}, {"heading": "4. Analysis", "text": "In this section, we analyze the properties of pixel level optimization using semantic segmentation and normal estimation of the interface in order to understand the design decisions for pixel level architectures. We also chose the two different tasks (classification and regression) for analysis in order to verify the generalizability of these results. However, we use a single scale of 224 x 224 images as input. We also show how the performance of our approach can be compared with careful batch normalizations, so that a model can be trained from scratch (without a pre-trained ImageNet model as initialization) for semantic segmentation and normal estimation of the interface. We explicitly compare the performance of our approach with previous approaches in section 5. Default network: For most experiments, we can fine-tune a VGG 16 network [84]. VGG-16 has 13 revolutionary layers and three completely connected (fc) layers. We compare the performance of our approach with previous approaches. We compare explicitly with previous approaches in section 5. Default network: For most experiments, we can fine-tune a VGG-16 network (without pre-trained ImageNet model as initialization). VGG-16 has 13 Convolutionary layers and three completely connected (fc) layers. We compare the performance of our approach to 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44 44, 42, 42, 42, 42, 44 44 44 44 44, 42, 42, 44 44 44 44 44 44, 42, 42, 42, 44 44 44 44 44 44 44 44, 42, 42, 42, 44 44 44 44 44 44 44, 42, 42, 44 44 44 44 44, 42, 42, 44 44 44 44 44 44, 42, 42, 44 44 44 44 44 44, 42, 42, 42, 44 44 44 44, 44 44, 44 44 44, 44, 44 44, 42, 42, 42, 42, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 42, 44, 44, 44, 44, 44, 44, 44, 44, 44, 42."}, {"heading": "4.1. Sampling", "text": "We study how scanning some pixels from a fixed set of images does not affect convergence (Figure 3 and Table 1), confirming our hypothesis that much of the training data for a pixel-level task within an image is correlated, implying that random scanning of a few pixels is sufficient. Our results are consistent with those reported in Long et al. [62], which similarly examine the effect of scanning a fraction (25-50%) of the fields per training image. Long et al. [62] are also conducting an additional experiment in which the total number of pixels in a stack is kept constant when comparing different scanning strategies. While this ensures that each stack contains more diversified pixels, each stack will also process a larger number of images."}, {"heading": "4.2. Linear vs. MLP", "text": "Most of them are able to abide by the rules they have imposed on themselves. (...) Most of them are able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are able to abide by the rules themselves. (...) Most of them are not able to abide by the rules. (...)"}, {"heading": "4.3. Training from scratch", "text": "This year, it will be able to take the lead, \"he said in an interview with the Taiwanese daily Le."}, {"heading": "5. Generalizability", "text": "In this section, we demonstrate the generalizability of PixelNet and apply it (with minor modifications) to the overall task of semantic segmentation, the normal estimation of intermediate-level surfaces, and the subordinate task of edge detection. Detailed analyses for each of these tasks are in the appendix. [2] We used a single scale for object detection, using the same parameters as Fast-RCNN, except for a step size of 70K, and refining them for 200K iterations. Doersch et al. [20] report in a newer version of this report better results through the use of multi-scale detection, as well as smarter initialization and recalculation."}, {"heading": "5.1. Semantic Segmentation", "text": "Training: For all experiments we used the publicly available Caffe library [49]. All trained models and codes are published. We use ImageNet pre-trained values for all revolutionary layers, but train our MLP layers \"from the ground up\" with Gaussian initialization (\u03c3 = 10 \u2212 3) and dropout [85] (r = 0.5). We fix the dynamics 0.9 and the weight decay 0.0005 throughout the fine-tuning process. We use the following update plan (unless otherwise specified): We tune the network for 80 epochs at a fixed learning rate (10 \u2212 3), lowering the rate by 10 x every 8 epochs until we reach 10 \u2212 5 dataset: The PASCAL context dataset [4] supplements the original sparse set of PASCAL VOC 2010 with a fixed learning rate (10 \u2212 3)."}, {"heading": "5.2. Surface Normal Estimation", "text": "While Bansal et al. [6] extracted hypercolumn characteristics from 1 x 1 x 4096 con7 of the VGG-16, at conv-6 we provided sufficient cushioning to have 4 x 4 x 4096 conv-7, which provided diversity in conv7 characteristics for different pixels in an image instead of the same conv-7. Furthermore, we use a multi-scale prediction to improve the results. Training: We use the same network architecture as described above. The last fc layer of the MLP has (\u03c3 = 5 x 10 \u2212 3). We set the initial learning rate to 10 \u2212 3, which reduces the rate by 10 x after 50K SGD iteration. The network is trained for 60K iterations. Results: Table 13 compares our improved results with previous approaches of B. Further details are in the analysis [24 x]."}, {"heading": "5.3. Edge Detection", "text": "Dataset: The standard edge detection dataset is BSDS500 [4], which consists of 200 training images, 100 validation images and 200 test images. Each image is commented on by 5 people to define the contours. We use the same advanced data (rotation, flipping, a total of 9600 images without resizing) that were used to train the most advanced holistic nested edge detector (HED) [94]. We report on the test images. During training, we follow HED and use only positive labels where a consensus (\u2265 3 out of 5) of the people agrees. Training: We use the same baseline network and training strategy previously defined in Section 5.1. Significant crossentropy loss is used to determine whether a pixel belongs to an edge or not. Due to the highly distorted class distribution, we can normalize the gradients for positives and negatives in each stack (as in [94])."}, {"heading": "6. Discussion", "text": "We have described a revolutionary pixel architecture that produces state-of-the-art accuracy with minor modifications for various tasks at the high, medium, and low levels. Our results are achieved by careful analysis of computational and statistical considerations associated with Convolutionary Predictors. Convolution uses the spatial redundancy of pixel environments for efficient computation, but this redundancy also impedes learning. We propose a simple solution based on stratified seed-pling that injects diversity while taking advantage of amortized revolutionary processing. Finally, our efficient learning scheme allows us to explore nonlinear functions of multi-scale functions that encode both high-level context and low-level spatial detail, which appears relevant for most pixel prediction tasks. Appendices In Section A, we present advanced analyses of the PixelNet architecture for semantic segmentation of PASS 68, which we compare with the normal PASS-4 approach."}, {"heading": "A. Semantic Segmentation", "text": "In fact, most of them are able to survive by themselves if they do not put themselves in a position to survive by themselves; most of them are able to survive by themselves, and most of them are not able to survive by themselves."}, {"heading": "B. Surface Normal Estimation", "text": "There are 1449 images, of which 795 are trainval and the remaining 654 are used for evaluation. In addition, there are 220,000 images extracted from raw Kinect data. We calculate six statistics previously used by [6, 24, 31, 32, 33, 89] and Wang et al. [89] on the angular error between the predicted standards and depth-based standards to evaluate performance - Mean, Median, RMSE, 11.25, 22.5, and 30 - The first three criteria capture the medium, medium, and medium label. The last three criteria capture the percentage of pixels within a given two-dimensional error, with higher results. We capture the first three criteria the medium, medium, and medium label."}, {"heading": "C. Edge Detection", "text": "This year, it has come to the point that it will only take a year to find a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, and that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution."}], "references": [{"title": "Learning to see by moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": "ICCV 2015, pages 37\u201345,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Analyzing the performance of multilayer neural networks for object recognition", "author": ["P. Agrawal", "R. Girshick", "J. Malik"], "venue": "ECCV.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic segmentation using regions and parts", "author": ["P. Arbel\u00e1ez", "B. Hariharan", "C. Gu", "S. Gupta", "L. Bourdev", "J. Malik"], "venue": "CVPR. IEEE,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Contour detection and hierarchical image segmentation", "author": ["P. Arbelaez", "M. Maire", "C. Fowlkes", "J. Malik"], "venue": "TPAMI,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A database and evaluation methodology for optical flow", "author": ["S. Baker", "D. Scharstein", "J. Lewis", "S. Roth", "M.J. Black", "R. Szeliski"], "venue": "IJCV, 92(1),", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Marr Revisited: 2D- 3D model alignment via surface normal prediction", "author": ["A. Bansal", "B. Russell", "A. Gupta"], "venue": "CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Midlevel elements for object detection", "author": ["A. Bansal", "A. Shrivastava", "C. Doersch", "A. Gupta"], "venue": "CoRR, abs/1504.07284,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "The fast bilateral solver", "author": ["J.T. Barron", "B. Poole"], "venue": "CoRR, abs/1511.03296,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural networks for pattern recognition", "author": ["C.M. Bishop"], "venue": "Oxford university press,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "Sgd-qn: Careful quasi-newton stochastic gradient descent", "author": ["A. Bordes", "L. Bottou", "P. Gallinari"], "venue": "JMLR, 10,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010, pages 177\u2013186. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Scene labeling with lstm recurrent neural networks", "author": ["W. Byeon", "T.M. Breuel", "F. Raue", "M. Liwicki"], "venue": "CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Realtime multiperson 2d pose estimation using part affinity fields", "author": ["Z. Cao", "T. Simon", "S.-E. Wei", "Y. Sheikh"], "venue": "arXiv preprint arXiv:1611.08050,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantic segmentation with second-order pooling", "author": ["J. Carreira", "R. Caseiro", "J. Batista", "C. Sminchisescu"], "venue": "ECCV.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs", "author": ["L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "CoRR, abs/1606.00915,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected CRFs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "ICLR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "An implementation of faster rcnn with study for region sampling", "author": ["X. Chen", "A. Gupta"], "venue": "arXiv preprint arXiv:1702.02138,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "R. Fergus"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Structured forests for fast edge detection", "author": ["P. Doll\u00e1r", "C. Zitnick"], "venue": "ICCV,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast edge detection using structured forests", "author": ["P. Doll\u00e1r", "C.L. Zitnick"], "venue": "TPAMI, 37(8),", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial feature learning", "author": ["J. Donahue", "P. Kr\u00e4henb\u00fchl", "T. Darrell"], "venue": "CoRR, abs/1605.09782,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture", "author": ["D. Eigen", "R. Fergus"], "venue": "ICCV,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Depth map prediction from a single image using a multi-scale deep network", "author": ["D. Eigen", "C. Puhrsch", "R. Fergus"], "venue": "NIPS,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "The PASCAL Visual Object Classes (VOC) Challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "IJCV,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "TPAMI, 35(8),", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Object detection with discriminatively trained partbased models", "author": ["P. Felzenszwalb", "R. Girshick", "D. McAllester", "D. Ramanan"], "venue": "PAMI, 32(9),", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient graphbased image segmentation", "author": ["P.F. Felzenszwalb", "D.P. Huttenlocher"], "venue": "IJCV, 59(2),", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["P. Fischer", "A. Dosovitskiy", "E. Ilg", "P. H\u00e4usser", "C. Haz\u0131rba\u015f", "V. Golkov", "P. van der Smagt", "D. Cremers", "T. Brox"], "venue": "arXiv preprint arXiv:1504.06852,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Data-driven 3D primitives for single image understanding", "author": ["D.F. Fouhey", "A. Gupta", "M. Hebert"], "venue": "ICCV,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Unfolding an indoor origami world", "author": ["D.F. Fouhey", "A. Gupta", "M. Hebert"], "venue": "ECCV,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Single image 3D without a single 3D image", "author": ["D.F. Fouhey", "A. Gupta", "M. Hebert"], "venue": "ICCV,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised CNN for single view depth estimation: Geometry to the rescue", "author": ["R. Garg", "B.G.V. Kumar", "G. Carneiro", "I.D. Reid"], "venue": "ECCV, pages 740\u2013756,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast r-cnn", "author": ["R. Girshick"], "venue": "ICCV,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Using k-poselets for detecting people and localizing their keypoints", "author": ["G. Gkioxari", "B. Hariharan", "R. Girshick", "J. Malik"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Unsupervised learning of spatiotemporally coherent metrics", "author": ["R. Goroshin", "J. Bruna", "J. Tompson", "D. Eigen", "Y. Le- Cun"], "venue": "ICCV 2015, pages 4086\u20134093,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Decomposing a scene into geometric and semantically consistent regions", "author": ["S. Gould", "R. Fulton", "D. Koller"], "venue": "ICCV. IEEE,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Oriented edge forests for boundary detection", "author": ["S. Hallman", "C.C. Fowlkes"], "venue": "CVPR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "CVPR,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic contours from inverse detectors", "author": ["B. Hariharan", "P. Arbelez", "L. Bourdev", "S. Maji", "J. Malik"], "venue": "ICCV,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Recovering the spatial layout of cluttered rooms", "author": ["V. Hedau", "D. Hoiem", "D. Forsyth"], "venue": "ICCV,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "Densebox: Unifying landmark localization with end to end object detection", "author": ["L. Huang", "Y. Yang", "Y. Deng", "Y. Yu"], "venue": "arXiv preprint arXiv:1509.04874,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Pixel-wise deep learning for contour detection", "author": ["J.-J. Hwang", "T.-L. Liu"], "venue": "arXiv preprint arXiv:1504.01989,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural Image Statistics: A Probabilistic Approach to Early Computational Vision., volume 39", "author": ["A. Hyv\u00e4rinen", "J. Hurri", "P.O. Hoyer"], "venue": "Springer Science & Business Media,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2009}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "D. Blei and F. Bach, editors, Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 448\u2013456. JMLR Workshop and Conference Proceedings,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Crisp boundary detection using pointwise mutual information", "author": ["P. Isola", "D. Zoran", "D. Krishnan", "E.H. Adelson"], "venue": "Computer Vision\u2013ECCV 2014, pages 799\u2013814. Springer,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning image representations tied to ego-motion", "author": ["D. Jayaraman", "K. Grauman"], "venue": "ICCV", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "ACMMM,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual boundary prediction: A deep neural prediction network and quality dissection", "author": ["J.J. Kivinen", "C.K. Williams", "N. Heess", "D. Technologies"], "venue": "AISTATS, volume 1, page 9,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2014}, {"title": "Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory", "author": ["I. Kokkinos"], "venue": "CoRR, abs/1609.02132,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient inference in fully connected crfs with gaussian edge potentials", "author": ["P. Kr\u00e4henb\u00fchl", "V. Koltun"], "venue": "NIPS,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}, {"title": "Discriminatively trained dense surface normal estimation", "author": ["L. Ladicky", "B. Zeisl", "M. Pollefeys"], "venue": "ECCV,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning representations for automatic colorization", "author": ["G. Larsson", "M. Maire", "G. Shakhnarovich"], "venue": "European Conference on Computer Vision (ECCV),", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural networks: Tricks of the trade, pages 9\u201348. Springer,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised visual representation learning by graph-based consistent constraints", "author": ["D. Li", "W.-C. Hung", "J.-B. Huang", "S. Wang", "N. Ahuja", "M.-H. Yang"], "venue": "ECCV,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "Sketch tokens: A learned mid-level representation for contour and object detection", "author": ["J. Lim", "C. Zitnick", "P. Doll\u00e1r"], "venue": "CVPR,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonparametric scene parsing via label transfer", "author": ["C. Liu", "J. Yuen", "A. Torralba"], "venue": "TPAMI, 33(12),", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2011}, {"title": "Parsenet: Looking wider to see better", "author": ["W. Liu", "A. Rabinovich", "A.C. Berg"], "venue": "arXiv preprint arXiv:1506.04579,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CoRR, abs/1411.4038,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional models for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2015}, {"title": "A real-time algorithm for signal analysis with the help of the wavelet transform", "author": ["J.M.M. Holschneider", "R. Kronland-Martinet", "P. Tchamitchian"], "venue": "Wavelets, Time-Frequency Methods and Phase Space, pages 289\u2013 297,", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1989}, {"title": "Learning to detect natural image boundaries using local brightness, color, and texture cues", "author": ["D.R. Martin", "C.C. Fowlkes", "J. Malik"], "venue": "TPAMI, 26(5),", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2004}, {"title": "Multidigit recognition using a space displacement neural network", "author": ["O. Matan", "C.J. Burges", "Y. LeCun", "J.S. Denker"], "venue": "NIPS, pages 488\u2013495,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1991}, {"title": "Shuffle and Learn: Unsupervised Learning using Temporal Order Verification", "author": ["I. Misra", "C.L. Zitnick", "M. Hebert"], "venue": "ECCV,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2016}, {"title": "Feedforward semantic segmentation with zoom-out features", "author": ["M. Mostajabi", "P. Yadollahpour", "G. Shakhnarovich"], "venue": "CVPR, pages 3376\u20133385,", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2015}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "CVPR,", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2014}, {"title": "Stacked hierarchical labeling", "author": ["D. Munoz", "J.A. Bagnell", "M. Hebert"], "venue": "Computer Vision\u2013ECCV 2010, pages 57\u201370. Springer,", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2010}, {"title": "A visual approach to proteomics", "author": ["S. Nickell", "C. Kofler", "A.P. Leis", "W. Baumeister"], "venue": "Nature reviews Molecular cell biology, 7(3):225\u2013230,", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "ICCV,", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of visual representations by solving jigsaw puzzles", "author": ["M. Noroozi", "P. Favaro"], "venue": "ECCV,", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2016}, {"title": "Ambient sound provides supervision for visual learning", "author": ["A. Owens", "J. Wu", "J.H. McDermott", "W.T. Freeman", "A. Torralba"], "venue": "ECCV, pages 801\u2013816,", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2016}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Kr\u00e4henb\u00fchl", "J. Donahue", "T. Darrell", "A. Efros"], "venue": "CVPR,", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent convolutional neural networks for scene parsing", "author": ["P.H. Pinheiro", "R. Collobert"], "venue": "ICML,", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2014}, {"title": "The curious robot: Learning visual representations via physical interactions", "author": ["L. Pinto", "D. Gandhi", "Y. Han", "Y. Park", "A. Gupta"], "venue": "ECCV, pages 3\u201318,", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2016}, {"title": "Postal address block location using a convolutional locator network", "author": ["J.C. Platt", "R. Wolf"], "venue": "NIPS,", "citeRegEx": "77", "shortCiteRegEx": null, "year": 1993}, {"title": "Learning to parse images of articulated bodies", "author": ["D. Ramanan"], "venue": "NIPS.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2007}, {"title": "ImageNet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "IJCV,", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2015}, {"title": "Associative hierarchical crfs for object class image segmentation", "author": ["C. Russell", "P. Kohli", "P.H. Torr"], "venue": "In ICCV. IEEE,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2009}, {"title": "3-d depth reconstruction from a single still image", "author": ["A. Saxena", "S.H. Chung", "A.Y. Ng"], "venue": "IJCV, 76(1),", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2008}, {"title": "Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context", "author": ["J. Shotton", "J. Winn", "C. Rother", "A. Criminisi"], "venue": "Int. Journal of Computer Vision (IJCV), January", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2009}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "ECCV,", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR, 15(1),", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to extract motion from videos in convolutional neural networks", "author": ["D. Teney", "M. Hebert"], "venue": "CoRR, abs/1601.07532,", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2016}, {"title": "Superparsing: scalable nonparametric image parsing with superpixels", "author": ["J. Tighe", "S. Lazebnik"], "venue": "Computer Vision\u2013ECCV 2010, pages 352\u2013365. Springer,", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2010}, {"title": "Auto-context and its application to highlevel vision tasks and 3d brain image segmentation", "author": ["Z. Tu", "X. Bai"], "venue": "TPAMI, 32(10),", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2010}, {"title": "Designing deep networks for surface normal estimation", "author": ["X. Wang", "D. Fouhey", "A. Gupta"], "venue": "CVPR,", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "ICCV 2015, pages 2794\u2013 2802,", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional pose machines", "author": ["S.-E. Wei", "V. Ramakrishna", "T. Kanade", "Y. Sheikh"], "venue": "CVPR,", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminatively trained sparse code gradients for contour detection", "author": ["R. Xiaofeng", "L. Bo"], "venue": "NIPS,", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional pseudo-prior for structured labeling", "author": ["S. Xie", "X. Huang", "Z. Tu"], "venue": "arXiv preprint arXiv:1511.07409,", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2015}, {"title": "Holistically-nested edge detection", "author": ["S. Xie", "Z. Tu"], "venue": "ICCV,", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2015}, {"title": "Automated target segmentation and real space fast alignment methods for high-throughput classification and averaging of crowded cryo-electron subtomograms", "author": ["M. Xu", "F. Alber"], "venue": "Bioinformatics, 29(13):i274\u2013i282,", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2013}, {"title": "Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation", "author": ["J. Yao", "S. Fidler", "R. Urtasun"], "venue": "CVPR. IEEE,", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "ICLR,", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2016}, {"title": "Consistent depth maps recovery from a video sequence", "author": ["G. Zhang", "J. Jia", "T.-T. Wong", "H. Bao"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(6):974\u2013 988,", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2009}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "ECCV,", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2016}, {"title": "Split-brain autoencoders: Unsupervised learning by cross-channel prediction", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "arXiv,", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H. Torr"], "venue": "ICCV,", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 53, "endOffset": 65}, {"referenceID": 63, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 53, "endOffset": 65}, {"referenceID": 93, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 53, "endOffset": 65}, {"referenceID": 4, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 83, "endOffset": 94}, {"referenceID": 29, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 83, "endOffset": 94}, {"referenceID": 85, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 83, "endOffset": 94}, {"referenceID": 5, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 142, "endOffset": 161}, {"referenceID": 23, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 142, "endOffset": 161}, {"referenceID": 24, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 142, "endOffset": 161}, {"referenceID": 80, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 142, "endOffset": 161}, {"referenceID": 88, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 142, "endOffset": 161}, {"referenceID": 12, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 212, "endOffset": 228}, {"referenceID": 35, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 212, "endOffset": 228}, {"referenceID": 77, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 212, "endOffset": 228}, {"referenceID": 90, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 212, "endOffset": 228}, {"referenceID": 42, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 247, "endOffset": 251}, {"referenceID": 15, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 279, "endOffset": 303}, {"referenceID": 26, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 279, "endOffset": 303}, {"referenceID": 39, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 279, "endOffset": 303}, {"referenceID": 61, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 279, "endOffset": 303}, {"referenceID": 66, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 279, "endOffset": 303}, {"referenceID": 81, "context": "These include low-level tasks such as edge detection [21, 64, 94] and optical flow [5, 30, 86], mid-level tasks such as depth/normal recovery [6, 24, 25, 81, 89], and high-level tasks such as keypoint prediction [13, 36, 78, 91], object detection [43], and semantic segmentation [16, 27, 40, 62, 67, 82].", "startOffset": 279, "endOffset": 303}, {"referenceID": 15, "context": "Neural networks with convolutional output predictions, also called Fully Convolutional Networks (FCNs) [16, 62, 65, 77], appear to be a promising architecture in this direction.", "startOffset": 103, "endOffset": 119}, {"referenceID": 61, "context": "Neural networks with convolutional output predictions, also called Fully Convolutional Networks (FCNs) [16, 62, 65, 77], appear to be a promising architecture in this direction.", "startOffset": 103, "endOffset": 119}, {"referenceID": 64, "context": "Neural networks with convolutional output predictions, also called Fully Convolutional Networks (FCNs) [16, 62, 65, 77], appear to be a promising architecture in this direction.", "startOffset": 103, "endOffset": 119}, {"referenceID": 76, "context": "Neural networks with convolutional output predictions, also called Fully Convolutional Networks (FCNs) [16, 62, 65, 77], appear to be a promising architecture in this direction.", "startOffset": 103, "endOffset": 119}, {"referenceID": 10, "context": ") [11].", "startOffset": 2, "endOffset": 6}, {"referenceID": 55, "context": "samples is random permutation of the training data, which can significantly improve learnability [56].", "startOffset": 97, "endOffset": 101}, {"referenceID": 44, "context": "It is well known that pixels in a given image are highly correlated and not independent [45].", "startOffset": 88, "endOffset": 92}, {"referenceID": 25, "context": "Without using any extra data, our model outperforms previous unsupervised/self-supervised approaches for semantic segmentation on PASCAL VOC-2012 [26], and is competitive to fine-tuning from pre-trained models for surface normal estimation.", "startOffset": 146, "endOffset": 150}, {"referenceID": 3, "context": "Using a single architecture and without much modification in parameters, we show state-of-the-art performance for edge detection on BSDS [4], surface normal estimation on NYUDv2 depth dataset [83], and semantic segmentation on the PASCAL-Context dataset [68].", "startOffset": 137, "endOffset": 140}, {"referenceID": 82, "context": "Using a single architecture and without much modification in parameters, we show state-of-the-art performance for edge detection on BSDS [4], surface normal estimation on NYUDv2 depth dataset [83], and semantic segmentation on the PASCAL-Context dataset [68].", "startOffset": 192, "endOffset": 196}, {"referenceID": 67, "context": "Using a single architecture and without much modification in parameters, we show state-of-the-art performance for edge detection on BSDS [4], surface normal estimation on NYUDv2 depth dataset [83], and semantic segmentation on the PASCAL-Context dataset [68].", "startOffset": 254, "endOffset": 258}, {"referenceID": 2, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 13, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 20, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 37, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 58, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 68, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 79, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 81, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 86, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 87, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 95, "context": "There is rich prior art in modeling this prediction problem using hand-designed features (representative examples include [3, 14, 21, 38, 59, 69, 80, 82, 87, 88, 96]).", "startOffset": 122, "endOffset": 165}, {"referenceID": 64, "context": "The family of fully-convolutional and skip networks [65, 77] are illustrative examples that have been successfully applied to, e.", "startOffset": 52, "endOffset": 60}, {"referenceID": 76, "context": "The family of fully-convolutional and skip networks [65, 77] are illustrative examples that have been successfully applied to, e.", "startOffset": 52, "endOffset": 60}, {"referenceID": 93, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 15, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 26, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 29, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 61, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 59, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 66, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 70, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 74, "context": ", edge detection [94] and semantic segmentation [12, 16, 27, 30, 62, 60, 67, 71, 75].", "startOffset": 48, "endOffset": 84}, {"referenceID": 15, "context": ", bilateral smoothing with fully-connected Gaussian CRFs [16, 52, 101] or bilateral solvers [8], dilated spatial convolutions [97], LSTMs [12], and convolutional pseudo priors [93].", "startOffset": 57, "endOffset": 70}, {"referenceID": 51, "context": ", bilateral smoothing with fully-connected Gaussian CRFs [16, 52, 101] or bilateral solvers [8], dilated spatial convolutions [97], LSTMs [12], and convolutional pseudo priors [93].", "startOffset": 57, "endOffset": 70}, {"referenceID": 100, "context": ", bilateral smoothing with fully-connected Gaussian CRFs [16, 52, 101] or bilateral solvers [8], dilated spatial convolutions [97], LSTMs [12], and convolutional pseudo priors [93].", "startOffset": 57, "endOffset": 70}, {"referenceID": 7, "context": ", bilateral smoothing with fully-connected Gaussian CRFs [16, 52, 101] or bilateral solvers [8], dilated spatial convolutions [97], LSTMs [12], and convolutional pseudo priors [93].", "startOffset": 92, "endOffset": 95}, {"referenceID": 96, "context": ", bilateral smoothing with fully-connected Gaussian CRFs [16, 52, 101] or bilateral solvers [8], dilated spatial convolutions [97], LSTMs [12], and convolutional pseudo priors [93].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": ", bilateral smoothing with fully-connected Gaussian CRFs [16, 52, 101] or bilateral solvers [8], dilated spatial convolutions [97], LSTMs [12], and convolutional pseudo priors [93].", "startOffset": 138, "endOffset": 142}, {"referenceID": 92, "context": ", bilateral smoothing with fully-connected Gaussian CRFs [16, 52, 101] or bilateral solvers [8], dilated spatial convolutions [97], LSTMs [12], and convolutional pseudo priors [93].", "startOffset": 176, "endOffset": 180}, {"referenceID": 18, "context": "Because such features may miss low-level details, numerous approaches have built predictors based on multiscale features extracted from multiple layers of a CNN [19, 24, 25, 27, 75, 89].", "startOffset": 161, "endOffset": 185}, {"referenceID": 23, "context": "Because such features may miss low-level details, numerous approaches have built predictors based on multiscale features extracted from multiple layers of a CNN [19, 24, 25, 27, 75, 89].", "startOffset": 161, "endOffset": 185}, {"referenceID": 24, "context": "Because such features may miss low-level details, numerous approaches have built predictors based on multiscale features extracted from multiple layers of a CNN [19, 24, 25, 27, 75, 89].", "startOffset": 161, "endOffset": 185}, {"referenceID": 26, "context": "Because such features may miss low-level details, numerous approaches have built predictors based on multiscale features extracted from multiple layers of a CNN [19, 24, 25, 27, 75, 89].", "startOffset": 161, "endOffset": 185}, {"referenceID": 74, "context": "Because such features may miss low-level details, numerous approaches have built predictors based on multiscale features extracted from multiple layers of a CNN [19, 24, 25, 27, 75, 89].", "startOffset": 161, "endOffset": 185}, {"referenceID": 88, "context": "Because such features may miss low-level details, numerous approaches have built predictors based on multiscale features extracted from multiple layers of a CNN [19, 24, 25, 27, 75, 89].", "startOffset": 161, "endOffset": 185}, {"referenceID": 39, "context": "[40] use the evocative term \u201chypercolumns\u201d to refer to features extracted from multiple layers that correspond to the same pixel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "Prior techniques for up-sampling include shift and stitch [62], converting convolutional filters to dilation operations [16] (inspired by the algorithme \u00e0 trous [63]), and deconvolution/unpooling [30, 62, 71].", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "Prior techniques for up-sampling include shift and stitch [62], converting convolutional filters to dilation operations [16] (inspired by the algorithme \u00e0 trous [63]), and deconvolution/unpooling [30, 62, 71].", "startOffset": 120, "endOffset": 124}, {"referenceID": 62, "context": "Prior techniques for up-sampling include shift and stitch [62], converting convolutional filters to dilation operations [16] (inspired by the algorithme \u00e0 trous [63]), and deconvolution/unpooling [30, 62, 71].", "startOffset": 161, "endOffset": 165}, {"referenceID": 29, "context": "Prior techniques for up-sampling include shift and stitch [62], converting convolutional filters to dilation operations [16] (inspired by the algorithme \u00e0 trous [63]), and deconvolution/unpooling [30, 62, 71].", "startOffset": 196, "endOffset": 208}, {"referenceID": 61, "context": "Prior techniques for up-sampling include shift and stitch [62], converting convolutional filters to dilation operations [16] (inspired by the algorithme \u00e0 trous [63]), and deconvolution/unpooling [30, 62, 71].", "startOffset": 196, "endOffset": 208}, {"referenceID": 70, "context": "Prior techniques for up-sampling include shift and stitch [62], converting convolutional filters to dilation operations [16] (inspired by the algorithme \u00e0 trous [63]), and deconvolution/unpooling [30, 62, 71].", "startOffset": 196, "endOffset": 208}, {"referenceID": 61, "context": "FCNs [62] point out that linear prediction can be efficiently implemented in a coarse-to-fine manner by upsampling coarse predictions (with deconvolution) rather than upsampling coarse features.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "DeepLab [16] incorporates filter dilation and applies similar deconvolution and linear-weighted fusion, in addition to reducing the dimensionality of the fully-connected layers to reduce memory footprint.", "startOffset": 8, "endOffset": 12}, {"referenceID": 59, "context": "ParseNet [60] added spatial context for a layer\u2019s responses by average pooling the feature responses, followed by normalization and concatenation.", "startOffset": 9, "endOffset": 13}, {"referenceID": 93, "context": "HED [94] output edge predictions from intermediate layers, which are deeply supervised, and fuses the predictions by linear weighting.", "startOffset": 4, "endOffset": 8}, {"referenceID": 66, "context": "Importantly, [67] and [27] are noteable exceptions to the linear trend in that non-linear predictors g are used.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "Importantly, [67] and [27] are noteable exceptions to the linear trend in that non-linear predictors g are used.", "startOffset": 22, "endOffset": 26}, {"referenceID": 66, "context": "This does pose difficulties during learning - [67] precomputes and stores superpixel feature maps due to memory constraints, and so cannot be trained end-to-end.", "startOffset": 46, "endOffset": 50}, {"referenceID": 5, "context": "Our insight is inspired by past approaches that use sampling to training networks for surface normal estimation [6] and image colorization [55], though we focus on general design principles by analyzing the impact of sampling for efficiency, accuracy, and tabula rasa learning for diverse tasks.", "startOffset": 112, "endOffset": 115}, {"referenceID": 54, "context": "Our insight is inspired by past approaches that use sampling to training networks for surface normal estimation [6] and image colorization [55], though we focus on general design principles by analyzing the impact of sampling for efficiency, accuracy, and tabula rasa learning for diverse tasks.", "startOffset": 139, "endOffset": 143}, {"referenceID": 10, "context": "We refer the reader to [11] for an excellent introduction.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "Though naturally a sequential algorithm that processes one data example at a time, much recent work focuses on mini-batch methods that can exploit parallelism in GPU architectures [18] or clusters [18].", "startOffset": 180, "endOffset": 184}, {"referenceID": 17, "context": "Though naturally a sequential algorithm that processes one data example at a time, much recent work focuses on mini-batch methods that can exploit parallelism in GPU architectures [18] or clusters [18].", "startOffset": 197, "endOffset": 201}, {"referenceID": 9, "context": "One general theme is efficient online approximation of second-order methods [10], which can model correlations between input features.", "startOffset": 76, "endOffset": 80}, {"referenceID": 45, "context": "Batch normalization [46] computes correlation statistics between samples in a batch, producing noticeable improvements in convergence speed.", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "We learn a nonlinear predictor f\u03b8,p = g(hp) implemented as a multi-layer perceptron (MLP) [9] defined over hypercolumn features.", "startOffset": 90, "endOffset": 93}, {"referenceID": 61, "context": "Approaches based on FCN [62] include features for all pixels from an image in a mini-batch.", "startOffset": 24, "endOffset": 28}, {"referenceID": 44, "context": "As nearby pixels in an image are highly correlated [45], sampling them will not hurt learning.", "startOffset": 51, "endOffset": 55}, {"referenceID": 83, "context": "Default network: For most experiments we fine-tune a VGG-16 network [84].", "startOffset": 68, "endOffset": 72}, {"referenceID": 61, "context": "Following [62], we transform the last two fc layers to convolutional filters1, and add them to the set of convolutional features that can be aggregated into our multiscale hypercolumn descriptor.", "startOffset": 10, "endOffset": 14}, {"referenceID": 52, "context": "We define a MLP over hypercolumn features with 3 fully-connected (fc) layers of size 4, 096 followed by ReLU [53] activations, where the last layer outputs predictions for K classes (with a softmax/cross-entropy loss) or K outputs with a euclidean loss for regression.", "startOffset": 109, "endOffset": 113}, {"referenceID": 25, "context": "Semantic Segmentation: We use training images from PASCAL VOC-2012 [26] for semantic segmentation, and additional labels collected on 8498 images by Hariharan et al.", "startOffset": 67, "endOffset": 71}, {"referenceID": 40, "context": "[41].", "startOffset": 0, "endOffset": 4}, {"referenceID": 82, "context": "Surface Normal Estimation: The NYU Depth v2 dataset [83] is used to evaluate the surface normal maps.", "startOffset": 52, "endOffset": 56}, {"referenceID": 53, "context": "[54] and Wang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 88, "context": "[89], computed from depth data of Kinect, as ground truth for 1449 images and 220K images respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 23, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 30, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 31, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 32, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 88, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 61, "context": "[62], who similarly examine the effect of sampling a fraction (25-50%) of patches per training image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[62] also perform an additional experiment where the total number of pixels in a batch is kept constant when comparing different sampling strategies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "Similar observation was made by [60].", "startOffset": 32, "endOffset": 36}, {"referenceID": 59, "context": "To counter this issue, previous work has explored normalization [60], scaling [40], etc.", "startOffset": 64, "endOffset": 68}, {"referenceID": 39, "context": "To counter this issue, previous work has explored normalization [60], scaling [40], etc.", "startOffset": 78, "endOffset": 82}, {"referenceID": 45, "context": "We use batch-normalization [46] for the convolutional layers before concatenating them to properly train a model with a linear predictor.", "startOffset": 27, "endOffset": 31}, {"referenceID": 39, "context": "Note that performance of linear model (with batch-normalization) is similar to one obtained by Hypercolum [40] (62.", "startOffset": 106, "endOffset": 110}, {"referenceID": 61, "context": "7%), and FCN [62] (62%).", "startOffset": 13, "endOffset": 17}, {"referenceID": 61, "context": "FCN-32s [62] 4,096 50,176 2,010 518 20.", "startOffset": 8, "endOffset": 12}, {"referenceID": 61, "context": "0 FCN-8s [62] 4,864 50,176 2,056 570 19.", "startOffset": 9, "endOffset": 13}, {"referenceID": 61, "context": "We compared our network with FCN [62] where a deconvolution layer is used to upsample the result in various settings.", "startOffset": 33, "endOffset": 37}, {"referenceID": 61, "context": "Besides FCN-8s and FCN-32s here we first compute the upsampled feature map, and then apply the classifiers for FCN [62].", "startOffset": 115, "endOffset": 119}, {"referenceID": 61, "context": "[62].", "startOffset": 0, "endOffset": 4}, {"referenceID": 78, "context": ", ImageNet [79]) model as initialization for fine-tuning for the task at hand.", "startOffset": 11, "endOffset": 15}, {"referenceID": 69, "context": "[70, 95]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 94, "context": "[70, 95]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 0, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 19, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 22, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 33, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 36, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 47, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 56, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 54, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 65, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 71, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 72, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 73, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 75, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 89, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 98, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 99, "context": "We will show that our results also have implications for unsupervised representation learning [1, 20, 23, 34, 37, 48, 57, 55, 66, 72, 73, 74, 76, 90, 99, 100].", "startOffset": 94, "endOffset": 158}, {"referenceID": 83, "context": "Training a VGG-16 network architecture is not straight forward, and required stage-wise training for the image classification task [84].", "startOffset": 131, "endOffset": 135}, {"referenceID": 19, "context": "To the best of our knowledge, these are the best numbers reported on these two tasks when trained from scratch, and exceeds the performance of other unsupervised/self-supervised approaches [20, 55, 74, 90, 99] that required extra ImageNet data [79].", "startOffset": 189, "endOffset": 209}, {"referenceID": 54, "context": "To the best of our knowledge, these are the best numbers reported on these two tasks when trained from scratch, and exceeds the performance of other unsupervised/self-supervised approaches [20, 55, 74, 90, 99] that required extra ImageNet data [79].", "startOffset": 189, "endOffset": 209}, {"referenceID": 73, "context": "To the best of our knowledge, these are the best numbers reported on these two tasks when trained from scratch, and exceeds the performance of other unsupervised/self-supervised approaches [20, 55, 74, 90, 99] that required extra ImageNet data [79].", "startOffset": 189, "endOffset": 209}, {"referenceID": 89, "context": "To the best of our knowledge, these are the best numbers reported on these two tasks when trained from scratch, and exceeds the performance of other unsupervised/self-supervised approaches [20, 55, 74, 90, 99] that required extra ImageNet data [79].", "startOffset": 189, "endOffset": 209}, {"referenceID": 98, "context": "To the best of our knowledge, these are the best numbers reported on these two tasks when trained from scratch, and exceeds the performance of other unsupervised/self-supervised approaches [20, 55, 74, 90, 99] that required extra ImageNet data [79].", "startOffset": 189, "endOffset": 209}, {"referenceID": 78, "context": "To the best of our knowledge, these are the best numbers reported on these two tasks when trained from scratch, and exceeds the performance of other unsupervised/self-supervised approaches [20, 55, 74, 90, 99] that required extra ImageNet data [79].", "startOffset": 244, "endOffset": 248}, {"referenceID": 19, "context": "It is best known result for semantic segmentation in an unsupervised/self-supervised manner, and is competitive with the previous unsupervised work [20] on object detection2 that uses ImageNet (without labels), particularly on indoor scene furniture categories (e.", "startOffset": 148, "endOffset": 152}, {"referenceID": 97, "context": "[98]) and use them to train models for surface normal estimation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] reports better results in a recent version by the use of multi-scale detection, and smarter initialization and rescaling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "DPM-v5 [28] 33.", "startOffset": 7, "endOffset": 11}, {"referenceID": 6, "context": "HOG+MID [7] 51.", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "RCNN-Scratch [2] 49.", "startOffset": 13, "endOffset": 16}, {"referenceID": 19, "context": "VGG-16-Scratch [20] 56.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "VGG-16-Context-v2 [20] 63.", "startOffset": 18, "endOffset": 22}, {"referenceID": 34, "context": "VGG-16-ImageNet [35] 73.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Evaluation on VOC-2007: Our model (VGG-16-Geometry) trained on a few indoor scene examples of NYU-depth dataset performs 9% better than scratch, and is competitive with [20] that used images from ImageNet (without labels) to train.", "startOffset": 169, "endOffset": 173}, {"referenceID": 19, "context": "[20] required training for around 8 weeks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "Training: For all the experiments we used the publicly available Caffe library [49].", "startOffset": 79, "endOffset": 83}, {"referenceID": 84, "context": "We make use of ImageNet-pretrained values for all convolutional layers, but train our MLP layers \u201cfrom scratch\u201d with Gaussian initialization (\u03c3 = 10\u22123) and dropout [85] (r = 0.", "startOffset": 164, "endOffset": 168}, {"referenceID": 3, "context": "Dataset: The PASCAL-Context dataset [4] augments the original sparse set of PASCAL VOC 2010 segmentation annotations [26] (defined for 20 categories) to pixel labels for the whole scene.", "startOffset": 36, "endOffset": 39}, {"referenceID": 25, "context": "Dataset: The PASCAL-Context dataset [4] augments the original sparse set of PASCAL VOC 2010 segmentation annotations [26] (defined for 20 categories) to pixel labels for the whole scene.", "startOffset": 117, "endOffset": 121}, {"referenceID": 25, "context": "The results for PASCAL VOC-2012 dataset [26] are in Appendix A.", "startOffset": 40, "endOffset": 44}, {"referenceID": 60, "context": "FCN-8s [61] 46.", "startOffset": 7, "endOffset": 11}, {"referenceID": 61, "context": "5 FCN-8s [62] 50.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "8 DeepLab (v2 [15]) 37.", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "DeepLab (v2) + CRF [15] 39.", "startOffset": 19, "endOffset": 23}, {"referenceID": 100, "context": "6 CRF-RNN [101] 39.", "startOffset": 10, "endOffset": 15}, {"referenceID": 92, "context": "3 ConvPP-8 [93] 41.", "startOffset": 11, "endOffset": 15}, {"referenceID": 3, "context": "Evaluation on PASCAL-Context [4]: Most recent approaches [15, 93, 101] except FCN-8s, use spatial context postprocessing.", "startOffset": 29, "endOffset": 32}, {"referenceID": 14, "context": "Evaluation on PASCAL-Context [4]: Most recent approaches [15, 93, 101] except FCN-8s, use spatial context postprocessing.", "startOffset": 57, "endOffset": 70}, {"referenceID": 92, "context": "Evaluation on PASCAL-Context [4]: Most recent approaches [15, 93, 101] except FCN-8s, use spatial context postprocessing.", "startOffset": 57, "endOffset": 70}, {"referenceID": 100, "context": "Evaluation on PASCAL-Context [4]: Most recent approaches [15, 93, 101] except FCN-8s, use spatial context postprocessing.", "startOffset": 57, "endOffset": 70}, {"referenceID": 61, "context": "Due to space constraints, we show only one example output in Figure 6 and compare against FCN-8s [62].", "startOffset": 97, "endOffset": 101}, {"referenceID": 5, "context": "We improve the stateof-the-art for surface normal estimation [6] using the analysis for general pixel-level optimization.", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "[6] extracted hypercolumn features from 1\u00d71\u00d74096 conv7 of VGG-16, we provided sufficient padding at conv-6 to", "startOffset": 0, "endOffset": 3}, {"referenceID": 93, "context": "Notice that our approach generates more semantic edges for zebra compared to HED [94].", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "[31] 35.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "E-F (VGG-16) [24] 20.", "startOffset": 13, "endOffset": 17}, {"referenceID": 50, "context": "UberNet (1-Task) [51] 21.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "MarrRevisited [6] 19.", "startOffset": 14, "endOffset": 17}, {"referenceID": 82, "context": "Evaluation on NYUv2 depth dataset [83]: We improve the previous state-of-the-art [6] using the analysis from general pixel-level prediction problems, and multi-scale prediction.", "startOffset": 34, "endOffset": 38}, {"referenceID": 5, "context": "Evaluation on NYUv2 depth dataset [83]: We improve the previous state-of-the-art [6] using the analysis from general pixel-level prediction problems, and multi-scale prediction.", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "Results: Table 13 compares our improved results with previous state-of-the-art approaches [6, 24].", "startOffset": 90, "endOffset": 97}, {"referenceID": 23, "context": "Results: Table 13 compares our improved results with previous state-of-the-art approaches [6, 24].", "startOffset": 90, "endOffset": 97}, {"referenceID": 3, "context": "Dataset: The standard dataset for edge detection is BSDS500 [4], which consists of 200 training, 100 validation, and 200 testing images.", "startOffset": 60, "endOffset": 63}, {"referenceID": 93, "context": "We use the same augmented data (rotation, flipping, totaling 9600 images without resizing) used to train the state-of-the-art Holistically-nested edge detector (HED) [94].", "startOffset": 166, "endOffset": 170}, {"referenceID": 93, "context": "Due to the highly skewed class distribution, we also normalized the gradients for positives and negatives in each batch (as in [94]).", "startOffset": 127, "endOffset": 131}, {"referenceID": 3, "context": "Human [4] .", "startOffset": 6, "endOffset": 9}, {"referenceID": 21, "context": "SE-Var [22] .", "startOffset": 7, "endOffset": 11}, {"referenceID": 38, "context": "803 OEF [39] .", "startOffset": 8, "endOffset": 12}, {"referenceID": 49, "context": "DeepNets [50] .", "startOffset": 9, "endOffset": 13}, {"referenceID": 43, "context": "758 CSCNN [44] .", "startOffset": 10, "endOffset": 14}, {"referenceID": 93, "context": "798 HED [94] (Updated version) .", "startOffset": 8, "endOffset": 12}, {"referenceID": 50, "context": "840 UberNet (1-Task) [51] .", "startOffset": 21, "endOffset": 25}, {"referenceID": 3, "context": "Evaluation on BSDS [4]: Our approach performs better than previous approaches specifically trained for edge detection.", "startOffset": 19, "endOffset": 22}, {"referenceID": 67, "context": "In Section A we present extended analysis of PixelNet architecture for semantic segmentation on PASCAL Context [68] and PASCAL VOC-2012 [26], and ablative analysis for parameter selection.", "startOffset": 111, "endOffset": 115}, {"referenceID": 25, "context": "In Section A we present extended analysis of PixelNet architecture for semantic segmentation on PASCAL Context [68] and PASCAL VOC-2012 [26], and ablative analysis for parameter selection.", "startOffset": 136, "endOffset": 140}, {"referenceID": 82, "context": "In Section B we show comparison of improved surface normal with previous state-of-theart approaches on NYU-v2 depth dataset [83].", "startOffset": 124, "endOffset": 128}, {"referenceID": 3, "context": "In Section C we compare our approach with prior work on edge detection on BSDS [4].", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "The PASCAL-Context dataset [4] augments the original sparse set of PASCAL VOC 2010 segmentation annotations [26] (defined for 20 categories) to pixel labels for the whole scene.", "startOffset": 27, "endOffset": 30}, {"referenceID": 25, "context": "The PASCAL-Context dataset [4] augments the original sparse set of PASCAL VOC 2010 segmentation annotations [26] (defined for 20 categories) to pixel labels for the whole scene.", "startOffset": 108, "endOffset": 112}, {"referenceID": 25, "context": "We also evaluated our approach on the standard PASCAL VOC-2012 dataset [26] to compare with a wide variety of approaches.", "startOffset": 71, "endOffset": 75}, {"referenceID": 48, "context": "For all the experiments we used the publicly available Caffe library [49].", "startOffset": 69, "endOffset": 73}, {"referenceID": 84, "context": "We make use of ImageNet-pretrained values for all convolutional layers, but train our MLP layers \u201cfrom scratch\u201d with Gaussian initialization (\u03c3 = 10\u22123) and dropout [85].", "startOffset": 164, "endOffset": 168}, {"referenceID": 61, "context": "We show qualitative outputs in Figure 6 and compare against FCN-8s [62].", "startOffset": 67, "endOffset": 71}, {"referenceID": 84, "context": "We can see that with more dimensions the network tends to learn better, potentially because it can capture more information (and with drop-out alleviating overfitting [85]).", "startOffset": 167, "endOffset": 171}, {"referenceID": 61, "context": "For example, a single-scale FCN-32s [62], without any lowlevel layers, can already achieve 35.", "startOffset": 36, "endOffset": 40}, {"referenceID": 61, "context": "Even with reduced scale, we are able to obtain a similar IU achieved by FCN-8s [62], without any extra modeling of context [16, 60, 93, 101].", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "Even with reduced scale, we are able to obtain a similar IU achieved by FCN-8s [62], without any extra modeling of context [16, 60, 93, 101].", "startOffset": 123, "endOffset": 140}, {"referenceID": 59, "context": "Even with reduced scale, we are able to obtain a similar IU achieved by FCN-8s [62], without any extra modeling of context [16, 60, 93, 101].", "startOffset": 123, "endOffset": 140}, {"referenceID": 92, "context": "Even with reduced scale, we are able to obtain a similar IU achieved by FCN-8s [62], without any extra modeling of context [16, 60, 93, 101].", "startOffset": 123, "endOffset": 140}, {"referenceID": 100, "context": "Even with reduced scale, we are able to obtain a similar IU achieved by FCN-8s [62], without any extra modeling of context [16, 60, 93, 101].", "startOffset": 123, "endOffset": 140}, {"referenceID": 15, "context": "5\u00d7 its original size), whereas most prior work [16, 60, 62, 101] use multiple scales from full-resolution images.", "startOffset": 47, "endOffset": 64}, {"referenceID": 59, "context": "5\u00d7 its original size), whereas most prior work [16, 60, 62, 101] use multiple scales from full-resolution images.", "startOffset": 47, "endOffset": 64}, {"referenceID": 61, "context": "5\u00d7 its original size), whereas most prior work [16, 60, 62, 101] use multiple scales from full-resolution images.", "startOffset": 47, "endOffset": 64}, {"referenceID": 100, "context": "5\u00d7 its original size), whereas most prior work [16, 60, 62, 101] use multiple scales from full-resolution images.", "startOffset": 47, "endOffset": 64}, {"referenceID": 14, "context": "Note our pixel-wise predictions do not make use of contextual post-processing (even outperforming some methods that post-processes FCNs to do so [15, 101]).", "startOffset": 145, "endOffset": 154}, {"referenceID": 100, "context": "Note our pixel-wise predictions do not make use of contextual post-processing (even outperforming some methods that post-processes FCNs to do so [15, 101]).", "startOffset": 145, "endOffset": 154}, {"referenceID": 25, "context": "Evaluation on PASCAL VOC-2012 [26].", "startOffset": 30, "endOffset": 34}, {"referenceID": 39, "context": "7% for Hypercolumns [40], 62% for FCN [62], 67% for DeepLab (without CRF) [16] etc.", "startOffset": 20, "endOffset": 24}, {"referenceID": 61, "context": "7% for Hypercolumns [40], 62% for FCN [62], 67% for DeepLab (without CRF) [16] etc.", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "7% for Hypercolumns [40], 62% for FCN [62], 67% for DeepLab (without CRF) [16] etc.", "startOffset": 74, "endOffset": 78}, {"referenceID": 66, "context": "is similar to Mostajabi et al [67] despite the fact we use information from only 6 layers while they used information from all the layers.", "startOffset": 30, "endOffset": 34}, {"referenceID": 66, "context": "Finally, the use of super-pixels in [67] inhibit capturing detailed segmentation mask (and rather gives \u201cblobby\u201d output), and it is computationally less-tractable to use their approach for per-pixel optimization as information for each pixel would be required to be stored on disk.", "startOffset": 36, "endOffset": 40}, {"referenceID": 82, "context": "The NYU Depth v2 dataset [83] is used to evaluate the surface normal maps.", "startOffset": 25, "endOffset": 29}, {"referenceID": 53, "context": "[54] and Wang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 88, "context": "[89], computed from depth data of Kinect, as ground truth for 1449 images and 220K images respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 23, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 30, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 31, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 32, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 88, "context": "We compute six statistics, previously used by [6, 24, 31, 32, 33, 89], over the angular error between the predicted normals and depth-based normals to evaluate the performance \u2013 Mean, Median, RMSE, 11.", "startOffset": 46, "endOffset": 69}, {"referenceID": 5, "context": "[6], and present our results both with and without Manhattan-world rectification to fairly compare against previous approaches, as [31, 32, 89] use it and [24] do not.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[6], and present our results both with and without Manhattan-world rectification to fairly compare against previous approaches, as [31, 32, 89] use it and [24] do not.", "startOffset": 131, "endOffset": 143}, {"referenceID": 31, "context": "[6], and present our results both with and without Manhattan-world rectification to fairly compare against previous approaches, as [31, 32, 89] use it and [24] do not.", "startOffset": 131, "endOffset": 143}, {"referenceID": 88, "context": "[6], and present our results both with and without Manhattan-world rectification to fairly compare against previous approaches, as [31, 32, 89] use it and [24] do not.", "startOffset": 131, "endOffset": 143}, {"referenceID": 23, "context": "[6], and present our results both with and without Manhattan-world rectification to fairly compare against previous approaches, as [31, 32, 89] use it and [24] do not.", "startOffset": 155, "endOffset": 159}, {"referenceID": 41, "context": "[42].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6], our approach performs worse with Manhattan-world rectification (unlike Fouhey et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[31]) though it improves slightly on this criteria as well.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] stressed the importance of fine details in the scene generally available around objects.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "We followed their [6] local object evaluation that considers only those pixels which belong to a particular object (such as chair, sofa and bed).", "startOffset": 18, "endOffset": 21}, {"referenceID": 88, "context": "[89], Eigen and Fergus [24], and MarrRevisited (Bansal et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[89], Eigen and Fergus [24], and MarrRevisited (Bansal et al.", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "[6]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The standard dataset for edge detection is BSDS500 [4], which consists of 200 training, 100 validation, and 200 testing images.", "startOffset": 51, "endOffset": 54}, {"referenceID": 93, "context": "We use the same augmented data (rotation, flipping, totaling 9600 images without resizing) used to train the state-of-the-art Holistically-nested edge detector (HED) [94].", "startOffset": 166, "endOffset": 170}, {"referenceID": 93, "context": "Due to the highly skewed class distribution, we also normalized the gradients for positives and negatives in each batch (as in [94]).", "startOffset": 127, "endOffset": 131}, {"referenceID": 60, "context": "FCN-8s [61] 46.", "startOffset": 7, "endOffset": 11}, {"referenceID": 61, "context": "5 FCN-8s [62] 50.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "8 DeepLab (v2 [15]) - 37.", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "DeepLab (v2) + CRF [15] - 39.", "startOffset": 19, "endOffset": 23}, {"referenceID": 100, "context": "6 CRF-RNN [101] - 39.", "startOffset": 10, "endOffset": 15}, {"referenceID": 59, "context": "3 ParseNet [60] - 40.", "startOffset": 11, "endOffset": 15}, {"referenceID": 92, "context": "4 ConvPP-8 [93] - 41.", "startOffset": 11, "endOffset": 15}, {"referenceID": 14, "context": "Note that while most recent approaches spatial context postprocessing [15, 60, 93, 101], we focus on the FCN [62] per-pixel predictor as most approaches are its descendants.", "startOffset": 70, "endOffset": 87}, {"referenceID": 59, "context": "Note that while most recent approaches spatial context postprocessing [15, 60, 93, 101], we focus on the FCN [62] per-pixel predictor as most approaches are its descendants.", "startOffset": 70, "endOffset": 87}, {"referenceID": 92, "context": "Note that while most recent approaches spatial context postprocessing [15, 60, 93, 101], we focus on the FCN [62] per-pixel predictor as most approaches are its descendants.", "startOffset": 70, "endOffset": 87}, {"referenceID": 100, "context": "Note that while most recent approaches spatial context postprocessing [15, 60, 93, 101], we focus on the FCN [62] per-pixel predictor as most approaches are its descendants.", "startOffset": 70, "endOffset": 87}, {"referenceID": 61, "context": "Note that while most recent approaches spatial context postprocessing [15, 60, 93, 101], we focus on the FCN [62] per-pixel predictor as most approaches are its descendants.", "startOffset": 109, "endOffset": 113}, {"referenceID": 5, "context": "[6]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "We apply standard non-maximal suppression and thinning technique using the code provided by [21].", "startOffset": 92, "endOffset": 96}, {"referenceID": 61, "context": "Whereas uniform sampling sufficed for semantic segmentation [62], we found the extreme rarity of positive pixels in edge detection required focused sampling of positives.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "Two obvious approaches are uniform and balanced sampling with an equal ratio of positives and negatives (shown to be useful for object detection [17, 35]).", "startOffset": 145, "endOffset": 153}, {"referenceID": 34, "context": "Two obvious approaches are uniform and balanced sampling with an equal ratio of positives and negatives (shown to be useful for object detection [17, 35]).", "startOffset": 145, "endOffset": 153}, {"referenceID": 93, "context": "6Note that simple class balancing [94] in each batch is already used, so the performance gain is unlikely from label re-balancing.", "startOffset": 34, "endOffset": 38}, {"referenceID": 30, "context": "[31] 35.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "E-F (VGG-16) [24] 20.", "startOffset": 13, "endOffset": 17}, {"referenceID": 50, "context": "UberNet (1-Task) [51] 21.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "MarrRevisited [6] 19.", "startOffset": 14, "endOffset": 17}, {"referenceID": 88, "context": "[89] 26.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] 35.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] 36.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "MarrRevisited [6] 23.", "startOffset": 14, "endOffset": 17}, {"referenceID": 82, "context": "Evaluation on NYUv2 depth dataset [83]: Global Scene Layout.", "startOffset": 34, "endOffset": 38}, {"referenceID": 5, "context": "We improve the previous state-of-the-art [6] using the analysis from general pixel-level prediction problems, and multi-scale prediction.", "startOffset": 41, "endOffset": 44}, {"referenceID": 88, "context": "[89] 44.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "E-F (AlexNet) [24] 38.", "startOffset": 14, "endOffset": 18}, {"referenceID": 23, "context": "E-F (VGG-16) [24] 33.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "MarrRevisited [6] 32.", "startOffset": 14, "endOffset": 17}, {"referenceID": 88, "context": "[89] 36.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "E-F (AlexNet) [24] 27.", "startOffset": 14, "endOffset": 18}, {"referenceID": 23, "context": "E-F (VGG-16) [24] 21.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "MarrRevisited [6] 20.", "startOffset": 14, "endOffset": 17}, {"referenceID": 88, "context": "[89] 28.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "E-F (AlexNet) [24] 23.", "startOffset": 14, "endOffset": 18}, {"referenceID": 23, "context": "E-F (VGG-16) [24] 19.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "MarrRevisited [6] 19.", "startOffset": 14, "endOffset": 17}, {"referenceID": 82, "context": "Evaluation on NYUv2 depth dataset [83]: Local Object Layout.", "startOffset": 34, "endOffset": 38}, {"referenceID": 3, "context": "Human [4] .", "startOffset": 6, "endOffset": 9}, {"referenceID": 28, "context": "580 Felz-Hutt [29] .", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "gPb-owt-ucm [4] .", "startOffset": 12, "endOffset": 15}, {"referenceID": 57, "context": "696 Sketch Tokens [58] .", "startOffset": 18, "endOffset": 22}, {"referenceID": 91, "context": "780 SCG [92] .", "startOffset": 8, "endOffset": 12}, {"referenceID": 46, "context": "PMI [47] .", "startOffset": 4, "endOffset": 8}, {"referenceID": 21, "context": "SE-Var [22] .", "startOffset": 7, "endOffset": 11}, {"referenceID": 38, "context": "803 OEF [39] .", "startOffset": 8, "endOffset": 12}, {"referenceID": 49, "context": "DeepNets [50] .", "startOffset": 9, "endOffset": 13}, {"referenceID": 43, "context": "758 CSCNN [44] .", "startOffset": 10, "endOffset": 14}, {"referenceID": 93, "context": "798 HED [94] .", "startOffset": 8, "endOffset": 12}, {"referenceID": 93, "context": "833 HED [94] (Updated version) .", "startOffset": 8, "endOffset": 12}, {"referenceID": 93, "context": "811 HED merging [94] (Updated version) .", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "Evaluation on BSDS [4].", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "Results on BSDS [4].", "startOffset": 16, "endOffset": 19}, {"referenceID": 93, "context": "Notice that our approach generates more semantic edges for zebra, eagle, and giraffe compared to HED [94].", "startOffset": 101, "endOffset": 105}], "year": 2017, "abstractText": "We explore design principles for general pixel-level prediction problems, from low-level edge detection to midlevel surface normal estimation to high-level semantic segmentation. Convolutional predictors, such as the fullyconvolutional network (FCN), have achieved remarkable success by exploiting the spatial redundancy of neighboring pixels through convolutional processing. Though computationally efficient, we point out that such approaches are not statistically efficient during learning precisely because spatial redundancy limits the information learned from neighboring pixels. We demonstrate that stratified sampling of pixels allows one to (1) add diversity during batch updates, speeding up learning; (2) explore complex nonlinear predictors, improving accuracy; and (3) efficiently train state-of-the-art models tabula rasa (i.e., \u201cfrom scratch\u201d) for diverse pixel-labeling tasks. Our single architecture produces state-of-the-art results for semantic segmentation on PASCAL-Context dataset, surface normal estimation on NYUDv2 depth dataset, and edge detection on BSDS.", "creator": "LaTeX with hyperref package"}}}