{"id": "1702.06589", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Neural Multi-Step Reasoning for Question Answering on Semi-Structured Tables", "abstract": "Advances in natural language processing tasks have gained momentum in recent years due to the increasingly popular neural network methods. In this paper, we explore deep learning techniques for answering multi-step reasoning questions that operate on semi-structured tables. Challenges here arise from the level of logical compositionality expressed by questions, as well as the domain openness. Our approach is weakly supervised, trained on question-answer-table triples without requiring intermediate strong supervision. It performs two phases: first, machine understandable logical forms (programs) are generated from natural language questions following the work of [Pasupat and Liang, 2015]. Second, paraphrases of logical forms and questions are embedded in a jointly learned vector space using word and character convolutional neural networks. A neural scoring function is further used to rank and retrieve the most probable logical form (interpretation) of a question. Our best single model achieves 34.8% accuracy on the WikiTableQuestions dataset, while the best ensemble of our models pushes the state-of-the-art score on this task to 38.7%, thus slightly surpassing both the engineered feature scoring baseline, as well as the Neural Programmer model of [Neelakantan et al., 2016].", "histories": [["v1", "Tue, 21 Feb 2017 21:24:26 GMT  (183kb,D)", "http://arxiv.org/abs/1702.06589v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["till haug", "octavian-eugen ganea", "paulina grnarova"], "accepted": false, "id": "1702.06589"}, "pdf": {"name": "1702.06589.pdf", "metadata": {"source": "CRF", "title": "Neural Multi-Step Reasoning for Question Answering on Semi-Structured Tables", "authors": ["Till Haug", "Octavian-Eugen Ganea", "Paulina Grnarova"], "emails": ["till@veezoo.com", "octavian.ganea@inf.ethz.ch", "paulina.grnarova@inf.ethz.ch"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point that it will be able to erenen.n"}, {"heading": "2 Related Work", "text": "In fact, the fact is that most of them will be able to put themselves in a situation where they are able to survive themselves, where they are able to put themselves in a situation where they put themselves in a situation where they are able to put themselves in a situation where they are able to unfold, and where they put themselves in a situation where they put themselves in a situation where they have entangled themselves in a situation where they have entangled themselves in another."}, {"heading": "3 Model", "text": "We now proceed to the detailed description of our QA system. In short, our model goes through the following phases. For each question q: i) a series of logical forms of the candidate {zi} i-Iq is generated using the method of [Pasupat and Liang, 2015]; ii) each such candidate program zi is described in a textual representation ti that provides precision, interpretability and comprehensibility; iii) all textual forms ti are evaluated against the input query q by means of a neural network model; iv) the logical form z \u0445 i, which corresponds to the highest rank, is selected as a machine-comprehensible translation of the question q; v) z-i is executed on the input table and its response is returned to the user. Our contributions are the novel models that perform the steps ii) and iii) while we rely on the work of [Pasupat and Liang, 2015] for i), iv) and iv)."}, {"heading": "3.1 Candidate Logical Form Generation", "text": "We use the method of PL2015 to generate a series of candidates logical forms relating to a question. Specifically, the + operation is defined as string concatenation with spaces. Details about lambda DCS language can be found in [Liang, 2013].1: Procedure PARAPHRASE (z). z is the root of a lambda DCS logical form 2: Switch z: case aggregation. e.g. number, max, min... 4: t \u2190 AGGREGATION (z) + PARAPHRASE (z). z is the root of a lambda DCS logical form 2: Switch z do 3: case aggregation. e.g. number, min... 4: t \u2190 AGGREGATION (z) + PARAPHRASS logical form (z) 5: Fall join relationships occur, e.g. Qx.Country (x, Australia) bAPHRASE: 6: t \u2190 PARAPHRASS (z.Relation) + PARDAz.7... Case reversal......"}, {"heading": "3.2 Converting Logical Forms to Text", "text": "We describe our proposed paraphrasing algorithm for converting logical forms into textual representations. These, and not the original logical forms, are further evaluated against the input request. In addition to the advantages of interpretability and intelligibility, we also note a gain in quality when using paraphrases based directly on the string representations of the original Lambda DCS expressions (details in Section 4.4). Usability and extensibility of QA systems can benefit from the disclosure of the translation of the human-linguistic question into machine language (e.g. Lambda DCS in our case), which is called transparency, i.e. the property of revealing the generated program from an input request in its raw format. However, this could not achieve intelligibility, the characteristics of a system being comprehensible to non-technical users. Transcribing the logical form z into a textual representation t fulfils both of these characteristics, since there is a non-knowledgeable description of the input request carried out on the basis of a program."}, {"heading": "3.3 Joint Embedding Model", "text": "For each question q, we generate a series of logical forms zi and apply algorithm 1 to retrieve their corresponding paraphrases ti. Afterwards, questions and paraphrases are embedded in a jointly learned vector space. Each ti is evaluated on the basis of its similarity to the question q, which is defined by a neural network acting over its corresponding embedding. The features used by our scoring system are automatically learned without the need for manual engineering. We use two separate Constitutional Neural Networks (CNNs) for question and paraphrase embedding, to which a max pooling operation is applied. CNNs are embedded as input tokens consisting of concatenation of word and character vectors. The details of our model are outlined in the following sections. For readability, some hyperparameter values are presented in Section 4.2."}, {"heading": "Token Embedding", "text": "This model is illustrated in Figure 1. Each token in our vocabulary is parameterized by a word and an embedding of characters both learned during the training.We find that word vector initialization influences both the convergence rate and the quality of our method.A typical choice for this initialization is the use of pre-trained vectors learned from monitored text data.These models are known to encode both syntactical and semantic information, such as similar or related words such as synonyms that are mapped to close points in vector space.We are experimenting with two different popular methods, namely GloVe [Pennington et al., 2014] and Word2vec [Mikolov et al al., 2013]."}, {"heading": "Sentence Embedding", "text": "We map both the question q and the paraphrase t into a common vector space by using sentence embedding obtained from two jointly trained CNNs. Input to the CNNs are tensors of the form Rs \u00d7 d, which contain an embedding for each character, where s is the maximum input length and d is the dimension of the embedding of the token. We use filters that contain a different number of tokens with the widths of a set L. For each filter width l \u0445L we learn n different filters, each of the dimension Rl \u00b7 d. After the conversion layer we apply a maximum overtime pooling to the resulting feature matrices, which results in a vector of the dimension n per filter width. Next, we concatenate the resulting max-over-time pooling vectors of the different filter widths in L to form our sentence embedding."}, {"heading": "3.4 Neural Similarity Measures", "text": "We are experimenting with different neural similarity values between u and v, which are called 1. uT v (DOTPRODUCT) 2. uTSv, where S-Rk \u00b7 k is a parameter matrix learned during training. (BILINEAR) 3. (u, v) concatenated followed by two sequential, fully connected layers. (FC) 4. BILINEAR concatenated with u and v and followed by fully connected layers. (FC-BILINEAR) 5. BILINEAR and FC linearly combined with learned weights (Figure 2). (FC-BILINEAR-SEP) The most powerful model is FC-BILINEAR-SEP, as shown in Table 1. Intuitively, BILINEAR and FC are able to extract different interaction characteristics between the two input vectors, while both have the best combination of their lines."}, {"heading": "Training Algorithm", "text": "For the training, we define two groups P (positive) and N (negative), which contain pairs (q, t) of questions and paraphrases of logical forms, which, when executed on their respective tables, each give the correct or wrong answers2. Our models presented in Section 3.4 map such a pair to a real value representing a question-logical similarity in form, which is here referred to by the function \u03a6: Q \u00b7 T \u2192 R. We use a maximum margin loss function (with margin tolerance), which aims to place pairs in P over pairs in N: L (P, N) = 1. We also experiment with the cross-entropy loss function, but achieve significantly worse results."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "We use the train validation test split of the WikiTableQuestions data set with 9,659, 1,200, and 4,344 questions, respectively. We get about 3.8 million training sessions (q, t, l) from PL2015, where l is a binary indicator of correctness (whether the logical form gives the correct answer when executed). During the training, we ignore questions for which a single matching pair (q, t) does not exist. The percentage of questions for which a logical candidate form exists that evaluates for the correct answer is called oracle score. PL2015 gives an oracle score of 76.7%, but a manual annotation of [Pasupat and Liang, 2015] shows that PL2015 can answer only 53.5% of the questions correctly."}, {"heading": "4.2 Training Details", "text": "Neural network models are implemented with TensorFlow [Abadi et al., 2016] and trained on a single Tesla P100 GPU. Training takes about 6 hours, while 50,000 mini-batches are processed. Generating text representations from logical forms for all questions with PL2015 takes about 14 hours on a 2016 Macbook Pro computer. Vocabulary consists of 14,151 tokens. We achieve the best results in initializing word embeddings using the 200-dimensional GloVe vectors. Using higher dimensional vectors does not result in significant precision gains. The size of word embeddings xemb = (xglove, xchar) is set2In some cases, when executed on a particular table, a logical form randomly gives the right answer without actually translating the input request."}, {"heading": "Baselines System Accuracy", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Our Models System Accuracy", "text": "The set in which CNNs are embedded includes several tokens with a width of L = {2, 4, 6, 8}, while for the character we use CNN widths of 1, 2 and 3 characters. The two fully connected layers in the FC models have 500 hidden neurons, which we regulate with dropouts [Srivastava et al., 2014] with a shelf life probability of 0.8. We use a mini batch size of 100, each batch contains 50 different q questions with a positive t1 and a negative t2 paraphrase. We set the margin of loss function to 0.2. Loss minimization is done with the Adam Optimizer [Kingma and Ba, 2014] with a learning rate of 7, 10 \u2212 4. All hyperparameters are matched to the division of the development data of the Wiki TableQuestions table. We evaluate the model every 500 steps after validation and select the best 50,000 steps after achieving the best training steps each."}, {"heading": "4.3 Results", "text": "Table 1 shows the performance of our models compared to the baselines of Neural Programmers [Neelakantan et al., 2016] and PL2015 [Pasupat and Liang, 2015]. The most powerful single model is a linear combination of BILINEAR and FC models, namely CNN-FC-BILINEAR-SEP, which results in an accuracy of 34.8%. One explanation for this is that the two methods are able to restore different error types. Our best final model is an interaction of 15 individual models, which achieves a state-of-the-art accuracy for this task of 38.7%. The ensemble score is calculated by averaging above the normalized values of its components. The significant increase in performance of the ensemble compared to the single model shows that the different models acquire unique traits traits traits. In an additional experiment, we use a recursive neural network (RNN) for the set embedding, bearing in mind that the NRFC-SEN model clearly overlaps with the NRFC-EIAR model."}, {"heading": "System Accuracy (Dev)", "text": "There are a few reasons for the low accuracy achieved in this task by various methods (including ours) compared to other NLP problems. Weak monitoring, small training size, and a high percentage of unanswerable questions3 contribute to this difficulty."}, {"heading": "4.4 Ablation Studies", "text": "For a better understanding of our model, we examine the usefulness of various components using an ablation study shown in Table 3. Omitting character embeddings has a marginal effect on accuracy, and the regulation of fully connected layers by dropouts is important. However, the greatest impact on accuracy comes from the use of GloVe pre-formed word vectors to initialize token embeddings, since switching to random initializations significantly reduces accuracy. To test the effect of paraphrasing on the quality of results, we conduct an additional experiment by replacing paraphrasing with the raw strings of Lambda DCS expressions, with results that are only slightly worse, confirming that the paraphrasing method does not cause any additional errors. Furthermore, our neural network component has the greatest impact on the success of our method."}, {"heading": "4.5 Analysis of Correct Answers", "text": "To gain further insights into our approach, we analyze how well our best single model, CNN-FC-BILINEAR-SEP, performs on different question types. We manually comment on 80 randomly selected questions that are correctly answered by our model. Results are presented in Table 4. The largest contribution to accuracy comes from questions that include aggregation, next or previous operations, although they only evaluate3 [Pasupat and Liang, 2015] state that 21% of questions cannot be answered due to various problems such as annotation errors or tables that require advanced normalization."}, {"heading": "System Amount (%)", "text": "on 20.5% of all questions 5th"}, {"heading": "4.6 Error Analysis", "text": "The questions that our models do not answer correctly can be divided into two categories: Either the correct logical form is not generated, or our scoring models do not rank the correct one at the top. In many cases, the correctness of a logical form depends greatly on the table structure. We perform a qualitative analysis, which is presented in Table 2, to uncover common question types that our models often misrank. The first two examples show questions whose correct logical form depends on the structure of the table. In these cases, a tendency toward the more general logical form is often shown. The third example shows that our model has difficulty distinguishing operations with minor changes (e.g. minor and minor of equal size), which may be due to weak monitoring. Since we do not apply or access the logical form of the basic truth during training, but only the correct answer, requests that use minor modified operations would yield the same answer, except in marginal cases."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a two-step QA system for semi-structured tables. Stage one consists of a standard method for generating logical forms for candidates and a simple approach to transforming logical forms into textual paraphrases that are comprehensible to laymen. Stage two is a fully neural model that indirectly classifies the logical forms of candidates by their respective paraphrases, eliminating the need for manually designed characteristics. Experiments show that the interaction of our models on the WikiTableQuestions dataset is state-of-the-art, indicating its ability to answer complex, multicompositional questions. In the future, we plan to advance this work by expanding it to be able to substantiate queries across multiple tables and to an5The distribution of the different question types in the WikiTableQuestions dataset can be found at: http: / / cs.stanford. edu / \u02dc ppasppat / qendf.acupf.lp.where code is available for both: http: / cs.pdpdf.lcf-resource / 2015-where-p.pdf-is a common code."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Abadi et al", "2016] Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "In ACL (1)", "author": ["Jonathan Berant", "Percy Liang. Semantic parsing via paraphrasing"], "venue": "pages 1415\u20131425,", "citeRegEx": "Berant and Liang. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Question answering with subgraph embeddings", "author": ["Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1406.3676,", "citeRegEx": "Bordes et al.. 2014a", "shortCiteRegEx": null, "year": 2014}, {"title": "pages 165\u2013180", "author": ["Antoine Bordes", "Jason Weston", "Nicolas Usunier. Open question answering with weakly supervised embedding models. In Joint European Conference on Machine Learning", "Knowledge Discovery in Databases"], "venue": "Springer,", "citeRegEx": "Bordes et al.. 2014b", "shortCiteRegEx": null, "year": 2014}, {"title": "statistics", "author": ["Peter Clark", "Oren Etzioni", "Tushar Khot", "Ashish Sabharwal", "Oyvind Tafjord", "Peter D Turney", "Daniel Khashabi. Combining retrieval"], "venue": "and inference to answer elementary science questions. In AAAI, pages 2580\u20132586,", "citeRegEx": "Clark et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289,", "citeRegEx": "Clevert et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "User conditional hashtag prediction for images", "author": ["Denton et al", "2015] Emily Denton", "Jason Weston", "Manohar Paluri", "Lubomir Bourdev", "Rob Fergus"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "In ACL (1)", "author": ["Li Dong", "Furu Wei", "Ming Zhou", "Ke Xu. Question answering over freebase with multicolumn convolutional neural networks"], "venue": "pages 260\u2013269,", "citeRegEx": "Dong et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "author": ["Sujay Kumar Jauhar", "Peter D Turney", "Eduard Hovy. Tables as semi-structured knowledge for question answering"], "venue": "volume 1, pages 474\u2013483,", "citeRegEx": "Jauhar et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "Kim. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Percy", "author": ["Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer. Scaling semantic parsers with on-the-fly ontology matching. In In Proceedings of EMNLP"], "venue": "Citeseer,", "citeRegEx": "Kwiatkowski et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Lambda dependency-based compositional semantics", "author": ["Percy Liang"], "venue": "arXiv preprint arXiv:1309.4408,", "citeRegEx": "Liang. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Advances in neural information processing systems", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean. Distributed representations of words", "phrases", "their compositionality"], "venue": "pages 3111\u20133119,", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning a natural language interface with neural programmer", "author": ["Arvind Neelakantan", "Quoc V Le", "Martin Abadi", "Andrew McCallum", "Dario Amodei"], "venue": "arXiv preprint arXiv:1611.08945,", "citeRegEx": "Neelakantan et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["Panupong Pasupat", "Percy Liang"], "venue": "arXiv preprint arXiv:1508.00305,", "citeRegEx": "Pasupat and Liang. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543,", "citeRegEx": "Pennington et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Transactions of the Association for Computational Linguistics", "author": ["Siva Reddy", "Mirella Lapata", "Mark Steedman. Large-scale semantic parsing without questionanswer pairs"], "venue": "2:377\u2013392,", "citeRegEx": "Reddy et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher et al", "2013] Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the conference on empirical methods in nat-", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958,", "citeRegEx": "Srivastava et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["Yushi Wang", "Jonathan Berant", "Percy Liang"], "venue": "Building a semantic parser overnight. In ACL (1), pages 1332\u20131342,", "citeRegEx": "Wang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural enquirer: Learning to query tables with natural language", "author": ["Pengcheng Yin", "Zhengdong Lu", "Hang Li", "Ben Kao"], "venue": "arXiv preprint arXiv:1512.00965,", "citeRegEx": "Yin et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in neural information processing systems", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun. Character-level convolutional networks for text classification"], "venue": "pages 649\u2013657,", "citeRegEx": "Zhang et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "It performs two phases: first, machine understandable logical forms (programs) are generated from natural language questions following the work of [Pasupat and Liang, 2015].", "startOffset": 147, "endOffset": 172}, {"referenceID": 15, "context": "7%, thus slightly surpassing both the engineered feature scoring baseline, as well as the Neural Programmer model of [Neelakantan et al., 2016].", "startOffset": 117, "endOffset": 143}, {"referenceID": 21, "context": "[Wang et al., 2015]).", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "[Bordes et al., 2014a]).", "startOffset": 0, "endOffset": 22}, {"referenceID": 16, "context": "Each natural language question is translated into a set of computer understandable candidate representations, called logical forms, based on the work of [Pasupat and Liang, 2015].", "startOffset": 153, "endOffset": 178}, {"referenceID": 16, "context": "We empirically confirm our approach on a series of experiments on WikiTableQuestions [Pasupat and Liang, 2015], a real-world dataset containing 22,033 pairs of questions and their corresponding manually retrieved answers with about 2,108 randomly selected Wikipedia tables.", "startOffset": 85, "endOffset": 110}, {"referenceID": 18, "context": "To tackle this problem, our method follows recent work of [Reddy et al., 2014; Kwiatkowski et al., 2013] that relies solely on weak-supervision through question-answertable triples.", "startOffset": 58, "endOffset": 104}, {"referenceID": 12, "context": "To tackle this problem, our method follows recent work of [Reddy et al., 2014; Kwiatkowski et al., 2013] that relies solely on weak-supervision through question-answertable triples.", "startOffset": 58, "endOffset": 104}, {"referenceID": 16, "context": "In the context of QA for semi-structured tables and dealing with multi-compositional queries, [Pasupat and Liang, 2015] generate and rank candidate logical forms with a log-linear model trained on question-answer pairs.", "startOffset": 94, "endOffset": 119}, {"referenceID": 16, "context": "In this work, we generate logical form candidates in the same way as [Pasupat and Liang, 2015].", "startOffset": 69, "endOffset": 94}, {"referenceID": 21, "context": "Paraphrases have been successfully used to facilitate semantic parsers [Wang et al., 2015; Berant and Liang, 2014].", "startOffset": 71, "endOffset": 114}, {"referenceID": 1, "context": "Paraphrases have been successfully used to facilitate semantic parsers [Wang et al., 2015; Berant and Liang, 2014].", "startOffset": 71, "endOffset": 114}, {"referenceID": 1, "context": "While [Berant and Liang, 2014] is suited for factoid questions with a modest amount of compositionality, [Wang et al.", "startOffset": 6, "endOffset": 30}, {"referenceID": 21, "context": "While [Berant and Liang, 2014] is suited for factoid questions with a modest amount of compositionality, [Wang et al., 2015] targets more complicated questions.", "startOffset": 105, "endOffset": 124}, {"referenceID": 15, "context": "[Neelakantan et al., 2016] also focus on compositional questions, but instead of generating and ranking multiple logical forms, they propose a model that directly constructs a logical form from an embedding of the question.", "startOffset": 0, "endOffset": 26}, {"referenceID": 22, "context": "Recently, [Yin et al., 2015] propose Neural Enquirer, a fully neural, end-to-end differentiable network that executes queries across multiple tables.", "startOffset": 10, "endOffset": 28}, {"referenceID": 2, "context": "Existing approaches often represent questions and knowledgebase constitutes in a single vector using simple bag-of-words (BOW) models [Bordes et al., 2014a; Bordes et al., 2014b] under the framework of memory networks.", "startOffset": 134, "endOffset": 178}, {"referenceID": 3, "context": "Existing approaches often represent questions and knowledgebase constitutes in a single vector using simple bag-of-words (BOW) models [Bordes et al., 2014a; Bordes et al., 2014b] under the framework of memory networks.", "startOffset": 134, "endOffset": 178}, {"referenceID": 7, "context": "[Dong et al., 2015] propose a multi-column convolutional neural network to account for the word order and higher order n-grams.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "in the context of sentiment classification, [Kim, 2014; Socher et al., 2013], or for image-hashtag prediction [Denton et al.", "startOffset": 44, "endOffset": 76}, {"referenceID": 3, "context": "Systems vary from operating on structured knowledge bases [Bordes et al., 2014b]; [Bordes et al.", "startOffset": 58, "endOffset": 80}, {"referenceID": 2, "context": ", 2014b]; [Bordes et al., 2014a] to semi-structured tables [Pasupat and Liang, 2015], [Neelakantan et al.", "startOffset": 10, "endOffset": 32}, {"referenceID": 16, "context": ", 2014a] to semi-structured tables [Pasupat and Liang, 2015], [Neelakantan et al.", "startOffset": 35, "endOffset": 60}, {"referenceID": 15, "context": ", 2014a] to semi-structured tables [Pasupat and Liang, 2015], [Neelakantan et al., 2016], [Jauhar et al.", "startOffset": 62, "endOffset": 88}, {"referenceID": 8, "context": ", 2016], [Jauhar et al., 2016] and completely unstructured text, which is related to information extraction [Clark et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 4, "context": ", 2016] and completely unstructured text, which is related to information extraction [Clark et al., 2016].", "startOffset": 85, "endOffset": 105}, {"referenceID": 16, "context": "For every question q : i) a set of candidate logical forms {zi}i\u2208Iq is generated using the method of [Pasupat and Liang, 2015]; ii) each such candidate program zi is paraphrased in a textual representation ti that offers accuracy gain, interpretability and comprehensibility ; iii) all textual forms ti are scored against the input question q using a neural network model; iv) the logical form z\u2217 i corresponding to the highest ranked ti is selected as the machine-understandable translation of question q; v) z\u2217 i is executed on the input table and its answer is returned to the user.", "startOffset": 101, "endOffset": 126}, {"referenceID": 16, "context": "Our contributions are the novel models that perform the steps ii) and iii), while for i), iv) and v) we rely on the work of [Pasupat and Liang, 2015] (henceforth: PL2015).", "startOffset": 124, "endOffset": 149}, {"referenceID": 13, "context": "Details about Lambda DCS language can be found in [Liang, 2013].", "startOffset": 50, "endOffset": 63}, {"referenceID": 13, "context": "Each candidate logical form is represented in Lambda DCS form [Liang, 2013] and can be transformed into a SPARQL query, whose execution against the KG yields an answer.", "startOffset": 62, "endOffset": 75}, {"referenceID": 9, "context": "Parts of figure taken from [Kim et al., 2015].", "startOffset": 27, "endOffset": 45}, {"referenceID": 17, "context": "We experiment with two different popular methods, namely GloVe [Pennington et al., 2014] and Word2vec [Mikolov et al.", "startOffset": 63, "endOffset": 88}, {"referenceID": 14, "context": ", 2014] and Word2vec [Mikolov et al., 2013], comparing them also with random initializations.", "startOffset": 21, "endOffset": 43}, {"referenceID": 9, "context": "One technique to mitigate these issues inspired from [Kim et al., 2015; Zhang et al., 2015] is to use character embeddings in addition to word vectors.", "startOffset": 53, "endOffset": 91}, {"referenceID": 23, "context": "One technique to mitigate these issues inspired from [Kim et al., 2015; Zhang et al., 2015] is to use character embeddings in addition to word vectors.", "startOffset": 53, "endOffset": 91}, {"referenceID": 5, "context": "As non-linearity we use Exponential Linear Units (ELUs) [Clevert et al., 2015].", "startOffset": 56, "endOffset": 78}, {"referenceID": 16, "context": "7%, but a manual annotation by [Pasupat and Liang, 2015] reveals that PL2015 can answer only 53.", "startOffset": 31, "endOffset": 56}, {"referenceID": 20, "context": "The two fully connected layers in the FC models have 500 hidden neurons, which we regularize using dropout [Srivastava et al., 2014] with a keep probability of 0.", "startOffset": 107, "endOffset": 132}, {"referenceID": 11, "context": "Loss minimization is done using the Adam optimizer [Kingma and Ba, 2014] with a learning rate of 7 \u2217 10\u22124.", "startOffset": 51, "endOffset": 72}, {"referenceID": 15, "context": "Table 1 shows the performance of our models compared to Neural Programmer [Neelakantan et al., 2016] and PL2015 [Pasupat and Liang, 2015] baselines.", "startOffset": 74, "endOffset": 100}, {"referenceID": 16, "context": ", 2016] and PL2015 [Pasupat and Liang, 2015] baselines.", "startOffset": 19, "endOffset": 44}, {"referenceID": 16, "context": "[Pasupat and Liang, 2015] state that 21% of questions cannot be answered because of various issues like annotation errors or tables requiring advanced normalization.", "startOffset": 0, "endOffset": 25}], "year": 2017, "abstractText": "Advances in natural language processing tasks have gained momentum in recent years due to the increasingly popular neural network methods. In this paper, we explore deep learning techniques for answering multi-step reasoning questions that operate on semi-structured tables. Challenges here arise from the level of logical compositionality expressed by questions, as well as the domain openness. Our approach is weakly supervised, trained on question-answer-table triples without requiring intermediate strong supervision. It performs two phases: first, machine understandable logical forms (programs) are generated from natural language questions following the work of [Pasupat and Liang, 2015]. Second, paraphrases of logical forms and questions are embedded in a jointly learned vector space using word and character convolutional neural networks. A neural scoring function is further used to rank and retrieve the most probable logical form (interpretation) of a question. Our best single model achieves 34.8% accuracy on the WikiTableQuestions dataset, while the best ensemble of our models pushes the state-of-the-art score on this task to 38.7%, thus slightly surpassing both the engineered feature scoring baseline, as well as the Neural Programmer model of [Neelakantan et al., 2016].", "creator": "LaTeX with hyperref package"}}}