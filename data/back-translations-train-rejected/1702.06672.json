{"id": "1702.06672", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Calculating Probabilities Simplifies Word Learning", "abstract": "Children can use the statistical regularities of their environment to learn word meanings, a mechanism known as cross-situational learning. We take a computational approach to investigate how the information present during each observation in a cross-situational framework can affect the overall acquisition of word meanings. We do so by formulating various in-the-moment learning mechanisms that are sensitive to different statistics of the environment, such as counts and conditional probabilities. Each mechanism introduces a unique source of competition or mutual exclusivity bias to the model; the mechanism that maximally uses the model's knowledge of word meanings performs the best. Moreover, the gap between this mechanism and others is amplified in more challenging learning scenarios, such as learning from few examples.", "histories": [["v1", "Wed, 22 Feb 2017 04:30:09 GMT  (387kb,D)", "http://arxiv.org/abs/1702.06672v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aida nematzadeh", "barend beekhuizen", "shanshan huang", "suzanne stevenson"], "accepted": false, "id": "1702.06672"}, "pdf": {"name": "1702.06672.pdf", "metadata": {"source": "CRF", "title": "Calculating Probabilities Simplifies Word Learning", "authors": ["Aida Nematzadeh", "Barend Beekhuizen", "Shanshan Huang32AndSuzanne"], "emails": ["1nematzadeh@berkeley.edu", "2barend@cs.toronto.edu", "3sunny.huang@mail.utoronto.ca", "4suzanne@cs.toronto.edu"], "sections": [{"heading": "Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "A Cross-situational Word Learning Framework", "text": "Over the past decade, interest in the development of computer models as tools for studying word learning in humans has increased, with particular interest in cross-situational, incremental learning patterns (e.g. Siskind, 1996; Fazly, Alishahi, & Stevenson, 2010; Kachergis, Yu, & Shiffrin, 2012) that are necessary for studying developmental learning patterns. In particular, the model developed by Fazly et al. (2008; 2010) (hereinafter FAS) is the first probabilistic model that reliably predicts a range of observed behavior in word learning in childhood. Furthermore, this model has been adopted and expanded by a series of successive studies (e.g. Nematzadeh, Fazly, & Stevenson, 2012a; Grant, Nematzadeh, & Stevenson, 2016), demonstrating its robustness in taking empirical data into account."}, {"heading": "The FAS Model", "text": "The model is a sequence of statement pairs that simulate what the child hears and perceives, respectively. Each utterance is a set of words (which ignore their sequence), and the corresponding scene is a set of semantic characteristics that represent possible meanings of words in the utterance. (1) The results of the model are represented by several characteristics that expose the model to naturalistic similarities between the words. (1) The current representation of the meaning of each word w as a probability distribution, p, eats} Scene: {PERSON, ACT, CONSUME,... (1) The output of the model, at each step in learning, is the present representation of the meaning of each word w as a probability distribution, p, over all possible semantic characteristics f, which the model has been observed in the input scenes.The word learning problem is a corpus of utterances of the uttered pairs of the uttered pairs of the utterances, the goal of the probability to learn the model is the probability distribution."}, {"heading": "Grouping Sets of Features into Referents", "text": "In FAS, an input scene is the specified unification of all meanings for all words in the corresponding utterance. This representation lacks information that would be obvious to a child as to how groups of features belong to a single entity or event - e.g. PERSON and JOEL or ACT and CONSUME in Ex. 1. However, replacing the attribute sets with a single symbol corresponding to the meaning would prevent the model from learning semantic similarities between the words (e.g. Nematzadeh, Fazly, & Stevenson, 2012b). Instead, following Alishahi, Fazly, Koehne and Crocker (2012), the semantic features in a scene are grouped into references that match a word in the utterance, as in Ex. 5: 55 We use the term referent to denote everything referred to by a word - an object or event, or a group of semantic properties (e.g. {INDEpt, SITALE)."}, {"heading": "In-the-Moment Learning Mechanisms", "text": "We observed above that the calculation of alignment strength in the Eqn group serves as a basis of comparison for the other two assumptions. (2) There is no form of mutual exclusivity. (3) We have the opportunity to explore different ways to determine the strength of alignments. (3) The three alignment formulations researched here do not compete between words or speakers, (2) the speakers \"competition for a word (as in Alishahi et al., 2012) and (3) the competition of words for a speaker (analogous to the competition of words in FAS). Each of these ways of looking at the contest implements a different approach of mutual exclusivity in the model, and we will explore the resulting effects on word learning in the results."}, {"heading": "Set-up", "text": "In order to create the corresponding scene representations, each word in the corpus is entered into a gold standard lexicon with a series of semantic features representing its gold standard meaning, following the procedure of Fazly et al. (2008). The referents shown in Ex. 1 correspond to the gold standard meaning of each of these words. (The word mapping in the lexicon is only used to create scenes, and is not seen by the model.) The model is trained using 20K speech and scene pairs, in which the behavior is stable. In the following experiments, we examine the quality of each learned word representation in two respects: the average capture of all the words observed by the model and the percentage of observed words that are learned."}, {"heading": "Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Overall Learning Patterns", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "The Role of Frequency", "text": "Previous research suggests that children use biases such as mutual exclusivity to guide their learning. Learning low-frequency words is also a challenge for computer models, and understanding the mechanisms that improve learning from little evidence can shed light on how children address this problem. The nature of the competition in the various models studied here plays an important role in their performance on low-frequency words. Figure 3a shows that for the two models competing for words - the FAS and word combinations - there is no decrease in performance for low-frequency words (< 5) compared to high frequency (> 10), while for the other two models, No-Comp and Ref-Comp, there is a dramatic drop in learning."}, {"heading": "The Role of Utterance Length", "text": "Similarly, we can investigate whether there is a differential influence of enunciation length on the different models. To simulate this, we manipulated the input generation process so that the model was trained only on enunciations of length 5 or higher (long corpus) or 3 and lower (short corpus). If we look at Figure 3b, we find that the acquisition results are lower globally if the models are trained only on long sentences, probably due to the fact that there is more uncertainty about which words and which references belong together. Here, we see that the word compensation model is the only one that does not significantly reduce the performance when comparing learning on the short corpus and long corpus. While the competition for words equally supports the FAS and word compensation models in dealing with the multiple words, in this case, we cannot compare the bundling of attributes to speakers as a word compase solution with the long corpus scale."}, {"heading": "The Role of Referential Uncertainty", "text": "To investigate the effects of referential uncertainty - the occurrence of many more potential speakers in a scene than there are words - we create a subcorpus that uses every ith utterance from our complete corpus and uses the utterances in between to generate \"additional\" references in the scenes for utterances in the subcorpus. At this point, we report results from 20K inputs with speakers that are added to each scene Si from 0, 1 or 2 in addition to speakers from the utterance Ui. Figure 4 shows the results without referential uncertainty, along with the two additional degrees of uncertainty. As we expect, the learning performance of all models with higher referential uncertainty is deteriorating. However, unlike our previous results, there is little benefit here from a word-based competition or bundling of features. The high degree of ambiguity introduced by these levels of referential uncertainty could be better addressed by attention-oriented mechanisms that focus the common attention on a partial alignment of the real world."}, {"heading": "Conclusions and Future Work", "text": "Previous research shows that children are sensitive to the cross-situational statistics of their environment: i.e. they can use the laws of different situations to learn word meanings. However, the detailed mechanisms responsible for cross-situational word learning are not yet fully understood, such as what information from each observation is used to determine the correct word meaning, and how this information feeds into the accumulated knowledge of a word. Furthermore, children are good at learning word meanings in a variety of situations: they can learn a new word from a few examples and also learn words from ambiguous / loud conditions. Previous research has suggested that children are equipped with biases that guide them in word learning by reducing the difficulty / ambiguity of a learning situation. The need for these biases in children, and whether they are innate or learnable, are questions that have been discussed among cognitive scientists."}], "references": [{"title": "Sentence-based attentional mechanisms in word learning: evidence from a computational model", "author": ["A. Alishahi", "A. Fazly", "J. Koehne", "M.W. Crocker"], "venue": "Frontiers in psychology,", "citeRegEx": "Alishahi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alishahi et al\\.", "year": 2012}, {"title": "A probabilistic incremental model of word learning in the presence of referential uncertainty", "author": ["A. Fazly", "A. Alishahi", "S. Stevenson"], "venue": "In CogSci Proceedings", "citeRegEx": "Fazly et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fazly et al\\.", "year": 2008}, {"title": "A probabilistic computational model of cross-situational word learning", "author": ["A. Fazly", "A. Alishahi", "S. Stevenson"], "venue": "Cognitive Science,", "citeRegEx": "Fazly et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fazly et al\\.", "year": 2010}, {"title": "The interaction of memory and attention in novel word generalization: A computational investigation", "author": ["E. Grant", "A. Nematzadeh", "S. Stevenson"], "venue": "In Proceedings of the 38th annual conference of the cognitive science society", "citeRegEx": "Grant et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Grant et al\\.", "year": 2016}, {"title": "An associative model of adaptive inference for learning word\u2013referent mappings", "author": ["G. Kachergis", "C. Yu", "R. Shiffrin"], "venue": "Psychonomic Bulletin and Review,", "citeRegEx": "Kachergis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kachergis et al\\.", "year": 2012}, {"title": "The CHILDES project: Tools for analyzing talk", "author": ["B. MacWhinney"], "venue": "(3rd ed.,", "citeRegEx": "MacWhinney,? \\Q2000\\E", "shortCiteRegEx": "MacWhinney", "year": 2000}, {"title": "How children constrain the possible meanings of words. In U. Neisser (Ed.), Concepts and conceptual development: Ecological and intellectual factors in categorization", "author": ["E.M. Markman"], "venue": null, "citeRegEx": "Markman,? \\Q1987\\E", "shortCiteRegEx": "Markman", "year": 1987}, {"title": "Children\u2019s use of mutual exclusivity to constrain the meanings of words", "author": ["E.M. Markman", "G.F. Wachtel"], "venue": "Cognitive Psychology,", "citeRegEx": "Markman and Wachtel,? \\Q1988\\E", "shortCiteRegEx": "Markman and Wachtel", "year": 1988}, {"title": "A computational model of memory, attention, and word learning", "author": ["A. Nematzadeh", "A. Fazly", "S. Stevenson"], "venue": "In CMCL Proceedings (pp. 80\u201389)", "citeRegEx": "Nematzadeh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nematzadeh et al\\.", "year": 2012}, {"title": "Interaction of word learning and semantic category formation in late talking", "author": ["A. Nematzadeh", "A. Fazly", "S. Stevenson"], "venue": "In CogSci Proceedings (pp. 2085\u20132090)", "citeRegEx": "Nematzadeh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nematzadeh et al\\.", "year": 2012}, {"title": "Learnability and cognition: The acquisition of argument structure", "author": ["S. Pinker"], "venue": null, "citeRegEx": "Pinker,? \\Q1989\\E", "shortCiteRegEx": "Pinker", "year": 1989}, {"title": "A computational study of crosssituational techniques for learning word-to-meaning", "author": ["J.M. Siskind"], "venue": "mappings. Cognition,", "citeRegEx": "Siskind,? \\Q1996\\E", "shortCiteRegEx": "Siskind", "year": 1996}, {"title": "Infants rapidly learn wordreferent mappings via cross-situational", "author": ["L.B. Smith", "C. Yu"], "venue": "statistics. Cognition,", "citeRegEx": "Smith and Yu,? \\Q2008\\E", "shortCiteRegEx": "Smith and Yu", "year": 2008}, {"title": "The role of performance limitations in the acquisition of verb\u2013argument structure: An alternative account", "author": ["A.L. Theakston", "E.V. Lieven", "J.M. Pine", "C.F. Rowland"], "venue": "Journal of Child Language,", "citeRegEx": "Theakston et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Theakston et al\\.", "year": 2001}, {"title": "Propose but verify: Fast mapping meets crosssituational word learning", "author": ["J.C. Trueswell", "T.N. Medina", "A. Hafri", "L.R. Gleitman"], "venue": "Cognitive Psychology,", "citeRegEx": "Trueswell et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Trueswell et al\\.", "year": 2013}, {"title": "Rapid word learning under uncertainty via cross-situational statistics", "author": ["C. Yu", "L.B. Smith"], "venue": "Psychological Science,", "citeRegEx": "Yu and Smith,? \\Q2007\\E", "shortCiteRegEx": "Yu and Smith", "year": 2007}, {"title": "The role of partial knowledge in statistical word learning", "author": ["D. Yurovsky", "D.C. Fricker", "C. Yu", "L.B. Smith"], "venue": "Psychonomic Bulletin and Review,", "citeRegEx": "Yurovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yurovsky et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Here we adopt the alignment formulation of Alishahi et al. (2012), which we call \u201cref-comp\u201d because referents compete for alignment with a word.", "startOffset": 43, "endOffset": 66}, {"referenceID": 5, "context": "Rowland, 2001) in CHILDES (MacWhinney, 2000).", "startOffset": 26, "endOffset": 44}, {"referenceID": 1, "context": "To create the associated scene representations, each word in the corpus is entered into a gold-standard lexicon with a set of semantic features representing its gold-standard meaning, following the procedure of Fazly et al. (2008). The referents shown in Ex.", "startOffset": 211, "endOffset": 231}, {"referenceID": 0, "context": "This is especially surprising given that Alishahi et al. (2012) used the ref-comp alignment mechanism in their work that modeled human behaviour in a language learning task.", "startOffset": 41, "endOffset": 64}, {"referenceID": 0, "context": "We can also now suggest why the Alishahi et al. (2012) model (the ref-comp approach) worked well in their experiments but not here: the utterances they used all had two words, unlike the naturalistic data we train on above,", "startOffset": 32, "endOffset": 55}], "year": 2017, "abstractText": "Children can use the statistical regularities of their environment to learn word meanings, a mechanism known as crosssituational learning. We take a computational approach to investigate how the information present during each observation in a cross-situational framework can affect the overall acquisition of word meanings. We do so by formulating various in-the-moment learning mechanisms that are sensitive to different statistics of the environment, such as counts and conditional probabilities. Each mechanism introduces a unique source of competition or mutual exclusivity bias to the model; the mechanism that maximally uses the model\u2019s knowledge of word meanings performs the best. Moreover, the gap between this mechanism and others is amplified in more challenging learning scenarios, such as learning from few examples.", "creator": "TeX"}}}