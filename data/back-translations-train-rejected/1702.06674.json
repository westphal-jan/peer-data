{"id": "1702.06674", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Unsupervised Diverse Colorization via Generative Adversarial Networks", "abstract": "Colorization of grayscale images has been a hot topic in computer vision. Previous research mainly focuses on producing a colored image to match the original one. However, since many colors share the same gray value, an input grayscale image could be diversely colored while maintaining its reality. In this paper, we design a novel solution for unsupervised diverse colorization. Specifically, we leverage conditional generative adversarial networks to model the distribution of real-world item colors, in which we develop a fully convolutional generator with multi-layer noise to enhance diversity, with multi-layer condition concatenation to maintain reality, and with stride 1 to keep spatial information. With such a novel network architecture, the model yields highly competitive performance on the open LSUN bedroom dataset. The Turing test of 80 humans further indicates our generated color schemes are highly convincible.", "histories": [["v1", "Wed, 22 Feb 2017 04:34:31 GMT  (4291kb,D)", "http://arxiv.org/abs/1702.06674v1", null], ["v2", "Sat, 1 Jul 2017 10:57:03 GMT  (4498kb,D)", "http://arxiv.org/abs/1702.06674v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["yun cao", "zhiming zhou", "weinan zhang", "yong yu"], "accepted": false, "id": "1702.06674"}, "pdf": {"name": "1702.06674.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Diverse Colorization via Generative Adversarial Networks", "authors": ["Yun Cao", "Zhiming Zhou", "Weinan Zhang", "Yong Yu"], "emails": ["yyu}@apex.sjtu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Diverse Colorization", "text": "The problem of coloring was proposed as early as the last century, but not much attention was paid to the study of multiple coloring until this decade. [Cheng et al., 2015] used specially trained models to deal with the different coloring of a scene, especially at daybreak and daybreak. [Zhang et al., 2016] presented the multi-layered coloring problem as a classification task and used class restructuring during training to increase color diversity in the result. [Deshpande et al., 2016] used a varying auto-encoder (VAE) to learn a low-dimensional embedding of color fields. They constructed loss terms for the VAE decoder, avoiding blurred outputs and taking into account the uneven distribution of pixel colors, and finally developed a conditional model for multimodal distribution between grayscale image and color field embedding. They constructed loss terms for the VAE decoder, which allow for blurred outputs and uneven distribution of pixel colors, and ultimately developed a conditional model for multimodal distribution between grayscale image and color field embedding."}, {"heading": "2.2 Conditional GAN", "text": "Generative adversarial networks (GANs) [Goodfellow et al., 2014] have received a lot of attention in unattended learning research over the last 3 years. Conditional GANs have been widely used in various computer vision scenarios. [Reed et al., 2016] used text to generate images by using opposing networks. [Isola et al., 2016] provided a universal picture-to-picture translation model that tasks such as label to scene, antenna to map, day to night, edges to photo, and even grayscale to color. Some of the above works may share a similar goal, but our conditional GAN structure differs greatly from previous work in several architectural decisions mainly for the generator. Unlike other generators that use an encode-like front part of multiple conversion layers and a decode-like end part consisting of multiple deconversion layers, our multigenerator only uses conversion layers in the overall architecture, not generating color layers during small-scale architecture."}, {"heading": "3 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Problem formulation", "text": "GANs are generative models that map random noise vector z to color image output x: G: z \u2192 x. Compared to GANs, conditional GANs learn to map observed grayscale image y and random noise vector z, to x: G: {y, z} \u2192 x. Generator G is trained to generate outputs that cannot be distinguished from \"real\" images by a hostile trained discriminator D trained to recognize the \"false\" images generated by the generator. This training method is shown in Figure 1. The target of a GAN can be shown as LGAN (G, D) = Ex \u0445 Pdata (x) [logD (x)] + Ez \u0445 Pz (z) [log (1 \u2212 D (G (z)))], while the target of a conditional GAN isLcGAN (G, D) = Ex \u0445 Pdata (x) + (2 \u2212 Y (G (z), the (G), Pnez (the), G (y) (the), G (the), Pnez (the), G (the)."}, {"heading": "3.2 Architecture and implementation details", "text": "The overall structure of our conditional GAN is consistent with the traditional [Isola et al., 2016; Deshpande et al., 2016], while the detailed architecture of our generator G is very different."}, {"heading": "Convolution or deconvolution", "text": "Folding layers and deflection layers are two basic components of image generators. Folding layers are mainly used for the exact consideration of conditional features. Furthermore, much research [Zhang et al., 2016; Isola et al., 2016; Deshpande et al., 2016] uses the superposition of multiple folding layers with more than 1 step to reduce the data form that functions as a data encoder. Deconvolution layers are then used to use the data form as a data representation decoder [Nguyen et al., 2016a; Isola et al., 2016; Deshpande et al., 2016]. While many other research shares this encoder-decoder structure, we opt to use only folding layers in our generator G. First, folding layers are well able to provide the extraction and transmission of traits. Meanwhile, all convolution steps are set to 1 to prevent the data from becoming smaller."}, {"heading": "YUV or RGB", "text": "A color image can be represented in various shapes, the most common being the RGB shape, which, due to its universality, splits a color pixel into three channels: red, green, blue. Other types of representations are also included, such as Lab [Deshpande et al., 2015; Isola et al., 2016; Nguyen et al., 2016b], Y UV (or Y CrCb) [Cheng et al., 2015]. For coloring tasks, we have grayscale image as conditional information, so it is convenient to use Y-UV representation, since the Y channel or so-called luminance channel represents exactly the grayscale information. So, while we use Y-UV representation, we can only predict 2 channels and then link z-color correction to the grayscale channel to produce a complete color image."}, {"heading": "Multi-layer noise", "text": "In the work of [Isola et al., 2016] they mentioned the ignorance of noise during the training of the generator. To solve this problem, they provide noise information only in the form of dropouts, which are applied to several layers of the generator both during the training and during the test period. To overcome this problem and make the results of the coloring more diverse, we link the noise channel to the first half of the generator layers (in our case to the first three layers). We conduct comparative experiments with single-layer noise and multi-layer noise representations, with the results in Section 4.2. Algorithm 1 training phase of our conditional GANs using the standard parameters kD = 1, kG = 1 miniature, m = 64, sz = 64.1: for the number of training steps."}, {"heading": "Multi-layer conditional information", "text": "Other conditional GANs usually add conditional information only in the first layer, as the layer shape of previous generators changes along their folding and defolding layers. However, due to the consistent layer shape of our generator, we can apply conditional grayscale information concatenation to the entire generator layers, which can allow sustained conditional monitoring. Although the \"UNet\" skip structure of [Isola et al., 2016] can also help provide conditional information to rear layers, our model modification is even easier and more convenient."}, {"heading": "3.3 Training and testing procedure", "text": "The training phase of our conditional GANs is presented in Algorithm 1. While to make the BatchNorm layers work properly, you cannot feed a batch of images with the same images to test different noise reactions, we use multi-round tests with the same batch and rearrange them to test different noise reactions of each image, which is described in Algorithm 2."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "There are different types of color image datasets, and we select the open LSUN bedroom dataset 1 [Yu et al., 2015] to perform our experiment. LSUN is a large color image dataset that is generated iteratively by human labeling with automatic deep1LSUN datasets. It is available at http: / / lsun.cs.princeton.edu.Algorithm 2 test phase of our conditional GANs using the default parameters m = 64, sz = 100.1: Example minibatch of m grayscale images {y (1),.., y (m)} 2: for round i in test rounds do 3: Generate minibatch of m of randomly sampled noise {z (1, i),.., z (m, i)} each of the size classes [sz],. 4: Generate color image use model G: {x (1, i),."}, {"heading": "4.2 Comparison Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "YUV and RGB", "text": "The resulting coloring results of the same grayscale Y-UV image and RGB image with additional L1 loss are shown in Figure 4. If we focus on the results in red boxes, we see that the RGB image is structurally missing due to the additional L1 loss. In addition, the RGB image model is supposed to predict 3 color channels, while the Y-UV image predicts only 2 channels, making RGB model training much more unstable."}, {"heading": "Single-layer and multi-layer noise", "text": "The coloring results obtained from the same grayscale images using a single-layer noise model and a multi-layer noise model are shown in Figure 5, which shows that multi-layer noise leads to much more diversity."}, {"heading": "Single-layer and multi-layer condition", "text": "The generated colorization results of the same grayscale images with single-layer condition model and multi-layer condition model are shown in Figure 6. Results show that the multi-layer condition model gives the generator more structural information and therefore the results of the multi-layer condition model are more stable, while the single-layer condition model suffers from colorization. Further results and discussions of our final model will be shown in the next section."}, {"heading": "5 Results and Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Colorization Results", "text": "A variety of image coloring results due to our conditional GANs can be found in Figure 7. Obviously, our fully revolutionary (no step speed) generator with multi-layer noise and multi-layer condition concatenation generates different types of coloring schemes while maintaining good reality. Almost all color parts remain within correct components without deviations."}, {"heading": "5.2 Evaluation via Human Study", "text": "Previous methods had the common goal of providing a color image similar to the original one. Therefore, many of their models used [Jung and Kang, 2016; Perarnau et al., 2016; Deshpande et al., 2016] image distances such as RMSE (Root Mean Square Error) and PSNR (Peak Signal-to-Noise Ratio) as measurements, and others [Iizuka et al., 2016; Isola et al., 2016] use additional classifiers to predict whether colored images can be recognized or still correctly classified. However, our goal is to generate multiple coloring schemes so that we cannot take this distance as measurements because there are reasonable colorations that differ greatly from the original color image. Note that some previous work [Cheng et al., 2015; Dong et al., 2017; Nguyen et al., 2016b; Nguyen et al., 2016a] does not provide quantified measurements."}, {"heading": "Single-layer condition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Multi-layer condition", "text": "Each of the 80 participants asks 20 questions in which we present 5 color images, one of which is the Ground Truth Image, the other is our produced colors of the grayscale image of the Ground Truth, and asks them if one of them is of poor reality. We add a Ground Truth Image as a reference if the participants believe that none of them is real, and we randomly order all the images so that the participants do not find the Ground Truth from their positions. The feedback from 80 participants shows that more than 62.6% of our produced color images are invincible, while the rate of the Ground Truth Images is 70.0%. In addition, we perform a Significance t Test between the percentages of the human rating as a true image for each test case. The p-value is 0.1359 > 0.05, indicating that our generated results do not differ significantly from the Ground Truth Images. In addition, we calculate the average credibility rate of the human rating as a true image for each test case. The p-value is 0.1359 > 0.05, which indicates that our generated results do not differ significantly from the Ground Truth Images. Furthermore, we calculate the average credibility rate of the image rating of a higher than 2.5, which means that the image placement of a higher value of the image value of the image of the image of the image of the image of the higher value means that it is correct."}, {"heading": "6 Conclusion", "text": "We present a novel generator architecture that consists of a completely winding (no-step) structure with multi-layer noise and multi-layer state concatenation, with which our model successfully generated diversified, high-quality color images for each grayscale image entered. As a Turing test, we conducted a questionnaire survey to measure our coloring result. Feedback from 80 participants indicates that our coloring schemes generated are fairly random. In future work, we plan to explore further application scenarios and achieve a more general, unsupervised, diverse generative model."}], "references": [{"title": "Automatic image colorization via multimodal predictions", "author": ["Charpiat et al", "2008] Guillaume Charpiat", "Matthias Hofmann", "Bernhard Scholkopf"], "venue": "In Computer Vision - ECCV 2008, 10th European Conference on Computer Vision,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "In ICCV 2015", "author": ["Zezhou Cheng", "Qingxiong Yang", "Bin Sheng. Deep colorization"], "venue": "Santiago, Chile, December 7-13, 2015, pages 415\u2013423,", "citeRegEx": "Cheng et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "ACM Trans", "author": ["Alex Yong Sang Chia", "Shaojie Zhuo", "Raj Kumar Gupta", "Yu-Wing Tai", "Siu-Yeung Cho", "Ping Tan", "Stephen Lin. Semantic colorization with internet images"], "venue": "Graph., 30(6):156:1\u2013156:8,", "citeRegEx": "Chia et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In ICCV 2015", "author": ["Aditya Deshpande", "Jason Rock", "David A. Forsyth. Learning large-scale automatic image colorization"], "venue": "Santiago, Chile, December 7-13, 2015, pages 567\u2013575,", "citeRegEx": "Deshpande et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "CoRR", "author": ["Aditya Deshpande", "Jiajun Lu", "Mao-Chuang Yeh", "David A. Forsyth. Learning diverse image colorization"], "venue": "abs/1612.01958,", "citeRegEx": "Deshpande et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "CoRR", "author": ["Hao Dong", "Paarth Neekhara", "Chao Wu", "Yike Guo. Unsupervised image-to-image translation with generative adversarial networks"], "venue": "abs/1701.02676,", "citeRegEx": "Dong et al.. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "Generative adversarial nets", "author": ["Goodfellow et al", "2014] Ian J. Goodfellow", "Jean PougetAbadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Image colorization using similar images", "author": ["Gupta et al", "2012] Raj Kumar Gupta", "Alex Yong Sang Chia", "Deepu Rajan", "Ee Sin Ng", "Zhiyong Huang"], "venue": "In Proceedings of the 20th ACM Multimedia Conference, MM \u201912,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "In SIGGRAPH 2001", "author": ["Aaron Hertzmann", "Charles E. Jacobs", "Nuria Oliver", "Brian Curless", "David Salesin. Image analogies"], "venue": "Los Angeles, California, USA, August 12-17, 2001, pages 327\u2013340,", "citeRegEx": "Hertzmann et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification", "author": ["Satoshi Iizuka", "Edgar Simo-Serra", "Hiroshi Ishikawa"], "venue": "ACM Trans. Graph., 35(4):110:1\u2013110:11,", "citeRegEx": "Iizuka et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "CoRR", "author": ["Phillip Isola", "Jun-Yan Zhu", "Tinghui Zhou", "Alexei A. Efros. Image-to-image translation with conditional adversarial networks"], "venue": "abs/1611.07004,", "citeRegEx": "Isola et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Variational image colorization models using higher-order", "author": ["Jung", "Kang", "2016] Miyoun Jung", "Myungjoo Kang"], "venue": null, "citeRegEx": "Jung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jung et al\\.", "year": 2016}, {"title": "Automatic colorization with deep convolutional generative adversarial networks. 2016", "author": ["Stephen Koo"], "venue": "[Koo,", "citeRegEx": "Koo.,? \\Q2016\\E", "shortCiteRegEx": "Koo.", "year": 2016}, {"title": "ACM Trans", "author": ["Anat Levin", "Dani Lischinski", "Yair Weiss. Colorization using optimization"], "venue": "Graph., 23(3):689\u2013694,", "citeRegEx": "Levin et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "In ICMLA 2016", "author": ["Matthias Limmer", "Hendrik P.A. Lensch. Infrared colorization using deep convolutional neural networks"], "venue": "Anaheim, CA, USA, December 18-20, 2016, pages 61\u201368,", "citeRegEx": "Limmer and Lensch. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "ACM Trans", "author": ["Xiaopei Liu", "Liang Wan", "Yingge Qu", "TienTsin Wong", "Stephen Lin", "Chi-Sing Leung", "PhengAnn Heng. Intrinsic colorization"], "venue": "Graph., 27(5):152:1\u2013152:9,", "citeRegEx": "Liu et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "CoRR", "author": ["Tung Duc Nguyen", "Kazuki Mori", "Ruck Thawonmas. Image colorization using a deep convolutional neural network"], "venue": "abs/1604.07904,", "citeRegEx": "Nguyen et al.. 2016a", "shortCiteRegEx": null, "year": 2016}, {"title": "In VISIGRAPP 2016 - Volume 4: VISAPP", "author": ["Van Nguyen", "Vicky Sintunata", "Terumasa Aoki. Automatic image colorization based on feature lines"], "venue": "Rome, Italy, February 27-29, 2016., pages 126\u2013133,", "citeRegEx": "Nguyen et al.. 2016b", "shortCiteRegEx": null, "year": 2016}, {"title": "CoRR", "author": ["Guim Perarnau", "Joost van de Weijer", "Bogdan Raducanu", "Jose M. \u00c1 lvarez. Invertible conditional gans for image editing"], "venue": "abs/1611.06355,", "citeRegEx": "Perarnau et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "ACM Trans", "author": ["Yingge Qu", "Tien-Tsin Wong", "PhengAnn Heng. Manga colorization"], "venue": "Graph., 25(3):1214\u20131220,", "citeRegEx": "Qu et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "In ICML 2016", "author": ["Scott E. Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee. Generative adversarial text to image synthesis"], "venue": "New York City, NY, USA, June 19-24, 2016, pages 1060\u20131069,", "citeRegEx": "Reed et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "U-net: Convolutional networks for biomedical image segmentation", "author": ["Olaf Ronneberger", "Philipp Fischer", "Thomas Brox"], "venue": "MICCAI 2015 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part III, pages 234\u2013241,", "citeRegEx": "Ronneberger et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In SIGGRAPH 2002", "author": ["Tomihisa Welsh", "Michael Ashikhmin", "Klaus Mueller. Transferring color to greyscale images"], "venue": "San Antonio, Texas, USA, July 23-26, 2002, pages 277\u2013280,", "citeRegEx": "Welsh et al.. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Lsun: Construction of a largescale image dataset using deep learning with humans in the loop", "author": ["Fisher Yu", "Yinda Zhang", "Shuran Song", "Ari Seff", "Jianxiong Xiao"], "venue": "CoRR, abs/1506.03365,", "citeRegEx": "Yu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Computer Vision - ECCV 2016 - 14th European Conference", "author": ["Richard Zhang", "Phillip Isola", "Alexei A. Efros. Colorful image colorization"], "venue": "Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III, pages 649\u2013666,", "citeRegEx": "Zhang et al.. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Early colorization methods [Levin et al., 2004; Qu et al., 2006] require users to provide considerable scribbles on the grayscale image, which is apparently timeconsuming and requires specialties.", "startOffset": 27, "endOffset": 64}, {"referenceID": 19, "context": "Early colorization methods [Levin et al., 2004; Qu et al., 2006] require users to provide considerable scribbles on the grayscale image, which is apparently timeconsuming and requires specialties.", "startOffset": 27, "endOffset": 64}, {"referenceID": 8, "context": "Then, following the Image Analogies framework [Hertzmann et al., 2001], the color is transferred onto the input image from analogous regions of the reference image(s) [Welsh et al.", "startOffset": 46, "endOffset": 70}, {"referenceID": 22, "context": ", 2001], the color is transferred onto the input image from analogous regions of the reference image(s) [Welsh et al., 2002; Liu et al., 2008; Gupta et al., 2012; Chia et al., 2011].", "startOffset": 104, "endOffset": 181}, {"referenceID": 15, "context": ", 2001], the color is transferred onto the input image from analogous regions of the reference image(s) [Welsh et al., 2002; Liu et al., 2008; Gupta et al., 2012; Chia et al., 2011].", "startOffset": 104, "endOffset": 181}, {"referenceID": 2, "context": ", 2001], the color is transferred onto the input image from analogous regions of the reference image(s) [Welsh et al., 2002; Liu et al., 2008; Gupta et al., 2012; Chia et al., 2011].", "startOffset": 104, "endOffset": 181}, {"referenceID": 1, "context": "Parametric methods, on the other hand, learn prediction functions from large datasets of color images in the training stage, posing the colorization problem as either regression in the continuous color space [Cheng et al., 2015; Deshpande et al., 2015; Zhang et al., 2016] or classification of quantized color values [Charpiat et al.", "startOffset": 208, "endOffset": 272}, {"referenceID": 3, "context": "Parametric methods, on the other hand, learn prediction functions from large datasets of color images in the training stage, posing the colorization problem as either regression in the continuous color space [Cheng et al., 2015; Deshpande et al., 2015; Zhang et al., 2016] or classification of quantized color values [Charpiat et al.", "startOffset": 208, "endOffset": 272}, {"referenceID": 24, "context": "Parametric methods, on the other hand, learn prediction functions from large datasets of color images in the training stage, posing the colorization problem as either regression in the continuous color space [Cheng et al., 2015; Deshpande et al., 2015; Zhang et al., 2016] or classification of quantized color values [Charpiat et al.", "startOffset": 208, "endOffset": 272}, {"referenceID": 12, "context": "Those methods all produce a deterministic mapping function, thus when an item could have diverse colors, their models tend to provide a weighted average brownish color as pointed out in [Koo, 2016].", "startOffset": 186, "endOffset": 197}, {"referenceID": 10, "context": "Unlike many other conditional GANs like [Isola et al., 2016] using convolution layers as encoder and deconvolution layers as decoder, we build a fully convolutional generator and each convolutional layer is splinted by a concatenate layer to continuously render the conditional grayscale information and a batch normalization layer to provide internal covariate shift.", "startOffset": 40, "endOffset": 60}, {"referenceID": 1, "context": "Thus we no longer need to train an additional independent model for each color scheme like [Cheng et al., 2015].", "startOffset": 91, "endOffset": 111}, {"referenceID": 1, "context": "[Cheng et al., 2015] used additionally trained model to handle diverse colorization of a scene image particularly in day and dawn.", "startOffset": 0, "endOffset": 20}, {"referenceID": 24, "context": "[Zhang et al., 2016] posed the diverse colorization problem as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "[Deshpande et al., 2016] learned a low dimensional embedding of color fields using a variational auto-encoder (VAE).", "startOffset": 0, "endOffset": 24}, {"referenceID": 20, "context": "[Reed et al., 2016] used text to generate image by applying adversarial networks.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "[Isola et al., 2016] provided a general-purpose image-to-image translation model that handles tasks like label to scene, aerial to map, day to night, edges to photo and also grayscale to color.", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "The high-level structure of our conditional GAN is consistent with traditional ones [Isola et al., 2016; Deshpande et al., 2016], while the detailed architecture of our generator G differs a lot.", "startOffset": 84, "endOffset": 128}, {"referenceID": 4, "context": "The high-level structure of our conditional GAN is consistent with traditional ones [Isola et al., 2016; Deshpande et al., 2016], while the detailed architecture of our generator G differs a lot.", "startOffset": 84, "endOffset": 128}, {"referenceID": 24, "context": "And additionally, many researches [Zhang et al., 2016; Isola et al., 2016; Deshpande et al., 2016] use superposition of multiple convolution layers with stride more than 1 to downsize the data shape, which works as a data encoder.", "startOffset": 34, "endOffset": 98}, {"referenceID": 10, "context": "And additionally, many researches [Zhang et al., 2016; Isola et al., 2016; Deshpande et al., 2016] use superposition of multiple convolution layers with stride more than 1 to downsize the data shape, which works as a data encoder.", "startOffset": 34, "endOffset": 98}, {"referenceID": 4, "context": "And additionally, many researches [Zhang et al., 2016; Isola et al., 2016; Deshpande et al., 2016] use superposition of multiple convolution layers with stride more than 1 to downsize the data shape, which works as a data encoder.", "startOffset": 34, "endOffset": 98}, {"referenceID": 16, "context": "Deconvolution layers are then used to upsize the data shape as a decoder of the data representation [Nguyen et al., 2016a; Isola et al., 2016; Deshpande et al., 2016].", "startOffset": 100, "endOffset": 166}, {"referenceID": 10, "context": "Deconvolution layers are then used to upsize the data shape as a decoder of the data representation [Nguyen et al., 2016a; Isola et al., 2016; Deshpande et al., 2016].", "startOffset": 100, "endOffset": 166}, {"referenceID": 4, "context": "Deconvolution layers are then used to upsize the data shape as a decoder of the data representation [Nguyen et al., 2016a; Isola et al., 2016; Deshpande et al., 2016].", "startOffset": 100, "endOffset": 166}, {"referenceID": 21, "context": "Other work [Ronneberger et al., 2015; Isola et al., 2016] also takes this spatial information into consideration.", "startOffset": 11, "endOffset": 57}, {"referenceID": 10, "context": "Other work [Ronneberger et al., 2015; Isola et al., 2016] also takes this spatial information into consideration.", "startOffset": 11, "endOffset": 57}, {"referenceID": 3, "context": "Most computer vision tasks use RGB representation like [Deshpande et al., 2015; Isola et al., 2016; Nguyen et al., 2016b] due to its generality.", "startOffset": 55, "endOffset": 121}, {"referenceID": 10, "context": "Most computer vision tasks use RGB representation like [Deshpande et al., 2015; Isola et al., 2016; Nguyen et al., 2016b] due to its generality.", "startOffset": 55, "endOffset": 121}, {"referenceID": 17, "context": "Most computer vision tasks use RGB representation like [Deshpande et al., 2015; Isola et al., 2016; Nguyen et al., 2016b] due to its generality.", "startOffset": 55, "endOffset": 121}, {"referenceID": 24, "context": "Other kinds of representations are also included like Lab [Charpiat et al., 2008; Zhang et al., 2016; Limmer and Lensch, 2016], Y UV (or Y CrCb) [Cheng et al.", "startOffset": 58, "endOffset": 126}, {"referenceID": 14, "context": "Other kinds of representations are also included like Lab [Charpiat et al., 2008; Zhang et al., 2016; Limmer and Lensch, 2016], Y UV (or Y CrCb) [Cheng et al.", "startOffset": 58, "endOffset": 126}, {"referenceID": 1, "context": ", 2016; Limmer and Lensch, 2016], Y UV (or Y CrCb) [Cheng et al., 2015].", "startOffset": 51, "endOffset": 71}, {"referenceID": 10, "context": "Multi-layer noise In the work of [Isola et al., 2016], they mentioned noise ignorance while training the generator.", "startOffset": 33, "endOffset": 53}, {"referenceID": 10, "context": "Though the \u201cUNet\u201d skip structure of [Isola et al., 2016] can also help posterior layers receive conditional information, our model modification is still more straightforward and convenient.", "startOffset": 36, "endOffset": 56}, {"referenceID": 23, "context": "There are various kinds of color image datasets, and we choose the open LSUN bedroom dataset1 [Yu et al., 2015] to conduct our experiment.", "startOffset": 94, "endOffset": 111}, {"referenceID": 18, "context": "That is why many of their models [Jung and Kang, 2016; Perarnau et al., 2016; Deshpande et al., 2016] take image distance like RMSE (Root Mean Square Error) and PSNR (Peak Signal-to-Noise Ratio) as their measurements.", "startOffset": 33, "endOffset": 101}, {"referenceID": 4, "context": "That is why many of their models [Jung and Kang, 2016; Perarnau et al., 2016; Deshpande et al., 2016] take image distance like RMSE (Root Mean Square Error) and PSNR (Peak Signal-to-Noise Ratio) as their measurements.", "startOffset": 33, "endOffset": 101}, {"referenceID": 9, "context": "And others [Iizuka et al., 2016; Isola et al., 2016] use additional classifiers to predict if colorized image can be detected or still correctly classified.", "startOffset": 11, "endOffset": 52}, {"referenceID": 10, "context": "And others [Iizuka et al., 2016; Isola et al., 2016] use additional classifiers to predict if colorized image can be detected or still correctly classified.", "startOffset": 11, "endOffset": 52}, {"referenceID": 1, "context": "Note that some previous work [Cheng et al., 2015; Dong et al., 2017; Nguyen et al., 2016b; Nguyen et al., 2016a] do not provide quantified measurements.", "startOffset": 29, "endOffset": 112}, {"referenceID": 5, "context": "Note that some previous work [Cheng et al., 2015; Dong et al., 2017; Nguyen et al., 2016b; Nguyen et al., 2016a] do not provide quantified measurements.", "startOffset": 29, "endOffset": 112}, {"referenceID": 17, "context": "Note that some previous work [Cheng et al., 2015; Dong et al., 2017; Nguyen et al., 2016b; Nguyen et al., 2016a] do not provide quantified measurements.", "startOffset": 29, "endOffset": 112}, {"referenceID": 16, "context": "Note that some previous work [Cheng et al., 2015; Dong et al., 2017; Nguyen et al., 2016b; Nguyen et al., 2016a] do not provide quantified measurements.", "startOffset": 29, "endOffset": 112}, {"referenceID": 10, "context": "Therefore, just like some previous researches [Isola et al., 2016; Zhang et al., 2016], we provide questionnaire surveys as a Turing test to measure our colorization results.", "startOffset": 46, "endOffset": 86}, {"referenceID": 24, "context": "Therefore, just like some previous researches [Isola et al., 2016; Zhang et al., 2016], we provide questionnaire surveys as a Turing test to measure our colorization results.", "startOffset": 46, "endOffset": 86}], "year": 2017, "abstractText": "Colorization of grayscale images has been a hot topic in computer vision. Previous research mainly focuses on producing a colored image to match the original one. However, since many colors share the same gray value, an input grayscale image could be diversely colored while maintaining its reality. In this paper, we design a novel solution for unsupervised diverse colorization. Specifically, we leverage conditional generative adversarial networks to model the distribution of real-world item colors, in which we develop a fully convolutional generator with multi-layer noise to enhance diversity, with multi-layer condition concatenation to maintain reality, and with stride 1 to keep spatial information. With such a novel network architecture, the model yields highly competitive performance on the open LSUN bedroom dataset. The Turing test of 80 humans further indicates our generated color schemes are highly convincible.", "creator": "LaTeX with hyperref package"}}}