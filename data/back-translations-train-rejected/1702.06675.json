{"id": "1702.06675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Context-Aware Prediction of Derivational Word-forms", "abstract": "Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose the new task of predicting the derivational form of a given base-form lemma that is appropriate for a given context. We present an encoder--decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under a lexicon agnostic setting.", "histories": [["v1", "Wed, 22 Feb 2017 04:50:23 GMT  (160kb,D)", "http://arxiv.org/abs/1702.06675v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ekaterina vylomova", "ryan cotterell", "timothy baldwin", "trevor cohn"], "accepted": false, "id": "1702.06675"}, "pdf": {"name": "1702.06675.pdf", "metadata": {"source": "CRF", "title": "Context-Aware Prediction of Derivational Word-forms", "authors": ["Ekaterina Vylomova", "Ryan Cotterell", "Timothy Baldwin", "Trevor Cohn"], "emails": ["evylomova@gmail.com", "ryan.cotterell@gmail.com", "tbaldwin@unimelb.edu.au", "tcohn@unimelb.edu.au"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Derivational Morphology", "text": "Morphology, the linguistic study of the inner structure of words, has two main objectives: (1) to describe the relationship between different words in the lexicon; and (2) to break down words into morphemes, the smallest linguistic units that carry meaning. Morphology can be divided into two types: inflective and derivative. Inflective morphology is the group of processes through which the word form outwardly displays syntactical information, such as verb tense. It follows that an inflective affix typically does not change either the part of speech (POS) or the semantics of the word. For example, the English verb \"run\" takes different forms: run, runs and ran, all of which convey the concept of \"moving by foot rapid,\" but occur in complementary syntactic contexts. Derivation, on the other hand, deals with the formation of new words that exhibit semantic shifts of meaning (often including POS) and is closely interwoven with the concept of \"moving by foot rapid,\" but unambiguously occurring in the \"contextual\" 7. \""}, {"heading": "3 Related Work", "text": "Although many neural morphological models have been proposed in recent years, most of them focus on inflection morphology (see e.g. Cotterell et al. (2016a)). With a focus on derivative processes, there are three main lines of research: The first deals with the evaluation of word embeddings either by a word analogy task (Gladkova et al., 2016) or the classification of binary relationships (Vylomova et al., 2016). In this context, it has been shown that in contrast to inflection morphology, most derivative relationships cannot be easily captured with distribution methods. Researchers working on the second type of task attempt to model derivative forms using the embedding of their corresponding basic form and a vector that encodes a \"derivative\" shift. Guevara (2011) notes that derivative affixes can be modelled as a geometric function via the Kiss derivatives of the basic forms."}, {"heading": "4 Dataset", "text": "We used the English dataset from CELEX (Baayen et al., 1993) as a starting point for the construction of our dataset. We extracted verb-noun-lemmas from CELEX, which covered 24 different nominal suffixes and 1,456 base lemmas. Suffixes, which occurred only in 5 or fewer lemmas, largely corresponded to loanwords and were thus filtered out. We expanded this dataset to include verb-verb pairs, one for each verb-noun pair present in the verb-noun pairs, to cover the case of a verbal form appropriate to the given context."}, {"heading": "5 Experiments", "text": "In this paper, we model derivative morphology as a predictive task formulated as follows: We take sentences that contain a derivative form of a given dim, and then obscure the derivative form by replacing it with its basic form dilemma, and the system must then predict the original (derivative) form that can take advantage of the sentence context. System predictions are judged correct if they are accurate 1We have also experimented without verb-verb pairs and have not observed much difference in outcomes.2Based on a 2008 / 03 / 12 dump. Sentences shorter than 3 words or longer than 50 words have been removed from datasets.3The code and dataset are available at https: / / github.com / ivri / dmorphmatch of the original derivative form."}, {"heading": "5.1 Baseline", "text": "As a baseline, we considered a trigram model with modified Kneser-Ney smoothing, trained on the training data set. Each sentence in the test data was supplemented by a series of confabulated sentences, replacing a target word with other derivatives or a base form. In contrast to the general task, in which we generate word forms as character strings, we use a series of well-known inflected forms for each (from the training data). We then use the language model to evaluate the collection of test sentences, and select the variant with the highest score of the language model and evaluate the accuracy of the selection of the original word form."}, {"heading": "5.2 Encoder\u2013Decoder Model", "text": "We propose an encoder decoder model, which combines the left and the right context and a representation of the basic form at the drawing level: t = max (0, H \u00b7 [h \u00b7 left; h \u2190 left; h \u2192 right; h \u2190 right; h \u2192 base; h \u2190 base] + bh), where h \u2192 left, h \u2190 left, h \u2192 right, h \u2190 right, h \u2192 base, h \u2190 base correspond to the last hidden states of an LSTM (Hochreiter and Schmidhuber, 1997) via the left and right context and the representation of the basic form at the drawing level (applied both forwards and backwards); H \u00b2 R [h \u00d7 l \u00d7 6] is a weight matrix, and bh \u00b2 R [h \u00d7 l \u00d7 1,5] is a bias term (h \u00d7 l \u00d7 1,5], representing the forward and backward model of the model."}, {"heading": "5.3 Settings", "text": "We used a three-layer bidirectional LSTM network with hidden dimensionality h for both contextual and base stem states of 100 and character embedding cj of 100.5. We used pre-trained 300-dimensional word embedding from Google News (Mikolov et al., 2013a; Mikolov et al., 2013b). During the training of the model, we recorded the word embedding to ensure better applicability to invisible test instances. All tokens not present in this set were replaced by UNK Guardian tokens. The network was trained with dynamics up to convergence with SGD."}, {"heading": "5.4 Results", "text": "With the encoder decoder model, we experimented with the encoder decoder symbol as described in Section 5.2 (\"biLSTM + CTX + BS\"), as well as with several variations, namely: without context information (\"biLSTM + BS\") and without the bidirectional stem (\"biLSTM + CTX\"). We also investigated how much improvement we could get from knowing the POS tag of the derived form by explicitly presenting it to the model as an additional conditional context (\"biLSTM + CTX + POS\"). The main motivation for this refers to Gerunds, where without the4We tried to feed the context information only in the initial step, and this led to poorer predictions in terms of context-sensitive suffixes.5 We also experimented with 15 dimensions, but found that this model worse.xOS, the model, often generates nominations."}, {"heading": "5.5 Error Analysis", "text": "We performed error analyses of the generated forms of the LSTM + CTX + BS + POS model, sometimes struggling to distinguish between nominal suffixes: in some cases, the model places an agent suffix (-er or -or) in contexts where a nonagentive nominalization (e.g. -ation or -ment) is appropriate. To illustrate this, Figure 2 is a t-SNE projection of the context representations for simulation vs. simulation that show that the various nominal forms have strong overlaps. Second, although the model learns whether it is producing a new symbol well, some forms will be misspelled. Examples of this are studint, studion or even studyant as the active nominalization of the study. Here, the problem is the opacity in the etymology with which the student borrowed from the old French student."}, {"heading": "6 Conclusions and Future Work", "text": "We investigated the novel task of context-sensitive derivative prediction for English and proposed an encoder-decoder model for generating nominations. Our best model achieved an accuracy of 90% for a common lexicon and 66% for a shared lexicon. This indicates that there is regularity in derivative processes, and in many cases the context is actually indicative. As we have already mentioned, there are still many open questions that we leave open for future work. In addition, we plan to scale our dataset to other languages and expand it with wiki data to realize a much wider range and variety of derivative forms."}, {"heading": "7 Acknowledgments", "text": "We thank all reviewers for their valuable comments and suggestions. The second author was supported by a DAAD Long-Term Research Grant and an NDSEG Fellowship. This research was partially supported by the Australian Research Council."}], "references": [{"title": "The CELEX lexical data base on CDROM", "author": ["Harald R Baayen", "Richard Piepenbrock", "H van Rijn"], "venue": null, "citeRegEx": "Baayen et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Baayen et al\\.", "year": 1993}, {"title": "Neural machine translation by jointly", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Joint semantic synthesis and morphological analysis of the derived word", "author": ["Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "CoRR, abs/1701.00946.", "citeRegEx": "Cotterell and Sch\u00fctze.,? 2017", "shortCiteRegEx": "Cotterell and Sch\u00fctze.", "year": 2017}, {"title": "Labeled morphological segmentation with semi-markov models", "author": ["Ryan Cotterell", "Thomas M\u00fcller", "Alexander Fraser", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 19th Conference on Computational Natural Language Learning (CoNLL 2015), pages 164\u2013", "citeRegEx": "Cotterell et al\\.,? 2015", "shortCiteRegEx": "Cotterell et al\\.", "year": 2015}, {"title": "The SIGMORPHON 2016 shared task morphological reinflection", "author": ["Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "David Yarowsky", "Jason Eisner", "Mans Hulden."], "venue": "Proceedings of the 14th SIGMORPHON Workshop on Computational Re-", "citeRegEx": "Cotterell et al\\.,? 2016a", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "A joint model of orthography and morphological segmentation", "author": ["Ryan Cotterell", "Tim Vieira", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Cotterell et al\\.,? 2016b", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn\u2019t", "author": ["Anna Gladkova", "Aleksandr Drozd", "Satoshi Matsuoka."], "venue": "Proceedings of the 15th Annual Conference of the North", "citeRegEx": "Gladkova et al\\.,? 2016", "shortCiteRegEx": "Gladkova et al\\.", "year": 2016}, {"title": "Argument structure", "author": ["Jane Grimshaw."], "venue": "The MIT Press, Cambridge, MA, US.", "citeRegEx": "Grimshaw.,? 1990", "shortCiteRegEx": "Grimshaw.", "year": 1990}, {"title": "Computing semantic compositionality in distributional semantics", "author": ["Emiliano Guevara."], "venue": "Proceedings of the 9th International Conference on Computational Semantics, pages 135\u2013144. Association for Computational Linguistics.", "citeRegEx": "Guevara.,? 2011", "shortCiteRegEx": "Guevara.", "year": 2011}, {"title": "Kenlm: Faster and smaller language model queries", "author": ["Kenneth Heafield."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187\u2013197. Association for Computational Linguistics.", "citeRegEx": "Heafield.,? 2011", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Incorporating side information into recurrent neural network language models", "author": ["Cong Duy Vu Hoang", "Trevor Cohn", "Gholamreza Haffari."], "venue": "Proceedings of the 15th Annual Conference of the North", "citeRegEx": "Hoang et al\\.,? 2016", "shortCiteRegEx": "Hoang et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Singlemodel encoder-decoder with explicit morphological representation for reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 54st Annual Meeting of the Association for Computational Linguistics (ACL 2016).", "citeRegEx": "Kann and Sch\u00fctze.,? 2016", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Obtaining a better understanding of distributional models of german derivational morphology", "author": ["Max Kisselew", "Sebastian Pad\u00f3", "Alexis Palmer", "Jan \u0160najder."], "venue": "Proceedings of the 11th International Conference on Computational Semantics", "citeRegEx": "Kisselew et al\\.,? 2015", "shortCiteRegEx": "Kisselew et al\\.", "year": 2015}, {"title": "Compositionally derived representations of morphologically complex words in distributional semantics", "author": ["Angeliki Lazaridou", "Marco Marelli", "Roberto Zamparelli", "Marco Baroni."], "venue": "Proceedings of the 51st Annual Meeting of the Association for", "citeRegEx": "Lazaridou et al\\.,? 2013", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2013}, {"title": "Morphological cues for lexical semantics", "author": ["Marc Light."], "venue": "Proceedings of the 34st Annual Meeting of the Association for Computational Linguistics (ACL 1996), pages 25\u201331.", "citeRegEx": "Light.,? 1996", "shortCiteRegEx": "Light.", "year": 1996}, {"title": "Visualizing data using t-SNE", "author": ["Laurens van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9(Nov):2579\u20132605.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the Workshop at the International Conference on Learning Representations, 2013.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of Neural Information Processing Systems Conference (NIPS 2013), pages", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "On the status of vowel shift in English", "author": ["Arlene Moskowitz."], "venue": "Timothy E. Moore, editor, Cognitive Development and the Acquisition of Language. Academic Press.", "citeRegEx": "Moskowitz.,? 1973", "shortCiteRegEx": "Moskowitz.", "year": 1973}, {"title": "Predictability of distributional semantics in derivational word formation", "author": ["Sebastian Pad\u00f3", "Aur\u00e9lie Herbelot", "Max Kisselew", "Jan \u0160najder."], "venue": "Proceedings of the 26th International Conference on Computational Linguistics (COLING 2016), pages", "citeRegEx": "Pad\u00f3 et al\\.,? 2016", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2016}, {"title": "Lexical derivation", "author": ["John TE Richardson."], "venue": "Journal of Psycholinguistic Research, 6(4):319\u2013336.", "citeRegEx": "Richardson.,? 1977", "shortCiteRegEx": "Richardson.", "year": 1977}, {"title": "The relation between reading ability and morphological skills: Evidence from derivational suffixes", "author": ["Maria Singson", "Diana Mahony", "Virginia Mann."], "venue": "Reading and writing, 12(3):219\u2013252.", "citeRegEx": "Singson et al\\.,? 2000", "shortCiteRegEx": "Singson et al\\.", "year": 2000}, {"title": "Spelling, phonology, and the older student", "author": ["Shane Templeton."], "venue": "Developmental and cognitive aspects of learning to spell: A reflection of word knowledge, pages 85\u201396.", "citeRegEx": "Templeton.,? 1980", "shortCiteRegEx": "Templeton.", "year": 1980}, {"title": "The teaching of English suffixes, volume 847", "author": ["Edward Lee Thorndike."], "venue": "Teachers College, Columbia University.", "citeRegEx": "Thorndike.,? 1941", "shortCiteRegEx": "Thorndike.", "year": 1941}, {"title": "Take and took, gaggle and goose, book and read: evaluating the utility of vector differences for lexical relation learning", "author": ["Ekaterina Vylomova", "Laura Rimmel", "Trevor Cohn", "Timothy Baldwin."], "venue": "Proceedings of the 54th Annual Meeting of the Asso-", "citeRegEx": "Vylomova et al\\.,? 2016", "shortCiteRegEx": "Vylomova et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 25, "context": "(Thorndike, 1941) analysed ambiguity of derivational suffixes themselves when the same suffix might present different semantics depending on the base form it is attached to (cf.", "startOffset": 0, "endOffset": 17}, {"referenceID": 22, "context": "Furthermore, as Richardson (1977) previously", "startOffset": 16, "endOffset": 34}, {"referenceID": 23, "context": "the context of studying how children master derivations (Singson et al., 2000).", "startOffset": 56, "endOffset": 78}, {"referenceID": 24, "context": "This observation confirms an earlier idea that orthographical regularities provide a clearer clues to morphological transformations comparing to phonological rules (Templeton, 1980; Moskowitz, 1973), especially in lanar X iv :1 70 2.", "startOffset": 164, "endOffset": 198}, {"referenceID": 20, "context": "This observation confirms an earlier idea that orthographical regularities provide a clearer clues to morphological transformations comparing to phonological rules (Templeton, 1980; Moskowitz, 1973), especially in lanar X iv :1 70 2.", "startOffset": 164, "endOffset": 198}, {"referenceID": 22, "context": "the context of studying how children master derivations (Singson et al., 2000). In their work, children were asked to complete a sentence by choosing one of four possible derivations. Each derivation corresponded either to a noun, verb, adjective, or adverbial form. Singson et al. (2000) showed that childrens\u2019 ability to recognize the correct form correlates with their reading ability.", "startOffset": 57, "endOffset": 289}, {"referenceID": 18, "context": "Deep learning models capable of learning real-valued word embeddings have been shown to perform well on a range of tasks, from language modelling (Mikolov et al., 2013a) to parsing (Dyer et al.", "startOffset": 146, "endOffset": 169}, {"referenceID": 6, "context": ", 2013a) to parsing (Dyer et al., 2015) and machine translation (Bahdanau et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 1, "context": ", 2015) and machine translation (Bahdanau et al., 2015).", "startOffset": 32, "endOffset": 55}, {"referenceID": 13, "context": "Recently, these models have also been successfully applied to morphological reinflection tasks (Kann and Sch\u00fctze, 2016; Cotterell et al., 2016a).", "startOffset": 95, "endOffset": 144}, {"referenceID": 4, "context": "Recently, these models have also been successfully applied to morphological reinflection tasks (Kann and Sch\u00fctze, 2016; Cotterell et al., 2016a).", "startOffset": 95, "endOffset": 144}, {"referenceID": 16, "context": "Derivation, on the other hand, deals with the formation of new words that have semantic shifts in meaning (often including POS) and is tightly intertwined with lexical semantics (Light, 1996).", "startOffset": 178, "endOffset": 191}, {"referenceID": 7, "context": "The first deals with the evaluation of word embeddings either using a word analogy task (Gladkova et al., 2016) or binary relation type classification (Vylomova et al.", "startOffset": 88, "endOffset": 111}, {"referenceID": 26, "context": ", 2016) or binary relation type classification (Vylomova et al., 2016).", "startOffset": 47, "endOffset": 70}, {"referenceID": 3, "context": ", see Cotterell et al. (2016a)).", "startOffset": 6, "endOffset": 31}, {"referenceID": 8, "context": "Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms.", "startOffset": 0, "endOffset": 15}, {"referenceID": 8, "context": "Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms. On the other hand, Lazaridou et al. (2013) and Cotterell and Sch\u00fctze (2017) represent derivational affixes as vectors and investigate various functions to combine them with base forms.", "startOffset": 0, "endOffset": 168}, {"referenceID": 2, "context": "(2013) and Cotterell and Sch\u00fctze (2017) represent derivational affixes as vectors and investigate various functions to combine them with base forms.", "startOffset": 11, "endOffset": 40}, {"referenceID": 21, "context": "(2015) and Pad\u00f3 et al. (2016) extend this line of", "startOffset": 11, "endOffset": 30}, {"referenceID": 8, "context": "This work demonstrates that various factors such as part of speech, semantic regularity and argument structure (Grimshaw, 1990) influence the predictability of a derived word.", "startOffset": 111, "endOffset": 127}, {"referenceID": 3, "context": ", unhappiness7\u2192un+happy+ness (Cotterell et al., 2015; Cotterell et al., 2016b).", "startOffset": 29, "endOffset": 78}, {"referenceID": 5, "context": ", unhappiness7\u2192un+happy+ness (Cotterell et al., 2015; Cotterell et al., 2016b).", "startOffset": 29, "endOffset": 78}, {"referenceID": 0, "context": "dataset, we used the CELEX English dataset (Baayen et al., 1993).", "startOffset": 43, "endOffset": 64}, {"referenceID": 12, "context": "where hleft, h \u2190 left, h \u2192 right, h \u2190 right, h \u2192 base,h \u2190 base correspond to the last hidden states of an LSTM (Hochreiter and Schmidhuber, 1997) over left and right", "startOffset": 111, "endOffset": 145}, {"referenceID": 11, "context": "It has been previously shown (Hoang et al., 2016) that this yields better results than other means of incorporation.", "startOffset": 29, "endOffset": 49}, {"referenceID": 18, "context": "5 We used pre-trained 300-dimensional Google News word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b).", "startOffset": 66, "endOffset": 112}, {"referenceID": 19, "context": "5 We used pre-trained 300-dimensional Google News word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b).", "startOffset": 66, "endOffset": 112}, {"referenceID": 17, "context": "Figure 2: An example of t-SNE projection (Maaten and Hinton, 2008) of context representations for simulate", "startOffset": 41, "endOffset": 66}], "year": 2017, "abstractText": "Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose the new task of predicting the derivational form of a given base-form lemma that is appropriate for a given context. We present an encoder\u2013 decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under a lexicon agnostic setting.", "creator": "TeX"}}}