{"id": "1702.06700", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Task-driven Visual Saliency and Attention-based Visual Question Answering", "abstract": "Visual question answering (VQA) has witnessed great progress since May, 2015 as a classic problem unifying visual and textual data into a system. Many enlightening VQA works explore deep into the image and question encodings and fusing methods, of which attention is the most effective and infusive mechanism. Current attention based methods focus on adequate fusion of visual and textual features, but lack the attention to where people focus to ask questions about the image. Traditional attention based methods attach a single value to the feature at each spatial location, which losses many useful information. To remedy these problems, we propose a general method to perform saliency-like pre-selection on overlapped region features by the interrelation of bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication based attention method to capture more competent correlation information between visual and textual features. We conduct experiments on the large-scale COCO-VQA dataset and analyze the effectiveness of our model demonstrated by strong empirical results.", "histories": [["v1", "Wed, 22 Feb 2017 08:19:38 GMT  (907kb,D)", "http://arxiv.org/abs/1702.06700v1", "8 pages, 3 figures"]], "COMMENTS": "8 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.NE", "authors": ["yuetan lin", "zhangyang pang", "donghui wang", "yueting zhuang"], "accepted": false, "id": "1702.06700"}, "pdf": {"name": "1702.06700.pdf", "metadata": {"source": "CRF", "title": "Task-driven Visual Saliency and Attention-based Visual Question Answering", "authors": ["Yuetan Lin", "Zhangyang Pang", "Donghui Wang", "Yueting Zhuang"], "emails": ["linyuetan@zju.edu.cn", "pzy@zju.edu.cn", "dhwang@zju.edu.cn", "yzhuang@zju.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "2. Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Saliency Detection Modeling", "text": "Emphasis methods mimic human attention in psychology, including bottom-up and top-down manners [29]. Typical highlighting methods [10, 18] are pixel-oriented or object-oriented, which are not suitable for detecting center highlighting, and have difficulties in capturing large-scale eye-tracking data. We think task-based highlighting of image features may be helpful in solving the problem of highlighting highlighting characteristics. What inspires us is that BiLSTM has performed well in detecting highlighting characteristics in text and video tasks. [16] In mood classification tasks, words related to sensation are used to visualize and understand the effects of BiLSTM in text sentences. During the highlighting of our auto-highlighting cursor in the auto-highlighting cursor, LSTM has not been used for highlighting in text sentences."}, {"heading": "2.2. Attention in VQA Models", "text": "The visual attention mechanism has aroused great interest in VQA [34, 36, 26] and achieved an improvement in performance compared to conventional methods that use holistic image characteristics. The attention mechanism is typically the weighted sum of characteristics of the image region in each spatial location where weights describe the correlation and are implemented as internal products of the question and image characteristics. It examines finer-grained visual characteristics and mimics the behavior that people perceive in different areas depending on the question posed. [34] it focuses on the \"knowledge where to look\" for multiple-choice VQA tasks, [26] uses 99 recognized object regions plus a holistic image characteristic to establish a correlation with the question coding, and uses the correlation values as weights to merge the characteristics. [34] uses the last pooling layer characteristics (512 x 14 x 14) of VGG-19 to obtain an integral image relationship [27]."}, {"heading": "3. Proposed Method", "text": "Compared to image captions that generate general descriptions of an image, VQA focuses on specific image regions depending on the issue. On the one hand, these regions include non-object and background content that is difficult for object recognition-based VQA methods. On the other hand, although people can ask questions at any point in a particular image, there are always some regional patterns that raise more questions. Broadly speaking, there are statistical regional interest patterns (RoI) that represent human-interest areas that are important for a later VQA task. We propose an outstanding regional preselection and an attention-based VQA framework that is shown in Figure 2. VQA is considered a classification task that can be easily and easily converted into a generation- or scoring model."}, {"heading": "3.1. Model", "text": "In this section, we elaborate our model, which consists of four parts: (a) image feature preselection, which models the tendency for people to focus on asking questions; (b) question encoding, which encodes the question words as compressed semantic embedding; (c) attention-based feature fusion, which performs a second selection to image features; and (d) response generation, which delivers the response output."}, {"heading": "3.1.1 Image Feature Pre-selection", "text": "As described above, current object-oriented VQA methods may not be qualified, and the answers cannot be derived from these specific object regions in images, for example, when asked, \"Where is the bird / cat?,\" the answers, \"Fence / sink\" are not included in ILSVRC [25] (200 categories) and Pascal VOC [5] (20 categories), when using a more general pattern recognition (20 categories), but it is not the only case for the VQA task. The current attention mechanism refers to the question where people focus on asking questions. General Visual Salience provides useful analog information about recognizable objects or areas that exceed the environment, but it is not the only case for the VQA task."}, {"heading": "3.1.2 Question Encoding", "text": "The question can be encoded using various methods of Natural Language Processing (NLP), such as BoW, LSTM, CNN [20, 34], gated recurrent units (GRU) [4], skipthought vectors [14], or it can be analyzed by Stanford Parser [15], etc. Since question-BoW encodings already dominate the contribution to response generation compared to image characteristics [35], we simply encode the question word as a word2vec embedding and use LSTM to encode the questions in such a way that they correspond to the selected regional characteristics. In order to encode more abstract and superior information and achieve better performance, our model adopts a deeper LSTM [3, 12] for question encoding. In our model, the question encoding LSTM shows hidden layers with r hidden units per layer, and the question representation is the last output and the resulting cell units of the STx and the STM = the STx and the resulting dimension."}, {"heading": "3.1.3 Attention-based Feature Fusion", "text": "According to the Triple Statistical Image-Question-Answer-Training (IQA), the preselection of image characteristics has linked the regions with different previous weight classes, resulting in more meaningful regional characteristics. However, different questions can focus on different aspects of visual content. It is necessary to use the attention mechanism to second regions by asking for more effective traits. We propose a novel attention method that records the elementary multiplication vector as a correlation between image and question characteristics in any spatial location. In particular, given the selected regional characteristics and question embedding, we map the visual and textual characteristics into a common space of the dC dimension and perform an elementary multiplication between them. The n \u00d7 n \u00d7 dC dimensional merged characteristics contain visual and textual information, and higher responses point to more correlative characteristics. In traditional attention models, the correlation between the inner and the molecular characteristics can be achieved by means of the correlation between the textual characteristics."}, {"heading": "3.1.4 Answer Generation", "text": "Taking the VQA problem as a classification task is easy to implement and evaluate, and it is easy to extend it to generation or multiple-choice tasks by a network operation using the Fused function used in the previous step. We use a linear level and a Softmax level to map from the Fused function to the answer candidates, of which the entries are the top 1000 responses from the training data. Considering multiple-choice VQA problems, e.g. Visual7W [36], which answer questions, and COCO-VQA [3] multiple-choice tasks, our model is adaptively extendable by linking the question and answer vectors with visual characteristics before fusion, or by using bilinear models between the final Fused feature and the answer function [26, 11] which is a possible future work. In the meantime, given the VA Generation problem, we can train the Phra6 to use the VM function, or use the STLused."}, {"heading": "3.2. Training", "text": "Our framework is trained end-to-end using back propagation, while the feature extraction part remains fixed using ResNet to speed up training and avoid the noisy gradients propagated backwards by the LSTM, as explained in [6].The RMSprop algorithm is used with a low initial learning rate of 3e-4, which is important to prevent the Softmax from rising too early and dominating the visual features too prematurely [26].Due to the simplicity and proven performance of pre-trained word embedding parameters, we initialize the network parameters with random numbers. We randomly sample 500 IQA tripling each iteration."}, {"heading": "4. Experiments", "text": "In this section, we describe the implementation details and evaluate our model (SalAtt) on the large-scale COCOVQA dataset. We also visualize and analyze the role of pre-selection and the novel attention method."}, {"heading": "4.1. Implementation Details", "text": "In our experiment, the input images are first scaled to 448 x 448 x 3 pixels before applying 4 x 4 raster to them. We get 3 x 3 regions by using 2 x 2 raster (i.e. 224 x 224 x 3 pixels) as a region with step 1 raster. Then, we extract the 2048-D feature per region from the layer before the last fully joined layer of ResNet. The dimension of the word embedding is 200, and the weights of the embedding are randomly initialized from an even distribution to [\u2212 0.08, 0.08), due to similar performance to the pre-trained layer. BiLSTM for regional features has one layer and the size is 1 layer, and the LSTM for question uses 2 layers and 512 hidden units per layer. The common space of visual and textual features is 1024-sional."}, {"heading": "4.2. Datasets", "text": "The COCO-VQA dataset [3] is the largest of the commonly used VQA datasets, containing two tasks (multiple choice task and perpetual task) on two sets of images (real picture MSCOCO dataset [17] and abstract scene dataset). We follow common practice for evaluating models on two tasks on the real picture dataset, which includes 248,349 training questions, 121,512 validation questions and 244,302 test questions. There are many types of questions that require image and question understanding, sound knowledge, knowledge conclusions and even external knowledge. Answers are roughly divided into 3 types, i.e. \"yes / no,\" \"number\" and \"others.\" To evaluate the results, each answer is compared with 10 humanly labeled answers, calculating the accuracy based on this metric: min (# consistent human-labeled answers, 3 times, \"number\" and \"others.\" To evaluate the results, each answer is compared with 10 humanely labeled answers, calculating the accuracy based on this metric: min (# consistent human-labeled answers, 3 times), i.e. the QA is the least likely to be predicted with the least accurate, 1%."}, {"heading": "4.3. Compared Models", "text": "We compare our proposed model (SalAtt) with some function-disabled models listed below in order to prove the effectiveness of regional preselection using BiLSTM and the novel attention method. \u2022 Holistic: The basic model that maps the holistic image feature and the LSTM-encoded question feature to a common space and performs an elementary multiplication between them. \u2022 TraAtt: The traditional attention model, the implementation of the WTL model [26] using the same 3 \u00d7 3 regions in the SalAtt model. \u2022 RegAtt: The regional attention model that applies our novel attention method, such as the SalAtt model, but without regional preselection. \u2022 ConAtt: The revolutionary regional preselection model that replaces the BiLSTM in the SalAtt model with a linear weight division that is implemented by a revolutionary layer, such as the SalAtt model [N, but without regional preselection.] Furthermore, we compare our WIMCO-W31 model with the WIMCO, WIMAtt-26.]"}, {"heading": "4.4. Results and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.4.1 Effectiveness of Proposed Functions", "text": "The columns show: (1) Holistic is better than TraAtt, demonstrating the effectiveness of merging elementary multiplication traits compared to concatenating traits. (2) RegAtt is better than holistic, suggesting that our novel attention method actually enriches visual traits and improves performance. (3) SalAtt is better than RegAtt, demonstrating the strength of our regional pre-selection mechanism. (4) ConAtt is worse than SalAtt, showing that BiLSTM is important for the regional pre-selection part. From each line we find the consistent improvement through ResNet traits, showing the importance of good CNN traits for VQA."}, {"heading": "4.4.2 Quantitative Results on Evaluation Server", "text": "Our results are comparative or higher than the attention-based methods, especially for multiple-choice tasks. The results of the \"Other\" answer type, which includes object and scene questions, demonstrate the competence of our model in RoI detection. Note that we only apply the proposed regional pre-selection mechanism to the basic VQA model [3], it can be embedded in all other attention-based models to improve its performance. Due to the calculation and training time, we only use 3 x 3 regions (e.g. 100 or 14 x 14 regional features) compared to other attention-based methods. By observing, we find that many small objects could not be divided by the 3 x 3 regions, which negatively affects the counting questions and could be further improved and represents a possible future work."}, {"heading": "4.4.3 Visualization of Intermediate Results", "text": "We illustrate three groups of samples generated by our model in Figure 3. Each group contains four numbers, from left to right and from top to bottom, each consisting of the original image, pre-selection weights in the image and two attention cards for different questions with the corresponding questions (Q), answers to the truth on the ground (A) and the predicted answers (P) below. And the number in parentheses means the amount for this human-marked answer entry. The weights are normalized to a minimum of 0 and a maximum of 1 for improving visualization, i.e. the weight in the dark region must not necessarily be 0.0. Take the first sample, the pre-selection operation places a high emphasis on the head region of the boy, which may be interesting for people and attract more questions (e.g. questions with the word \"boy\"). In the question \"Is the boy dressed for the weather?,\" the attention card focuses on the boy, his clothing and the surrounding regions, so the positive question can be answered by the tourist."}, {"heading": "5. Conclusion", "text": "In this paper, we propose a general VQA solution that integrates regional pre-selection and a novel attention method to capture generic class regions and richer merged feature representations, both of which are independent and now contribute to better VQA performance. Although the model is simple, it achieves comparative or higher empirical results than state-of-the-art models. Possible future work includes the introduction of finer-grained grids that capture more precise regions, the use of stacked attention layers for multi-level reasoning and more accurate response localization, and the application of the general pre-selection method to other attention-based VQA models. The preselection mechanism is valuable and applicable to tasks similar to captions."}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "NAACL,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "CVPR, pages 39\u201348,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "ICCV, pages 2425\u20132433,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "On the properties of neural machine translation: Encoderdecoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "IJCV, 88(2):303\u2013338,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "NIPS, pages 2296\u20132304,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, 18(5):602\u2013610,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR, June", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "A focused dynamic attention model for visual question answering", "author": ["I. Ilievski", "S. Yan", "J. Feng"], "venue": "ECCV,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "A model of saliencybased visual attention for rapid scene analysis", "author": ["L. Itti", "C. Koch", "E. Niebur"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Revisiting visual question answering baselines", "author": ["A. Jabri", "A. Joulin", "L. van der Maaten"], "venue": "arXiv preprint arXiv:1606.08390,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "L. Fei-Fei"], "venue": "ICLR Workshop,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal residual learning for visual qa", "author": ["J.-H. Kim", "S.-W. Lee", "D.-H. Kwak", "M.-O. Heo", "J. Kim", "J.- W. Ha", "B.-T. Zhang"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R.R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "NIPS, pages 3294\u20133302,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Accurate unlexicalized parsing", "author": ["D. Klein", "C.D. Manning"], "venue": "ACL, pages 423\u2013430. Association for Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Visualizing and understanding neural models in nlp", "author": ["J. Li", "X. Chen", "E. Hovy", "D. Jurafsky"], "venue": "NAACL,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV, pages 740\u2013755. Springer,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to detect a salient object", "author": ["T. Liu", "Z. Yuan", "J. Sun", "J. Wang", "N. Zheng", "X. Tang", "H.-Y. Shum"], "venue": "PAMI, 33(2):353\u2013367,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "NIPS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["L. Ma", "Z. Lu", "H. Li"], "venue": "AAAI,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS, pages 1682\u20131690,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV, pages 1\u20139,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["H. Noh", "P.H. Seo", "B. Han"], "venue": "CVPR, June", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "NIPS, pages 2953\u20132961,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "CVPR, June", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Task-dependent learning of attention", "author": ["P. Van De Laar", "T. Heskes", "S. Gielen"], "venue": "Neural Networks,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1997}, {"title": "Ask me anything: Free-form visual question answering based on knowledge from external sources", "author": ["Q. Wu", "P. Wang", "C. Shen", "A. v. d. Hengel", "A. Dick"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "ICML,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "ICML, volume 37, pages 2048\u20132057,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised extraction of video highlights via robust recurrent auto-encoders", "author": ["H. Yang", "B. Wang", "S. Lin", "D. Wipf", "M. Guo", "B. Guo"], "venue": "ICCV, pages 4633\u20134641,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A. Smola"], "venue": "CVPR, June", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "arXiv preprint arXiv:1512.02167,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual7w: Grounded question answering in images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "CVPR, June", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 20, "context": "After the first attempt and introduction of VQA [21], more than thirty works on VQA have sprung up over the past one year from May, 2015.", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "and a big VQA challenge [3] have been proposed so far.", "startOffset": 24, "endOffset": 27}, {"referenceID": 20, "context": "DAQUAR [21], COCOQA [24], COCO-VQA [3] and Visual7W [36]) feature different aspects.", "startOffset": 7, "endOffset": 11}, {"referenceID": 23, "context": "DAQUAR [21], COCOQA [24], COCO-VQA [3] and Visual7W [36]) feature different aspects.", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "DAQUAR [21], COCOQA [24], COCO-VQA [3] and Visual7W [36]) feature different aspects.", "startOffset": 35, "endOffset": 38}, {"referenceID": 35, "context": "DAQUAR [21], COCOQA [24], COCO-VQA [3] and Visual7W [36]) feature different aspects.", "startOffset": 52, "endOffset": 56}, {"referenceID": 34, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 163, "endOffset": 174}, {"referenceID": 21, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 163, "endOffset": 174}, {"referenceID": 5, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 163, "endOffset": 174}, {"referenceID": 2, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 203, "endOffset": 206}, {"referenceID": 22, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 235, "endOffset": 239}, {"referenceID": 30, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 256, "endOffset": 260}, {"referenceID": 25, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 282, "endOffset": 294}, {"referenceID": 12, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 282, "endOffset": 294}, {"referenceID": 18, "context": "The following important step is to combine the image and question representations through some kind of fusing methods for answer generation, such as concatenation [35, 22, 6], elementwise multiplication [3], parameter prediction layer [23], episode memory [31], attention mechanism [26, 13, 19], etc.", "startOffset": 282, "endOffset": 294}, {"referenceID": 6, "context": "Taking advantage of the bidirectional LSTM (BiLSTM) [7] that the output at an arbitrary time step has complete and sequential information about all time steps before and after it, we compute the weight of interest for each region feature which is relative to all of them.", "startOffset": 52, "endOffset": 55}, {"referenceID": 34, "context": "As a simple and effective VQA baseline method, [35] shows that question feature always contributes more to predict the answer than image feature.", "startOffset": 47, "endOffset": 51}, {"referenceID": 31, "context": "attention mechanism [32].", "startOffset": 20, "endOffset": 24}, {"referenceID": 33, "context": "containing multiple attention layers) [34, 19] dig deeper into the image understanding and help achieve better VQA performance than the \u201cregular\u201d attention models.", "startOffset": 38, "endOffset": 46}, {"referenceID": 18, "context": "containing multiple attention layers) [34, 19] dig deeper into the image understanding and help achieve better VQA performance than the \u201cregular\u201d attention models.", "startOffset": 38, "endOffset": 46}, {"referenceID": 2, "context": "Besides, [3] shows that element-wise multiplication of these features achieves more accurate results than concatenation of them in the baseline model.", "startOffset": 9, "endOffset": 12}, {"referenceID": 28, "context": "Saliency detection methods mimic the human attention in psychology, including both bottomup and top-down manners [29].", "startOffset": 113, "endOffset": 117}, {"referenceID": 9, "context": "Typical saliency methods [10, 18] are pixel- or object-oriented, which are not appropriate for VQA due to center bias and are difficulty in collecting large scale eye tracking data.", "startOffset": 25, "endOffset": 33}, {"referenceID": 17, "context": "Typical saliency methods [10, 18] are pixel- or object-oriented, which are not appropriate for VQA due to center bias and are difficulty in collecting large scale eye tracking data.", "startOffset": 25, "endOffset": 33}, {"referenceID": 15, "context": "In sentiment classification tasks, [16] assigns saliency scores to words related to sentiment for visualizing and understanding the effects of BiLSTM in textual sentence.", "startOffset": 35, "endOffset": 39}, {"referenceID": 32, "context": "While in video highlight detection, [33] uses a recurrent auto-encoder configured with BiLSTM cells and extracts video highlight segments effectively.", "startOffset": 36, "endOffset": 40}, {"referenceID": 33, "context": "Visual attention mechanism has drawn great interest in VQA [34, 36, 26] and gained performance improvement from traditional methods using holistic image features.", "startOffset": 59, "endOffset": 71}, {"referenceID": 35, "context": "Visual attention mechanism has drawn great interest in VQA [34, 36, 26] and gained performance improvement from traditional methods using holistic image features.", "startOffset": 59, "endOffset": 71}, {"referenceID": 25, "context": "Visual attention mechanism has drawn great interest in VQA [34, 36, 26] and gained performance improvement from traditional methods using holistic image features.", "startOffset": 59, "endOffset": 71}, {"referenceID": 25, "context": "Focusing on \u201cknowing where to look\u201d for multiplechoice VQA tasks, [26] uses 99 detected object regions plus a holistic image feature to make correlation with the question encoding, and uses the correlation scores as weights to fuse the features.", "startOffset": 66, "endOffset": 70}, {"referenceID": 33, "context": "[34] uses the last pooling layer features (512 \u00d7 14 \u00d7 14) of VGG-19 [27] as image region partitions, and adopts two-layer attention to obtain more effective fused features for complex questions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[34] uses the last pooling layer features (512 \u00d7 14 \u00d7 14) of VGG-19 [27] as image region partitions, and adopts two-layer attention to obtain more effective fused features for complex questions.", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "[2] proposes an ingenious idea to use assembled network modules according to the parsed questions, and achieves multi-step transforming attention by specific rules.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Besides, the concatenation of image and question features is less accurate than the element-wise multiplication vector of them shown in the baseline model [3].", "startOffset": 155, "endOffset": 158}, {"referenceID": 24, "context": "As described above, current object detection based VQA methods may not be qualified and the answers may not be derived from these specific object regions in images, for example, when asked \u201cWhere is the bird/cat?\u201d, the answers \u201cfence/sink\u201d are not contained in ILSVRC [25] (200 categories) and Pascal VOC [5] (20 categories) detection classes.", "startOffset": 268, "endOffset": 272}, {"referenceID": 4, "context": "As described above, current object detection based VQA methods may not be qualified and the answers may not be derived from these specific object regions in images, for example, when asked \u201cWhere is the bird/cat?\u201d, the answers \u201cfence/sink\u201d are not contained in ILSVRC [25] (200 categories) and Pascal VOC [5] (20 categories) detection classes.", "startOffset": 305, "endOffset": 308}, {"referenceID": 7, "context": "We then feed the regions to a pre-trained ResNet [8] deep convolutional neural network to produce n\u00d7n\u00d7dI -dimensional region features, where dI is the dimension of feature from the layer before the last fully-connected layer.", "startOffset": 49, "endOffset": 52}, {"referenceID": 30, "context": "Note that, although the DMN+ work [31] uses similar bi-directional gated recurrent units (BiGRU) in the visual input module, their purpose is to produce input", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "Question can be encoded using various kinds of natural language processing (NLP) methods, such as BoW, LSTM, CNN [20, 34], gated recurrent units (GRU) [4], skipthought vectors [14], or it can be parsed by Stanford Parser [15], etc.", "startOffset": 113, "endOffset": 121}, {"referenceID": 33, "context": "Question can be encoded using various kinds of natural language processing (NLP) methods, such as BoW, LSTM, CNN [20, 34], gated recurrent units (GRU) [4], skipthought vectors [14], or it can be parsed by Stanford Parser [15], etc.", "startOffset": 113, "endOffset": 121}, {"referenceID": 3, "context": "Question can be encoded using various kinds of natural language processing (NLP) methods, such as BoW, LSTM, CNN [20, 34], gated recurrent units (GRU) [4], skipthought vectors [14], or it can be parsed by Stanford Parser [15], etc.", "startOffset": 151, "endOffset": 154}, {"referenceID": 13, "context": "Question can be encoded using various kinds of natural language processing (NLP) methods, such as BoW, LSTM, CNN [20, 34], gated recurrent units (GRU) [4], skipthought vectors [14], or it can be parsed by Stanford Parser [15], etc.", "startOffset": 176, "endOffset": 180}, {"referenceID": 14, "context": "Question can be encoded using various kinds of natural language processing (NLP) methods, such as BoW, LSTM, CNN [20, 34], gated recurrent units (GRU) [4], skipthought vectors [14], or it can be parsed by Stanford Parser [15], etc.", "startOffset": 221, "endOffset": 225}, {"referenceID": 34, "context": "Since question BoW encodings already dominate the contribution to answer generation compared with the image features [35], we simply encode the question word as word2vec embedding, and use LSTM to encode the questions to match the pre-selected region features.", "startOffset": 117, "endOffset": 121}, {"referenceID": 2, "context": "To encode more abstract and higher-level information and achieve better performance, a deeper LSTM [3, 12] for question encoding is adopted in our model.", "startOffset": 99, "endOffset": 106}, {"referenceID": 11, "context": "To encode more abstract and higher-level information and achieve better performance, a deeper LSTM [3, 12] for question encoding is adopted in our model.", "startOffset": 99, "endOffset": 106}, {"referenceID": 35, "context": "Visual7W [36] telling questions and COCO-VQA [3] multiple choice tasks, our model is adaptive to be extended by concatenating the question and answer vectors before fusion with visual features or by using bilinear model between the final fused feature and answer feature [26, 11], which is a possible future work.", "startOffset": 9, "endOffset": 13}, {"referenceID": 2, "context": "Visual7W [36] telling questions and COCO-VQA [3] multiple choice tasks, our model is adaptive to be extended by concatenating the question and answer vectors before fusion with visual features or by using bilinear model between the final fused feature and answer feature [26, 11], which is a possible future work.", "startOffset": 45, "endOffset": 48}, {"referenceID": 25, "context": "Visual7W [36] telling questions and COCO-VQA [3] multiple choice tasks, our model is adaptive to be extended by concatenating the question and answer vectors before fusion with visual features or by using bilinear model between the final fused feature and answer feature [26, 11], which is a possible future work.", "startOffset": 271, "endOffset": 279}, {"referenceID": 10, "context": "Visual7W [36] telling questions and COCO-VQA [3] multiple choice tasks, our model is adaptive to be extended by concatenating the question and answer vectors before fusion with visual features or by using bilinear model between the final fused feature and answer feature [26, 11], which is a possible future work.", "startOffset": 271, "endOffset": 279}, {"referenceID": 21, "context": "Meanwhile, in view of generation VQA problem, we can train an LSTM taking the fused feature as input to obtain answer word lists, phrases or sentences [22, 6].", "startOffset": 151, "endOffset": 158}, {"referenceID": 5, "context": "Meanwhile, in view of generation VQA problem, we can train an LSTM taking the fused feature as input to obtain answer word lists, phrases or sentences [22, 6].", "startOffset": 151, "endOffset": 158}, {"referenceID": 5, "context": "Our framework is trained end-to-end using backpropagation, while the feature extraction part using ResNet is kept fixed to speed up training and avoid the noisy gradients back-propagated from the LSTM as elaborated in [6].", "startOffset": 218, "endOffset": 221}, {"referenceID": 25, "context": "RMSprop algorithm is employed with low initial learning rate of 3e-4 which is proved important to prevent the softmax from spiking too early and prevent the visual features from dominating too early [26].", "startOffset": 199, "endOffset": 203}, {"referenceID": 27, "context": "We use dropout [28] after all convolutional and linear layers.", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "The COCO-VQA dataset [3] is the largest among the commonly used VQA datasets, which contains two tasks (i.", "startOffset": 21, "endOffset": 24}, {"referenceID": 16, "context": "real image MSCOCO dataset [17] and abstract scene dataset).", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "\u2022 TraAtt: The traditional attention model, implementation of WTL model [26] using the same 3\u00d7 3 regions in SalAtt model.", "startOffset": 71, "endOffset": 75}, {"referenceID": 34, "context": "iBOWIMG [35], VQA [3], and the state-of-the-art attention-based models i.", "startOffset": 8, "endOffset": 12}, {"referenceID": 2, "context": "iBOWIMG [35], VQA [3], and the state-of-the-art attention-based models i.", "startOffset": 18, "endOffset": 21}, {"referenceID": 25, "context": "WTL [26], NMN [2], SAN [34], AMA [30], FDA [9], D-NMN [1], DMN+ [31] on two tasks of COCO-VQA.", "startOffset": 4, "endOffset": 8}, {"referenceID": 1, "context": "WTL [26], NMN [2], SAN [34], AMA [30], FDA [9], D-NMN [1], DMN+ [31] on two tasks of COCO-VQA.", "startOffset": 14, "endOffset": 17}, {"referenceID": 33, "context": "WTL [26], NMN [2], SAN [34], AMA [30], FDA [9], D-NMN [1], DMN+ [31] on two tasks of COCO-VQA.", "startOffset": 23, "endOffset": 27}, {"referenceID": 29, "context": "WTL [26], NMN [2], SAN [34], AMA [30], FDA [9], D-NMN [1], DMN+ [31] on two tasks of COCO-VQA.", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "WTL [26], NMN [2], SAN [34], AMA [30], FDA [9], D-NMN [1], DMN+ [31] on two tasks of COCO-VQA.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "WTL [26], NMN [2], SAN [34], AMA [30], FDA [9], D-NMN [1], DMN+ [31] on two tasks of COCO-VQA.", "startOffset": 54, "endOffset": 57}, {"referenceID": 30, "context": "WTL [26], NMN [2], SAN [34], AMA [30], FDA [9], D-NMN [1], DMN+ [31] on two tasks of COCO-VQA.", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "Note that, we only apply the proposed region preselection mechanism to the basic VQA model [3], it can be embedded into any other attention-based models to improve their performance.", "startOffset": 91, "endOffset": 94}], "year": 2017, "abstractText": "Visual question answering (VQA) has witnessed great progress since May, 2015 as a classic problem unifying visual and textual data into a system. Many enlightening VQA works explore deep into the image and question encodings and fusing methods, of which attention is the most effective and infusive mechanism. Current attention based methods focus on adequate fusion of visual and textual features, but lack the attention to where people focus to ask questions about the image. Traditional attention based methods attach a single value to the feature at each spatial location, which losses many useful information. To remedy these problems, we propose a general method to perform saliency-like pre-selection on overlapped region features by the interrelation of bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication based attention method to capture more competent correlation information between visual and textual features. We conduct experiments on the large-scale COCO-VQA dataset and analyze the effectiveness of our model demonstrated by strong empirical results.", "creator": "LaTeX with hyperref package"}}}