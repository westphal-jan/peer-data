{"id": "1702.06703", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Data Distillation for Controlling Specificity in Dialogue Generation", "abstract": "People speak at different levels of specificity in different situations. Depending on their knowledge, interlocutors, mood, etc.} A conversational agent should have this ability and know when to be specific and when to be general. We propose an approach that gives a neural network--based conversational agent this ability. Our approach involves alternating between \\emph{data distillation} and model training : removing training examples that are closest to the responses most commonly produced by the model trained from the last round and then retrain the model on the remaining dataset. Dialogue generation models trained with different degrees of data distillation manifest different levels of specificity.", "histories": [["v1", "Wed, 22 Feb 2017 08:32:47 GMT  (156kb,D)", "http://arxiv.org/abs/1702.06703v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "will monroe", "dan jurafsky"], "accepted": false, "id": "1702.06703"}, "pdf": {"name": "1702.06703.pdf", "metadata": {"source": "CRF", "title": "Data Distillation for Controlling Specificity in Dialogue Generation", "authors": ["Jiwei Li", "Will Monroe", "Dan Jurafsky"], "emails": ["jiweil@stanford.edu", "wmonroe4@stanford.edu", "jurafsky@stanford.edu"], "sections": [{"heading": null, "text": "We propose an approach that gives a neural network-based conversation agent this ability. Our approach alternates between data distillation and model training: we remove training examples that are closest to the answers most commonly produced by the last round of training, and then train the model on the remaining data set. Dialog generation models that have been trained with varying degrees of data distillation manifest different levels of specification. We then train an enhanced learning system to select from this pool of generational models to select the best degree of specificity for a given input. Compared to the original generative model that was trained without distillation, the proposed system is able to generate more interesting and high-quality responses, in addition to adequately adapting the specificity to the context. Our research represents a specific case of a more comprehensive approach where multiple subsystems are formed from a single set of data, distinguished by differences in specific characteristics."}, {"heading": "1 Introduction", "text": "rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrln rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "2 Related Work", "text": "Generic responses in open dialogue end-to-end dialog systems (Ritter et al., 2011; Serban et al., 2016c; Vinyals and Le, 2015; Serban et al., 2016d, a; Asghar et al., 2016; Mei et al., 2016) tend to generate highly generic and everyday responses (Sordoni et al., 2015; Mou et al., 2016). The goal of controlling output specificity is closely related to recent attempts to address this problem. Li et al. (2016a) suggest using mutual information as an alternative training target function instead of maximum probability, in which an N leaderboard generated by p (t | s) strategies is generated by the backward probability p (s | t).The goal of this work is more general: Instead of trying to avoid generic responses, our goal is to provide the system with flexibility."}, {"heading": "3 Data Distillation", "text": "In the section we describe the proposed data distillation model in detail. We use OpenSubtitles (Tiedemann, 2009).4"}, {"heading": "3.1 Distilling common responses", "text": "Let's start by using the following simple example to illustrate the core ideas of our system: Consider a model that predicts multinomic distribution over an output variant (e.g., which fruits to choose).The probability of picking apples is 0.3, orange 0.25, blueberry 0.15, and raspberry 0.15. Outputs that are awesome are usually very likely, since the wide variety of specific results in each of them has a lower probability. So we treat all apples as the most generic fruit, and the different berries as more specific. Maximum probability will lead the model to always choose apples, since it has the highest probability that apple is the most common output, we will remove all apples from the training program and retrain the model that will select orange this time, as it has the highest probability of reaching for apples and repeating the process."}, {"heading": "3.2 Choosing a specificity model", "text": "The data distillation process produces a pool of SEQ2SEQ models, each based on the data sets we receive after another round of data distillation. If we are confronted with an input message in the test phase, the system must decide which generation model from the pool will be used to decode a response to the input. We repeat the data distillation process 8 times, meaning that we have 8 models in the pool to choose how the system will handle different models in response to Properties5Other options include a response to decode the input. (Kiros et al., 2015) and bag representations of the word representations. We find the use of the trained encoder, which works accordingly well. 6The amount to be empirically set to 8-10% requires two Tesla K40 GPUs to match the 8 models of the input. For example, a good dialog system should provide concrete answers, if it is safe but generic."}, {"heading": "3.3 Stochastic Greedy Sampling", "text": "Modelling diversity also provides an indirect way to handle the problem of specificity. In addition, there is a degree of randomness in human language production: in the real world, if we ask a person the same question twice, even with the same environment and environment, it is unlikely that the person will give the same answer both times. Sampling from the distribution not only leads to a better imitation of the way people generate tokens, but also provides a way to handle the problem of language specificity. A simple solution is to try directly from the distribution p (y | x), which leads to an incoherent, ungrammatic or even irrelevant response. We expect there to be a sweet spot in the spectrum of randomness, between complete sampling at one end and greedy or searching at the other."}, {"heading": "4 Experimental Results", "text": "In this section we present the results of experiments.9Since greedy decoding has been shown to produce better quality answers than bar search when generating dialog answers (Li et al., 2016a), we focus on greedy decoding. However, all algorithms can be easily adapted to the use of bar search decoding."}, {"heading": "4.1 Comparing generative models from different iterations", "text": "It is interesting to first compare the generative models and the remaining training data from each of the 8 rounds of data distillation. We use Iter + N to mark the generational model on the dataset after N repetitions of data distillation. Perplexity increases for models that are equipped with more data, and the perplexity of the corresponding models is shown in the first two columns of the table. Perplexity increases for models that are marked with distillation (as expected, as distillation increases the chances for the model)."}, {"heading": "4.2 Choosing the correct model for decoding", "text": "The diffusion of different models for the deciphering of ingredients in the development phase can be seen in Figures 1, 3 and 4, which are responsible for the deciphering of ingredients."}, {"heading": "5 Conclusion", "text": "The success of the proposed system confirms the importance of data processing in the formation of a successful open domain dialogue system. We assume that strategies similar to the model we are proposing can be used more generally to control properties of dialogue generation other than specificity, by training several models on different subsets of a single data set that differ in the desired property, and selecting between them to produce outputs that match the quality of interest in the respective situation."}], "references": [{"title": "Stochastic optimization", "author": ["V.M. Aleksandrov", "V.I. Sysoyev", "V.V. Shemeneva."], "venue": "Engineering Cybernetics 5:11\u201316.", "citeRegEx": "Aleksandrov et al\\.,? 1968", "shortCiteRegEx": "Aleksandrov et al\\.", "year": 1968}, {"title": "Adversarial evaluation of dialogue models", "author": ["Kannan Anjuli", "Vinyals Oriol"], "venue": null, "citeRegEx": "Anjuli and Oriol.,? \\Q2016\\E", "shortCiteRegEx": "Anjuli and Oriol.", "year": 2016}, {"title": "Online sequence-to-sequence reinforcement learning for open-domain conversational agents", "author": ["Nabiha Asghar", "Pasca Poupart", "Jiang Xin", "Hang Li."], "venue": "arXiv preprint arXiv:1612.03929 .", "citeRegEx": "Asghar et al\\.,? 2016", "shortCiteRegEx": "Asghar et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Generating sentences from a continuous space", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio."], "venue": "arXiv preprint arXiv:1511.06349 .", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Bagging predictors", "author": ["Leo Breiman."], "venue": "Machine learning 24(2):123\u2013140.", "citeRegEx": "Breiman.,? 1996a", "shortCiteRegEx": "Breiman.", "year": 1996}, {"title": "Bias, variance, and arcing classifiers", "author": ["Leo Breiman"], "venue": null, "citeRegEx": "Breiman.,? \\Q1996\\E", "shortCiteRegEx": "Breiman.", "year": 1996}, {"title": "Ensemble learning", "author": ["Thomas G Dietterich."], "venue": "The handbook of brain theory and neural networks 2:110\u2013125.", "citeRegEx": "Dietterich.,? 2002", "shortCiteRegEx": "Dietterich.", "year": 2002}, {"title": "The semantics of specificity", "author": ["M\u00fcrvet En\u00e7."], "venue": "Linguistic inquiry pages 1\u201325.", "citeRegEx": "En\u00e7.,? 1991", "shortCiteRegEx": "En\u00e7.", "year": 1991}, {"title": "Scaling the indian buffet process via submodular maximization", "author": ["Zoubin Ghahramani."], "venue": "arXiv preprint arXiv:1304.3285 .", "citeRegEx": "Ghahramani.,? 2013", "shortCiteRegEx": "Ghahramani.", "year": 2013}, {"title": "Submodular optimization with submodular cover and submodular knapsack constraints", "author": ["Rishabh K Iyer", "Jeff A Bilmes."], "venue": "Advances in Neural Information Processing Systems. pages 2436\u20132444.", "citeRegEx": "Iyer and Bilmes.,? 2013", "shortCiteRegEx": "Iyer and Bilmes.", "year": 2013}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in neural information processing systems. pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Neural network ensembles, cross validation, and active learning. Advances in neural information processing systems 7:231\u2013238", "author": ["Anders Krogh", "Jesper Vedelsby"], "venue": null, "citeRegEx": "Krogh and Vedelsby,? \\Q1995\\E", "shortCiteRegEx": "Krogh and Vedelsby", "year": 1995}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of NAACL-HLT .", "citeRegEx": "Li et al\\.,? 2016a", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1606.01541 .", "citeRegEx": "Li et al\\.,? 2016b", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Adversarial reinforcement learning for neural dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "Dan Jurafsky"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "arXiv preprint", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Towards an automatic turing test: Learning to evaluate dialogue responses", "author": ["Ryan Lowe", "Michael Noseworthy", "Iulian Serban", "Nicolas Angelard-Gontier", "Yoshua Bengio", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1605.05414 .", "citeRegEx": "Lowe et al\\.,? 2016", "shortCiteRegEx": "Lowe et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1508.04025 .", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Linguistic semantics: An introduction", "author": ["John Lyons."], "venue": "Cambridge University Press.", "citeRegEx": "Lyons.,? 1995", "shortCiteRegEx": "Lyons.", "year": 1995}, {"title": "Coherent dialogue with attention-based language models", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R Walter."], "venue": "arXiv preprint arXiv:1611.06997 .", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation", "author": ["Lili Mou", "Yiping Song", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin."], "venue": "arXiv preprint arXiv:1607.00970 .", "citeRegEx": "Mou et al\\.,? 2016", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Submodular meets structured: Finding diverse subsets in exponentially-large structured item sets", "author": ["Adarsh Prasad", "Stefanie Jegelka", "Dhruv Batra."], "venue": "Advances in Neural Information Processing Systems. pages 2645\u20132653.", "citeRegEx": "Prasad et al\\.,? 2014", "shortCiteRegEx": "Prasad et al\\.", "year": 2014}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, pages 583\u2013593.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Multimodal variational encoder-decoders", "author": ["Iulian V Serban", "II Ororbia", "G Alexander", "Joelle Pineau", "Aaron Courville."], "venue": "arXiv preprint arXiv:1612.00377 .", "citeRegEx": "Serban et al\\.,? 2016a", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proceedings of AAAI.", "citeRegEx": "Serban et al\\.,? 2016b", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Generative deep neural networks for dialogue: A short review", "author": ["Iulian Vlad Serban", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1605.06069 .", "citeRegEx": "Serban et al\\.,? 2016d", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Meg Mitchell", "JianYun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "News from OPUS \u2013 A collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "Recent Advances in Natural Language Processing. volume 5, pages 237\u2013248.", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "Proceedings of ICML Deep Learning Workshop.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Submodularity in data subset selection and active learning", "author": ["Kai Wei", "Rishabh Iyer", "Jeff Bilmes."], "venue": "Proceedings of the 32nd International Conference on Machine Learning, Lille, Fran. pages 6\u201311.", "citeRegEx": "Wei et al\\.,? 2015", "shortCiteRegEx": "Wei et al\\.", "year": 2015}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Reinforcement learning neural Turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1505.00521 .", "citeRegEx": "Zaremba and Sutskever.,? 2015", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}, {"title": "Submodular attribute selection for action recognition in video", "author": ["Jingjing Zheng", "Zhuolin Jiang", "Rama Chellappa", "Jonathon P Phillips."], "venue": "Advances in Neural Information Processing Systems. pages 1341\u20131349.", "citeRegEx": "Zheng et al\\.,? 2014", "shortCiteRegEx": "Zheng et al\\.", "year": 2014}, {"title": "Ensembling neural networks: many could be better than all", "author": ["Zhi-Hua Zhou", "Jianxin Wu", "Wei Tang."], "venue": "Artificial intelligence 137(1):239\u2013263.", "citeRegEx": "Zhou et al\\.,? 2002", "shortCiteRegEx": "Zhou et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 8, "context": "Language specificity has been historically studied for noun phrases, and a few specificityindicative features have been identified, such as singular terms, negations, or actual/non-actual moods (En\u00e7, 1991; Lyons, 1995).", "startOffset": 194, "endOffset": 218}, {"referenceID": 19, "context": "Language specificity has been historically studied for noun phrases, and a few specificityindicative features have been identified, such as singular terms, negations, or actual/non-actual moods (En\u00e7, 1991; Lyons, 1995).", "startOffset": 194, "endOffset": 218}, {"referenceID": 24, "context": "Generic responses in open-domain dialogue End-to-end dialogue systems (Ritter et al., 2011; Serban et al., 2016c; Vinyals and Le, 2015; Serban et al., 2016d,a; Asghar et al., 2016; Mei et al., 2016), tend to generate highly generic and commonplace responses (Sordoni et al.", "startOffset": 70, "endOffset": 198}, {"referenceID": 31, "context": "Generic responses in open-domain dialogue End-to-end dialogue systems (Ritter et al., 2011; Serban et al., 2016c; Vinyals and Le, 2015; Serban et al., 2016d,a; Asghar et al., 2016; Mei et al., 2016), tend to generate highly generic and commonplace responses (Sordoni et al.", "startOffset": 70, "endOffset": 198}, {"referenceID": 2, "context": "Generic responses in open-domain dialogue End-to-end dialogue systems (Ritter et al., 2011; Serban et al., 2016c; Vinyals and Le, 2015; Serban et al., 2016d,a; Asghar et al., 2016; Mei et al., 2016), tend to generate highly generic and commonplace responses (Sordoni et al.", "startOffset": 70, "endOffset": 198}, {"referenceID": 20, "context": "Generic responses in open-domain dialogue End-to-end dialogue systems (Ritter et al., 2011; Serban et al., 2016c; Vinyals and Le, 2015; Serban et al., 2016d,a; Asghar et al., 2016; Mei et al., 2016), tend to generate highly generic and commonplace responses (Sordoni et al.", "startOffset": 70, "endOffset": 198}, {"referenceID": 29, "context": ", 2016), tend to generate highly generic and commonplace responses (Sordoni et al., 2015; Mou et al., 2016).", "startOffset": 67, "endOffset": 107}, {"referenceID": 21, "context": ", 2016), tend to generate highly generic and commonplace responses (Sordoni et al., 2015; Mou et al., 2016).", "startOffset": 67, "endOffset": 107}, {"referenceID": 2, "context": ", 2016d,a; Asghar et al., 2016; Mei et al., 2016), tend to generate highly generic and commonplace responses (Sordoni et al., 2015; Mou et al., 2016). The goal of controlling output specificity is closely related to recent attempts to address this issue. Li et al. (2016a) propose using mutual information as an alternative training objective function in place of maximum likelihood, in which an N-best list generated by p(t|s) is reranked by the backward probability p(s|t).", "startOffset": 11, "endOffset": 273}, {"referenceID": 32, "context": "Data manipulation The idea of training with data distillation is inspired by a variety of work in the active learning and subdata selection literature, the key idea of which is to select a subset of a large dataset to train a classifier with minimal performance loss, for when the training dataset is extremely large or training is extremely timeintensive (Wei et al., 2015; Zheng et al., 2014; Prasad et al., 2014; Ghahramani, 2013; Iyer and Bilmes, 2013).", "startOffset": 356, "endOffset": 456}, {"referenceID": 35, "context": "Data manipulation The idea of training with data distillation is inspired by a variety of work in the active learning and subdata selection literature, the key idea of which is to select a subset of a large dataset to train a classifier with minimal performance loss, for when the training dataset is extremely large or training is extremely timeintensive (Wei et al., 2015; Zheng et al., 2014; Prasad et al., 2014; Ghahramani, 2013; Iyer and Bilmes, 2013).", "startOffset": 356, "endOffset": 456}, {"referenceID": 22, "context": "Data manipulation The idea of training with data distillation is inspired by a variety of work in the active learning and subdata selection literature, the key idea of which is to select a subset of a large dataset to train a classifier with minimal performance loss, for when the training dataset is extremely large or training is extremely timeintensive (Wei et al., 2015; Zheng et al., 2014; Prasad et al., 2014; Ghahramani, 2013; Iyer and Bilmes, 2013).", "startOffset": 356, "endOffset": 456}, {"referenceID": 9, "context": "Data manipulation The idea of training with data distillation is inspired by a variety of work in the active learning and subdata selection literature, the key idea of which is to select a subset of a large dataset to train a classifier with minimal performance loss, for when the training dataset is extremely large or training is extremely timeintensive (Wei et al., 2015; Zheng et al., 2014; Prasad et al., 2014; Ghahramani, 2013; Iyer and Bilmes, 2013).", "startOffset": 356, "endOffset": 456}, {"referenceID": 10, "context": "Data manipulation The idea of training with data distillation is inspired by a variety of work in the active learning and subdata selection literature, the key idea of which is to select a subset of a large dataset to train a classifier with minimal performance loss, for when the training dataset is extremely large or training is extremely timeintensive (Wei et al., 2015; Zheng et al., 2014; Prasad et al., 2014; Ghahramani, 2013; Iyer and Bilmes, 2013).", "startOffset": 356, "endOffset": 456}, {"referenceID": 7, "context": "The system we propose is also related to data manipulation strategies such as boosting (Breiman, 1996b), a type of ensemble method (Dietterich, 2002; Zhou et al., 2002; Krogh et al., 1995) that uses subsets of the original data to produce a series of models and then \u201dboosts\u201d their performance by combining them together; and bagging (Breiman, 1996a), which generates additional data for training using the original dataset to produce multisets", "startOffset": 131, "endOffset": 188}, {"referenceID": 36, "context": "The system we propose is also related to data manipulation strategies such as boosting (Breiman, 1996b), a type of ensemble method (Dietterich, 2002; Zhou et al., 2002; Krogh et al., 1995) that uses subsets of the original data to produce a series of models and then \u201dboosts\u201d their performance by combining them together; and bagging (Breiman, 1996a), which generates additional data for training using the original dataset to produce multisets", "startOffset": 131, "endOffset": 188}, {"referenceID": 5, "context": ", 1995) that uses subsets of the original data to produce a series of models and then \u201dboosts\u201d their performance by combining them together; and bagging (Breiman, 1996a), which generates additional data for training using the original dataset to produce multisets", "startOffset": 153, "endOffset": 169}, {"referenceID": 30, "context": "We use OpenSubtitles (Tiedemann, 2009) as our training dataset.", "startOffset": 21, "endOffset": 38}, {"referenceID": 3, "context": "In the context of dialogue response generation, our approach works as follows: for each iteration, we first train a SEQ2SEQ model using attention (Bahdanau et al., 2014; Luong et al., 2015) on the original training set.", "startOffset": 146, "endOffset": 189}, {"referenceID": 18, "context": "In the context of dialogue response generation, our approach works as follows: for each iteration, we first train a SEQ2SEQ model using attention (Bahdanau et al., 2014; Luong et al., 2015) on the original training set.", "startOffset": 146, "endOffset": 189}, {"referenceID": 11, "context": "Other options include skip-thought vectors (Kiros et al., 2015) and bag-of-word representations.", "startOffset": 43, "endOffset": 63}, {"referenceID": 33, "context": "We use the REINFORCE algorithm (Williams, 1992), a kind of policy gradient method, to find the optimal policy by maximizing the expected reward E\u03c0(gi|X)[R(y)].", "startOffset": 31, "endOffset": 47}, {"referenceID": 0, "context": "The expectation is approximated by sampling from \u03c0 and the gradient is computed using the likelihood ratio (Aleksandrov et al., 1968):", "startOffset": 107, "endOffset": 133}, {"referenceID": 16, "context": ") using word-overlap metrics such as BLEU and METEOR scores used for machine translation, which have recently been found to correlate poorly with human evaluations (Liu et al., 2016).", "startOffset": 164, "endOffset": 182}, {"referenceID": 17, "context": "more flexible and reliable evaluation metrics; automatic prediction of human ratings (Lowe et al., 2016) is one such metric, but this approach requires a large amount of human labeling effort to train a prediction model.", "startOffset": 85, "endOffset": 104}, {"referenceID": 22, "context": "We refer the readers to Ranzato et al. (2015) and Zaremba and Sutskever (2015) for more details.", "startOffset": 24, "endOffset": 46}, {"referenceID": 22, "context": "We refer the readers to Ranzato et al. (2015) and Zaremba and Sutskever (2015) for more details.", "startOffset": 24, "endOffset": 79}, {"referenceID": 1, "context": "We employ adversarial evaluation (Li et al., 2016c; Anjuli and Oriol, 2016) for reward calculation.", "startOffset": 33, "endOffset": 75}, {"referenceID": 26, "context": "The evaluator is a hierarchical neural model (Serban et al., 2016b): dialogue utterances (i.", "startOffset": 45, "endOffset": 67}, {"referenceID": 1, "context": ", 2016c; Anjuli and Oriol, 2016) for reward calculation. The idea of adversarial evaluation, first proposed by Bowman et al. (2015), is to train a discriminator (or evaluator) function to labels dialogues as machine-generated (negative) or human-generated (positive), a binary classification task.", "startOffset": 9, "endOffset": 132}, {"referenceID": 1, "context": "We report AdverSuc and machinevs-random proposed by Anjuli and Oriol (2016). machine-vs-random denotes the the accuracy of dis-", "startOffset": 52, "endOffset": 76}, {"referenceID": 13, "context": "Since greedy decoding has been shown to generate higher-quality responses than beam search in dialogue response generation (Li et al., 2016a), we focus on greedy decoding.", "startOffset": 123, "endOffset": 141}, {"referenceID": 13, "context": "Table 2 also shows a measure of the diversity of generated responses, namely, the number of distinct unigrams (\u201cdiv-1\u201d) and bigrams (\u201cdiv-2\u201d) in generated responses as a fraction of the total generated tokens, as described in (Li et al., 2016a).", "startOffset": 226, "endOffset": 244}, {"referenceID": 13, "context": "Human evaluation For human evaluation, we follow protocols defined in Li et al. (2016b), employing crowdsourced judges to evaluate a random", "startOffset": 70, "endOffset": 88}, {"referenceID": 13, "context": "Adversarial evaluation Table 6 reports adversarial success and machine-vs-random accuracy described in Li et al. (2016c). Adversarial success (AdverSuc) refers to the percentage of machinegenerated responses that are able to fool an trained evaluator model into believe that they are generated by humans; machine-vs-random accuracy denotes the accuracy of a trained evaluator model (a different evaluator from the one used in adversarial success) at distinguishing between machine-generated responses and human utterances randomly sampled without regard for the input.", "startOffset": 103, "endOffset": 121}, {"referenceID": 13, "context": "We refer readers to Li et al. (2016c) for more details.", "startOffset": 20, "endOffset": 38}], "year": 2017, "abstractText": "People speak at different levels of specificity in different situations.1 A conversational agent should have this ability and know when to be specific and when to be general. We propose an approach that gives a neural network\u2013based conversational agent this ability. Our approach involves alternating between data distillation and model training : removing training examples that are closest to the responses most commonly produced by the model trained from the last round and then retrain the model on the remaining dataset. Dialogue generation models trained with different degrees of data distillation manifest different levels of specificity. We then train a reinforcement learning system for selecting among this pool of generation models, to choose the best level of specificity for a given input. Compared to the original generative model trained without distillation, the proposed system is capable of generating more interesting and higher-quality responses, in addition to appropriately adjusting specificity depending on the context. Our research constitutes a specific case of a broader approach involving training multiple subsystems from a single dataset distinguished by differences in a specific property one wishes to model. We show that from such a set of subsystems, one can use reinforcement learning to build a system that tailors its output to different input contexts at test time. Depending on their knowledge, interlocutors, mood, etc.", "creator": "LaTeX with hyperref package"}}}