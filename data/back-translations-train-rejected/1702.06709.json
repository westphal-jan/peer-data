{"id": "1702.06709", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings", "abstract": "Fine-grained entity type classification (FETC) is the task of classifying an entity mention to a broad set of types. Distant supervision paradigm is extensively used to generate training data for this task. However, generated training data assigns same set of labels to every mention of an entity without considering its local context. Existing FETC systems have two major drawbacks: assuming training data to be noise free and use of hand crafted features. Our work overcomes both drawbacks. We propose a neural network model that jointly learns entity mentions and their context representation to eliminate use of hand crafted features. Our model treats training data as noisy and uses non-parametric variant of hinge loss function. Experiments show that the proposed model outperforms previous state-of-the-art methods on two publicly available datasets, namely FIGER (GOLD) and BBN with an average relative improvement of 2.69% in micro-F1 score. Knowledge learnt by our model on one dataset can be transferred to other datasets while using same model or other FETC systems. These approaches of transferring knowledge further improve the performance of respective models.", "histories": [["v1", "Wed, 22 Feb 2017 08:59:37 GMT  (958kb,D)", "http://arxiv.org/abs/1702.06709v1", "11 pages, 5 figures, accepted at EACL 2017 conference"]], "COMMENTS": "11 pages, 5 figures, accepted at EACL 2017 conference", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["abhishek", "ashish anand", "amit awekar"], "accepted": false, "id": "1702.06709"}, "pdf": {"name": "1702.06709.pdf", "metadata": {"source": "CRF", "title": "Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings", "authors": ["Abhishek", "Ashish Anand", "Amit Awekar"], "emails": ["awekar}@iitg.ernet.in"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if most of them are able to determine for themselves what they want to do and what they want to do. (...) It is not as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) They do it as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) They do it as if they do it. (...) They do it as if they do it. \"(...) They do it as if they do it.\" (...) It is as if they do it. (...) It is as if they do it."}, {"heading": "2 Related Work", "text": "Ling et al. (2012) proposed the first system for the FETC task, in which 112 overlapping ET labels were used. They used linear classification preceptron for multi-label classification. Yosef et al. (2012) used several binary SVM classification techniques in a hierarchy, but to classify a group of 505 types. Whereas the initial work assumed that all labels present in a training dataset for a mention of an entity were correct, Gillick et al. (2014) introduced context-dependent FETC and proposed a number of heuristics for the pruning labels, which may not be relevant given the local context of the mention of the entity. Yogatama et al et al. (2015) proposed an embedding model in which user-defined features and labels were used al in a low-dimensional label exchange to facilitate the STokonal information space between the labels."}, {"heading": "3 The Proposed Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Problem description", "text": "Figure 3 shows a general overview of our proposed approach. Input: The input into the model is a training and test corpus consisting of a series of sentences on which entity mentions have been identified. In the training corpus, each entity mention is given corresponding names according to a predefined hierarchy. Formally, a training corpus Dtrain consists of a series of sentences, S = {si} Ni = 1. For each entity mentioning mij, there will be one or more entity names denoted by mij, k, where j and k mean indices for start and end marks respectively. SetM consists of all entity names mij, k. For each entity mentioning mij, k, there will be a corresponding designation vector-lij, k, which is a binary vector, where lij, kt = 1, if the entity type is true, otherwise there will be a zero entity."}, {"heading": "3.2 Training set partition", "text": "Similar to AFET, we divide the mention of setM of the training corpus Dtrain into two parts, a setMc, which consists only of clean mentions of the entity, and a sentence Mn, which consists only of loud mentions of the entity. An entity mentioning mij, k should be clean if its names lij, k belong to a single path (not necessary to be leaf) in the hierarchy, i.e. its names are not ambiguous; otherwise, it is loud. For example, according to the hierarchy given in Figure 1, an entity labeled person, artist, and politician is considered loud, while mentioning the entity labeled person, artist, and actor is considered clean."}, {"heading": "3.3 Feature representations", "text": "In fact, it is such that it is a very strange, self-contained group, which sees itself in a position to identify itself and is able to identify itself. (...) It is as if it is a group of people who are able to identify themselves. (...) It is as if it is a group of people who are able to identify themselves. (...) It is as if it is a group of people who are able to identify themselves in a group of people. (...) It is as if it is a group of people who are in a group of people. (...) It is as if it is a group of people who are able to identify themselves in a group of people. (...) It is as if it is a group of people who are able to identify themselves, to be a group of people. (...) It is a group of people who are in a group of people who are in a group of people who are in a group of people who are in a group of people. (...) It is a group of people who are in a group of people who are in a group of people. (...) It is a group of people who are in a group of people who are in a group of people who are in a group of people. (...) It is a group of people who are in a group of people who are in a group of people who are in a group of people who are in a group of people."}, {"heading": "3.4 Feature and label embeddings", "text": "Similar to Yogatama et al. (2015) and Ren et al. (2016), we embed representations and labels in an equal-dimensional space so that an object is embedded closer to the objects that share similar types than the objects that do not. Formally, we try to learn linear mapping functions \u03c6M: RDf \u2192 RDe and \u03c6L: RDK \u2192 RDe, where De is the size of the embedding space. These mappings are illuminated by: \u03c6M (f i) = f i T U; \u03c6L (l i t) = l iT t t V (2), where U-RDf x De and V-RDK x De are projection matrices for feature representations or type labels and illuminated is a uniform vector representation for labeling. We assign a score to each label type t and feature vector as a point product of their labeling. Formally, we call a Score: (f) (M)."}, {"heading": "3.5 Optimization", "text": "We use two different loss functions to model clean and noisy loss functions. For clean loss functions, we use a hinge loss function. Intuition is simple: a margin that is centered at zero, between positive and negative loss values. Values are calculated by similarity between an entity specification and label types (eq. 3). Hinge loss function has two advantages: first, it intuitively separates positive and negative labels during inference. Second, it is independent of data-dependent parameters. Formally, for a given entity specification mi and its labeling li, we calculate the associated loss as given: Lc (m i, li) = positive labeling of the label types as given."}, {"heading": "3.6 Inference", "text": "Starting from the tree root, we recursively calculate the best type among the children of the node by calculating its score with the attribute representations obtained. We select the node that has the maximum number of points among other nodes. We continue this process until a leaf node is encountered or the score associated with a node falls below an absolute threshold of zero. The threshold is set in all data sets used."}, {"heading": "3.7 Transfer learning", "text": "We will examine how these feature representations contribute to an existing feature engineering-based method such as AFET. We will learn the proposed model from a training dataset, namely the wiki dataset, which has the highest number of entity mentions among other datasets, and we will use this model to generate representations that are F (mij, k, si) for other training and test data. These representations, which are Df-dimensional vectors, will be used as a feature of an existing state-of-the-art model, AFET, instead of the handmade features originally used. The AFET model will then be trained on these feature representations. We will call this feature-level transfer learning. On the other hand, we will also evaluate model-level transfer learning, where we evaluate weights of LSTM encoders for a new dataset with the weights we have learned from another dataset, namely Wiki dataset."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets used", "text": "We evaluate the proposed model based on three publicly available datasets provided in a pre-processed tokenized format by Ren et al. (2016). Statistics of the datasets used in this work are presented in Table 1. Details of the datasets are as follows: Wiki / FIGER (GOLD): Training data consists of Wikipedia records and was automatically generated in a remote supervision paradigm by mapping hyperlinks in Wikipedia articles to Freebase. Test data, which consists mainly of sets of news reports, was commented manually as described in (Ling and Weld, 2012). OntoNotes: OntoNotes dataset consists of sets of news documents present in OntoNotes text corpus (Weischedel et al., 2013)."}, {"heading": "4.2 Evaluation settings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Baselines", "text": "We compared the proposed model with state-of-the-art classification methods2: (1) FIGER (Ling and Weld, 2012); (2) HYENA (Yosef et al., 2012); (3) AFETNoCo (Ren et al., 2016): AFET without data-based label-label correlation modelled in loss function; (4) AFET-CoH (Ren et al., 2016): AFET with hierarchy-based label-label correlation modelled in loss function; (5) AFET (Ren et al., 2016); (6) Attentive (Shimaoka et al., 2016): An attentive neural network-based model. We compare these baselines with variants of our proposed model: (1) our: complete model; (2) our-AllC assuming all mentions are clean; (3) our-NoM without mention of representation."}, {"heading": "4.2.2 Experimental setup", "text": "We use Accuracy or Strict-F1 Score, Macro-averaged F1 Score, and Micro-bedbedbedbedbedaged F1 Score as metrics for evaluation. Existing methods for FETC use the same metrics (Ling and Weld, 2012; Yogatama et al., 2015; Shimaoka et al., 2016; Ren et al., 2016). We removed entity mentions that have no labeling in training or in the test set. We also remove entity mentions that have false indices (i.e. entity mentioned length of 0).3 For all three sets of data we randomly stamped 10% of the test set and use it as a development set on which we tune model parameters. The remaining 90% is used for final evaluation. In all our experiments, we train each model with the same hyperparameters five times and report their performance in terms of micro-F1 Score."}, {"heading": "4.3 Transfer learning", "text": "When learning functional-level transfer, we use the best-functioning model trained on wiki datasets to generate representations that are Df-dimensional vector for each entity contained in the train, development and test set of the BBN and the OntoNotes dataset. Figure 4 illustrates an example of the encoding process. Subsequently, we use these representations as feature-level transfer learning instead of the user-defined features and train the AFET4http: / / sorflow.org / model. Its hyperparameters have been tuned to the development set. These results are presented in Table 2 as feature-level transfer learning.When learning at model level, we use the learned weights of the LSTM encoders from the best-functioning proposed model trained on wiki dataset, and initialize the LSTM encoders of the same model with these weights during training on NBB- and Notto2 data sets as these are shown in the training on BB- table transfer results."}, {"heading": "4.4 Performance comparison and analysis", "text": "The results of the study show that the number of new entrants who are able to hold their own and that the number of new entrants who are able to acquire the necessary qualifications is able to acquire the necessary qualifications, and that the results of the study show that the number of new entrants who are able to acquire the necessary qualifications is able to acquire the necessary qualifications, and that the number of new entrants who are able to acquire the necessary qualifications is able to acquire the necessary qualifications."}, {"heading": "4.5 Case analysis: OntoNotes dataset", "text": "We observed three things: (i) all models perform relatively poorly on OntoNotes datasets compared to their performance on other two datasets; (iii) the two variants of transfer learning significantly improve the performance of the proposed model on the BBN dataset, but only resulted in a subtle change in performance on OntoNotes datasets. Data set statistics (Table 1) suggest that the presence of pronominal or other types of mentions in OntoNotes (6.78% in the test set) is relatively higher than the other two datasets (0% in the test set). Examples of such mentions are 100 persons, the director, etc. Table 3 shows that 20 randomly sampled entity mentions from the OntoNotes dataset test set. Some of these mentions are very general and probably dependent on these results."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we propose a neural network-based model for the task of fine-grained entity classification. Experiments show that the proposed model exceeds existing state-of-the-art models on two publicly available datasets without explicitly doing data-dependent parameters. Our analysis shows the following observations: Firstly, the OntoNotes dataset has a different distribution of entity mentions than the other two datasets; secondly, when the data distribution is similar, transfer learning is very helpful; thirdly, the inclusion of data-based label correlations helps in the case of labels of mixed types; fourthly, there is an inherent limitation in the assumption that all labels are clean if they belong to the same path of the dataset; fifthly, the proposed model fails to learn label types that are very noisy.In the future, the effect of label reduction techniques on the proposed labeling may be considered as clean and the labels that are not considered as clean and labeled."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their valuable and revealing comments. Abhishek is supported by an MHRD grant from the Government of India. We acknowledge the use of computing resources provided by the Board of Research in Nuclear Science (BRNS), Dept. of Atomic Energy (DAE), Govt. of India and sponsored by Dr. Aryabartta Sahu from the Department of Computer Science and Engineering, IIT Guwahati (No. 2013 / 13 / 8-BRNS / 10026)."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["Auer et al.2007] S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives"], "venue": "In Proceedings of the 6th International The Semantic Web and 2nd Asian", "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Freebase: A collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD International", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Unsupervised models for named entity classification", "author": ["Collins", "Singer1999] Michael Collins", "Yoram Singer"], "venue": "Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,", "citeRegEx": "Collins et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Collins et al\\.", "year": 1999}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["Craven", "Kumlien1999] Mark Craven", "Johan Kumlien"], "venue": "In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology,", "citeRegEx": "Craven et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Craven et al\\.", "year": 1999}, {"title": "Improving efficiency and accuracy in multilingual entity extraction", "author": ["Max Jakob", "Chris Hokamp", "Pablo N. Mendes"], "venue": "In Proceedings of the 9th International Conference on Semantic Systems,", "citeRegEx": "Daiber et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Daiber et al\\.", "year": 2013}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["Dong et al.2014] Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": null, "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "A hybrid neural model for type classification of entity mentions", "author": ["Dong et al.2015] Li Dong", "Furu Wei", "Hong Sun", "Ming Zhou", "Ke Xu"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Context-dependent fine-grained entity type tagging", "author": ["Gillick et al.2014] Dan Gillick", "Nevena Lazic", "Kuzman Ganchev", "Jesse Kirchner", "David Huynh"], "venue": "arXiv preprint arXiv:1412.1820", "citeRegEx": "Gillick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves et al.2013] Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Type-aware distantly supervised relation extraction with linked arguments", "author": ["Koch et al.2014] Mitchell Koch", "John Gilmer", "Stephen Soderland", "Daniel S. Weld"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Koch et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koch et al\\.", "year": 2014}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Herbst."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,", "citeRegEx": "Herbst.,? 2007", "shortCiteRegEx": "Herbst.", "year": 2007}, {"title": "Learning question classifiers", "author": ["Li", "Roth2002] Xin Li", "Dan Roth"], "venue": "In Proceedings of the 19th International Conference on Computational Linguistics - Volume 1,", "citeRegEx": "Li et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Li et al\\.", "year": 2002}, {"title": "No noun phrase left behind: Detecting and typing unlinkable entities", "author": ["Lin et al.2012] Thomas Lin", "Mausam", "Oren Etzioni"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "citeRegEx": "Lin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "Fine-grained entity recognition", "author": ["Ling", "Weld2012] Xiao Ling", "Daniel S. Weld"], "venue": "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2012}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Daniel Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Neverending learning", "author": ["E. Platanios", "A. Ritter", "M. Samadi", "B. Settles", "R. Wang", "D. Wijaya", "A. Gupta", "X. Chen", "A. Saparov", "M. Greaves", "J. Welling."], "venue": "Proceedings of the TwentyNinth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Platanios et al\\.,? 2015", "shortCiteRegEx": "Platanios et al\\.", "year": 2015}, {"title": "Distilling word embeddings: An encoding approach", "author": ["Mou et al.2016] Lili Mou", "Ran Jia", "Yan Xu", "Ge Li", "Lu Zhang", "Zhi Jin"], "venue": "In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Discriminabilitybased transfer between neural networks", "author": ["Lorien Y. Pratt"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Pratt.,? \\Q1993\\E", "shortCiteRegEx": "Pratt.", "year": 1993}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Afet: Automatic fine-grained entity typing by hierarchical partial-label embedding", "author": ["Ren et al.2016] Xiang Ren", "Wenqi He", "Meng Qu", "Lifu Huang", "Heng Ji", "Jiawei Han"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Ren et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2016}, {"title": "Cross language text classification by model translation and semi-supervised learning", "author": ["Shi et al.2010] Lei Shi", "Rada Mihalcea", "Mingjun Tian"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Shi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2010}, {"title": "An attentive neural architecture for fine-grained entity type classification", "author": ["Pontus Stenetorp", "Kentaro Inui", "Sebastian Riedel"], "venue": "In Proceedings of the 5th Workshop on Automated Knowledge Base Construction,", "citeRegEx": "Shimaoka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shimaoka et al\\.", "year": 2016}, {"title": "Yago: A core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of the 16th International Conference on World Wide Web,", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Ryan McDonald", "Jakob Uszkoreit"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": null, "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Transfer learning for speech and language processing", "author": ["Wang", "Zheng2015] Dong Wang", "Thomas Fang Zheng"], "venue": "In 2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA),", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "BBN Pronoun Coreference and Entity Type Corpus LDC2005T33", "author": ["Weischedel", "Brunstein2005] Ralph Weischedel", "Ada Brunstein"], "venue": "Linguistic Data Consortium,", "citeRegEx": "Weischedel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weischedel et al\\.", "year": 2005}, {"title": "Embedding methods for fine grained entity type classification", "author": ["Daniel Gillick", "Nevena Lazic"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-", "citeRegEx": "Yogatama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2015}, {"title": "HYENA: Hierarchical type classification for entity names", "author": ["Sandro Bauer", "Johannes Hoffart", "Marc Spaniol", "Gerhard Weikum"], "venue": "In Proceedings of COLING 2012: Posters,", "citeRegEx": "Yosef et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yosef et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "This classification is useful for many natural language processing (NLP) tasks such as relation extraction (Mintz et al., 2009), machine translation (Koehn et al.", "startOffset": 107, "endOffset": 127}, {"referenceID": 14, "context": ", 2007), question answering (Lin et al., 2012) and knowledge base construction (Dong et al.", "startOffset": 28, "endOffset": 46}, {"referenceID": 5, "context": ", 2012) and knowledge base construction (Dong et al., 2014).", "startOffset": 40, "endOffset": 59}, {"referenceID": 11, "context": "tor (Ling and Weld, 2012; Koch et al., 2014; Mitchell et al., 2015) since this helps in filtering out candidate relation types that do not follow the type constrain.", "startOffset": 4, "endOffset": 67}, {"referenceID": 6, "context": "to its potential answers and significantly improves performance (Dong et al., 2015).", "startOffset": 64, "endOffset": 83}, {"referenceID": 5, "context": "to its potential answers and significantly improves performance (Dong et al., 2015). For example, Li and Roth (2002) rank questions based on their expected answer types (will the answer be food, vehicle or disease).", "startOffset": 65, "endOffset": 117}, {"referenceID": 1, "context": "pus to knowledge bases such as Freebase (Bollacker et al., 2008), DBpedia (Auer et al.", "startOffset": 40, "endOffset": 64}, {"referenceID": 0, "context": ", 2008), DBpedia (Auer et al., 2007), YAGO (Suchanek et al.", "startOffset": 17, "endOffset": 36}, {"referenceID": 26, "context": ", 2007), YAGO (Suchanek et al., 2007).", "startOffset": 14, "endOffset": 37}, {"referenceID": 32, "context": "Assuming training data to be noise free (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Shimaoka et al., 2016)", "startOffset": 40, "endOffset": 127}, {"referenceID": 31, "context": "Assuming training data to be noise free (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Shimaoka et al., 2016)", "startOffset": 40, "endOffset": 127}, {"referenceID": 25, "context": "Assuming training data to be noise free (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Shimaoka et al., 2016)", "startOffset": 40, "endOffset": 127}, {"referenceID": 32, "context": "Use of hand crafted features (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Ren et al., 2016)", "startOffset": 29, "endOffset": 111}, {"referenceID": 31, "context": "Use of hand crafted features (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Ren et al., 2016)", "startOffset": 29, "endOffset": 111}, {"referenceID": 23, "context": "Use of hand crafted features (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Ren et al., 2016)", "startOffset": 29, "endOffset": 111}, {"referenceID": 23, "context": "AFET system (Ren et al., 2016).", "startOffset": 12, "endOffset": 30}, {"referenceID": 21, "context": "ing transfer learning (Pratt, 1993) for FETC task both at feature and model level.", "startOffset": 22, "endOffset": 35}, {"referenceID": 7, "context": "While the initial work assumed that all labels present in a training dataset for an entity mention are correct, Gillick et al. (2014) introduced context dependent FETC and proposed a set of heuristics for pruning labels that might not be relevant given the entity mention\u2019s local context.", "startOffset": 112, "endOffset": 134}, {"referenceID": 7, "context": "While the initial work assumed that all labels present in a training dataset for an entity mention are correct, Gillick et al. (2014) introduced context dependent FETC and proposed a set of heuristics for pruning labels that might not be relevant given the entity mention\u2019s local context. Yogatama et al. (2015)", "startOffset": 112, "endOffset": 312}, {"referenceID": 23, "context": "Most recently, Ren et al. (2016) have proposed AFET, an FETC system.", "startOffset": 15, "endOffset": 33}, {"referenceID": 24, "context": "Transfer learning is well applied to many NLP applications, such as cross-domain document classification (Shi et al., 2010), multi-lingual word clustering (T\u00e4ckstr\u00f6m et al.", "startOffset": 105, "endOffset": 123}, {"referenceID": 27, "context": ", 2010), multi-lingual word clustering (T\u00e4ckstr\u00f6m et al., 2012) and sentiment classification (Mou et al.", "startOffset": 39, "endOffset": 63}, {"referenceID": 19, "context": ", 2012) and sentiment classification (Mou et al., 2016).", "startOffset": 37, "endOffset": 55}, {"referenceID": 19, "context": ", 2012) and sentiment classification (Mou et al., 2016). Initialization of word vectors with pre-trained word vectors in neural network models can be considered as one of the best example of transfer learning in NLP. Wang et al. (2015) provide a broad overview of transfer learning techniques used for language processing.", "startOffset": 38, "endOffset": 236}, {"referenceID": 8, "context": "We use bi-directional LSTM encoders (Graves et al., 2013) to encode token level sequences of both context to a fixed dimensional vector.", "startOffset": 36, "endOffset": 57}, {"referenceID": 25, "context": "The context representation described above is slightly different from what was proposed in (Shimaoka et al., 2016), here we include entity men-", "startOffset": 91, "endOffset": 114}, {"referenceID": 30, "context": "Similar to Yogatama et al. (2015) and Ren et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 23, "context": "(2015) and Ren et al. (2016), we embed feature representations and labels in a same dimensional space such that an", "startOffset": 11, "endOffset": 29}, {"referenceID": 31, "context": "Whereas in (Yogatama et al., 2015; Ren et al., 2016) positive labels should have a higher score than negative labels.", "startOffset": 11, "endOffset": 52}, {"referenceID": 23, "context": "Whereas in (Yogatama et al., 2015; Ren et al., 2016) positive labels should have a higher score than negative labels.", "startOffset": 11, "endOffset": 52}, {"referenceID": 4, "context": "DBpedia spotlight (Daiber et al., 2013) was used to automatically link entity mention in sentences to Freebase.", "startOffset": 18, "endOffset": 39}, {"referenceID": 21, "context": "available datasets, provided in a pre-processed tokenized format by Ren et al. (2016). Statistics of the datasets used in this work are shown in Table 1.", "startOffset": 68, "endOffset": 86}, {"referenceID": 4, "context": "DBpedia spotlight (Daiber et al., 2013) was used to automatically link entity mention in sentences to Freebase. For this corpus, manually annotated test data was shared by Gillick et al. (2014). BBN: BBN dataset consists of sentences from Wall Street Journal articles and is completely manually annotated (Weischedel and Brunstein, 2005).", "startOffset": 19, "endOffset": 194}, {"referenceID": 23, "context": "Please refer to (Ren et al., 2016) for more details Datasets Wiki/FIGER(GOLD) OntoNotes BBN", "startOffset": 16, "endOffset": 34}, {"referenceID": 32, "context": "state-of-the-art entity classification methods2: (1) FIGER (Ling and Weld, 2012); (2) HYENA (Yosef et al., 2012); (3) AFETNoCo (Ren et al.", "startOffset": 92, "endOffset": 112}, {"referenceID": 23, "context": ", 2012); (3) AFETNoCo (Ren et al., 2016): AFET without data based label-label correlation modeled in loss function; (4) AFET-CoH (Ren et al.", "startOffset": 22, "endOffset": 40}, {"referenceID": 23, "context": ", 2016): AFET without data based label-label correlation modeled in loss function; (4) AFET-CoH (Ren et al., 2016):", "startOffset": 96, "endOffset": 114}, {"referenceID": 23, "context": "AFET with hierarchy based label-label correlation modeled in loss function; (5) AFET (Ren et al., 2016); (6) Attentive (Shimaoka et al.", "startOffset": 85, "endOffset": 103}, {"referenceID": 25, "context": ", 2016); (6) Attentive (Shimaoka et al., 2016): An attentive neural network based model.", "startOffset": 23, "endOffset": 46}, {"referenceID": 31, "context": "FETC use same measures (Ling and Weld, 2012; Yogatama et al., 2015; Shimaoka et al., 2016; Ren et al., 2016).", "startOffset": 23, "endOffset": 108}, {"referenceID": 25, "context": "FETC use same measures (Ling and Weld, 2012; Yogatama et al., 2015; Shimaoka et al., 2016; Ren et al., 2016).", "startOffset": 23, "endOffset": 108}, {"referenceID": 23, "context": "FETC use same measures (Ling and Weld, 2012; Yogatama et al., 2015; Shimaoka et al., 2016; Ren et al., 2016).", "startOffset": 23, "endOffset": 108}, {"referenceID": 23, "context": "Whenever possible, the baselines result are reported from (Ren et al., 2016), otherwise we re-implemented baseline methods based on description available in corresponding papers.", "startOffset": 58, "endOffset": 76}, {"referenceID": 25, "context": "Comparison with other feature learning methods: The proposed model and its variants (ourAllC, our-NoM) perform better than the existing feature learning method by Shimaoka et al. (2016) (Attentive), consistently on all datasets.", "startOffset": 163, "endOffset": 186}, {"referenceID": 32, "context": "612 HYENA* (Yosef et al., 2012) 0.", "startOffset": 11, "endOffset": 31}, {"referenceID": 23, "context": "587 AFET-NoCo* (Ren et al., 2016) 0.", "startOffset": 15, "endOffset": 33}, {"referenceID": 23, "context": "716 AFET-CoH* (Ren et al., 2016) 0.", "startOffset": 14, "endOffset": 32}, {"referenceID": 23, "context": "712 AFET* (Ren et al., 2016) 0.", "startOffset": 10, "endOffset": 28}, {"referenceID": 23, "context": "735 AFET\u2020\u2021 (Ren et al., 2016) 0.", "startOffset": 11, "endOffset": 29}, {"referenceID": 25, "context": "747 Attentive\u2020 (Shimaoka et al., 2016) 0.", "startOffset": 15, "endOffset": 38}, {"referenceID": 23, "context": "These results are from (Ren et al., 2016) that also uses 10% of the test set as development set and the remaining for evaluation.", "startOffset": 23, "endOffset": 41}, {"referenceID": 23, "context": "These results are from (Ren et al., 2016) that also uses 10% of the test set as development set and the remaining for evaluation. \u2021We used the publicly available code distributed by Ren et al. (2016). \u2020All of these results are on exact same train, development and test set.", "startOffset": 24, "endOffset": 200}], "year": 2017, "abstractText": "Fine-grained entity type classification (FETC) is the task of classifying an entity mention to a broad set of types. Distant supervision paradigm is extensively used to generate training data for this task. However, generated training data assigns same set of labels to every mention of an entity without considering its local context. Existing FETC systems have two major drawbacks: assuming training data to be noise free and use of hand crafted features. Our work overcomes both drawbacks. We propose a neural network model that jointly learns entity mentions and their context representation to eliminate use of hand crafted features. Our model treats training data as noisy and uses non-parametric variant of hinge loss function. Experiments show that the proposed model outperforms previous stateof-the-art methods on two publicly available datasets, namely FIGER(GOLD) and BBN with an average relative improvement of 2.69% in micro-F1 score. Knowledge learnt by our model on one dataset can be transferred to other datasets while using same model or other FETC systems. These approaches of transferring knowledge further improve the performance of respective models.", "creator": "LaTeX with hyperref package"}}}