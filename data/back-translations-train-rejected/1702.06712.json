{"id": "1702.06712", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Ensembles of Randomized Time Series Shapelets Provide Improved Accuracy while Reducing Computational Costs", "abstract": "Shapelets are discriminative time series subsequences that allow generation of interpretable classification models, which provide faster and generally better classification than the nearest neighbor approach. However, the shapelet discovery process requires the evaluation of all possible subsequences of all time series in the training set, making it extremely computation intensive. Consequently, shapelet discovery for large time series datasets quickly becomes intractable. A number of improvements have been proposed to reduce the training time. These techniques use approximation or discretization and often lead to reduced classification accuracy compared to the exact method.", "histories": [["v1", "Wed, 22 Feb 2017 09:07:00 GMT  (933kb,D)", "http://arxiv.org/abs/1702.06712v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["atif raza", "stefan kramer"], "accepted": false, "id": "1702.06712"}, "pdf": {"name": "1702.06712.pdf", "metadata": {"source": "CRF", "title": "Ensembles of Randomized Time Series Shapelets Provide Improved Accuracy while Reducing Computational Costs", "authors": ["Atif Raza", "Stefan Kramer"], "emails": ["atifraza@uni-mainz.de", "kramer@informatik.uni-mainz.de"], "sections": [{"heading": null, "text": "We propose the use of ensembles of shapelet-based classifiers obtained from random samples of shapelet candidates. Random samples reduce the number of candidates evaluated and thus the computational costs required, while the classification accuracy of the resulting models does not differ significantly from that of the exact algorithm. Combining randomised classifiers corrects the inaccuracies of individual models due to the variety of solutions. Based on the experiments conducted, it appears that the proposed approach of using an ensemble of cost-effective classifiers offers better classification accuracy compared to the exact method at significantly lower computational costs."}, {"heading": "1. INTRODUCTION", "text": "This year it is more than ever before in the history of the city."}, {"heading": "2. BACKGROUND", "text": "The vast amounts of time series data are a treasure trove of information waiting to be mined and examined for the hidden insights they can provide. Time series classification using the closest neighbor approach is a very simple and highly effective technique that has been extensively used. However, the computational complexity of the candidates in the classification phase, together with little or no interpretation, determines a sequence or shape-based classification approach. In form series-based classification, the presence (or absence) of a specific sequence or \"shape\" determines their class in a time series. If the distance of a time series from the shape sequence is less than a threshold, then it is said that this shape sequence is included and vice versa. We will summarize the form finding algorithm and some of the proposed speed techniques in Sections 2.1 and 2.2. For a detailed introduction, we refer readers to the respective papers [6-11.9] A time series is evaluated."}, {"heading": "2.1 Shapelet Discovery", "text": "The Shapelet discovery algorithm aims to divide the data set into \"purer\" subsets, so that the instances with and without Shapelet form two separate splits. Purity of the obtained splits is evaluated on the basis of the amount of information gained, although other approaches can also be used [8]. The Shapelet discovery process is embedded in a decision tree that locates learners of the respective nodes. (This Shapelet discovery process represents the decision criterion for the respective candidate and splits the data set for the subsequent nodes of the tree. (The decision tree learners initiate the Shapelet discovery process or form leaf nodes with the splits according to their degree of purity). Algorithm 1 provides the basic algorithm for learning the classification models.Algorithm 2 lists the time series of shapes."}, {"heading": "2.2 Speed-up Strategies", "text": "The authors of the YK shapelets algorithm have proposed early distance calculations and early candidate truncations using an upper limit for information gathering and reported an acceleration of three orders of magnitude compared to the brute force approach [13]. The logical shapelets algorithm reduces the computational complexity of candidates to O (k2m3) by reusing previously calculated distances and truncating candidates using triangular inequality [9]. It can reduce the computational complexity of candidates to O (k2m3), but the caching series of candidates requires a memory requirement in the order of O (km2), which limits the use of this algorithm for large datasets in memory-enclosed environments. The fast shapelets algorithm reduces the dimensionality of the data using SAX [7] and then performs a random projection based on shape finding data."}, {"heading": "2.3 Ensemble Methods", "text": "Ensemble methods are based on the idea of \"combining\" the opinions of different \"experts\" to arrive at a decision on a particular issue. Members of an ensemble can have a different view of the data, or they can use different characteristics to make their decision, or they can be completely different algorithms. This ensures diversity in ensemble decisions, which often makes them more accurate than individual models. Several studies have shown that ensembles can often be more effective than individual machine learning models. Due to the variety of proposed combinations in the ensemble literature, it is possible to experiment with a few options. A basic ensemble ensemble of classifiers can be achieved by combining several different models, all of which are trained using the same data. Another approach to building ensembles is bagging [2], which trains N models with different strands of data each, so that instances can be scanned with substitutes from the original data sets."}, {"heading": "3. PROPOSED METHOD", "text": "In this section we describe the proposed method, which is a combination of the random shapelets algorithms and standard ensemble approaches. (This variability makes the random shapelets algorithm a diverse algorithm and can be used to our advantage.) The lower computing costs and inherent randomization of the random shapelets algorithms make it a primary candidate for inclusion in an ensemble. Therefore, we can use the random shapelets algorithm as a basic classifier in a common learning approach. (This combines the strengths of the ensemble algorithms 4: Boosting Ensembles (D, N) 1: M: 2: w1i 2 | D: for n = 1 to N: 4: MAKs."}, {"heading": "3.1 Optimizations", "text": "The size of the sample directly influences the overall reduction in the calculation costs achieved by the random shapelets algorithm, and smaller sample ratios lead to higher reduction and vice versa. Authors of the random shapelets algorithm only evaluated the impact of the sample of the shapelets candidates, but the approach can be further optimized by incorporating the various acceleration techniques proposed in some other research efforts. Primarily, the early abandonment of distance calculations and early candidate circumcision approaches introduced in the YK shapelets work can also be incorporated into the random shapelets algorithm. Distance calculations are abandoned as soon as the distance between the candidate and the subsequence from the current window exceeds the current \"best value to date.\" The candidates themselves are truncated on the basis of a cost-effective data acquisition calculation so that the current value of the calculation is calculated."}, {"heading": "3.2 Normalization", "text": "Normalizing partial sequences before calculating the distance provides a better overall accuracy for the Shapelet detection algorithm, since the similarity of time series benefits from scaling and offset invariance. Therefore, we need to normalize the partial sequences before calculating the distance z. This requires the calculation of mean and standard deviations for each partial sequence before calculating the distance. Calculation of these values accounts for the majority of the calculation required for normalizing the partial sequences and renders the calculation costs unsustainable. Using a \"summary statistical\" [12] approach can drastically reduce the computational effort required for calculating the mean and standard deviation of a given partial sequence. \"Summary statistical data\" for each time series instance in the training set are calculated at the time of loading the data sets, while for the Shapelet candidates at the time of generating the mean and standard deviation value, the standard deviation value can be calculated as a more or less consistent method of calculating the mean and standard deviation value prior to the standardization process."}, {"heading": "4. EXPERIMENTAL DESIGN", "text": "The main objective of our experimental evaluation is twofold: to assess whether (1) the proposed approach offers better or equivalent classification accuracy compared to the YK Shapelets approach and (2) whether it requires less computational effort compared to the exact method. Therefore, an extensive series of experiments has been conducted to evaluate and compare the different approaches; we have established the classification accuracy and runtime of the YK Shapelets approach as a starting point; we have also compared the results for the Fast Shapelets algorithm [10] and the gRSF algorithm [6]; and the YK Shapelets, Random Shapelets and the three ensemble approaches have been implemented in Java using a consistent program structure so that no algorithm is unduly biased."}, {"heading": "4.1 Datasets", "text": "The empirical evaluations were conducted using 45 data sets publicly available in the UCR Time Series Archive.5 The data sets cover a variety of areas, including ECG measurements, image outlines, motion capture data, sensor values, spectral analysis and synthetically generated data. The issues covered in these data sets range from binary to multi-class issues. The original training and test splits are used as such, using the training splits to train classifiers and report training time, while the test set is used to report classification accuracy."}, {"heading": "4.2 Experiments", "text": "We have conducted experiments with YK shapelets, FastShapelets, Random shapelets and ensembles of RandomShapelets-based classifiers. Since YK shapelets are an exact method and their classification accuracy remains the same over different runs, provided the parameters of candidate lengths remain the same, each data set is evaluated once. In all other algorithms, each data set is evaluated 100 times with each algorithm and the mean and standard deviation of the achieved classification accuracy is given. The number of classification models per group is set at ten for each variant and the vote is used for the final decision. In all of our experiments, we use full-grown decision tree models for all algorithms."}, {"heading": "4.3 Parameter Settings", "text": "The most important parameters required for the shapelet discovery process are minLen and maxLen = 67, which define the range of possible shapelet candidate lengths. The shapelet discovery process searches for candidates in all possible window sizes between the provided minimum and maximum length sizes. For example, if minLen = 10 and maxLen = 20, then the shapelet discovery process will search for shapelets in all window sizes, starting at 10 and ending at 20. Therefore, these values are set to the extreme cases where minLen = 1 and maxLen = m, where m is the instance length of the time series, making the algorithm search across the entire candidate set. Another approach is to set these parameters based on some assumptions about the possible shapelet lengths. However, setting these parameters may be incorrect for the shapelet discovery process."}, {"heading": "5. RESULTS", "text": "We evaluate the competing algorithms on the basis of classification accuracy and required computational costs. We used 45 different sets of data for a thorough evaluation of the algorithms. All experiments were conducted in a high-performance cluster. The maximum permissible time for evaluating a data set with any algorithm was set at 10 days. If the experiment for a data set was not completed in that timeframe, we register it as \"DNF.\" We summarize the results in this paragraph.6"}, {"heading": "5.1 Classification Accuracy", "text": "In our experiments, the ensembles of the random shapelets classifiers consistently performed better than the other algorithms and yielded better classification accuracy. This observation is particularly interesting because the individual models in the ensembles used only 1% of the possible candidates, which were consistently sampled from the set of all possible candidates. Figures 2a and 2b show the diagram of critical differences in classification accuracy of the individual algorithms for a p = 0.05 level of significance. The ensembles outperformed the other algorithms. The large difference between the average ranks of the ensembles and other algorithms, especially YKShapelets, shows that the average improvement in classification accuracy is also significant. In many data sets, the improvement in classification accuracy was up to 20%, whereas the ensembles compared to the classification accuracy achieved with the YK shapelets algorithm, the total number of victories for ensembles over the other algorithms was 35 fixed and 35 fixed parameters."}, {"heading": "5.2 Run Time", "text": "6Detailed results can be found at https: / / dx.doi.org / 10.6084 / m9.figshare.4299479The training time for shapelet-based classifiers is almost the entire running time of the algorithms, as the test time is negligible compared to training time. The time required for the training classification models using the various approaches was recorded for the training phase using standard Java timing utilities.The fast shapelets algorithm was the fastest overall, followed by the RandomShapelets algorithm and then the ensembles (bagging, simple combination, boosting) and finally the YK shapelets. The ensembles consistently performed faster than the YKShapelets algorithm and achieved an acceleration of more than an order of magnitude on average. Tables 3 and 4 list the average time (in seconds) for evaluating each dataset with the appropriate algorithm using optimized parameters."}, {"heading": "5.3 EnRS-Bagging vs. gRSF", "text": "The Generalized Random Shapelet Forests or gRSF algorithm is very similar to the approach we call Ensembles of Random Shapelets using dredging or EnRS dredging, so we compared the classification accuracy of the two algorithms. We used the Java implementation provided by the authors of the gRSF, and evaluated all the data sets evaluated in our other experiments. MinLen and maxLen parameters were determined using the best parameters reported in the gRSF thesis by setting the ensemble size to 500, while we performed all of our experiments with only 10 models per group, so that we performed the experiments for gRSF with 10 models per group to make a fair comparison between the two algorithms. Figure 4 shows the critical differences chart and the average ranks for the classification accuracy of the results of the two algorithms, which are not even measured as an average score for each of the two algorithms, but always indicates a lower than an average score for each of the two candidates."}, {"heading": "6. DISCUSSION", "text": "The EnRS Bagging Approach is the fastest of the three ensemble approaches tested in our experiments, while EnRS Boosting is the slowest. EnRS Bagging Approach also has a higher number of overall wins achieved by the other two ensemble approaches. EnRS Bagging results in faster identification of good or bad candidates, so bagging allows the algorithms to run faster. Duplicated instances in the training datasets also introduce a bias toward the majority-class instances, leading to early extraction of the majority-class Shapelet-specific instances and allowing the algorithms to effectively divide the data and then search for shapelets for the other instances, making the process efficient."}, {"heading": "7. CONCLUSION", "text": "The advantages of the proposed method are twofold: better classification accuracy and less computational effort; better classification accuracy has been achieved for almost all the data sets evaluated, while the runtime has been reduced in all cases; the simplicity and added value of the approach make it very suitable for form finding and classification; the use of bagging may reduce the required calculation, but in some cases the classification accuracy of the model obtained is slightly inferior to that of the ensemble of RandomShapelets classifiers trained on the original training data set, although not significant. Currently, the Random Shapelets algorithm can only evaluate the candidates with a sampling ratio determined at the beginning of the process; the ability to modify the fraction of the evaluated candidates and use the results in an additive way to verify the step towards further classification results that could already be achieved."}, {"heading": "8. REFERENCES", "text": "[1] A. Bagnall and J. Lines. An Experimental Evaluationof Nearest Neighbour Time Series Classification. CoRR, 1406.4757, 2014. [2] L. Breiman. Bagging predictors. Machine learning, 24 (2): 123-140, 1996. [3] H. Ding, G. Trajcevski, P. Scheuermann, X. Wang, and E. Keogh. Querying and mining of time series data: experimental comparison of representations and distance measures. Proceedings of the VLDB Endowment, 1 (2): 1542-1552, 2008. [4] Y. Freund and R. E. Schapire. A desicion-theoretical generalization of on-line learning and an application to boosting. In Computational learning theory, pp. 23-37. Springer, 1995. [5] L. Hansen and P. Salamon. Neural network ensembles. IEEE Transactions on tern Pattern Analysis and Machine Intelligence, 12 (10): 993-1001 I. Karlsfeld."}], "references": [{"title": "An Experimental Evaluation of Nearest Neighbour Time Series Classification", "author": ["A. Bagnall", "J. Lines"], "venue": "CoRR, 1406.4757", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine learning, 24(2):123\u2013140", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Querying and mining of time series data: experimental comparison of representations and distance measures", "author": ["H. Ding", "G. Trajcevski", "P. Scheuermann", "X. Wang", "E. Keogh"], "venue": "Proceedings of the VLDB Endowment, 1(2):1542\u20131552", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Computational learning theory, pages 23\u201337. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Neural network ensembles", "author": ["L. Hansen", "P. Salamon"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(10):993\u20131001", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1990}, {"title": "Generalized random shapelet forests", "author": ["I. Karlsson", "P. Papapetrou", "H. Bostr\u00f6m"], "venue": "Data Mining and Knowledge Discovery, 30(5):1053\u20131085", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Experiencing SAX: a novel symbolic representation of time series", "author": ["J. Lin", "E. Keogh", "L. Wei", "S. Lonardi"], "venue": "Data Mining and Knowledge Discovery, 15(2):107\u2013144", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "A shapelet transform for time series classification", "author": ["J. Lines", "L.M. Davis", "J. Hills", "A. Bagnall"], "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD \u201912, pages 289\u2013297, New York, New York, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Logical-Shapelets: An Expressive Primitive for Time Series Classification", "author": ["A. Mueen", "E. Keogh", "N. Young"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD \u201911, page 1154, New York, New York, USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast Shapelets: A Scalable Algorithm for Discovering Time Series Shapelets", "author": ["T. Rakthanmanon", "E. Keogh"], "venue": "In Proceedings of the 2013 SIAM International Conference on Data Mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Random-shapelet: An algorithm for fast shapelet discovery", "author": ["X. Renard", "M. Rifqi", "W. Erray", "M. Detyniecki"], "venue": "In 2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Braid: Stream mining through group lag correlations", "author": ["Y. Sakurai", "S. Papadimitriou", "C. Faloutsos"], "venue": "Proceedings of the 2005 ACM SIGMOD international conference on Management of data, pages 599\u2013610. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Time Series Shapelets: A New Primitive for Data Mining", "author": ["L. Ye", "E. Keogh"], "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD \u201909, pages 947\u2013956, New York, New York, USA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "The most investigated approach for time series classification has been the nearest neighbor algorithm coupled with various distance measures [1,3].", "startOffset": 141, "endOffset": 146}, {"referenceID": 2, "context": "The most investigated approach for time series classification has been the nearest neighbor algorithm coupled with various distance measures [1,3].", "startOffset": 141, "endOffset": 146}, {"referenceID": 12, "context": "dress the drawbacks of nearest neighbor based time series classification [13].", "startOffset": 73, "endOffset": 77}, {"referenceID": 4, "context": "make different errors for an unseen problem [5].", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "For a detailed introduction, we refer the readers to the respective papers [6, 9\u201311,13].", "startOffset": 75, "endOffset": 87}, {"referenceID": 8, "context": "For a detailed introduction, we refer the readers to the respective papers [6, 9\u201311,13].", "startOffset": 75, "endOffset": 87}, {"referenceID": 9, "context": "For a detailed introduction, we refer the readers to the respective papers [6, 9\u201311,13].", "startOffset": 75, "endOffset": 87}, {"referenceID": 10, "context": "For a detailed introduction, we refer the readers to the respective papers [6, 9\u201311,13].", "startOffset": 75, "endOffset": 87}, {"referenceID": 12, "context": "For a detailed introduction, we refer the readers to the respective papers [6, 9\u201311,13].", "startOffset": 75, "endOffset": 87}, {"referenceID": 7, "context": "The purity of the obtained splits is evaluated using the information gain measure although other approaches can be used as well [8].", "startOffset": 128, "endOffset": 131}, {"referenceID": 12, "context": "The authors of the YK-Shapelets algorithm proposed early abandoning distance calculations and early candidate pruning using an upper-bound on the information gain and reported a speed-up of three orders of magnitude compared to the brute force approach [13].", "startOffset": 253, "endOffset": 257}, {"referenceID": 8, "context": "The Logical-Shapelets algorithm reduces computational costs by reusing previously calculated distances and pruning candidates using the triangular inequality [9].", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "The Fast-Shapelets algorithm reduces the dimensionality of the data using SAX [7] and then performs a random projection based shapelet discovery using this lower dimensional data [10].", "startOffset": 78, "endOffset": 81}, {"referenceID": 9, "context": "The Fast-Shapelets algorithm reduces the dimensionality of the data using SAX [7] and then performs a random projection based shapelet discovery using this lower dimensional data [10].", "startOffset": 179, "endOffset": 183}, {"referenceID": 10, "context": "The Random-Shapelets algorithm performs a uniform random sampling of candidates to reduce the number of evaluated candidates [11].", "startOffset": 125, "endOffset": 129}, {"referenceID": 5, "context": "A recently published approach called Generalized Random Shapelet Forests (gRSF) also employs ensembles and a randomized candidate sampling based shapelet discovery process for improved classification accuracy and reduced runtime [6].", "startOffset": 229, "endOffset": 232}, {"referenceID": 1, "context": "Another approach for constructing ensembles is that of Bagging [2] which trains N models, each with a different bootstrap of data such that |D| instances are sampled with replacement from the original dataset.", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "Another approach called Boosting [4] relies on weighted instances.", "startOffset": 33, "endOffset": 36}, {"referenceID": 11, "context": "Using a \u201csummary statistics\u201d [12] based approach can drastically reduce the amount of computation required to calculate the mean and standard deviation of a given subsequence.", "startOffset": 29, "endOffset": 33}, {"referenceID": 9, "context": "We have also compared the results for the Fast-Shapelets algorithm [10] and the gRSF algorithm [6].", "startOffset": 67, "endOffset": 71}, {"referenceID": 5, "context": "We have also compared the results for the Fast-Shapelets algorithm [10] and the gRSF algorithm [6].", "startOffset": 95, "endOffset": 98}], "year": 2017, "abstractText": "Shapelets are discriminative time series subsequences that allow generation of interpretable classification models, which provide faster and generally better classification than the nearest neighbor approach. However, the shapelet discovery process requires the evaluation of all possible subsequences of all time series in the training set, making it extremely computation intensive. Consequently, shapelet discovery for large time series datasets quickly becomes intractable. A number of improvements have been proposed to reduce the training time. These techniques use approximation or discretization and often lead to reduced classification accuracy compared to the exact method. We are proposing the use of ensembles of shapelet-based classifiers obtained using random sampling of the shapelet candidates. Using random sampling reduces the number of evaluated candidates and consequently the required computational cost, while the classification accuracy of the resulting models is also not significantly different than that of the exact algorithm. The combination of randomized classifiers rectifies the inaccuracies of individual models because of the diversity of the solutions. Based on the experiments performed, it is shown that the proposed approach of using an ensemble of inexpensive classifiers provides better classification accuracy compared to the exact method at a significantly lesser computational cost.", "creator": "LaTeX with hyperref package"}}}