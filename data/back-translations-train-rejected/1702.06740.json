{"id": "1702.06740", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Improving Chinese SRL with Heterogeneous Annotations", "abstract": "Previous studies on Chinese semantic role labeling (SRL) have concentrated on single semantically annotated corpus. But the training data of single corpus is often limited. Meanwhile, there usually exists other semantically annotated corpora for Chinese SRL scattered across different annotation frameworks. Data sparsity remains a bottleneck. This situation calls for larger training datasets, or effective approaches which can take advantage of highly heterogeneous data. In these papers, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together. We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and effectively transfer knowledge between them. We also release a new corpus, Chinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that ours model outperforms state-of-the-art methods.", "histories": [["v1", "Wed, 22 Feb 2017 10:34:47 GMT  (136kb,D)", "http://arxiv.org/abs/1702.06740v1", "9 pages, 4 figures"], ["v2", "Mon, 13 Mar 2017 06:46:12 GMT  (136kb,D)", "http://arxiv.org/abs/1702.06740v2", "9 pages, 4 figures"], ["v3", "Tue, 14 Mar 2017 13:05:23 GMT  (0kb,I)", "http://arxiv.org/abs/1702.06740v3", "This paper has been withdrawn by the author due to a crucial error in equation 10"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qiaolin xia", "baobao chang", "zhifang sui"], "accepted": false, "id": "1702.06740"}, "pdf": {"name": "1702.06740.pdf", "metadata": {"source": "CRF", "title": "A Progressive Learning Approach to Chinese SRL Using Heterogeneous Data", "authors": ["Qiaolin Xia", "Baobao Chang", "Zhifang Sui"], "emails": ["xql@pku.edu.cn", "chbb@pku.edu.cn", "szf@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to go to another world, to go to another world, to go to another world, to go to another world."}, {"heading": "2 Heterogeneous Corpora for Chinese SRL", "text": "In this paper, we present a new SRL-Corpus Chinese SemBank (COD) and use it as an example of heterogeneous data in our experiments. In this section, we first briefly introduce the corpus and then compare it with existing corporaties.Sentences in COD come from a variety of sources, including online articles and news. The aim of this project is to build a very large and complete Chinese semantic corpus in the future. Currently, COD focuses only on predicate argument structures in a sentence without any notice of temporal relationships and correspondences. Compared to the most commonly used datasets CPB, COD is different in the following aspects: \u2022 In terms of predicates, COD takes into account a wider range of predicates. We have not only come common verbs, but also nominal verbs, such as nominal verbs, and state words as well."}, {"heading": "3 Challenges in Inheriting Knowledge from Heterogeneous Corpora", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4 Progressive Learning Approach", "text": "We propose a progressive learning approach that is ideal for combining heterogeneous SRL data for several reasons: first, it can accommodate different inputs with different schemes, syntactic information and domains, because it allows models for heterogeneous resources to be extremely different, such as different network structures, different width and different learning rates, etc. Second, it is immune to oblivion by freezing learned weights and harnessing previous knowledge of lateral connections. Third, the lateral connections can be augmented by recurring structure and gate mechanisms to overcome problems over long distances. Our model is mainly inspired by Rusu et al. (2016), who proposed progressive neural networks for a variety of reinforcement learning tasks (e.g. Atari games and robot simulation). In their cases, inputs are pixels, outputs are learned, and each column consisting of simple layers and evolutionary layers is trained to solve a particular dekkov process."}, {"heading": "4.1 Progressive Neural Networks", "text": "Fig. 2a is an illustration of the basic model of progressive neural networks. It begins with a single column (a neural network) in which there are hidden layers, and the output for ith layer (i \u2264 L) with ni units is h1i-Rni. H1 denotes the parameters to be learned in the first column. When switching to a second corpus, it \"freezes\" the parameter \"1\" and randomly initializes a new column with the parameters \"2 and multiple lateral connections between two columns, so that the layers\" h2i-1 and h-1 i-1. In this straightforward way, progressive neural networks can use columns with arbitrary structures or compile lateral connections in an ensemble setting. To be more general, we calculate the output of ith layer in kth column \"hki-1 and h-1.\""}, {"heading": "4.2 PNN with Gated Recurrent Adapter for Chinese SRL", "text": "We reconstruct PNN with bidirectional LSTM to solve SRL problems. Our model is illustrated in Figure 3. < Firstly, each column in the PNN architecture is a stacked bidirectional LSTM RNN \u2212 \u2212 \u2212 \u2212 \u2212 instead of Convolutionary Neural Networks, since inputs are non-pixels, and bi-LSTM RNN has proven powerful for Chinese SRL (Wang et al., 2015). Secondly, we improve the adapters \u2212 \u2212 \u2212 with recursive structure and gate mechanism, since the simple Multi-Layer Perceptron (MLP) adapters have a limitation: their weights are learned word for word independently of each other. For tasks such as transferring reinforcement measures, this is sufficient because there are few interdependencies between activities. But in the NLP domain, things are different. Therefore, we add internal memory to help them remember what is needed from heterogeneous memory resources, to maintain a short-term equilibrium."}, {"heading": "4.3 Training Criteria", "text": "We use the sentence marker approach like Wang et al. (2015), because words in a sentence can be closely related to each other, independently of each other, the designation of each word is inappropriate; the sentence marker approach only takes into account valid transitional paths of tags when calculating costs. For example, when using the IOBES tagging scheme, the transition from I-Arg0 to B-Arg0 is invalid, and the transition from I-Arg0 to I-Arg1 is also invalid because the nature of the role within the semantic piece has been changed. For each task (column), the log probability of the sentence x and its correct path y islog p (y | x) = protocol xp, Nt ot, yt-z exp, Ni t ot, zt (10), where N is the number of words, ot-RM is the best output of the last layer at a given time. yt = k means that the test word has the least setic role."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experiment Settings", "text": "To compare our approach with others, we designed four experimental setups: (1) A simple LSTM setup on COD and CPB with automatic PoS tagging. Since CPB is about twice the size of the new corpus, we need to know if COD can be used to train good semantic parsers and how much information can be learned from COD by machine. So we conduct this experiment to create two baselines for COD and CPB. In this setup, we train and evaluate a single-column LSTM model on COD. (2) A simple LSTM setup on CPB with upstream word embedding on COD (marked as biLSTM + COD embedding)."}, {"heading": "5.2 Results", "text": "This year it has come to the point where it will be able to retaliate, \"he said.\" We have never lost as much time as we have in recent years, \"he said.\" We have never lost so much time, \"he said.\" We have never waited so long to be able to retaliate, \"he said."}, {"heading": "6 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Chinese Semantic Role Labeling", "text": "The concept of semantic role labeling was first proposed by Gildea and Jurafsky (2002). Previous work on the Chinese SRL focused mainly on improving the SRL on single corpus. Approaches are divided into two categories: feature-based machine learning approaches and neural network-based approaches. Sun and Jurafsky (2004) did the groundwork and achieved promising results without large commented corpus. After the CPB was established by Xue and Palmer (2003), more comprehensive and systematic research was conducted on the Chinese SRL (Xue and Palmer, 2005; Chen and Chang, 2006; Ding and Chang, 2009; Yang and al., 2014).Neural networking methods do not rely on handmade functions. For the Chinese SRL, Wang et al. (2015) proposed a bi-directional LSTM-RNN model. And based on their work, Shqua (2016) proposed further enhancing the post-workover methodology."}, {"heading": "6.2 Learning with Heterogeneous Data", "text": "In this paper, we focus mainly on learning with heterogeneous semantic resources for the Chinese SRL. Wang et al. (2015) introduced heterogeneous data by using pre-trained embeddings during initialization and achieved promising results. Guo et al. (2016) proposed a multi-task learning method with a unified neural network model to jointly learn SRL and classification tasks and also achieved improvements. In contrast to previous work, we proposed a progressive neural network model with gated recurrent adapters to harness knowledge from heterogeneous semantic data. In line with earlier methods, this approach is more constructive and not destructive, as it uses lateral connections to access already learned functions that are fixed when learning new tasks. And, by introducing gated recurrent adapters, we are expanding our model to deal with long sentences and achieve state-of-the-art performance based on the Chinese Propbank."}, {"heading": "7 Conclusion and Future Work", "text": "In this paper, we proposed a progressive neural network model with gated recurrent adapters to take advantage of the heterogeneous corpus of the Chinese SRL. Unlike previous methods such as finetuning, we use previous knowledge of lateral connections. Experiments have shown that our model performs better at the CPB than all base models. Furthermore, we proposed novel gated recurrent adapters to cope with long-sentence transmission, and the experiment has demonstrated the effectiveness of the new adapter structure. We believe that progressive learning with heterogeneous data is a promising path, so in the future we may try to combine more heterogeneous semantic data for other tasks such as event extraction and classification of relationships, etc. We also publish the new Corpus Chinese SemBank4 for the Chinese SRL in the hope that it will be helpful to provide common benchmarks for future tasks of the Chinese SRL."}], "references": [{"title": "Textual inference and meaning representation in human robot interaction", "author": ["Emanuele Bastianelli", "Giuseppe Castellucci", "Danilo Croce", "Roberto Basili."], "venue": "In Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Struc-", "citeRegEx": "Bastianelli et al\\.,? 2013", "shortCiteRegEx": "Bastianelli et al\\.", "year": 2013}, {"title": "On the parameter space of generative lexicalized statistical parsing models", "author": ["Daniel M Bikel."], "venue": "Ph.D. thesis, Citeseer.", "citeRegEx": "Bikel.,? 2004", "shortCiteRegEx": "Bikel.", "year": 2004}, {"title": "An empirical study of chinese chunking", "author": ["Wenliang Chen", "Yujie Zhang", "Hitoshi Isahara."], "venue": "Proceedings of the COLING/ACL on Main conference poster sessions. Association for Computational Linguistics, pages 97\u2013104.", "citeRegEx": "Chen et al\\.,? 2006", "shortCiteRegEx": "Chen et al\\.", "year": 2006}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, pages 160\u2013167.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Word based chinese semantic role labeling with semantic chunking", "author": ["Weiwei Ding", "Baobao Chang."], "venue": "International Journal of Computer Processing Of Languages 22(02n03):133\u2013154.", "citeRegEx": "Ding and Chang.,? 2009", "shortCiteRegEx": "Ding and Chang.", "year": 2009}, {"title": "Automatic labeling of semantic roles", "author": ["Daniel Gildea", "Daniel Jurafsky."], "venue": "Computational linguistics 28(3):245\u2013288.", "citeRegEx": "Gildea and Jurafsky.,? 2002", "shortCiteRegEx": "Gildea and Jurafsky.", "year": 2002}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u00edk", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber."], "venue": "arXiv preprint arXiv:1503.04069 .", "citeRegEx": "Greff et al\\.,? 2015", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "A unified architecture for semantic role labeling and relation classification", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu", "Jun Xu."], "venue": "Proc. of the 26th International Conference on Computational Linguistics (COLING).", "citeRegEx": "Guo et al\\.,? 2016", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Improving chinese semantic role labeling with english proposition bank", "author": ["Tianshi Li", "Qi Li", "BaoBao Chang."], "venue": "China National Conference on Chinese Computational Linguistics. Springer, pages 3\u201311.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Progressive neural networks", "author": ["Andrei A. Rusu", "Neil C. Rabinowitz", "Guillaume Desjardins", "Hubert Soyer", "James Kirkpatrick", "Koray Kavukcuoglu", "Razvan Pascanu", "Raia Hadsell."], "venue": "CoRR abs/1606.04671.", "citeRegEx": "Rusu et al\\.,? 2016", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Capturing argument relationships for chinese semantic role labeling", "author": ["Lei Sha", "Tingsong Jiang", "Sujian Li", "Baobao Chang", "Zhifang Sui"], "venue": null, "citeRegEx": "Sha et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sha et al\\.", "year": 2016}, {"title": "Shallow semantic parsing of chinese", "author": ["Honglin Sun", "Daniel Jurafsky."], "venue": "Proceedings of NAACL 2004. pages 249\u2013256.", "citeRegEx": "Sun and Jurafsky.,? 2004", "shortCiteRegEx": "Sun and Jurafsky.", "year": 2004}, {"title": "Chinese semantic role labeling with shallow", "author": ["Weiwei Sun", "Zhifang Sui", "Meng Wang", "Xin Wang"], "venue": null, "citeRegEx": "Sun et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2009}, {"title": "Chinese semantic role labeling with bidirectional recurrent neural networks", "author": ["Zhen Wang", "Tingsong Jiang", "Baobao Chang", "Zhifang Sui."], "venue": "Proc. of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1626\u20131631.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Modeling the translation of predicate-argument structure for smt", "author": ["Deyi Xiong", "Min Zhang", "Haizhou Li."], "venue": "In Proc. of the 50th Annual Meeting of the Association for Computational Linguistics. pages 902\u2013911.", "citeRegEx": "Xiong et al\\.,? 2012", "shortCiteRegEx": "Xiong et al\\.", "year": 2012}, {"title": "Annotating the predicateargument structure of chinese nominalizations", "author": ["Nianwen Xue."], "venue": "Proceedings of the fifth international conference on Language Resources and Evaluation. pages 1382\u2013 1387.", "citeRegEx": "Xue.,? 2006", "shortCiteRegEx": "Xue.", "year": 2006}, {"title": "Labeling chinese predicates with semantic roles", "author": ["Nianwen Xue."], "venue": "Computational linguistics 34(2):225\u2013255.", "citeRegEx": "Xue.,? 2008", "shortCiteRegEx": "Xue.", "year": 2008}, {"title": "Annotating the propositions in the penn chinese treebank", "author": ["Nianwen Xue", "Martha Palmer."], "venue": "Proceedings of the second SIGHAN workshop on Chinese language processing-Volume 17. Association for Computational Linguistics, pages 47\u201354.", "citeRegEx": "Xue and Palmer.,? 2003", "shortCiteRegEx": "Xue and Palmer.", "year": 2003}, {"title": "Automatic semantic role labeling for chinese verbs", "author": ["Nianwen Xue", "Martha Palmer."], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence. pages 1160\u20131165.", "citeRegEx": "Xue and Palmer.,? 2005", "shortCiteRegEx": "Xue and Palmer.", "year": 2005}, {"title": "Multipredicate semantic role labeling", "author": ["Haitong Yang", "Chengqing Zong"], "venue": "In EMNLP", "citeRegEx": "Yang and Zong,? \\Q2014\\E", "shortCiteRegEx": "Yang and Zong", "year": 2014}, {"title": "The fineness hierarchy of semantic roles and its application in nlp", "author": ["Yuan Yulin."], "venue": "Journal of Chinese Information Processing 21(4):10\u201320.", "citeRegEx": "Yulin.,? 2007", "shortCiteRegEx": "Yulin.", "year": 2007}, {"title": "Building a chinese treebank", "author": ["Qiang Zhou", "Wei Zhang", "Shiwen Yu."], "venue": "Journal of Chinese Information Processing 4.", "citeRegEx": "Zhou et al\\.,? 1997", "shortCiteRegEx": "Zhou et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Semantic role labeling (SRL) is one of the fundamental tasks in natural language processing because of its important role in information extraction (Bastianelli et al., 2013), statistical machine translation (Aziz et al.", "startOffset": 148, "endOffset": 174}, {"referenceID": 14, "context": ", 2013), statistical machine translation (Aziz et al., 2016; Xiong et al., 2012), and so on.", "startOffset": 41, "endOffset": 80}, {"referenceID": 17, "context": "benchmark dataset PropBank (Xue and Palmer, 2003) has about 54,900 sentences.", "startOffset": 27, "endOffset": 49}, {"referenceID": 16, "context": "0 (CPB) (with about 35,700 propositions) (Xue, 2008).", "startOffset": 41, "endOffset": 52}, {"referenceID": 13, "context": "To mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (Wang et al., 2015; Guo et al., 2016; Li et al., 2016).", "startOffset": 132, "endOffset": 186}, {"referenceID": 7, "context": "To mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (Wang et al., 2015; Guo et al., 2016; Li et al., 2016).", "startOffset": 132, "endOffset": 186}, {"referenceID": 8, "context": "To mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (Wang et al., 2015; Guo et al., 2016; Li et al., 2016).", "startOffset": 132, "endOffset": 186}, {"referenceID": 7, "context": ", 2015; Guo et al., 2016; Li et al., 2016). The heterogeneous resources introduced by these models include other semantically annotated corpora with annotation schema different to that used in PropBank, and even of a different language. The challenge here lies in the fact that those newly introduced resources are heterogeneous in nature, without sharing the same tagging schema, semantic role set, syntactic tag set and domain. For example, Wang et al. (2015) introduced a heterogeneous dataset, Chinese NetBank, by pretraining word embeddings.", "startOffset": 8, "endOffset": 462}, {"referenceID": 20, "context": "Chinese NetBank (Yulin, 2007) is also a corpus annotated with semantic roles, but using a very different roleset and annotation schema.", "startOffset": 16, "endOffset": 29}, {"referenceID": 18, "context": "Chinese NetBank (Yulin, 2007) is also a corpus annotated with semantic roles, but using a very different roleset and annotation schema. Wang\u2019s method can inherit knowledge acquired from other resources conveniently, but only at word representation level, missing more generalized semantic meanings in higher hidden layers. Li (2016) proposed a twopass training approach to use corpora of two languages, but a few non-common roles are ignored in the first pass.", "startOffset": 17, "endOffset": 333}, {"referenceID": 7, "context": "Guo et al. (2016) proposed a unified neural network model for SRL and relation classification (RC).", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "Guo et al. (2016) proposed a unified neural network model for SRL and relation classification (RC). It can learn two tasks at the same time, but cannot filter out harmful features learned in incompatible tasks. Recently, Progressive Neural Networks (PNN) model was proposed by Rusu et al. (2016) to transfer learned reinforcement learning policies from one game to another, or from simulation to the real robot.", "startOffset": 0, "endOffset": 296}, {"referenceID": 12, "context": "For example, in CPB, an agent of a verb is often marked as its Arg0, but not all Arg0 are agents, according to Chinese verb formation theory (Sun et al., 2009).", "startOffset": 141, "endOffset": 159}, {"referenceID": 15, "context": "Other Corpora for Chinese SRL Other popular semantic role labeling corpora include Chinese NomBank (Xue, 2006), Peking University Chinese NetBank (Yulin, 2007).", "startOffset": 99, "endOffset": 110}, {"referenceID": 20, "context": "Other Corpora for Chinese SRL Other popular semantic role labeling corpora include Chinese NomBank (Xue, 2006), Peking University Chinese NetBank (Yulin, 2007).", "startOffset": 146, "endOffset": 159}, {"referenceID": 21, "context": "Peking University Chinese NetBank was created by adding a semantic layer to Peking University Chinese TreeBank (Zhou et al., 1997).", "startOffset": 111, "endOffset": 130}, {"referenceID": 17, "context": "0 dataset (Xue and Palmer, 2003), with the new corpus, CSB, because there are huge differences between them, as we discussed in Section 2.", "startOffset": 10, "endOffset": 32}, {"referenceID": 1, "context": "However, it is well known that adding data in very different genre to training data may hurts parser performance (Bikel, 2004).", "startOffset": 113, "endOffset": 126}, {"referenceID": 4, "context": "Different from English, previous works (Ding and Chang, 2009; Sun et al., 2009) on Chinese SRL task often use both correct segmentation and part-of-speech tagging, and even treebank gold-standard parses (Xue, 2008) as their features.", "startOffset": 39, "endOffset": 79}, {"referenceID": 12, "context": "Different from English, previous works (Ding and Chang, 2009; Sun et al., 2009) on Chinese SRL task often use both correct segmentation and part-of-speech tagging, and even treebank gold-standard parses (Xue, 2008) as their features.", "startOffset": 39, "endOffset": 79}, {"referenceID": 16, "context": ", 2009) on Chinese SRL task often use both correct segmentation and part-of-speech tagging, and even treebank gold-standard parses (Xue, 2008) as their features.", "startOffset": 131, "endOffset": 142}, {"referenceID": 13, "context": "Some previous techniques, such as finetuning after pretraining (Wang et al., 2015; Li et al., 2016) and multi-task learning (Guo et al.", "startOffset": 63, "endOffset": 99}, {"referenceID": 8, "context": "Some previous techniques, such as finetuning after pretraining (Wang et al., 2015; Li et al., 2016) and multi-task learning (Guo et al.", "startOffset": 63, "endOffset": 99}, {"referenceID": 7, "context": ", 2016) and multi-task learning (Guo et al., 2016), have been used to deal with these challenges.", "startOffset": 32, "endOffset": 50}, {"referenceID": 9, "context": "Our model is mainly inspired by Rusu et al. (2016). They proposed progressive neural networks for a wide variety of reinforcement learning tasks (e.", "startOffset": 32, "endOffset": 51}, {"referenceID": 9, "context": "Our model is mainly inspired by Rusu et al. (2016). They proposed progressive neural networks for a wide variety of reinforcement learning tasks (e.g. Atari games and robot simulation). In their cases, inputs are pixels, outputs are learned policies, and each column, consisting of simple layers and convolutional layers, is trained to solve a particular Markov Decision Process. But in our case, inputs are sentences annotated using different syntactic tagsets and outputs are semantic role sequences. So we change the structure of columns to recurrent neural networks with LSTM, similar to the model proposed by Wang et al. (2015). Below we first introduce basic progressive neural network architecture, then describe our model, PNN with gated recurrent adapters.", "startOffset": 32, "endOffset": 633}, {"referenceID": 13, "context": "First, each column in the PNN architecture is a stacked bidirectional LSTM RNN, rather than convolutional neural networks, because inputs are sentences not pixels, and bi-LSTM RNN has proved powerful for Chinese SRL (Wang et al., 2015).", "startOffset": 216, "endOffset": 235}, {"referenceID": 6, "context": "The input gate and the forget gate can be coupled as a uniform gate, that is gi = 1\u2212gf to alleviate the problem of information redundancy and reduce the possibility of overfitting (Greff et al., 2015).", "startOffset": 180, "endOffset": 200}, {"referenceID": 13, "context": "We adopt the sentence tagging approach as Wang et al. (2015) did, because words in a sentence may closely related with each other, independently labeling each word is inappropriate.", "startOffset": 42, "endOffset": 61}, {"referenceID": 13, "context": "Previous work found that using pretrained word embeddings can improve performance (Wang et al., 2015) on Chinese SRL.", "startOffset": 82, "endOffset": 101}, {"referenceID": 16, "context": "We follow the same data setting as previous work (Xue, 2008; Sun et al., 2009), which divided CPB dataset2 into three parts: 648 files, from chtb_081.", "startOffset": 49, "endOffset": 78}, {"referenceID": 12, "context": "We follow the same data setting as previous work (Xue, 2008; Sun et al., 2009), which divided CPB dataset2 into three parts: 648 files, from chtb_081.", "startOffset": 49, "endOffset": 78}, {"referenceID": 3, "context": "Collobert and Weston only conducted their experiments on English corpus, but we notice that their approach has been implemented and tested on CPB by Wang et al. (2015), so we also put their result here for comparison.", "startOffset": 0, "endOffset": 168}, {"referenceID": 3, "context": "Collobert and Weston only conducted their experiments on English corpus, but we notice that their approach has been implemented and tested on CPB by Wang et al. (2015), so we also put their result here for comparison. We can make several observations from these results. Our approach significantly outperforms Sha et al. (2016) by a large margin (Wilcoxon Signed Rank Test, p < 0.", "startOffset": 0, "endOffset": 328}, {"referenceID": 18, "context": "After CPB was built by Xue and Palmer (2003), more complete and systematic research on Chinese SRL were done (Xue and Palmer, 2005; Chen et al., 2006; Ding and Chang, 2009; Yang et al., 2014).", "startOffset": 109, "endOffset": 191}, {"referenceID": 2, "context": "After CPB was built by Xue and Palmer (2003), more complete and systematic research on Chinese SRL were done (Xue and Palmer, 2005; Chen et al., 2006; Ding and Chang, 2009; Yang et al., 2014).", "startOffset": 109, "endOffset": 191}, {"referenceID": 4, "context": "After CPB was built by Xue and Palmer (2003), more complete and systematic research on Chinese SRL were done (Xue and Palmer, 2005; Chen et al., 2006; Ding and Chang, 2009; Yang et al., 2014).", "startOffset": 109, "endOffset": 191}, {"referenceID": 3, "context": "The concept of Semantic Role Labeling is first proposed by Gildea and Jurafsky(2002). Previous work on Chinese SRL mainly focused on how to improve SRL on single corpus.", "startOffset": 59, "endOffset": 85}, {"referenceID": 3, "context": "The concept of Semantic Role Labeling is first proposed by Gildea and Jurafsky(2002). Previous work on Chinese SRL mainly focused on how to improve SRL on single corpus. Approaches falls into two categories: feature-based machine learning approaches and neural-network-based approaches. Using feature-based method, Sun and Jurafsky (2004) did the preliminary work and achieved promising results without using any large annotated corpus.", "startOffset": 59, "endOffset": 339}, {"referenceID": 3, "context": "The concept of Semantic Role Labeling is first proposed by Gildea and Jurafsky(2002). Previous work on Chinese SRL mainly focused on how to improve SRL on single corpus. Approaches falls into two categories: feature-based machine learning approaches and neural-network-based approaches. Using feature-based method, Sun and Jurafsky (2004) did the preliminary work and achieved promising results without using any large annotated corpus. After CPB was built by Xue and Palmer (2003), more complete and systematic research on Chinese SRL were done (Xue and Palmer, 2005; Chen et al.", "startOffset": 59, "endOffset": 482}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.", "startOffset": 3, "endOffset": 31}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.05 Ding and Chang (2009) CRF 72.", "startOffset": 3, "endOffset": 63}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.05 Ding and Chang (2009) CRF 72.64 Yang et al. (2014) Multi-Predicate 75.", "startOffset": 3, "endOffset": 92}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.05 Ding and Chang (2009) CRF 72.64 Yang et al. (2014) Multi-Predicate 75.31 Wang et al. (2015) bi-LSTM 77.", "startOffset": 3, "endOffset": 133}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.05 Ding and Chang (2009) CRF 72.64 Yang et al. (2014) Multi-Predicate 75.31 Wang et al. (2015) bi-LSTM 77.09 (+0.00) Sha et al. (2016) bi-LSTM+QOM 77.", "startOffset": 3, "endOffset": 173}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.05 Ding and Chang (2009) CRF 72.64 Yang et al. (2014) Multi-Predicate 75.31 Wang et al. (2015) bi-LSTM 77.09 (+0.00) Sha et al. (2016) bi-LSTM+QOM 77.69 With external language resources Wang et al. (2015) +Gigaword embedding 77.", "startOffset": 3, "endOffset": 243}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.05 Ding and Chang (2009) CRF 72.64 Yang et al. (2014) Multi-Predicate 75.31 Wang et al. (2015) bi-LSTM 77.09 (+0.00) Sha et al. (2016) bi-LSTM+QOM 77.69 With external language resources Wang et al. (2015) +Gigaword embedding 77.21 Wang et al. (2015) +NetBank embedding 77.", "startOffset": 3, "endOffset": 288}, {"referenceID": 3, "context": "90 Collobert and Weston (2008) MTL 74.05 Ding and Chang (2009) CRF 72.64 Yang et al. (2014) Multi-Predicate 75.31 Wang et al. (2015) bi-LSTM 77.09 (+0.00) Sha et al. (2016) bi-LSTM+QOM 77.69 With external language resources Wang et al. (2015) +Gigaword embedding 77.21 Wang et al. (2015) +NetBank embedding 77.59 Guo et al. (2016) +Relataion Classification 75.", "startOffset": 3, "endOffset": 331}, {"referenceID": 13, "context": "For Chinese SRL, Wang et al. (2015) proposed bidirectional a LSTM RNN model.", "startOffset": 17, "endOffset": 36}, {"referenceID": 13, "context": "For Chinese SRL, Wang et al. (2015) proposed bidirectional a LSTM RNN model. And based on their work, Sha (2016) proposed quadratic optimization method as a postprocessing module and further improved the result.", "startOffset": 17, "endOffset": 113}, {"referenceID": 12, "context": "Wang et al. (2015) introduced heterogeneous data by using pretrained embeddings at initialization and achieved promising results.", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "Guo et al. (2016) proposed a multitask learning method with a unified neural network model to learn SRL and relation classification task together and also achieved improvement.", "startOffset": 0, "endOffset": 18}], "year": 2017, "abstractText": "Previous studies on Chinese semantic role labeling (SRL) have concentrated on single semantically annotated corpus. But the training data of single corpus is often limited. Meanwhile, there usually exists other semantically annotated corpora for Chinese SRL scattered across different annotation frameworks. Data sparsity remains a bottleneck. This situation calls for larger training datasets, or effective approaches which can take advantage of highly heterogeneous data. In these papers, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together. We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and effectively transfer knowledge between them. We also release a new corpus, Chinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that ours model outperforms state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}