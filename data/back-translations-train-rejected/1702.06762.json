{"id": "1702.06762", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Style Transfer Generative Adversarial Networks: Learning to Play Chess Differently", "abstract": "The idea of style transfer has largely only been explored in image-based tasks, which we attribute in part to the specific nature of loss functions used for style transfer. We propose a general formulation of style transfer as an extension of generative adversarial networks, by using a discriminator to regularize a generator with an otherwise separate loss function. We apply our approach to the task of learning to play chess in the style of a specific player, and present empirical evidence for the viability of our approach.", "histories": [["v1", "Wed, 22 Feb 2017 11:43:50 GMT  (262kb,D)", "http://arxiv.org/abs/1702.06762v1", "style transfer, Generative Adversarial Networks"], ["v2", "Sun, 7 May 2017 13:42:01 GMT  (712kb,D)", "http://arxiv.org/abs/1702.06762v2", "style transfer, Generative Adversarial Networks"]], "COMMENTS": "style transfer, Generative Adversarial Networks", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["muthuraman chidambaram", "yanjun qi"], "accepted": false, "id": "1702.06762"}, "pdf": {"name": "1702.06762.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["CHESS DIFFERENTLY", "Muthuraman Chidambaram", "Yanjun Qi"], "emails": ["mc4xf@virginia.edu", "yanjun@virginia.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Gatys et al. (2015) demonstrated that a revolutionary neural network model (CNN) could be trained to transfer the unique styles present in human art to other images. However, the style transfer loss used in their work, as well as the losses used in the follow-up work by Ulayanov et al. (2016) and Johnson et al. (2016), were specific to image-based tasks. This makes it difficult to extend their work on style transfers to other tasks where unique human styles exist, such as chess. Motivated by this problem, we present a general framework for style transfers, which we call style transfer generative opposing networks (STGANs), as an extension of generative opposing networks (GANs) described by Goodfellow et al. (2014). Our proposed framework consists of a G generator learning to perform a specific task, and a D discriminator learning to predict whether the same task was performed in a particular style."}, {"heading": "2 STGAN MODEL", "text": "The main difference between our proposed STGAN model and the GAN model is that the generator loss in our model is not structured purely in relation to the discriminator. Instead, the generator loss is defined as being specific to the given task, which we consider to be generating optimal checkerboard evaluations in this work. We then define a style transfer generator loss by regulating the original generator loss with the discriminator."}, {"heading": "2.1 GENERATOR", "text": "We have structured our generator to resemble the deep pink model described by Bernhardsson, and G is a random function (G > G).The generator G is thus a fully connected forward-facing neural network with a 768-x input layer, two 2048-wide hidden layers with ReLU activations and a single linear output unit (the generator takes as input a chessboard (G) that is represented as a 768 element vector corresponding to the positions of the 12 different chess pieces, and gives a real number as an evaluation. Positive Xiv: 170 2.06 762v 1 [cs.L] 22 Feb 2017 Evaluations mean that the board is in favor of white, while negative evaluations mean that the board is in favor of black. We train the generator with triplets of chess boards (xG, yG, rG) taken from games played by top chess players."}, {"heading": "2.2 DISCRIMINATOR", "text": "The discriminator learning a function D is constructed identically to the generator, except for a 1536-unit input layer and a sigmoid output; the discriminator inputs a valid move, which is presented as a concatenation of the vector representations of a board pair, and outputs the probability that the move was played by a particular player; the training is performed using pairs of consecutive boards (xD, yD) taken from a particular player's games, as well as fake move pairs (xD, M (xD)), which are generated by selecting moves with generator G. The selection of the board M (xD) is performed using the Negamax search described by Campbell & Marsland (1983) with a search depth of one and the generator as a board rating function; the discriminator is optimized by maximizing D (xD, yD) and minimizing D (xD, M (xD)), which matches the minimization of the following J (D) discriminator."}, {"heading": "2.3 STYLE TRANSFER", "text": "Style transfer occurs by using the discriminator to regulate the generator. This is achieved by defining a style transfer loss of the generator J (G) ST (\u03b8G) as follows: J (G) ST (\u03b8G) = J (G) (\u03b8G) \u2212 1mm \u2211 i = 1kD (((x (i) D, M (x (i) D))))) (3) Where k is a hyperparameter that controls the extent of influence that the style designated by the discriminator should have on the generator. Since certain boards x (i) G may not be displayed in the training data of the discriminator, we select the initial boards x (i) D. for the regularization term."}, {"heading": "2.4 TRAINING", "text": "The discriminator and the generator are updated simultaneously by gradient drop to J (D) and J (G) (ST), but the discriminator is updated 5 times for each generator update, as described by Arjovsky et al. (2017) in the WGAN publication. Examples from the most recent discriminator training are used for regularization in each generator update, and the weights of the discriminator are adjusted to be in the range [\u2212 0.01, 0.01], which is in line with the WGAN approach."}, {"heading": "3 RESULTS", "text": "The training data for the generator was obtained by extracting all standard chess games played in 2016 between players with ratings over 2000 from the FICS game database. For the discriminator, we chose to predict the style of the late chess grandmaster Mikhail Tal and extracted his 2431 available games from PGN Mentor as training data. We trained several generator networks with different values of regularization parameter k, treating k = 0 as a baseline. Due to the cost of performing a Negamax search for each turn generated during training, all networks were trained for 10 epochs, with only 100 lots of size 64 being sampled from the training data of each epoch. After training, each network was tested by generating moves (again with a Negamax search with depth one) on boards taken from valleys. Figure 2 shows the moves selected by the generator networks for a given input board, as well as an actual move played by the valley."}], "references": [{"title": "Deep learning for.", "author": ["Erik Bernhardsson"], "venue": "URL https://erikbern.com/2014/ 11/29/deep-learning-for-chess/", "citeRegEx": "Bernhardsson.,? \\Q2014\\E", "shortCiteRegEx": "Bernhardsson.", "year": 2014}, {"title": "A comparison of minimax tree search algorithms", "author": ["Martin S. Campbell", "T.A. Marsland"], "venue": "Artificial Intelligence,", "citeRegEx": "Campbell and Marsland.,? \\Q1983\\E", "shortCiteRegEx": "Campbell and Marsland.", "year": 1983}, {"title": "A neural algorithm of artistic style", "author": ["Leon A. Gatys", "Alexander S. Ecker", "Matthias Bethge"], "venue": null, "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Big Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "Advances in Neural Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Perceptual losses for real-time style transfer and super-resolution", "author": ["Justin Johnson", "Alexandre Alahi", "Fei-Fei Li"], "venue": "European Conference on Computer Vision,", "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Texture networks: Feedforward synthesis of textures and stylized images", "author": ["Dmitry Ulayanov", "Vadim Lebedev", "Andrea Vedaldi", "Victor Lempitsky"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ulayanov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ulayanov et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "1 GENERATOR We structured our generator to be similar to the Deep Pink model described by Bernhardsson (2014). The generator G is thus a fully connected feedforward neural network with a 768 unit-wide input layer, two 2048 unit-wide hidden layers with ReLU activations, and a single linear output unit.", "startOffset": 90, "endOffset": 110}], "year": 2017, "abstractText": "The idea of style transfer has largely only been explored in image-based tasks, which we attribute in part to the specific nature of loss functions used for style transfer. We propose a general formulation of style transfer as an extension of generative adversarial networks, by using a discriminator to regularize a generator with an otherwise separate loss function. We apply our approach to the task of learning to play chess in the style of a specific player, and present empirical evidence for the viability of our approach.", "creator": "LaTeX with hyperref package"}}}