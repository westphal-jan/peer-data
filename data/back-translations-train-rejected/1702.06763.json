{"id": "1702.06763", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples", "abstract": "Recent studies have shown that deep neural networks (DNN) are vulnerable to adversarial samples: maliciously-perturbed samples crafted to yield incorrect model outputs. Such attacks can severely undermine DNN systems, particularly in security-sensitive settings. It was observed that an adversary could easily generate adversarial samples by making a small perturbation on irrelevant feature dimensions that are unnecessary for the current classification task. To overcome this problem, we introduce a defensive mechanism called DeepMask. By identifying and removing unnecessary features in a DNN model, DeepMask limits the capacity an attacker can use generating adversarial samples and therefore increase the robustness against such inputs. Comparing with other defensive approaches, DeepMask is easy to implement and computationally efficient. Experimental results show that DeepMask can increase the performance of state-of-the-art DNN models against adversarial samples.", "histories": [["v1", "Wed, 22 Feb 2017 11:48:35 GMT  (209kb,D)", "http://arxiv.org/abs/1702.06763v1", "adversarial samples, deep neural network"], ["v2", "Wed, 1 Mar 2017 05:55:00 GMT  (216kb,D)", "http://arxiv.org/abs/1702.06763v2", "adversarial samples, deep neural network"], ["v3", "Thu, 2 Mar 2017 16:42:12 GMT  (216kb,D)", "http://arxiv.org/abs/1702.06763v3", "adversarial samples, deep neural network"], ["v4", "Wed, 8 Mar 2017 17:18:19 GMT  (289kb,D)", "http://arxiv.org/abs/1702.06763v4", "adversarial samples, deep neural network"], ["v5", "Fri, 10 Mar 2017 00:02:04 GMT  (329kb,D)", "http://arxiv.org/abs/1702.06763v5", "adversarial samples, deep neural network"], ["v6", "Tue, 21 Mar 2017 16:26:33 GMT  (388kb,D)", "http://arxiv.org/abs/1702.06763v6", "adversarial samples, deep neural network"], ["v7", "Sat, 25 Mar 2017 22:16:05 GMT  (394kb,D)", "http://arxiv.org/abs/1702.06763v7", "adversarial samples, deep neural network"], ["v8", "Mon, 17 Apr 2017 21:54:30 GMT  (395kb,D)", "http://arxiv.org/abs/1702.06763v8", "adversarial samples, deep neural network"]], "COMMENTS": "adversarial samples, deep neural network", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CR", "authors": ["ji gao", "beilun wang", "zeming lin", "weilin xu", "yanjun qi"], "accepted": false, "id": "1702.06763"}, "pdf": {"name": "1702.06763.pdf", "metadata": {"source": "CRF", "title": "DEEPMASK: MASKING DNN MODELS FOR ROBUST- NESS AGAINST ADVERSARIAL SAMPLES", "authors": ["Ji Gao", "Beilun Wang", "Yanjun Qi"], "emails": ["jg6yd@virginia.edu", "bw4mw@virginia.edu", "yanjun@virginia.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep Neural Networks (DNNs) have achieved great success in a wide range of applications."}, {"heading": "2 BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 ADVERSARIAL SAMPLES", "text": "For the following analysis, we refer to a basic DNN classifier as F (\u00b7): X \u2192 Y. X stands for input space. Y stands for output room. x \u0445 X stands for a sample. Countersamples are deliberately generated samples. We define an opposing sample as: x \u2032 = x + \u2206 x, | \u2206 x | < F (x) 6 = F (x \u2032) (2.1) \u0445 x must be small so that x and x \u2032 are imperceptible to the human eye for image classification. However, for the machine classifier F, it classifies x and x \u2032 into two different classes. Therefore, such a sample x \u2032 can successfully deceive a machine classifier."}, {"heading": "2.2 ADVERSARIAL SAMPLES AND IRRELEVANT FEATURES", "text": "Wang et al. (2016) show that the effectiveness of opposing samples is related to additional irrelevant characteristics extracted by the machine classifier. If a classifier extracts many irrelevant characteristics in the training process, it is susceptible to enemy attacks. An example of this vulnerability is shown in the appendix section 6.2. To solve this problem, we then propose an approach that can reduce the number of irrelevant characteristics. Figure 6.2 illustrates the basic motivation that the distance between an opposing sample and its seed example will be small along the relevant characteristic dimension (e.g. barely visible to human eyes) and relatively large in terms of irrelevant dimensions (i.e. misclassification) for the current task. DNN combines feature extraction and classification in one model. Therefore, we propose to remove irrelevant characteristics for a DNN model by directly changing its network structure."}, {"heading": "3 METHOD: DEEPMASK", "text": "The basic idea is to remove these irrelevant traits used by opposing samples. To determine which trait is irrelevant, we test pairs of opposing samples x \u00b2 and its normal seed x and compare the difference between the extracted traits in DNN \u2032. These characteristics, which change rapidly, are used by the opponent and should therefore be removed to improve the robustness of the model. To remove these traits, we insert a mask layer in the DNN model, just before the linear layer classification. The proposed structure is shown in Figure 1. Input of the mask layer is extracted by layers of the DNN model. The weight of the mask layer is either 0 \u2212 or 1. Elemental multiplication occurs in the mask layer, so the output is either the input function or 0. Characteristics are sorted according to their sensitivity to opposing samples: The sensitivity of the mask layer is measured by layers of the N model extracting the mask layer, so the output is either the input function or 0."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 EXPERIMENT SETTING", "text": "\u2022 Dataset: We choose CIFAR-10 (Krizhevsky & Hinton, 2009) as the dataset of our experiment. CIFAR-10 is a tiny dataset with 50,000 32x32 training images and 10,000 test images. \u2022 We choose a residual network with 164 layers (He et al., 2016) as our DNN target model. The model is pre-trained and achieves state-of-the-art performance on corresponding datasets. \u2022 Metric: We generate contrasting samples for each sample in the test set and test all contrasting samples on each DNN model. Accuracy on the contrasting test set is given as \"contrasting accuracy\" to measure the contrasting robustness of a DNN model."}, {"heading": "4.2 EXPERIMENT RESULT", "text": "In general, DeepMask can reduce the effectiveness of such enemy samples by 10%. Enemy disturbance is generated using the method of fast gradient marks (Appendix section 6.1). Table 1 also shows that only a small percentage of characteristics are important for enemy samples. In the illustration, masking 1% of the characteristics can increase enemy performance by 5%. Therefore, we need to remove only a small percent of the characteristics to greatly improve enemy robustness, and the model still maintains high accuracy in test samples."}, {"heading": "5 CONCLUSION", "text": "In this study, we present DeepMask, a simple and cost-effective strategy to reduce the effectiveness of opposing samples on DNN classifiers. Many other possible strategies can be designed, for example by adding masks that take into account the difference between output labels."}, {"heading": "6 APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 FAST GRADIENT SIGN ALGORITHM", "text": "The Fast Gradient Sign Algorithm proposed by Goodfellow et al. (2014) is an efficient algorithm for generating contrary samples. The perturbation is calculated by the following equation: \u2206 x = character (\u2206 zLoss (F (z), F (x))) (6.1) This is motivated by the control of the L \u00b0 standard of \u0445 x, which results in the magnitude of the perturbation being equal in each characteristic dimension. To maximize the difference between x and x while limiting the L \u00b0 standard of \u0445 x, one must follow the gradient direction to x in each dimension, which is exactly a sign of the gradient function."}, {"heading": "6.2 EXAMPLES OF AN IRRELEVANT FEATURE CAN BE UTILIZED BY ADVERSARIAL SAMPLES", "text": "Figure 6.2 shows a situation where a linear classification model is confronted with irrelevant features. In this case, human annotators use only one feature to perform the separation. However, since the machine classifier uses an additional feature, the attacker can move the original sample (blue cross) along this additional feature dimension (y-axis) to produce an opposing sample (green cross) that is misclassified by the classification model."}], "references": [{"title": "Large-scale malware classification using random projections and neural networks", "author": ["George E Dahl", "Jack W Stokes", "Li Deng", "Dong Yu"], "venue": "In ICASSP,", "citeRegEx": "Dahl et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2013}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Technique report, University of Toronto,", "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "The limitations of deep learning in adversarial settings", "author": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami"], "venue": "arXiv preprint arXiv:1511.07528,", "citeRegEx": "Papernot et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2015}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami"], "venue": "In Security and Privacy (SP),", "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "A theoretical framework for robustness of (deep) classifiers under adversarial noise", "author": ["Beilun Wang", "Ji Gao", "Yanjun Qi"], "venue": "arXiv preprint arXiv:1612.00334,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013).", "startOffset": 101, "endOffset": 149}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al.", "startOffset": 131, "endOffset": 699}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al.", "startOffset": 131, "endOffset": 723}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al.", "startOffset": 131, "endOffset": 749}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al.", "startOffset": 131, "endOffset": 769}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al. (2016b;a). Different algorithms for generating adversarial samples have been invented. Szegedy et al. (2013) proposed an algorithm to generate adversarial samples using Box L-BFGS method.", "startOffset": 131, "endOffset": 895}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al. (2016b;a). Different algorithms for generating adversarial samples have been invented. Szegedy et al. (2013) proposed an algorithm to generate adversarial samples using Box L-BFGS method. It also showed that same adversarial sample could be transferred to fool different DNN classifiers. Goodfellow et al. (2014) purposed fast gradient sign method, which maximizes the consequence of the attack under limited size of L\u221e-norm.", "startOffset": 131, "endOffset": 1099}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al. (2016b;a). Different algorithms for generating adversarial samples have been invented. Szegedy et al. (2013) proposed an algorithm to generate adversarial samples using Box L-BFGS method. It also showed that same adversarial sample could be transferred to fool different DNN classifiers. Goodfellow et al. (2014) purposed fast gradient sign method, which maximizes the consequence of the attack under limited size of L\u221e-norm. Papernot et al. (2015) purposed another algorithm that generates adversarial samples following the saliency value.", "startOffset": 131, "endOffset": 1235}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al. (2016b;a). Different algorithms for generating adversarial samples have been invented. Szegedy et al. (2013) proposed an algorithm to generate adversarial samples using Box L-BFGS method. It also showed that same adversarial sample could be transferred to fool different DNN classifiers. Goodfellow et al. (2014) purposed fast gradient sign method, which maximizes the consequence of the attack under limited size of L\u221e-norm. Papernot et al. (2015) purposed another algorithm that generates adversarial samples following the saliency value. A recent paper Papernot et al. (2016a) proposed a black-box attack, which first approximates the target classifier and then generate adversarial samples to the approximated model.", "startOffset": 131, "endOffset": 1366}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al. (2016b;a). Different algorithms for generating adversarial samples have been invented. Szegedy et al. (2013) proposed an algorithm to generate adversarial samples using Box L-BFGS method. It also showed that same adversarial sample could be transferred to fool different DNN classifiers. Goodfellow et al. (2014) purposed fast gradient sign method, which maximizes the consequence of the attack under limited size of L\u221e-norm. Papernot et al. (2015) purposed another algorithm that generates adversarial samples following the saliency value. A recent paper Papernot et al. (2016a) proposed a black-box attack, which first approximates the target classifier and then generate adversarial samples to the approximated model. Researchers also studied how to defend attacks enforced by adversarial samples. Goodfellow et al. (2014) showed retraining the model using adversarial samples can improve the adversarial robustness of the model.", "startOffset": 131, "endOffset": 1612}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al. (2016b;a). Different algorithms for generating adversarial samples have been invented. Szegedy et al. (2013) proposed an algorithm to generate adversarial samples using Box L-BFGS method. It also showed that same adversarial sample could be transferred to fool different DNN classifiers. Goodfellow et al. (2014) purposed fast gradient sign method, which maximizes the consequence of the attack under limited size of L\u221e-norm. Papernot et al. (2015) purposed another algorithm that generates adversarial samples following the saliency value. A recent paper Papernot et al. (2016a) proposed a black-box attack, which first approximates the target classifier and then generate adversarial samples to the approximated model. Researchers also studied how to defend attacks enforced by adversarial samples. Goodfellow et al. (2014) showed retraining the model using adversarial samples can improve the adversarial robustness of the model. Papernot et al. (2016b) proposed defensive distillation to make the model less sensible to gradient change.", "startOffset": 131, "endOffset": 1743}, {"referenceID": 0, "context": "Classifiers based on DNN models have attained great performance on multiple security-sensitive tasks (Microsoft Corporation, 2015; Dahl et al., 2013). However, recent studies show that machine learning classifiers are vulnerable to deliberate attacks. Attackers can easily generate a malicious sample by adding a small perturbation on a normal sample. Then the malicious sample can fool the classifier and force it to yield a wrong output. Such sample is called an adversarial sample. This paper focuses on finding a defensive approach that can make DNN models perform more reliable in face of adversarial samples. Many recent studies focused on adversarial samples, including Szegedy et al. (2013); Papernot et al. (2015); Goodfellow et al. (2014); Wang et al. (2016); Papernot et al. (2016b;a). Different algorithms for generating adversarial samples have been invented. Szegedy et al. (2013) proposed an algorithm to generate adversarial samples using Box L-BFGS method. It also showed that same adversarial sample could be transferred to fool different DNN classifiers. Goodfellow et al. (2014) purposed fast gradient sign method, which maximizes the consequence of the attack under limited size of L\u221e-norm. Papernot et al. (2015) purposed another algorithm that generates adversarial samples following the saliency value. A recent paper Papernot et al. (2016a) proposed a black-box attack, which first approximates the target classifier and then generate adversarial samples to the approximated model. Researchers also studied how to defend attacks enforced by adversarial samples. Goodfellow et al. (2014) showed retraining the model using adversarial samples can improve the adversarial robustness of the model. Papernot et al. (2016b) proposed defensive distillation to make the model less sensible to gradient change. Wang et al. (2016) showed that one cause of adversarial sample is those redundant features that are irrelevant to classification.", "startOffset": 131, "endOffset": 1846}, {"referenceID": 2, "context": "\u2022 We choose a Residual network with 164 layers(He et al., 2016) as our target DNN model.", "startOffset": 46, "endOffset": 63}], "year": 2017, "abstractText": "Recent studies have shown that deep neural networks (DNN) are vulnerable to adversarial samples: maliciously-perturbed samples crafted to yield incorrect model outputs. Such attacks can severely undermine DNN systems, particularly in security-sensitive settings. It was observed that an adversary could easily generate adversarial samples by making a small perturbation on irrelevant feature dimensions that are unnecessary for the current classification task. To overcome this problem, we introduce a defensive mechanism called DeepMask. By identifying and removing unnecessary features in a DNN model, DeepMask limits the capacity an attacker can use generating adversarial samples and therefore increase the robustness against such inputs. Comparing with other defensive approaches, DeepMask is easy to implement and computationally efficient. Experimental results show that DeepMask can increase the performance of state-of-the-art DNN models against adversarial samples.", "creator": "LaTeX with hyperref package"}}}