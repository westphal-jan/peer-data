{"id": "1702.06776", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Causal Inference by Stochastic Complexity", "abstract": "The algorithmic Markov condition states that the most likely causal direction between two random variables X and Y can be identified as that direction with the lowest Kolmogorov complexity. Due to the halting problem, however, this notion is not computable.", "histories": [["v1", "Wed, 22 Feb 2017 12:36:21 GMT  (1032kb,D)", "http://arxiv.org/abs/1702.06776v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["kailash budhathoki", "jilles vreeken"], "accepted": false, "id": "1702.06776"}, "pdf": {"name": "1702.06776.pdf", "metadata": {"source": "META", "title": "Causal Inference by Stochastic Complexity", "authors": ["KAILASH BUDHATHOKI", "JILLES VREEKEN"], "emails": [], "sections": [{"heading": null, "text": "Causal Inference by Stochastic ComplexityKAILASH BUDHATHOKI, Max Planck Institute for Informatics and Saarland University, GermanyJILLES VREEKEN, Max Planck Institute for Informatics and Saarland University, Germanye algorithmic Markov condition states that the most likely causal direction between two random variables X and Y can be identified as this direction with the lowest Kolmogorov complexity. However, due to the maintenance problem, this term is not predictable. We therefore propose to make causal conclusions by stochastic complexity. At, we suggest approximating the complexity of Kolmogorov by using the Minimum Description Length (MDL) principle by using a score that is optimal in terms of the model class taking into account."}, {"heading": "1 INTRODUCTION", "text": "In fact, it is such that most of them will be able to move to another world, in which they will be able to move to another world, in which they will be able to move to another world, in which they will be able to move, in which they will move, in which they will move, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which will live, in which they will live, in which they will live, in which they will live, in which will live, in which they will live, in which they will live, in which they will live, in which will live, in which they will live, in"}, {"heading": "2 PRELIMINARIES", "text": "In this section, we present notations and background information that we will use in the following sections."}, {"heading": "2.1 Kolmogorov Complexity", "text": "e Kolmogorov's complexity of a nite binary string x is the length of the shortest binary program p \u00b2 for a universal Turing machineU that generates x and then stops [10, 11]. Formally, we have the shortest algorithmic description of x, and Kolmogorov's complexity of x is the length of its ultimate lossless compression. Conditional Kolmogorov's complexity, K (x | y) \u2264 K (x), is then the length of the shortest binary program p \u00b2 that generates x, and stops as y is given as input.The amount of algorithmic information contained in y via x is I (y: x) = K (y | x) \u2212 K et."}, {"heading": "3 CAUSAL INFERENCE BY COMPLEXITY", "text": "In view of two correlated variables X and Y, we are interested in deriving their causal relationship. In particular, we want to derive whether X causes Y, whether Y is only correlated, or whether they are only correlated. We assume that there is a causal distribution structure in which there is no confusing variable, i.e. that there is a hidden common cause for X and Y. We use X \u2192 Y to refer to X. We base our causal distribution method on the following postulate: Postulate 1 (independence of input and mechanism). If there is a marginal distribution of the cause for the cause, then it is a conditional distribution of the cause, P (Y and X) are independent."}, {"heading": "4 CAUSAL INFERENCE BY COMPRESSION", "text": "In this section, we discuss how stochastic complexity can be used for practical causal conclusions, and we move gradually toward this goal, starting with the MDL, covering the basics."}, {"heading": "4.1 Minimum Description Length Principle", "text": "e Minimum Description Length (MDL) [21] principle is a practice version of Kolmogorov complexity = universal (MDL). Instead of all possible programs, it considers only programs that we know to be x and last. at is, lossless compressors. In MDL theory, however, programs are called models. e MDL principle is rooted in the two-part decomposition of Kolmogorov complexity [11]. It can be roughly described as follows [3]. In view of a number of models M and data D, the best model M-M is the one that uses L (D) = L (M) + L (D | M) codes, where L (M) is the length, in bits, the description of the model, and L (D | M) is the length, in bits, the description of the data, in bits, the description of the data when encoded with the model M. M (M) intuitively stands for L (M) is the compressible part of the data."}, {"heading": "4.2 Stochastic Complexity", "text": "It is not possible that the data we provide for the distribution category Xn, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, S, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, S, M, S, M, S, M, S, M, S, M, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S,"}, {"heading": "4.3 Causal Inference by Stochastic Complexity", "text": "Unless otherwise stated, we write X for Xn and Y for Yn. e stochastic complexity of the data X > S = S corresponds to the complexity of the NML distribution of the data relative to M. This means that we can use the stochastic complexity of X as an approximation of the Kolmogorov complexity of P (X). As such, it provides a general, yet predictable, theoretically solid basis for causal inference based on algorithmic information theory. To facilitate notation, we write S (X) for S (X | M). To derive the causal direction from this, we consider total stochastic complexity in two directions - X for Y and vice versa. e total stochastic complexity of X for Y, with the approximation of K (P (X)) + K (P (Y | X))) denoting causal complexity as insufficient."}, {"heading": "4.4 Multinomial Stochastic Complexity", "text": "We are looking at random variable X with m values. Beyond that, we are assuming that our data Xn = (x1,.., xn) are multilaterally distributed. (xn). We are assuming that our data Xn = (x1,.., m) complex classMm is de ned asMm = (X,.,.). (xm,.). (xm,.). (xm,.). (xm,.). (xm,.). (xm,.). (xm,.). (xm,.). (xm,.). (xm,.). (xm,.). (xm,.). (xm,.). (xm,.). (xm,.). (xm,.). (xm,. (.). (xm,.). (xm,.). (. (xm,.). (xm.). (. (xm,.). (xm,.). (xm. (xm,.). (xm,.). (xm. (xm,.). (xm,. (xm,.). (xm,.). (xm. (xm,.). (xm,. (xm,.). (xm). (xm). (xm,. (xm,.). (xm). (xm,. (xm,.). (xm,. (xm,.). (xm.).). (xm.). (xm,. (xm,.). (. (xm,.). (xm.).). (xm,. (xm. (.). (xm,.). (xm,.). (xm,.). (xm,.). (xm.). (..... (. (. (.).. (. (.). (...... (.). (xm,..)."}, {"heading": "4.5 Computing Conditional Complexity", "text": "So far we have only discussed how to calculate the stochastic complexity of data under a model class. For our purpose we must also calculate the conditional stochastic complexity S (Y | X) and vice versa. Let S (Y | X = x) calculate the stochastic complexity of Y conditioned by X = x. en the conditional stochastic complexity S (Y | X) is the sum of S (Y | X = x) across all possible values of X. Let X calculate the domain of X. en given the stochastic complexity of Y X is regarded as S (Y | X) = x x. Calculation complexity - We can calculate S (Y | X = x) in O (n). To calculate the conditional stochastic complexity of Y (Y | X) we must calculate S (Y | X = x) across all x x."}, {"heading": "5 RELATEDWORK", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to embark on the path to the future."}, {"heading": "6 EXPERIMENTS", "text": "We have implemented cisc in Python and provide the source code for research purposes, together with the used datasets and the synthetic dataset generator.1 All experiments were done single-threaded on an Intel Xeon E5-2643 v3 machine with 256GB of memory under Linux. We look at synthetic, benchmark and real data. In particular, we note that cisc is parameter-free. We compare cisc with discrete regression (dr) [19] and DC [12]. Specifically, we use signal frequency levels of \u03b1 = 0.05 for the independence test in dr and threshold of B = 0.0 for DC."}, {"heading": "6.1 Synthetic Data", "text": "To generate the data with known basic truths, we look at synthetic data (Y-X) (Y-X) (X-X). The generation of non-trivial synthetic data with causal direction is not surprising to us, but we generate synthetic pairs of causes with the basic truth X \u2192 Y that use the additive noise model (ANM). [19] We try X from the following distributions, using the generated uniform noise independently. [19] We use uniform models from {1, X) + N, N, where it is a function, and N is additive noise, which is independent of X. [19] We try X from the following distributions, using uniform noise."}, {"heading": "6.2 Benchmark Data", "text": "Next, we evaluate cisc using benchmark cause-effect pairs with known ground truth [14]. In particular, we take 95 univariate cause-effect pairs. So far, there is no discretization strategy that demonstrably maintains the causal relationship between variables. Since each cause-effect pair comes from a specific domain, the use of a discretization strategy that is presented to the ACM for all pairs is also unfair. Furthermore, we do not know the underlying domain of the data. Consequently, we treat the data as discrete for all pairs. In Figure 4, we compare the accuracy of cisc against dc and dr at different decision rates along with the 95% consistency interval for a random coin ip. If we look at all pairs, we find that cisc corrects the direction in about 67% of all pairs. Let's consider only those pairs where cisc is most critical - with a very high value \u2192 S | Y (X), as close to the top pairs of both pairs."}, {"heading": "6.3 alitative Case Studies", "text": "Next, we evaluate cisc on real data for exploratory purposes. Manuscript submitted ACMAbalone - First, we consider the abalone data set repository.3 e data set contains the physical measurements of 4 177 abalons, which are large, edible marine snails. Of the nine measurements, we consider the sex (X), the length (Y1), the diameter (Y2) and the height of the heel (Y3). e Length, diameter and height of the heel are all measured in millimeters and have 70, 57 and 28 di erent values, with the sex of the abalone being nominal."}, {"heading": "7 DISCUSSION", "text": "It is not only a matter of time before there will be an agreement, but also whether and to what extent there will be an agreement. (...) It is a matter of time before there will be an agreement. (...) It is a matter of time before there will be an agreement. (...) It is a matter of time before there will be an agreement. (...) It is a matter of time before there will be an agreement. (...) It is a matter of time before there will be an agreement. (...) It is a matter of time before there will be an agreement. (...) It is a matter of time before there will be an agreement. (...) It is a matter of time until there will be an agreement. \"(...) It is a matter of time until there will be a solution. (...) It is a matter of time until there will be a solution. (...) It is a question of time until there will be a solution. (...) It is a question of time until there will be a solution."}, {"heading": "8 CONCLUSION", "text": "We examined causal conclusions from observational data. We proposed a general but predictable framework for information theory causal conclusions with guarantees of optimality. In particular, we proposed to make causal conclusions through stochastic complexity. To illustrate the strength of these conclusions, we proposed cisc for pairs of univariate discrete variables, applying stochastic complexity across the class of multinomial distributions. Extensive evaluations of synthetic, benchmark-based and real data showed that cisc is highly accurate, significantly exceeds the state of the art, and extremely well scaled in both sample and domain size."}, {"heading": "ACKNOWLEDGMENTS", "text": "Kailash Budhathoki is supported by the International Max Planck Research School for Computer Science. Both authors are supported by the Cluster of Excellence \"Multimodal Computing and Interaction\" within the Excellence Initiative of the Federal Government."}], "references": [{"title": "Causal Inference by Compression", "author": ["Kailash Budhathoki", "Jilles Vreeken"], "venue": "In ICDM", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Nonlinear Causal Discovery for High Dimensional Data: A Kernelized Trace Method", "author": ["Z. Chen", "K. Zhang", "L. Chan"], "venue": "In ICDM", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "\u008ae Minimum Description Length Principle", "author": ["Peter Gr\u00fcnwald"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Algorithmic Information \u008ceory", "author": ["Peter D. Gr\u00fcnwald", "Paul M.B. Vit\u00e1nyi"], "venue": "CoRR abs/0809.2754", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Nonlinear causal discovery with additive noise models", "author": ["PO. Hoyer", "D. Janzing", "JM. Mooij", "J. Peters", "B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Telling cause from e\u0082ect based on high-dimensional observations", "author": ["D. Janzing", "P. Hoyer", "B. Sch\u00f6lkopf"], "venue": "In ICML", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Information-geometric approach to inferring causal directions", "author": ["Dominik Janzing", "Joris Mooij", "Kun Zhang", "Jan Lemeire", "Jakob Zscheischler", "Povilas Daniu\u0161is", "Bastian Steudel", "Bernhard Sch\u00f6lkopf"], "venue": "AIJ", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Causal Inference Using the Algorithmic Markov Condition", "author": ["D. Janzing", "B. Sch\u00f6lkopf"], "venue": "IEEE TIT 56,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Justifying Additive Noise Model-Based Causal Discovery via Algorithmic Information \u008ceory", "author": ["D. Janzing", "B. Steudel"], "venue": "OSID 17,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "\u008cree Approaches to the \u008bantitative De\u0080nition of Information", "author": ["A.N. Kolmogorov"], "venue": "Problemy Peredachi Informatsii", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1965}, {"title": "An Introduction to Kolmogorov Complexity and its Applications", "author": ["M. Li", "P. Vit\u00e1nyi"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Causal Inference on Discrete Data via Estimating Distance Correlations", "author": ["Furui Liu", "Laiwan Chan"], "venue": "Neur. Comp. 28,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Computing the Multinomial Stochastic Complexity in Sub-Linear Time", "author": ["Tommi Mononen", "Petri Myllym\u00e4ki"], "venue": "In PGM", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Distinguishing Cause from E\u0082ect Using Observational Data: Methods and Benchmarks", "author": ["Joris M. Mooij", "Jonas Peters", "Dominik Janzing", "Jakob Zscheischler", "Bernhard Sch\u00f6lkopf"], "venue": "JMLR 17,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Probabilistic latent variable models for distinguishing between cause and e\u0082ect", "author": ["J.M. Mooij", "O. Stegle", "D. Janzing", "K. Zhang", "B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Model selection by normalized maximum likelihood", "author": ["Jay I. Myung", "Daniel J. Navarro", "Mark A. Pi"], "venue": "J. Math. Psych. 50,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Causality: Models, Reasoning, and Inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Causality: Models, Reasoning and Inference (2nd ed.)", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Identifying Cause and E\u0082ect on Discrete Data using Additive Noise Models", "author": ["J. Peters", "D. Janzing", "B. Sch\u00f6lkopf"], "venue": "In AISTATS", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Causal Discovery with Continuous Additive Noise Models", "author": ["J. Peters", "JM. Mooij", "D. Janzing", "B. Sch\u00f6lkopf"], "venue": "JMLR", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Modeling by shortest data description", "author": ["Jorma Rissanen"], "venue": "Automatica 14,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1978}, {"title": "Strong optimality of the normalized ML models as universal codes and information in data", "author": ["Jorma Rissanen"], "venue": "IEEE TIT 47,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "Bayesian network structure learning using factorized NML universal models", "author": ["T. Roos", "T. Silander", "P. Kontkanen", "P. Myllym\u00e4ki"], "venue": "In Proc. Information \u008aeory and Applications Workshop (ITA). IEEE", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Inference of Cause and E\u0082ect with Unsupervised Inverse Regression", "author": ["E. Sgouritsa", "D. Janzing", "P. Hennig", "B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "A Linear Non-Gaussian Acyclic Model for Causal Discovery", "author": ["Shohei Shimizu", "Patrik O. Hoyer", "Aapo Hyv\u00e4rinen", "An\u008ai Kerminen"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Universal sequential coding of single messages", "author": ["Y.M. Shtarkov"], "venue": "Problems of Information Transmission 23,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1987}, {"title": "Kolmogorov\u2019s Structure functions and model selection", "author": ["N.K. Vereshchagin", "P.M.B. Vitanyi"], "venue": "IEEE TIT 50,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Causal Inference by Direction of Information", "author": ["Jilles Vreeken"], "venue": "In SDM", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "On the Identi\u0080ability of the Post-nonlinear Causal Model. In UAI", "author": ["Kun Zhang", "Aapo Hyv\u00e4rinen"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Testing whether linear equations are causal: A free probability theory approach", "author": ["J. Zscheischler", "D. Janzing", "K. Zhang"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}], "referenceMentions": [{"referenceID": 17, "context": "1 INTRODUCTION Causal inference from observational data\u2014that is, identifying cause and e\u0082ect in data that was not collected through carefully controlled randomised trials\u2014is a fundamental problem in both business and science [18, 28].", "startOffset": 225, "endOffset": 233}, {"referenceID": 18, "context": "\u008cese ideas include that of the Additive Noise Model (ANM), where we assume the e\u0082ect is a function of the cause with additive noise independent of the cause [19, 20, 26], and that of the algorithmic Markov condition [1, 8] which is based on Kolmogorov Complexity.", "startOffset": 157, "endOffset": 169}, {"referenceID": 19, "context": "\u008cese ideas include that of the Additive Noise Model (ANM), where we assume the e\u0082ect is a function of the cause with additive noise independent of the cause [19, 20, 26], and that of the algorithmic Markov condition [1, 8] which is based on Kolmogorov Complexity.", "startOffset": 157, "endOffset": 169}, {"referenceID": 24, "context": "\u008cese ideas include that of the Additive Noise Model (ANM), where we assume the e\u0082ect is a function of the cause with additive noise independent of the cause [19, 20, 26], and that of the algorithmic Markov condition [1, 8] which is based on Kolmogorov Complexity.", "startOffset": 157, "endOffset": 169}, {"referenceID": 0, "context": "\u008cese ideas include that of the Additive Noise Model (ANM), where we assume the e\u0082ect is a function of the cause with additive noise independent of the cause [19, 20, 26], and that of the algorithmic Markov condition [1, 8] which is based on Kolmogorov Complexity.", "startOffset": 216, "endOffset": 222}, {"referenceID": 7, "context": "\u008cese ideas include that of the Additive Noise Model (ANM), where we assume the e\u0082ect is a function of the cause with additive noise independent of the cause [19, 20, 26], and that of the algorithmic Markov condition [1, 8] which is based on Kolmogorov Complexity.", "startOffset": 216, "endOffset": 222}, {"referenceID": 6, "context": "complexity is not computable, any method using this observation requires a computable approximation of this notion, which in general involves arbitrary choices [7, 12, 25, 30].", "startOffset": 160, "endOffset": 175}, {"referenceID": 11, "context": "complexity is not computable, any method using this observation requires a computable approximation of this notion, which in general involves arbitrary choices [7, 12, 25, 30].", "startOffset": 160, "endOffset": 175}, {"referenceID": 23, "context": "complexity is not computable, any method using this observation requires a computable approximation of this notion, which in general involves arbitrary choices [7, 12, 25, 30].", "startOffset": 160, "endOffset": 175}, {"referenceID": 27, "context": "complexity is not computable, any method using this observation requires a computable approximation of this notion, which in general involves arbitrary choices [7, 12, 25, 30].", "startOffset": 160, "endOffset": 175}, {"referenceID": 2, "context": "\u008cis means that even if the true data generating distribution does not reside in the model classM under consideration, we still obtain the optimal encoding for the data relative toM [3].", "startOffset": 181, "endOffset": 184}, {"referenceID": 9, "context": "1 Kolmogorov Complexity \u008ce Kolmogorov complexity of a \u0080nite binary string x is the length of the shortest binary program p\u2217 for a Universal Turing machineU that generates x , and then halts [10, 11].", "startOffset": 190, "endOffset": 198}, {"referenceID": 10, "context": "1 Kolmogorov Complexity \u008ce Kolmogorov complexity of a \u0080nite binary string x is the length of the shortest binary program p\u2217 for a Universal Turing machineU that generates x , and then halts [10, 11].", "startOffset": 190, "endOffset": 198}, {"referenceID": 10, "context": "I (y : x) + = I (x : y), where + = denotes equality up to an additive constant, and therefore also called algorithmic mutual information [11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 3, "context": "\u008ce Kolmogorov complexity of a probability distribution P , K(P), is the length of the shortest program that outputs P(x) to precision q on input \u3008x ,q\u3009 [4].", "startOffset": 152, "endOffset": 155}, {"referenceID": 10, "context": "We refer the interested reader to Li & Vit\u00e1nyi [11] for more details on Kolmogorov complexity.", "startOffset": 47, "endOffset": 51}, {"referenceID": 23, "context": "Postulate 1 (independence of input and mechanism [25]).", "startOffset": 49, "endOffset": 53}, {"referenceID": 6, "context": "\u008cis postulate provides the foundation for many successful causal inference frameworks designed for a pair of variables [7, 9, 24, 25].", "startOffset": 119, "endOffset": 133}, {"referenceID": 8, "context": "\u008cis postulate provides the foundation for many successful causal inference frameworks designed for a pair of variables [7, 9, 24, 25].", "startOffset": 119, "endOffset": 133}, {"referenceID": 23, "context": "\u008cis postulate provides the foundation for many successful causal inference frameworks designed for a pair of variables [7, 9, 24, 25].", "startOffset": 119, "endOffset": 133}, {"referenceID": 6, "context": "[7] de\u0080ne independence in terms of information geometry.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Liu & Chan [12] formulate independence in terms of the distance correlation between marginal and conditional empirical distribution.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "Janzing & Sch\u00f6lkopf [8] formalise independence using algorithmic information theory, and postulate algorithmic independence of P(X ) and P(Y | X ).", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "Postulate 2 (algorithmic independence of Markov kernels [8]).", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "Postulate 2 is equivalent to saying that if X \u2192 Y , factorizing the joint distribution over X and Y into P(X ) and P(Y | X ), will lead to simpler \u2014 in terms of Kolmogorov complexity \u2014 models than factorizing it into P(Y ) and P(X | Y ) [8].", "startOffset": 237, "endOffset": 240}, {"referenceID": 14, "context": "[15]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "We can, for instance, approximate Kolmogorov complexity from above through lossless compression [11].", "startOffset": 96, "endOffset": 100}, {"referenceID": 2, "context": "More generally, the Minimum Description Length (MDL) principle [3, 21] provides a statistically sound and computable means for approximating Kolmogorov complexity [3, 29].", "startOffset": 63, "endOffset": 70}, {"referenceID": 20, "context": "More generally, the Minimum Description Length (MDL) principle [3, 21] provides a statistically sound and computable means for approximating Kolmogorov complexity [3, 29].", "startOffset": 63, "endOffset": 70}, {"referenceID": 2, "context": "More generally, the Minimum Description Length (MDL) principle [3, 21] provides a statistically sound and computable means for approximating Kolmogorov complexity [3, 29].", "startOffset": 163, "endOffset": 170}, {"referenceID": 26, "context": "More generally, the Minimum Description Length (MDL) principle [3, 21] provides a statistically sound and computable means for approximating Kolmogorov complexity [3, 29].", "startOffset": 163, "endOffset": 170}, {"referenceID": 20, "context": "1 Minimum Description Length Principle \u008ce Minimum Description Length (MDL) [21] principle is a practical version of Kolmogorov complexity.", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "\u008ce MDL principle has its root in the two-part decomposition of the Kolmogorov complexity [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "It can be roughly described as follows [3].", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "Unlike crude MDL, re\u0080ned MDL encodes D with the (entire) model classM, resulting in single one-part code L\u0304(D | M) [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "Although the coding schemes are di\u0082erent across those codes, the resulting code lengths L\u0304(D | M) are almost the same [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 25, "context": "First, it gives a unique solution to the minimax problem posed by Shtarkov [27],", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "In other words, the NML distribution is the mini-max optimal universal model with respect to the model class [16].", "startOffset": 109, "endOffset": 113}, {"referenceID": 21, "context": "Second, it also provides solution to another mini-max problem formulated by Rissanen [22], which is given by", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "However, we can approximate the normalising sum up to a \u0080nite \u0083oating-point precision in sub-linear time with respect to the data size n given precomputed counts hi [13].", "startOffset": 165, "endOffset": 169}, {"referenceID": 7, "context": "However, it has also a\u008aracted quite a lot of a\u008aention over the years [8, 17, 26, 28].", "startOffset": 69, "endOffset": 84}, {"referenceID": 16, "context": "However, it has also a\u008aracted quite a lot of a\u008aention over the years [8, 17, 26, 28].", "startOffset": 69, "endOffset": 84}, {"referenceID": 24, "context": "However, it has also a\u008aracted quite a lot of a\u008aention over the years [8, 17, 26, 28].", "startOffset": 69, "endOffset": 84}, {"referenceID": 16, "context": "Constraint-based approaches like conditional independence test [17, 28] are one of the widely used causal inference frameworks.", "startOffset": 63, "endOffset": 71}, {"referenceID": 5, "context": "\u008ce linear trace method [6, 32] infers linear causal relations of the form Y = AX , where A is the structure matrix that maps the cause to the e\u0082ect, using the linear trace condition.", "startOffset": 23, "endOffset": 30}, {"referenceID": 29, "context": "\u008ce linear trace method [6, 32] infers linear causal relations of the form Y = AX , where A is the structure matrix that maps the cause to the e\u0082ect, using the linear trace condition.", "startOffset": 23, "endOffset": 30}, {"referenceID": 1, "context": "\u008ce kernelized trace method [2] can infer non-linear causal relations, but requires the causal relation to be deterministic, functional, and invertible.", "startOffset": 27, "endOffset": 30}, {"referenceID": 24, "context": "One of the key frameworks for causal inference are the Additive Noise Models (ANMs) [26].", "startOffset": 84, "endOffset": 88}, {"referenceID": 4, "context": "Over the years, many frameworks for causal inference from real-valued data have been proposed using ANMs [5, 20, 26, 31].", "startOffset": 105, "endOffset": 120}, {"referenceID": 19, "context": "Over the years, many frameworks for causal inference from real-valued data have been proposed using ANMs [5, 20, 26, 31].", "startOffset": 105, "endOffset": 120}, {"referenceID": 24, "context": "Over the years, many frameworks for causal inference from real-valued data have been proposed using ANMs [5, 20, 26, 31].", "startOffset": 105, "endOffset": 120}, {"referenceID": 28, "context": "Over the years, many frameworks for causal inference from real-valued data have been proposed using ANMs [5, 20, 26, 31].", "startOffset": 105, "endOffset": 120}, {"referenceID": 7, "context": "Algorithmic information theory provides a sound general theoretical foundation for causal inference [8].", "startOffset": 100, "endOffset": 103}, {"referenceID": 7, "context": "of the distributions P(X ) and P(Y | X ) [8].", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "It has also been used in justifying the additive noise model based causal discovery [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "For instance, the information-geometric approach [7] de\u0080nes independence via orthogonality in information space.", "startOffset": 49, "endOffset": 52}, {"referenceID": 23, "context": "Cure [25] de\u0080nes independence in terms of the accuracy of the estimations of P(Y | X ) and P(X | Y ).", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "Using algorithmic information theory, Vreeken [30] proposes a causal framework based on relative conditional complexity and instantiates it with cumulative entropy to infer the causal direction in continuous real-valued data.", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": "Budhathoki & Vreeken [1] propose a decision tree based approach for causal inference on univariate and multivariate binary data.", "startOffset": 21, "endOffset": 24}, {"referenceID": 18, "context": "[19] (dr) extend additive noise models to discrete data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Liu & Chan [12] (dc) de\u0080ne independence in terms of the distance correlation between empirical distributions P(X ) and P(Y | X ) to infer the causal direction from categorical data.", "startOffset": 11, "endOffset": 15}, {"referenceID": 18, "context": "We compare cisc against Discrete Regression (dr) [19], and dc [12].", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "We compare cisc against Discrete Regression (dr) [19], and dc [12].", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "[19], we sample X from the following distributions, using independently generated uniform noise.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "However, this happens in only few trivial instances [19].", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "2 Benchmark Data Next we evaluate cisc on benchmark cause-e\u0082ect pairs with known ground truth [14].", "startOffset": 94, "endOffset": 98}, {"referenceID": 6, "context": "When we consider only those pairs where cisc is most decisive\u2014with a very high value of |S(X \u2192 Y ) \u2212 S(Y \u2192 X )|, it is 100% accurate on top 22% of the pairs, 80% accurate on top 45% of the pairs, which is on-par with the top-performing causal inference frameworks for continuous real-valued data [7, 25].", "startOffset": 296, "endOffset": 303}, {"referenceID": 23, "context": "When we consider only those pairs where cisc is most decisive\u2014with a very high value of |S(X \u2192 Y ) \u2212 S(Y \u2192 X )|, it is 100% accurate on top 22% of the pairs, 80% accurate on top 45% of the pairs, which is on-par with the top-performing causal inference frameworks for continuous real-valued data [7, 25].", "startOffset": 296, "endOffset": 303}, {"referenceID": 18, "context": "[19], we regard the data as discrete, and consider X \u2192 Y1, X \u2192 Y2, and X \u2192 Y3 as the ground truth as sex causes the size of the abalone and not the other way around.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "\u008ce proposed framework lays a clear computable foundation for algorithmic causal inference principle postulated by Janzing & Sch\u00f6lkopf [8].", "startOffset": 134, "endOffset": 137}, {"referenceID": 22, "context": "It would also be interesting to explore factorized normalized maximum likelihood models [23] to instantiate the framework for multivariate data [1].", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "It would also be interesting to explore factorized normalized maximum likelihood models [23] to instantiate the framework for multivariate data [1].", "startOffset": 144, "endOffset": 147}], "year": 2017, "abstractText": "\u008ce algorithmic Markov condition states that the most likely causal direction between two random variables X and Y can be identi\u0080ed as that direction with the lowest Kolmogorov complexity. Due to the halting problem, however, this notion is not computable. We hence propose to do causal inference by stochastic complexity. \u008cat is, we propose to approximate Kolmogorov complexity via the Minimum Description Length (MDL) principle, using a score that is mini-max optimal with regard to the model class under consideration. \u008cis means that even in an adversarial se\u008aing, such as when the true distribution is not in this class, we still obtain the optimal encoding for the data relative to the class. We instantiate this framework, which we call cisc, for pairs of univariate discrete variables, using the class of multinomial distributions. Experiments show that cisc is highly accurate on synthetic, benchmark, as well as real-world data, outperforming the state of the art by a margin, and scales extremely well with regard to sample and domain sizes.", "creator": "LaTeX with hyperref package"}}}