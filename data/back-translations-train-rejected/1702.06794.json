{"id": "1702.06794", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing", "abstract": "Error propagation is a common problem in NLP. Reinforcement learning explores erroneous states during training and can therefore be more robust when mistakes are made early in a process. In this paper, we apply reinforcement learning to greedy dependency parsing which is known to suffer from error propagation. Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency. We investigate the portion of errors which are the result of error propagation and confirm that reinforcement learning reduces the occurrence of error propagation.", "histories": [["v1", "Wed, 22 Feb 2017 13:49:18 GMT  (620kb,D)", "http://arxiv.org/abs/1702.06794v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["minh le", "antske fokkens"], "accepted": false, "id": "1702.06794"}, "pdf": {"name": "1702.06794.pdf", "metadata": {"source": "CRF", "title": "Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing", "authors": ["Minh L\u00ea", "Antske Fokkens"], "emails": ["m.n.le@vu.nl", "antske.fokkens@vu.nl"], "sections": [{"heading": "1 Introduction", "text": "In fact, NLP tools applied early in a pipeline can lead to errors that have a negative impact on higher tasks. It can also occur within the application of a specific task, when sequential decisions are made and mistakes are made that are later applied. A system that actively tries different courses of action will contain some errors. We hypothesize that a system trained in this way is more robust and less prone to errors. We test our hypotheses by applying reinforcement measures."}, {"heading": "2 Related Work", "text": "In this section, we cover related work on dependency parsing, including alternative approaches to reducing error propagation and learning reinforcement."}, {"heading": "2.1 Dependency Parsing", "text": "It's not just a matter of time until it's done, it's a matter of time until it's done, until it's done. \"And so it's also:\" It's a matter of time until it's done. \"It's a matter of time until it's done.\" It's a matter of time until it's done, \"he says.\" But it's a matter of time until it's done. \"The only question is whether there will be a process in which there will be a process in which there will be a process in which there will be a process in which there will be a process in which there will be a process in which there will be a process in which there will be a process in which there will be a process.\""}, {"heading": "2.2 Reinforcement Learning", "text": "Reinforcement learning has been successfully applied to several NLP tasks, e.g. agenda-based parsing (Jiang et al., 2012), semantic parsing (Berant and Liang, 2015), and simultaneous machine translation (Grissom II et al., 2014), but to our knowledge none of these studies examined the impact of reinforcement learning on error propagation. Learning to Search (L2S) is probably the most prominent research direction that applies reinforcement learning (more precisely: imitation learning) to NLP. Various algorithms, e.g. SEARN (Daum\u00e9 III et al., 2009) and DAgger (Ross et al., 2011), have developed common high-level steps: A roll-in policy is implemented to generate training states from which a roll-out policy is used to estimate the loss of certain actions."}, {"heading": "2.3 Reinforcement and error propagation", "text": "As noted above, previous work applying reinforcement learning to NLP has not, to our knowledge, shown that it improved results by reducing error propagation. The work to determine the impact of error propagation on parsing is rare, with Ng and Curran (2015) being a notable exception. They provide detailed error analysis to parse and classify what kind of analysis errors go hand in hand with error propagation. There are four major differences between their approaches and ours: First, Ng and Curran perform correct arcs in the tree, and our algorithm corrects parsing algorithm decisions. Second, our approach distinguishes between cases where a faulty action deterministically leads to multiple faulty arcs, and cases where a faulty action indirectly leads to further errors (see Section 5.1 for a detailed explanation). Third, Ng and Curran's algorithm correct all faulty arcs that result in multiple parsing errors, and finally, the type of parsing errors can be presented as the same, and that parsing errors are not detailed in multiple directions."}, {"heading": "3 A Reinforced Greedy Parser", "text": "This section describes the systems used in our experiments. First, we describe the arc standard algorithm because its familiarity helps to understand our error propagation analysis, then we briefly outline the main differences between the arc standard algorithm and the alternative algorithms we have experimented with (arc eagle and swap standard), and then we outline the traditional and novel machine learning approaches. The features we use are identical to those described in Chen and Manning (2014)."}, {"heading": "3.1 Transition-Based Dependency Parsing", "text": "In a standard Arc system (Nivre, 2004), a parsing configuration consists of a triple < \u03a3, \u03b2, A >, where \u03a3 is a stack, \u03b2 is a buffer containing the remaining input marks, and A is the dependency arcs generated during the parsing process. At the start, the stack contains only the root symbol (\u03a3 = [ROOT]), the buffer contains the tokens of the set (\u03b2 = [w1,..., wn]), and the set of arcs is empty (A = \u2205).The standard Arc system supports three transitions. If \u03c31 is the uppermost element and \u03c32 the second element on the stack, and \u03b21 is the first element of the buffer: 1LEFTl adds an arc \u03c31 l \u2212 \u2192 Fern2 toA and removes \u03c32 toA from the stack."}, {"heading": "3.2 Training with a Static Oracle", "text": "In transition-based dependency parsing, it is customary to convert a dependency treebank D 3 (x, y) into a collection of s-S input features and corresponding gold standard actions a-A for training with the help of a static oracle O. In Chen and Manning (2014), a neural network acts as a function mapping of input features on the probability of actions: fNN: S \u00b7 A \u2192 [0, 1]. The neural network is trained to minimize negative log probabilities for loss of the converted treebank: L = \u2211 (x, y) and D \u2211 (s, a) and O (x, y) \u2212 log fNN (s, a) (1)."}, {"heading": "3.3 Reinforcement Learning", "text": "Following Maes et al. (2009), we consider transitional dependence to be a deterministic Markov decision-making process. The problem is summarized by a tuple < S, A, T, r >, where S is the totality of all possible states, A contains all possible actions, T is a figure called S \u00b7 A \u2192 S, and r: S \u00b7 A \u2192 R is a reward function. A state corresponds to a configuration and is summarized in input characteristics. Possible actions are defined for each transition system described in Section 3.1. We keep the training approach simple by using only one reward function r (y) at the end of each paragraph. Given this framework, a stochastic policy guides our parser by assigning each state to a probable distribution of actions. During the training, we use the function fNN, which is described in Section 3.2, as a stochastic policy. At test times, actions are selected according to the existing literature."}, {"heading": "3.4 Approximate Policy Gradient", "text": "D \"s.\" D \"s.\" D \"s.\" D \"s.\" D \"s.\" D \"s.\" i \"s\" i \"s.\" D \"s.\" D \"s\" i \"s\" i \"s.\" D \"s.\" i \"s\" i \"s.\" D \"s\" i \"s.\" D \"s.\" D \"i\" s \"i\" s. \"D\" s. \"i\" s. \"D\" s. \"i\" s. \"D\" s. \"i\" s. \"D\" s. \"D\" s. \"D\" s. \"D\" s. \"D.\" D. \"D.\" D. \"D.\" D. \"s.\" D. \"D\" s. \"D.\" D. \"D.\" D. \"s.\" D. \"D.\" s. \"D.\" D. \"D.\" D. \"s.\" D. \"D.\" D. \"D.\" s. \"D.\" D. \"D.\" D. \"D.\" s. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" s. \"D.\" D. \"D.\" D. \"D.\" D. \"s.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" s. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" s. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"s.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"s.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" s. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" s. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" D. \"D.\" s. \"D.\" D. \"D.\" D. \""}, {"heading": "4 Reinforcement Learning Experiments", "text": "We train a parser on the basis of a supervised learning process and then improve its performance using APG. Empirically, we have tested that a second training with supervised learning has little or no impact on performance."}, {"heading": "4.1 Experimental Setup", "text": "We use PENN Treebank 3 with standard split (training, development and test set) for our experiments with arg-standard and arg-eager. As the swap standard parser is mainly suitable for non-projective structures that are rare in the PENN Treebank, we evaluate this parser based on the German section of the SPMRL dataset. For PENN Treebank, we follow Chen and Manning's pre-processing steps. In addition, we use their pre-trained Model3 for arc standard and train our own models in similar settings for other transition systems. For reinforcement learning, we use AdaGrad for optimization. We do not use dropout because we have observed that it destabilizes the training process. The r (y) reward is the number of correctly described arcs (i.e. LAS multiplied by the number of tokens).4 The baseline is set to half the number of tokens (corresponding to a 0.5 LAS score). Since the REC takes a lot of time to select a few 00000 = 1 tokens, and the REC takes a lot of time to develop in 8000."}, {"heading": "4.2 Effectiveness of Reinforcement Learning", "text": "Table 2 shows the performance of the various approaches to developing dependency savers. Although we used the pre-built Chen and Manning (2014) model and Stanford's open source software, the results of our baseline are slightly worse than what is reported in their work. This may be due to slight differences in settings and does not affect our conclusions. Across transition systems and two languages, APG performs better in supervised learning, which confirms our hypothesis. Furthermore, it is not easy because the learners are exposed to more examples than their monitored counterparts. RL-ORACLE3We use PTB _ Stanford _ params.txt.gz, which is downloaded from http: / / nlp.ford.edu / software / nndep.shtml on December 30, 2015.4Punctuation is not considered, following Chen and Manning (2014).5Downloaded from http: / nlp.stanford.u / nlp.software / lshtml is up to three school shootings."}, {"heading": "5 Error Propagation Experiment", "text": "In this section we describe our algorithm and the experiment that identifies the error propagation in the standard arc parsers."}, {"heading": "5.1 Error Propagation", "text": "Section 3.1 explains that a transition-based parser traverses the sentence incrementally and must select a transition of [SHIFT, LEFTl, RIGHTl] at each step. We use the term arc error to refer to a faulty arc in the resulting tree. The term decision error refers to a transition that leads to a loss of parsing accuracy. Decision errors in the parsing process lead to one or more arc errors in the resulting tree. There are two ways in which a single decision error can lead to multiple arc errors. First, the decision can lead deterministically to more than one arc error because (e.g.) a faulty shaped arc also blocks other correct arcs. Second, a faulty parse decision changes some of the features that the model uses for future decisions, and these changes can lead to further (decision) errors."}, {"heading": "5.2 Examining the impact of decision errors", "text": "We study the impact of individual decision makers on the overall results in our test by combining a dynamic oracle and a recursive function. We use a dynamic oracle based on Goldberg et al. (2014), which gives us the total loss at any point during the derivative. Loss is equal to the minimum number of arc errors made to determine how many arc errors were caused by a particular decision. The prevalence of decision errors cannot be determined simply by examining the loss during the derivative process. We use a recursive function to determine whether a particular arc error arose from it."}, {"heading": "6 Conclusion", "text": "This work introduced Approximate Policy Gradient (APG), an efficient amplification learning algorithm for NLP, and applied it to a greedy high-performance dependency parser. We hypothesized that amplification learning would be more robust against error propagation and thus improve the accuracy of error analysis. To verify our hypothesis, we conducted experiments that applied APG to three transition systems and two languages. Furthermore, we conducted an experiment to investigate which part of the error was the result of error propagation and compared the results of standardized machine learning with amplification learning. Our results showed that (a) amplification learning actually improved the accuracy of the analysis and (b) spread errors were overrepresented in the series of avoidable errors, which confirmed our hypothesis. To our knowledge, this work is the first to experimentally show that amplification learning can reduce the error propagation in a NLP task."}, {"heading": "Acknowledgments", "text": "The research on this paper was supported by the Dutch Organisation for Scientific Research (NWO) through the projects of the Spinoza Prize Vossen (SPI 30-673, 2014-2019) and the VENI project Reading between the lines (VENI 275-89-029). Experiments on the national e-infrastructure of the Netherlands were carried out with the support of the SURF Cooperative. We would like to thank our friends and colleagues Piek Vossen, Roser Morante, Tommaso Caselli, Emiel van Miltenburg and Ngoc Do for many useful comments and discussions."}], "references": [{"title": "Improved Transition-Based Parsing and Tagging with Neural Networks", "author": ["David Weiss", "Greg Coppola", "Slav Petrov"], "venue": "EMNLP", "citeRegEx": "Alberti et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alberti et al\\.", "year": 2015}, {"title": "Globally Normalized Transition-Based Neural Networks. arXiv.org, cs.CL", "author": ["Andor et al.2016] Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": null, "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Imitation Learning of Agenda-based Semantic Parsers", "author": ["Berant", "Liang2015] Jonathan Berant", "Percy Liang"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2015}, {"title": "Non-Deterministic Oracles for Unrestricted Non-Projective Transition-Based Dependency Parsing", "author": ["Bj\u00f6rkelund", "Nivre2015] Anders Bj\u00f6rkelund", "Joakim Nivre"], "venue": "IWPT", "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2015}, {"title": "Learning to search better than your teacher", "author": ["Chang et al.2015] Kai-Wei Chang", "Akshay Krishnamurthy", "Alekh Agarwal", "Hal Daum\u00e9 III", "John Langford"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Search-based Structured Prediction", "author": ["John Langford", "Daniel Marcu"], "venue": "Machine Learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "The Necessity of Parsing for Predicate Argument Recognition", "author": ["Gildea", "Palmer2002] Daniel Gildea", "Martha Palmer"], "venue": "ACL", "citeRegEx": "Gildea et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gildea et al\\.", "year": 2002}, {"title": "A Dynamic Oracle for Arc-Eager Dependency Parsing", "author": ["Goldberg", "Nivre2012] Yoav Goldberg", "Joakim Nivre"], "venue": "COLING", "citeRegEx": "Goldberg et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2012}, {"title": "Training Deterministic Parsers with Non-Deterministic Oracles", "author": ["Goldberg", "Nivre2013] Yoav Goldberg", "Joakim Nivre"], "venue": "In TACL 2013,", "citeRegEx": "Goldberg et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2013}, {"title": "A tabular method for dynamic oracles in transition-based parsing", "author": ["Francesco Sartorio", "Giorgio Satta"], "venue": "In TACL 2014,", "citeRegEx": "Goldberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2014}, {"title": "A Polynomial-Time Dynamic Oracle for Non-Projective Dependency Parsing", "author": ["GomezRodriguez", "Francesco Sartorio", "Giorgio Satta."], "venue": "EMNLP 2014, pages 917\u2013927. ACL.", "citeRegEx": "GomezRodriguez et al\\.,? 2014", "shortCiteRegEx": "GomezRodriguez et al\\.", "year": 2014}, {"title": "Don\u2019t Until the Final Verb Wait: Reinforcement Learning for Simultaneous Machine Translation", "author": ["Jordan Boyd-Graber", "He He", "John Morgan", "Hal Daume III"], "venue": null, "citeRegEx": "II et al\\.,? \\Q2014\\E", "shortCiteRegEx": "II et al\\.", "year": 2014}, {"title": "Effects of parsing errors on pre-reordering performance for Chinese-to-Japanese", "author": ["Han et al.2013] Dan Han", "Pascual Mart\u00ednez-G\u00f3mez", "Yusuke Miyao", "Katsuhito Sudoh", "Masaaki Nagata"], "venue": "SMT. PACLIC", "citeRegEx": "Han et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Han et al\\.", "year": 2013}, {"title": "An Improved Non-monotonic Transition System for Dependency Parsing", "author": ["Honnibal", "Johnson2015] Matthew Honnibal", "Mark Johnson"], "venue": "EMNLP", "citeRegEx": "Honnibal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Honnibal et al\\.", "year": 2015}, {"title": "Learned Prioritization for Trading Off Accuracy and Speed", "author": ["Jiang et al.2012] Jiarong Jiang", "Adam Teichert", "Hal Daum\u00e9 III", "Jason Eisner"], "venue": "ICML workshop on Inferning: Interactions between Inference and Learning,", "citeRegEx": "Jiang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2012}, {"title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations. CoRR, abs/1603.0", "author": ["Kiperwasser", "Yoav Goldberg"], "venue": null, "citeRegEx": "Kiperwasser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser et al\\.", "year": 2016}, {"title": "Ambiguity-aware Ensemble Training for Semi-supervised Dependency Parsing", "author": ["Li et al.2014] Zhenghua Li", "Min Zhang", "Wenliang Chen"], "venue": "ACL", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Structured prediction with reinforcement learning", "author": ["Maes et al.2009] Francis Maes", "Ludovic Denoyer", "Patrick Gallinari"], "venue": "Machine Learning,", "citeRegEx": "Maes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Maes et al\\.", "year": 2009}, {"title": "Characterizing the Errors of Data-Driven Dependency Parsing Models", "author": ["McDonald", "Nivre2007] Ryan McDonald", "Joakim Nivre"], "venue": "EMNLP-CoNLL", "citeRegEx": "McDonald et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2007}, {"title": "Nonprojective dependency parsing using spanning tree algorithms", "author": ["Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d"], "venue": "HLT-EMNLP", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Identifying Cascading Errors using Constraints in Dependency Parsing", "author": ["Ng", "Curran2015] Dominick Ng", "James R Curran"], "venue": "In ACL-IJCNLP,", "citeRegEx": "Ng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2015}, {"title": "Arc-eager Parsing with the Tree Constraint", "author": ["Nivre", "Daniel Fern\u00e1ndez-Gonz\u00e1lez"], "venue": "Computational Linguistics,", "citeRegEx": "Nivre et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2014}, {"title": "MaltParser: A data-driven parsergenerator for dependency parsing", "author": ["Nivre et al.2006a] Joakim Nivre", "Johan Hall", "Jens Nilsson"], "venue": "In LREC 2006,", "citeRegEx": "Nivre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "Labeled pseudo-projective dependency parsing with support vector machines", "author": ["Nivre et al.2006b] Joakim Nivre", "Johan Hall", "Jens Nilsson", "G\u00fcl\u015fen Eryi\u011fit", "Svetoslav Marinov"], "venue": "In CoNLL", "citeRegEx": "Nivre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2006}, {"title": "MaltParser: A language-independent system for datadriven dependency parsing", "author": ["Nivre et al.2007] Joakim Nivre", "Johan Hall", "Jens Nilsson", "Atanas Chanev", "Eryi\u01e7it G\u00fcl\u015fen", "Sandra K\u00fcbler", "Svetoslav Marinov", "Erwin Marsi"], "venue": "Natural Language En-", "citeRegEx": "Nivre et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "An Efficient Algorithm for Projective Dependency Parsing", "author": ["Joakim Nivre"], "venue": "IWPT", "citeRegEx": "Nivre.,? \\Q2003\\E", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "Incrementality in Deterministic Dependency Parsing", "author": ["Joakim Nivre"], "venue": "In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together", "citeRegEx": "Nivre.,? \\Q2004\\E", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Non-projective Dependency Parsing in Expected Linear Time", "author": ["Joakim Nivre"], "venue": "ACLIJCNLP", "citeRegEx": "Nivre.,? \\Q2009\\E", "shortCiteRegEx": "Nivre.", "year": 2009}, {"title": "The impact of parse quality on syntactically-informed statistical machine translation", "author": ["Quirk", "Corston-Oliver2006] Chris Quirk", "Simon Corston-Oliver"], "venue": "EMNLP", "citeRegEx": "Quirk et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Quirk et al\\.", "year": 2006}, {"title": "Sequence Level Training with Recurrent Neural Networks", "author": ["Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "A Reduction of Imitation Learning and Structured Prediction to NoRegret Online Learning", "author": ["Ross et al.2011] Stephane Ross", "Geoffrey J Gordon", "J Andrew Bagnell"], "venue": null, "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Introducing the SPMRL 2014 Shared Task on Parsing Morphologically-Rich Languages", "author": ["Seddah et al.2014] Djam\u00e9 Seddah", "Sandra K\u00fcbler", "Reut Tsarfaty"], "venue": "In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Lan-", "citeRegEx": "Seddah et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seddah et al\\.", "year": 2014}, {"title": "Minimum Risk Training for Neural Machine Translation", "author": ["Shen et al.2016] Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "ACL", "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors", "author": ["Song et al.2012] Hyun-Je Song", "Jeong-Woo Son", "TaeGil Noh", "Seong-Bae Park", "Sang-Jo Lee"], "venue": "ACL", "citeRegEx": "Song et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Song et al\\.", "year": 2012}, {"title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation", "author": ["David Mcallester", "Satinder Singh", "Yishay Mansour"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Structured Training for Neural Network Transition-Based Parsing", "author": ["Weiss et al.2015] David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "ACL-IJCNLP", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Statistical Dependency Analysis with Support Vector Machines", "author": ["Yamada", "Matsumoto2003] Hiroyasu Yamada", "Yuji Matsumoto"], "venue": "In Proceedings of IWPT,", "citeRegEx": "Yamada et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2003}, {"title": "Joint Inference for Fine-grained Opinion Extraction", "author": ["Yang", "Cardie2013] Bishan Yang", "Claire Cardie"], "venue": "ACL", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "Dependency Parsing with Energybased Reinforcement Learning", "author": ["Zhang", "Chan2009] Lidan Zhang", "Kwok Ping Chan"], "venue": "IWPT", "citeRegEx": "Zhang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 35, "context": "Error propagation is a common problem for many NLP tasks (Song et al., 2012; Quirk and CorstonOliver, 2006; Han et al., 2013; Gildea and Palmer, 2002; Yang and Cardie, 2013).", "startOffset": 57, "endOffset": 173}, {"referenceID": 14, "context": "Error propagation is a common problem for many NLP tasks (Song et al., 2012; Quirk and CorstonOliver, 2006; Han et al., 2013; Gildea and Palmer, 2002; Yang and Cardie, 2013).", "startOffset": 57, "endOffset": 173}, {"referenceID": 28, "context": "We test our hypothesis by applying reinforcement learning to greedy transition-based parsers (Yamada and Matsumoto, 2003; Nivre, 2004), which have been popular because of superior efficiency and accuracy nearing state-of-the-art.", "startOffset": 93, "endOffset": 134}, {"referenceID": 27, "context": "We test our hypothesis by applying reinforcement learning to greedy transition-based parsers (Yamada and Matsumoto, 2003; Nivre, 2004), which have been popular because of superior efficiency and accuracy nearing state-of-the-art. They are also known to suffer from error propagation. Because they work by carrying out a sequence of actions without reconsideration, an erroneous action can exert a negative effect on all subsequent decisions. By rendering correct parses unreachable or promoting incorrect features, the first error induces the second error and so on. McDonald and Nivre (2007) argue that the observed negative correlation between parsing accuracy and sentence length indicates error propagation is at work.", "startOffset": 122, "endOffset": 593}, {"referenceID": 33, "context": "Reinforcement learning increased both unlabeled and labeled accuracy on the Penn TreeBank and German part of SPMRL (Seddah et al., 2014).", "startOffset": 115, "endOffset": 136}, {"referenceID": 26, "context": "This small set of features makes their parser significantly more efficient than other popular parsers, such as the Malt (Nivre et al., 2007) or MST (McDonald et al.", "startOffset": 120, "endOffset": 140}, {"referenceID": 21, "context": ", 2007) or MST (McDonald et al., 2005) parser while obtaining higher accuracy.", "startOffset": 15, "endOffset": 38}, {"referenceID": 1, "context": "Andor et al. (2016) present a mathematical proof that globally normalized models are more expressive than locally normalized counterparts and propose to use global normalization with beam search at both training and testing.", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "Therefore, a large number of dynamic oracles have been developed in recent years (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Gomez-Rodriguez et al., 2014; Bj\u00f6rkelund and Nivre, 2015).", "startOffset": 81, "endOffset": 214}, {"referenceID": 23, "context": "They compare their results to Nivre et al. (2006b) using the same features, but they also change the model and apply beam search.", "startOffset": 30, "endOffset": 51}, {"referenceID": 1, "context": "dependency parsing have been achieved since (notably, Andor et al. (2016)), Chen and Manning\u2019s parser provides a better baseline for our purposes.", "startOffset": 54, "endOffset": 74}, {"referenceID": 16, "context": "agenda-based parsing (Jiang et al., 2012), semantic parsing (Berant and Liang, 2015) and simultaneous machine translation (Grissom II et al.", "startOffset": 21, "endOffset": 41}, {"referenceID": 32, "context": ", 2009) and DAgger (Ross et al., 2011), have been developed sharing common high-level steps: a roll-in policy is executed to generate training states from which a roll-out policy is used to estimate the loss of certain actions.", "startOffset": 19, "endOffset": 38}, {"referenceID": 36, "context": "APG belongs to the family of policy gradient algorithms (Sutton et al., 1999), i.", "startOffset": 56, "endOffset": 77}, {"referenceID": 38, "context": "REINFORCE (Williams, 1992; Ranzato et al., 2016) is a widely-used policy gradient algorithm but it is also well-known for suffering from high variance (Sutton et al.", "startOffset": 10, "endOffset": 48}, {"referenceID": 31, "context": "REINFORCE (Williams, 1992; Ranzato et al., 2016) is a widely-used policy gradient algorithm but it is also well-known for suffering from high variance (Sutton et al.", "startOffset": 10, "endOffset": 48}, {"referenceID": 36, "context": ", 2016) is a widely-used policy gradient algorithm but it is also well-known for suffering from high variance (Sutton et al., 1999).", "startOffset": 110, "endOffset": 131}, {"referenceID": 34, "context": "Shen et al. (2016)\u2019s algorithm is roughly equivalent to the combination of an oracle and random sampling.", "startOffset": 0, "endOffset": 19}, {"referenceID": 28, "context": "In an arc-standard system (Nivre, 2004), a parsing configuration consists of a triple \u3008\u03a3, \u03b2, A\u3009, where \u03a3 is a stack, \u03b2 is a buffer containing the remaining input tokens and A are the dependency arcs that are created during parsing process.", "startOffset": 26, "endOffset": 39}, {"referenceID": 27, "context": "To demonstrate that reinforcement learning can train different systems, we also carried out experiments with arc-eager (Nivre, 2003) and swapstandard (Nivre, 2009).", "startOffset": 119, "endOffset": 132}, {"referenceID": 29, "context": "To demonstrate that reinforcement learning can train different systems, we also carried out experiments with arc-eager (Nivre, 2003) and swapstandard (Nivre, 2009).", "startOffset": 150, "endOffset": 163}, {"referenceID": 19, "context": "Following Maes et al. (2009), we view transitionbased dependency parsing as a deterministic Markov Decision Process.", "startOffset": 10, "endOffset": 29}, {"referenceID": 9, "context": "We use a dynamic oracle based on Goldberg et al. (2014) which gives us the overall loss at any point during the derivation.", "startOffset": 33, "endOffset": 56}, {"referenceID": 1, "context": "Recent research on parsing has seen impressive improvement during the last year achieving UAS around 94% (Andor et al., 2016).", "startOffset": 105, "endOffset": 125}, {"referenceID": 1, "context": "Recent research on parsing has seen impressive improvement during the last year achieving UAS around 94% (Andor et al., 2016). This improvement is partially due to other approaches that, at least in theory, address error propagation, such as beam search. Both the reinforcement learning algorithm and the error propagation study we developed can be applied to other parsing approaches. There are two (related) main questions to be addressed in future work in the domain of parsing. The first addresses whether our method is complementary to alternative approaches and could also improve the current state-of-the-art. The second question would address the impact of various approaches on error propagation and the kind of errors they manage to avoid (following Ng and Curran (2015)).", "startOffset": 106, "endOffset": 781}], "year": 2017, "abstractText": "Error propagation is a common problem in NLP. Reinforcement learning explores erroneous states during training and can therefore be more robust when mistakes are made early in a process. In this paper, we apply reinforcement learning to greedy dependency parsing which is known to suffer from error propagation. Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency. We investigate the portion of errors which are the result of error propagation and confirm that reinforcement learning reduces the occurrence of error propagation.", "creator": "LaTeX with hyperref package"}}}