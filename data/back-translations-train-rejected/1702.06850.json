{"id": "1702.06850", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Scene Recognition by Combining Local and Global Image Descriptors", "abstract": "Object recognition is an important problem in computer vision, having diverse applications. In this work, we construct an end-to-end scene recognition pipeline consisting of feature extraction, encoding, pooling and classification. Our approach simultaneously utilize global feature descriptors as well as local feature descriptors from images, to form a hybrid feature descriptor corresponding to each image. We utilize DAISY features associated with key points within images as our local feature descriptor and histogram of oriented gradients (HOG) corresponding to an entire image as a global descriptor. We make use of a bag-of-visual-words encoding and apply Mini- Batch K-Means algorithm to reduce the complexity of our feature encoding scheme. A 2-level pooling procedure is used to combine DAISY and HOG features corresponding to each image. Finally, we experiment with a multi-class SVM classifier with several kernels, in a cross-validation setting, and tabulate our results on the fifteen scene categories dataset. The average accuracy of our model was 76.4% in the case of a 40%-60% random split of images into training and testing datasets respectively. The primary objective of this work is to clearly outline the practical implementation of a basic screne-recognition pipeline having a reasonable accuracy, in python, using open-source libraries. A full implementation of the proposed model is available in our github repository.", "histories": [["v1", "Tue, 21 Feb 2017 06:57:37 GMT  (713kb,D)", "http://arxiv.org/abs/1702.06850v1", "A full implementation of our model is available atthis https URL"]], "COMMENTS": "A full implementation of our model is available atthis https URL", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["jobin wilson", "muhammad arif"], "accepted": false, "id": "1702.06850"}, "pdf": {"name": "1702.06850.pdf", "metadata": {"source": "CRF", "title": "Scene Recognition by Combining Local and Global Image Descriptors", "authors": ["Jobin Wilson", "Muhammad Arif"], "emails": ["jobin.wilson@flytxt.com", "muhammad.arif@flytxt.com"], "sections": [{"heading": null, "text": "I. INTRODUCTIONObject recognition is an interesting computing problem with diverse applications in a variety of areas such as automated surveillance, robotics, human computer interaction, video indexing, and vehicle navigation [1]. In this work, we are building an end-to-end pipeline for detecting natural scene categories in Python and reporting our results in the fifteen scene categories dataset [2]. Our pipeline is essentially feature extraction, coding, pooling, and classification steps. However, we are expanding this standard pipeline to include a hybrid feature descriptor that simultaneously uses local feature descriptors and global feature descriptors of varying granularity, with the goal of improving classification accuracy. We are also reducing the time complexity of our encoding steps by using our mini-batch K-Means algorithms instead of the standard visual K-algorithm for determining the kernel words."}, {"heading": "II. METHODOLOGY", "text": "Our scene detection pipeline includes steps for feature extraction, encoding, pooling, and classification. Our model has several tunable parameters such as the size of the visual vocabulary (K), the kernel to be used by the SVM classifier (e.g. linear vs. gauss), and SVM hyperparameters such as C and \u03b3. We use cross-validation to empirically select the optimal hyperparameters. We first empirically discover the optimal K for a linear SVM. We use this K and then perform a grid search to find the optimal C and \u03b3 corresponding to an SVM model with a Radial Base Function (RBF). Details of each of our pipeline steps are presented below."}, {"heading": "A. Feature Extraction", "text": "From each image, we first extract the characteristics of DAISY [3] that correspond to the key points determined from the image. We use the Skimage library (scikit-image) [4] for feature extraction. According to each key point in the image, we extract a DAISY descriptor. We also extract a standard HOG descriptor that corresponds to the overall image in a different granularity (using the parameters pixels per cell, cells per block and orientations), which effectively allows us to select characteristics on different scales. We use the DAISY descriptors extracted from all training images to form a bag of visual words by clustering them using mini-batch KMeans. We empirically determine the best visual vocabulary size (K) by cross validation. ar Xiv: 170 2.06 850v 1 [cs.C V] February 21, 2017"}, {"heading": "B. Encoding", "text": "We use the local DAISY characteristics corresponding to the respective key points to encode each image as a histogram of visual words. Here, we use the standard concept of \"bag-of-visual-words.\" Specifically, we use the K-mean algorithm to quantify DAISY characteristics in \"K\" clusters to form \"visual words\" in a vocabulary. K represents the vocabulary size. In accordance with each image, we form a histogram with \"K\" as dimensionality by using this vocabulary. This encoded representation of the image forms the \"DAISY histogram\" and forms part of our hybrid feature descriptor."}, {"heading": "C. Pooling", "text": "We use a 2-step pooling scheme in our scene detection pipeline. We construct a histogram from the DAISY characteristics by representing the frequency of each visual word in each image. We do this by selecting each key point in the image and looking up the cluster ID that corresponds to that DAISY descriptor, and increasing the number that corresponds to that container in our \"DAISY histogram.\" This procedure is effectively a \"sum pooling.\" We perform an L2 normalization of the resulting histogram to form a DAISY histogram characteristic that we call a \"DAISY histogram.\" For 2-step pooling, we take the global HOG descriptor that corresponds to each image and perform an L2 normalization followed by a concatenation with the corresponding DAISY histogram characteristic to form our hybrid characteristic descriptor."}, {"heading": "D. Classification", "text": "For the SVM implementations, we use the sklearn library (sckitlearn) [5]. We perform cross-validation by randomly splitting the data set into a training and validation set. We create the \"visual vocabulary\" and training function vectors from the training split. We report on the general accuracy, confusion matrix, and standard retrieval statistics such as precision, retrieval, and measurement that correspond to our experiments."}, {"heading": "III. RESULTS", "text": "We conducted our experiments on a standard DELL Precision Tower 5810 workstation running 64-bit Ubuntu 14.04 LTS. Once the execution pipeline described in Section II was built, the optimal size of the visual vocabulary (K) was emperor-ly discovered by running the pipeline 3 times according to each K and averaging its accuracy. A linear SVM model was used for the classification, the optimal value of K was considered to be 600 and the average accuracy over 3 runs to be 73.14%. Results are summarized in Table I and presented in Figure 1."}, {"heading": "IV. DISCUSSION", "text": "We observe in Section III that the use of a hybrid feature descriptor significantly improves the accuracy of our model. However, the relatively poor performance of models that only use HOG descriptors can be attributed to its inability to deal with scale, orientation, or translation. However, HOG descriptors can be combined with dense feature descriptors such as DAISY, which are able to achieve superior results with scaling, orientation, and translation. The two-level pooling scheme used by our model improves the overall result. L2 normalization is performed on the DAISY and HOG descriptors to scale both components so that their concatenation would lead to a meaningful feature descriptor. We also used an approximation technique while building the sack-of-visual-word representation to reduce the algorithmic complexity involved in this process to the number of 8,000 in our DAY experiment."}, {"heading": "V. CONCLUSION", "text": "We implemented an end-to-end scene detection pipeline consisting of feature extraction, encoding, pooling and classification steps using open source libraries in Python. At the same time, we used global feature descriptors as well as local feature descriptors from images to form a hybrid feature descriptor for each image. DAISY descriptors associated with key points in images were used as a local feature descriptor and HOG feature (with a different granularity) according to Fig. 9. For example, an incorrect image 3 of the entire image was used as a global feature descriptor. We used a bag of visual words, the encoded and applied MiniBatch K-Means algorithm to significantly reduce the complexity of our encoding scheme."}], "references": [{"title": "Object tracking: A survey", "author": ["A. Yilmaz", "O. Javed", "M. Shah"], "venue": "Acm computing surveys (CSUR), vol. 38, no. 4, p. 13, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906), vol. 2. IEEE, 2006, pp. 2169\u20132178.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "A fast local descriptor for dense matching", "author": ["E. Tola", "V. Lepetit", "P. Fua"], "venue": "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008, pp. 1\u20138.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "scikit-image: image processing in python", "author": ["S. Van der Walt", "J.L. Sch\u00f6nberger", "J. Nunez-Iglesias", "F. Boulogne", "J.D. Warner", "N. Yager", "E. Gouillart", "T. Yu"], "venue": "PeerJ, vol. 2, p. e453, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg"], "venue": "Journal of Machine Learning Research, vol. 12, no. Oct, pp. 2825\u20132830, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Web-scale k-means clustering", "author": ["D. Sculley"], "venue": "Proceedings of the 19th international conference on World wide web. ACM, 2010, pp. 1177\u2013 1178.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Scene recognition", "author": ["J. Wilson", "M. Arif"], "venue": "2017 (accessed February 20, 2017). [Online]. Available: https://github.com/flytxtds/scene-recognition", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Object recognition is an interesting computer vision problem with diverse applications in a variety of fields such as automated surveillance, robotics, human computer interaction, video indexing and vehicle navigation [1].", "startOffset": 218, "endOffset": 221}, {"referenceID": 1, "context": "In this work, we build an end-to-end pipeline for recognizing natural scene categories in python and report our results on the fifteen scene categories dataset [2].", "startOffset": 160, "endOffset": 163}, {"referenceID": 2, "context": "From each image, we first extract DAISY[3] features corresponding to the key-points detected from the image.", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "We make use of the skimage library (scikit\u2013image) [4] for feature extraction.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "We utilize the sklearn library (sckitlearn) [5] for the SVM implementations.", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "After obtaining the optimal K using a linear SVM, we changed the kernel to RBF and performed a grid search for discovering the optimal C and \u03b3 by varying log(C) within the interval [-3 3] and log(\u03b3) within the interval [-3 2] with the value of K as 600.", "startOffset": 181, "endOffset": 187}, {"referenceID": 1, "context": "After obtaining the optimal K using a linear SVM, we changed the kernel to RBF and performed a grid search for discovering the optimal C and \u03b3 by varying log(C) within the interval [-3 3] and log(\u03b3) within the interval [-3 2] with the value of K as 600.", "startOffset": 219, "endOffset": 225}, {"referenceID": 5, "context": "We made use of the Mini-Batch K-means proposed by David Sculley in his paper titled \u201cWeb-scale k-means clustering\u201d [6].", "startOffset": 115, "endOffset": 118}, {"referenceID": 6, "context": "A full implementation of the proposed model is available in our github repository[7].", "startOffset": 81, "endOffset": 84}], "year": 2017, "abstractText": "Object recognition is an important problem in computer vision, having diverse applications. In this work, we construct an end-to-end scene recognition pipeline consisting of feature extraction, encoding, pooling and classification. Our approach simultaneously utilize global feature descriptors as well as local feature descriptors from images, to form a hybrid feature descriptor corresponding to each image. We utilize DAISY features associated with key points within images as our local feature descriptor and histogram of oriented gradients (HOG) corresponding to an entire image as a global descriptor. We make use of a bag-of-visual-words encoding and apply MiniBatch K-Means algorithm to reduce the complexity of our feature encoding scheme. A 2-level pooling procedure is used to combine DAISY and HOG features corresponding to each image. Finally, we experiment with a multi-class SVM classifier with several kernels, in a cross-validation setting, and tabulate our results on the fifteen scene categories dataset. The average accuracy of our model was 76.4% in the case of a 40%\u201360% random split of images into training and testing datasets respectively. The primary objective of this work is to clearly outline the practical implementation of a basic screne-recognition pipeline having a reasonable accuracy, in python, using open-source libraries. A full implementation of the proposed model is available in our github repository. 1", "creator": "LaTeX with hyperref package"}}}