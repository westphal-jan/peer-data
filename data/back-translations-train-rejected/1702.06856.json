{"id": "1702.06856", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Robustness to Adversarial Examples through an Ensemble of Specialists", "abstract": "We are proposing to use an ensemble of diverse specialists, where speciality is defined according to the confusion matrix. Indeed, we observed that for adversarial instances originating from a given class, labeling tend to be done into a small subset of (incorrect) classes. Therefore, we argue that an ensemble of specialists should be better able to identify and reject fooling instances, with a high entropy (i.e., disagreement) over the decisions in the presence of adversaries. Experimental results obtained confirm that interpretation, opening a way to make the system more robust to adversarial examples through a rejection mechanism, rather than trying to classify them properly at any cost.", "histories": [["v1", "Wed, 22 Feb 2017 15:37:50 GMT  (240kb,D)", "https://arxiv.org/abs/1702.06856v1", "Submitted to ICLR 2017 Workshop Track"], ["v2", "Tue, 28 Feb 2017 05:14:14 GMT  (248kb,D)", "http://arxiv.org/abs/1702.06856v2", "Submitted to ICLR 2017 Workshop Track"], ["v3", "Fri, 10 Mar 2017 04:10:18 GMT  (230kb,D)", "http://arxiv.org/abs/1702.06856v3", "Submitted to ICLR 2017 Workshop Track"]], "COMMENTS": "Submitted to ICLR 2017 Workshop Track", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["mahdieh abbasi", "christian gagn\\'e"], "accepted": false, "id": "1702.06856"}, "pdf": {"name": "1702.06856.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Mahdieh Abbasi", "Christian Gagn\u00e9"], "emails": ["mahdieh.abbasi.1@ulaval.ca,", "christian.gagne@gel.ulaval.ca"], "sections": [{"heading": "ROBUSTNESS TO ADVERSARIAL EXAMPLES THROUGH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "AN ENSEMBLE OF SPECIALISTS", "text": "Mahdieh Abbasi & Christian Gagne \"Computer Vision and Systems Laboratory, Electrical and Computer Engineering Department Universite\" Laval, Que \"bec (Que\" bec), Canada mahdieh.abbasi.1 @ ulaval.ca, christian.gagne @ gel.ulaval.ca"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is a high risk to take, because it is an adequate example, characterized by the addition of small perturbations to the human eye. To mitigate this risk, it has been proposed that it be pure training samples and corresponding adequate examples generated by some existing algorithms."}, {"heading": "2 SPECIALISTS+1 ENSEMBLE", "text": "The confusion matrices of the FGS opponents (fig. 1) show that samples from each class have a high tendency to be deceived in the direction of a limited number of classes. From these confusion matrices of the training opponents, we define subsets of classes, similar to the formation of pure samples. (Considering a classification problem of the K classes (C = {c1, c2, cK}), each line of the confusion matrix is used to identify two subsets of classes: 1) the confusing subset for the class ci (subset Ui), which is built by adding classes in which the confusion values are decreased to cover up to 80% of the confusion matrix, and 2) the remaining classes are formed with less confusion."}, {"heading": "3 EMPIRICAL EVALUATION", "text": "Similar to Hinton et al. (2012), we use CNNs with three convolutionary layers, each with 32, 32 and 64 filters, and a fully connected layer followed by a Softmax. Networks are trained to the usual training and testing kits of MNIST and CIFAR-10, without increasing in size. Also, see the appendix, which contains all the details about the network architecture and hyperparameters used for each dataset. Note that all the CNNs presented in the experiments have an identical architecture. We compared our proposed specialists with a pure ensemble consisting of 5 generalist CNNs with different initializations, and a naive CNN * whose weight initialization differs from GA-CNN (i.e., CNN is used to generate the adversaries)."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was made possible by the support of NSERC-Canada, MITACS and E Machine Learning Inc. Computer resources were provided by Compute Canada / Calcul Que \ufffd bec and a GPU grant from NVIDIA. Annette Schwerdtfeger is also grateful for the proofreading of this manuscript."}, {"heading": "A APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 EXPERIMENTAL PROCEDURES", "text": "We look at a CNN with three revolutionary layers and a fully connected layer, in which each revolutionary layer is networked with ReLU, local contrast normalization, and a pooling layer. To regularize, dropout is used on the last layer, i.e. a fully connected layer with p = 0.5. All hyperparameters such as initial learning rate, training plans, etc. are scaled to [0, 1] according to Hinton et al. (2012).MNIST This dataset contains grayscale images of size 28x28, in which each image contains a handwritten digit. Training and testing kits have 60,000 and 10,000 samples, respectively. All images are scaled to [0, 1]. 150 epochs for training with batch size 128 and initial learning rate 0.1 with dynamics 0.9 are evaluated. The learning rate is decompiled twice during training by a factor of 10 and twice during training, with epochs 50 and C100.IF10 images each consisting of a set of 32GB-10,000 images."}, {"heading": "A.2 GENERATING ADVERSARIES", "text": "Fast Gradient Sign (FGS) (Goodfellow et al., 2014), DeepFool (DF) (Moosavi-Dezfooli et al., 2016) and the algorithm proposed by (Szegedy et al., 2013) are used to generate adversary examples, the latter algorithm finds minimal interference at high computing costs, while FGS and DF generate adversaries much faster, i.e. in less than 3 iterations. GA-CNN (the basic parameter CNN for generating adversaries) is then used to identify correctly classified clean samples and use them to generate adversarial examples. Therefore, 9943 and 8152 adversaries are generated from MNIST and CIFAR-10 test sets, respectively. Optimal values for hyperparameters of FGS and Szegedy et al. (2013) are determined for each dataset in such a way that GA-CNN generates 100% of correctly classified clean samples after the addition of blue turbulences (FGS and Szegedy et al)."}, {"heading": "A.3 EXTRA EXPERIMENTAL RESULTS", "text": "In fact, it is so that most of them are able to unfold in a different way, as if they could unfold in a different way. (...) Most of them are able to unfold in a different way. (...) Most of them are not able to unfold in a different way. (...) Most of them are not able to unfold in a different way. (...) Most of them are not able to unfold in a different way. (...) Most of them are not able to unfold in a different way. (...) Most of them are not able to unfold in a different way. (...) Most of them are not able to unfold in a different way. (...) Most of them are not able to unfold in a different way. (...) Most of them are not able to unfold in a different way. (...) Most of them are not able to unfold in a different way. (...) Most of them are not able to unfold in a different way. (...) Most of them are not able to unfold in a different way."}], "references": [{"title": "Towards open set deep networks", "author": ["Abhijit Bendale", "Terrance E Boult"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Bendale and Boult.,? \\Q2016\\E", "shortCiteRegEx": "Bendale and Boult.", "year": 2016}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Robust convolutional neural networks under adversarial noise", "author": ["Jonghoon Jin", "Aysegul Dundar", "Eugenio Culurciello"], "venue": "arXiv preprint arXiv:1511.06306,", "citeRegEx": "Jin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2015}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["Andrej Karpathy", "George Toderici", "Sanketh Shetty", "Thomas Leung", "Rahul Sukthankar", "Li FeiFei"], "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["Seyed-Mohsen Moosavi-Dezfooli", "Alhussein Fawzi", "Pascal Frossard"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Moosavi.Dezfooli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moosavi.Dezfooli et al\\.", "year": 2016}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Adversarial diversity and hard positive generation", "author": ["Andras Rozsa", "Ethan M Rudd", "Terrance E Boult"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "Rozsa et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rozsa et al\\.", "year": 2016}, {"title": "Adversarial manipulation of deep representations", "author": ["Sara Sabour", "Yanshuai Cao", "Fartash Faghri", "David J Fleet"], "venue": "arXiv preprint arXiv:1511.05122,", "citeRegEx": "Sabour et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sabour et al\\.", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Taigman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "1 INTRODUCTION Due to the recent breakthroughs achieved by Convolutional Neural Networks (CNNs) for various computer vision tasks (He et al., 2015; Taigman et al., 2014; Karpathy et al., 2014), CNNs are highly regarded technology for inclusion into real-life vision applications.", "startOffset": 130, "endOffset": 192}, {"referenceID": 12, "context": "1 INTRODUCTION Due to the recent breakthroughs achieved by Convolutional Neural Networks (CNNs) for various computer vision tasks (He et al., 2015; Taigman et al., 2014; Karpathy et al., 2014), CNNs are highly regarded technology for inclusion into real-life vision applications.", "startOffset": 130, "endOffset": 192}, {"referenceID": 6, "context": "1 INTRODUCTION Due to the recent breakthroughs achieved by Convolutional Neural Networks (CNNs) for various computer vision tasks (He et al., 2015; Taigman et al., 2014; Karpathy et al., 2014), CNNs are highly regarded technology for inclusion into real-life vision applications.", "startOffset": 130, "endOffset": 192}, {"referenceID": 1, "context": "To mitigate this risk, it has been proposed to train CNNs on both clean training samples and corresponding adversarial examples, generated by some existing algorithms (Goodfellow et al., 2014; Szegedy et al., 2013; Moosavi-Dezfooli et al., 2016; Sabour et al., 2015).", "startOffset": 167, "endOffset": 266}, {"referenceID": 11, "context": "To mitigate this risk, it has been proposed to train CNNs on both clean training samples and corresponding adversarial examples, generated by some existing algorithms (Goodfellow et al., 2014; Szegedy et al., 2013; Moosavi-Dezfooli et al., 2016; Sabour et al., 2015).", "startOffset": 167, "endOffset": 266}, {"referenceID": 7, "context": "To mitigate this risk, it has been proposed to train CNNs on both clean training samples and corresponding adversarial examples, generated by some existing algorithms (Goodfellow et al., 2014; Szegedy et al., 2013; Moosavi-Dezfooli et al., 2016; Sabour et al., 2015).", "startOffset": 167, "endOffset": 266}, {"referenceID": 10, "context": "To mitigate this risk, it has been proposed to train CNNs on both clean training samples and corresponding adversarial examples, generated by some existing algorithms (Goodfellow et al., 2014; Szegedy et al., 2013; Moosavi-Dezfooli et al., 2016; Sabour et al., 2015).", "startOffset": 167, "endOffset": 266}, {"referenceID": 9, "context": "To increase the robustness of CNNs, it has been proposed to train them on a diverse set of adversaries, generating adversarial examples for any single images with various algorithms (Rozsa et al., 2016).", "startOffset": 182, "endOffset": 202}, {"referenceID": 5, "context": "Moreover, training on some type of adversaries has been demonstrating to harm the performance on clean test samples (Jin et al., 2015; Moosavi-Dezfooli et al., 2016).", "startOffset": 116, "endOffset": 165}, {"referenceID": 7, "context": "Moreover, training on some type of adversaries has been demonstrating to harm the performance on clean test samples (Jin et al., 2015; Moosavi-Dezfooli et al., 2016).", "startOffset": 116, "endOffset": 165}, {"referenceID": 1, "context": "However, as mentioned by the authors, the method fails to detect hard adversaries where the target class and the true class of an adversary are close together, like those generated by Fast Gradient Sign (FGS) (Goodfellow et al., 2014) and DeepFool (DF) (Moosavi-Dezfooli et al.", "startOffset": 209, "endOffset": 234}, {"referenceID": 7, "context": ", 2014) and DeepFool (DF) (Moosavi-Dezfooli et al., 2016).", "startOffset": 26, "endOffset": 57}, {"referenceID": 1, "context": "To mitigate this risk, it has been proposed to train CNNs on both clean training samples and corresponding adversarial examples, generated by some existing algorithms (Goodfellow et al., 2014; Szegedy et al., 2013; Moosavi-Dezfooli et al., 2016; Sabour et al., 2015). Although such trained CNNs are robust to specific types of adversaries, they are not necessarily protected from all possible types. To increase the robustness of CNNs, it has been proposed to train them on a diverse set of adversaries, generating adversarial examples for any single images with various algorithms (Rozsa et al., 2016). However, it is still possible to produce other types of adversaries, uncovered by the current set, impacting significantly the reliability of CNNs. Moreover, training on some type of adversaries has been demonstrating to harm the performance on clean test samples (Jin et al., 2015; Moosavi-Dezfooli et al., 2016). We are rather considering recognition of adversarial examples as an open set recognition problem, where unknown samples should be detected and rejected by the underlying models. Bendale & Boult (2016) have adapted CNNs by adding an extra layer designed to recognize the unknown samples, which can be either from unknown classes or fooling adversarial instances from Nguyen et al.", "startOffset": 168, "endOffset": 1120}, {"referenceID": 1, "context": "To mitigate this risk, it has been proposed to train CNNs on both clean training samples and corresponding adversarial examples, generated by some existing algorithms (Goodfellow et al., 2014; Szegedy et al., 2013; Moosavi-Dezfooli et al., 2016; Sabour et al., 2015). Although such trained CNNs are robust to specific types of adversaries, they are not necessarily protected from all possible types. To increase the robustness of CNNs, it has been proposed to train them on a diverse set of adversaries, generating adversarial examples for any single images with various algorithms (Rozsa et al., 2016). However, it is still possible to produce other types of adversaries, uncovered by the current set, impacting significantly the reliability of CNNs. Moreover, training on some type of adversaries has been demonstrating to harm the performance on clean test samples (Jin et al., 2015; Moosavi-Dezfooli et al., 2016). We are rather considering recognition of adversarial examples as an open set recognition problem, where unknown samples should be detected and rejected by the underlying models. Bendale & Boult (2016) have adapted CNNs by adding an extra layer designed to recognize the unknown samples, which can be either from unknown classes or fooling adversarial instances from Nguyen et al. (2015). However, as mentioned by the authors, the method fails to detect hard adversaries where the target class and the true class of an adversary are close together, like those generated by Fast Gradient Sign (FGS) (Goodfellow et al.", "startOffset": 168, "endOffset": 1306}, {"referenceID": 1, "context": "To mitigate this risk, it has been proposed to train CNNs on both clean training samples and corresponding adversarial examples, generated by some existing algorithms (Goodfellow et al., 2014; Szegedy et al., 2013; Moosavi-Dezfooli et al., 2016; Sabour et al., 2015). Although such trained CNNs are robust to specific types of adversaries, they are not necessarily protected from all possible types. To increase the robustness of CNNs, it has been proposed to train them on a diverse set of adversaries, generating adversarial examples for any single images with various algorithms (Rozsa et al., 2016). However, it is still possible to produce other types of adversaries, uncovered by the current set, impacting significantly the reliability of CNNs. Moreover, training on some type of adversaries has been demonstrating to harm the performance on clean test samples (Jin et al., 2015; Moosavi-Dezfooli et al., 2016). We are rather considering recognition of adversarial examples as an open set recognition problem, where unknown samples should be detected and rejected by the underlying models. Bendale & Boult (2016) have adapted CNNs by adding an extra layer designed to recognize the unknown samples, which can be either from unknown classes or fooling adversarial instances from Nguyen et al. (2015). However, as mentioned by the authors, the method fails to detect hard adversaries where the target class and the true class of an adversary are close together, like those generated by Fast Gradient Sign (FGS) (Goodfellow et al., 2014) and DeepFool (DF) (Moosavi-Dezfooli et al., 2016). We are proposing to use an ensemble of diverse specialists, where speciality is defined according to the confusion matrix. Indeed, we observed that for adversarial instances originating from a given class, labeling tend to be done into a small subset of (incorrect) classes. Therefore, we argue that an ensemble of specialists should be better able to identify and reject fooling instances, with a high entropy (i.e., disagreement) over the decisions in the presence of adversaries. Experimental results obtained confirm this interpretation that a rejection mechanism can provide a means of rendering the system more robust to adversarial examples, rather than trying to classify them properly at any cost. 2 SPECIALISTS+1 ENSEMBLE Ensemble construction The confusion matrices of FGS adversaries (Fig. 1) reveals that samples from each class have a high tendency of being fooled toward a limited number of classes. From these confusion matrices of training adversaries we define subsets of classes, similarly to Hinton et al. (2015) on training clean samples.", "startOffset": 168, "endOffset": 2626}, {"referenceID": 3, "context": "3 EMPIRICAL EVALUATION Networks and datasets Similarly to Hinton et al. (2012), we used CNNs with three convolutional layers having 32, 32, and 64 filters respectively, and a fully-connected layer followed by a softmax.", "startOffset": 58, "endOffset": 79}, {"referenceID": 11, "context": "Using this GA-CNN, three types of adversaries, namely FGS, DF, and Szegedy et al. (2013) adversaries, are generated for correctly classified clean test samples.", "startOffset": 67, "endOffset": 89}, {"referenceID": 1, "context": "2 GENERATING ADVERSARIES Fast Gradient Sign (FGS) (Goodfellow et al., 2014), DeepFool (DF) (Moosavi-Dezfooli et al.", "startOffset": 50, "endOffset": 75}, {"referenceID": 7, "context": ", 2014), DeepFool (DF) (Moosavi-Dezfooli et al., 2016), and the algorithm proposed by (Szegedy et al.", "startOffset": 23, "endOffset": 54}, {"referenceID": 11, "context": ", 2016), and the algorithm proposed by (Szegedy et al., 2013) are used for adversarial example generation.", "startOffset": 39, "endOffset": 61}, {"referenceID": 2, "context": "All of the hyper parameters such as initial learning rate, training schedule, and so on are set according to Hinton et al. (2012). MNIST This dataset contains grayscale images of size 28x28, where each image holds a handwritten digit.", "startOffset": 109, "endOffset": 130}, {"referenceID": 1, "context": "2 GENERATING ADVERSARIES Fast Gradient Sign (FGS) (Goodfellow et al., 2014), DeepFool (DF) (Moosavi-Dezfooli et al., 2016), and the algorithm proposed by (Szegedy et al., 2013) are used for adversarial example generation. The latter algorithm finds minimum required perturbations at a high computational cost, while FGS and DF generates adversaries significantly faster, i.e., less than 3 iterations. Using the GA-CNN (the baseline CNN for generating adversaries), correctly classified clean samples are identified then used for generating adversarial examples. Therefore, 9943 and 8152 adversaries are generated from MNIST and CIFAR-10 test sets, respectively. The optimal values for hyper parameters of FGS and Szegedy et al. (2013) are obtained for each dataset such that GA-CNN misclassifies 100% of the correctly classified clean samples after adding perturbations.", "startOffset": 51, "endOffset": 735}, {"referenceID": 11, "context": "Figure 4: Average distortion to MNIST and CIFAR-10 samples by FGS, DF, and Szegedy et al. (2013). The average misclassification confidences are shown by blue text.", "startOffset": 75, "endOffset": 97}, {"referenceID": 11, "context": "their cross-model generalization property, while adversaries by Szegedy et al. (2013) are less generalized across different models.", "startOffset": 64, "endOffset": 86}, {"referenceID": 11, "context": "(d) EA(%) on MNIST adversaries by Szegedy et al. (2013)", "startOffset": 34, "endOffset": 56}, {"referenceID": 11, "context": "(h) EA(%) on CIFAR-10 adversaries by Szegedy et al. (2013) Figure 5: Error rates ED on clean test samples, and error rates EA on their corresponding adversaries, as a function of threshold (\u03c4 ), for the MNIST and CIFAR-10 datasets.", "startOffset": 37, "endOffset": 59}], "year": 2017, "abstractText": "Due to the recent breakthroughs achieved by Convolutional Neural Networks (CNNs) for various computer vision tasks (He et al., 2015; Taigman et al., 2014; Karpathy et al., 2014), CNNs are highly regarded technology for inclusion into real-life vision applications. However, CNNs have a high risk of failing due to adversarial examples, which fool them consistently with the addition of small perturbations to natural images, undetectable by the human eyes.", "creator": "LaTeX with hyperref package"}}}