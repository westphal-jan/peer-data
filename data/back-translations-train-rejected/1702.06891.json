{"id": "1702.06891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "EVE: Explainable Vector Based Embedding Technique Using Wikipedia", "abstract": "We present an unsupervised explainable word embedding technique, called EVE, which is built upon the structure of Wikipedia. The proposed model defines the dimensions of a semantic vector representing a word using human-readable labels, thereby it readily interpretable. Specifically, each vector is constructed using the Wikipedia category graph structure together with the Wikipedia article link structure. To test the effectiveness of the proposed word embedding model, we consider its usefulness in three fundamental tasks: 1) intruder detection - to evaluate its ability to identify a non-coherent vector from a list of coherent vectors, 2) ability to cluster - to evaluate its tendency to group related vectors together while keeping unrelated vectors in separate clusters, and 3) sorting relevant items first - to evaluate its ability to rank vectors (items) relevant to the query in the top order of the result. For each task, we also propose a strategy to generate a task-specific human-interpretable explanation from the model. These demonstrate the overall effectiveness of the explainable embeddings generated by EVE. Finally, we compare EVE with the Word2Vec, FastText, and GloVe embedding techniques across the three tasks, and report improvements over the state-of-the-art.", "histories": [["v1", "Wed, 22 Feb 2017 16:50:25 GMT  (776kb,D)", "http://arxiv.org/abs/1702.06891v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["m atif qureshi", "derek greene"], "accepted": false, "id": "1702.06891"}, "pdf": {"name": "1702.06891.pdf", "metadata": {"source": "CRF", "title": "EVE: Explainable Vector Based Embedding Technique Using Wikipedia", "authors": ["M. Atif Qureshi", "Derek Greene"], "emails": ["muhammad.qureshi@ucd.ie", "derek.greene@ucd.ie"], "sections": [{"heading": null, "text": "Distribution semantics \u00b7 Unsupervised learning \u00b7 Wikipedia"}, {"heading": "1 Introduction", "text": "This year it has come to the point where we see ourselves able to put ourselves at the top, \"he said in an interview with the German Press Agency.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said."}, {"heading": "2 Related Work", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "3.2.1 Vector dimensions related to Wikipedia articles", "text": "First, we define the strategy for generating vector dimensions corresponding to individual Wikipedia articles. In view of the target concept associated with a Wikipedia article called Aconcept, we list all incoming links and outgoing links between this article and all other articles. Then, we create a dimension corresponding to each of these linked articles, defining the strength of the association for one dimension as the sum of the incoming and outgoing links comprising an article and Aconcept. Once we have created dimensions for all linked articles, 1 this may be an exact match or partial best match using an information retrieval aluation algorithm. In the first step, all inlinks and outlinks are counted for the other non-concept articles (e.g. Aconcept has 3 inlinks and 1 outlink from A3). In the next step, Figure 2 shows an example of the strategy. In the second step, all inlinks and outlinks are counted for the other non-concept articles."}, {"heading": "3.2.2 Vector dimensions related to Wikipedia categories", "text": "Next, we define the method for generating vector dimensions that correspond to all Wikipedia categories related to the concept article. 2. The strategy for mapping a score to related Wikipedia categories is as follows: 1. Let's start by spreading the score evenly to the categories to which the concept article belongs (see Fig. 1). 2. Part of the score is propagated by the likelihood of leaping from a category to the categories in the neighborhood. 3. The spread of the score continues until a certain score is reached (i.e., a threshold for the category depth), or there are no other categories in the neighborhood. 2. This dimension defines the most relevant dimension that defines the concept that is the item itself. 3. In the case of the best match strategy, where more than one item is mapped to a concept, i.e., Aconcept1, Aconcept2, Aconcept2, the score calculated is determined by the Relevance Score-Score-Score-Score-Score-Score-itself."}, {"heading": "3.2.3 Overall vector dimensions", "text": "Once the dimension sets for related Wikipedia articles and categories have been created, we construct an overall vector for the concept article as follows: Equation 1 shows the vector representation of a concept, where norm is a normalization function, article score and category score are the two dimension sets, while bias articles and bias categories are the bias categories that control the meaning of associations with Wikipedia articles or categories. Biascategory weights can be set to attach more importance to each type of association. In Equation 2, we normalize the entire vector so that the sum of the results of all dimensions 4 In the case of partial best match, it is the relevance value returned by the BM25 algorithm. Equated to 1, so that a length vector is achieved."}, {"heading": "4 Evaluation", "text": "In this section, we will examine the extent to which the EVE model is used in three basic categories. First, we will describe a number of alternative methods, along with the relevant parameters. Then, we will describe the data sets used for the assessments, and finally, we will discuss the effectiveness of the model. We will also highlight the benefits of the explanations generated in this process."}, {"heading": "4.3.1 Experiment 1: Intruder detection", "text": "This year, it has reached the point that it will be able to reactivate the ones mentioned, but is not yet able to do so."}, {"heading": "4.3.2 Experiment 2: Ability to cluster", "text": "In this experiment, we evaluate the extent to which the distances calculated on EVE embedding are divided into five categories to capture the semantically related items while separating other items. This is a basic prerequisite for distance-based methods of cluster analysis. This is done by measuring distances in the space between items that should belong together (i.e. intra-cluster distances) and items that should be kept apart (i.e. inter-cluster distances) as determined by the categories. Since there are seven \"topical types,\" there are also queries in this task.Example of a query: For the \"topical type,\" we are provided with a list of 100 items in total, where each of the five categories has to answer 20 items."}, {"heading": "4.3.3 Experiment 3: Sorting relevant items first", "text": "In fact, it is the case that most of them are in a position to move into a different world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they are able"}, {"heading": "5 Conclusion and Future Directions", "text": "In this paper, we presented a novel technique, EVE, for generating vector representations of words using information from Wikipedia. This work represents a first step toward explainable word embeddings, the core of which lies in the use of labeled vector dimensions that correspond to either Wikipedia categories or Wikipedia articles. We have shown that the resulting embeddings are not only useful for basic data mining tasks, but that providing labeled dimensions willingly supports the generation of task-specific explanations through simple vector operations. We are not arguing that embeddings that are generated on structured data as generated by the EVE model would replace the prevailing Word embeddedness models. Rather, we have shown that using structured data can offer additional benefits that go beyond the existing approaches. An interesting aspect to consider in the future would be the use of hybrid text, the unstructured data, and the structured aspects that have been proposed in this work."}], "references": [{"title": "A (2016) A latent variable model approach to pmi-based word embeddings", "author": ["S Arora", "Y Li", "Y Liang", "T Ma", "Risteski"], "venue": "Tr Assoc Computational Linguistics", "citeRegEx": "Arora et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["M Baroni", "A Lenci"], "venue": null, "citeRegEx": "Baroni and Lenci,? \\Q2010\\E", "shortCiteRegEx": "Baroni and Lenci", "year": 2010}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["M Baroni", "G Dinu", "G Kruszewski"], "venue": "ACL", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Y Bengio", "R Ducharme", "P Vincent", "C Jauvin"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Knowledge-powered deep learning for word embedding", "author": ["J Bian", "B Gao", "TY Liu"], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Bian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bian et al\\.", "year": 2014}, {"title": "Enriching word vectors with subword information", "author": ["P Bojanowski", "E Grave", "A Joulin", "T Mikolov"], "venue": null, "citeRegEx": "Bojanowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A Bordes", "J Weston", "R Collobert", "Y Bengio"], "venue": "In: Conference on artificial intelligence,", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Evaluating wordnet-based measures of lexical semantic relatedness", "author": ["A Budanitsky", "G Hirst"], "venue": "Computational Linguistics", "citeRegEx": "Budanitsky and Hirst,? \\Q2006\\E", "shortCiteRegEx": "Budanitsky and Hirst", "year": 2006}, {"title": "A dendrite method for cluster analysis. Communications in Statistics-theory and Methods", "author": ["T Cali\u0144ski", "J Harabasz"], "venue": null, "citeRegEx": "Cali\u0144ski and Harabasz,? \\Q1974\\E", "shortCiteRegEx": "Cali\u0144ski and Harabasz", "year": 1974}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R Collobert", "J Weston"], "venue": "Proc. ICML\u20192008,", "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Query expansion with locally-trained word embeddings", "author": ["Qureshi M. Atif", "F Derek Greene Diaz", "B Mitra", "N Craswell"], "venue": null, "citeRegEx": "Atif et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Atif et al\\.", "year": 2016}, {"title": "A synopsis of linguistic theory 1930-1955. Studies in linguistic analysis pp 1\u201332", "author": ["B Everitt", "S Landau", "M Leese"], "venue": "Cluster Analysis. Hodder Arnold Publication, Wiley Firth J", "citeRegEx": "Everitt et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Everitt et al\\.", "year": 2001}, {"title": "Euclidean embedding of co-occurrence", "author": ["A Globerson", "G Chechik", "F Pereira", "N Tishby"], "venue": "SIGIR Forum, ACM,", "citeRegEx": "Globerson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2007}, {"title": "2016) European union regulations on algorithmic decision-making", "author": ["B Goodman", "S Flaxman"], "venue": null, "citeRegEx": "Goodman and Flaxman,? \\Q2016\\E", "shortCiteRegEx": "Goodman and Flaxman", "year": 2016}, {"title": "a\u201d right to explanation", "author": ["J Hoffart", "S Seufert", "DB Nguyen", "M Theobald", "G Weikum"], "venue": "arXiv preprint arXiv:160608813 Harris ZS", "citeRegEx": "Hoffart et al\\.,? \\Q1954\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 1954}, {"title": "Feature-based approaches to semantic similarity", "author": ["Y Jiang", "X Zhang", "Y Tang", "R Nie"], "venue": null, "citeRegEx": "Jiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2015}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O Levy", "Y Goldberg"], "venue": "Proc. ICML\u20192015,", "citeRegEx": "Levy and Goldberg,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Ramat-Gan I (2014) Linguistic regularities in sparse and explicit word", "author": ["O Levy", "Y Goldberg"], "venue": null, "citeRegEx": "Levy and Goldberg,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Improving distributional similarity with lessons learned", "author": ["O Levy", "Y Goldberg", "I Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Topical word embeddings. In: AAAI, pp 2418\u20132424", "author": ["Y Liu", "Z Liu", "TS Chua", "M Sun"], "venue": "Maaten Lvd, Hinton G", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations", "author": ["T Mikolov", "K Chen", "G Corrado", "J Dean"], "venue": "European Conference on Information Retrieval,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J Pennington", "R Socher", "CD Manning"], "venue": "words and phrases and their compositionality. In: Proc. NIPS\u20192013,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Vector embedding of wikipedia concepts and entities", "author": ["H Ireland Galway Sch\u00fctze"], "venue": "Word space. In: Proc. NIPS\u20191992,", "citeRegEx": "Sch\u00fctze,? \\Q1992\\E", "shortCiteRegEx": "Sch\u00fctze", "year": 1992}, {"title": "A (2013) Reasoning with neural tensor networks", "author": ["R Socher", "D Chen", "CD Manning", "Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Rc-net: A general framework", "author": ["Synergy", "C pp 25\u201330 Xu", "Y Bai", "J Bian", "B Gao", "G Wang", "X Liu", "TY Liu"], "venue": null, "citeRegEx": "Synergy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Synergy et al\\.", "year": 2014}, {"title": "Analysis of the wikipedia category graph for nlp applications", "author": ["T Zesch", "I Gurevych"], "venue": "Natural Language Processing,", "citeRegEx": "Zesch and Gurevych,? \\Q2007\\E", "shortCiteRegEx": "Zesch and Gurevych", "year": 2007}], "referenceMentions": [{"referenceID": 13, "context": "Recently the European Union has approved a regulation which requires that citizens have a \u201cright to explanation\u201d in relation to any algorithmic decision-making (Goodman and Flaxman 2016).", "startOffset": 160, "endOffset": 186}, {"referenceID": 16, "context": "To generate these vectors, a number of unsupervised techniques have been proposed which includes applying neural networks (Mikolov et al 2013a,b; Bojanowski et al 2016), constructing a co-occurrence matrix followed by dimensionality reduction (Levy and Goldberg 2014; Pennington et al 2014), probabilistic models (Globerson et al 2007; Arora et al 2016), and explicit representation of words appearing in a context (Levy et al 2014, 2015).", "startOffset": 243, "endOffset": 290}, {"referenceID": 1, "context": "Concretely, distributional semantic models (DSMs) keep count-based vectors corresponding to co-occurring words, followed by a transformation of the vectors via weighting schemes or dimensionality reduction (Baroni and Lenci 2010; Gallant et al 1992; Sch\u00fctze 1992).", "startOffset": 206, "endOffset": 263}, {"referenceID": 22, "context": "Concretely, distributional semantic models (DSMs) keep count-based vectors corresponding to co-occurring words, followed by a transformation of the vectors via weighting schemes or dimensionality reduction (Baroni and Lenci 2010; Gallant et al 1992; Sch\u00fctze 1992).", "startOffset": 206, "endOffset": 263}, {"referenceID": 9, "context": "A new family of methods, generally known as \u201cword embeddings\u201d, learns word representations in a vector space, where vector weights are set to maximize the probability of the contexts in which the word is observed in the corpus (Bengio et al 2003; Collobert and Weston 2008).", "startOffset": 227, "endOffset": 273}, {"referenceID": 7, "context": "Both Budanitsky and Hirst (2006) and Jarmasz (2012) used generalization (\u2018is a\u2019) relations between words using WordNet-based techniques; Metzler et al (2007) used", "startOffset": 5, "endOffset": 33}, {"referenceID": 7, "context": "Both Budanitsky and Hirst (2006) and Jarmasz (2012) used generalization (\u2018is a\u2019) relations between words using WordNet-based techniques; Metzler et al (2007) used", "startOffset": 5, "endOffset": 52}, {"referenceID": 7, "context": "Both Budanitsky and Hirst (2006) and Jarmasz (2012) used generalization (\u2018is a\u2019) relations between words using WordNet-based techniques; Metzler et al (2007) used", "startOffset": 5, "endOffset": 158}, {"referenceID": 25, "context": "These Wikipedia categories serve as a semantic tag for the articles to which they link (Zesch and Gurevych 2007).", "startOffset": 87, "endOffset": 112}, {"referenceID": 8, "context": "Finally, the two above objectives are combined via the CH-Index (Cali\u0144ski and Harabasz 1974), using the ratio:", "startOffset": 64, "endOffset": 92}], "year": 2017, "abstractText": "We present an unsupervised explainable word embedding technique, called EVE, which is built upon the structure of Wikipedia. The proposed model defines the dimensions of a semantic vector representing a word using humanreadable labels, thereby it readily interpretable. Specifically, each vector is constructed using the Wikipedia category graph structure together with the Wikipedia article link structure. To test the effectiveness of the proposed word embedding model, we consider its usefulness in three fundamental tasks: 1) intruder detection \u2014 to evaluate its ability to identify a non-coherent vector from a list of coherent vectors, 2) ability to cluster \u2014 to evaluate its tendency to group related vectors together while keeping unrelated vectors in separate clusters, and 3) sorting relevant items first \u2014 to evaluate its ability to rank vectors (items) relevant to the query in the top order of the result. For each task, we also propose a strategy to generate a task-specific human-interpretable explanation from the model. These demonstrate the overall effectiveness of the explainable embeddings generated by EVE. Finally, we compare EVE with the Word2Vec, FastText, and GloVe embedding techniques across the three tasks, and report improvements over the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}