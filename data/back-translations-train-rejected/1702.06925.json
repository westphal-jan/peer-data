{"id": "1702.06925", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Regularizing Face Verification Nets For Pain Intensity Regression", "abstract": "Limited annotated data is available for the research of estimating facial expression intensities, which makes the training of deep networks for automated expression assessment very challenging. Fortunately, fine-tuning from a data-extensive pre-trained domain such as face verification can alleviate the problem. In this paper, we propose a transferred network that fine-tunes a state-of-the-art face verification network using expression-intensity labeled data with a regression layer. In this way, the expression regression task can benefit from the rich feature representations trained on a huge amount of data for face verification. The proposed transferred deep regressor is applied in estimating the intensity of facial action units (2017 EmotionNet Challenge) and in particular pain intensity estimation (UNBS-McMaster Shoulder-Pain dataset). It wins the second place in the challenge and achieves the state-of-the-art performance on Shoulder-Pain dataset. Particularly for Shoulder-Pain with the imbalance issue of different pain levels, a new weighted evaluation metric is proposed.", "histories": [["v1", "Wed, 22 Feb 2017 18:15:42 GMT  (1234kb,D)", "http://arxiv.org/abs/1702.06925v1", "5 pages, 3 figure; submitted to IEEE ICIP 2017 which does not perform blind reviews"], ["v2", "Tue, 9 May 2017 17:58:58 GMT  (1235kb,D)", "http://arxiv.org/abs/1702.06925v2", "5 pages, 3 figure; To appear in IEEE ICIP 2017"], ["v3", "Thu, 1 Jun 2017 17:49:56 GMT  (1228kb,D)", "http://arxiv.org/abs/1702.06925v3", "5 pages, 3 figure; Camera-ready version to appear at IEEE ICIP 2017"]], "COMMENTS": "5 pages, 3 figure; submitted to IEEE ICIP 2017 which does not perform blind reviews", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG cs.MM", "authors": ["feng wang", "xiang xiang", "chang liu", "trac d tran", "austin reiter", "gregory d hager", "harry quon", "jian cheng", "alan l yuille"], "accepted": false, "id": "1702.06925"}, "pdf": {"name": "1702.06925.pdf", "metadata": {"source": "CRF", "title": "TRANSFERRING FACE VERIFICATION NETS TO PAIN AND EXPRESSION REGRESSION", "authors": ["Feng Wang", "Xiang Xiang", "Chang Liu", "Trac D. Tran", "Austin Reiter", "Gregory D. Hager", "Harry Quon", "Jian Cheng", "Alan L. Yuille"], "emails": ["xxiang@cs.jhu.edu)."], "sections": [{"heading": null, "text": "Index Terms - CNN, Regression, Intensity of Expression"}, {"heading": "1. INTRODUCTION", "text": "In fact, the fact is that most of them are only a pure formation, able to move around without being able to achieve their goals."}, {"heading": "2. RELATED WORKS", "text": "In the recognition of facial expression in general, there is a trade-off between the simplicity of the method and performance, i.e., image-based [6, 7] and video-based [8, 9, 10] methods. Since videos are sequential signals, visual methods, including ours, cannot model the dynamics of a temporal model [11] or spatio-temporal models [12, 13, 14]. Linear models include sparse representation methods, ordinal regression [4, 15, 16] and boosting [17]. A similar trade-off lies in the linear model versus non-linear models. Among the non-linear models, one approach consists of kernel-based methods [18], while another consists of deep learning [5, 10, 19, 20, 21]. By introducing further information, there is an approach in 3D models [22], while another in multimodal models [23]. In terms of the transfer of network-based learning networks with neural networks, the facial expression is more complete, the network-regulated, the network-based work with neural expression."}, {"heading": "3. TRANSFERRED DEEP REGRESSIOR", "text": "A face f is generated visually by confusion factors whose primary problem is the identity i, the facial expression e and the head represent p. Normally, given a number of rich training examples (f, i), deep face checking algorithms [24] seek a function g: F \u2192 I, where F is the input space spanned by all possible facial expressions, and I am the output space formed by all possible identities. Now, with g and limited training samples (f, e), we examine how g and h can relate, since they are mappings from the same output space to another output space. Namely, we design a network to learn h using a few additional training examples of (f, e) in the face of g. Our network is transferred from a state-of-the-art facial characteristic to another output space."}, {"heading": "3.1. Regression Loss", "text": "At the end of the network, we use both '1 and' 2 standard spacing to complete the regression task; y standard is the soil truth designation and p denotes which standard should be used. In practice, the choice between '1 standard or' 2 standard depends on the assessment scale, e.g. '2 standard performs better with Mean Square Error (MSE) and' 1 standard performs better with Mean Abosolute Error (MAE). In practice, however, we have found that the 1 model at https: / / github.com / ydwen / caffe-facedirect does not work due to the large gradient magnitude - this phenomenon is also described in [26]. To solve this problem, we must apply the '2 standard spacing layer' directly if the '1 standard deviation layer' does not work due to the large gradient magnitude."}, {"heading": "3.2. Regularizing by Reducing Inter-Class Variance", "text": "Since the pain intensity level is specified discretely in the shoulder pain dataset, it is natural to add some types of classification signals to the loss function in order to make the regressive values more \"discrete.\" In this work, we try two types of classification loss functions, one is the Softmax, LS = \u2212 log WTy x + by n i = 1 W T i x + bi, (3) another is the center loss [24], LC = \u2212 x \u2212 cy pp, (4) where x is the output of the second to last layer, y is the corresponding label, cy represent the center for class y and p is the norm to be selected. Classification losses are added after the first fully connected layer, which affects the training of the neural network simultaneously with the regression loss. Deviating from the description of the center loss in [24], we jointly learn the centers and minimize the distances within the classes by grading soft loss, during the middle [24]."}, {"heading": "4. EXPERIMENTS", "text": "In this section we present implementations and experiments. Project site2 was set up with programs and data."}, {"heading": "4.1. Dataset and Training Details", "text": "We are testing our network using the UNBC-McMaster Shoulder Pain dataset [1], widely used for benchmarking the intensity of pain expressions in particular and facial expression units in general. It contains four types of labels, the three labels commented online during the video collection are the sensory scale (SEN), the affective scale (AFF), and the visual analog scale (VAS), which ranges from 0 (no pain) to 15 (severe pain). Additionally, observers evaluated the pain intensity (OPI) offline from recorded videos from 0 (no pain) to 5 (severe pain). Similar to previous work [4, 5, 27] we are using the same online label and quantifying the initial level of pain in the range [0, 15] as in the range [0, 5]. The facial verification network [24] is trained on the basis of the CASIAWeb dataset [10.5, 27] which contains 4.475 school identification images from 4949."}, {"heading": "4.2. Pain Intensity Regression", "text": "Each time, the videos of a patient2https: / / github.com / happynear / PainRegression.are reserved for the test. All other videos are used to train the deep regression network.The performance is summarized in Table 1. From the table, it can be concluded that our algorithm performs best or best on various evaluation variables, especially the combination of gentle '1 loss and' 1 center loss. Although the MAE is also competitive in using Softmax loss as regularization, we find that it only learns to predict more zeros by observing the predicted curve."}, {"heading": "4.3. Class Imbalance Problem", "text": "In Table 1, we provide the power of predicting all zeros as a starting point. Interestingly, however, the shoulder-pain dataset on the measurements MAE and MSE is highly unbalanced. 91.35% of the measurements are labeled as pain level 0. Therefore, it is relatively safe for the regression algorithms to predict the problem of data imbalance to zero. To evaluate the performance fairly, we suggest the weighted version of the measurements, i.e. weighted MAE (wMAE) and MSE (wMSE), to solve the problem of data imbalance. Thus, the wMAE is simply the mean of the MAE at each pain level. In this way, the MAE is weighted by the baseline of each pain level. We apply two techniques to stitch the training data to make our training program more consistent with the new measurements. First, we eliminate the mesh values at the next level, if we want to be consistent at the following level 5."}, {"heading": "4.4. Facial Action Unit Recognition in General", "text": "Replacing the regression layer with a Softmax layer, we apply our proposed method to the EmotionNet Challenge3, a face recognition task organized by the Ohio State University CBCSL. The competition consists of two tracks. The first requires the detection of 11 Action Units (AUs), and the second is an emotion detection task. 26,117 and 2,477 labeled images, respectively, are provided by the organizer for the training of the two tracks. Predictions generated by our program are submitted to the organizer and he further evaluates the results of our program using the previously defined metrics. In the AU detection task, we also came in second place with averaged metrics of 0.7101, while the best value is 0.7290. Our individual F scores are all at the top (F1 is 0.6405, F2 is 0.6354, and F.5 is 0.6380). In the emotion detection task, we also came in second place with averaged metrics of 0.72101, while the best value is 0.7290. Our final result is 0.99, while the additional 4768 is worth noting that the top 0.5968 is the company."}, {"heading": "5. SUMMARY", "text": "Given the limitation of marked data that prevents us from directly training a low pain intensity regressor, fine tuning from a data-rich, pre-trained area such as identities can ease the problem. In this post, we transmit a facial verification network for pain intensity regression. The finely tuned transmitted network with a regression layer is tested on the UNBC-McMaster Shoulder Pain dataset and achieves peak performance in estimating pain intensity. Details and results of the challenge can be found at http: / / cbcsl.ece. ohio-state.edu / EmotionNetChallenge /."}, {"heading": "6. REFERENCES", "text": "[1] Patrick Lucey, Jeffrey F Cohn, Kenneth M Prkachin, Patricia E Solomon, and Iain Matthews, \"Painful data: The unbcmcmaster shoulder pain expression archive database,\" in IEEE International Conference on Automatic Face & Gesture Wang facial recognition, 2011, pp. 57-64. [2] Min Aung, Sebastian Kaltwang, Bernardino Romera-Paredes, Brais Martinez, Aneesha Singh, Matteo Cella, Michel Valstar, Hongying Meng facial expression, Andrew Kemp, Aaron Elkins, et al. [Automatic recognition of chronic pain related expression: requirements, challenges, and a multimodal dataset, \"IEEE Trans. Affective Computing, Pavo facial expression, Andrew Kemp, Aaron Elkins, et al.]. [Requirements, challenges, and a multimodal dataset.\""}], "references": [{"title": "Painful data: The unbcmcmaster shoulder pain expression archive database", "author": ["Patrick Lucey", "Jeffrey F Cohn", "Kenneth M Prkachin", "Patricia E Solomon", "Iain Matthews"], "venue": "IEEE International Conference on Automatic Face & Gesture Recognition, 2011, pp. 57\u201364.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "The automatic detection of chronic pain-related expression: requirements, challenges and a multimodal dataset", "author": ["Min Aung", "Sebastian Kaltwang", "Bernardino Romera-Paredes", "Brais Martinez", "Aneesha Singh", "Matteo Cella", "Michel Valstar", "Hongying Meng", "Andrew Kemp", "Aaron Elkins"], "venue": "IEEE Trans. Affective Computing.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 0}, {"title": "Towards pain monitoring: Facial expression, head pose, a new database, an automatic system and remaining challenges", "author": ["Philipp Werner", "Ayoub Al-Hamadi", "Robert Niese", "Steffen Walter", "Sascha Gruss", "Harald C Traue"], "venue": "BMVC, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Facial expression intensity estimation using ordinal information", "author": ["Rui Zhao", "Quan Gan", "Shangfei Wang", "Qiang Ji"], "venue": "CVPR, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent convolutional neural network regression for continuous pain intensity estimation in video", "author": ["Jing Zhou", "Xiaopeng Hong", "Fei Su", "Guoying Zhao"], "venue": "CVPR workshops, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Facenet2expnet: Regularizing a deep face recognition net for expression recognition", "author": ["Hui Ding", "Shaohua Kevin Zhou", "Rama Chellappa"], "venue": "FG, 2017.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2017}, {"title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild", "author": ["C Fabian Benitez-Quiroz", "Ramprakash Srinivasan", "Aleix M Martinez"], "venue": "CVPR, 2016, pp. 5562\u20135570.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint patch and multi-label learning for facial action unit detection", "author": ["Kaili Zhao", "Wen-Sheng Chu", "Fernando De la Torre", "Jeffrey F. Cohn", "Honggang Zhang"], "venue": "CVPR, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Feature disentangling machinea novel approach of feature selection and disentangling in facial expression analysis", "author": ["Ping Liu", "Joey Tianyi Zhou", "Ivor Wai-Hung Tsang", "Zibo Meng", "Shizhong Han", "Yan Tong"], "venue": "ECCV, 2014, pp. 151\u2013166.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Disentangling factors of variation for facial expression recognition", "author": ["Salah Rifai", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent", "Mehdi Mirza"], "venue": "ECCV, 2012, pp. 808\u2013822.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Pairwise conditional random forests for facial expression recognition", "author": ["Arnaud Dapogny", "Kevin Bailly", "Severine Dubuisson"], "venue": "ICCV, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition", "author": ["Mengyi Liu", "Shiguang Shan", "Ruiping Wang", "Xilin Chen"], "venue": "CVPR, 2014, pp. 1749\u20131756.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Capturing complex spatio-temporal relations among facial muscles for facial expression recognition", "author": ["Ziheng Wang", "Shangfei Wang", "Qiang Ji"], "venue": "CVPR, 2013, pp. 3422\u20133429.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic facial expression recognition using longitudinal facial expression atlases", "author": ["Yimo Guo", "Guoying Zhao", "Matti Pietik\u00e4inen"], "venue": "ECCV, pp. 631\u2013644. 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Multioutput laplacian dynamic ordinal regression for facial expression recognition and intensity estimation", "author": ["Ognjen Rudovic", "Vladimir Pavlovic", "Maja Pantic"], "venue": "CVPR, 2012, pp. 2634\u20132641.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Structured output ordinal regression for dynamic facial emotion intensity prediction", "author": ["Minyoung Kim", "Vladimir Pavlovic"], "venue": "ECCV, 2010, pp. 649\u2013662.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Exploring facial expressions with compositional features", "author": ["Peng Yang", "Qingshan Liu", "Dimitris N Metaxas"], "venue": "CVPR, 2010, pp. 2638\u20132644.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild", "author": ["C. Fabian Benitez-Quiroz", "Ramprakash Srinivasan", "Aleix M. Martinez"], "venue": "CVPR, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Peak-piloted deep network for facial expression recognition", "author": ["Xiangyun Zhao", "Xiaodan Liang", "Luoqi Liu", "Teng Li", "Yugang Han", "Nuno Vasconcelos", "Shuicheng Yan"], "venue": "ECCV, 2016, pp. 425\u2013442.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint fine-tuning in deep neural networks for facial expression recognition", "author": ["Heechul Jung", "Sihaeng Lee", "Junho Yim", "Sunjeong Park", "Junmo Kim"], "venue": "ICCV, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Facial expression recognition via a boosted deep belief network", "author": ["Ping Liu", "Shizhong Han", "Zibo Meng", "Yan Tong"], "venue": "CVPR, 2014, pp. 1805\u20131812.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "3d model-based continuous emotion recognition", "author": ["Hui Chen", "Jiangdong Li", "Fengjun Zhang", "Yang Li", "Hongan Wang"], "venue": "CVPR, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal spontaneous emotion corpus for human behavior analysis", "author": ["Zheng Zhang", "Jeff M. Girard", "Yue Wu", "Xing Zhang", "Peng Liu", "Umur Ciftci", "Shaun Canavan", "Michael Reale", "Andy Horowitz", "Huiyuan Yang", "Jeffrey F. Cohn", "Qiang Ji", "Lijun Yin"], "venue": "CVPR, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "A discriminative feature learning approach for deep face recognition", "author": ["Yandong Wen", "Kaipeng Zhang", "Zhifeng Li", "Yu Qiao"], "venue": "ECCV, 2016, pp. 499\u2013515.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1929}, {"title": "Fast r-cnn", "author": ["Ross Girshick"], "venue": "ICCV, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic pain intensity estimation with heteroscedastic conditional ordinal random fields", "author": ["Ognjen Rudovic", "Vladimir Pavlovic", "Maja Pantic"], "venue": "International Symposium on Visual Computing, 2013, pp. 234\u2013243.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning face representation from scratch", "author": ["Dong Yi", "Zhen Lei", "Shengcai Liao", "Stan Z Li"], "venue": "arXiv preprint arXiv:1411.7923, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint face detection and alignment using multitask cascaded convolutional networks", "author": ["K. Zhang", "Z. Zhang", "Z. Li", "Y. Qiao"], "venue": "IEEE Signal Processing Letters, vol. 23, no. 10, pp. 1499\u20131503, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Face Samples are also provided to give an intuitive feeling about the Shoulder-Pain dataset[1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": ", annotated datasets such as EmoPain [2], Shoulder-Pain [1], BioVid Heat Pain [3].", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": ", annotated datasets such as EmoPain [2], Shoulder-Pain [1], BioVid Heat Pain [3].", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": ", annotated datasets such as EmoPain [2], Shoulder-Pain [1], BioVid Heat Pain [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "Two pieces of recent works make progress in estimating pain intensity visually using the Shoulder-Pain dataset only: Ordinal Support Vector Regression (OSVR) [4] and Recurrent Convolutional Regression (RCR) [5].", "startOffset": 158, "endOffset": 161}, {"referenceID": 4, "context": "Two pieces of recent works make progress in estimating pain intensity visually using the Shoulder-Pain dataset only: Ordinal Support Vector Regression (OSVR) [4] and Recurrent Convolutional Regression (RCR) [5].", "startOffset": 207, "endOffset": 210}, {"referenceID": 4, "context": "[5] is trained end-to-end achieving sub-optimal performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "\u2022A novel evaluation metric is proposed to fairly judge the performance on imbalanced dataset, such as the video-based Shoulder-Pain [1] where mostly painless expression occurs.", "startOffset": 132, "endOffset": 135}, {"referenceID": 5, "context": ", imagebased [6, 7] vs.", "startOffset": 13, "endOffset": 19}, {"referenceID": 6, "context": ", imagebased [6, 7] vs.", "startOffset": 13, "endOffset": 19}, {"referenceID": 7, "context": "video-based [8, 9, 10] methods.", "startOffset": 12, "endOffset": 22}, {"referenceID": 8, "context": "video-based [8, 9, 10] methods.", "startOffset": 12, "endOffset": 22}, {"referenceID": 9, "context": "video-based [8, 9, 10] methods.", "startOffset": 12, "endOffset": 22}, {"referenceID": 10, "context": "Furthermore, as videos are sequential signals, appearance-based methods including ours cannot model the dynamics given by a temporal model [11] or spatio-temporal models [12, 13, 14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 11, "context": "Furthermore, as videos are sequential signals, appearance-based methods including ours cannot model the dynamics given by a temporal model [11] or spatio-temporal models [12, 13, 14].", "startOffset": 170, "endOffset": 182}, {"referenceID": 12, "context": "Furthermore, as videos are sequential signals, appearance-based methods including ours cannot model the dynamics given by a temporal model [11] or spatio-temporal models [12, 13, 14].", "startOffset": 170, "endOffset": 182}, {"referenceID": 13, "context": "Furthermore, as videos are sequential signals, appearance-based methods including ours cannot model the dynamics given by a temporal model [11] or spatio-temporal models [12, 13, 14].", "startOffset": 170, "endOffset": 182}, {"referenceID": 3, "context": "Linear models include sparse representation based method, ordinal regression [4, 15, 16] and boosting [17].", "startOffset": 77, "endOffset": 88}, {"referenceID": 14, "context": "Linear models include sparse representation based method, ordinal regression [4, 15, 16] and boosting [17].", "startOffset": 77, "endOffset": 88}, {"referenceID": 15, "context": "Linear models include sparse representation based method, ordinal regression [4, 15, 16] and boosting [17].", "startOffset": 77, "endOffset": 88}, {"referenceID": 16, "context": "Linear models include sparse representation based method, ordinal regression [4, 15, 16] and boosting [17].", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].", "startOffset": 98, "endOffset": 117}, {"referenceID": 9, "context": "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].", "startOffset": 98, "endOffset": 117}, {"referenceID": 18, "context": "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].", "startOffset": 98, "endOffset": 117}, {"referenceID": 19, "context": "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].", "startOffset": 98, "endOffset": 117}, {"referenceID": 20, "context": "Among non-linear models, one approach is kernel-based methods [18] while another is deep learning [5, 10, 19, 20, 21].", "startOffset": 98, "endOffset": 117}, {"referenceID": 21, "context": "By introducing more information, one approach is 3D models [22] while another is multi-modal models [23].", "startOffset": 59, "endOffset": 63}, {"referenceID": 22, "context": "By introducing more information, one approach is 3D models [22] while another is multi-modal models [23].", "startOffset": 100, "endOffset": 104}, {"referenceID": 5, "context": "As regards transfer learning with deep networks, there exist recent works that regularize deep face recognition nets for expression classification - FaceNet2ExpNet [6].", "startOffset": 164, "endOffset": 167}, {"referenceID": 23, "context": "Normally, given a set of rich training examples (f , i), deep face verification algorithms [24] seek a function g : F \u2192 I where F is the input space spanned by all possible face appearances and I is the output space formed by all possible identities.", "startOffset": 91, "endOffset": 95}, {"referenceID": 23, "context": "Our network is transferred from a state-of-the-art face feature learning and verification network [24]1 which is trained using the CASIA-WebFace dataset contaning 0.", "startOffset": 98, "endOffset": 102}, {"referenceID": 24, "context": "Since over-fitting is a severe problem when training with limited data, the number of neurons in our hidden FC layer is relatively smaller than those in the original network (50 vs 512), and Dropout[25] operation is applied before the two FC layers.", "startOffset": 198, "endOffset": 202}, {"referenceID": 4, "context": "We truncate the output of the second layer to be in the range of [0, 5] by a scaled sigmoid activation y = 5 1+e\u2212x to fit the range of pain intensity level of the Shoulder Pain Dataset[1], which is also in range [0, 5].", "startOffset": 65, "endOffset": 71}, {"referenceID": 0, "context": "We truncate the output of the second layer to be in the range of [0, 5] by a scaled sigmoid activation y = 5 1+e\u2212x to fit the range of pain intensity level of the Shoulder Pain Dataset[1], which is also in range [0, 5].", "startOffset": 184, "endOffset": 187}, {"referenceID": 4, "context": "We truncate the output of the second layer to be in the range of [0, 5] by a scaled sigmoid activation y = 5 1+e\u2212x to fit the range of pain intensity level of the Shoulder Pain Dataset[1], which is also in range [0, 5].", "startOffset": 212, "endOffset": 218}, {"referenceID": 25, "context": "This phenomenon is also described in [26].", "startOffset": 37, "endOffset": 41}, {"referenceID": 25, "context": "To solve this problem, we follow [26] to use a smooth `1 loss instead of `2 loss, to make the gradients smaller when the error |y \u2212 \u1ef9| is very large.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "another is center loss [24],", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "Different from the description of center loss in [24], we jointly learn the centers and minimize the distances within classes by gradient descending, while [24]\u2019s centers are learned in a moving-average way.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "Different from the description of center loss in [24], we jointly learn the centers and minimize the distances within classes by gradient descending, while [24]\u2019s centers are learned in a moving-average way.", "startOffset": 156, "endOffset": 160}, {"referenceID": 0, "context": "We test our network on the UNBC-McMaster Shoulder-Pain dataset [1] that is widely used for benchmarking intensity estimations of the pain expression in particular and facial action units in general.", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].", "startOffset": 34, "endOffset": 44}, {"referenceID": 4, "context": "In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].", "startOffset": 34, "endOffset": 44}, {"referenceID": 26, "context": "In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].", "startOffset": 34, "endOffset": 44}, {"referenceID": 14, "context": "In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].", "startOffset": 129, "endOffset": 136}, {"referenceID": 4, "context": "In the same way as previous works [4, 5, 27], we take the same online label and quantify the original pain level in the range of [0, 15] to be in range [0, 5].", "startOffset": 152, "endOffset": 158}, {"referenceID": 23, "context": "The face verification network [24] is trained on CASIAWebface dataset [28], which contains 494,414 training images from 10,575 identities.", "startOffset": 30, "endOffset": 34}, {"referenceID": 27, "context": "The face verification network [24] is trained on CASIAWebface dataset [28], which contains 494,414 training images from 10,575 identities.", "startOffset": 70, "endOffset": 74}, {"referenceID": 28, "context": "To be specific, we leverage MTCNN model [29] to detect faces and facial landmarks.", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "OSVR-L1 (CVPR16) [4] 1.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "OSVR-L2 (CVPR16) [4] 0.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "601 RCNN (CVPR16w) [5] N/A 1.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "First, we eliminate the redundant frames on the sequences following [4].", "startOffset": 68, "endOffset": 71}], "year": 2017, "abstractText": "Limited annotated data is available for the research of estimating facial expression intensities, which makes the training of deep networks for automated expression assessment very challenging. Fortunately, fine-tuning from a data-extensive pre-trained domain such as face verification can alleviate the problem. In this paper, we propose a transferred network that fine-tunes a state-of-the-art face verification network using expression-intensity labeled data with a regression layer. In this way, the expression regression task can benefit from the rich feature representations trained on a huge amount of data for face verification. The proposed transferred deep regressor is applied in estimating the intensity of facial action units (2017 EmotionNet Challenge) and in particular pain intensity estimation (UNBS-McMaster Shoulder-Pain dataset). It wins the second place in the challenge and achieves the stateof-the-art performance on Shoulder-Pain dataset. Particularly for Shoulder-Pain with the imbalance issue of different pain levels, a novel weighted evaluation metric is proposed.", "creator": "LaTeX with hyperref package"}}}