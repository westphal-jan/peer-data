{"id": "1702.07028", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "On the ability of neural nets to express distributions", "abstract": "Deep neural nets have caused a revolution in many classification tasks. A related ongoing revolution---also theoretically not understood---concerns their ability to serve as generative models for complicated types of data such as images and texts. These models are trained using ideas like variational autoencoders and Generative Adversarial Networks.", "histories": [["v1", "Wed, 22 Feb 2017 22:21:38 GMT  (37kb)", "https://arxiv.org/abs/1702.07028v1", "Under review for COLT 2017"], ["v2", "Fri, 2 Jun 2017 15:14:17 GMT  (39kb)", "http://arxiv.org/abs/1702.07028v2", "Accepted to COLT 2017"]], "COMMENTS": "Under review for COLT 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["holden lee", "rong ge", "tengyu ma", "rej risteski", "sanjeev arora"], "accepted": false, "id": "1702.07028"}, "pdf": {"name": "1702.07028.pdf", "metadata": {"source": "CRF", "title": "On the ability of neural nets to express distributions", "authors": ["Holden Lee", "Rong Ge", "Tengyu Ma", "Andrej Risteski", "Sanjeev Arora"], "emails": ["holdenl@princeton.edu", "rongge@cs.duke.edu", "tengyu@cs.princeton.edu", "risteski@princeton.edu", "arora@cs.princeton.edu."], "sections": [{"heading": null, "text": "ar Xiv: 170 2.07 028v 2 [cs.L G] 2J un2 01A key component is Barron's theorem [Bar93], which gives a Fourier criterion for the approximability of a function by a neural network with a hidden layer. We show that a composition of n functions fulfilling certain Fourier conditions (\"Barron functions\") can be approximated by an n + 1 layer neural network. For probability distributions, this translates into a criterion for a probability distribution approximate to Waterstone distance - a natural measure of probability distributions - by a neural network applied to a fixed base distribution (e.g. multivariate Gaussian)."}, {"heading": "1 Introduction", "text": "In many areas such as computer vision, speech recognition and amplification learning ([BCV13; Sch15]), deep neural networks have led to state-of-the-art performance in classification tasks. A neural network can be seen as a way to learn a function that maps inputs x to outputs y. For image classification, input is a vector representing an image, and the output can be the probability of being in different classes. However, another more recent (and less understood) use of neural networks is as generative models for complicated probability distributions, such as distributions via images on ImageNet, handwritten characters from different alphabets or language. Here, the network can map a stochastic input - like a uniform normal gauze - to a realistic image. Such networks are trained using different methods, such as varying autoencoders ([KW13], [RMW14] or generative adversarial networks (GANs + 14)."}, {"heading": "1.1 Our work", "text": "We specify a sufficient criterion for a function to be approximated by a neural network with n hidden layers (theorem 3.1). This criterion applies to any distribution of inputs supported on a compact set. As a result of our main result, we obtain a criterion for a distribution roughly generated by a neural network with n hidden layers in the Waterstone metric W2, a natural metric for the space of distributions (episode 3.3).Our criterion is based on Fourier properties of the function. We build on Barron's theorem [Bar93], which states that if a certain quantity involving the Fourier transform is small, the function can be approximated by a neural network with a hidden layer and a small number of nodes. If we call such a function Barron function a Barron function, our criterion Neural says to say that a composition can be generated by a composition with n."}, {"heading": "1.2 Related work", "text": "A first attempt to understand the effectiveness of neural networks was through their approximation properties. A series of studies has shown that any continuous function in a limited range can be approximated by a sufficiently large, two-layered neural network ([Cyb89], [HSW89]) that the network size can be exponential in dimension. Barron ([Bar93]) provided an upper limit for the size of the network required in relation to a Fourier criterion. He showed that a function f in L2 can be up to an error in neural networking with O\u00c5 C2f\u03b50 units in which Fourier properties of fourier criteria are required. A noteworthy consequence is that neural networks can circumvent the curse of dimensionality: the number of parameters required to achieve a fixed error rate."}, {"heading": "1.3 Notation and Definitions", "text": "First, we formally define the model of a supplying neural network that we will use. Definition 1.1. A neural network with n hidden layers (also referred to as n + 1 layer neural network) is defined as follows: A neural network has an associated input space Rm0, output space Rmn + 1, and n hidden layers of size m1,.., mn. The neural network has the parameters A (l), Rml \u2212 1 \u00b7 ml, and b (l), Rml for 1 \u2264 l \u2264 n + 1. The neural network has a fixed activation function that is applied component-wise to a vector."}, {"heading": "2 Barron\u2019s Theorem", "text": "For the Fourier inversion formula, sufficient for all \"nice\" functions, isf x) = \"Rn f\" (x) ei < x > dx \"ei < x > dx\" i < x > dx \"i < x > dx = (2) ng\" i \"n.\""}, {"heading": "3 Multilayer Barron\u2019s Theorem", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Main theorem", "text": "Barron's Theorem states that a Barron function can be approximated by a neural network with a hidden layer. < M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M"}, {"heading": "3.2 Approximating probability distributions", "text": "Theorem 3.1 can be interpreted in a very natural way if the goal is to approximate the probability distribution."}, {"heading": "3.3 Proof of main theorem", "text": "To prove this, we must first prove the following theorems: Theorem 3.5. Condition 1-4 and the notation of theorem 3.1. Then there is a neural network g with hidden layers and S-Rm0. (1) Satisfactory is 0 (S) 0 (S) 0 (S) 0 (S) 0 (S) 0 (S) 1 (S) 1 (S) 1 (S) 1 (S) 1 (S) 1 (S) 1 (S) 1 (S) 1 (S) 1). We will show that we have g = 1, where g1,.., gl.: 1 (1) are functionally defined: R mi (1) 1 (x)."}, {"heading": "4 Separation between Barron functions and composition of Bar-", "text": "It is not as if it is an unforeseen situation where for all r, s > 0, k: Rn \"R is O (nr3) -Barron on rBn, and j: R\" R is O (sn2) -Barron on sB1.Condition n \"3 (mod 4) is not necessary; we include it only to avoid case analysis; we note that this theorem separates Barron functions from compositions of Barron functions."}, {"heading": "4.1 Definition of f", "text": "Let f1: R \u2192 R be a function so that f1 is not negative, Supp (f1) for all i = 0, 1, 2. This function exists by Lemma D.1 (1). We choose K1, \u03b5 according to n. According to Theorem A.5, f = 122, 122, 122 for all i = 0, 1, 2. We choose K1, \u03b5 according to n. According to Theorem A.5, f = 122, 122, 122 for all i = 1, 0 rn 2 \u2212 1f1 (r) Jn2 \u2212 1 (r) Jn2 \u2212 1 (c) dr. (17) We choose [K1, K1 + \u03b5] as an interval where Jn 2 (c) is large and positive for some large countries. We use the notation of Lemma B.1. For x \u00b2 n, (fn, xx) = x \u00b2 \u2212 Kn."}, {"heading": "4.2 A technique to lower bound the Barron constant", "text": "The main difficulty in representing a function is not that Barron lowers the integral principle (Rn) for each function (Rn) (Rn). Generally, it is not known how to calculate the constant (Rn) for all extensions (Rn). (Rn) Generally, it is not known how the constant (Rn) for each extension F, we choose g for each extension F, we choose g for each extension F, and we calculate the Rn constant for f over a sphere rBn. This does not depend on the extension F, because (Rn) g = (Rn) properties for each extension F, we choose g for each extension F, we choose g for support in B, and calculate the Rn constant x x. (Rn) This does not depend on the extension F, because (Rn) g = (Rn) properties for each extension F) g."}, {"heading": "4.3 h is a composition of Barron functions", "text": "We can write f as the composition of a function that calculates the quadratic norm, and a one-dimensional function. The Barron constant for both functions can be limited by polynomials. Lemma 4.4. Suppose that C1 < C3. f is the composition of the two functions x 7 \u2192 x 2 Rn \u2192 R (21) y 7 \u2192 f1 (\u221a y) R \u2192 R. (22) The function x 7 \u2192 x 2 is sufficient for all. Intuitively, the proof utilizes the fact that polynomials are barrons, and all \"nice\" one-dimensional functions are barrons. Let's leave the detailed proofs in Section E. Now it's easy to see the separation: Theorem 4.1 By Lemma 4.1."}, {"heading": "5 Conclusion", "text": "In this paper, we show whether a generative model can be expressed as a composition of n Barron functions, then it can be approximated by an n + 1 layer neural network. Along the way, we have tested a multi-layered version of Barron's theorem [Bar93], and a central observation is the use of the Waterstone distance W 2 as a distance measure between distributions. This partially explains the significance of neural networks as generative models. However, there are still many outstanding problems: What natural transformations can be represented by a composition of Barron functions? Is there a separation between the composition of n Barron functions and the composition of n + 1 Barron functions? How can we efficiently learn such representation? We hope that this paper serves as a first step in understanding the power of deep generative models."}, {"heading": "A Background from Fourier Analysis", "text": "The Fourier transformation is defined in (5).Theorem A.1 (Fourier inversion).Theorem A.1 (Fourier inversion).Theorem A.1 (Fourier inversion).Theorem A.1 (Fourier inversion)."}, {"heading": "B Bessel functions", "text": "We need some facts about Bessel functions J\u03b1 (x), \u03b1-R. J\u03b1 (x) has an oscillating form like a attenuated sinusoidal shape. Lemma B.1 ([Kra14, Theorem 5], [ES15, Lemma 21]. If d is \u2265 2 and x \u2265 d, then this is the case."}, {"heading": "C Properties of Wasserstein Distance", "text": "Lemma C.1 (Lemma 3.4 redefined). For all two distributions \u00b5, \u03bd over Rn, W1 (\u00b5, \u03bd) \u2264 W2 (\u00b5, \u03bd). (35) Furthermore, for each Lipschitz function f: Rn \u2192 R, and for each other a coupling of \u00b5 f (x) \u2212 Ey. Then through the Cauchy-Black inequality, W1 (\u00b5, \u03bd) \u2264 Rn \u2264 Lip (f). (36) Proof. Allow there to be a coupling of \u00b5 f (x, \u03bd). (Then through the Cauchy-Black inequality, W1 (\u00b5, \u03bd) \u2264 Rn \u00d7 Rn \u00b2 x \u2212 y \u00b2 (x, y)."}, {"heading": "D Test functions", "text": "For a function f (K) is f (K) (x): = f (x K) = k (b) = b (b). Let m (2) be a given positive integer. 1. There is a function g: R \u2192 R with the following properties. (a) g \u2264 0 everywhere. (b) Supp (g) (0, 1]. (c) 1 0 g (x) dx = 1. (d) g is continuously differentiable and for all k \u2264 m, | g (k) (x) (x) | O (m) (m). The function 1K g (K) (x) (x) fulfills Supp (g) x (K) 1) (0 g (K), and for all k \u2264 m (g) (k) (k) (k) (K) (x) (O) = O (m)."}, {"heading": "E Omitted Proofs in Section 4", "text": "Theorem E.1 (Theorem 4.2): If f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f = f"}], "references": [{"title": "Towards principled methods for training generative adversarial networks", "author": ["M. Arjovsky", "L. Bottou"], "venue": "NIPS 2016 Workshop on Adversarial Training. In review for ICLR", "citeRegEx": "Arjovsky and Bottou.,? \\Q2017\\E", "shortCiteRegEx": "Arjovsky and Bottou.", "year": 2017}, {"title": "Wasserstein GAN", "author": ["M. Arjovsky", "S. Chintala", "L. Bottou"], "venue": "arXiv preprint arXiv:1701.07875", "citeRegEx": "Arjovsky et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2017}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["A.R. Barron"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "Barron.,? \\Q1993\\E", "shortCiteRegEx": "Barron.", "year": 1993}, {"title": "Approximation and estimation bounds for artificial neural networks", "author": ["A.R. Barron"], "venue": "Machine Learning", "citeRegEx": "Barron.,? \\Q1994\\E", "shortCiteRegEx": "Barron.", "year": 1994}, {"title": "Representation learning: A review and new perspectives\u201d. In: IEEE transactions on pattern analysis and machine intelligence", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "On the expressive power of deep learning: A tensor analysis", "author": ["N. Cohen", "O. Sharir", "A. Shashua"], "venue": "arXiv preprint arXiv:1509.05009", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of Control, Signals, and Systems (MCSS)", "citeRegEx": "Cybenko.,? \\Q1989\\E", "shortCiteRegEx": "Cybenko.", "year": 1989}, {"title": "Depth Separation for Neural Networks", "author": ["A. Daniely"], "venue": "arXiv preprint arXiv:1702.08489", "citeRegEx": "Daniely.,? \\Q2017\\E", "shortCiteRegEx": "Daniely.", "year": 2017}, {"title": "Training generative neural networks via maximum mean discrepancy optimization", "author": ["G.K. Dziugaite", "D.M. Roy", "Z. Ghahramani"], "venue": null, "citeRegEx": "Dziugaite et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dziugaite et al\\.", "year": 2015}, {"title": "The Power of Depth for Feedforward Neural Networks", "author": ["R. Eldan", "O. Shamir"], "venue": null, "citeRegEx": "Eldan and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Eldan and Shamir.", "year": 2015}, {"title": "On the approximate realization of continuous mappings by neural networks", "author": ["K.-I. Funahashi"], "venue": "Neural networks", "citeRegEx": "Funahashi.,? \\Q1989\\E", "shortCiteRegEx": "Funahashi.", "year": 1989}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow"], "venue": "Advances in neural information processing systems", "citeRegEx": "Goodfellow,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow", "year": 2014}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": null, "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}, {"title": "Detecting change in data streams", "author": ["D. Kifer", "S. Ben-David", "J. Gehrke"], "venue": "Proceedings of the Thirtieth international conference on Very large data bases-Volume 30. VLDB Endowment", "citeRegEx": "Kifer et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kifer et al\\.", "year": 2004}, {"title": "On the learnability of discrete distributions", "author": ["M. Kearns"], "venue": "Proceedings of the twenty-sixth annual ACM symposium on Theory of computing", "citeRegEx": "Kearns,? \\Q1994\\E", "shortCiteRegEx": "Kearns", "year": 1994}, {"title": "On a space of completely additive functions", "author": ["L.V. Kantorovich", "G.S. Rubinstein"], "venue": "Vestnik Leningrad. Univ", "citeRegEx": "Kantorovich and Rubinstein.,? \\Q1958\\E", "shortCiteRegEx": "Kantorovich and Rubinstein.", "year": 1958}, {"title": "Approximations for the Bessel and Airy functions with an explicit error term", "author": ["I. Krasikov"], "venue": "LMS Journal of Computation and Mathematics", "citeRegEx": "Krasikov.,? \\Q2014\\E", "shortCiteRegEx": "Krasikov.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Super-linear gate and super-quadratic wire lower bounds for depth-two and depth-three threshold circuits", "author": ["D.M. Kane", "R. Williams"], "venue": "Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing", "citeRegEx": "Kane and Williams.,? \\Q2016\\E", "shortCiteRegEx": "Kane and Williams.", "year": 2016}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": null, "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Deep Learning in Neural Networks: An Overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "Benefits of depth in neural networks", "author": ["M. Telgarsky"], "venue": "arXiv preprint arXiv:1602.04485", "citeRegEx": "Telgarsky.,? \\Q2016\\E", "shortCiteRegEx": "Telgarsky.", "year": 2016}], "referenceMentions": [], "year": 2017, "abstractText": "Deep neural nets have caused a revolution in many classification tasks. A related ongoing revolution\u2014also theoretically not understood\u2014concerns their ability to serve as generative models for complicated types of data such as images and texts. These models are trained using ideas like variational autoencoders and Generative Adversarial Networks. We take a first cut at explaining the expressivity of multilayer nets by giving a sufficient criterion for a function to be approximable by a neural network with n hidden layers. A key ingredient is Barron\u2019s Theorem [Bar93], which gives a Fourier criterion for approximability of a function by a neural network with 1 hidden layer. We show that a composition of n functions which satisfy certain Fourier conditions (\u201cBarron functions\u201d) can be approximated by a n+ 1layer neural network. For probability distributions, this translates into a criterion for a probability distribution to be approximable in Wasserstein distance\u2014a natural metric on probability distributions\u2014by a neural network applied to a fixed base distribution (e.g., multivariate gaussian). Building up recent lower bound work, we also give an example function that shows that composition of Barron functions is more expressive than Barron functions alone.", "creator": "LaTeX with hyperref package"}}}