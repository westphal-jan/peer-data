{"id": "1702.07071", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2017", "title": "Pronunciation recognition of English phonemes /\\textipa{@}/, /{\\ae}/, /\\textipa{A}:/ and /\\textipa{2}/ using Formants and Mel Frequency Cepstral Coefficients", "abstract": "The Vocal Joystick Vowel Corpus, by Washington University, was used to study monophthongs pronounced by native English speakers. The objective of this study was to quantitatively measure the extent at which speech recognition methods can distinguish between similar sounding vowels. In particular, the phonemes /\\textipa{@}/, /{\\ae}/, /\\textipa{A}:/ and /\\textipa{2}/ were analysed. 748 sound files from the corpus were used and subjected to Linear Predictive Coding (LPC) to compute their formants, and to Mel Frequency Cepstral Coefficients (MFCC) algorithm, to compute the cepstral coefficients. A Decision Tree Classifier was used to build a predictive model that learnt the patterns of the two first formants measured in the data set, as well as the patterns of the 13 cepstral coefficients. An accuracy of 70\\% was achieved using formants for the mentioned phonemes. For the MFCC analysis an accuracy of 52 \\% was achieved and an accuracy of 71\\% when /\\textipa{@}/ was ignored. The results obtained show that the studied algorithms are far from mimicking the ability of distinguishing subtle differences in sounds like human hearing does.", "histories": [["v1", "Thu, 23 Feb 2017 02:31:03 GMT  (824kb,D)", "http://arxiv.org/abs/1702.07071v1", "11 pages, pre-print version"]], "COMMENTS": "11 pages, pre-print version", "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["keith y patarroyo", "vladimir vargas-calder\\'on"], "accepted": false, "id": "1702.07071"}, "pdf": {"name": "1702.07071.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Keith Y. Patarroyo", "Vladimir Vargas-Calder\u00f3n"], "emails": ["/@/,", "/@/,", "/@/", "vvargasc@unal.edu.co"], "sections": [{"heading": null, "text": "The pronunciation of English phonemes / @ /, / \u00e6 /, / A: / and / 2 / using formants and Mel Frequency Cepstral Coefficient by Keith Y. Patarroyo and Vladimir Vargas-Caldero \u0301 n \u043a The Vocal Joystick Vowel Corpus from Washington University was used to study monophthongs pronounced by native English speakers to quantitatively measure the extent to which speech recognition methods can distinguish between similar-sounding vowels. Specifically, the phonemes / @ /, / \u00e6 /, / A: / and / 2 / were analyzed. 748 sound files from the corpus were used and subjected to Linear Predictive Coding (LPC) to calculate their performance, and Mel Frequency Cepstral Coefficients (MFCC) to calculate the receiver coefficient."}, {"heading": "1 Introduction", "text": "Throughout computer history, scientists have developed an enormous amount of widely known speech recognition theories and algorithms (e.g., the work of Lee (1988)), many of which are motivated by using some of the principles of human ear surgery (Davis & Mermelstein, 1980).In recent years, the flourishing of high-speed processing computers has enabled us to use deep machine learning tools to improve the effectiveness of our speech recognition systems (Baker et al., 2009).The techniques currently used in speech recognition systems lead to high prediction rates (Hinton et al., 2012).The most common and successful technique is the implementation of multi-layered neural learning tools (Zegers, 1998; Wellekens, 1998)."}, {"heading": "2 Theoretical Framework", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Sound Formation", "text": "The frequency range of pressure waves in the air, which make up the audible sounds of humans, ranges from 20 Hz to 20 kHz (Rosen & Howell, 2011).The diaphragm and intercostal contractions generate a stream of air from the chest to the mouth, which initially passes through the larynx, where the vocal cords are located. These are muscles that can be distributed geometrically in different ways, each of which generates vibrations or vibrations through the air passing by, leading to noises. The throat, oral cavity and nasal cavity are filters and resonators of the aforenised sounds. The tongue and lips also allow us to quickly articulate and change the shape of the vocal cavity to filter some of these frequencies. These studies are carried out with vocal sounds that we generate with vowels."}, {"heading": "2.2 Human Hearing Detection Principles", "text": "The human ear analyzes pressure fluctuations in the air at different frequencies. Roughly speaking, the human ear performs a Fourier transformation of the pressure signal P (t), where t is the time, and transfers | P (\u03c9) | 2, where \u03c9 is the frequency, to the brain (Sethna, 2006).However, this initial model is insufficient for many analyses, some of the reasons being (Lyons, n.d.): Tonal information changes over the course of a word or melody; the difference between two closely related frequencies is difficult for humans to detect, especially at high frequencies; and humans hear the volume on a nonlinear scale. All of these factors pose a significant challenge to the imitation of the human hearing system. Davis and Mermelstein (1980) developed a method (Mel frequency cepstral coefficients) to imitate this process, and have been state of the art in the field of speech recognition ever since."}, {"heading": "2.3 Formants and Linear Predictive Coding (LPC)", "text": "The vowel tract can be considered as a vibrating cavity or resonator whose geometry (morphology) is constantly changing in continuous speech, and this change in geometry allows different modes of oscillation and thus different sounds to overlap. Experiments have shown that there are some characteristic frequencies for English vowels called formants (Hillenbrand, Getty, Wheeler, & Clark, 1994; Hunter & Kebede, 2012; Deterding, 2006) that correspond to the maxima of oscillation, i.e. where the acoustic energy is focused, the first formant is roughly between 0 and 1kHz, while the second formant is localized from 1kHz to 2kHz, and so on (these frequency limits are not rigid because there may be some second formants that focus the signal below 1kHz, as seen in the references)."}, {"heading": "2.4 Mel Frequency Cepstral Coefficients (MFCC)", "text": "This method aims to mimic the human hearing system's approach to decoding a sound signal. It can be summarised in the steps in Table 1 (Lyons, n.d.)."}, {"heading": "2.5 Decision Tree Classifiers (DTC)", "text": "In other words, let's look at a dataset D = {(di, li)} in which di is a vector of N components (characteristics). To see how DTCs work, let's consider the following example. Let's assume that a dataset consists of samples in one dimension: Each sample is the salary of every single person in Colombia. The labels for this dataset are the social layer. We might have people who make a lot of money, but they live in layer 2, or vice versa. However, these cases are strange and in general there is a structure that makes it possible to predict the label (social layer)."}, {"heading": "3 Methodology", "text": "The pipeline of this study (see Figure 1) was to extract and prepare audio files from the vocal joystick vowel corpus, then calculate formants and Cepstral coefficients for each file. Data sets with their corresponding formants and Cepstral coefficients were divided into training and test data. A DTC was trained with the training data and tested against the test data, resulting in percentage accuracy. Characteristics of each step are explained in this section."}, {"heading": "3.1 Corpus and Audio Processing", "text": "In this study, we used the Vocal Joystick Vocal Corpus (Bilmes, Wright, Xiao, Malkin Kilanski, 2006), which selected a group of nine monophthongs and 12 vowel-to-vowel transitions. For each vowel, sounds of varying duration, amplitude and intonation were recorded: \u2022 Duration: short (1 second), long (2 seconds) and nudges (very short repetitions of the same vowel). \u2022 Amplitude: quiet, normal, loud, quiet too loud and loud too quiet. \u2022 Intonation: level, ascending and falling. From this corpus, we looked at all the sound files available in the corpus that correspond to the phonemes / @ /, / \u00e6 /, / A: / and / 2 / whose duration was either short or long, whose amplitude was whole, normal or loud, and whose intonation was on a single level. A total of 784 files met these conditions."}, {"heading": "3.2 Data Preparation", "text": "An example of a raw data file from the corpus is shown in Figure 2a. You can see that there are still parts and the amplitude is not normalized, and even if we have selected a single tone, it is clear that the amplitude of the sound decreases with time. Therefore, we deleted the silent part for each file. Then, a section of 40 ms duration was selected so that the signal does not change substantially and is approximately periodic (Figure 2b). Finally, a hamming window was applied to the resulting sound signal to smooth out the Fourier transform used in the MFCC extraction (Figure 2c)."}, {"heading": "3.3 Evaluation of Formants and MFCC Performance with a DTC", "text": "Then the LPC Python implementation by Danilo Bellini was used to calculate the formants (Bellini, 2016). To perform the calculation of the MFCC, the Python implementation by James Lyons (Lyons, 2016) was used. Finally, the set of calculated formants and the set of calculated Cepstral coefficients was divided into two groups, one of which contained two thirds of the data, and the other contained a third. The first was the training data, and the later was the test data. To illustrate this procedure, name S = {(s, v)} as the set of sound signal files, each of which is labeled with a vowel v. Let F (s) be a function that calculates the formants F of a sound signal, and the later were the test data. Let X = {(F (s), v), s \u00b2 s, the set of vocals, their respective vocals, labeled by their vocals."}, {"heading": "4 Results, Analysis and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Formants", "text": "Of the 784 sound files, 142 were discarded for this analysis and for those corresponding to the MFCC. The reason for discarding these files was that some recorded sound files contained portions in which the signal was not periodic due to a trembling voice of the speaker, which could cause the LPC and MFCC algorithms to be affected. The criterion used to discard the data is now explained. Describe the recordings or sound signals belonging to vowel v as (s, v). Also, indicate the mean value of the i-th formant (i = 1, 2) and the standard deviation of the i-th formant, for a vowel v, as (\u00b5i, v), (\u03c3i, v). The remaining 642 sound signals fulfilled the condition (s) - < < < 1,5\u0445i, (1) for each vowel v, as forts, as manten, as manten, as manten, as manten."}, {"heading": "4.2 MFCC", "text": "13 MFCCs were determined for each sound signal. To visualize the MFCC of each sound signal, Principal Component Analysis (PCA) performed a dimensional reduction from 13 to 2 (i.e. from the MFCC space to a level), which projects the data from the high-dimensional space to a level that preserves the greatest possible deviation of the data. The distribution of the MFCC for the different phonemes at the level calculated with PCA is shown in Figure 4.As shown in Figure 4a, the MFCC of the phonemes appears to be mixed. In particular, it should be noted that the points corresponding to the phoneme / A: / those corresponding to the phoneme / 2 / corresponding phonemes are mixed with the points. This can also be seen in Figure 4a. Ignoring the / A: / points of the phonemes results in Figure 4b that the MFCs in which the clusters of the data can be identified."}, {"heading": "5 Conclusions and Perspectives", "text": "The Vocal Joystick Corpus was used to create a dataset of sounds corresponding to the phonemes / @ /, / \u00e6 /, / A: / and / 2 /. From each of the sound signals, both formants and MFCC were calculated, which were used to train a DTC that functioned as a classifier. An accuracy of 70% was achieved with the help of formants; an accuracy of 52% was measured with the help of MFCC. MFCC's performance was much better when the DTC was trained with the coefficients calculated from the phonemes / 2 /, / e / and / u /, reaching 88%. However, these speech recognition tools do not correctly fail the phonemes of similar-sounding signals. In response to this shortcoming, computer and linguists are working to develop computer tools that learn subtle differences between sounds in order to correctly assess a person's pronunciation."}], "references": [{"title": "Developments and directions in speech recognition and understanding, Part 1 [DSP Education", "author": ["J. Baker", "Li Deng", "J. Glass", "S. Khudanpur", "Chin-hui Lee", "N. Morgan", "D. O\u2019Shaughnessy"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Expressive Digital Signal Processing (DSP) package for Python, 2016", "author": ["Bellini", "D. (n.d"], "venue": "Retrieved November", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences", "author": ["S. Davis", "P. Mermelstein"], "venue": "IEEE Transactions On Acoustics, Speech, And Signal Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1980}, {"title": "Speech processing (1st ed., pp. 19-26)", "author": ["L. Deng", "D. O\u2019Shaughnessy"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "The North Wind versus a Wolf: short texts for the description and measurement of English pronunciation", "author": ["D. Deterding"], "venue": "Journal Of The International Phonetic Association,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Perceptual assimilation predicts acquisition of foreign language sounds: The case of Azerbaijani learners\u2019 production and perception of Standard Southern British English vowels", "author": ["P. Ghaffarvand Mokari", "S. Werner"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Formant frequencies of RP monophthongs in four age groups of speakers", "author": ["S. Hawkins", "J. Midgley"], "venue": "Journal Of The International Phonetic Association,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Acoustic characteristics of American English vowels", "author": ["J. Hillenbrand", "L. Getty", "M. Clark", "K. Wheeler"], "venue": "The Journal Of The Acoustical Society Of America,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "Speech technologies for pronunciation feedback and evaluation", "author": ["R. Hincks"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition. Retrieved from http://static.googleusercontent.com/media/research.google.com/ en//pubs/archive/38131.pdf", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N Jailty"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Formant frequencies of British English vowels produced by native speakers of Farsi. Soci\u00e9t\u00e9 Fran\u00e7aise d\u2019Acoustique", "author": ["G. Hunter", "H. Kebede"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "On large-vocabulary speaker-independent continuous speech recognition", "author": ["K. Lee"], "venue": "Speech Communication,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1988}, {"title": "Mel Frequency Cepstral Coefficient (MFCC) tutorial, Practical Cryptography", "author": ["J. Lyons"], "venue": "Retrieved October", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Pronunciation Training Facilitates the Learning and Retention of L2 Grammatical Structures", "author": ["I. Martin", "C. Jackson"], "venue": "Foreign Language Annals,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Comparing the Phonological Performance of Kurdish and Persian EFL Learners in Pronunciation of English Vowels", "author": ["K. Mirzaei", "H. Gowhary", "A. Azizifar", "Z. Esmaeili"], "venue": "Procedia - Social And Behavioral Sciences,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "The effectiveness of computer assisted pronunciation training for foreign language learning by children", "author": ["A. Neri", "O. Mich", "M. Gerosa", "D. Giuliani"], "venue": "Computer Assisted Language Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Human Time-Frequency Acuity Beats the Fourier Uncertainty Principle", "author": ["J. Oppenheim", "M. Magnasco"], "venue": "Physical Review Letters,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Recognition of vowels in continuous speech by using formants. Facta Universitatis - Series: Electronics And Energetics", "author": ["B. Prica", "S. Ili\u0107"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "An online model for vowel imitation learning", "author": ["H. Rasilo", "O. R\u00e4s\u00e4nen"], "venue": "Speech Communication,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "Signals and systems for speech and hearing (2nd ed., p. 163)", "author": ["S. Rosen", "P. Howell"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Statistical mechanics: Entropy, Order Parameters and Complexity (1st ed., p. 299)", "author": ["J. Sethna"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Introduction to Data", "author": ["P. Tan", "M. Steinbach", "V. Kumar"], "venue": "Mining (1st ed.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "A new method for mispronunciation detection using Support Vector Machine based on Pronunciation Space Models", "author": ["S. Wei", "G. Hu", "Y. Hu", "R. Wang"], "venue": "Speech Communication,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Introduction to Speech Recognition Using Neural Networks. Bruges: ESANN. Retrieved from https://www.elen.ucl.ac.be/Proceedings/ esann/esannpdf/es1998-456.pdf", "author": ["C. Wellekens"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "Speech Recognition Using Neural Networks (Master of Science with a major in Electrical Engineering)", "author": ["P. Zegers"], "venue": "University of Arizona", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}], "referenceMentions": [], "year": 2017, "abstractText": "The Vocal Joystick Vowel Corpus, by Washington University, was used to study monophthongs pronounced by native English speakers. The objective of this study was to quantitatively measure the extent at which speech recognition methods can distinguish between similar sounding vowels. In particular, the phonemes /@/, /\u00e6/, /A:/ and /2/ were analysed. 748 sound files from the corpus were used and subjected to Linear Predictive Coding (LPC) to compute their formants, and to Mel Frequency Cepstral Coefficients (MFCC) algorithm, to compute the cepstral coefficients. A Decision Tree Classifier was used to build a predictive model that learnt the patterns of the two first formants measured in the data set, as well as the patterns of the 13 cepstral coefficients. An accuracy of 70% was achieved using formants for the mentioned phonemes. For the MFCC analysis an accuracy of 52 % was achieved and an accuracy of 71% when /@/ was ignored. The results obtained show that the studied algorithms are far from mimicking the ability of distinguishing subtle differences in sounds like human hearing does.", "creator": "LaTeX with hyperref package"}}}