{"id": "1702.07103", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2017", "title": "Discriminating Traces with Time", "abstract": "What properties about the internals of a program explain the possible differences in its overall running time for different inputs? In this paper, we propose a formal framework for considering this question we dub trace-set discrimination. We show that even though the algorithmic problem of computing maximum likelihood discriminants is NP-hard, approaches based on integer linear programming (ILP) and decision tree learning can be useful in zeroing-in on the program internals. On a set of Java benchmarks, we find that compactly-represented decision trees scalably discriminate with high accuracy---more scalably than maximum likelihood discriminants and with comparable accuracy. We demonstrate on three larger case studies how decision-tree discriminants produced by our tool are useful for debugging timing side-channel vulnerabilities (i.e., where a malicious observer infers secrets simply from passively watching execution times) and availability vulnerabilities.", "histories": [["v1", "Thu, 23 Feb 2017 05:48:22 GMT  (2259kb,D)", "http://arxiv.org/abs/1702.07103v1", "Published in TACAS 2017"]], "COMMENTS": "Published in TACAS 2017", "reviews": [], "SUBJECTS": "cs.PL cs.CR cs.FL cs.LG cs.SE", "authors": ["saeid tizpaz-niari", "pavol cerny", "bor-yuh evan chang", "sriram sankaranarayanan", "ashutosh trivedi"], "accepted": false, "id": "1702.07103"}, "pdf": {"name": "1702.07103.pdf", "metadata": {"source": "CRF", "title": "Discriminating Traces with Time", "authors": ["Saeid Tizpaz-Niari", "Pavol \u010cern\u00fd", "Bor-Yuh Evan Chang", "Sriram Sankaranarayanan", "Ashutosh Trivedi"], "emails": ["ashutosh.trivedi}@colorado.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of us are able to determine for ourselves what we want and what we want."}, {"heading": "3 Trace-Set Discrimination Problem", "text": "A discrete probability distribution, or equitable distribution, over a finite set L is a function d > > labels in a finite set. < K \u2192 [0, 1] such that \"L d (') = 1. Let D (L) represent the set of all discriminated distributions over L.Let p1,.., pm represent a series of atomic predicates over tracks. < each predicate evaluates to a Boolean value over a given track. < K) Therefore, we represent a track simply by the truth evaluations of the predicates over the track. In addition to atomic predicates, tracks are associated with a distribution over labels. These distributions are generated by the first measurement of the execution time t of the track. Execution time is achieved as an average over a specified number of measurements. M > 0. Therefore, the timing is taken to be a Gaussian variable with mean t and a standard deviation."}, {"heading": "3.1 Maximum Likelihood Learning", "text": "In view of discrimination and a number of predicates, we define the probability of discrimination as the probability that each trace < \u03c4i, di > carries the label \"label\" (< \u03c4i, di >). < < < < < < < M > label \"label\" is the maximum probability that the label \"label\" will be used. < M > label \"label\" is defined as the discriminant among all possible Boolean formulas that are maximized (i.e.).ml = argmaximized (i.e.) We assume that the maximization runs over all possible tuples of K predicates."}, {"heading": "3.2 Decision Tree Learning", "text": "As mentioned above, the Max Probability approach using structured Boolean formulas can be prohibitively expensive if the number of tracks, predicates, and labels is large. An efficient alternative is to consider approaches to learning decision trees that can efficiently produce precise discriminants and keep the size of the discriminant as small as possible. A discriminant's weighted accuracy over tracks < \u03c4i, di >, i = 1,..., N is additively defined as: 1N, N = 1 di (labels (< \u03c4i, di >)). This accuracy is a fraction between [0, 1] with a higher accuracy representing a better discriminant. A decision tree learning algorithm tries to use a discriminant as a decision tree over predicate p1,.., pm, and result labels \"1,\" K. Typically, algorithms maximize the length (sciences) while maintaining the descriptive length as the smaller predicate tree over the predicate p1."}, {"heading": "4 Discriminant Analysis", "text": "In this section, we provide details of the maximum probability and approaches of the decision tree and compare their performance against scalable micro-benchmarks."}, {"heading": "4.1 Maximum Likelihood Approach", "text": "We will use an approach to derive a conjunctive discrimination based on the respective N-values. < < < # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "4.2 Decision Tree Learning Appraoch", "text": "To distinguish tracks, Discriminer uses Decision Tree Learning to learn classifiers that distinguish tracks. Faced with a set of N tracks on an experienced variable (label) L, containing endless values in the domain {'1,...,' K} and m feature variables (predicates) F = {f1,..., fm}, the goal of a classification algorithm is to create a partition of the space of feature variables in K-Disjoints A1,..., AK, so that the predicted value of L is i when the F variables assume value in Ai. Decision Tree Methods yield rectangular sets of Ai by partitioning the data set recursively one F variable at a time. CART (Classification and Regression Trees) is a popular and effective algorithm to learn Decision Tree-based classifiers. It constructs binary decision trees by researching characteristics and thresholds (the largest GART)."}, {"heading": "4.3 Performance Evaluation", "text": "We created a series of micro-benchmarks - which contain a side channel in time - to evaluate the performance of the decision-tree discriminator, which is calculated using Scikit Learn implementation of CART, and the maximum probability of conjunctive discriminants using an ILP implementation from the GLPK library. These micro-benchmarks consist of a series of programs that take a sequence of binary digits as input (say, a secret information), and perform some calculations whose execution time (forced with sleep commands) depends on a property of secret information. For the micro-benchmark series LSB0 and MSB0, the execution time shows a Gauss-distributed random variable, the mean of which is proportional to the position of the least significant 0 and most significant 0 in the secret. In addition, we have a micro-benchmark series of patents whose execution time is an execution time dependent on the position of the mean of the muster."}, {"heading": "5 Case Study: Understanding Traces with Decision Trees", "text": "Most of them are able to go in search of new ways to describe them. Most of them are able to go in search of new ways to describe them. Most of them are able to go in search of new ways to describe them.Most of them are able to go in search of new ways to describe them.Most of them are able to go in search of new ways to describe them.Most of them are able to go in search of new way.Most of them are able to go in search of new way.Most of them are able to go in search of new way.Most of them are able to go in search of new way.Most of them are able to go in search of new way.Most of them are able to go in search of new way.Most of them are in search of new way.Most of them are able to descrip.Most of them are descripte.Most of them are descripte.Most of them are descripte.Most of them are to descripte.Most of them are to descripte.Most of them are descripte.Most of them are to descripte.Most of them are to descripte.Most of them are to descripte.Most of them are to descripte.Most of them are to descripte.Most of them are to descripte.Most of them are to descripte.Most of them are to descripte.Most of them are to descripte.Most of them are to descripteas.Most of them are to descripteas.Most of them are to descripteas.most of them."}, {"heading": "6 Related Work", "text": "In [3], machine learning techniques are used to synthesize an NFA (non-deterministic finite automata) that represents all the correct traces of a program. In our setting, this would be equivalent to learning a differentiator for a cluster (of correct traces). In contrast, our decision trees distinguish several clusters. However, the discriminators we looked at in this paper are less meaningful than NFAs. The survey [22] provides an overview of other specification mineralization approaches. Malware and bug detection. In malware detection, machine learning techniques are used to learn classifiers, the programs into benign and malicious [1, 4, 7, 10, 17, 21]. In malware and bug detection, these normal patterns are detected. In malware detection, machine learning techniques are used to detect classifiers that detect errors in benign and malicious [1, 13, 17]."}, {"heading": "7 Conclusion", "text": "Abstract: We have introduced the problem of trace-set discrimination as a formalization of the practical problem of figuring out what can be derived from time-limited observations of the system. We have shown that the problem is NP-hard, and have proposed two scalable techniques to solve it. The first is ILP-based and can give formal guarantees about the discriminant found, but minority discriminators of a limited form. The second is based on decision trees, includes general discriminants, but does not provide formal guarantees. For three realistic applications, our tool produces a decision tree that is useful to explain time differences between executions.Future work has several interesting directions for future research.First, we will examine the extension of our framework to reactive systems by generalizing our idea of execution-time observations to sequences of time-driven events. Second, we will build the ability to monitor the network traffic of our tool to make it usable for security analysts for distributed architectures."}], "references": [{"title": "DroidAPIMiner: Mining API-level features for robust malware detection in Android", "author": ["Yousra Aafer", "Wenliang Du", "Heng Yin"], "venue": "In SPCN,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Rapidminer 5 operator reference", "author": ["Fareed Akthar", "Caroline Hahne"], "venue": "Rapid-I GmbH,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Automated classification and analysis of internet malware", "author": ["Michael Bailey", "Jon Oberheide", "Jon Andersen", "Z Morley Mao", "Farnam Jahanian", "Jose Nazario"], "venue": "In RAID,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Classification and Regression Trees", "author": ["L. Breiman", "J. Friedman", "R. Olshen", "C. Stone"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1984}, {"title": "Classification and regression trees", "author": ["L. Breiman", "J.H. Friedman", "R.A. Olshen", "C.I. Stone"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1984}, {"title": "Crowdroid: behaviorbased malware detection system for Android", "author": ["Iker Burguera", "Urko Zurutuza", "Simin Nadjm-Tehrani"], "venue": "In Workshop on Security and privacy in smartphones and mobile devices,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "The role of Occam\u2019s razor in knowledge discovery", "author": ["Pedro Domingos"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Predicting defect-prone software modules using support vector machines", "author": ["Karim O Elish", "Mahmoud O Elish"], "venue": "Journal of Systems and Software,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Near-optimal malware specifications from suspicious behaviors", "author": ["Matt Fredrikson", "Somesh Jha", "Mihai Christodorescu", "Reiner Sailer", "Xifeng Yan"], "venue": "In Security and Privacy (SP),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Constructing optimal binary decision trees is np-complete", "author": ["Laurent Hyafil", "Ronald L Rivest"], "venue": "Information Processing Letters,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1976}, {"title": "An exploratory technique for investigating large quantities of categorical data", "author": ["G.V. Kass"], "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1980}, {"title": "Effective and efficient malware detection at the end host", "author": ["Clemens Kolbitsch", "Paolo Milani Comparetti", "Christopher Kruegel", "Engin Kirda", "Xiao-yong Zhou", "XiaoFeng Wang"], "venue": "In USENIX Security,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Classification of software behaviors for failure detection: a discriminative pattern mining approach", "author": ["David Lo", "Hong Cheng", "Jiawei Han", "Siau-Cheng Khoo", "Chengnian Sun"], "venue": "In SIGKDD,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Foundations of Machine Learning", "author": ["Mehryar Mohri", "Afshin Rostamizadeh", "Ameet Talwalkar"], "venue": "ISBN 026201825X,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Induction of decision trees", "author": ["J. Ross Quinlan"], "venue": "Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1986}, {"title": "Learning and classification of malware behavior", "author": ["Konrad Rieck", "Thorsten Holz", "Carsten Willems", "Patrick D\u00fcssel", "Pavel Laskov"], "venue": "In Detection of Intrusions and Malware, and Vulnerability Assessment,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "A discriminative model approach for accurate duplicate bug report retrieval", "author": ["Chengnian Sun", "David Lo", "Xiaoyin Wang", "Jing Jiang", "Siau-Cheng Khoo"], "venue": "In ICSE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Introduction to data mining, volume 1", "author": ["Pang-Ning Tan", "Michael Steinbach", "Vipin Kumar"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Mining temporal specifications for error detection", "author": ["Westley Weimer", "George C Necula"], "venue": "In TACAS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Droidmat: Android malware detection through manifest and api calls tracing", "author": ["Dong-Jie Wu", "Ching-Hao Mao", "Te-En Wei", "Hahn-Ming Lee", "Kuo-Ping Wu"], "venue": "In JCIS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Specifications for free", "author": ["Andreas Zeller"], "venue": "In NFM,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "Then Discriminer uses the standard CART decision tree learning algorithm [5] to infer a decision tree that succinctly represents a discriminant using atomic predicates that characterize whether or not the trace invoked a particular method (shown in Figure 2).", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "A discrete probability distribution, or just distribution, over a finite set L is a function d : L\u2192[0, 1] such that \u2211 `\u2208L d(`) = 1.", "startOffset": 99, "endOffset": 105}, {"referenceID": 9, "context": "K ) possible discriminants! In particular, Hyafil and Rivest [11] show that the problem of learning optimal decision trees is NP-hard.", "startOffset": 61, "endOffset": 65}, {"referenceID": 6, "context": "Moreover, working with simpler monotone conjunctive discriminants is preferable [8] in the presence of noisy data, as using formal maximum likelihood model to learn arbitrary complex Boolean function would lead to over-fitting.", "startOffset": 80, "endOffset": 83}, {"referenceID": 9, "context": "We refine the result of [11] in our context to show that the problem of learning (monotone) conjunctive discriminants is already NP-hard.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "This accuracy is a fraction between [0, 1] with higher accuracy representing a better discriminant.", "startOffset": 36, "endOffset": 42}, {"referenceID": 14, "context": "A variety of efficient tree learning algorithms have been defined including ID3 [16], CART [6], CHAID [12] and many others [15, 19].", "startOffset": 80, "endOffset": 84}, {"referenceID": 4, "context": "A variety of efficient tree learning algorithms have been defined including ID3 [16], CART [6], CHAID [12] and many others [15, 19].", "startOffset": 91, "endOffset": 94}, {"referenceID": 10, "context": "A variety of efficient tree learning algorithms have been defined including ID3 [16], CART [6], CHAID [12] and many others [15, 19].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "A variety of efficient tree learning algorithms have been defined including ID3 [16], CART [6], CHAID [12] and many others [15, 19].", "startOffset": 123, "endOffset": 131}, {"referenceID": 17, "context": "A variety of efficient tree learning algorithms have been defined including ID3 [16], CART [6], CHAID [12] and many others [15, 19].", "startOffset": 123, "endOffset": 131}, {"referenceID": 1, "context": "org/stable/) and RapidMiner [2].", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "For a detailed description of the CART, we refer to [5].", "startOffset": 52, "endOffset": 55}, {"referenceID": 20, "context": "The survey [22] provides an overview of other specification mining approaches.", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "In malware detection, machine learning techniques are used to learn classifiers that classify programs into benign and malicious [1, 4, 7, 10, 13, 17, 21].", "startOffset": 129, "endOffset": 154}, {"referenceID": 2, "context": "In malware detection, machine learning techniques are used to learn classifiers that classify programs into benign and malicious [1, 4, 7, 10, 13, 17, 21].", "startOffset": 129, "endOffset": 154}, {"referenceID": 5, "context": "In malware detection, machine learning techniques are used to learn classifiers that classify programs into benign and malicious [1, 4, 7, 10, 13, 17, 21].", "startOffset": 129, "endOffset": 154}, {"referenceID": 8, "context": "In malware detection, machine learning techniques are used to learn classifiers that classify programs into benign and malicious [1, 4, 7, 10, 13, 17, 21].", "startOffset": 129, "endOffset": 154}, {"referenceID": 11, "context": "In malware detection, machine learning techniques are used to learn classifiers that classify programs into benign and malicious [1, 4, 7, 10, 13, 17, 21].", "startOffset": 129, "endOffset": 154}, {"referenceID": 15, "context": "In malware detection, machine learning techniques are used to learn classifiers that classify programs into benign and malicious [1, 4, 7, 10, 13, 17, 21].", "startOffset": 129, "endOffset": 154}, {"referenceID": 19, "context": "In malware detection, machine learning techniques are used to learn classifiers that classify programs into benign and malicious [1, 4, 7, 10, 13, 17, 21].", "startOffset": 129, "endOffset": 154}, {"referenceID": 7, "context": "In software bug detection, the task is to learn classifiers that classify programs behaviors into faulty and non-faulty [9, 14, 18, 20].", "startOffset": 120, "endOffset": 135}, {"referenceID": 12, "context": "In software bug detection, the task is to learn classifiers that classify programs behaviors into faulty and non-faulty [9, 14, 18, 20].", "startOffset": 120, "endOffset": 135}, {"referenceID": 16, "context": "In software bug detection, the task is to learn classifiers that classify programs behaviors into faulty and non-faulty [9, 14, 18, 20].", "startOffset": 120, "endOffset": 135}, {"referenceID": 18, "context": "In software bug detection, the task is to learn classifiers that classify programs behaviors into faulty and non-faulty [9, 14, 18, 20].", "startOffset": 120, "endOffset": 135}, {"referenceID": 12, "context": "[14] constructs a classifier to generalize known failures of software systems and to further detect (predict) other unknown failures.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "What properties about the internals of a program explain the possible differences in its overall running time for different inputs? In this paper, we propose a formal framework for considering this question we dub trace-set discrimination. We show that even though the algorithmic problem of computing maximum likelihood discriminants is NP-hard, approaches based on integer linear programming (ILP) and decision tree learning can be useful in zeroing-in on the program internals. On a set of Java benchmarks, we find that compactly-represented decision trees scalably discriminate with high accuracy\u2014more scalably than maximum likelihood discriminants and with comparable accuracy. We demonstrate on three larger case studies how decision-tree discriminants produced by our tool are useful for debugging timing side-channel vulnerabilities (i.e., where a malicious observer infers secrets simply from passively watching execution times) and availability vulnerabilities.", "creator": "LaTeX with hyperref package"}}}