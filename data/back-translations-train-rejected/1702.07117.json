{"id": "1702.07117", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2017", "title": "LTSG: Latent Topical Skip-Gram for Mutually Learning Topic Model and Vector Representations", "abstract": "Topic models have been widely used in discovering latent topics which are shared across documents in text mining. Vector representations, word embeddings and topic embeddings, map words and topics into a low-dimensional and dense real-value vector space, which have obtained high performance in NLP tasks. However, most of the existing models assume the result trained by one of them are perfect correct and used as prior knowledge for improving the other model. Some other models use the information trained from external large corpus to help improving smaller corpus. In this paper, we aim to build such an algorithm framework that makes topic models and vector representations mutually improve each other within the same corpus. An EM-style algorithm framework is employed to iteratively optimize both topic model and vector representations. Experimental results show that our model outperforms state-of-art methods on various NLP tasks.", "histories": [["v1", "Thu, 23 Feb 2017 07:16:03 GMT  (96kb,D)", "http://arxiv.org/abs/1702.07117v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jarvan law", "hankz hankui zhuo", "junhua he", "erhu rong"], "accepted": false, "id": "1702.07117"}, "pdf": {"name": "1702.07117.pdf", "metadata": {"source": "CRF", "title": "LTSG: Latent Topical Skip-Gram for Mutually Learning Topic Model and Vector Representations", "authors": ["Jarvan Law", "Hankz Hankui Zhuo", "Junhua He", "Erhu Rong"], "emails": ["JarvanLaw@gmail.com,", "zhuohank@mail.sysu.edu.cn", "hejunh@mail2.sysu.edu.cn", "rongerhu@mail2.sysu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "The fact is that we are in a position to be in a world in which we are in a position, in which we are able to change the world, in which we are able to change the world, \"he said."}, {"heading": "2 Preliminaries", "text": "In this section we briefly review the preparatory work for Latent Dirichlet Allocation (LDA), Skip-Gram and Topical Word Embeddings (TWE). We show some notations and their corresponding meanings in Table 1, which are used to describe the details of LDA, Skip-Gram and TWE."}, {"heading": "2.1 Latent Dirichlet Allocation", "text": "The latent dirichlet allocation (LDA) [Lead et al., 2003], a hierarchical Bayesian three-step model, is a well-developed and widely used probabilistic topic model. Extended to include probabilistic latent semantic indexing (PLSI) [Hofmann, 1999], LDA adds dirichlet priors to document-specific topic mixtures to overcome the excessive problem in PLSI. The generative process of the LDA aims to model each document as a mixture of topic complexes, each of which is associated with a multinomic word distribution. Given a document corpus D, it is assumed that each document has a distribution on K topics. The generative process of the LDA is presented as follows: 1. Draw for each topic k = 1 \u2192 K a distribution on the words k Dir (\u03b2) 2. Draw a distribution on the words Dir (\u03b2) 2. Draw for each document wqm, a sample."}, {"heading": "2.2 The Skip-Gram Model", "text": "The Skip-Gram model is a well-known framework for learning word vectors [Mikolov et al., 2013]. Skip-Gram aims to predict context words containing a target word in a sliding window, as shown in Figure 1 (A). In view of a document corpus D defined in Table 1, the goal of Skip-Gram is to maximize the average log probability L (D) = 1 \u2211 M m = 1Nm M \u2211 m = 1 Nm. The basic Skip-Gram formulation defines Pr (wm, n + j | wm, n) using the softmax function: Pr (wm, n + j | wm, n) = exp (vwm, Hierj \u00b7 vwm, n) sow = 1 + vw, vw (vw, vm) and wm (wm, wm) as effective and wm (wm, wm) as effective."}, {"heading": "2.3 Topical Word Embeddings", "text": "The TWE model uses topic zm, n of the target word to predict context topic, compared to just using the target word wm, n to predict context word in Skip-Gram. TWE is defined to maximize the following average log probabilities for each word token. TWE model uses topic zm, n of the target word to predict context word in Skip-Gram. TWE is defined to maximize the following average log probabilities. TWE model uses topic zm = 1 Nm topic to predict context word compared to only the target word wm, n to predict context word in Skip-Gram. TWE is defined to maximize the following average log probabilities."}, {"heading": "3.1 Topic Assignment via Gibbs Sampling", "text": "In order to perform Gibbs sampling, the main objective is to assign themes zm, n for each word token wm, n. In view of all subject mappings for all other words, the complete conditional distribution becomes Pr (zm, n = k | z \u2212 (m, n), w) when applying collapsed Gibbs sampling [Griffiths and Steyvers, 2004], Pr (zm, n = k | z \u2212 (m, n), w), n \u2212 (m, n) k \u2212 (m, n) k, wm, n + \u03b2 w = 1 n \u2212 (m, n) k \u2212 (m, n) m, n \u00b2 + K\u03b1, (5), where \u2212 (m, n) the current mappings of zm, n \u00b2 n \u00b2, n \u00b2, n \u00b2, n \u00b2, n \u00b2, n \u00b2, n \u00b2, n \u00b2, n \u00b2, n, n \u00b2, n \u00b2, n \u00b2, n \u00b2, n, n \u00b2, n \u00b2, n, n \u00b2, n \u00b2, n, n \u00b2, n, n \u00b2, n \u00b2, n, n \u00b2, n \u00b2, n, n, n \u00b2, n, n \u00b2, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, and, n, n, n, n, n, n, n, n, n, n, n, n, n, and, n, n, n, n, n, n, n, n, and, n, n, n, n, n, n, n, n, and, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, and, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n,"}, {"heading": "3.2 Topic Embeddings Computing", "text": "Thematic embedding aims rather at approximating the latent semantic centroids in the vector space than at a multinomial distribution. TWE trains theme embedding by word embedding learned from Skip-Gram. In the LTSG, we use a simple way to calculate the topic embedding for each topic. For the kgest topic, the topic embedding is calculated by averaging all words with their topic mapping z equivalent to k, i.e. tk = M \u2211 m = 1 Nm \u2211 n = 1I (zm, n = k) \u00b7 vwm, n \u0445 W w = 1 nk, w (9), where I (x) are indicator functions defined as 1 if x true and 0 otherwise. Similarly, you can design your own more complex training rules to train topic embedding such as TopicVec [Li et al., 2016] and Latent Topic Embedding (LTE)."}, {"heading": "3.3 Word Embeddings Training", "text": "LTSG aims for a child (u) to become an arbitrary child (u). (i) Let L (w) be the length of this way, then node (w) node (w, 1). (i) be the length of this way, then node (w, 1). (i) be the length of this way, then node (w, 1). (i) be the length of this way, then node (w, 1). (i) be the length of this way, then node (w, 1). (i) be the length of this way, then node (w, 1) = root and node (w). (i) be the length of this way, then node (w, 1) = root and node (w)."}, {"heading": "4 Experiments", "text": "In this section we evaluate our LTSG model under three aspects: Contextual word similarity, text classification and topic coherence. We use the data set 20NewsGroup, which consists of approximately 20,000 documents from 20 different newsgroups. For the baseline, we use the default settings of parameters, unless otherwise specified. Similar to TWE, we set the number of topics K = 80 and the dimensionality of both word embedding and topic embedding d = 400 for all relative models. In the LTSG, we initialize with I = 2500. We perform nItrs = 5 runs within our framework. We perform nGS = 200 Gibbs sampling iterations to update the topic assignment with \u03b1 = 0.01, \u03b2 = 0.1."}, {"heading": "4.1 Contextual Word Similarity", "text": "To evaluate the contextual word similarity, we use Stanford's Word Contextual Word Similarities (SCWS) dataset introduced by [Huang et al., 2012], which was also used to evaluate the current topic [Liu et al., 2015]. There are a total of 2,003 word pairs and their sensual contexts. For comparison, we consider the Spearman correlation similarity of the results of different models and human assessments. Following the TWE model, we use two results AvgSimC and MaxSimC to evaluate the multi-prototype model for contextual word similarity. The topic distribution Pr (z | w, c) is determined by considering c as a document with Pr (z | w, c), Pr (w | z), Pr (z | c), Pr (z), z), wz (z), z (z), cj), AvgSimC to measure the similarity between two words."}, {"heading": "4.2 Text Classification", "text": "In this subsection, we examine the effectiveness of LTSG for document modeling using multi-class text classification. 20NewsGroup Corpus is divided into training sets and test sets with a ratio of 60% to 40% for each category. We calculate macro-mean precision, recall, and F1 measurement to measure the performance of LTSG. We learn word and topic embedding on the training set and then model document embedding for both training sets and test sets. We then consider topic embedding as document features and train a linear classification using Liblinear [Fan et al., 2008]. We use VM, TK, vw to display document embedding, topic embedding, text embedding, and model documents on both topic-based and embedding methods as shown below. \u2022 LTSG-theta-theta. Document theme distribution is vm, Topic-Equmated (TSG-7)."}, {"heading": "4.3 Topic Coherence", "text": "In this section, we evaluate the topics generated by the LTSG using both quantitative and qualitative analyses. Here, we follow the same corpus and the parameters set for the LSTG model in Section 4.2. Quantitative Analysis Although helplessness (sustained similarity) has been widely used to evaluate topic models, [Chang et al., 2009] found that helplessness can hardly reflect the semantic coherence of individual topics. A higher topic coherence value [Mimno et al., 2011] revealed a higher correlation with human assessments in assessing the topic quality that has become popular in assessing topic models [Arora et al., 2013; Chen and Liu, 2014]. A higher topic coherence value indicates a more coherent topic. We calculate the score of the ten best words for each topic. We present the score for some topics of the LLLLLLLZ-LZ-ics in the top category of the DA Topic Category of the last topics of the overall SDA Table 4."}, {"heading": "5 Releated Work", "text": "Rencently, research on collaborative topic models and vector representations has made great strides in the NLP community. [Xie et al., 2015] proposed a Markov Random Field regulated LDA model (MRF-LDA) to include word similarities in topic modeling, and the MRF-LDA model encourages similar words to share the same topic in order to learn more coherent topics. [Das et al., 2015] suggested Gaussian LDA to use pre-trained word embedding in Gibbs patterns based on multivariate Gaussian distributions. [Nguyen et al., 2015] suggested LFLDA, which is modeled as a mixture of traditional categorical pollution distribution and an embedding link function. This work has given the belief that vector representations are capable of contributing to improving topic models. On the contrary, vector representations, in particular topic modeling, were promoted by large topic settings."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we present a general framework in which topic models and vector representations help each other. Experimental results show that the LTSG achieves the competitive outcomes associated with the state-of-the-art models. In particular, we can conclude that the topic model contributes to promoting word embedding in the LTSG model. We will consider the following future research directions: I) The number of topics must be predefined and the Gibbs sample is time consuming to train large-scale data using a single thread. we will examine non-parametric topic models [Teh et al., 2006] and parallel topic models [Liu et al., 2011]. II) There are many topic models and word embedding models that have been proposed to use in various tasks and specific areas. We will put together a package that can be extended to other word embedding models [Liu et al., 2011]."}], "references": [{"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["Arora et al", "2013] Sanjeev Arora", "Rong Ge", "Yonatan Halpern", "David M. Mimno", "Ankur Moitra", "David Sontag", "Yichen Wu", "Michael Zhu"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Latent dirichlet allocation", "author": ["Blei et al", "2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["Chang et al", "2009] Jonathan Chang", "Sean Gerrish", "Chong Wang", "Jordan L Boyd-Graber", "David M Blei"], "venue": "In NIPS,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Topic modeling using topics from many domains, lifelong learning and big data", "author": ["Chen", "Liu", "2014] Zhiyuan Chen", "Bing Liu"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Collobert et al", "2011] Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Gaussian LDA for topic models with word embeddings", "author": ["Das et al", "2015] Rajarshi Das", "Manzil Zaheer", "Chris Dyer"], "venue": "In Proceedings of ACL,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Topic modeling with document relative similarities", "author": ["Du et al", "2015] Jianguang Du", "Jing Jiang", "Dandan Song", "Lejian Liao"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["Fan et al", "2008] Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Finding scientific topics", "author": ["Griffiths", "Steyvers", "2004] Thomas L Griffiths", "Mark Steyvers"], "venue": "Proceedings of the National academy of Sciences,", "citeRegEx": "Griffiths et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2004}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann"], "venue": "[Hofmann,", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al", "2012] Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In ACL,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Latent topic embedding", "author": ["Jiang et al", "2016] Di Jiang", "Lei Shi", "Rongzhong Lian", "Hua Wu"], "venue": "In COLING,", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Generative topic embedding: a continuous representation of documents", "author": ["Li et al", "2016] Shaohua Li", "Tat-Seng Chua", "Jun Zhu", "Chunyan Miao"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "PLDA+: parallel latent dirichlet allocation with data placement and pipeline processing", "author": ["Liu et al", "2011] Zhiyuan Liu", "Yuzhou Zhang", "Edward Y. Chang", "Maosong Sun"], "venue": "ACM TIST,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Topical word embeddings", "author": ["Liu et al", "2015] Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "venue": "In AAAI,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al", "2013] Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean"], "venue": "In NIPS,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Optimizing semantic coherence in topic models", "author": ["Mimno et al", "2011] David M. Mimno", "Hanna M. Wallach", "Edmund M. Talley", "Miriam Leenders", "Andrew McCallum"], "venue": "In EMNLP,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Improving topic models with latent feature word representations", "author": ["Nguyen et al", "2015] Dat Quoc Nguyen", "Richard Billingsley", "Lan Du", "Mark Johnson"], "venue": "TACL,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Word features for latent dirichlet allocation", "author": ["Petterson et al", "2010] James Petterson", "Alexander J. Smola", "Tib\u00e9rio S. Caetano", "Wray L. Buntine", "Shravan M. Narayanamurthy"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1921\\E", "shortCiteRegEx": "al. et al\\.", "year": 1921}, {"title": "A hidden topic-based framework toward building applications with short web documents", "author": ["Phan et al", "2011] Xuan Hieu Phan", "Cam-Tu Nguyen", "DieuThu Le", "Minh Le Nguyen", "Susumu Horiguchi", "QuangThuy Ha"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Reisinger", "Mooney", "2010] Joseph Reisinger", "Raymond J. Mooney"], "venue": "In Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics,", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Socher et al", "2011] Richard Socher", "Cliff Chiung-Yu Lin", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In ICML,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Parsing with compositional vector grammars", "author": ["Socher et al", "2013] Richard Socher", "John Bauer", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In ACL,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Hierarchical dirichlet processes", "author": ["Teh et al", "2006] Yee Whye Teh", "Michael I Jordan", "Matthew J Beal", "David M Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "al. et al\\.,? \\Q2006\\E", "shortCiteRegEx": "al. et al\\.", "year": 2006}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian et al", "2010] Joseph P. Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "In ACL,", "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Incorporating word correlation knowledge into topic modeling", "author": ["Xie et al", "2015] Pengtao Xie", "Diyi Yang", "Eric P. Xing"], "venue": "In The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Extending Probabilistic Latent Semantic Indexing (PLSI) [Hofmann, 1999], LDA adds Dirichlet priors to document-specific topic mixtures to overcome the overfitting problem in PLSI.", "startOffset": 56, "endOffset": 71}], "year": 2017, "abstractText": "Topic models have been widely used in discovering latent topics which are shared across documents in text mining. Vector representations, word embeddings and topic embeddings, map words and topics into a low-dimensional and dense real-value vector space, which have obtained high performance in NLP tasks. However, most of the existing models assume the result trained by one of them are perfect correct and used as prior knowledge for improving the other model. Some other models use the information trained from external large corpus to help improving smaller corpus. In this paper, we aim to build such an algorithm framework that makes topic models and vector representations mutually improve each other within the same corpus. An EM-style algorithm framework is employed to iteratively optimize both topic model and vector representations. Experimental results show that our model outperforms state-of-art methods on various NLP tasks.", "creator": "LaTeX with hyperref package"}}}