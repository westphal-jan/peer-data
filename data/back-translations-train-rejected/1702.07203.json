{"id": "1702.07203", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2017", "title": "Utilizing Lexical Similarity for pivot translation involving resource-poor, related languages", "abstract": "We investigate the use of pivot languages for phrase-based statistical machine translation (PB-SMT) between related languages with limited parallel corpora. We show that subword-level pivot translation via a related pivot language is: (i) highly competitive with the best direct translation model and (ii) better than a pivot model which uses an unrelated pivot language, but has at its disposal large parallel corpora to build the source-pivot (S-P) and pivot-target (P-T) translation models. In contrast, pivot models trained at word and morpheme level are far inferior to their direct counterparts. We also show that using multiple related pivot languages can outperform a direct translation model. Thus, the use of subwords as translation units coupled with the use of multiple related pivot languages can compensate for the lack of a direct parallel corpus. Subword units make pivot models competitive by (i) utilizing lexical similarity to improve the underlying S-P and P-T translation models, and (ii) reducing loss of translation candidates during pivoting.", "histories": [["v1", "Thu, 23 Feb 2017 13:13:53 GMT  (28kb)", "http://arxiv.org/abs/1702.07203v1", "Submitted to ACL 2017, 10 pages, 9 tables"], ["v2", "Wed, 4 Oct 2017 20:55:03 GMT  (25kb)", "http://arxiv.org/abs/1702.07203v2", "Accepted at IJCNLP 2017, 7 pages, 7 tables"]], "COMMENTS": "Submitted to ACL 2017, 10 pages, 9 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["anoop kunchukuttan", "maulik shah", "pradyot prakash", "pushpak bhattacharyya"], "accepted": false, "id": "1702.07203"}, "pdf": {"name": "1702.07203.pdf", "metadata": {"source": "CRF", "title": "Utilizing Lexical Similarity for pivot translation involving resource-poor, related languages", "authors": ["Anoop Kunchukuttan", "Maulik Shah", "Pradyot Prakash"], "emails": ["anoopk@cse.iitb.ac.in", "maulik.shah@cse.iitb.ac.in", "pradyot@cse.iitb.ac.in", "pb@cse.iitb.ac.in"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.07 203v 1 [cs.C L] February 23, 2017 for phrase-based statistical machine translation (PB-SMT) between related languages with limited parallel corpora. We show that pivot translation at the subword level has a related pivot language: (i) is very competitive with the best direct translation model, and (ii) better than a pivot model that uses an unrelated pivot language but has large parallel corpora to build the source pivot (S-P) and pivot target (P-T) translation model. In contrast, pivot models trained at the word and morph level are significantly inferior to their direct counterparts. We also show that the use of multiple related pivot languages can surpass a direct translation model. Therefore, the use of subwords as translation units in a combination of several related languages can significantly correspond to the use of a parallel pivot."}, {"heading": "1 Introduction", "text": "This year, we will be able to put ourselves in a position to take the lead."}, {"heading": "2 Related Work", "text": "Our work encompasses two strands of research in statistical machine translation: (i) subwords as basic units for translation between related languages, and (ii) pivot-based machine translation. Modelling the lexical similarity between related languages is the key to building high-quality SMT systems with limited parallel corpora. This can be achieved through transformations at subword level. An alternative method of improving translation quality is the use of subwords as basic translation units. (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative method of improving translation quality is subwords as basic translation units. Subword units such as characters (Vilar et al., 2007; Tiedemann, 2009), character n-gram (SMi and Nakov, 2013), we do not have the translation unit."}, {"heading": "3 Lexical Similarity in related languages", "text": "Two words should be lexically similar if they have a similar form (spelling / pronunciation) and meaning, e.g. time is samay in Hindi, samayam in Malayalam. These words could be cognates, lateral borrowings or borrowings from other languages. Two languages should be lexically similar if they share a lot of lexically similar words. Lexical similarity is a key feature of related languages (Rensch, 1992), but this requires extensive fieldwork and linguistic analysis. For our analysis, we use a simpler measure of lexical similarities that depend on orthography and count the forms that show similarity in form and meaning (Latixus, 1992)."}, {"heading": "4 Pivot Translation for Related Languages", "text": "First, we train phrase-based SMT models between S-P and P-T language pairs with subverbs (in our case orthographic syllables). We create a pivot translation system by combining the S-P and P-T models using phrase table triangulation. If multiple pivot languages are available, linear interpolation is used to combine pivot translation models. In this section, we describe each component of our system and design decisions."}, {"heading": "4.1 Orthographic Syllable level translation", "text": "We use orthographic syllables (OS) (Kunchukuttan and Bhattacharyya, 2016c) as basic units of translation. It is a linguistically motivated variable unit of length, consisting of a consonant core with zero or more vowels (a C + V combination) (e.g. spacious as spa ciou s segmented). Since the vocabulary is much smaller than models at the morpheme and word level, data economy is no problem. The variable units of length provide the appropriate context for translation between related languages. This unit has surpassed N-gram, word and morpheme models in the task of translating between related languages. Or thographic syllables surpass other units even if: (i) the languages are not very closely related (ii), the languages have no genetic relationship, but only a contact relationship. The use of OS-level models for pivot translation ensures that the corresponding T and P models are better placed."}, {"heading": "4.2 Using a related pivot language via Triangulation", "text": "We use phrase table triangulation to merge the models srcpivot and pivot-tgt at OS level to create the phrase table of the pivot model. Triangulation connects the two tables on common phrases in the pivot language and calculates the probabilities in the phrase table (direct / inverse phrase and lexical translation probabilities) by starting from a generative process and making some assumptions of independence: P (s), p (s) and t (3), where s (s), p (s) and t (t) are source, pivot and target phrases. Compared to word and morpheme level models, models at OS level are likely to find more frequent pivot phrases due to the smaller vocabulary size."}, {"heading": "4.3 Multiple Pivot Languages", "text": "When multiple pivot languages are available, we can either use all pivot languages or select the best pivot language.The choice of pivot language is influenced by its relationship to the source and target languages (Paul et al., 2013).We also experimented with combining multiple pivot language translation models using linear interpolation (Bisazza et al., 2011).Linear interpolation assigns weights to each phrase table and the characteristic values for each phrase pair interpolated with these weights: P (s-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t."}, {"heading": "5 Experimental Setup", "text": "This section describes languages and data sets used in our experiments, and details of our system."}, {"heading": "5.1 Languages", "text": "We experimented with several languages from the two major language families of the Indian subcontinent (Indo-Aryan branch of Indo-European and Dravidian), which is considered a linguistic area over a long period of time due to the convergence of linguistic characteristics resulting from contact between languages (Emeneau, 1956). In particular, there is a considerable overlap between the vocabulary of these languages to varying degrees due to relatives, language contact and loan words from Sanskrit and English. All these languages exhibit a rich inflective morphology with Dravidian languages (and to a certain extent Marathi) and are agglutinative. Table 1 shows the languages involved and their lexical similarities."}, {"heading": "5.2 Dataset", "text": "For our experiments (Jha, 2012), we used the multilingual corpus of the Indian Language Corpora Initiative (ILCI) corpus2, which contains sentences from the fields of tourism and health and contains sentences that are geared towards 11 languages. This multilingual corpus allows a fair comparison of (i) direct and pivot translation systems, (ii) several pivot languages and (iii) lexical similarity of languages. Data distribution is as follows: - Training: 44,777, Tuning 1K, Test: 2K sentences. Language models for word-level systems have been trained on the landing page of Training Corpus plus monolingual corpora from different sources [back: 10M (Bojar et al., 2014), tam: 1M (Ramasamy et al., 2012), mar: 1.8M (news websites), times: 200K, ben: 400K, pan: 100K (Quasthoff. 2006), the target sentences of the S-2D are available."}, {"heading": "5.3 System details", "text": "PBSMT systems were trained with the Moses system (Koehn et al., 2007), with mgiza3 for alignment, the Grow-Diag-Final-and-Heuristic for symmetrization of word alignments, and batch MIRA (Cherry and Foster, 2012) for tuning. Cube pruning with Pop-Limit = 1000 was used for decoding (Kunchukuttan and Bhattacharyya, 2016a). We trained 5-gram LMs with Kneser-Ney for smoothing word and morph planes, and 10-gram LMs for orthographic syllable plane models. We used morphological segments trained with Morfessor (Virpioja et al., 2013) to obtain morphemes. These segments were trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006). We used the tmulat for phrapairing of 1 or 1 phrase pairing."}, {"heading": "6 Discussion: Subword units for pivot translation", "text": "In this section, we present and analyze the results of different subword units."}, {"heading": "6.1 Comparison of different subword units", "text": "Table 2 compares pivot-based SMT systems built with different units. We experimented with language triples across different combinations of language families (Indo-Aryan and Dravidian) and found that the pivot model at the OS level clearly outperforms the pivot models at the word and morphine level (an average improvement of about 61% and 14%, respectively) in each case. OS-level models show the greatest improvement over other units when the source and target languages belong to different families, showing that OS translation models can take advantage of the lexical similarity between these languages. Translation between agglutinative Dravidian languages also shows a significant improvement. OS-level pivot models are better than other units for two reasons alone. First, the underlying S-P and PT translation models are better (an average of 16% and 3% improvement over word and morphine models), but this alone does not explain the significant improvement in vocabulary size at the OS level."}, {"heading": "6.2 Comparison of pivot models with direct models", "text": "In addition, we compared the pivot system at the OS level with a direct system trained on different translation units (see Table 3). It outperforms a direct translation system at the word level between source and target by 5%, which is encouraging. Even more remarkable is that the pivot model at the OS level competes with the models at the morp and OS level (about 95% and 91% of their respective BLEU values, respectively).To put this fact in perspective, the BLEU values for pivot systems at the word level are far below their corresponding direct systems (about 15% and 35%, respectively).These observations suggest that a pivot at the OS level can better reconstruct the direct translation system compared to the pivot systems at the word and Morphe level, and that the resulting pivot systems at the OS level come pretty close to the best direct translation systems."}, {"heading": "6.3 Using an unrelated pivot language", "text": "In the experiments described so far, we have considered a related pivot language, with which source and target languages share small parallel corpora. We compare this to a very likely pivot scenario - the pivot language is a related language like English, with which source and target languages share many parallel corpora. For this experiment, we used Google Translate5 as a translation system with an unrelated pivot. It is known that Google Translate uses English as a pivot language for many translation pairs with which no direct translation corpus is available (TAUS, 2013). For the languages with which we have experimented, this can be demonstrated by the fact that English words appear in the translations provided by Google Translate. Google Translate is probably trained on large corpora, certainly on assignments from 3 github.com / moses-smt / mgiza 4github.com / tamtamthd / multigooghd / googoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoogoog"}, {"heading": "6.4 Cross-Domain Translation", "text": "We also investigated whether the pivot models at the OS level are resilient to domain changes by evaluating the translation models trained in tourism and healthcare on a 1000-sentence agricultural domain test set (from the ILCI corpus). Table 5 shows the results of these experiments. Also in this cross-domain translation scenario, the pivot models at the OS level outperform the pivot models at the Morph level and correspond to a direct morph-level model. Since OS-level models encounter unknown vocabulary in a new domain, they are less resistant to domain changes than OS-level models."}, {"heading": "7 Discussion: Multiple pivot languages", "text": "We discuss the results of multi-pivot experiments and examine the choice of pivot language. The experiments in this section refer to pivot models at OS level unless otherwise stated."}, {"heading": "7.1 Combining Multiple Pivot Models", "text": "We investigated whether multiple pivot translation systems could replace a direct translation system by combining several pivot translation systems using linear interpolation. We tried different weighting strategies: equal weighting and proportional to the lexical similarity of the pivot to (i) source, (ii) target, (iii) average of (i) and (ii) (see Table 6 for results). For each weighting strategy, the interpolated system outperformed not only the individual pivot systems, but also the direct translation system. We see an improvement of more than 1.5 BLEU points over the best pivot model. Previous studies have shown that multilingual pivot systems at the word and morph level were not able to outperform the direct system, possibly due to the effect of sparseness on triangulation (More et al., 2015; Dabre et al., 2015). Thus, it is notable that multilingualism between the two languages could contribute to overcoming the overlap."}, {"heading": "7.2 Augmenting direct translation with pivot translation", "text": "We extended the direct translation system with each of the interpolated systems discussed in the previous section by means of equal-weighted interpolation. In addition, we also tried to combine all pivot systems and the direct system by means of equal-weighted interpolation (all interpolated).The results are shown in Table 7. We note that the enhanced system improved the translation accuracy over the direct system by about 1 BLEU dot. Thus, the use of all related languages by means of pivoting and interpolation resulted in the best translation between a language pair."}, {"heading": "7.3 Choice of pivot language", "text": "Previous studies on the choice of pivot language have been hampered by the morphological diversity of pivot languages, and morphologically poorer pivots tend to perform better (More et al., 2015; Dabre et al., 2015; Paul et al., 2013). Thus, the different sparseness effects caused by morphological characteristics dictated the choice of pivot rather than the intrinsic properties of pivot language. Thrift is not a major problem in OS-level models, so we are able to investigate the effects of language characteristics on the choice of pivot language. We investigated whether the lexical similarity of the pivot language with the source and / or target language hadan has an impact on the quality of translation. We studiedMal-hin translation andmar-ben translation for several pivot languages (Table 8 shows the translation accuracy for different pivot languages along with xlecal similarity to the jargon of the language)."}, {"heading": "8 Translation involving an unrelated language", "text": "So far, we have reported on our investigations related to scenario [A], but scenario [B] (translation between unrelated languages via a source or target-related pivot point) is also a common situation. In this case, we use the transfer approach for pivot translation, but the two legs of pivot translation use different translation units. Marathi-Hindi translation takes place at the subword level, while Hindi translation takes place at the word level. In this section, we present the results of our investigations in scenario [B]. We experimented with the Indian language into English and vice versa via the Hindi OS as a pivot point. We trained our Hindi-English translation models at the level of the ILCI swinging-swinging-swinging-swinging model."}, {"heading": "9 Conclusion & Future Work", "text": "We have studied the use of pivot translations for translations with related languages when no direct parallel corpora is available. We show that pivot translations between related languages can compete with direct translation or perform better when subword translations are performed with multiple related pivot languages. Subword units make this possible by using lexical similarity and reducing panning losses (due to the low vocabulary). Our observations also contain lessons for designing multilingual translation systems. It is better to invest in building / dismantling a parallel corpus with a related pivot (preferably closer to the target) than with an unrelated pivot. When building translation systems between a lingua franca and an unrelated language, the construction / dismantling of parallel corpus between the lingua franca and related languages is not the best choice. Instead, it is better to place one of the related languages as a pivot as a lower level of translation that can result in more accurate design decisions for these current languages."}], "references": [{"title": "Catalanenglish statistical machine translation without parallel corpus: bridging through spanish", "author": ["Jose B Marino Adri\u2018a De Gispert"], "venue": "In In Proc. of 5th International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "Gispert.,? \\Q2006\\E", "shortCiteRegEx": "Gispert.", "year": 2006}, {"title": "Statistical machine translation between related languages", "author": ["Pushpak Bhattacharyya", "Mitesh Khapra", "Anoop Kunchukuttan."], "venue": "www.cfilt.iitb.ac.in/publications/naacl-2016-tutorial.pdf. NAACL Tutorials.", "citeRegEx": "Bhattacharyya et al\\.,? 2016", "shortCiteRegEx": "Bhattacharyya et al\\.", "year": 2016}, {"title": "Fill-up versus interpolation methods for phrase-based smt adaptation", "author": ["Arianna Bisazza", "Nick Ruiz", "Marcello Federico", "FBK-Fondazione Bruno Kessler."], "venue": "IWSLT. pages 136\u2013143.", "citeRegEx": "Bisazza et al\\.,? 2011", "shortCiteRegEx": "Bisazza et al\\.", "year": 2011}, {"title": "HindEnCorp \u2013 Hindi-English and Hindi-only Corpus for Machine Translation", "author": ["Ond\u0159ej Bojar", "Vojt\u011bch Diatka", "Pavel Rychl\u00fd", "Pavel Stra\u0148\u00e1k", "V\u0131\u0301t Suchomel", "Ale\u0161 Tamchyna", "Daniel Zeman"], "venue": "In Proceedings of the 9th International Conference", "citeRegEx": "Bojar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2014}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Colin Cherry", "George Foster."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Cherry and Foster.,? 2012", "shortCiteRegEx": "Cherry and Foster.", "year": 2012}, {"title": "Machine translation by triangulation: Making effective use of multi-parallel corpora", "author": ["Trevor Cohn", "Mirella Lapata."], "venue": "ACL.", "citeRegEx": "Cohn and Lapata.,? 2007", "shortCiteRegEx": "Cohn and Lapata.", "year": 2007}, {"title": "Leveraging small multilingual corpora for smt using many pivot languages", "author": ["Raj Dabre", "Fabien Cromieres", "Sadao Kurohashi", "Pushpak Bhattacharyya."], "venue": "HLT-NAACL. pages 1192\u20131202.", "citeRegEx": "Dabre et al\\.,? 2015", "shortCiteRegEx": "Dabre et al\\.", "year": 2015}, {"title": "Hindi-to-Urdu machine translation through transliteration", "author": ["Nadir Durrani", "Hassan Sajjad", "Alexander Fraser", "Helmut Schmid."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Durrani et al\\.,? 2010", "shortCiteRegEx": "Durrani et al\\.", "year": 2010}, {"title": "India as a lingustic area", "author": ["Murray B Emeneau."], "venue": "Language .", "citeRegEx": "Emeneau.,? 1956", "shortCiteRegEx": "Emeneau.", "year": 1956}, {"title": "The TDIL program and the Indian Language Corpora Initiative", "author": ["Girish Nath Jha."], "venue": "Language Resources and Evaluation Conference.", "citeRegEx": "Jha.,? 2012", "shortCiteRegEx": "Jha.", "year": 2012}, {"title": "Moses: Open source toolkit for Statistical Machine Translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Cognates and word alignment in bitexts", "author": ["Grzegorz Kondrak."], "venue": "Proceedings of the tenth machine translation summit (mt summit x) pages 305\u2013312.", "citeRegEx": "Kondrak.,? 2005", "shortCiteRegEx": "Kondrak.", "year": 2005}, {"title": "Faster decoding for subword level phrasebased smt between related languages", "author": ["Anoop Kunchukuttan", "Pushpak Bhattacharyya."], "venue": "Third Workshop on NLP for Similar Languages, Varieties and Dialects.", "citeRegEx": "Kunchukuttan and Bhattacharyya.,? 2016a", "shortCiteRegEx": "Kunchukuttan and Bhattacharyya.", "year": 2016}, {"title": "Learning variable length units for SMT between related languages via byte pair encoding", "author": ["Anoop Kunchukuttan", "Pushpak Bhattacharyya."], "venue": "ArXiv e-prints, arxiv:1610.06510 .", "citeRegEx": "Kunchukuttan and Bhattacharyya.,? 2016b", "shortCiteRegEx": "Kunchukuttan and Bhattacharyya.", "year": 2016}, {"title": "Orthographic syllable as basic unit for smt between related languages", "author": ["Anoop Kunchukutt n an Pushpak Bhattacharyya."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Bhattacharyya.,? 2016c", "shortCiteRegEx": "Bhattacharyya.", "year": 2016}, {"title": "The IIT Bombay SMT System for ICON 2014 Tools contest", "author": ["Anoop Kunchukuttan", "Ratish Pudupully", "Rajen Chatterjee", "Abhijit Mishra", "Pushpak Bhattacharyya."], "venue": "NLP Tools Contest at ICON 2014.", "citeRegEx": "Kunchukuttan et al\\.,? 2014", "shortCiteRegEx": "Kunchukuttan et al\\.", "year": 2014}, {"title": "Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons", "author": ["I Dan Melamed."], "venue": "Third Workshop on Very Large Corpora.", "citeRegEx": "Melamed.,? 1995", "shortCiteRegEx": "Melamed.", "year": 1995}, {"title": "Bitext maps and alignment via pattern recognition", "author": ["I Dan Melamed."], "venue": "Computational Linguistics 25(1):107\u2013130.", "citeRegEx": "Melamed.,? 1999", "shortCiteRegEx": "Melamed.", "year": 1999}, {"title": "Augmenting pivot based smt with word segmentation", "author": ["Rohit More", "Anoop Kunchukuttan", "Raj Dabre", "Pushpak Bhattacharyya."], "venue": "International Conference on Natural Language Processing.", "citeRegEx": "More et al\\.,? 2015", "shortCiteRegEx": "More et al\\.", "year": 2015}, {"title": "Combining word-level and character-level models for machine translation between closely-related languages", "author": ["Preslav Nakov", "J\u00f6rg Tiedemann."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short", "citeRegEx": "Nakov and Tiedemann.,? 2012", "shortCiteRegEx": "Nakov and Tiedemann.", "year": 2012}, {"title": "How to choose the best pivot language for automatic translation of low-resource languages", "author": ["Michael Paul", "Andrew Finch", "Eiichrio Sumita."], "venue": "ACM Transactions on Asian Language Information Processing (TALIP) 12(4):14.", "citeRegEx": "Paul et al\\.,? 2013", "shortCiteRegEx": "Paul et al\\.", "year": 2013}, {"title": "Corpus portal for search in monolingual corpora", "author": ["Uwe Quasthoff", "Matthias Richter", "Christian Biemann."], "venue": "Proceedings of the fifth international conference on language resources and evaluation.", "citeRegEx": "Quasthoff et al\\.,? 2006", "shortCiteRegEx": "Quasthoff et al\\.", "year": 2006}, {"title": "Simple syntactic and morphological processing can help english-hindi statistical machine translation", "author": ["Ananthakrishnan Ramanathan", "Jayprasad Hegde", "Ritesh M Shah", "Pushpak Bhattacharyya", "M Sasikumar."], "venue": "IJCNLP. pages 513\u2013520.", "citeRegEx": "Ramanathan et al\\.,? 2008", "shortCiteRegEx": "Ramanathan et al\\.", "year": 2008}, {"title": "Morphological Processing for English-Tamil Statistical Machine Translation", "author": ["Loganathan Ramasamy", "Ond\u0159ej Bojar", "Zden\u011bk \u017dabokrtsk\u00fd."], "venue": "Proceedings of the Workshop on Machine Translation and Parsing in Indian Languages.", "citeRegEx": "Ramasamy et al\\.,? 2012", "shortCiteRegEx": "Ramasamy et al\\.", "year": 2012}, {"title": "Calculating lexical similarity", "author": ["Calvin R Rensch."], "venue": "Eugene H. Casad, editor, Windows on bilingualism, Summer Institute of Linguistics and the University of Texas at Arlington.", "citeRegEx": "Rensch.,? 1992", "shortCiteRegEx": "Rensch.", "year": 1992}, {"title": "Character-based PBSMT for closely related languages", "author": ["J\u00f6rg Tiedemann."], "venue": "Proceedings of the 13th Conference of the European Association for Machine Translation.", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "Character-based pivot translation for under-resourced languages and domains", "author": ["J\u00f6rg Tiedemann."], "venue": "EACL.", "citeRegEx": "Tiedemann.,? 2012", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Analyzing the use of character-level translation with sparse and noisy datasets", "author": ["J\u00f6rg Tiedemann", "Preslav Nakov."], "venue": "RANLP.", "citeRegEx": "Tiedemann and Nakov.,? 2013", "shortCiteRegEx": "Tiedemann and Nakov.", "year": 2013}, {"title": "A comparison of pivot methods for phrase-based statistical machine translation", "author": ["Masao Utiyama", "Hitoshi Isahara."], "venue": "HLT-NAACL. pages 484\u2013491.", "citeRegEx": "Utiyama and Isahara.,? 2007", "shortCiteRegEx": "Utiyama and Isahara.", "year": 2007}, {"title": "Can we translate letters", "author": ["David Vilar", "Jan-T Peter", "Hermann Ney"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation", "citeRegEx": "Vilar et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vilar et al\\.", "year": 2007}, {"title": "Morfessor 2.0: Python implementation and extensions for morfessor baseline", "author": ["Sami Virpioja", "Peter Smit", "Stig-Arne Gr\u00f6nroos", "Mikko Kurimo"], "venue": null, "citeRegEx": "Virpioja et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Virpioja et al\\.", "year": 2013}, {"title": "Pivot language approach for phrase-based statistical machine translation", "author": ["Hua Wu", "Haifeng Wang."], "venue": "Machine Translation 21(3):165\u2013181.", "citeRegEx": "Wu and Wang.,? 2007", "shortCiteRegEx": "Wu and Wang.", "year": 2007}], "referenceMentions": [{"referenceID": 1, "context": "long period of time (Bhattacharyya et al., 2016).", "startOffset": 20, "endOffset": 48}, {"referenceID": 29, "context": "is useful in scenario [A] (Vilar et al., 2007; Kunchukuttan and Bhattacharyya, 2016c).", "startOffset": 26, "endOffset": 85}, {"referenceID": 7, "context": "Lexical similarity can be modelled in the standard SMT pipeline by transliteration of words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al.", "startOffset": 107, "endOffset": 129}, {"referenceID": 19, "context": ", 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014).", "startOffset": 27, "endOffset": 81}, {"referenceID": 15, "context": ", 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014).", "startOffset": 27, "endOffset": 81}, {"referenceID": 29, "context": "Subword units like character (Vilar et al., 2007; Tiedemann, 2009), character n-gram (Tiedemann and Nakov, 2013), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016c) and Byte Pair Encoded units (Kunchukuttan and Bhattacharyya, 2016b) have been explored and have been shown to improve translation quality to varying degrees.", "startOffset": 29, "endOffset": 66}, {"referenceID": 25, "context": "Subword units like character (Vilar et al., 2007; Tiedemann, 2009), character n-gram (Tiedemann and Nakov, 2013), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016c) and Byte Pair Encoded units (Kunchukuttan and Bhattacharyya, 2016b) have been explored and have been shown to improve translation quality to varying degrees.", "startOffset": 29, "endOffset": 66}, {"referenceID": 27, "context": ", 2007; Tiedemann, 2009), character n-gram (Tiedemann and Nakov, 2013), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016c) and Byte Pair Encoded units (Kunchukuttan and Bhattacharyya, 2016b) have been explored and have been shown to improve translation quality to varying degrees.", "startOffset": 43, "endOffset": 70}, {"referenceID": 13, "context": ", 2007; Tiedemann, 2009), character n-gram (Tiedemann and Nakov, 2013), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016c) and Byte Pair Encoded units (Kunchukuttan and Bhattacharyya, 2016b) have been explored and have been shown to improve translation quality to varying degrees.", "startOffset": 163, "endOffset": 202}, {"referenceID": 28, "context": "(i) synthetic corpus generation (Adri\u2018a De Gispert, 2006) (ii) transferbased/pipelining (Utiyama and Isahara, 2007) (iii) phrase-table triangulation (Utiyama and Isahara, 2007; Wu and Wang, 2007; Cohn and Lapata, 2007).", "startOffset": 88, "endOffset": 115}, {"referenceID": 28, "context": "(i) synthetic corpus generation (Adri\u2018a De Gispert, 2006) (ii) transferbased/pipelining (Utiyama and Isahara, 2007) (iii) phrase-table triangulation (Utiyama and Isahara, 2007; Wu and Wang, 2007; Cohn and Lapata, 2007).", "startOffset": 149, "endOffset": 218}, {"referenceID": 31, "context": "(i) synthetic corpus generation (Adri\u2018a De Gispert, 2006) (ii) transferbased/pipelining (Utiyama and Isahara, 2007) (iii) phrase-table triangulation (Utiyama and Isahara, 2007; Wu and Wang, 2007; Cohn and Lapata, 2007).", "startOffset": 149, "endOffset": 218}, {"referenceID": 5, "context": "(i) synthetic corpus generation (Adri\u2018a De Gispert, 2006) (ii) transferbased/pipelining (Utiyama and Isahara, 2007) (iii) phrase-table triangulation (Utiyama and Isahara, 2007; Wu and Wang, 2007; Cohn and Lapata, 2007).", "startOffset": 149, "endOffset": 218}, {"referenceID": 26, "context": "Characterbased pivot translation has been explored for one leg of the pivot system (S-P or P-T) when the pivot is related to either the source or target (but not both) (Tiedemann, 2012; Tiedemann and Nakov, 2013).", "startOffset": 168, "endOffset": 212}, {"referenceID": 27, "context": "Characterbased pivot translation has been explored for one leg of the pivot system (S-P or P-T) when the pivot is related to either the source or target (but not both) (Tiedemann, 2012; Tiedemann and Nakov, 2013).", "startOffset": 168, "endOffset": 212}, {"referenceID": 18, "context": "Morpheme level pivot translation has been shown to be better than word level pivot translation (More et al., 2015).", "startOffset": 95, "endOffset": 114}, {"referenceID": 6, "context": "(2015) and Dabre et al. (2015) have experimented with multiple pivot languages, but the translation accuracy is lower than the direct model.", "startOffset": 11, "endOffset": 31}, {"referenceID": 24, "context": "Ethnologue computes the percentage of lexical similarity between two linguistic varieties by comparing a set of standardized wordlists and counting those forms that show similarity in both form and meaning (Rensch, 1992).", "startOffset": 206, "endOffset": 220}, {"referenceID": 16, "context": "We use the Longest Common Subsequence Ratio (LCSR) as a measure of lexical similarity between two strings (Melamed, 1995):", "startOffset": 106, "endOffset": 121}, {"referenceID": 17, "context": "In addition to its utility for measuring lexical similarity between words (Melamed, 1999; Kondrak, 2005), it can also be used to compare sentences (assuming the two sentences to be a sequence of characters).", "startOffset": 74, "endOffset": 104}, {"referenceID": 11, "context": "In addition to its utility for measuring lexical similarity between words (Melamed, 1999; Kondrak, 2005), it can also be used to compare sentences (assuming the two sentences to be a sequence of characters).", "startOffset": 74, "endOffset": 104}, {"referenceID": 26, "context": "reference translation and output of MT system (Tiedemann, 2012)) or different languages (e.", "startOffset": 46, "endOffset": 63}, {"referenceID": 9, "context": "These were computed on the multilingually aligned ILCI parallel corpus (Jha, 2012).", "startOffset": 71, "endOffset": 82}, {"referenceID": 6, "context": "Hence, data sparsity, a problem recognized by Dabre et al. (2015) and More et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 6, "context": "Hence, data sparsity, a problem recognized by Dabre et al. (2015) and More et al. (2015), will be a lesser impediment to pivoting for OS level models.", "startOffset": 46, "endOffset": 89}, {"referenceID": 20, "context": "The choice of pivot language is affected by its relatedness to the source and target language (Paul et al., 2013).", "startOffset": 94, "endOffset": 113}, {"referenceID": 2, "context": "We also experimented with combining multiple pivot language translation models using linear interpolation (Bisazza et al., 2011).", "startOffset": 106, "endOffset": 128}, {"referenceID": 8, "context": "The Indian subcontinent is considered a linguistic area (Emeneau, 1956) due to convergence of linguistic properties as a result of contact between languages over a long period of time.", "startOffset": 56, "endOffset": 71}, {"referenceID": 9, "context": "We used the multilingual Indian Language Corpora Initiative (ILCI) corpus for our experiments (Jha, 2012), containing sentences from tourism and health domains.", "startOffset": 94, "endOffset": 105}, {"referenceID": 3, "context": "Language models for word-level systems were trained on the target side of training corpora plus monolingual corpora from various sources [hin: 10M (Bojar et al., 2014), tam: 1M (Ramasamy et al.", "startOffset": 147, "endOffset": 167}, {"referenceID": 23, "context": ", 2014), tam: 1M (Ramasamy et al., 2012), mar: 1.", "startOffset": 17, "endOffset": 40}, {"referenceID": 21, "context": "8M (news websites), mal: 200K, ben: 400K, pan: 100K (Quasthoff et al., 2006) sentences].", "startOffset": 52, "endOffset": 76}, {"referenceID": 10, "context": "PBSMT systems were trained using the Moses system (Koehn et al., 2007), withmgiza for alignment, the grow-diag-final-and heuristic for symmetrization of word alignments, and Batch MIRA (Cherry and Foster, 2012) for tuning.", "startOffset": 50, "endOffset": 70}, {"referenceID": 4, "context": ", 2007), withmgiza for alignment, the grow-diag-final-and heuristic for symmetrization of word alignments, and Batch MIRA (Cherry and Foster, 2012) for tuning.", "startOffset": 122, "endOffset": 147}, {"referenceID": 12, "context": "Cube pruning with pop-limit=1000 was used for decoding (Kunchukuttan and Bhattacharyya, 2016a).", "startOffset": 55, "endOffset": 94}, {"referenceID": 30, "context": "We use unsupervised morphological segmenters trained with Morfessor (Virpioja et al., 2013) for obtaining morphemes.", "startOffset": 68, "endOffset": 91}, {"referenceID": 21, "context": "These segmenters were trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006).", "startOffset": 72, "endOffset": 96}, {"referenceID": 2, "context": "We use the combine-ptables (Bisazza et al., 2011), (part of the Moses distribution), for linear interpolation of phrase tables.", "startOffset": 27, "endOffset": 49}, {"referenceID": 18, "context": "Previous studies have shown that word and morph level multiple pivot systems were not able to outperform the direct system, possibly due to the effect of sparsity on triangulation (More et al., 2015; Dabre et al., 2015).", "startOffset": 180, "endOffset": 219}, {"referenceID": 6, "context": "Previous studies have shown that word and morph level multiple pivot systems were not able to outperform the direct system, possibly due to the effect of sparsity on triangulation (More et al., 2015; Dabre et al., 2015).", "startOffset": 180, "endOffset": 219}, {"referenceID": 18, "context": "Previous studies on the choice of pivot language have been impeded by the morphological diversity of the pivot languages and morphologically poorer pivots tend to perform better (More et al., 2015; Dabre et al., 2015; Paul et al., 2013).", "startOffset": 178, "endOffset": 236}, {"referenceID": 6, "context": "Previous studies on the choice of pivot language have been impeded by the morphological diversity of the pivot languages and morphologically poorer pivots tend to perform better (More et al., 2015; Dabre et al., 2015; Paul et al., 2013).", "startOffset": 178, "endOffset": 236}, {"referenceID": 20, "context": "Previous studies on the choice of pivot language have been impeded by the morphological diversity of the pivot languages and morphologically poorer pivots tend to perform better (More et al., 2015; Dabre et al., 2015; Paul et al., 2013).", "startOffset": 178, "endOffset": 236}, {"referenceID": 20, "context": "This provides further evidence to the observations by Paul et al. (2013) that target language features are more important for \u201ccoherent\u201d language pairs.", "startOffset": 54, "endOffset": 73}, {"referenceID": 22, "context": "For EnglishIndian language (IL) translation, rule-based source reordering (Ramanathan et al., 2008) was used to overcome the structural divergence between English and Indian languages (English is an SVO languages and Indian languages are SOV).", "startOffset": 74, "endOffset": 99}], "year": 2017, "abstractText": "We investigate the use of pivot languages for phrase-based statistical machine translation (PB-SMT) between related languages with limited parallel corpora. We show that subword-level pivot translation via a related pivot language is: (i) highly competitive with the best direct translation model and (ii) better than a pivot model which uses an unrelated pivot language, but has at its disposal large parallel corpora to build the source-pivot (S-P) and pivot-target (P-T) translation models. In contrast, pivot models trained at word and morpheme level are far inferior to their direct counterparts. We also show that using multiple related pivot languages can outperform a direct translation model. Thus, the use of subwords as translation units coupled with the use of multiple related pivot languages can compensate for the lack of a direct parallel corpus. Subword units make pivot models competitive by (i) utilizing lexical similarity to improve the underlying S-P and P-T translation models, and (ii) reducing loss of translation candidates during pivoting.", "creator": "LaTeX with hyperref package"}}}