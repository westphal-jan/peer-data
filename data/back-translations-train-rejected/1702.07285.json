{"id": "1702.07285", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2017", "title": "Are Emojis Predictable?", "abstract": "Emojis are ideograms which are naturally combined with plain text to visually complement or condense the meaning of a message. Despite being widely used in social media, their underlying semantics have received little attention from a Natural Language Processing standpoint. In this paper, we investigate the relation between words and emojis, studying the novel task of predicting which emojis are evoked by text-based tweet messages. We train several models based on Long Short-Term Memory networks (LSTMs) in this task. Our experimental results show that our neural model outperforms two baselines as well as humans solving the same task, suggesting that computational models are able to better capture the underlying semantics of emojis.", "histories": [["v1", "Thu, 23 Feb 2017 16:47:01 GMT  (316kb,D)", "https://arxiv.org/abs/1702.07285v1", "To appear at EACL 2017"], ["v2", "Fri, 24 Feb 2017 12:59:19 GMT  (316kb,D)", "http://arxiv.org/abs/1702.07285v2", "To appear at EACL 2017"]], "COMMENTS": "To appear at EACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["francesco barbieri", "miguel ballesteros", "horacio saggion"], "accepted": false, "id": "1702.07285"}, "pdf": {"name": "1702.07285.pdf", "metadata": {"source": "CRF", "title": "Are Emojis Predictable?", "authors": ["Francesco Barbieri", "Miguel Ballesteros", "Horacio Saggion"], "emails": ["horacio.saggion}@upf.edu", "miguel.ballesteros@ibm.com"], "sections": [{"heading": "1 Introduction", "text": "The advent of social media has brought with it a novel way of communicating, in which meaning is created through the combination of short text messages and visual enhancements, the so-called emojis. This visual language is now a de facto standard for online communication, available not only on Twitter, but also on other major online platforms such as Facebook, Whatsapp or Instagram.Despite its status as a language form, emojis have hardly been studied from the standpoint of Natural Language Processing (NLP) until now. Notable exceptions include studies focusing on the semantics and use of emojis (Aoki and Uchida, 2011; Barbieri et al, 2016a; Barbieri et al, 2016b; Barbieri et al, 2016c; Eisner et al., 2016; Ljubes, ic and Fis, 2016) or feelings (Novak et al, 2015)."}, {"heading": "100.7 89.9 59 33.8 28.6 27.9 22.5 21.5 21 20.8", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Dataset and Task", "text": "Record: We retrieved 40 million tweets using the Twitter APIs3. Tweets were posted between October 2015 and May 2016, geographically located in the United States of America. We removed all hyperlinks from each tweet and lowered all textual content to reduce noise and sparseness. From the record, we selected tweets containing one and only one of the 20 most common emojis, resulting in a definitive dataset4 of 584,600 tweets. In the experiments, we also took into account the subsets of the 10 (502,700 tweets) and 5 most common emojis (341,500 tweets). See Table 1 for the 20 most common emojis we consider in this work. Task: We remove the emoji from the sequence of tokens and use it as a label for both training and testing. The task of our machine learning models is to predict the single emoji that will appear in the input tweet."}, {"heading": "3 Models", "text": "In this section, we present and motivate the models we use to predict an emoji given to a tweet. The first model is an architecture based on recurrent neural networks (Section 3.1) and the second and third are the two baselines (Section 3.2.1 and 3.2.2). The two biggest differences between the RNs and the baselines are that the RNNs take word sequences into account, and thus the entire context."}, {"heading": "3.1 Bi-Directional LSTMs", "text": "Given the proven effectiveness and impact of recurrent neural networks in various tasks (Chung et al., 2014; Vinyals et al., 2015; Dzmitry et al., 2014; Dyer et al., 2015; Lample et al., 2016; Wang et al., 2016, among others), which includes the modeling of tweets (Dhingra et al., 2016), our emoji prediction model is based on bidirektional3https: / / dev.twitter.com 4Available at http: / / sempub.taln.upf.edu / tw / eacl17Long Short-term memory networks (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005). The B-LSTM word can be formalized as follows: s = max {0, W [fw] + d}, where W is a learned parameter matrix, f."}, {"heading": "3.2 Baselines", "text": "In this section we describe the two baselines. In contrast to the previous model, the baselines do not consider the word sequence. In the second baseline (Section 3.2.2), however, we abstract via pure word representation using semantic vectors previously trained on Twitter data."}, {"heading": "3.2.1 Bag of Words", "text": "We used a bag of word classifiers as a starting point, as it has been used successfully in various classification tasks such as mood analysis and subject modeling (Wallach, 2006; Lead, 2012; Titov and McDonald, 2008; Maas et al., 2011; Davidov et al., 2010). We represent each message with a vector of the most informative characters (including punctuation marks), selected by Term Frequency \u2212 Inverse Document Frequency (TFIDF). We use an L2 regulated logistic regression classifier to make predictions."}, {"heading": "3.2.2 Skip-Gram Vector Average", "text": "We train a Skip-gram model (Mikolov et al., 2013), which we learned from 65M tweets (where test cases were removed), to learn Twitter semantic vectors. Then, we build a model (henceforth AVG), which represents each message as the average of the vectors corresponding to each token of the tweet. Formally, each message m becomes with the vector Vm: V m = \u2211 t-Tm St | Tm | Where Tm is the set of tokens contained in message m, St is the vector of tokens t in the Skipgram model, and | Tm | is the number of tokens in m. After we receive a representation of each message, we train an L2-regulated logistic regression (with apparently equal to 0.001)."}, {"heading": "4 Experiments and Evaluation", "text": "To investigate the relationship between words and emojis, we conducted two different experiments: in the first experiment, we compared our machine learning models, and in the second experiment, we selected the system that works best and compared it to humans."}, {"heading": "4.1 First Experiment", "text": "This experiment is a classification task in which the unique emojis are removed from each tweet and used as a label for the entire tweet. We use three sets of data, each containing the 5, 10 and 20 most common emojis (see Section 2). We analyze the performance of the five models described in Section 3: a bag of words, a bi-directional LSTM model with character-based representations (Char-BLSTM), a bi-directional LSTM model with standard word representations (Word-BLSTM), but the latter two models were trained with / without pre-formed word vectors. To pre-train the word vectors, we use a modified Skip-gram model (Ling et al., 2015a), which is based on Gigaword corpus7 Version 5.We divide each data set into three parts, Training (80%), Development (10%) and Testing (10%). The three subsets are selected from the automated and oldest systems in order."}, {"heading": "4.2 Second Experiment", "text": "Given that Miller et al. (2016) pointed out that people tend to interpret emojis multiple times, we conducted an experiment in which we evaluated human and machine performance on the same task. We randomly selected 1,000 tweets from our test set of the 5 most common emojis used in the previous experiment, and asked people if, after reading a tweet (where the emoji was removed), they would predict the emoji that the text had generated. We chose to abandon the 5 most common emojis in order to reduce the annotation effort. After showing the text of the tweet, we asked the human comments \"What is the emoji you would include in the tweet?\" and gave the opportunity to select one of 5 possible emojis, and, and, and. Using crowdsourcing platform CrowdFlower, we designed an experiment in which the same tweet was presented to four commentators (selecting the final label by majority agreement)."}, {"heading": "5 Conclusions", "text": "Emojis are widely used on social media, although little is known about their use and semantics, especially because emojis are used differently in different communities (Barbieri et al., 2016a; Barbieri et al., 2016b). In this work, we provide a neural architecture for modeling the semantics of emojis and examine the relationship between words and emojis. For the first time, we proposed an automatic method to predict the most likely associated emojis in a tweet. We showed that the LSTMs outperform humans in the same emoji prediction task, suggesting that automatic systems can generalize the use of emojis better than humans. Furthermore, the good accuracy of the LSTMs suggests that there is an important and unique relationship between word sequences and emojis. As future work, we plan to make the model able to predict more than one emoji per tweet and explore the position of the emoji in the tweet, as the prediction of the emoji can be an important clusion for the task."}, {"heading": "Acknowledgments", "text": "We thank the three reviewers for their time and useful suggestions. First and third authors acknowledge the support of the TUNER project (TIN2015-65308-C5-5-R, MINECO / FEDER, UE) and the Maria de Maeztu Units of Excellence Programme (MDM-2015-0502)."}], "references": [{"title": "A method for automatically generating the emotional vectors of emoticons using weblog articles", "author": ["Aoki", "Uchida2011] Sho Aoki", "Osamu Uchida"], "venue": "In Proceedings of the 10th WSEAS International Conference on Applied Computer and Applied", "citeRegEx": "Aoki et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Aoki et al\\.", "year": 2011}, {"title": "Improved transitionbased parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A. Smith"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Revealing Patterns of Twitter Emoji Usage in Barcelona and Madrid", "author": ["Luis Espinosa Anke", "Horacio Saggion"], "venue": "In 19 th International Conference of the Catalan Association for Artificial Intelligence,", "citeRegEx": "Barbieri et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Barbieri et al\\.", "year": 2016}, {"title": "2016b. How Cosmopolitan Are Emojis? Exploring Emojis Usage and Meaning over Different Languages with Distributional Semantics", "author": ["German Kruszewski", "Francesco Ronzano", "Horacio Saggion"], "venue": null, "citeRegEx": "Barbieri et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Barbieri et al\\.", "year": 2016}, {"title": "What does this emoji mean? a vector space skip-gram model for twitter emojis", "author": ["Francesco Ronzano", "Horacio Saggion"], "venue": "In Language Resources and Evaluation conference,", "citeRegEx": "Barbieri et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Barbieri et al\\.", "year": 2016}, {"title": "Probabilistic topic models", "author": ["David M Blei"], "venue": "Communications of the ACM,", "citeRegEx": "Blei.,? \\Q2012\\E", "shortCiteRegEx": "Blei.", "year": 2012}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Semi-supervised recognition of sarcastic sentences in twitter and amazon", "author": ["Oren Tsur", "Ari Rappoport"], "venue": "In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Davidov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Tweet2vec: Character-based distributed representations for social media", "author": ["Zhong Zhou", "Dylan Fitzpatrick", "Michael Muehl", "William Cohen"], "venue": "In Proceedings of the 54th Annual Meeting of the Association", "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho Kyunghyun", "Bengio Yoshua"], "venue": "Proceeding of the third International Conference on Learning Representations,", "citeRegEx": "Dzmitry et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dzmitry et al\\.", "year": 2014}, {"title": "emoji2vec: Learning emoji representations from their description", "author": ["Eisner et al.2016] Ben Eisner", "Tim Rockt\u00e4schel", "Isabelle Augenstein", "Matko Bosnjak", "Sebastian Riedel"], "venue": "In Proceedings of The Fourth International Workshop on Natural Lan-", "citeRegEx": "Eisner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eisner et al\\.", "year": 2016}, {"title": "Framewise phoneme classification with bidirectional LSTM networks", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Neural architectures for named entity recognition", "author": ["Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "2015a. Two/too simple adaptations of word2vec for syntax problems", "author": ["Ling et al.2015a] Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Compu-", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models", "author": ["Ling et al.2015b] Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "A global analysis of emoji usage", "author": ["Ljube\u0161ic", "Fi\u0161er2016] Nikola Ljube\u0161ic", "Darja Fi\u0161er"], "venue": "In Proceedings of the 10th Web as Corpus Workshop (WAC-X) and the EmpiriST Shared Task,", "citeRegEx": "Ljube\u0161ic et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ljube\u0161ic et al\\.", "year": 2016}, {"title": "Learning word vectors for sentiment analysis", "author": ["Maas et al.2011] Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Quoc V Le", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1309.4168", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Blissfully Happy\u201d or Ready to Fight: Varying Interpretations of Emoji", "author": ["Miller et al.2016] Hannah Miller", "Jacob ThebaultSpieker", "Shuo Chang", "Isaac Johnson", "Loren Terveen", "Brent Hecht"], "venue": "Proceeding of the International AAAI Conference", "citeRegEx": "Miller et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Sentiment of emojis", "author": ["Jasmina Smailovi\u0107", "Borut Sluban", "Igor Mozeti\u010d"], "venue": "PloS one,", "citeRegEx": "Novak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Novak et al\\.", "year": 2015}, {"title": "Modeling online reviews with multigrain topic models", "author": ["Titov", "McDonald2008] Ivan Titov", "Ryan McDonald"], "venue": "In Proceedings of the 17th international conference on World Wide Web,", "citeRegEx": "Titov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2008}, {"title": "Grammar as a foreign language", "author": ["Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Proceeding of the conference on Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Topic modeling: Beyond bag-of-words", "author": ["Hanna M Wallach"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Wallach.,? \\Q2006\\E", "shortCiteRegEx": "Wallach.", "year": 2006}, {"title": "Learning distributed word representations for bidirectional lstm recurrent neural network", "author": ["Wang et al.2016] Peilu Wang", "Yao Qian", "Frank K. Soong", "Lei He", "Hai Zhao"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": "Notable exceptions include studies focused on emojis\u2019 semantics and usage (Aoki and Uchida, 2011; Barbieri et al., 2016a; Barbieri et al., 2016b; Barbieri et al., 2016c; Eisner et al., 2016; Ljube\u0161ic and Fi\u0161er, 2016), or sentiment (Novak et al.", "startOffset": 74, "endOffset": 216}, {"referenceID": 21, "context": ", 2016; Ljube\u0161ic and Fi\u0161er, 2016), or sentiment (Novak et al., 2015).", "startOffset": 48, "endOffset": 68}, {"referenceID": 21, "context": "The ambiguity of emojis raises an interesting question in human-computer interaction: how can we teach an artificial agent to correctly interpret and recognise emojis\u2019 use in spontaneous conversation?2 The main motivation of our research is that an artificial intelligence system that is able to predict emojis could contribute to better natural language understanding (Novak et al., 2015) and thus to different natural language processing tasks such as generating emoji-enriched social media content, enhance emotion/sentiment analysis systems, and improve retrieval of social network material.", "startOffset": 369, "endOffset": 389}, {"referenceID": 8, "context": ", 2016, inter-alia), which also includes modeling of tweets (Dhingra et al., 2016), our emoji prediction model is based on bi-directional", "startOffset": 60, "endOffset": 82}, {"referenceID": 1, "context": "Character-based Representations: We compute character-based continuous-space vector embeddings (Ling et al., 2015b; Ballesteros et al., 2015) of the tokens in each tweet using, again, bidirectional LSTMs.", "startOffset": 95, "endOffset": 141}, {"referenceID": 8, "context": "This is similar to the treatment of word embeddings by Dyer et al. (2015). Character-based Representations: We compute character-based continuous-space vector embeddings (Ling et al.", "startOffset": 55, "endOffset": 74}, {"referenceID": 24, "context": "We applied a bag of words classifier as baseline, since it has been successfully employed in several classification tasks, like sentiment analysis and topic modeling (Wallach, 2006; Blei, 2012; Titov and McDonald, 2008; Maas et al., 2011; Davidov et al., 2010).", "startOffset": 166, "endOffset": 260}, {"referenceID": 5, "context": "We applied a bag of words classifier as baseline, since it has been successfully employed in several classification tasks, like sentiment analysis and topic modeling (Wallach, 2006; Blei, 2012; Titov and McDonald, 2008; Maas et al., 2011; Davidov et al., 2010).", "startOffset": 166, "endOffset": 260}, {"referenceID": 18, "context": "We applied a bag of words classifier as baseline, since it has been successfully employed in several classification tasks, like sentiment analysis and topic modeling (Wallach, 2006; Blei, 2012; Titov and McDonald, 2008; Maas et al., 2011; Davidov et al., 2010).", "startOffset": 166, "endOffset": 260}, {"referenceID": 7, "context": "We applied a bag of words classifier as baseline, since it has been successfully employed in several classification tasks, like sentiment analysis and topic modeling (Wallach, 2006; Blei, 2012; Titov and McDonald, 2008; Maas et al., 2011; Davidov et al., 2010).", "startOffset": 166, "endOffset": 260}, {"referenceID": 19, "context": "We train a Skip-gram model (Mikolov et al., 2013) learned from 65M Tweets (where testing instances have been removed) to learn Twitter semantic vectors.", "startOffset": 27, "endOffset": 49}, {"referenceID": 20, "context": "Given that Miller et al. (2016) pointed out that people tend to give multiple interpretations to emojis, we carried out an experiment in which we evaluated human and machine performances on the same task.", "startOffset": 11, "endOffset": 32}, {"referenceID": 20, "context": "The overall inter-annotator agreement was 73% (in line with previous findings (Miller et al., 2016)).", "startOffset": 78, "endOffset": 99}], "year": 2017, "abstractText": "Emojis are ideograms which are naturally combined with plain text to visually complement or condense the meaning of a message. Despite being widely used in social media, their underlying semantics have received little attention from a Natural Language Processing standpoint. In this paper, we investigate the relation between words and emojis, studying the novel task of predicting which emojis are evoked by text-based tweet messages. We train several models based on Long ShortTerm Memory networks (LSTMs) in this task. Our experimental results show that our neural model outperforms two baselines as well as humans solving the same task, suggesting that computational models are able to better capture the underlying semantics of emojis.", "creator": "LaTeX with hyperref package"}}}