{"id": "1702.07324", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2017", "title": "Inherent Biases of Recurrent Neural Networks for Phonological Assimilation and Dissimilation", "abstract": "A recurrent neural network model of phonological pattern learning is proposed. The model is a relatively simple neural network with one recurrent layer, and displays biases in learning that mimic observed biases in human learning. Single-feature patterns are learned faster than two-feature patterns, and vowel or consonant-only patterns are learned faster than patterns involving vowels and consonants, mimicking the results of laboratory learning experiments. In non-recurrent models, capturing these biases requires the use of alpha features or some other representation of repeated features, but with a recurrent neural network, these elaborations are not necessary.", "histories": [["v1", "Thu, 23 Feb 2017 18:19:35 GMT  (20kb)", "http://arxiv.org/abs/1702.07324v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amanda doucette"], "accepted": false, "id": "1702.07324"}, "pdf": {"name": "1702.07324.pdf", "metadata": {"source": "CRF", "title": "Inherent Biases of Recurrent Neural Networks for Phonological Assimilation and Dissimilation", "authors": ["Amanda Doucette"], "emails": ["amandakdoucette@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.07 324v 1 [cs.C L] February 23, 2017Phonological pattern learning is proposed; the model is a relatively simple neural network with a relapsing layer and exhibits distortions in learning that mimic observed distortions in human learning. Patterns with individual characteristics are learned faster than patterns with two characteristics, and patterns with vowels or consonants are learned faster than patterns with vowels and consonants, imitating the results of learning experiments in the laboratory. In non-recurring models, capturing these distortions requires the use of alpha characteristics or some other representation of repeated characteristics, but with a relapsing neural network, these elaborations are not necessary."}, {"heading": "1 Introduction", "text": "In fact, it is so that we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in which we are in a time, in which we are in which we are in a time, in which we are in which we are in a time, in which we are in which we are in a time, in which we are in which we are in which we are in a time, in which we are in which we are in which we are in a time, in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in a time, in which we are in which we are in which we are in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in which we are in a time, in which we are in which we are in which we are in which we are in a time, in which we are in a time, in which we are in which we are in a time are in which we are in a time, in which we are in"}, {"heading": "2 Model", "text": "The model used in these simulations is a simple recursive neural network model. The \"words\" that make up the patterns are the inputs in the first layer of the model. At each step, the four characteristics representing a phoneme are entered into the network. The second layer is a hidden recursive layer with ten neurons. The third layer is a log-softmax layer with two initial neurons. After the entire sequence has been entered into the model, the outputs at the last time step represent the log probabilities of the input belonging to each of the two classes that are referred to as IN in the pattern or OUT of the pattern. The probability that a pattern is IN or OUT is the probability that it is allowed in the language.The model was trained for negative log probabilities based on gradient drop, with a learning rate of 0.01. Weights were adjusted after each word in the training data, rather than in groups."}, {"heading": "3 Patterns", "text": "The patterns used in testing the model are four phonological characters - two consonant characters, and two vowel patterns based on real characters. Each character has a value of + 1 or -1. For consonants, the vowel characters have a value of 0, and for vowels, the consonant patterns have a value of 0. The consonant characters used in these patterns are invoicing (+ / \u2212 voi) and the vowel characters, the vowels [d, k, g] and the vowels [i, u, c) and the vowel characters, the vowels [i, c, c, c]. All the \"words\" in the patterns have the form C1V1C2V2, where C and V reach over the four consonants and vowels described by the four characters, so there are 256 in total. For each pattern, the words are divided into the two classes, IN and OUT, each with 12outsiders."}, {"heading": "4 Results: Training on full patterns", "text": "For each pattern, 3000 times were trained with random starting weights on all 256 examples; for each training run, the number of eras was recorded to learn the pattern according to the criterion in Section 2. Table 2 shows averages of the 3000 runs for each pattern; the model was able to learn the patterns in all but 25 training runs, which were stopped after 400 epochs and excluded from these results because the weights in these training runs were likely to be local minimums and the model was unable to learn the pattern. In a two-step t-test, there is no evidence that there is a significant difference between the number of epochs taken to learn patterns 3 and 4, the harmony and disharmony patterns (p > 0.01). Patterns 3 and 4, the one-character patterns, were learned significantly faster than patterns 1, 2 and 6, the two-character patterns, which comprise two segments < two segments each < 0.01)."}, {"heading": "5 Results: Training on subset of patterns", "text": "The model was trained on randomly selected subsets of training data for each pattern, because when learning phonological patterns, people cannot access every possible example of the pattern, and every possible example of something that does not match the pattern. Rather, people are only exposed to correct examples of their language as they learn. To test the model under these conditions, 32 examples of each pattern were randomly selected from the 128 available. If the model was trained only on positive examples, it would predict that everything will be included in the pattern, requiring some negative training examples. 32 of the remaining 224 were selected as negative training examples after the 32 positive examples already selected were removed. Therefore, the negative examples are not real negative examples, but only randomly selected examples from the complete datasets. Although this is an unusual method of training, a neural model was not properly trained on 3,000 training examples to test whether the model is capable of generalizing to invisible examples of the pattern."}, {"heading": "6 Discussion and conclusion", "text": "In fact, most people are able to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to another world, to go to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live in, to live, to live in, to live in, to live, to live in, to live in, to live, to live in, to live, to live in, to live in, to live, to live in, to live, to live in, to live, to live in, to live in, to live, to live in, to live, to live, to live in, to live in, to live, in, to live, to live, in, to live, in, to live, to live, to live, to live, in, to live, to live, in, to live, to live, in, to live, to live, to live, in, to live, in, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, in, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live,"}, {"heading": "Acknowledgments", "text": "Thanks to Joe Pater and Brendan O'Connor for their guidance and helpful discussion while I was working on this study, as well as to the four anonymous reviewers for their comments."}], "references": [{"title": "The role of triggertarget similarity in the vowel harmony process", "author": ["Mary Hare"], "venue": "In Annual Meeting of the Berkeley Linguistics Society,", "citeRegEx": "Hare.,? \\Q1990\\E", "shortCiteRegEx": "Hare.", "year": 1990}, {"title": "A maximum entropy model of phonotactics and phonotactic learning", "author": ["Hayes", "Wilson2008] Bruce Hayes", "Colin Wilson"], "venue": "Linguistic Inquiry,", "citeRegEx": "Hayes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hayes et al\\.", "year": 2008}, {"title": "Feature geometry and dependency: A review", "author": ["John J McCarthy"], "venue": "Phonetica,", "citeRegEx": "McCarthy.,? \\Q1988\\E", "shortCiteRegEx": "McCarthy.", "year": 1988}, {"title": "Joe Pater", "author": ["Elliott Moreton"], "venue": "and Katya Pertsova.", "citeRegEx": "Moreton et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Inter-and intra-dimensional dependencies in implicit phonotactic learning", "author": ["Elliott Moreton"], "venue": "Journal of Memory and Language,", "citeRegEx": "Moreton.,? \\Q2012\\E", "shortCiteRegEx": "Moreton.", "year": 2012}, {"title": "Eurie Shin", "author": ["Anne Pycha", "Pawel Nowak"], "venue": "and Ryan Shosted.", "citeRegEx": "Pycha et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Recurrent neuralnetwork learning of phonological regularities in turkish", "author": ["Jennifer M Rodd"], "venue": "In CoNLL,", "citeRegEx": "Rodd.,? \\Q1997\\E", "shortCiteRegEx": "Rodd.", "year": 1997}, {"title": "Pattern induction by infant language learners", "author": ["Saffran", "Thiessen2003] Jenny R Saffran", "Erik D Thiessen"], "venue": "Developmental Psychology,", "citeRegEx": "Saffran et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Saffran et al\\.", "year": 2003}, {"title": "Adaptation to novel accents: Feature-based learning of contextsensitive phonological regularities", "author": ["Skoruppa", "Peperkamp2011] Katrin Skoruppa", "Sharon Peperkamp"], "venue": null, "citeRegEx": "Skoruppa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Skoruppa et al\\.", "year": 2011}], "referenceMentions": [], "year": 2017, "abstractText": "A recurrent neural network model of phonological pattern learning is proposed. The model is a relatively simple neural network with one recurrent layer, and displays biases in learning that mimic observed biases in human learning. Single-feature patterns are learned faster than two-feature patterns, and vowel or consonant-only patterns are learned faster than patterns involving vowels and consonants, mimicking the results of laboratory learning experiments. In non-recurrent models, capturing these biases requires the use of alpha features or some other representation of repeated features, but with a recurrent neural network, these elaborations are not necessary.", "creator": "LaTeX with hyperref package"}}}