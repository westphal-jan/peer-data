{"id": "1702.07490", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "Online Meta-learning by Parallel Algorithm Competition", "abstract": "The efficiency of reinforcement learning algorithms depends critically on a few meta-parameters that modulates the learning updates and the trade-off between exploration and exploitation. The adaptation of the meta-parameters is an open question in reinforcement learning, which arguably has become more of an issue recently with the success of deep reinforcement learning in high-dimensional state spaces. The long learning times in domains such as Atari 2600 video games makes it not feasible to perform comprehensive searches of appropriate meta-parameter values. We propose the Online Meta-learning by Parallel Algorithm Competition (OMPAC) method. In the OMPAC method, several instances of a reinforcement learning algorithm are run in parallel with small differences in the initial values of the meta-parameters. After a fixed number of episodes, the instances are selected based on their performance in the task at hand. Before continuing the learning, Gaussian noise is added to the meta-parameters with a predefined probability. We validate the OMPAC method by improving the state-of-the-art results in stochastic SZ-Tetris and in standard Tetris with a smaller, 10$\\times$10, board, by 31% and 84%, respectively, and by improving the results for deep Sarsa($\\lambda$) agents in three Atari 2600 games by 62% or more. The experiments also show the ability of the OMPAC method to adapt the meta-parameters according to the learning progress in different tasks.", "histories": [["v1", "Fri, 24 Feb 2017 08:25:23 GMT  (77kb)", "http://arxiv.org/abs/1702.07490v1", "15 pages, 10 figures. arXiv admin note: text overlap witharXiv:1702.03118"]], "COMMENTS": "15 pages, 10 figures. arXiv admin note: text overlap witharXiv:1702.03118", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["stefan elfwing", "eiji uchibe", "kenji doya"], "accepted": false, "id": "1702.07490"}, "pdf": {"name": "1702.07490.pdf", "metadata": {"source": "CRF", "title": "Online Meta-learning by Parallel Algorithm Competition", "authors": ["Stefan Elfwing", "Eiji Uchibe", "Kenji Doya"], "emails": ["elfwing@atr.jp", "uchibe@atr.jp", "doya@oist.jp"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.07 490v 1 [cs.L G] 24 Fe"}, {"heading": "1 Introduction", "text": "The efficiency of enhanced learning (Sutton and Barto, 1998) algorithms depend crucially on a few meta-parameters that modulate learning progress and the trade-off between exploring for new knowledge and using existing knowledge. Ideally, these meta-parameters should not be fixed during learning, but should instead be adapted to current learning progress in the task. Adapting meta-parameters is an open question of enhanced learning, which has lately become more of a problem with the success of in-depth learning in tasks with high-dimensional government spaces, such as the ability of the DQN algorithm to achieve human learning performance in many Atari 2600 video games (Mnih et al, 2015). The complexity and long learning times in such tasks, where episode length often increases with improvements in performance, makes it impossible to perform comprehensive network searches with appropriate meta-parameter."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 TD(\u03bb) and Sarsa(\u03bb)", "text": "In this study we use two amplification learning algorithms: TD (\u03bb) (Sutton, 1988) and Sarsa (\u03bb) (Rummery and Niranjan, 1994; Sutton, 1996). TD (\u03bb) learns an estimate of the state value function, V \u03c0, and Sarsa (\u03bb) learns an estimate of the action value function, Q \u03c0, while the agent follows the policy \u03c0. If the approximate value functions, Vt \u2248 V \u03c0 and Qt \u2248 Q \u03c0, are calculated by the parameter Vt (st + 1) \u2212 Vt (st) (2) for the gradient drop learning update of the parameters, then the gradient update of the parameters is calculated by VT + 1 = \u03b8t + \u03b1\u03b4tet, (1) where the TD error, \u03b4t = rt + \u03b3t, Vt (st + 1) \u2212 Vt (st) for the state rate, Vt (st) for the determination time, VT (VT) for the determination time, VT) and VT (VT) for the determination time."}, {"heading": "2.2 Sigmoid-weighted Linear Units", "text": "We have recently (Elfwing et al., 2017) proposed the sigmoid-weighted linear unit (SiL) and its derived function (dSiL) as activation functions for approximating the neural network function in amplification learning. Activation ak of the SiL unit k for an input vector s is calculated by the sigmoid function (\u00b7) multiplied by its input, zk: ak (s) = zk\u03c3 (zk), (6) zk (s) = \u2211 iwiksi + bk, (7) \u03c3 (x) = 11 + e \u2212 x. (8) Here wik is the weight of the weight connection state si and the hidden unit k, bk is the bias weight for hidden unit k. Activation of the dSiL unit is calculated by: ak (s) = circle diameter (zk) = circle diameter (zk) (1 + zk parameter (zk)."}, {"heading": "2.3 Action selection", "text": "In all experiments, we use a Softmax action selection with a Boltzmann distribution. (12) For the model-based TD (\u03bb) algorithm, we select an action that leads to the next state s, where the probability is as\u03c0 (a | s) = exp (V (f (s, a) / \u03c4) \u2211 b exp (V (f (s, b) / \u03c4). (13) Here f (s, a) returns the next state s according to the state transition dynamics. (14) Here is the temperature that controls the goal conflict between exploration and exploitation."}, {"heading": "3 The OMPAC method", "text": "In this section we present the OMPAC (Online Meta-Learning by Parallel Algorithm Competition) method. Algorithm 1 shows the pseudo code for the OMPAC method with Sarsa (\u03bb) and the Softmax action selection. Multiple, N, instances of the algorithm are executed in parallel, with small differences in meta parameter values. After a specified number of episodes, the instances for continued learning are selected based on their performance in the task. The OMPAC algorithm 1 OMPAC with Sarsa (\u03bb) and Softmax action selection initialization matrix of the parameter vectors is selected."}, {"heading": "4 Experiments", "text": "In order to compare learning performance directly with and without the OMPAC method, we used the same experimental arrangements as in our previous study (Elfwing et al., 2017). In stochastic SZ-Tetris and standard Tetris with a smaller, 10 \u00d7 10, board size, we trained agents with flat approximators for neural network functions with hidden dSiL units using TD (\u03bb) and Softmax action selection (hereinafter: flat dSiL agents). In the Atari 2600 domain, we trained agents with deep Convolutionary Neural Network Function Approximators with SiL hidden units in the folding layers and dSiL hidden units in the fully connected layer using Sarsa (\u03bb) and Softmax action selection (hereinafter: deep SiL agents). In all OMPAC experiments, the number of 100 probabilities of the generation of learning parameters was set to 0.05, the number of learning parameters to 0.012 in the chain instances, and the number of metal instances to 0.05."}, {"heading": "4.1 Tetris", "text": "Due to the prohibitively long learning times (in the case of high performance), it then appears unfeasible to apply value-oriented enhancement of learning to standard Tetris with a board height of 20 points. The current state-of-the-art result for a single run of an algorithm, achieved by the CBMPI algorithm, which uses only the S-shaped and Z-shaped Tetris episodes, is an average score of 51 million solved lines. Instead, we consider stochastic SZ-Tetris, 1997; Szita and Szepesv\u00e1ri, 2010), which uses only the S-shaped and Z-shaped Tetris-shaped episodes, and standard Tetris with a smaller, 10 x 10, board.In both versions of Tetris, in each time step a randomly selected tetromino appears above the board. The agent selects a rotation and a horizontal position, and the etTetromino stops, and then the row is completed, and another mint appears."}, {"heading": "4.2 Atari 2600 games", "text": "We evaluated the OMPAC method in the Atari 2600 domain using the Arcade Learning Environment results (Bellemare et al., 2013). We followed the experimental setup in our earlier study (Elfwing et al., 2017). However, the raw 210 \u00d7 160 Atari 2600 RGB results were pre-processed by extracting the brightness scale, using the maximum pixel values across successive frames to prevent patching, and then downloading the Grayscale images to 105 \u00d7 80. The deep Convolutionary Neural Network used by the deep SiL agent consisted of two counterproductive layers of SiL units (16 filters of size 8 \u00d7 8 with a strip of 4 \u00d7 4 and 32 filters of size 4 \u00d7 4 with a strip of 2), each followed by a maximum consolidation layer (pooling window of size 3 \u00d7 3 with a strip of 2), a fully connected linear layer of 12 \u00d7 4 and a fully connected SiL."}, {"heading": "5 Analysis", "text": "In this section we examine the learning ability of the OMPAC / gamma method when we assume poor meta parameter settings. Experiments in the two Tetris and Atari 2600 domains have shown that OMPAC adjustment of meta parameters can significantly improve learning performance when appropriate meta parameter start values are used. However, if we encounter a new task, it would be worthwhile to use OMPAC either as a method for achieving high performance, even if the initial meta parameter values are not suitable for the task at hand, or as a method for determining suitable meta parameter values for future experiments. We have decided to limit our investigation to different settings of gamma and gamma effects. We have flat dSiL agents in SZ-Tetris with three settings of the initial value of the MPA parameters (0.8, 0.9, 0.99} and three settings of the initial gamma / gamma / gamma / gamma value of gamma and / gamma / gamma / and / / / gamma / gamma / gamma / / gamma / gamma / and / / / / / gamma / gamma parameter start value of the MPA parameters (0.8, 0.9, 0.99})."}, {"heading": "6 Conclusions", "text": "In this study, we proposed the OMPAC method for online adaptation of meta-parameters in amplification learning through competition between parallel algorithm instances. We validated the proposed method by significantly improving the state-of-the-art scoresin stochastic SZ-Tetris and 10 \u00d7 10 Tetris, and significantly improving the performance of deep Sarsa (\u03bb) agents in three Atari 2600 games. Experiments also demonstrated the ability of the OMPAC method to adapt meta-parameters to learning progress in various tasks."}, {"heading": "Acknowledgments", "text": "This work was supported by the project commissioned by the New Energy and Industrial Technology Development Organization (NEDO), JSPS KAKENHI grant 16K12504 and Okinawa Institute of Science and Technology Graduate University Research support to KD."}], "references": [{"title": "Reducing bias and inefficiency in the selection algorithm", "author": ["J.E. Baker"], "venue": "Proceedings of the Second Genetic Algorithms on Genetic Algorithms and Their Application, pages 14\u2013", "citeRegEx": "Baker,? 1987", "shortCiteRegEx": "Baker", "year": 1987}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279.", "citeRegEx": "Bellemare et al\\.,? 2013", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Temporal differences based policy iteration and applications in neuro-dynamic programming", "author": ["D.P. Bertsekas", "S. Ioffe"], "venue": "Technical Report LIDS-P-2349, MIT.", "citeRegEx": "Bertsekas and Ioffe,? 1996", "shortCiteRegEx": "Bertsekas and Ioffe", "year": 1996}, {"title": "How to lose at Tetris", "author": ["H. Burgiel"], "venue": "Mathematical Gazette, 81:194\u2013200.", "citeRegEx": "Burgiel,? 1997", "shortCiteRegEx": "Burgiel", "year": 1997}, {"title": "Temporal difference bayesian model averaging: A bayesian perspective on adapting lambda", "author": ["C. Downey", "S. Sanner"], "venue": "Proceedings of the International Conference on Machine Learning (ICML2010), pages 311\u2013318.", "citeRegEx": "Downey and Sanner,? 2010", "shortCiteRegEx": "Downey and Sanner", "year": 2010}, {"title": "Sigmoid-weighted linear units for neural network function approximation in reinforcement learning", "author": ["S. Elfwing", "E. Uchibe", "K. Doya"], "venue": "CoRR, abs/1702.03118.", "citeRegEx": "Elfwing et al\\.,? 2017", "shortCiteRegEx": "Elfwing et al\\.", "year": 2017}, {"title": "Co-evolution of shaping rewards and meta-parameters in reinforcement learning", "author": ["S. Elfwing", "E. Uchibe", "K. Doya", "H.I. Christensen"], "venue": "Adaptive Behavior, 16(6):400\u2013 412.", "citeRegEx": "Elfwing et al\\.,? 2008", "shortCiteRegEx": "Elfwing et al\\.", "year": 2008}, {"title": "Darwinian embodied evolution of the learning ability for survival", "author": ["S. Elfwing", "E. Uchibe", "K. Doya", "H.I. Christensen"], "venue": "Adaptive Behavior, 19(2):101\u2013120.", "citeRegEx": "Elfwing et al\\.,? 2011", "shortCiteRegEx": "Elfwing et al\\.", "year": 2011}, {"title": "Evolution of meta-parameters in reinforcement learning algorithm", "author": ["A. Eriksson", "G. Capi", "K. Doya"], "venue": "Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems (IROS2003), pages 412\u2013417.", "citeRegEx": "Eriksson et al\\.,? 2003", "shortCiteRegEx": "Eriksson et al\\.", "year": 2003}, {"title": "Neural network ensembles in reinforcement learning", "author": ["S. Fau\u00dfer", "F. Schwenker"], "venue": "Neural Processing Letters, pages 1\u201315.", "citeRegEx": "Fau\u00dfer and Schwenker,? 2013", "shortCiteRegEx": "Fau\u00dfer and Schwenker", "year": 2013}, {"title": "How to discount deep reinforcement learning: Towards new dynamic strategies", "author": ["V. Fran\u00e7ois-Lavet", "R. Fonteneau", "D. Ernst"], "venue": "CoRR, abs/1512.02011.", "citeRegEx": "Fran\u00e7ois.Lavet et al\\.,? 2015", "shortCiteRegEx": "Fran\u00e7ois.Lavet et al\\.", "year": 2015}, {"title": "Approximate dynamic programming finally performs well in the game of Tetris", "author": ["V. Gabillon", "M. Ghavamzadeh", "B. Scherrer"], "venue": "Proceedings of Advances in Neural Information Processing Systems (NIPS2013), pages 1754\u20131762.", "citeRegEx": "Gabillon et al\\.,? 2013", "shortCiteRegEx": "Gabillon et al\\.", "year": 2013}, {"title": "Control of exploitation\u00e2\u0102\u015eexploration meta-parameter in reinforcement learning", "author": ["S. Ishii", "W. Yoshida", "J. Yoshimoto"], "venue": "Neural Networks, 15(4-6):665\u2013687.", "citeRegEx": "Ishii et al\\.,? 2002", "shortCiteRegEx": "Ishii et al\\.", "year": 2002}, {"title": "A meta-learning method based on temporal difference error", "author": ["K. Kobayashi", "H. Mizoue", "T. Kuremoto", "M. Obayashi"], "venue": "Proceedings of International Conference on Neural Information Processing (ICONIP2009), pages 530\u2013537.", "citeRegEx": "Kobayashi et al\\.,? 2009", "shortCiteRegEx": "Kobayashi et al\\.", "year": 2009}, {"title": "Philosophie Zoologique", "author": ["J.B. Lamarck"], "venue": "Chez Dentu.", "citeRegEx": "Lamarck,? 1809", "shortCiteRegEx": "Lamarck", "year": 1809}, {"title": "Adaptive \u03bb least-squares temporal difference learning", "author": ["T.A. Mann", "H. Penedones", "T. Hester"], "venue": "CoRR, abs/1612.09465.", "citeRegEx": "Mann et al\\.,? 2016", "shortCiteRegEx": "Mann et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533.", "citeRegEx": "Mnih et al\\.,? 2015", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "A.D. Maria", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S. Petersen", "S. Legg", "V. Mnih", "K. Kavukcuoglu", "D. Silver"], "venue": "CoRR, abs/1507.04296.", "citeRegEx": "Nair et al\\.,? 2015", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "On-line Q-learning using connectionist systems", "author": ["G.A. Rummery", "M. Niranjan"], "venue": "Technical Report CUED/F-INFENG/TR 166, Cambridge University Engineering Department.", "citeRegEx": "Rummery and Niranjan,? 1994", "shortCiteRegEx": "Rummery and Niranjan", "year": 1994}, {"title": "Meta-learning in reinforcement learning", "author": ["N. Schweighofer", "K. Doya"], "venue": "Neural Networks, 16(1):5\u20139.", "citeRegEx": "Schweighofer and Doya,? 2003", "shortCiteRegEx": "Schweighofer and Doya", "year": 2003}, {"title": "Learning to predict by the method of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning, 3:9\u201344.", "citeRegEx": "Sutton,? 1988", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Adapting bias by gradient descent: an incremental version of the delta-bar-delta", "author": ["R.S. Sutton"], "venue": "Proceedings of the National Conference on Artificial Intelligence.", "citeRegEx": "Sutton,? 1992", "shortCiteRegEx": "Sutton", "year": 1992}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["R.S. Sutton"], "venue": "Proceedings of Advances in Neural Information Processing Systems (NIPS1996), pages 1038\u20131044. MIT Press.", "citeRegEx": "Sutton,? 1996", "shortCiteRegEx": "Sutton", "year": 1996}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A. Barto"], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "SZ-Tetris as a benchmark for studying key problems of reinforcement learning", "author": ["I. Szita", "C. Szepesv\u00e1ri"], "venue": "ICML 2010 workshop on machine learning and games.", "citeRegEx": "Szita and Szepesv\u00e1ri,? 2010", "shortCiteRegEx": "Szita and Szepesv\u00e1ri", "year": 2010}, {"title": "Differentiation of learning abilities \u2013 a case study on optimizing parameter values in qlearning by a genetic algorithm", "author": ["T. Unemi", "M. Nagaoyoshi", "N. Hirayama", "T. Nade", "K. Yano", "Y. Masujima"], "venue": "Proceedings of the International Workshop on the Synthesis and Simulation of Living Systems, pages 331\u2013336.", "citeRegEx": "Unemi et al\\.,? 1994", "shortCiteRegEx": "Unemi et al\\.", "year": 1994}, {"title": "Deep reinforcement learning with double q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": null, "citeRegEx": "Hasselt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 23, "context": "The efficiency of reinforcement learning (Sutton and Barto, 1998) algorithms depends critically on a few meta-parameters that modulates the learning updates and the trade-off between exploration for new knowledge and exploitation of existing knowledge.", "startOffset": 41, "endOffset": 65}, {"referenceID": 16, "context": "performance in many Atari 2600 video games (Mnih et al., 2015).", "startOffset": 43, "endOffset": 62}, {"referenceID": 14, "context": "The OMPAC method is similar to a Lamarckian (Lamarck, 1809) evolutionary process without the crossover operator, but with two main differences compared with standard applications of artificial evolution.", "startOffset": 44, "endOffset": 59}, {"referenceID": 25, "context": ", the learning started from scratch in each generation) to find appropriate fixed values of the meta-parameters (Unemi et al., 1994; Eriksson et al., 2003; Elfwing et al., 2008, 2011).", "startOffset": 112, "endOffset": 183}, {"referenceID": 8, "context": ", the learning started from scratch in each generation) to find appropriate fixed values of the meta-parameters (Unemi et al., 1994; Eriksson et al., 2003; Elfwing et al., 2008, 2011).", "startOffset": 112, "endOffset": 183}, {"referenceID": 21, "context": "Proposed approaches for adapting individual meta-parameters include the incremental delta bar delta method (Sutton, 1992) to tune \u03b1, a method based on variance of the action value function (Ishii et al.", "startOffset": 107, "endOffset": 121}, {"referenceID": 12, "context": "Proposed approaches for adapting individual meta-parameters include the incremental delta bar delta method (Sutton, 1992) to tune \u03b1, a method based on variance of the action value function (Ishii et al., 2002) to tune the inverse temperature \u03b2 in softmax action selection, and a Bayesian model averaging approach (Downey and Sanner, 2010) and the Adaptive \u03bb Least-Squares Temporal Difference Learning method (Mann et al.", "startOffset": 189, "endOffset": 209}, {"referenceID": 4, "context": ", 2002) to tune the inverse temperature \u03b2 in softmax action selection, and a Bayesian model averaging approach (Downey and Sanner, 2010) and the Adaptive \u03bb Least-Squares Temporal Difference Learning method (Mann et al.", "startOffset": 111, "endOffset": 136}, {"referenceID": 15, "context": ", 2002) to tune the inverse temperature \u03b2 in softmax action selection, and a Bayesian model averaging approach (Downey and Sanner, 2010) and the Adaptive \u03bb Least-Squares Temporal Difference Learning method (Mann et al., 2016) to tune \u03bb.", "startOffset": 206, "endOffset": 225}, {"referenceID": 5, "context": "To be able to directly compare the learning performance with and without the OMPAC method, we use the same experimental setups as in our earlier study (Elfwing et al., 2017), where we proposed the sigmoid-weighted linear (SiL) unit and its derivative function (dSiL) as activation functions for neural network function approximation in reinforcement learning.", "startOffset": 151, "endOffset": 173}, {"referenceID": 4, "context": ", 2003; Elfwing et al., 2008, 2011). Schweighofer and Doya (2003) proposed a meta-learning method based on the mid-term and the long-term running averages of the reward, and Kobayashi et al.", "startOffset": 8, "endOffset": 66}, {"referenceID": 4, "context": ", 2003; Elfwing et al., 2008, 2011). Schweighofer and Doya (2003) proposed a meta-learning method based on the mid-term and the long-term running averages of the reward, and Kobayashi et al. (2009) proposed a metalearning method based on the absolute values of the TD-errors.", "startOffset": 8, "endOffset": 198}, {"referenceID": 4, "context": ", 2002) to tune the inverse temperature \u03b2 in softmax action selection, and a Bayesian model averaging approach (Downey and Sanner, 2010) and the Adaptive \u03bb Least-Squares Temporal Difference Learning method (Mann et al., 2016) to tune \u03bb. Fran\u00e7ois-Lavet et al. (2015) demonstrated that the performance of DQN could be improved in some Atari 2600 games by a rather ad-hoc tuning scheme that increase \u03b3 (\u03b3 \u2190 min(0.", "startOffset": 112, "endOffset": 266}, {"referenceID": 20, "context": "1 TD(\u03bb) and Sarsa(\u03bb) In this study, we use two reinforcement learning algorithms: TD(\u03bb) (Sutton, 1988) and Sarsa(\u03bb) (Rummery and Niranjan, 1994; Sutton, 1996).", "startOffset": 88, "endOffset": 102}, {"referenceID": 18, "context": "1 TD(\u03bb) and Sarsa(\u03bb) In this study, we use two reinforcement learning algorithms: TD(\u03bb) (Sutton, 1988) and Sarsa(\u03bb) (Rummery and Niranjan, 1994; Sutton, 1996).", "startOffset": 116, "endOffset": 158}, {"referenceID": 22, "context": "1 TD(\u03bb) and Sarsa(\u03bb) In this study, we use two reinforcement learning algorithms: TD(\u03bb) (Sutton, 1988) and Sarsa(\u03bb) (Rummery and Niranjan, 1994; Sutton, 1996).", "startOffset": 116, "endOffset": 158}, {"referenceID": 5, "context": "We recently proposed (Elfwing et al., 2017) the sigmoid-weighted linear (SiL) unit and its derivative function (dSiL) as activation functions for neural network function approximation in reinforcement learning.", "startOffset": 21, "endOffset": 43}, {"referenceID": 0, "context": "We use stochastic universal sampling (SUS; Baker, 1987) combined with elitism as the Selection() method.", "startOffset": 37, "endOffset": 55}, {"referenceID": 5, "context": "To be able to directly compare the learning performance with and without the OMPAC method, we used the same experimental setups as in our earlier study (Elfwing et al., 2017).", "startOffset": 152, "endOffset": 174}, {"referenceID": 11, "context": "The current state-of-the-art result for a single run of an algorithm, achieved by the CBMPI algorithm (Gabillon et al., 2013), is a mean score of 51 million cleared lines.", "startOffset": 102, "endOffset": 125}, {"referenceID": 3, "context": "We instead consider stochastic SZ-Tetris (Burgiel, 1997; Szita and Szepesv\u00e1ri, 2010), which only uses the S-shaped and the Z-shaped tetrominos, and standard Tetris with a smaller, 10\u00d710, board.", "startOffset": 41, "endOffset": 84}, {"referenceID": 24, "context": "We instead consider stochastic SZ-Tetris (Burgiel, 1997; Szita and Szepesv\u00e1ri, 2010), which only uses the S-shaped and the Z-shaped tetrominos, and standard Tetris with a smaller, 10\u00d710, board.", "startOffset": 41, "endOffset": 84}, {"referenceID": 5, "context": "We recently achieved the current state-of-the-art results (Elfwing et al., 2017) using shallow dSiL agents.", "startOffset": 58, "endOffset": 80}, {"referenceID": 11, "context": "In 10\u00d710 Tetris, a shallow dSiL agent with 250 hidden nodes achieved a final (over 10,000 episodes) mean score of 4,900 points when averaged over 5 separate runs of and 5,300 points for the best run, which improved the average score of 4,200 points and the best score of 5,000 points achieved by the CBMPI algorithm (Gabillon et al., 2013).", "startOffset": 316, "endOffset": 339}, {"referenceID": 2, "context": "The features were similar to the original 21 features proposed by Bertsekas and Ioffe (1996), except for not including the maximum column height and using the differences in column heights instead of the absolute differences.", "startOffset": 66, "endOffset": 93}, {"referenceID": 2, "context": "The features were similar to the original 21 features proposed by Bertsekas and Ioffe (1996), except for not including the maximum column height and using the differences in column heights instead of the absolute differences. The binary state vectors were of length 460 in SZ-Tetris and 260 in 10\u00d710 Tetris. We used the following reward function proposed by Fau\u00dfer and Schwenker (2013):", "startOffset": 66, "endOffset": 386}, {"referenceID": 5, "context": "We used the meta-parameters in Elfwing et al. (2017) as starting values to initialize the meta-parameters according to Equations 15 and 16: \u03b1: 0.", "startOffset": 31, "endOffset": 53}, {"referenceID": 1, "context": "2 Atari 2600 games We evaluated the OMPAC method in the Atari 2600 domain using the Arcade Learning Environment (Bellemare et al., 2013).", "startOffset": 112, "endOffset": 136}, {"referenceID": 5, "context": "We followed the experimental setup in our earlier study (Elfwing et al., 2017).", "startOffset": 56, "endOffset": 78}, {"referenceID": 16, "context": "As in the DQN experiments (Mnih et al., 2015), we clipped the rewards to be between \u22121 and +1, but we did not clip the values of the TD-errors.", "startOffset": 26, "endOffset": 45}, {"referenceID": 17, "context": "(2017) outperformed DQN, the Gorila implementation of DQN (Nair et al., 2015) and double DQN (van Hasselt et al.", "startOffset": 58, "endOffset": 77}, {"referenceID": 1, "context": "2 Atari 2600 games We evaluated the OMPAC method in the Atari 2600 domain using the Arcade Learning Environment (Bellemare et al., 2013). We followed the experimental setup in our earlier study (Elfwing et al., 2017). The raw 210\u00d7160 Atari 2600 RGB frames were pre-processed by extracting the luminance channel, taking the maximum pixel values over consecutive frames to prevent flickering, and then downsampling the grayscale images to 105\u00d780. The deep convolutional neural network used by the deep SiL agent consisted of two convolutional layers with SiL units (16 filters of size 8\u00d78 with a stride of 4 and 32 filters of size 4\u00d74 with a stride of 2), each followed by a max-pooling layer (pooling windows of size 3\u00d73 with a stride of 2), a fully-connected hidden layer with 512 dSiL units, and a fully-connected linear output layer with 4 to 18 output (or action-value) units, depending on the number of valid actions in the considered game. We used frame skipping where actions were selected every fourth frame and repeated for the next four frames. The input to the network was a 105\u00d780\u00d72 image consisting of the current and the fourth previous pre-processed frame. As in the DQN experiments (Mnih et al., 2015), we clipped the rewards to be between \u22121 and +1, but we did not clip the values of the TD-errors. An episode started with up to 30 \u2019do nothing\u2019 actions (no-op condition) and it was played until the end of the game or for a maximum of 18,000 frames (i.e., 5 minutes). We used the meta-parameters in Elfwing et al. (2017) as starting values to initialize the meta-parameters according to Equations 15 and 16: \u03b1: 0.", "startOffset": 113, "endOffset": 1537}, {"referenceID": 1, "context": "2 Atari 2600 games We evaluated the OMPAC method in the Atari 2600 domain using the Arcade Learning Environment (Bellemare et al., 2013). We followed the experimental setup in our earlier study (Elfwing et al., 2017). The raw 210\u00d7160 Atari 2600 RGB frames were pre-processed by extracting the luminance channel, taking the maximum pixel values over consecutive frames to prevent flickering, and then downsampling the grayscale images to 105\u00d780. The deep convolutional neural network used by the deep SiL agent consisted of two convolutional layers with SiL units (16 filters of size 8\u00d78 with a stride of 4 and 32 filters of size 4\u00d74 with a stride of 2), each followed by a max-pooling layer (pooling windows of size 3\u00d73 with a stride of 2), a fully-connected hidden layer with 512 dSiL units, and a fully-connected linear output layer with 4 to 18 output (or action-value) units, depending on the number of valid actions in the considered game. We used frame skipping where actions were selected every fourth frame and repeated for the next four frames. The input to the network was a 105\u00d780\u00d72 image consisting of the current and the fourth previous pre-processed frame. As in the DQN experiments (Mnih et al., 2015), we clipped the rewards to be between \u22121 and +1, but we did not clip the values of the TD-errors. An episode started with up to 30 \u2019do nothing\u2019 actions (no-op condition) and it was played until the end of the game or for a maximum of 18,000 frames (i.e., 5 minutes). We used the meta-parameters in Elfwing et al. (2017) as starting values to initialize the meta-parameters according to Equations 15 and 16: \u03b1: 0.001, \u03b3: 0.99, \u03bb: 0.8, \u03c40: 0.5, and \u03c4k: 0.0005. The experiments ran for 2,000 generations (i.e., 200,000 episodes of learning in total). The score used for selection was the total raw score received by an algorithm instance in one generation. The deep SiL agent in Elfwing et al. (2017) outperformed DQN, the Gorila implementation of DQN (Nair et al.", "startOffset": 113, "endOffset": 1915}], "year": 2017, "abstractText": "The efficiency of reinforcement learning algorithms depends critically on a few metaparameters that modulates the learning updates and the trade-off between exploration and exploitation. The adaptation of the meta-parameters is an open question in reinforcement learning, which arguably has become more of an issue recently with the success of deep reinforcement learning in high-dimensional state spaces. The long learning times in domains such as Atari 2600 video games makes it not feasible to perform comprehensive searches of appropriate meta-parameter values. We propose the Online Meta-learning by Parallel Algorithm Competition (OMPAC) method. In the OMPAC method, several instances of a reinforcement learning algorithm are run in parallel with small differences in the initial values of the meta-parameters. After a fixed number of episodes, the instances are selected based on their performance in the task at hand. Before continuing the learning, Gaussian noise is added to the meta-parameters with a predefined probability. We validate the OMPAC method by improving the state-of-theart results in stochastic SZ-Tetris and in standard Tetris with a smaller, 10\u00d710, board, by 31% and 84%, respectively, and by improving the results for deep Sarsa(\u03bb) agents in three Atari 2600 games by 62% or more. The experiments also show the ability of the OMPAC method to adapt the meta-parameters according to the learning progress in different tasks.", "creator": "LaTeX with hyperref package"}}}