{"id": "1702.07495", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "Dirichlet-vMF Mixture Model", "abstract": "This document is about the multi-document Von-Mises-Fisher mixture model with a Dirichlet prior, referred to as VMFMix. VMFMix is analogous to Latent Dirichlet Allocation (LDA) in that they can capture the co-occurrence patterns acorss multiple documents. The difference is that in VMFMix, the topic-word distribution is defined on a continuous n-dimensional hypersphere. Hence VMFMix is used to derive topic embeddings, i.e., representative vectors, from multiple sets of embedding vectors. An efficient Variational Expectation-Maximization inference algorithm is derived. The performance of VMFMix on two document classification tasks is reported, with some preliminary analysis.", "histories": [["v1", "Fri, 24 Feb 2017 08:35:10 GMT  (4kb)", "http://arxiv.org/abs/1702.07495v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shaohua li"], "accepted": false, "id": "1702.07495"}, "pdf": {"name": "1702.07495.pdf", "metadata": {"source": "CRF", "title": "Dirichlet-vMF Mixture Model", "authors": ["Shaohua Li"], "emails": ["shaohua@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.07 495v 1 [cs.C L] 2This document deals with Von-Mises-Fisher's multi-document mixing model with an earlier dirichlet called VMFMix. VMFMix corresponds to the latent dirichlet allocation (LDA) in that it can capture the coexistence patterns across multiple documents. The difference is that in VMFMix the theme-word distribution is defined on a continuous n-dimensional hypersphere. Therefore, VMFMix is used to derive topic embedding, i.e. representative vectors, from multiple sets of embedding vectors. An efficient Variational Expectation-Maximization inference algorithm is derived. The performance of VMFMix in two document classification tasks is reported, with some preliminary analyses. We present a simplification of the Bayesian MF mixing model proposed in [2]."}, {"heading": "1 Model Specification", "text": "The generative process proceeds as follows: 1. \u03b8i \u0445 Dir (\u03b1); 2. zij \u0445 Cat (\u03b8i); 3. xij \u0445 vMF (\u00b5zij, \u03bazij). This is a hyperparameter, {\u00b5k, \u03bak} are parameters of mixture components to be learned. 1This model appears in [4] under the name \"Mix vMF Theme Model.\" But [4] offers only a sample-based inference scheme, which is usually less accurate than the EM algorithm presented in this document."}, {"heading": "2 Model Likelihood and Inference", "text": "The complete data probability of a dataset is: p (X, Z, p), p (X, Z, p), p (X, Z, p). (1) The incomplete data probability of a dataset is derived from the integration of the latent variables Z, p (X, p), p (X, p), p (Z), p (Z), p (Z), p (2), p), p (2), p), p (Z), p (2), p), p (Z), p (Z), p), p (Z), p), p), p), p), p), p (Z), p (Z), p (Z), p (Z), p)."}, {"heading": "3 Evaluation", "text": "The performance of this model was evaluated on the basis of two text classification tasks relating to 20 newsgroups (20News) and Reuters, respectively, and the experimental structure of the methods compared was identical to that in [3]. Similar to TopicVec, VMFMix learns an individual set of K-topic embeddings from each category of documents, and all these sentences are combined to form a larger set of topic embeddings for the entire corpus.This set of topic embeddings is used to derive the topic proportions of each document taken as characteristics for the SVM classifier. TheK for 20News and Reuters are selected as 15 and 12, respectively, identical to TopicVec.The macro-averaged precision, callback and F1 values of all methods are shown in Table 1.We can see from Table 1 that VMFMix performs better than Doc2Vec, LDA and LFTM."}], "references": [{"title": "Clustering on the unit hypersphere using von mises-fisher distributions", "author": ["Arindam Banerjee", "Inderjit S Dhillon", "Joydeep Ghosh", "Suvrit Sra"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Von mises-fisher clustering models", "author": ["Siddharth Gopal", "Yiming Yang"], "venue": "In ICML, pages 154\u2013162,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Generative topic embedding: a continuous representation of documents. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers, 2016. Each set of word embeddings can be viewed as a finite and discrete sample from a continuous embedding space", "author": ["Shaohua Li", "Tat-Seng Chua", "Jun Zhu", "ChunyanMiao"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Integrating topic modeling with word embeddings by mixtures of vmfs", "author": ["Ximing Li", "Jinjin Chi", "Changchun Li", "Jihong OuYang", "Bo Fu"], "venue": "In COLING,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "We present a simplification of the Bayesian vMF mixture model proposed in [2].", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "This model reappears in [4] under the name \u201cmix-vMF topic model\u201d.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "But [4] only offers a samplingbased inference scheme, which is usually less accurate than the EM algorithm presented in this document.", "startOffset": 4, "endOffset": 7}], "year": 2017, "abstractText": "This document is about the multi-document Von-Mises-Fisher mixture model with a Dirichlet prior, referred to as VMFMix. VMFMix is analogous to Latent Dirichlet Allocation (LDA) in that they can capture the co-occurrence patterns acorss multiple documents. The difference is that in VMFMix, the topic-word distribution is defined on a continuous n-dimensional hypersphere. Hence VMFMix is used to derive topic embeddings, i.e., representative vectors, from multiple sets of embedding vectors. An efficient Variational Expectation-Maximization inference algorithm is derived. The performance of VMFMix on two document classification tasks is reported, with some preliminary analysis.", "creator": "LaTeX with hyperref package"}}}