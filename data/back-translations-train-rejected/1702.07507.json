{"id": "1702.07507", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "Use Generalized Representations, But Do Not Forget Surface Features", "abstract": "Only a year ago, all state-of-the-art coreference resolvers were using an extensive amount of surface features. Recently, there was a paradigm shift towards using word embeddings and deep neural networks, where the use of surface features is very limited. In this paper, we show that a simple SVM model with surface features outperforms more complex neural models for detecting anaphoric mentions. Our analysis suggests that using generalized representations and surface features have different strength that should be both taken into account for improving coreference resolution.", "histories": [["v1", "Fri, 24 Feb 2017 09:26:10 GMT  (25kb)", "http://arxiv.org/abs/1702.07507v1", "CORBON workshop@EACL 2017"]], "COMMENTS": "CORBON workshop@EACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nafise sadat moosavi", "michael strube"], "accepted": false, "id": "1702.07507"}, "pdf": {"name": "1702.07507.pdf", "metadata": {"source": "CRF", "title": "Use Generalized Representations, But Do Not Forget Surface Features", "authors": ["Nafise Sadat Moosavi"], "emails": ["nafise.moosavi@h-its.org", "michael.strube@h-its.org"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.07 507v 1 [cs.C L] 24 Feb 2017erence resolvers were using an extensive amount of surface features. Recently, there has been a paradigm shift towards the use of word embedings and deep neural networks, where the use of surface features is very limited. In this paper, we show that a simple SVM model with surface features outperforms more complex neural models for detecting anaphorical mentions. Our analysis suggests that the use of generalized representations and surface features have different strengths, both of which should be taken into account to improve correlation resolution."}, {"heading": "1 Introduction", "text": "Nuclear resolution is the task of finding different mentions that relate to the same entity in a particular text. However, detecting anaphoricity is an important step for nuclear resolution. An anaphoricity recognition module distinguishes mentions that coincide with one of the previous mentions. If a system recognizes mentions as non-anaphorical, it does not need to create co-operative linkages for the pairs in which m is the anaphoric.Current state-of-the-art nuclear resolution systems (Wiseman et al., 2016; Clark and Manning, 2016a; Clark and Manning, 2016b), as well as their anaphoricity recognition modules, use deep neural networks, word embeddings, and a small set of features that describe surface properties of mentions. While it is shown that this small set of features has a significant impact on overall performance (Clark and Manning, 2016a), their use is very limited compared to the other surface properties described by the systems that describe."}, {"heading": "2 Discriminating Mentions for Coreference Resolution", "text": "Recognition of different categories of mentions can be advantageous for co-reference resolution. The most common categories in the literature are: (1) non-reference, (2) discursive and (3) Korean mentions. Other categories of mentions can also be distinguished, such as mentions that are probably not precursors or new mentions to the discourse (Uryupina, 2009), but they are not common in comparison to the above categories."}, {"heading": "2.1 Non-Referential Mentions", "text": "The approaches proposed by Evans (2001), Mu \u00bc ller (2006), Bergsma et al. (2008), Bergsma and Yarowsky (2011) are examples of recognizing non-referential cases of the pronoun it. Byron and Gegg-Harrison (2004) present a more general approach to recognizing non-referential noun phrases."}, {"heading": "2.2 Discourse-Old Mentions", "text": "Any mention can be evaluated from the standpoint of the discourse model (Prince, 1992). According to the discourse model, a mention can be new, old or inderivable. Mentions that introduce a new entity into the discourse are discourse-new mentions. A discourse-new mention can be a singleton or it can be the first mention of a coreference chain. For example, the first \"Plato\" in Example 2.1 refers to a discourse-new mention. Example 2.1. Plato was a philosopher in classical Greece. This philosopher is the founder of the Academy in Athens. Plato died at the age of 81. A discourse-old mention refers to an entity that is already evoked in the discourse. Except for the first mention of coreference chains, other coreference chains are old mentions. For example, \"this philosopher\" and the second \"Plato\" in Example 2.1 are old mentions."}, {"heading": "2.3 Coreferent Mentions", "text": "The suggested approaches of Recasens et al. (2013), Marneffe et al. (2015) and Moosavi and Strube (2016) differentiate mentions as coreferent versus non-coreferent. Coreferent mentions are mentions that appear in a coreferent chain. A non-coreferent mention can therefore be a non-referential noun proposition or a referential noun proposition whose entity is mentioned only once (i.e. singleton)."}, {"heading": "3 Anaphoricity Detection Models", "text": "All state-of-the-art coreference detectors use anaphorizontal detection. In this paper, we compare three different anaphorizontal detection approaches: two approaches that use neural networks and word embedding, and one that uses an SVM model and surface features. Clark and Manning (2016a) introduce the first neural model. Since Clark and Manning (2016a) train their anaphorizontal model together with the coreference model, we refer to this model as the common model. We introduce a new anaphorizontal detection model as the second neural model that uses a Long-Short Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997), and the third approach is adapted to our state-of-the-art coreference detection (Moosavi and Strube, 2016)."}, {"heading": "3.1 Joint Model", "text": "As one of the neural models for detecting anaphoricity, we consider the anaphoricity module of Deep Coref1, the state-of-the-art coreference resolution system introduced by Clark and Manning (2016a). This model comprises three layers encoding different types of information about a mention. The first layer encodes the word embedding of the head, first, last, two preceding / following words and the syntactic parent of the mention. The second layer encodes the averaged word embedding of the five preceding / following words, all words of mention, sentences and document words. The third layer encodes the following features of a mention: type, length, position and whether it is embedded in another mention. The results of this trie1Available at https: / / github.com / clarkkev / Deep Coreflayer are combined to form a vector and then trained through a network with two hidden layers of this horizontal model."}, {"heading": "3.2 LSTMModel", "text": "In this section, we propose a new neural model for recognizing anaphoricity. Apart from the properties of the mention itself, we consider a limited number of surrounding words. First, we generalize the context of a mention by removing the mention from the context and replacing it with a special placeholder. However, in our experiments, we look at the 10 preceding and subsequent words of a mention. We associate the mention marks and header marks with the generalized word sequence. We separate the head and mention tokens in the concatenated sequence using two different placeholders. The word embedding of the above sequence is encoded using a bi-directional LSTM. LSTMs show convincing results in generating meaningful representations for various NLP tasks (e.g. Sutskever et al. (2014) and Vinyals et al. (2014))). We also include a number of surface features that (1), the text (incorrectly, (2) (incorrectly) alominate (they) with another."}, {"heading": "3.2.1 Implementation Details", "text": "We minimize cross entropy loss through gradient-based optimization and the Adam Update Rule (Kingma and Ba, 2014). We use size 50 minibatches. A dropout (Hinton et al., 2012) at a rate of 0.3 is applied to the output of LSTM. We initialize the embedding with the 300-dimensional embedding of gloves (Pennington et al., 2014). The size of the hidden layer of LSTM is set to 128. The model is trained in just one epoch."}, {"heading": "3.3 SVMModel", "text": "Our SVM model, introduced in Moosavi and Strube (2016), achieves state-of-the-art results in detecting correct mentions. This model uses the following features: Lemmata and POS tags of all words in a mention, Lemmata and POS tags of the two previous / following words, Mention string, Mention length, Mention type (correct, nominal, pronoun, list), String match in the text and Header match in the text. We use a similar SVM model for detecting anaphorizontal. In addition to the features we have used for mentioning correct mentions, we also add the following features for detecting anaphorizontal: String match in the previous context, Header match in the previous context, Mention words are included in another mention, Mention words are included in a previous mention, 2016 Mention includes Moosavi, 2007 Mention contains other words anchored in the previous context, contains other words such as 2009 Mention."}, {"heading": "4 Performance Evaluation", "text": "We evaluate the anaphorizontal models on the CoNLL 2012 dataset. It is worth noting that all the anaphorizontal detectors examined in this section use the same mention detection module and the results are reported on the basis of system-relevant mentions. The performance of the mention detection module is critical for detecting the anaphorizontal characteristics used in this section. Therefore, it is important that the comparative anaphorizontal detectors use the same mention detector. The LSTM model described in Section 3.2 is referred to as LSTM in Table 1. To investigate the effect of the surface characteristics used, we also report on the results of the LSTM model without using these characteristics (LSTM). The following observations can be taken from Table 1: Results of the CoNLL 2012 test set. The results of Table 1: (1) of our LSTM model do not exceed the articulated model when using fewer characteristics at the same time, and in particular the STM model formation, the STL2 results are significantly lower."}, {"heading": "4.1 Generalization Evaluation", "text": "To investigate the generalization of new domains, we evaluate the LSTM and SVM models based on the WikiCoref dataset (Ghaddar and Langlais, 2016).The WikiCoref dataset is commented according to the same annotation policy as the CoNLL dataset. Therefore, it is a suitable dataset for conducting external evaluations when CoNLL is used for education.For the experiments in Table 2, all models are trained on the CoNLL 2012 training dataset and tested on the WikiCoref datasets.The dictionary used for the LSTM model is based on the CoNLL 2012 training dataset. All words not included in this dictionary are treated like vocabulary words, with randomly initialized word embeddings. We continue to improve the performance of LSTM on WikiCoref by adding the words from the WikiCoref dataset to the dictionary."}, {"heading": "5 Analysis Based on Mention Types", "text": "We analyze the results of the LSTM and SVM models on the CoNLL 2012 test set to see how well they perform among different types of men. As shown in Table 3, there is not much difference between the performance of LSTM and SVM in recognizing anaphorical pronouns. SVM recognizes anaphorical proper names better, while LSTM recognizes anaphorical proper names about 24 percent better. To see if the same pattern applies to coreference resolution, we compare the memory and precision errors of the best Korean system that uses only surface features, i.e. the corort (Martschat and Strube, 2015) with singleton features (Moosavi and Strube, 2016) and the status errors of the coreference system that uses only surface features, i.e. the correction (Martschat and Strube, 2015) with singleton features."}, {"heading": "6 Discussion", "text": "Our analysis shows that surface features as they were known are important. Based on our results, the effects of the inclusion of surface properties and generalized representations differ in different types of mentions. These results suggest that, apart from a single model, we should consider different models or at least different characteristics for processing different types of mentions, rather than placing the entire burden on a single model to learn the differences. Lassalle and Denis (2013) and Denis and Valdridge (2008) are examples of models in which different models were used for different types of mentions. In addition, our analysis shows the importance of surface features for proper names. Word embeddings are very useful for grasping semantic correlations. A correlation resolver using word embeddings has a major advantage in better resolution of common nouns and pronouns. However, the use of surface features in the current state of the art is very useful when determining the properties of correlation sources that are very limited."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Kevin Clark for his help with the Deep Coref software and MarkChristoph Mueller for his helpful comments. We would also like to thank the four anonymous reviewers for their detailed comments on an earlier draft of the paper. This work was funded by the Klaus Tschira Foundation, Heidelberg. The first author was supported by a doctoral scholarship from the Heidelberg Institute for Theoretical Studies."}], "references": [{"title": "NADA: A robust system for non-referential pronoun detection", "author": ["Bergsma", "Yarowsky2011] Shane Bergsma", "David Yarowsky"], "venue": "In Anaphora Processing and Applications. Proceedings of the 8th Discourse Anaphora and Anaphor Resolution", "citeRegEx": "Bergsma et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergsma et al\\.", "year": 2011}, {"title": "Distributional identification of non-referential pronouns", "author": ["Dekang Lin", "Randy Goebel"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technolo-", "citeRegEx": "Bergsma et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bergsma et al\\.", "year": 2008}, {"title": "Eliminating non-referring noun phrases from coreference resolution", "author": ["Byron", "Gegg-Harrison2004] Donna Byron", "Whitney Gegg-Harrison"], "venue": "In Proceedings the Discourse Anaphora and Reference Resolution Conference,", "citeRegEx": "Byron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Byron et al\\.", "year": 2004}, {"title": "Improving coreference resolution by learning entity-level distributed representations", "author": ["Clark", "Manning2016a] Kevin Clark", "Christopher D. Manning"], "venue": "In Proceedings of the 54th Annual Meeting of the Association", "citeRegEx": "Clark et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for mention-ranking coreference models", "author": ["Clark", "Manning2016b] Kevin Clark", "Christopher D. Manning"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Clark et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2016}, {"title": "Specialized models and ranking for coreference resolution", "author": ["Denis", "Baldridge2008] Pascal Denis", "Jason Baldridge"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Denis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Denis et al\\.", "year": 2008}, {"title": "Applying machine learning toward an automatic classification of it", "author": ["Richard Evans"], "venue": "Literary and Linguistic Computing,", "citeRegEx": "Evans.,? \\Q2001\\E", "shortCiteRegEx": "Evans.", "year": 2001}, {"title": "WikiCoref: An English coreference-annotated corpus of Wikipedia articles", "author": ["Ghaddar", "Langlais2016] Abbas Ghaddar", "Philippe Langlais"], "venue": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation", "citeRegEx": "Ghaddar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ghaddar et al\\.", "year": 2016}, {"title": "SVM model tampering and anchored learning: a case study in Hebrew NP chunking", "author": ["Goldberg", "Elhadad2007] Yoav Goldberg", "Michael Elhadad"], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Goldberg et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2007}, {"title": "On the role of lexical features in sequence labeling", "author": ["Goldberg", "Elhadad2009] Yoav Goldberg", "Michael Elhadad"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, Singapore,", "citeRegEx": "Goldberg et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2009}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Improving pairwise coreference models through feature space hierarchy learning", "author": ["Lassalle", "Denis2013] Emmanuel Lassalle", "Pascal Denis"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Lassalle et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lassalle et al\\.", "year": 2013}, {"title": "Joint anaphoricity detection and coreference resolution with constrained latent structures", "author": ["Lassalle", "Denis2015] Emmanuel Lassalle", "Pascal Denis"], "venue": "In Proceedings of the 29th Conference on the Advancement of Artificial Intelligence,", "citeRegEx": "Lassalle et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lassalle et al\\.", "year": 2015}, {"title": "Modeling the lifespan of discourse entities with application to coreference resolution", "author": ["Marta Recasens", "Christopher Potts"], "venue": "Journal of Artificial Intelligent Research,", "citeRegEx": "Marneffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2015}, {"title": "Recall error analysis for coreference resolution", "author": ["Martschat", "Strube2014] Sebastian Martschat", "Michael Strube"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Martschat et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Martschat et al\\.", "year": 2014}, {"title": "Latent structures for coreference resolution. Transactions of the Association for Computational Linguistics, 3:405\u2013418", "author": ["Martschat", "Strube2015] Sebastian Martschat", "Michael Strube"], "venue": null, "citeRegEx": "Martschat et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Martschat et al\\.", "year": 2015}, {"title": "Search space pruning: A simple solution for better coreference resolvers", "author": ["Moosavi", "Strube2016] Nafise Sadat Moosavi", "Michael Strube"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Moosavi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moosavi et al\\.", "year": 2016}, {"title": "Automatic detection of nonreferential it in spoken multi-party dialog", "author": ["Christoph M\u00fcller"], "venue": "In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy,", "citeRegEx": "M\u00fcller.,? \\Q2006\\E", "shortCiteRegEx": "M\u00fcller.", "year": 2006}, {"title": "Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution", "author": ["Ng", "Cardie2002] Vincent Ng", "Claire Cardie"], "venue": "In Proceedings of the 19th International Conference on Computational Linguistics, Taipei, Taiwan,", "citeRegEx": "Ng et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2002}, {"title": "Learning noun phrase anaphoricity to improve coreference resolution", "author": ["Vincent Ng"], "venue": "In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Ng.,? \\Q2004\\E", "shortCiteRegEx": "Ng.", "year": 2004}, {"title": "Graph-cut-based anaphoricity determination for coreference resolution", "author": ["Vincent Ng"], "venue": "In Proceedings of Human Language Technologies 2009: The Conference of the North American Chapter of the Association", "citeRegEx": "Ng.,? \\Q2009\\E", "shortCiteRegEx": "Ng.", "year": 2009}, {"title": "GloVe: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, Doha, Qatar,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The ZPG letter: Subjects, definiteness, and information-status", "author": ["Ellen F. Prince"], "venue": "Discourse Description. Diverse Linguistic Analyses of a FundRaising Text,", "citeRegEx": "Prince.,? \\Q1992\\E", "shortCiteRegEx": "Prince.", "year": 1992}, {"title": "The life and death of discourse entities: Identifying singleton mentions", "author": ["MarieCatherine de Marneffe", "Christopher Potts"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter", "citeRegEx": "Recasens et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Recasens et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Detecting Anaphoricity and Antecedenthood for Coreference Resolution", "author": ["Olga Uryupina"], "venue": "Procesamiento del Lenguaje Natural,", "citeRegEx": "Uryupina.,? \\Q2009\\E", "shortCiteRegEx": "Uryupina.", "year": 2009}, {"title": "On coreferring: Coreference in MUC and related annotation schemes", "author": ["van Deemter", "Kibble2000] Kees van Deemter", "Rodger Kibble"], "venue": null, "citeRegEx": "Deemter et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Deemter et al\\.", "year": 2000}, {"title": "Learning anaphoricity and antecedent ranking features for coreference resolution", "author": ["Wiseman et al.2015] Sam Wiseman", "Alexander M. Rush", "Stuart Shieber", "Jason Weston"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Wiseman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wiseman et al\\.", "year": 2015}, {"title": "Learning global features for coreference resolution", "author": ["Wiseman et al.2016] Sam Wiseman", "Alexander M. Rush", "Stuart Shieber"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Wiseman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wiseman et al\\.", "year": 2016}, {"title": "Global learning of noun phrase anaphoricity in coreference resolution via label propagation", "author": ["Zhou", "Kong2009] Guodong Zhou", "Fang Kong"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Zhou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 30, "context": "The current state-of-the-art coreference resolvers (Wiseman et al., 2016; Clark and Manning, 2016a; Clark and Manning, 2016b), as well as their", "startOffset": 51, "endOffset": 125}, {"referenceID": 27, "context": "discourse-new mentions (Uryupina, 2009).", "startOffset": 23, "endOffset": 39}, {"referenceID": 4, "context": "The approaches proposed by Evans (2001), M\u00fcller (2006), Bergsma et al.", "startOffset": 27, "endOffset": 40}, {"referenceID": 4, "context": "The approaches proposed by Evans (2001), M\u00fcller (2006), Bergsma et al.", "startOffset": 27, "endOffset": 55}, {"referenceID": 0, "context": "The approaches proposed by Evans (2001), M\u00fcller (2006), Bergsma et al. (2008), Bergsma and Yarowsky (2011) are examples of", "startOffset": 56, "endOffset": 78}, {"referenceID": 0, "context": "The approaches proposed by Evans (2001), M\u00fcller (2006), Bergsma et al. (2008), Bergsma and Yarowsky (2011) are examples of", "startOffset": 56, "endOffset": 107}, {"referenceID": 24, "context": "Each mention can be assessed from the point of view of the discourse model (Prince, 1992).", "startOffset": 75, "endOffset": 89}, {"referenceID": 21, "context": "Zhou and Kong (2009), Ng (2009), Wiseman et al.", "startOffset": 22, "endOffset": 32}, {"referenceID": 21, "context": "Zhou and Kong (2009), Ng (2009), Wiseman et al. (2015), Lassalle and Denis (2015), inter alia) while the task of anaphoric mention detection, based on its original definition, is of no use", "startOffset": 22, "endOffset": 55}, {"referenceID": 21, "context": "Zhou and Kong (2009), Ng (2009), Wiseman et al. (2015), Lassalle and Denis (2015), inter alia) while the task of anaphoric mention detection, based on its original definition, is of no use", "startOffset": 22, "endOffset": 82}, {"referenceID": 21, "context": "The approaches proposed by Ng and Cardie (2002), Ng (2004), Ng (2009), Zhou and Kong (2009),", "startOffset": 27, "endOffset": 48}, {"referenceID": 21, "context": "The approaches proposed by Ng and Cardie (2002), Ng (2004), Ng (2009), Zhou and Kong (2009),", "startOffset": 27, "endOffset": 59}, {"referenceID": 21, "context": "The approaches proposed by Ng and Cardie (2002), Ng (2004), Ng (2009), Zhou and Kong (2009),", "startOffset": 27, "endOffset": 70}, {"referenceID": 21, "context": "The approaches proposed by Ng and Cardie (2002), Ng (2004), Ng (2009), Zhou and Kong (2009),", "startOffset": 27, "endOffset": 92}, {"referenceID": 26, "context": "Sutskever et al. (2014) and Vinyals et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 26, "context": "Sutskever et al. (2014) and Vinyals et al. (2014)).", "startOffset": 0, "endOffset": 50}, {"referenceID": 10, "context": "A dropout (Hinton et al., 2012) with a rate of 0.", "startOffset": 10, "endOffset": 31}, {"referenceID": 23, "context": "We initialize the embeddings with the 300-dimensional Glove embeddings (Pennington et al., 2014).", "startOffset": 71, "endOffset": 96}], "year": 2017, "abstractText": "Only a year ago, all state-of-the-art coreference resolvers were using an extensive amount of surface features. Recently, there was a paradigm shift towards using word embeddings and deep neural networks, where the use of surface features is very limited. In this paper, we show that a simple SVM model with surface features outperforms more complex neural models for detecting anaphoric mentions. Our analysis suggests that using generalized representations and surface features have different strength that should be both taken into account for improving coreference resolution.", "creator": "LaTeX with hyperref package"}}}