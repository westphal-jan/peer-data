{"id": "1702.07539", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "Tight Bounds for Bandit Combinatorial Optimization", "abstract": "We revisit the study of optimal regret rates in bandit combinatorial optimization---a fundamental framework for sequential decision making under uncertainty that abstracts numerous combinatorial prediction problems. We prove that the attainable regret in this setting grows as $\\widetilde{\\Theta}(k^{3/2}\\sqrt{dT})$ where $d$ is the dimension of the problem and $k$ is a bound over the maximal instantaneous loss, disproving a conjecture of Audibert, Bubeck, and Lugosi (2013) who argued that the optimal rate should be of the form $\\widetilde{\\Theta}(k\\sqrt{dT})$. Our bounds apply to several important instances of the framework, and in particular, imply a tight bound for the well-studied bandit shortest path problem. By that, we also resolve an open problem posed by Cesa-Bianchi and Lugosi (2012).", "histories": [["v1", "Fri, 24 Feb 2017 11:17:33 GMT  (17kb)", "http://arxiv.org/abs/1702.07539v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alon cohen", "tamir hazan", "tomer koren"], "accepted": false, "id": "1702.07539"}, "pdf": {"name": "1702.07539.pdf", "metadata": {"source": "CRF", "title": "Tight Bounds for Bandit Combinatorial Optimization", "authors": ["Alon Cohen", "Tamir Hazan", "Tomer Koren"], "emails": ["alon.cohen@technion.ac.il", "tamir.hazan@technion.ac.il", "tkoren@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.07 539v 1 [cs.L G] 24 Feb 2017? dT q, where d is the dimension of the problem and k is a limit above the maximum momentary loss, refutes a conjecture by Audibert, Bubeck and Lugosi (2013), who argued that the optimal rate should be in the form r.pk? dT q. Our limits apply to several important instances of the framework, and in particular imply a narrow limit to the well-studied bandit problem with the shortest route. Thus, we also solve an open problem posed by Cesa-Bianchi and Lugosi (2012)."}, {"heading": "1 Introduction", "text": "In fact, it is the case that you are able to put yourself at the top, in the way that you are able to put yourself at the top."}, {"heading": "1.1 Related work", "text": "Combinatorial bandit optimization is closely related to a somewhat more general online learning scenario known as bandit linear optimization, first considered by Dani et al. (2008) and Abernethy et al. (2008). In this environment, decision group S is not limited to subsets of 1Note that the correlation discussed here is between different entries of the same loss vector and not between different loss vectors in different rounds. In particular, the loss vectors in our subordinate constructions are still i.i.d. so our limits also apply to stochastic i.i.d. case.the hypercube t0, 1ud and may be an arbitrary compact convex set in Rd; instead, the only requirement is that the loss of the learner be limited by the selection of measures in S (say by 1 in absolute value) for all possible loss vectors in the environment."}, {"heading": "2 Main results", "text": "\"We will now be able to solve the problem where a learner solves the problems of the learners at the same time.\" (\"We will now focus on solving the problems of the learners.\") (\"We will focus on solving the problem.\") (\"We will focus on solving the problems of the learners.\") (\"We will focus on solving the problem.\") (\"We will focus on solving the problem of the learners.\") (\"We will focus on solving the problem of the learners.\") (\"We will focus on solving the problems of the learners.\") (\"We will focus on solving the problems of the learners.\") (\"We will focus on solving the problems of the learners.\") (\"We will focus on solving the problems of the learners.\""}, {"heading": "3 Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Main result", "text": "In this section, we will prove theorem 1. We will show a lower limit of r\u0442 pk3 {2? dT q on the regret of any online learning algorithm applied to an instance of the multitask bandit problem. Surprisingly, the factor? k improvement is achieved by a simple modification of earlier constructions (Audibert et al., 2013). We will start with the application of the Yao Minimax principle, which means that it is sufficient to point out randomized strategies for the environment that force any deterministic learning algorithm to suffer losses (2? dT q Regret in expectation. We will construct the strategy of the environment as a consequence."}, {"heading": "3.2 Bandit shortest path", "text": "In this section, we show a lower boundary for the bandit shortest way problem, proving that theorem 2 q. Let's assume, without loss of generality, that k and d are even, and that d is a multiple of k. We show a lower boundary for regret by constructing a graph that simulates the multitask bandit problem with k {2 problems of d {k arms among each other. This graph consists of d edges and d {2'k {2 '1 corners in k {2 layers. Each layer has an incoming vertex connected to d {k intermediate points, all connected to the same starting vertex. This outgoing vertex is the incoming vertex of the next layer and so on. Note that to form an s-t path, the learner must pass through exactly one of the d {k vertex points in each layer, and each such vertex in each layer has such vertex and exactly such a vertex point."}, {"heading": "3.3 Online ranking", "text": "In this section, we prove theorem 3 by a construction similar to that in section 3.1, for which we introduce the following random environment. For the sentence S defined in Equation (4), let us set the vector indicating this choice by x \u2039 P S. For each round t, the environment tries Zt \"N p0, \u03c32q. Call the loss generated by the environment in round t L1tpiq\" 1 {2'x \u00b2 Piq'Zt for all i \"1, 2,...., d.We have the following Lemma.Lemma 6. Every deterministic player must suffer remorse for at least \u0441k3 {2? dT {8 in anticipation of an environment playing the losses L1,.., L1T. Now, to prove theorem 3, the above result can be adjusted to the losses in the same way as Theorem 5."}, {"heading": "4 Additional proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Proof of Lemma 4", "text": "\"We need to remember that we are able to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position,\" he said. \"We need to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position to act.\""}, {"heading": "4.2 Proof of Lemma 7", "text": "Proof. For each choice i \u2039 1, i \u2039 2,.., i \u2039 k, let's use xpi \u2039 q to denote the corresponding x \u2039 P S. According to Audibert et al. (2013) we consider xPSEx, 'j rTjs' \u00ffi \u2039 1,..., i \u2039 j '1, i \u2039 j '1, i \u2039 j '1,..., i \u2039 k\u00ffi \u2039 jExpi \u2039 q, 'j rTjs. Now that we have i \u2039 1,.., i \u2039 j '1, i \u2039 j '1,.., i \u2039 k the distribution pxpi \u2039 q, \"\" j is the same for each choice of i \u2039 j, and therefore we have to choose exactly one arm in the j'th problem for each round of the game, i \u2039 j '1, i \u2039 j \u2039 q, \"jrTjs\" T. \""}, {"heading": "4.3 Proof of Theorem 5", "text": "Proof: X1, X2,.., XT are the predictions of the learner against an environment that plays L1, L2,.., LT, and let R-T be the regret that the learner achieves, R-T \"T\u00fft\" 1Lt \"Xt\" min xPST\u00fft \"1Lt\" x. also define the feigned regret that is achieved by playing X1, X2,.., XT against an enivronment that is L11, L1 2,.., L1T asR \"1T\" T\u00fft \"1L1L1t\" Xt \"min xPST\u0430t\" 1L1t \"x.\" Now note that if it happens in each turn t, all coordinates of L1t lie between 0 and 1, then R \"T\" 1T. \""}, {"heading": "4.4 Proof of Lemma 6", "text": "Next we introduce the random variables T1,.., Tk, where each Tj is the number of times the learner played an xt in such a way that xtpi \"j q\" 1. For each x P S we introduce the notations Px and Ex, which indicate the probability and expectation in relation to the boundary distributions under which x \"x.\" Then RT \"E\" T \"1L1t\" j \"min xPST.\" 1L1t \"x ff.\" kT \"1pn\" kq! n! xPSEx rTjs, \"(9) and to continue, we must use the Th\" x. \""}, {"heading": "4.5 Proof of Lemma 8", "text": "Proof. Remember that we scan x \u2039 evenly according to the random principle of S, the set defined in Equation (4), and use UpSq to denote the even distribution over S. Then, remembering the random variables i \u2039 1, i \u2039 2,..., i \u2039 k we can calculate Ex \u2039 \"UpSqEx,\" \"j rTjs\" by taking i \u2039 1,...., i \u2039 j '1,.., i \u2039 k and the outer expectation only via i \u2039 j. Now, there is exactly n'k '1 possible way to choose i \u2039 j to achieve maximum agreement. Besides, the distribution Px \u2039, \"j 'is the same for any choice of i \u2039 j, and since the learner has to choose exactly one position for the j'te element in each round of the game, we have to choose' haveEx \u2039,\" UpSq \"Ex,\" 'j. \""}, {"heading": "5 Conclusion and open problems", "text": "In this essay, we have precisely characterized the optimal repentance rate for the combinatorial optimization of bandits and proved that it grows as a repentance rate. Our lower limits apply to important instances of the framework, including the bandit versions of the shortest online path and the online ranking problems. An interesting direction for future work is to explore instance-specific boundaries, i.e. boundaries that depend on the structure of the specific action set S used by the learner. What are the geometric and combinatorial properties of the group S nodes that dictate the optimal repentance rate in the learning problem? Especially in the specific context of the bandit problem with the shortest path, what are the graphical-theoretical properties of the network that regulate the difficulty of the online problem nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes nodes"}], "references": [{"title": "Competing in the dark: An efficient algorithm for bandit linear optimization", "author": ["J.D. Abernethy", "E. Hazan", "A. Rakhlin"], "venue": "In 21st Annual Conference on Learning Theory,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Regret in online combinatorial optimization", "author": ["J.-Y. Audibert", "S. Bubeck", "G. Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Audibert et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2013}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM journal on computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches", "author": ["B. Awerbuch", "R.D. Kleinberg"], "venue": "In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing,", "citeRegEx": "Awerbuch and Kleinberg.,? \\Q2004\\E", "shortCiteRegEx": "Awerbuch and Kleinberg.", "year": 2004}, {"title": "Towards minimax policies for online linear optimization with bandit feedback", "author": ["S. Bubeck", "N. Cesa-Bianchi", "S.M. Kakade", "S. Mannor", "N. Srebro", "R.C. Williamson"], "venue": "In COLT,", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "Combinatorial bandits", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2012}, {"title": "The price of bandit information for online optimization", "author": ["V. Dani", "S.M. Kakade", "T.P. Hayes"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "The on-line shortest path problem under partial monitoring", "author": ["A. Gy\u00f6rgy", "T. Linder", "G. Lugosi", "G. Ottucs\u00e1k"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gy\u00f6rgy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gy\u00f6rgy et al\\.", "year": 2007}, {"title": "Volumetric spanners: An efficient exploration basis for learning", "author": ["E. Hazan", "Z. Karnin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hazan and Karnin.,? \\Q2016\\E", "shortCiteRegEx": "Hazan and Karnin.", "year": 2016}, {"title": "Learning permutations with exponential weights", "author": ["D.P. Helmbold", "M.K. Warmuth"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Helmbold and Warmuth.,? \\Q2009\\E", "shortCiteRegEx": "Helmbold and Warmuth.", "year": 2009}, {"title": "Efficient algorithms for online decision problems", "author": ["A. Kalai", "S. Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "Non-stochastic bandit slate problems", "author": ["S. Kale", "L. Reyzin", "R.E. Schapire"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kale et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kale et al\\.", "year": 2010}, {"title": "First-order regret bounds for combinatorial semi-bandits", "author": ["G. Neu"], "venue": "In Proceedings of The 28th Conference on Learning Theory, pages 1360\u20131375,", "citeRegEx": "Neu.,? \\Q2015\\E", "shortCiteRegEx": "Neu.", "year": 2015}, {"title": "Importance weighting without importance weights: An efficient algorithm for combinatorial semi-bandits", "author": ["G. Neu", "G. Bart\u00f3k"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Neu and Bart\u00f3k.,? \\Q2016\\E", "shortCiteRegEx": "Neu and Bart\u00f3k.", "year": 2016}, {"title": "On the complexity of bandit linear optimization", "author": ["O. Shamir"], "venue": "In Proceedings of The 28th Conference on Learning Theory, pages 1523\u20131551,", "citeRegEx": "Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Shamir.", "year": 2015}, {"title": "Path kernels and multiplicative updates", "author": ["E. Takimoto", "M.K. Warmuth"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Takimoto and Warmuth.,? \\Q2003\\E", "shortCiteRegEx": "Takimoto and Warmuth.", "year": 2003}], "referenceMentions": [{"referenceID": 5, "context": "By that, we also resolve an open problem posed by Cesa-Bianchi and Lugosi (2012).", "startOffset": 50, "endOffset": 81}, {"referenceID": 15, "context": "Perhaps the most important and well-studied problem captured by this framework is online network routing, also known as the online shortest path problem (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005).", "startOffset": 153, "endOffset": 206}, {"referenceID": 10, "context": "Perhaps the most important and well-studied problem captured by this framework is online network routing, also known as the online shortest path problem (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005).", "startOffset": 153, "endOffset": 206}, {"referenceID": 9, "context": "Furthermore, we show how this lower bound can be adapted to the setting of online ranking (Helmbold and Warmuth, 2009).", "startOffset": 90, "endOffset": 118}, {"referenceID": 1, "context": "The study of bandit combinatorial optimization dates back to the work of Awerbuch and Kleinberg (2004), who considered the online shortest path problem in the bandit setting, henceforth called the bandit shortest path problem, in which the learner observes only the loss that she has suffered, and showed an Opkd5{3T 2{3q bound on the expected regret.", "startOffset": 73, "endOffset": 103}, {"referenceID": 1, "context": "The study of bandit combinatorial optimization dates back to the work of Awerbuch and Kleinberg (2004), who considered the online shortest path problem in the bandit setting, henceforth called the bandit shortest path problem, in which the learner observes only the loss that she has suffered, and showed an Opkd5{3T 2{3q bound on the expected regret. Dani et al. (2008) and Abernethy et al.", "startOffset": 73, "endOffset": 371}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence.", "startOffset": 11, "endOffset": 35}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms.", "startOffset": 11, "endOffset": 218}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form r Opk ? dT q, and could be obtained by a clever adaptation of their algorithm.", "startOffset": 11, "endOffset": 483}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form r Opk ? dT q, and could be obtained by a clever adaptation of their algorithm. More recently, Audibert et al. (2013) showed that the aforementioned r Opk3{2 ? dT q upper bound holds for any combinatorial bandit problem using a general online optimization algorithm.", "startOffset": 11, "endOffset": 711}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form r Opk ? dT q, and could be obtained by a clever adaptation of their algorithm. More recently, Audibert et al. (2013) showed that the aforementioned r Opk3{2 ? dT q upper bound holds for any combinatorial bandit problem using a general online optimization algorithm. Additionally, the authors gave a new lower bound of \u03a9pk ? dT q on the expected regret in combinatorial bandits, which leaves a gap of ? k between that and their upper bound (ignoring logarithmic factors). They conjectured as well that the lower bound is, in fact, the correct rate and articulated that the upper bound could be improved by non-trivial modifications of the existing algorithmic techniques. In this paper, we revisit the study of optimal regret rates in bandit combinatorial optimization. Our main contribution is in disproving the conjectures of Cesa-Bianchi and Lugosi (2012) and Audibert et al.", "startOffset": 11, "endOffset": 1452}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form r Opk ? dT q, and could be obtained by a clever adaptation of their algorithm. More recently, Audibert et al. (2013) showed that the aforementioned r Opk3{2 ? dT q upper bound holds for any combinatorial bandit problem using a general online optimization algorithm. Additionally, the authors gave a new lower bound of \u03a9pk ? dT q on the expected regret in combinatorial bandits, which leaves a gap of ? k between that and their upper bound (ignoring logarithmic factors). They conjectured as well that the lower bound is, in fact, the correct rate and articulated that the upper bound could be improved by non-trivial modifications of the existing algorithmic techniques. In this paper, we revisit the study of optimal regret rates in bandit combinatorial optimization. Our main contribution is in disproving the conjectures of Cesa-Bianchi and Lugosi (2012) and Audibert et al. (2013) and showing that the expected regret of combinatorial bandits in general, and of the bandit shortest path problem in particular, is in fact r \u0398pk3{2 ? dT q.", "startOffset": 11, "endOffset": 1479}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form r Opk ? dT q, and could be obtained by a clever adaptation of their algorithm. More recently, Audibert et al. (2013) showed that the aforementioned r Opk3{2 ? dT q upper bound holds for any combinatorial bandit problem using a general online optimization algorithm. Additionally, the authors gave a new lower bound of \u03a9pk ? dT q on the expected regret in combinatorial bandits, which leaves a gap of ? k between that and their upper bound (ignoring logarithmic factors). They conjectured as well that the lower bound is, in fact, the correct rate and articulated that the upper bound could be improved by non-trivial modifications of the existing algorithmic techniques. In this paper, we revisit the study of optimal regret rates in bandit combinatorial optimization. Our main contribution is in disproving the conjectures of Cesa-Bianchi and Lugosi (2012) and Audibert et al. (2013) and showing that the expected regret of combinatorial bandits in general, and of the bandit shortest path problem in particular, is in fact r \u0398pk3{2 ? dT q. Namely, we show a new lower bound of r \u03a9pk3{2 ? dT q for combinatorial bandits that matches the best known upper bound up to logarithmic factors, and also holds (via simple adaptations) in the context of bandit shortest path. Furthermore, we show how this lower bound can be adapted to the setting of online ranking (Helmbold and Warmuth, 2009). Surprisingly, the construction used in our lower bound is very simple and is based on straightforward adaptations of the one used by Audibert et al. (2013). Furthermore, our analysis is also significantly simpler and shorter than theirs.", "startOffset": 11, "endOffset": 2138}, {"referenceID": 5, "context": "1 Related work Combinatorial bandit optimization is closely related to a somewhat more general online learning scenario known as bandit linear optimization, which was first considered by Dani et al. (2008) and Abernethy et al.", "startOffset": 187, "endOffset": 206}, {"referenceID": 0, "context": "(2008) and Abernethy et al. (2008). In this setting, the decision set S is not restricted to subsets of Note that the correlation discussed here is between different entries of the same loss vector, rather than between different loss vectors at different rounds.", "startOffset": 11, "endOffset": 35}, {"referenceID": 11, "context": "A significant amount of work has been devoted to combinatorial optimization in the closely related semi-bandit feedback model (e.g., Gy\u00f6rgy et al., 2007; Kale et al., 2010; Audibert et al., 2013; Neu, 2015; Neu and Bart\u00f3k, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq \u201c 1.", "startOffset": 126, "endOffset": 228}, {"referenceID": 1, "context": "A significant amount of work has been devoted to combinatorial optimization in the closely related semi-bandit feedback model (e.g., Gy\u00f6rgy et al., 2007; Kale et al., 2010; Audibert et al., 2013; Neu, 2015; Neu and Bart\u00f3k, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq \u201c 1.", "startOffset": 126, "endOffset": 228}, {"referenceID": 12, "context": "A significant amount of work has been devoted to combinatorial optimization in the closely related semi-bandit feedback model (e.g., Gy\u00f6rgy et al., 2007; Kale et al., 2010; Audibert et al., 2013; Neu, 2015; Neu and Bart\u00f3k, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq \u201c 1.", "startOffset": 126, "endOffset": 228}, {"referenceID": 13, "context": "A significant amount of work has been devoted to combinatorial optimization in the closely related semi-bandit feedback model (e.g., Gy\u00f6rgy et al., 2007; Kale et al., 2010; Audibert et al., 2013; Neu, 2015; Neu and Bart\u00f3k, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq \u201c 1.", "startOffset": 126, "endOffset": 228}, {"referenceID": 3, "context": "State-of-the-art bounds for this problem were obtained by Bubeck et al. (2012a) and Hazan and Karnin (2016), the latter using computationally-efficient algorithms.", "startOffset": 58, "endOffset": 80}, {"referenceID": 3, "context": "State-of-the-art bounds for this problem were obtained by Bubeck et al. (2012a) and Hazan and Karnin (2016), the latter using computationally-efficient algorithms.", "startOffset": 58, "endOffset": 108}, {"referenceID": 3, "context": "State-of-the-art bounds for this problem were obtained by Bubeck et al. (2012a) and Hazan and Karnin (2016), the latter using computationally-efficient algorithms. The general linear optimization setting allows for more general geometries of the sets in which the decisions and the loss vectors reside (e.g., they are typically assumed to be subsets of the Euclidean unit ball), and consequently the bounds obtained in that setting are often not immediately comparable to those in the combinatorial one. In particular, the lower bounds proved by Dani et al. (2008) and more recently by Shamir (2015) hold in the general linear optimization setting (with Euclidean geometry) and do not apply to any natural problem in the combinatorial setting.", "startOffset": 58, "endOffset": 565}, {"referenceID": 3, "context": "State-of-the-art bounds for this problem were obtained by Bubeck et al. (2012a) and Hazan and Karnin (2016), the latter using computationally-efficient algorithms. The general linear optimization setting allows for more general geometries of the sets in which the decisions and the loss vectors reside (e.g., they are typically assumed to be subsets of the Euclidean unit ball), and consequently the bounds obtained in that setting are often not immediately comparable to those in the combinatorial one. In particular, the lower bounds proved by Dani et al. (2008) and more recently by Shamir (2015) hold in the general linear optimization setting (with Euclidean geometry) and do not apply to any natural problem in the combinatorial setting.", "startOffset": 58, "endOffset": 600}, {"referenceID": 1, "context": ", 2010; Audibert et al., 2013; Neu, 2015; Neu and Bart\u00f3k, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq \u201c 1. For example, in the context of the online shortest path problem, instead of observing just the overall cost of the chosen path (as is the case in the bandit setting), the player may observe the individual cost of each edge in that path. In the semi-bandit case, however, the regret of bandit combinatorial optimization is by now well understood, and is known to be of the form \u0398p ? kdT q; see Audibert et al. (2013) and the references therein.", "startOffset": 8, "endOffset": 671}, {"referenceID": 1, "context": ", 2010; Audibert et al., 2013; Neu, 2015; Neu and Bart\u00f3k, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq \u201c 1. For example, in the context of the online shortest path problem, instead of observing just the overall cost of the chosen path (as is the case in the bandit setting), the player may observe the individual cost of each edge in that path. In the semi-bandit case, however, the regret of bandit combinatorial optimization is by now well understood, and is known to be of the form \u0398p ? kdT q; see Audibert et al. (2013) and the references therein. For further and more detailed account on related partial information models and their regret analysis, we refer to the recent survey by Bubeck et al. (2012b).", "startOffset": 8, "endOffset": 857}, {"referenceID": 2, "context": "Our lower bound is attained in the multitask bandit problem, in which a learner is simultaneously trying to solve k instances of the n-armed bandit problem (Auer et al., 2002) with n \u201c d{k (we assume for simplicity that the latter is an integer).", "startOffset": 156, "endOffset": 175}, {"referenceID": 4, "context": "Note, however, that up to logarithmic factors the bound is tight and matches the upper bounds of Bubeck et al. (2012a) and Hazan and Karnin (2016).", "startOffset": 97, "endOffset": 119}, {"referenceID": 4, "context": "Note, however, that up to logarithmic factors the bound is tight and matches the upper bounds of Bubeck et al. (2012a) and Hazan and Karnin (2016). The lower bound of Theorem 1 does not hold for any set S but rather to an instance of the multitask bandit problem.", "startOffset": 97, "endOffset": 147}, {"referenceID": 5, "context": "Again, the theorem implies that the tight regret rate for bandit shortest path is r \u0398pk3{2 ? dT q, contrary to what was conjectured in the literature (Cesa-Bianchi and Lugosi, 2012).", "startOffset": 150, "endOffset": 181}, {"referenceID": 1, "context": "Surprisingly, the factor ? k improvement is obtained via a simple modification of previous constructions (Audibert et al., 2013).", "startOffset": 105, "endOffset": 128}, {"referenceID": 1, "context": "Following Audibert et al. (2013), we consider \u00ff", "startOffset": 10, "endOffset": 33}, {"referenceID": 1, "context": "Cesa-Bianchi and Lugosi (2012) and Audibert et al. (2013). Our lower bounds apply to important instances of the framework, including the bandit versions of the online shortest path and the online ranking problems.", "startOffset": 35, "endOffset": 58}], "year": 2017, "abstractText": "We revisit the study of optimal regret rates in bandit combinatorial optimization\u2014a fundamental framework for sequential decision making under uncertainty that abstracts numerous combinatorial prediction problems. We prove that the attainable regret in this setting grows as r \u0398pk3{2 ? dT q where d is the dimension of the problem and k is a bound over the maximal instantaneous loss, disproving a conjecture of Audibert, Bubeck, and Lugosi (2013) who argued that the optimal rate should be of the form r \u0398pk ? dT q. Our bounds apply to several important instances of the framework, and in particular, imply a tight bound for the well-studied bandit shortest path problem. By that, we also resolve an open problem posed by Cesa-Bianchi and Lugosi (2012).", "creator": "LaTeX with hyperref package"}}}