{"id": "1702.07664", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "How ConvNets model Non-linear Transformations", "abstract": "In this paper, we theoretically address three fundamental problems involving deep convolutional networks regarding invariance, depth and hierarchy. We introduce the paradigm of Transformation Networks (TN) which are a direct generalization of Convolutional Networks (ConvNets). Theoretically, we show that TNs (and thereby ConvNets) are can be invariant to non-linear transformations of the input despite pooling over mere local translations. Our analysis provides clear insights into the increase in invariance with depth in these networks. Deeper networks are able to model much richer classes of transformations. We also find that a hierarchical architecture allows the network to generate invariance much more efficiently than a non-hierarchical network. Our results provide useful insight into these three fundamental problems in deep learning using ConvNets.", "histories": [["v1", "Fri, 24 Feb 2017 17:09:22 GMT  (113kb,D)", "http://arxiv.org/abs/1702.07664v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["dipan k pal", "marios savvides"], "accepted": false, "id": "1702.07664"}, "pdf": {"name": "1702.07664.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Dipan K. Pal", "Marios Savvides"], "emails": ["dipanp@cmu.edu", "msavvid@cmu.edu"], "sections": [{"heading": null, "text": "In this paper, we will theoretically address three fundamental problems of deep Convolutionary Networks in terms of invariance, depth, and hierarchy. We will present the paradigm of Transformation Networks (TN), which is a direct generalization of Convolutional Networks (ConvNets). Theoretically, we will show that TNs (and thus ConvNets) can be invariant to nonlinear transformations of input despite bundling via mere local translations. Our analysis provides clear insights into the increase in inventory with depth in these networks. Deeper networks are able to model much richer classes of transformations. We also find that a hierarchical architecture allows the network to generate inventory much more efficiently than a non-hierarchical network. Our results provide useful insights into these three fundamental problems of deep learning with ConvNets."}, {"heading": "1 INTRODUCTION", "text": "It is a matter of time before there will be a realignment. (...) It is a matter of time before there will be a realignment. (...) It is a matter of time before there will be a realignment. (...) It is a matter of time before there will be a realignment. (...) It is a matter of time before there will be a realignment. (...) It is a matter of time before there will be a realignment. (...) It is a matter of time before there will be a realignment. \""}, {"heading": "2 TRANSFORMATION NETWORKS", "text": "The question is whether the \"new\" idea of the \"old,\" \"new,\" \"new,\" \"new,\" \"old,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"old,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" \"new,\" new, \"\" new, \"new,\" \"new,\" new, \"\" new, \"new,\" \"new,\" \"new,\" \"new,\" new, \"\" new, \"\" new, \"\" new, \"new,\" \"new,\" \"new,\" new, \"\" new, \"\" new, \"new,\" \"new,\" new, \"\" new, \"\" new, \"new,\" new, \"\" new, \"\" new, \"new,\" \"new,\" new, \"\" new, \"new,\" \"new,\" \"new,\" new, \"new,\" \"new,\" \"new,\" new, \"new,\" \"new,\" \"new,\" \"new,\" new, \"new,\" \"new,\" new, \"\" \"\" new, \"new,\" new, \"\" new, \"new,\" new, \"\" new, \"\" \"\" new, \"new,\" \"new,\" \"new,\" new, \"\" new, \"new,\" \"\" new, \"\" new, \"new,\" \"\" \"new,\" \"new,\" \"\" new, \"\" new, \"new,\" \"\" \"new,\" \"\" new, \"\" \"\" new, \"new,\" \"\" new, \"new,\" \"\" \"\" new, \"new,\" new, \"\" \"\" \"new,\" \"\" \"new,\" new, \"\" \"new,\" \""}, {"heading": "3 INVARIANCES IN A TRANSFORMATION NETWORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 LINEAR UNITARY GROUP INVARIANCE IN SINGLE LAYER TRANSFORMATION NETWORKS", "text": "The fact is that we see ourselves as being able to be in a position, and that we are able to be in a position, we will be able to be in a position, we will be able to be in a position, we will be able to be in a position, we will be able to be in a position, we will be able to be in a position, we will be able to be in a position, we will be able to be in a position, we will be able to put ourselves in a position, we will be able to put ourselves in a position. \""}, {"heading": "3.2 NON-LINEAR ACTIVATION IN TRANSFORMATION NETWORKS", "text": "In the case of a TN with 2 or more levels, the non-linear activation function (under certain conditions) is an aid in generating invariance to non-linear transformations in the input area. To show this, we must first show that such non-linear activation can maintain the uniform group structure in the area of the function. This uniform group structure is observed by TN nodes in the downstream area of the function (higher up to the layers), which are then able to generate inventory through group integration in order to generate the non-linear activation function 3.2.Conditions on the non-linear activation function: We now specify the conditions on the non-linear activation function (\u00b7).1. Condition 1: (Unitarity) We define a function to be."}, {"heading": "3.3 NON-LINEAR GROUP INVARIANCE IN MULTI-LAYER TRANSFORMATION NETWORKS", "text": "Analysis of a two-layer TN: Consider a simple two-layer TN with four TN nodes in layer 1 (11, 12, 13, 14), each using non-overlapping areas of the raw input image. Let's leave a TN node in layer 2 (11, G11), (t12, G12), (t13, G13), (t14, G14), (t2, G2), etc. The transformed templates were learned using the unsupervised Learning Protocol or a supervised Learning Protocol that maintains the group structure of each template."}, {"heading": "3.4 THE NEED FOR MULTIPLE TEMPLATES OR CHANNELS", "text": "So far, our analysis has assumed that the TN nodes have a single channel or a single template, and the attribute at level 2 and higher was multidimensional simply because of the different support rates above the image. However, our results naturally extend to multiple channels with multiple templates, as we do not make any assumption as to the relationship between the templates. Anselmi et al. (2013) suggest that several templates are needed to better measure the invariant probability distribution (over pixels) of a group of transformed images. In fact, the quantity < g (x), t > ig-G is a 1-D projection along the distribution of the quantity {g (x)} g-G-G. More the number of templates or channels, a better estimate of the probability distribution based on the Cramer-Wold theorem. This result also applies to our framework, as the dot products in a TN 1-D projection of the transformed data onto a TN node are likely to improve the distribution based on the Wold theorem."}, {"heading": "4 TOWARDS CONVOLUTIONAL ARCHITECTURES", "text": "Our framework for transformation networks models the transforming templates in each TN node as unified network structures. To apply supervised learning or backpropagation to these architectures, one must address the critical problem of maintaining the group structure in the template sets while optimizing the templates themselves. If the backpropagation is naively applied to all the templates in a template set, assuming they start with an intact group structure, they will converge to the same template throughout the set and the group structure will be lost. One way to address this problem is to assume that all the groups in the TN are identical and parametric. A parametric transformation that can be applied efficiently would allow us to explicitly generate the template sets for merging or group integration."}, {"heading": "5 CONCLUSION", "text": "We have shown that TNs (and thus ConvNets) can be invariant to nonlinear transformations of the input, even though they only bundle local uniform transformations. We have also shown that deeper networks are able to model much richer classes of transformations. Furthermore, we have found that a hierarchical architecture allows the network to generate invariance much more efficiently than a non-hierarchical network."}, {"heading": "6 PROOFS OF THEORETICAL RESULTS", "text": "All group-theoretical summaries also apply to finite groups."}, {"heading": "6.1 PROOF OF LEMMA 3.1", "text": "Proof. We have, g \"(ig\" G (x) dg) =, g \"g (x) dg =, g\" (x) g \"(x) dg\" =, g \"g (x) dgBecause the normalized hair measure is invariant, i.e., dg = dg.\" Intuitively, g \"simply rearranges the group integral based on elementary group properties."}, {"heading": "6.2 PROOF OF LEMMA 3.2", "text": "Proof. We have,,,,, G \"(x) =,,, G\" (x),, g \"(t) > dg\" (14) =,,,, G \"< x, g\" \u2212 1 (g \"t) > dg\") (15) =,,,, G \"< x\" (t) > dg \") (16) =,,,, G\" (x) (17) Equation 15 uses the fact that g \"G\" is uniform. Equation 16 shows a change in variables. Since g, \"g\" G, \"ie g\" G \"G. Furthermore dg = dg,\" because the hair measure is uniform."}, {"heading": "6.3 PROOF OF THEOREM 3.3", "text": "Proof. We have < \u03b7 (gx), \u03b7 (gy) > = < \u03b7 (x), \u03b7 (y) > = < g\u03b7 (x), g\u03b7 (y) > because the function \u03b7 is uniform. We define g\u03b7 (x) as the action or transformation of g\u03b7 (x). However, for any vector p and a scalar \u03b1 we must be linear. Linearity of g\u03b7 can be derived from the linearity of the internal product and its conservation under g\u03b7 in \u03b7. For any vector p and a scalar \u03b1 we have | | short-term (p) \u2212 g\u03b7 (p) \u2212 g\u03b7 (\u03b1p) | 2 (18) = < short-term p \u2212 g\u043e p \u2212 g\u0430 (\u03b1p), short-term p \u2212 g\u0430 (\u03b1p), short-term p \u2212 and long-term g properties are fixed."}], "references": [{"title": "Unsupervised learning of invariant representations in hierarchical architectures", "author": ["Fabio Anselmi", "Joel Z Leibo", "Lorenzo Rosasco", "Jim Mutch", "Andrea Tacchetti", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1311.4158,", "citeRegEx": "Anselmi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anselmi et al\\.", "year": 2013}, {"title": "Learning stable group invariant representations with convolutional networks", "author": ["Joan Bruna", "Arthur Szlam", "Yann LeCun"], "venue": "arXiv preprint arXiv:1301.3537,", "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "On the expressive power of deep learning: A tensor analysis", "author": ["Nadav Cohen", "Or Sharir", "Amnon Shashua"], "venue": "CoRR, abs/1509.05009,", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "Shallow vs. deep sum-product networks", "author": ["Olivier Delalleau", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Delalleau and Bengio.,? \\Q2011\\E", "shortCiteRegEx": "Delalleau and Bengio.", "year": 2011}, {"title": "Measuring invariances in deep networks. In Advances in neural information processing", "author": ["Ian Goodfellow", "Honglak Lee", "Quoc V Le", "Andrew Saxe", "Andrew Y Ng"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2009}, {"title": "Global optimality in tensor factorization, deep learning, and beyond", "author": ["Benjamin D. Haeffele", "Ren\u00e9 Vidal"], "venue": "CoRR, abs/1506.07540,", "citeRegEx": "Haeffele and Vidal.,? \\Q2015\\E", "shortCiteRegEx": "Haeffele and Vidal.", "year": 2015}, {"title": "Deep learning without poor local minima", "author": ["Kenji Kawaguchi"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Kawaguchi.,? \\Q2016\\E", "shortCiteRegEx": "Kawaguchi.", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Group invariant scattering", "author": ["St\u00e9phane Mallat"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Mallat.,? \\Q2012\\E", "shortCiteRegEx": "Mallat.", "year": 2012}, {"title": "On the expressive efficiency of sum product networks", "author": ["James Martens", "Venkatesh Medabalimi"], "venue": "arXiv preprint arXiv:1411.7717,", "citeRegEx": "Martens and Medabalimi.,? \\Q2014\\E", "shortCiteRegEx": "Martens and Medabalimi.", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Why does deep learning work?-a perspective from group theory", "author": ["Arnab Paul", "Suresh Venkatasubramanian"], "venue": "arXiv preprint arXiv:1412.6621,", "citeRegEx": "Paul and Venkatasubramanian.,? \\Q2014\\E", "shortCiteRegEx": "Paul and Venkatasubramanian.", "year": 2014}, {"title": "A short note about the application of polynomial kernels with fractional degree in support vector learning", "author": ["Rolf Rossius", "G\u00e9rard Zenker", "Andreas Ittner", "Werner Dilger"], "venue": "In European Conference on Machine Learning,", "citeRegEx": "Rossius et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Rossius et al\\.", "year": 1998}], "referenceMentions": [{"referenceID": 7, "context": "It is a well known fact that deep Convolutional Networks (or ConvNets) LeCun et al. (1998) generate invariance to local translations due to convolutions followed by a form of pooling.", "startOffset": 71, "endOffset": 91}, {"referenceID": 7, "context": "In practice, however, studies such as Krizhevsky et al. (2012) have applied these models very successfully to domains such as vision, which typically involve data undergoing highly non-linear transformations.", "startOffset": 38, "endOffset": 63}, {"referenceID": 4, "context": "For instance, Kawaguchi (2016) proved important results for deep neural networks.", "startOffset": 14, "endOffset": 31}, {"referenceID": 1, "context": "Whereas Cohen et al. (2015); Haeffele & Vidal (2015) approached deep learning from the perspective of general tensor decompositions.", "startOffset": 8, "endOffset": 28}, {"referenceID": 1, "context": "Whereas Cohen et al. (2015); Haeffele & Vidal (2015) approached deep learning from the perspective of general tensor decompositions.", "startOffset": 8, "endOffset": 53}, {"referenceID": 1, "context": "Whereas Cohen et al. (2015); Haeffele & Vidal (2015) approached deep learning from the perspective of general tensor decompositions. All of these studies however, have focused on the supervised version of deep learning. Under supervision, theoretical results can be broadly described to be concerned with the optimality of a solution or properties of the optimization landscape. Given the success of supervised models, such an approach is definitely beneficial in advancing overall understanding. It however, considers architectures more general in nature since supervised results for specialized architectures are more difficult to obtain. Unsupervised deep learning however, promises to play an important role in the future not to mention kindling interests from a neoroscientific perspective. The analysis of our models is therefore aimed at the unsupervised setting and focuses more on the invariance properties of such networks. This reveals new insights into properties of the architecture itself and provides an explanation as to why increasing depth is useful on many fronts. Even though there have been theoretical efforts Delalleau & Bengio (2011); Martens & Medabalimi (2014) to provide results related to the \u201cdepth\u201d of a network, the models studied do not immediately resemble the most successful architecture class in practice, ConvNets and its variants.", "startOffset": 8, "endOffset": 1158}, {"referenceID": 1, "context": "Whereas Cohen et al. (2015); Haeffele & Vidal (2015) approached deep learning from the perspective of general tensor decompositions. All of these studies however, have focused on the supervised version of deep learning. Under supervision, theoretical results can be broadly described to be concerned with the optimality of a solution or properties of the optimization landscape. Given the success of supervised models, such an approach is definitely beneficial in advancing overall understanding. It however, considers architectures more general in nature since supervised results for specialized architectures are more difficult to obtain. Unsupervised deep learning however, promises to play an important role in the future not to mention kindling interests from a neoroscientific perspective. The analysis of our models is therefore aimed at the unsupervised setting and focuses more on the invariance properties of such networks. This reveals new insights into properties of the architecture itself and provides an explanation as to why increasing depth is useful on many fronts. Even though there have been theoretical efforts Delalleau & Bengio (2011); Martens & Medabalimi (2014) to provide results related to the \u201cdepth\u201d of a network, the models studied do not immediately resemble the most successful architecture class in practice, ConvNets and its variants.", "startOffset": 8, "endOffset": 1187}, {"referenceID": 0, "context": "There have been a few important efforts towards providing results from a unsupervised standpoint Anselmi et al. (2013); Mallat (2012).", "startOffset": 97, "endOffset": 119}, {"referenceID": 0, "context": "There have been a few important efforts towards providing results from a unsupervised standpoint Anselmi et al. (2013); Mallat (2012). Mallat (2012) shows that local translation invariance leads to contractions in space.", "startOffset": 97, "endOffset": 134}, {"referenceID": 0, "context": "There have been a few important efforts towards providing results from a unsupervised standpoint Anselmi et al. (2013); Mallat (2012). Mallat (2012) shows that local translation invariance leads to contractions in space.", "startOffset": 97, "endOffset": 149}, {"referenceID": 0, "context": "Anselmi et al. (2013) approach the problem in a fashion more similar to ours with the use of unitary groups to \u201ctransfer\u201d invariance.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Anselmi et al. (2013) approach the problem in a fashion more similar to ours with the use of unitary groups to \u201ctransfer\u201d invariance. They show that for a hierarchical feed-forward network with unitary group structure, the features at top layers would be exactly invariant to groups of transformations acting over a larger receptive field. Our main result, on the other hand is more precise. We show that the top layer features is in fact invariant to non-linear transformations despite only pooling over linear transforms. Further, these non-linear transformations need not form a group overall. They are only required to form a group locally at every layer. The architecture we consider is very closely related to practical architectures used for ConvNets, whereas Anselmi et al. (2013) model the architecture utilizing simple and complex cell constructions from a more biologically motivated approach.", "startOffset": 0, "endOffset": 789}, {"referenceID": 0, "context": "Anselmi et al. (2013) approach the problem in a fashion more similar to ours with the use of unitary groups to \u201ctransfer\u201d invariance. They show that for a hierarchical feed-forward network with unitary group structure, the features at top layers would be exactly invariant to groups of transformations acting over a larger receptive field. Our main result, on the other hand is more precise. We show that the top layer features is in fact invariant to non-linear transformations despite only pooling over linear transforms. Further, these non-linear transformations need not form a group overall. They are only required to form a group locally at every layer. The architecture we consider is very closely related to practical architectures used for ConvNets, whereas Anselmi et al. (2013) model the architecture utilizing simple and complex cell constructions from a more biologically motivated approach. Further, they hypothesize that the non-linearity serves as a way measuring bins of the CDF of an invariant distribution. On the other hand, we consider the non-linearity to be an integral part of the process to preserve unitary group structure in the feature space. This also leads to it being a part of the class or range of transformations to be invariant towards. In turn this observation leads to the critical result that the overall architecture is invariant to non-linear transformations despite pooling over linear transforms. Finally, Bruna et al. (2013); Paul & Venkatasubramanian (2014) also applied group theory to a certain extent to the problem of representation learning.", "startOffset": 0, "endOffset": 1468}, {"referenceID": 0, "context": "Anselmi et al. (2013) approach the problem in a fashion more similar to ours with the use of unitary groups to \u201ctransfer\u201d invariance. They show that for a hierarchical feed-forward network with unitary group structure, the features at top layers would be exactly invariant to groups of transformations acting over a larger receptive field. Our main result, on the other hand is more precise. We show that the top layer features is in fact invariant to non-linear transformations despite only pooling over linear transforms. Further, these non-linear transformations need not form a group overall. They are only required to form a group locally at every layer. The architecture we consider is very closely related to practical architectures used for ConvNets, whereas Anselmi et al. (2013) model the architecture utilizing simple and complex cell constructions from a more biologically motivated approach. Further, they hypothesize that the non-linearity serves as a way measuring bins of the CDF of an invariant distribution. On the other hand, we consider the non-linearity to be an integral part of the process to preserve unitary group structure in the feature space. This also leads to it being a part of the class or range of transformations to be invariant towards. In turn this observation leads to the critical result that the overall architecture is invariant to non-linear transformations despite pooling over linear transforms. Finally, Bruna et al. (2013); Paul & Venkatasubramanian (2014) also applied group theory to a certain extent to the problem of representation learning.", "startOffset": 0, "endOffset": 1502}, {"referenceID": 13, "context": "Although not prevalent, such kernels are valid Rossius et al. (1998). These functions exactly unitary and are approximately stable (d being arbitrarily close to 1 but not equal) for the range of values typical in activation functions.", "startOffset": 47, "endOffset": 69}, {"referenceID": 0, "context": "We express this formally through a property which was previously shown to be true for hierarchical architectures employing group integrals Anselmi et al. (2013). Property 3.", "startOffset": 139, "endOffset": 161}, {"referenceID": 0, "context": "Anselmi et al. (2013) suggest the need for multiple templates as a way of better measuring the invariant probability distribution (over pixels) of a group of transformed images.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Anselmi et al. (2013) suggest the need for multiple templates as a way of better measuring the invariant probability distribution (over pixels) of a group of transformed images. Indeed, the quantity \u3008g(x), t\u3009 \u2200g \u2208 G is a 1-D projection along t of the distribution of the set {g(x)} \u2200g \u2208 G. More the number of templates or channels, better the estimate of the probability distribution due to the Cramer-Wold\u2019s theorem. This result also holds true for our framework since the dot-products in a TN are 1-D projections of the transformed data onto a TN node template. The reason that this probability distribution is important is because Anselmi et al. (2013) show that it itself is an invariant to the action of the group G.", "startOffset": 0, "endOffset": 656}, {"referenceID": 4, "context": "Goodfellow et al. (2009) has studied the problem of visualizing and measuring these invariances generated by a ConvNet and provided empirical justifications for increasing depth.", "startOffset": 0, "endOffset": 25}], "year": 2017, "abstractText": "In this paper, we theoretically address three fundamental problems involving deep convolutional networks regarding invariance, depth and hierarchy. We introduce the paradigm of Transformation Networks (TN) which are a direct generalization of Convolutional Networks (ConvNets). Theoretically, we show that TNs (and thereby ConvNets) are can be invariant to non-linear transformations of the input despite pooling over mere local translations. Our analysis provides clear insights into the increase in invariance with depth in these networks. Deeper networks are able to model much richer classes of transformations. We also find that a hierarchical architecture allows the network to generate invariance much more efficiently than a non-hierarchical network. Our results provide useful insight into these three fundamental problems in deep learning using ConvNets.", "creator": "LaTeX with hyperref package"}}}