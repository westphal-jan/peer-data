{"id": "1702.07793", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "Residual Convolutional CTC Networks for Automatic Speech Recognition", "abstract": "Deep learning approaches have been widely used in Automatic Speech Recognition (ASR) and they have achieved a significant accuracy improvement. Especially, Convolutional Neural Networks (CNNs) have been revisited in ASR recently. However, most CNNs used in existing work have less than 10 layers which may not be deep enough to capture all human speech signal information. In this paper, we propose a novel deep and wide CNN architecture denoted as RCNN-CTC, which has residual connections and Connectionist Temporal Classification (CTC) loss function. RCNN-CTC is an end-to-end system which can exploit temporal and spectral structures of speech signals simultaneously. Furthermore, we introduce a CTC-based system combination, which is different from the conventional frame-wise senone-based one. The basic subsystems adopted in the combination are different types and thus mutually complementary to each other. Experimental results show that our proposed single system RCNN-CTC can achieve the lowest word error rate (WER) on WSJ and Tencent Chat data sets, compared to several widely used neural network systems in ASR. In addition, the proposed system combination can offer a further error reduction on these two data sets, resulting in relative WER reductions of $14.91\\%$ and $6.52\\%$ on WSJ dev93 and Tencent Chat data sets respectively.", "histories": [["v1", "Fri, 24 Feb 2017 22:49:13 GMT  (126kb,D)", "http://arxiv.org/abs/1702.07793v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yisen wang", "xuejiao deng", "songbai pu", "zhiheng huang"], "accepted": false, "id": "1702.07793"}, "pdf": {"name": "1702.07793.pdf", "metadata": {"source": "META", "title": "Residual Convolutional CTC Networks for Automatic Speech Recognition", "authors": ["Yisen Wang", "Xuejiao Deng", "Songbai Pu", "Zhiheng Huang"], "emails": ["WANGYS14@MAILS.TSINGHUA.EDU.CN", "SOPHIADENG@TENCENT.COM", "JOHNSONPU@TENCENT.COM", "ZHIHHUANG@TENCENT.COM"], "sections": [{"heading": null, "text": "However, most of the CNNs used in the existing work have less than 10 layers, which may not be deep enough to capture all human speech signal information. In this paper, we propose a novel deep and broad CNN architecture, known as RCNN-CTC, with residual connections and connectionist temporal classification (CTC) loss function. RCNN-CTC is an end system that can exploit temporal and spectral structures of speech signals simultaneously. In addition, we are introducing a CTC-based system combination that differs from the traditional frame-based senon system. The basic subsystems used in the combination are different types and therefore complement each other. Experimental results show that our proposed single RCNN-CTC system can achieve the lowest word error rate (WER-senon-based), with the WER-R systems reducing two cent SDR sets of data sets by two percent each compared to DR-2 percent."}, {"heading": "1. Introduction", "text": "In recent years, the deep learning system (Yu & Deng, 2014) has been successfully applied to ASR to increase recognition accuracy. Recently, CNN has become an attractive model in ASR that converts speech signals into features used in computer vision (LeCun & Bengio, 1998). Compared to other deep learning architectures, CNN has several advantages: 1) CNN is capable of utilizing local correlations of human speech signals both in time and frequency. 2) CNN has the ability to utilize translational inventory in signal systems. Most of CNN's earlier uses in ASR have few revolutionary layers, followed by a number of recursive layers. These CNN structures are often less than 10 layers, which may not be deep enough to capture all of the information of human speech signals, especially for a long time."}, {"heading": "2. Related Work", "text": "In recent years, the number of people who are able to reproduce has multiplied, not only in the United States, but also in many other countries. In the United States, the number of people who are able to do their work is very high. In the United States, the number of people who are able to do their work is much higher than in other countries. In the United States, the number of people who are able to do their work is much higher than in other countries. In the United States, the number of people who are able to do their work is much higher than in the United States."}, {"heading": "3. Residual Convolutional CTC Networks", "text": "As mentioned above, CNN and CTC both possess excellent properties for the ASR task, but the combination of these two components has not yet been fully explored. In this article, we propose a novel Remaining Convolutionary CTC network architecture, namely RCNN-CTC, which is very deep (more than 40 layers) in order to obtain the full value of CNN, Remaining Connections and CTC."}, {"heading": "3.1. Residual CNN", "text": "Generally speaking, deep CNNs can improve generalization and outperform flat networks. However, they tend to be more deeply defined and slower to converge. Residual networks (ResNets) (He et al., 2016) have recently been proposed to facilitate the formation of very deep CNNs. ResNet is composed of a number of stacked residual blocks, and each block contains direct connections between the lower layer exits and the higher layer entrances. The residual block (described in Figure 1) is defined as: y = F (x, Wi) + x, (1) where x and y are the input and output of the considered layers, and F is the stacked nonlinear layer mapping function. Note that identity shortcut connections of x do not add additional parameters and computational complexity. With the presence of residual connections, ResNet can improve convergence speed in training. ResNet can also achieve accuracy gains from the previous layer, which are much better."}, {"heading": "3.2. CTC", "text": "Traditional acoustic model training is based on frame-level labels with cross-entropy criterion (CE), which requires a lengthy label alignment process. Subsequently (Hannun et al., 2014; Amodei et al., 2015; Miao et al., 2015), we adopt the CTC target (Graves et al., 2006) to automatically learn the alignment between speech frames and their label sequences, resulting in end-to-end training. To align the network outputs with the label sequences, an intermediate representation of the CTC path is introduced in (Graves et al., 2006). The label sequence z can then be mapped to the corresponding CTC paths. It is a one-dimensional mapping, as multiple CTC paths can match the same label sequence. For example, both \"A-A paths\" of A-D paths as well as \"A-B and D-D-D-D paths are mapped."}, {"heading": "4. CTC-based System Combination", "text": "As for the conventional system combination, its performance improvement is small due to the small difference between subsystems. Therefore, we propose a system combination method that takes into account the diversity and complementarity between subsystems. As a result, our proposed system combination can achieve an absolute WER reduction of 1% on WSJ and Tencent chat records."}, {"heading": "4.1. Subsystems Selection", "text": "Our selection of subsystems is guided by the following principles: compared to the transcription (Ground Truth) G, we first identify the correct parts / words Ci in the decryption text of each subsystem i. We then search for the combination of subsystems and calculate their composition of correct words U = i Ci. We define a maximum correct word rate (MCWR) as the selection criterion: MCWR = 4 G I (w, U) |, (3) where I () is the indicator function that 1 if (\u00b7) is true and 0 otherwise, and | is the length of the basic truth G. Our goal is to select the combination that achieves the highest MCWR while using a minimum number of subsystems at the same time. Through this method, we can use the lowest cost to find subsystems that complement each other."}, {"heading": "4.2. Challenges", "text": "Our proposed single system RCNN-CTC uses a CTC output. Therefore, the system combination is CTC-based and differs from the framed CE-based system because the peak reactions of CTC may not match in each subsystem. Furthermore, the output reliability of each subsystem is not on the same time scale, which confuses the decryption process of the Weighted Finite-State Transducer (WFST) used in our experiments. Inspired by ROVER (Fiscus, 1997), we propose our CTC-based system combination method (Figure 3) as follows: For each subsystem, after decryption using the WFST diagram (TLG) 1-best hypotheses 3 with reliability value are prepared for the following processes. Alignment and composition are applied to the hypotheses of different subsystems to select a single composite word Transition3We have the uppermost N (N) > 1 hypotheses tested for the following processes."}, {"heading": "4.3. Alignment", "text": "Time Normalization. With respect to each subsystem, after searching for the best word transition, we obtain a hypotheses sequence with each element that includes a label, a trust, a start time, and a duration that may not be on the same scale due to CTC decoding. Therefore, we need to unify the time and start / duration time on the same scale before constructing a WTN (WTN-BASE) construction. After time normalization, we can align the hypotheses sequences word by word and combine them into a single compound WTN. Specifically, one of the sequences is selected as the base WTN (WTN-BASE), and other sequences are added word by word to WTN-BASE. Comparing the word in the sequence and the associated word in WTN-BASE is the transition between WTN-BASE, we take different operations for different conditions. A transition point 1) A WASTE is created and the transition point is created."}, {"heading": "4.4. Voting", "text": "Once the composite WTN has been generated, a tuning module is used to select the best sequence of words by searching the WTN. According to ROVER, there are three tuning schemes, i.e., voting is done according to 1) frequency of occurrence, 2) frequency of occurrence and average verbal reliability, and 3) frequency of occurrence and maximum reliability. Generally, the third tuning scheme, i.e. frequency of occurrence and maximum reliability, usually yields the best results (Fiscus, 1997), which is therefore adopted in our system combination. In terms of selecting the reliability value, we use the minimum Bayes risk value (Xu et al., 2011) to ensure maximum reliability."}, {"heading": "5. Experiments", "text": "We analyze the performance of our proposed RCNN-CTC and CTC-based system combination using a benchmark dataset, the Wall Street Journal (WSJ), and a large mobile chat dataset, Tencent Chat, from Tencent. Tencent Chat's dataset contains approximately 2.3 million expressions representing 1400 hours of voice data."}, {"heading": "5.1. Experimental Setup", "text": "For the WSJ dataset, we use the standard si284 configuration for training, eval92 for validation, and dev93 for the test. Our input functions are 40-dimensional filter base functions with delta and delta delta configuration. Our input functions are normalized by medium subtraction and variance normalization on the speaker basics. For the Tencent chat dataset, we use about 1400 hours of internal speech data for training and independent 2000 expressions for the test. Our input functions are 40-dimensional filter functions combined with 3-dimensional pitch features and are normalized by mean and variance as there is no speaker information.We use the Kaldi recipe (Povey et al., 2011) to prepare the dictionary for WSJ and Tencent Chat datasets. In fact, it uses CMU dictionary and Sequitur GP to prepare phone sequences for both English and Chinese datasets."}, {"heading": "5.2. Results on WSJ data set", "text": "We compare our proposed single system RCNN-CTC with several commonly used neural network baseline systems in ASR, i.e. BLSTM (Sak et al., 2014), CLDNN (Sainath et al., 2015) and VGG (Simonyan & Zisserman, 2014).BLSTM is implemented according to (Miao et al., 2015), which uses 4 bi-directional LSTM layers and 2 fully connected layers. Kernel sizes of the three revolutionary layers are (11, 21), (11, 11), and strides are (3,2), (1,2), (1,1)."}, {"heading": "5.3. Results on Tencent Chat data set", "text": "Below we examine the performance of RCNNCTC and system combination on a large chat record J TC 0.77%. Here we show only the results trained with CTC loss to avoid lengthy label alignment work in CE. Baseline systems are the same as those in Section 5.2, but some network parameters are slightly adjusted. CLDNN uses the same network architecture, but the kernel sizes of the three Convolutionary Layers are (11, 11), (5, 5), (3, 3), and the advances are (3.1), (1.1), (1.1) respectively. As for RCNNCTC we re-use the parameters in Table 1, where the difference is N = 5. With respect to BLSTM and VGG, parameters of these systems are the same as in Section 5.2.Table 4 summarizes the WERs of individual systems on Tencent Chat dataset."}, {"heading": "5.4. Analysis and Discussion", "text": "As explained in Section 4, we choose the 1-best hypothesis for the combination because we find that N-best is no better than 1-best in our experiments, as shown in Table 6. Here, N-best (N = 10) different hypotheses of each subsystem provide no further benefit. Although N-best hypotheses cause the WTN to contain more branching and word choice, the maximum trust vote gets almost the same result as the 1-best hypothesis. The first two rows of Table 6 confirm the above conclusions. In addition, we perform another experiment using the frequency of occurrence as a vote result for N-best subsystem combination. We find that the results of the maximum trust vote are close to the 1-best based on the WSJ data, but slightly worse based on Tencet Chat data."}, {"heading": "6. Conclusions", "text": "We argued that CNN is capable of exploiting local correlations of human speech signals in both time and frequency dimensions and has the ability to exploit translational invariance in signals. In our proposed RCNN-CTC, we are using a broad and deep CNN architecture (more than 40 layers) with residual connections that has more expressiveness and better generalization capacities. RCNN-CTC can be trained end-to-end thanks to the introduction of CTC loss, effectively avoiding the lengthy process of frame alignment in ASR. In addition, we proposed a CTC-based system combination through subsystem selection, alignment and coordination procedures. Experiments on WSJ and Tencent Chat datasets show that among the widely used neural network systems in ASR, RCNN-CTC achieves the lowest WER-WER values, respectively, compared to the W52% proposed WTC-WTC combination values."}], "references": [{"title": "Convolutional neural networks for speech recognition", "author": ["Abdel-Hamid", "Ossama", "Mohamed", "Abdel-rahman", "Jiang", "Hui", "Deng", "Li", "Penn", "Gerald", "Yu", "Dong"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Abdel.Hamid et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Abdel.Hamid et al\\.", "year": 2014}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Amodei", "Dario", "Anubhai", "Rishita", "Battenberg", "Eric", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Chen", "Jingdong", "Chrzanowski", "Mike", "Coates", "Adam", "Diamos", "Greg"], "venue": "arXiv preprint arXiv:1512.02595,", "citeRegEx": "Amodei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2015}, {"title": "A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (rover)", "author": ["Fiscus", "Jonathan G"], "venue": "In Proceedings of 1997 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU", "citeRegEx": "Fiscus and G.,? \\Q1997\\E", "shortCiteRegEx": "Fiscus and G.", "year": 1997}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Graves", "Alex", "Fern\u00e1ndez", "Santiago", "Gomez", "Faustino J", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the 23th International Conference on Machine Learning (ICML", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Hannun", "Awni", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates", "Adam"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Residual lstm: Design of a deep recurrent architecture for distant speech recognition", "author": ["Kim", "Jaeyoung", "El-Khamy", "Mostafa", "Lee", "Jungwon"], "venue": "arXiv preprint arXiv:1701.03360,", "citeRegEx": "Kim et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Convolutional networks for images, speech, and time series", "author": ["LeCun", "Yann", "Bengio", "Yoshua"], "venue": "In The handbook of Brain Theory and Neural Networks,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Miao", "Yajie", "Gowayyed", "Mohammad", "Metze", "Florian"], "venue": "In Proceedings of 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU", "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tom\u00e1\u0161", "Karafi\u00e1t", "Martin", "Burget", "Luk\u00e1\u0161", "\u010cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In Proceedings of 11th Annual Conference of the International Speech Communication Association (Interspeech", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Analysis of cnn-based speech recognition system using raw speech as input", "author": ["Palaz", "Dimitri", "Magimai-Doss", "Mathew", "Collobert", "Ronan"], "venue": "In Proceedings of 16th Annual Conference of the International Speech Communication Association (Interspeech", "citeRegEx": "Palaz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Palaz et al\\.", "year": 2015}, {"title": "The kaldi speech recognition toolkit", "author": ["Povey", "Daniel", "Ghoshal", "Arnab", "Boulianne", "Gilles", "Burget", "Lukas", "Glembek", "Ondrej", "Goel", "Nagendra", "Hannemann", "Mirko", "Motlicek", "Petr", "Qian", "Yanmin", "Schwarz"], "venue": "In Proceedings of 2011 IEEE workshop on Automatic Speech", "citeRegEx": "Povey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Povey et al\\.", "year": 2011}, {"title": "Very deep convolutional neural networks for robust speech recognition", "author": ["Qian", "Yanmin", "Woodland", "Philip C"], "venue": "arXiv preprint arXiv:1610.00277,", "citeRegEx": "Qian et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Qian et al\\.", "year": 2016}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["Rabiner", "Lawrence R"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rabiner and R.,? \\Q1989\\E", "shortCiteRegEx": "Rabiner and R.", "year": 1989}, {"title": "Learning the speech frontend with raw waveform cldnns", "author": ["Sainath", "Tara N", "Weiss", "Ron J", "Senior", "Andrew", "Wilson", "Kevin W", "Vinyals", "Oriol"], "venue": "In Proceedings of 16th Annual Conference of the International Speech Communication Association (Interspeech 2015),", "citeRegEx": "Sainath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2015}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Sak", "Hasim", "Senior", "Andrew W", "Beaufays", "Fran\u00e7oise"], "venue": "In Proceedings of 15th Annual Conference of the International Speech Communication Association (Interspeech", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Advances in very deep convolutional neural networks for lvcsr", "author": ["Sercu", "Tom", "Goel", "Vaibhava"], "venue": "arXiv preprint arXiv:1604.01792,", "citeRegEx": "Sercu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sercu et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Minimum bayes risk decoding and system combination based on a recursion for edit distance", "author": ["Xu", "Haihua", "Povey", "Daniel", "Mangu", "Lidia", "Zhu", "Jie"], "venue": "Computer Speech & Language,", "citeRegEx": "Xu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2011}, {"title": "Automatic Speech Recognition: A Deep Learning Approach", "author": ["Yu", "Dong", "Deng", "Li"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Wide residual networks", "author": ["Zagoruyko", "Sergey", "Komodakis", "Nikos"], "venue": "arXiv preprint arXiv:1605.07146,", "citeRegEx": "Zagoruyko et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for end-to-end speech recognition", "author": ["Zhang", "Yu", "Chan", "William", "Jaitly", "Navdeep"], "venue": "arXiv preprint arXiv:1610.03022,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Deep recurrent convolutional neural network: Improving performance for speech recognition", "author": ["Zhang", "Zewang", "Sun", "Zheng", "Liu", "Jiaqi", "Chen", "Jingwen", "Huo", "Zhao", "Xiao"], "venue": "arXiv preprint arXiv:1611.07174,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "CTC technique has shown promising results in Deep Speech (Hannun et al., 2014; Amodei et al., 2015) and EESEN (Miao et al.", "startOffset": 57, "endOffset": 99}, {"referenceID": 1, "context": "CTC technique has shown promising results in Deep Speech (Hannun et al., 2014; Amodei et al., 2015) and EESEN (Miao et al.", "startOffset": 57, "endOffset": 99}, {"referenceID": 11, "context": ", 2015) and EESEN (Miao et al., 2015).", "startOffset": 18, "endOffset": 37}, {"referenceID": 2, "context": "To simplify this process, Graves et al. (2006) introduced CTC objective function to infer speech-label alignments automatically without any intermediate process, leading to an end-to-end system for ASR.", "startOffset": 26, "endOffset": 47}, {"referenceID": 6, "context": "2) Its network architecture can be very deep (more than 40 layers) to obtain more expressive power and better generalization capacity through residual connections between layers, as inspired by Residual Networks (ResNets) (He et al., 2016).", "startOffset": 222, "endOffset": 239}, {"referenceID": 18, "context": "The basic subsystems adopted in our combination are RCNN-CTC, Bidirectional Long Short Term Memory (BLSTM) (Sak et al., 2014) and Convolutional Long short term memory Deep Neural Network (CLDNN) (Sainath et al.", "startOffset": 107, "endOffset": 125}, {"referenceID": 17, "context": ", 2014) and Convolutional Long short term memory Deep Neural Network (CLDNN) (Sainath et al., 2015).", "startOffset": 77, "endOffset": 99}, {"referenceID": 12, "context": "In the last few years, Recurrent Neural Networks (RNNs) have been widely used for sequential modeling due to its capability of modeling long history (Mikolov et al., 2010).", "startOffset": 149, "endOffset": 171}, {"referenceID": 0, "context": "For example, Abdel-Hamid et al. (2014) used one convolutional layer, one pooling layer and a few fullyconnected layers.", "startOffset": 13, "endOffset": 39}, {"referenceID": 0, "context": "For example, Abdel-Hamid et al. (2014) used one convolutional layer, one pooling layer and a few fullyconnected layers. Amodei et al. (2015) used three convolutional layers as the feature preprocessing layers.", "startOffset": 13, "endOffset": 141}, {"referenceID": 0, "context": "For example, Abdel-Hamid et al. (2014) used one convolutional layer, one pooling layer and a few fullyconnected layers. Amodei et al. (2015) used three convolutional layers as the feature preprocessing layers. Palaz et al. (2015) showed that CNN-based speech recognition which uses raw speech as input can be more robust.", "startOffset": 13, "endOffset": 230}, {"referenceID": 6, "context": "Recently, ResNet (He et al., 2016) has been shown to achieve compelling convergence and high accuracy in computer vision, which attributes to its identity mapping as the skip connection in residual blocks.", "startOffset": 17, "endOffset": 34}, {"referenceID": 9, "context": "Residual LSTM architecture was proposed in (Kim et al., 2017).", "startOffset": 43, "endOffset": 61}, {"referenceID": 6, "context": "Recently, ResNet (He et al., 2016) has been shown to achieve compelling convergence and high accuracy in computer vision, which attributes to its identity mapping as the skip connection in residual blocks. Successful attempts along this line in ASR have also been reported very recently. Zhang et al. (2016a) proposed a deep convolutional network with batch normalization (BN), residual connections and convolutional LSTM structure.", "startOffset": 18, "endOffset": 309}, {"referenceID": 6, "context": "Residual Networks (ResNets) (He et al., 2016) have been proposed recently to ease the training of very deep CNNs.", "startOffset": 28, "endOffset": 45}, {"referenceID": 1, "context": "The standard formulation of BN for CNN can be readily applied here, and we do not need the sequence-wise normalization of RNN (Amodei et al., 2015).", "startOffset": 126, "endOffset": 147}, {"referenceID": 5, "context": "Following (Hannun et al., 2014; Amodei et al., 2015; Miao et al., 2015), we adopt the CTC objective (Graves et al.", "startOffset": 10, "endOffset": 71}, {"referenceID": 1, "context": "Following (Hannun et al., 2014; Amodei et al., 2015; Miao et al., 2015), we adopt the CTC objective (Graves et al.", "startOffset": 10, "endOffset": 71}, {"referenceID": 11, "context": "Following (Hannun et al., 2014; Amodei et al., 2015; Miao et al., 2015), we adopt the CTC objective (Graves et al.", "startOffset": 10, "endOffset": 71}, {"referenceID": 4, "context": ", 2015), we adopt the CTC objective (Graves et al., 2006) to automatically learn the alignments between speech frames and their label sequences, leading to an end-to-end training.", "startOffset": 36, "endOffset": 57}, {"referenceID": 4, "context": "To align the network outputs with the label sequences, an intermediate representation of CTC path is introduced in (Graves et al., 2006).", "startOffset": 115, "endOffset": 136}, {"referenceID": 18, "context": "Therefore, the following three subsystems2 are selected: 1) The proposed RCNN-CTC in Section 3; 2) BLSTM (Sak et al., 2014) which consists of several bidirectional LSTM layers; and 3) CLDNN (Sainath et al.", "startOffset": 105, "endOffset": 123}, {"referenceID": 17, "context": ", 2014) which consists of several bidirectional LSTM layers; and 3) CLDNN (Sainath et al., 2015), which consists of convolutional layers, LSTM layers and DNN layers.", "startOffset": 74, "endOffset": 96}, {"referenceID": 21, "context": "As for the choice of confidence score, we use the minimum Bayes risk score (Xu et al., 2011) to serve as maximum confidence.", "startOffset": 75, "endOffset": 92}, {"referenceID": 14, "context": "We use the Kaldi recipe (Povey et al., 2011) to prepare the dictionary for WSJ and Tencent Chat data sets.", "startOffset": 24, "endOffset": 44}, {"referenceID": 11, "context": "(Miao et al., 2015).", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "As for the language model, we apply the WSJ pruned trigram language model with expanded lexicon (Povey et al., 2011) in the ARPA format on WSJ data set.", "startOffset": 96, "endOffset": 116}, {"referenceID": 18, "context": ", BLSTM (Sak et al., 2014), CLDNN (Sainath et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 17, "context": ", 2014), CLDNN (Sainath et al., 2015) and VGG (Simonyan & Zisserman, 2014).", "startOffset": 15, "endOffset": 37}, {"referenceID": 11, "context": "BLSTM is implemented according to (Miao et al., 2015), which uses 4 bidirectional LSTM layers.", "startOffset": 34, "endOffset": 53}, {"referenceID": 1, "context": "CLDNN is implemented following (Amodei et al., 2015), which contains 3 convolutional layers, 3 bidirectional LSTM layers and 2 fully-connected layers.", "startOffset": 31, "endOffset": 52}], "year": 2017, "abstractText": "Deep learning approaches have been widely used in Automatic Speech Recognition (ASR) and they have achieved a significant accuracy improvement. Especially, Convolutional Neural Networks (CNNs) have been revisited in ASR recently. However, most CNNs used in existing work have less than 10 layers which may not be deep enough to capture all human speech signal information. In this paper, we propose a novel deep and wide CNN architecture denoted as RCNN-CTC, which has residual connections and Connectionist Temporal Classification (CTC) loss function. RCNN-CTC is an endto-end system which can exploit temporal and spectral structures of speech signals simultaneously. Furthermore, we introduce a CTC-based system combination, which is different from the conventional frame-wise senone-based one. The basic subsystems adopted in the combination are different types and thus mutually complementary to each other. Experimental results show that our proposed single system RCNN-CTC can achieve the lowest word error rate (WER) on WSJ and Tencent Chat data sets, compared to several widely used neural network systems in ASR. In addition, the proposed system combination can offer a further error reduction on these two data sets, resulting in relative WER reductions of 14.91% and 6.52% on WSJ dev93 and Tencent Chat data sets respectively. \u2217 Equal contribution.", "creator": "LaTeX with hyperref package"}}}