{"id": "1702.07826", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2017", "title": "Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations", "abstract": "We introduce AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had done the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of the autonomous agent into natural language. We evaluate our technique in the Frogger game environment. The natural language is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation, show the results of experiments on the accuracy of our rationalization technique, and describe future research agenda.", "histories": [["v1", "Sat, 25 Feb 2017 03:20:49 GMT  (183kb,D)", "http://arxiv.org/abs/1702.07826v1", "9 pages, 4 figures"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.HC cs.LG", "authors": ["brent harrison", "upol ehsan", "mark o riedl"], "accepted": false, "id": "1702.07826"}, "pdf": {"name": "1702.07826.pdf", "metadata": {"source": "META", "title": "Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations", "authors": ["Brent Harrison", "Upol Ehsan", "Mark O. Riedl"], "emails": ["<brent.harrison@cc.gatech.edu>."], "sections": [{"heading": "1. Introduction", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "2. Related Work", "text": "To be able to interpret, it must be possible to explain why it produces certain results or behaves in a particular way. Inherently, some machine learning techniques produce models that are more interpretable than others. This is because one only needs to examine the output to understand the model's decision-making process (in the case of rule-based approaches such as decision trees) or examine some internal features that show which areas of the model have the greatest impact on prediction (in the case of attention and regression-based approaches). While these techniques are effective and rely exclusively on interpretative models, researchers impede the types of models they can use for learning."}, {"heading": "3. AI Rationalization", "text": "Rationalization is a form of explanation that attempts to justify or explain an action or behavior. While explanation implies an accurate representation of a decision-making process, rationalization suggests that the explanation is the one that a human being would most likely give if he or she had full control over an agent or robot. There is no clear guidance on what is a good explanation; for an agent using Q-Learning (Watkins & Dayan, 1992), explanations of decisions could range from \"the action had the highest Q value given to that state\" to \"I have studied numerous possible future state paths and considered this action the most likely to achieve the highest expected reward according to the iterative application of the Bellman actualization equation.\" The closer the explanation of the actual implementation of decision-making or machine learning algorithm is, the more background knowledge the human operator will need to find the explanation."}, {"heading": "3.1. Training Corpus", "text": "In order to learn the translation of state-action pairs into language, we first need to create a corpus of natural language descriptions of behavior and the corresponding state and action information. To do this, we ask people to perform the agent's task in a virtual environment and are asked to \"think out loud\" when performing the task. We record the states visited and actions performed along with natural language expressions of critical states and actions. This method of body building ensures that the collected comments are associated with certain states and actions. Essentially, we create parallel corpora, one of which contains state representations and actions, the other natural language expressions. State representation and actions form a \"machine language.\" The general approach is agnostic in terms of the details of how the parallel corpora is obtained. We describe the specific methodology we have used to create the parallel corpora specific role for the purpose of experimenting in Section 1. To create an autonomous representation of actions in the environment, as long as it cannot be a specific representation of actions in the environment."}, {"heading": "3.2. Translation from Internal Representation to Natural Language", "text": "In this work, we use encoder decoder networks to translate between complex state and action information and rationalizations of natural language. Encoder decoder networks, which are mainly used in machine translation and dialog systems, are a generative architecture consisting of two component networks that learn how to translate an input sequence X = (x1,..., xT) into an output sequence Y = (y1,..., y \u2032 T. The first component network, the encoder, is a recursive neural network (RNN) that learns to encode the input vector X into a fixed-length context vector. This vector is then used as an input into the second component network, the decoder decoder is an RNN that learns to decode this vector iteratively into the target output Y. Since the input data we work with in this essay may be quite long, it is possible that the context loses."}, {"heading": "4. Experiments", "text": "The aim of the experiments we describe below is to assess the accuracy of the neural machine translation approach to rationalization; evaluating the generation of natural language is a challenge; utterances can be \"correct,\" even if they do not exactly match known utterances from a test dataset. To facilitate the assessment of the rationalizations generated by our technique, we have developed a technique in which semi-synthetic natural language has been paired with state representations assigned to an autonomous system; semi-synthetic language has been produced by \"thinking aloud\" as we perform a task and then creating a grammar that reproduces and generalizes the utterances (described below); our evaluation method allows us to identify the rules that fire during autonomous system execution and compare rationalizations generated by the neural machine translation network to a soil truth. Later, we can replace the grammar with a fully trained in a cornucopation task."}, {"heading": "4.1. Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1. GRAMMAR CREATION", "text": "To generate this information, we used crowdsourcing to collect a series of gameplay videos of human participants engaging in a thought-out protocol that follows the work of (Dorst & Cross, 2001; Fonteyn et al., 1993). In total, we collected recordings of 12 participants from 3 continents (4 from Asia, 3 from Europe, and 5 from North America) who were normally recruited online. In these recordings, we asked participants to verbalize their internal thoughts while talking to another person as if they were playing one level of the game, and each person played the same game level as the other participants. After the players completed the game, video evidence was uploaded to an online language transcription service."}, {"heading": "4.1.2. TRAINING AND TEST SET GENERATION", "text": "Since we produce a grammar that makes rationalizations of the earth, the role of the encoder network can be interpreted as a learning process to reproduce this grammar. To get the network to do so, we use grammar to generate rationalizations for each state in the environment. Rules that grammar uses to generate rationalizations are based on a combination of the state of the world and the measures taken. Specifically, grammar uses the following three rationalizations to determine which rationalizations to generate: (S1, S2). S1 is the initial state of the world, an action performed in S1, and S2 is the resulting state of execution. States S1 and S2 consist of the coordinates of the agent and the current environment."}, {"heading": "4.1.3. TRAINING AND TESTING THE NETWORK", "text": "The parallel corpus of state actions and natural language is used to train an encoder decoder neural algorithm based on (Luong et al., 2015). Specifically, we use a two-layer encoder decoder network with an embedding layer and an attention mechanism. The encoder and decoder networks are long-term short-term memory (LSTM), in which each LSTM node has a hidden size of 300. We train the network for 50 epochs, according to which the trained model generates rationalizations for each triple in the test phase. To further test the generalization of this technique, we produce rationalizations for each triple in the test phase."}, {"heading": "4.2. Results", "text": "A screenshot of an agent trained to play Frogger and \"think out loud\" after each action with our system is shown in Figure 4. The results of our experiments can be found in Table 1. As you can see in the table, the encoder decoder network was able to consistently outperform both the random base and the majority baseline models. To evaluate the significance of the observed differences between these models, we conducted a chisquared test between the models generated by the encoder decoder network and the random predictor, as well as between the encoder decoder network models and the majority classifier. Each difference was deemed statistically significant (p < 0.05) across all three maps. To test how well this technique is generalized to other types of maps, we performed another series of experiments using specific sets to predict 124g predicted results."}, {"heading": "4.3. Discussion", "text": "The first thing we have to say about our results is that the models produced by the encoder decoder network significantly outperformed the base models in terms of accuracy per percentage, which means that this network was able to learn better when it was appropriate to generate certain rationalizations compared to the random and majority base models. Considering the way in which our tests were conducted, there is evidence to suggest that these models can also generalize invisible states. It is important to note that the claims to the possibility of these cards extend only to states with similar obstacles."}, {"heading": "5. Future Work", "text": "Once we have established that neural machine translation can produce precise rationalizations for state-action pairs, the next step is to examine hypotheses about how rationalizations affect human trust, rapport, and willingness to use autonomous and semi-autonomous systems. To answer these questions, it is necessary to conduct experiments in which non-experienced human operators work with autonomous systems to perform a specific task. Questionnaires can be developed to assess the subjective feeling of trust and rapport in human operators. Trust and rapport are treated as dependent variables, and studies can be designed between subjects so that different populations work with autonomous systems with and without rationalization capabilities. One can consider the effects of \"thinking aloud\" during execution and rationalization when something unexpected occurs. Furthermore, one can develop interventions to ensure that an error occurs or that the agent is trained to describe the rationalization function more accurately before we can more accurately describe the expectations of the reward function."}, {"heading": "6. Conclusions", "text": "AI rationalization provides a new lens through which we can examine the question of what is a good explanation for the decisions of an autonomous or semi-autonomous agent. In the future, we imagine the increasing need for human operators with autonomous or semi-autonomous backgrounds who do not have computer science or AI knowledge. Of course, these operators will want to know why an agent does not achieve a goal or why his behavior deviates from expectations. Furthermore, operators need to model the reasoning behind agent decisions quickly and effortlessly in real-time scenarios. Since AI rationalization is the process of generating explanations as if a human had done the behavior, we assume that rationalization will increase the confidence, relationship and comfort of human operators using autonomous and semi-autonomous systems. We have shown that our approach to AI rationalization, by translating neural machine translations from the internal state and representations of autonomous agents into natural rationalizations, can translate rationalizations into full language."}], "references": [{"title": "Tracery: Approachable story grammar authoring for casual users", "author": ["Compton", "Kate", "Filstrup", "Benjamin"], "venue": "In Seventh Intelligent Narrative Technologies Workshop,", "citeRegEx": "Compton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Compton et al\\.", "year": 2014}, {"title": "Building Explainable Artificial Intelligence Systems", "author": ["Core", "Mark", "Lane", "H. Chad", "van Lent", "Michael", "Gomboc", "Dave", "Solomon", "Steve", "Rosenberg", "Milton"], "venue": "In Proceedings of the 18th Innovative Applications of Artificial Intelligence Conference,", "citeRegEx": "Core et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Core et al\\.", "year": 2006}, {"title": "Creativity in the design process: co-evolution of problem\u2013solution", "author": ["Dorst", "Kees", "Cross", "Nigel"], "venue": "Design studies,", "citeRegEx": "Dorst et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Dorst et al\\.", "year": 2001}, {"title": "A description of think aloud method and protocol analysis", "author": ["Fonteyn", "Marsha E", "Kuipers", "Benjamin", "Grobe", "Susan J"], "venue": "Qualitative Health Research,", "citeRegEx": "Fonteyn et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Fonteyn et al\\.", "year": 1993}, {"title": "Interacting with predictions: Visual inspection of black-box machine learning models", "author": ["Krause", "Josua", "Perer", "Adam", "Ng", "Kenney"], "venue": "In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems,", "citeRegEx": "Krause et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2016}, {"title": "Human-level ais killer application: Interactive computer", "author": ["Laird", "John", "van Lent", "Michael"], "venue": "games. AI Magazine,", "citeRegEx": "Laird et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Laird et al\\.", "year": 2001}, {"title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model", "author": ["Letham", "Benjamin", "Rudin", "Cynthia", "McCormick", "Tyler H", "Madigan", "David"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Letham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Letham et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong", "Minh-Thang", "Pham", "Hieu", "Manning", "Christopher D"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": null, "citeRegEx": "Wierstra et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wierstra et al\\.", "year": 2015}, {"title": "Why should i trust you?: Explaining the predictions of any classifier", "author": ["Ribeiro", "Marco Tulio", "Singh", "Sameer", "Guestrin", "Carlos"], "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Ribeiro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2016}, {"title": "Grounded theory methodology", "author": ["Strauss", "Anselm", "Corbin", "Juliet"], "venue": "Handbook of qualitative research,", "citeRegEx": "Strauss et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Strauss et al\\.", "year": 1994}, {"title": "An explainable artificial intelligence system for small-unit tactical behavior", "author": ["van Lent", "Michael", "Fisher", "William", "Mancuso"], "venue": "In Proceedings of the 16th conference on Innovative Applications of Artifical Intelligence,", "citeRegEx": "Lent et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lent et al\\.", "year": 2004}, {"title": "What grounded theory is a critically reflective conversation among scholars", "author": ["Walsh", "Isabelle", "Holton", "Judith A", "Bailyn", "Lotte", "Fernandez", "Walter", "Levina", "Natalia", "Glaser", "Barney"], "venue": "Organizational Research Methods,", "citeRegEx": "Walsh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Walsh et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Bengio", "Yoshua"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Bengio and Yoshua.,? \\Q2015\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2015}, {"title": "Understanding neural networks through deep visualization", "author": ["Yosinski", "Jason", "Clune", "Jeff", "Fuchs", "Thomas", "Lipson", "Hod"], "venue": "ICML Workshop on Deep Learning. Citeseer,", "citeRegEx": "Yosinski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In European conference on computer vision,", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "For example, decision trees are considered to be relatively interpretable and neural networks are generally considered to be uninterpretable without additional processes to visualize patterns of neuron activation (Zeiler & Fergus, 2014; Yosinski et al., 2015).", "startOffset": 213, "endOffset": 259}, {"referenceID": 7, "context": ", (Luong et al., 2015)) has", "startOffset": 2, "endOffset": 22}, {"referenceID": 6, "context": "Models such as decision trees (Letham et al., 2015), generalized additive models (Caruana et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 9, "context": "These approaches, sometimes called model-agnostic (Ribeiro et al., 2016) approaches, allow greater flexibility in model selection since they enable black-box models to become interpretable; however, the added layer of complexity incurred by creat-", "startOffset": 50, "endOffset": 72}, {"referenceID": 14, "context": "proach to creating model-agnostic is to visualize the internal state of the model, as has been done with convolutional neural networks (Zeiler & Fergus, 2014; Yosinski et al., 2015).", "startOffset": 135, "endOffset": 181}, {"referenceID": 4, "context": "Other approaches seek to learn a naturally interpretable model which describes predictions that were made (Krause et al., 2016) or by intelligently modifying model inputs so that resulting models can describe how outputs are affected.", "startOffset": 106, "endOffset": 127}, {"referenceID": 1, "context": ", 2004), intelligent tutoring systems (Core et al., 2006), and transforming AI plans into natural language (van Lent et al.", "startOffset": 38, "endOffset": 57}, {"referenceID": 7, "context": "attention mechanism (Luong et al., 2015).", "startOffset": 20, "endOffset": 40}, {"referenceID": 3, "context": "To generate this information, we used crowdsourcing to gather a set of gameplay videos of human participants playing Froggerwhile engaging in a think-aloud protocol, following the work by (Dorst & Cross, 2001; Fonteyn et al., 1993).", "startOffset": 188, "endOffset": 231}, {"referenceID": 12, "context": "Our technique was inspired by Grounded Theory (Strauss & Corbin, 1994), which provides an analytical tool and facilitates discovery of emerging patterns in data (Walsh et al., 2015).", "startOffset": 161, "endOffset": 181}, {"referenceID": 0, "context": "Once clustered, we used Tracery (Compton et al., 2014), an authoring tool based on a standard context-free grammar (CFG), to build the grammar which we used to generate ground truth rationalizations.", "startOffset": 32, "endOffset": 54}, {"referenceID": 7, "context": "The parallel corpus of state-action representations and natural language are used to train an encoder-decoder neural translation algorithm based on (Luong et al., 2015).", "startOffset": 148, "endOffset": 168}], "year": 2017, "abstractText": "We introduce AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had done the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of the autonomous agent into natural language. We evaluate our technique in the Frogger game environment. The natural language is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation, show the results of experiments on the accuracy of our rationalization technique, and describe future research agenda.", "creator": "LaTeX with hyperref package"}}}