{"id": "1702.07904", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2017", "title": "Coarse Grained Exponential Variational Autoencoders", "abstract": "Variational autoencoders (VAE) often use Gaussian or category distribution to model the inference process. This puts a limit on variational learning because this simplified assumption does not match the true posterior distribution, which is usually much more sophisticated. To break this limitation and apply arbitrary parametric distribution during inference, this paper derives a \\emph{semi-continuous} latent representation, which approximates a continuous density up to a prescribed precision, and is much easier to analyze than its continuous counterpart because it is fundamentally discrete. We showcase the proposition by applying polynomial exponential family distributions as the posterior, which are universal probability density function generators. Our experimental results show consistent improvements over commonly used VAE models.", "histories": [["v1", "Sat, 25 Feb 2017 15:08:53 GMT  (3140kb,D)", "http://arxiv.org/abs/1702.07904v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ke sun", "xiangliang zhang"], "accepted": false, "id": "1702.07904"}, "pdf": {"name": "1702.07904.pdf", "metadata": {"source": "CRF", "title": "Coarse Grained Exponential Variational Autoencoders", "authors": ["Ke Sun", "Xiangliang Zhang"], "emails": ["sunk@ieee.org", "xiangliang.zhang@kaust.edu.sa"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of them are able to survive on their own, and that they see themselves as able to survive on their own. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "2 Prerequisites: Variational Autoencoders", "text": "x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,"}, {"heading": "3 CG-BPEF-VAE", "text": "We would like to expand the UAE to include a general inference process in which the model itself can learn a suitable p (z | x) within a flexible family of distributions that is not limited to Gaussian or category distributions and can capture moments of higher order of the posterior plane. In this section we will therefore derive a variant of the UAE called CG-BPEF-UAE for coarse-grained Polynomial Bounded Polynomial Exponential Family VAE."}, {"heading": "3.1 Bounded Polynomial Exponential Family", "text": "We try to model the latent z with a factorial polynomial exponential family (PEF) (Cobb et al., 1983; Nielsen & Nock, 2016) probability density function: p (z) = d-j = 1 exp (M-m = 1 cjmz m-j (cj)), (2) where M is the polynomial order, C = (cjm) d-M denotes any number of arbitrary p (z) satisfactory weak regularity conditions (Cobb et al., 1983). Moreover, we limit z to having limited support, so that z-1, 1] d, is a hypercube. This results in a focused density that does not wait for cases where the real distribution (Gyj-1) is unlikely."}, {"heading": "3.2 Coarse Grain", "text": "Our basic idea is to restrict the BPEF pdf into a discrete distribution (with a probability of only 1 percent), then samples based on the pmf = 1, then we reconstruct the continuous distribution (with a probability of 1 percent)."}, {"heading": "3.3 The Model", "text": "Based on the previous subsections, we assume that the following generation is processual: Jr = M = 1 = 1 = 1 = 1 = 1 = 1 (1 = 1 = 1), where A = (ajm) d \u00b7 M are the parameters of priority 1 and f is defined by a neural network. Always choose: M < R \u2212 1, because the polynomial equilibrium R \u2212 1 m = 1 cjm3 m with R \u2212 1 free parameters already has any distribution in R \u2212 1. Setting M \u2265 R \u2212 1 makes the polynomial structure redundant, and the corresponding inference process is given by inference T: 1 m = 1 bjm = 1 bjm = 1 bjm = 1 impulsive distribution."}, {"heading": "3.4 Information Mononicity", "text": "This is the most complex part, because these pdfs are not in closed form. However, we know that since T \u2192 0 + they are evenly distributed to category distributions over R, the same positions over R are evenly distributed (1). Therefore, we both have different terms about the KL distributions, that is, Term1. \"r = 1\" exp. \"(exp)\" exp. \"(exp) exp.\" exp. \"exp.\" (exp) exp. \"exp.\" (exp) exp. \"exp.\" exp. \"exp.\" exp. \"exp.\" (exp) exp. \"exp.\" (exp) exp. \"exp.\" (exp) exp. \"(exp.\" exp. \"exp.\" (exp) exp. \"exp.\" exp. \"(exp) exp. (exp.\" exp. \"exp.\" exp. \"exp.\" (exp) exp. (exp. \"exp.\" exp. \"exp.\" (exp) exp. (exp. \"exp.\" exp. \"exp.\" (exp) exp. (exp. \"exp.\" (exp. \") exp. (exp.\" exp. (exp. \") exp.\" (exp. (exp. \") exp.\" exp. \"(exp.\" (exp. \") exp.\" (exp. \"exp.\" exp. \"(exp.) exp. (exp.\" (exp.) exp."}, {"heading": "4 Experimental Results", "text": "We have the proposed method with TensorFlow (Abadi, Mart\u00ed \u0301 n et al., 2015) and tetn it on two different datasets. (D) It is the best way to ask if and how it is about the question if and how it is about the future. (D) It is the best way how it is about the future. (D) It is the best way how it is about the future. (D) It is the best way how it is about the future. (D) It is the best way how it is about the future. (D) It is the best way how it is about the future. (D) It is the best way it is about the future. (D) It is the best way it is about the future. (D) We are only investigating the unmonitored density. (D) It is nevertheless useful to have the selected datasets for the future. (D)"}, {"heading": "5 Information Geometry of VAE", "text": "This is a relatively separate section. We present a geometric theory that may be useful to reveal the intrinsics of the general UAE model (i.e. not limited to the proposed CG-BPEF-UAE). We also use this geometry to discuss the advantages of the proposed CG-BPEF-VAE.Notice, where this geometry is based not only on the input space or the latent space of x and z, but also on the models (space of x and z) that discuss the advantages of the proposed CG-BPEF-VAE.Notice, this geometry not on the input space (space of x and z), but on the models (space of x and z). We consider the cost function L (progression) in relation to i.i.d. observations {xk} nk = 1. term1 is the average KL divergence between q (z | xk) and z (p)."}, {"heading": "6 Concluding Remarks", "text": "Within the scope of the variable auto-encoding method (Kingma & Welling, 2014), a new method CG-BPEF-UAE was proposed in this thesis. Among numerous variations of UAE (Burda et al., 2016; Jang et al., 2017), CG-BPEF-UAE is represented by the use of a universal BPEF density generator in the inference model, which provides a principal way to simulate continuous densities using discrete latent variables. For example, to apply further complex distribution to the latent variance, our CG technique can be used to apply the repair parameterization trick. This study touches on a fundamental problem in uncontrolled learning: how to build a discrete latent structure to factor information in a continuous representation? We provide preliminary results on uncontrolled density estimates showing performance improvements over the original VAE and category AE."}, {"heading": "Acknowledgments", "text": "This research is funded by the King Abdullah University of Science and Technology. Experiments will be carried out at the Manda cluster of the Computational Bioscience Research Center at KAUST."}, {"heading": "A Proof of Theorem 1", "text": "Proof. We first prove (1) that the figure z = yr = y = y = y (1) and then (2) y (1) y (2) y (2) y (2) y (1) y (2) y (1) y (1) y (1) i (1) i = 1) i (1) i (1) i (1) i (1) i (1) i (1) i (1) i (1) i (1) i (1) i (1) i (1) i (1) i (1) i (1) i (1) i (1) i (1) i (1) i) i (1) i (1) i) i (1) i (1) i (1) i (1) i) (1) i (1) (1) i (1) (1) (1) i) (1) (1) (1) (1) i) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1 (1) (1) (1 (1) (1) (1 (1) (1 (1) (1 (1) (1 (1) (1) (1 (1) (1 (1) (1 (1) (1 (1) (1 (1) (1 (1) (1 (1) (1 (1) (1) (1 (1) (1 (1) (1 (1) (1 (1) (1 (1) (1 (1) (1 (1) (1 (1) (1) (1 (1) (1) (1 (1) (1) (1 (1) (1) (1 (1) (1) (1 (1) (1) (1 (1) (1) (1 (1) (1)"}, {"heading": "B Proof of Theorem 2", "text": "Lemma 1. Let p (z) = exp (t (z) \u2212 f (k) \u2212 f (k) = k (k) is a distribution (k) in an exponential family, then we have I (t) \u2212 f (n) t (z) dz \u2212 f (z) = p (z).z (z) and p (z) are in the same exponential family, we havep (z) = exp (z) \u2212 f (z), q (z) \u2212 f (z). z (z) z z (k) z z z (z) z (k) z (k)."}, {"heading": "C The Effect of the Dimensionality Reduction Layer of CG-BPEFVAE", "text": "Fig. 5 shows the KL (p (z): uniform) (KL (z)) and KL (\u03b1: uniform) (KL (category)) when \u03b1 is generated by Dirichlet distributions with different configurations. In all cases, KL (\u03b1: uniform) is delimited from KL (p (z): uniform) at low temperature. D Visualization of the concrete distribution Fig. 6, Fig. 7 and Fig. 8 show concrete densities generated by random samples. For each experiment (partial illustration) we generate 106 concrete samples and record the resulting density. There are very dense regions near the corner (the red region) that are cut off so that the visualization is clearer. An interesting observation is that the density \"seeps\" to the simple surfaces when T is low, although the density in this case is concentrated on the corners."}, {"heading": "E Details of the Convolutional Layers", "text": "The encoder is specified by \u2022 Input: 1 x 32 x 32 x 32 (RGB is averaged in 1 channel) \u2022 Volume layer: 32 (5 x 5) filters, with ReLU activation and without padding (\u2192 32 x 28 x 28) \u2022 Pooling layer: 2 x 2 filters with increments of 2 and without padding zeros (\u2192 32 x 14 x 14) \u2022 Volume layer: 64 (5 x 5) filters, with ReLU activation and without padding (\u2192 64 x 10 x 10) \u2022 Pooling layer: 2 x 2 filters with increments of 2 and without padding (\u2192 64 x 5 x 5) \u2022 Volume layer: 128 (5 x 5) filters with RELU activation and without padding (\u2192 128 x 1 x 1 x 1 x 1 x 1) The decoder is specified by \u2022 A dense linear layer with RELU activation to transform the dimension to 8 (5 x 5) with RELU activation."}], "references": [{"title": "TensorFlow: Large-scale machine learning", "author": ["Abadi", "Mart\u0131\u0301n"], "venue": "on heterogeneous systems,", "citeRegEx": "Abadi and Mart\u0131\u0301n,? \\Q2015\\E", "shortCiteRegEx": "Abadi and Mart\u0131\u0301n", "year": 2015}, {"title": "Information Geometry and its Applications, volume 194 of Applied Mathematical Sciences", "author": ["Amari", "Shun-ichi"], "venue": null, "citeRegEx": "Amari and Shun.ichi.,? \\Q2016\\E", "shortCiteRegEx": "Amari and Shun.ichi.", "year": 2016}, {"title": "Importance weighted autoencoders", "author": ["Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan"], "venue": "In ICLR,", "citeRegEx": "Burda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2016}, {"title": "Estimation and moment recursion relations for multimodal distributions of the exponential family", "author": ["Cobb", "Loren", "Koppstein", "Peter", "Chen", "Neng Hsin"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Cobb et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Cobb et al\\.", "year": 1983}, {"title": "Mathematical Methods of Statistics, volume 9 of Princeton Mathematical Series", "author": ["Cram\u00e9r", "Harald"], "venue": null, "citeRegEx": "Cram\u00e9r and Harald.,? \\Q1946\\E", "shortCiteRegEx": "Cram\u00e9r and Harald.", "year": 1946}, {"title": "Extreme Value Theory: An Introduction. Springer Series in Operations Research and Financial Engineering", "author": ["de Haan", "Laurens", "Ferreira", "Ana"], "venue": null, "citeRegEx": "Haan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Haan et al\\.", "year": 2006}, {"title": "Deep unsupervised clustering with Gaussian mixture variational autoencoders", "author": ["Dilokthanakul", "Nat", "Mediano", "Pedro A.M", "Garnelo", "Marta", "Lee", "Matthew C.H", "Salimbeni", "Hugh", "Arulkumaran", "Kai", "Shanahan", "Murray"], "venue": "In ICLR,", "citeRegEx": "Dilokthanakul et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Dilokthanakul et al\\.", "year": 2017}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In AISTATS; JMLR W&CP", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo Jimenez", "Wierstra", "Daan"], "venue": "In ICML; JMLR W & CP", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Statistical theory of extreme values and some practical applications: a series of lectures", "author": ["Gumbel", "Emil Julius"], "venue": "Applied mathematics series. U. S. Govt. Print. Office,", "citeRegEx": "Gumbel and Julius.,? \\Q1954\\E", "shortCiteRegEx": "Gumbel and Julius.", "year": 1954}, {"title": "Autoencoders, minimum description length and Helmholtz free energy", "author": ["Hinton", "Geoffrey E", "Zemel", "Richard S"], "venue": "In NIPS", "citeRegEx": "Hinton et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1994}, {"title": "Categorical reparameterization with Gumbel-softmax", "author": ["Jang", "Eric", "Gu", "Shixiang", "Poole", "Ben"], "venue": "In ICLR,", "citeRegEx": "Jang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jang et al\\.", "year": 2017}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Jimenez Rezende", "Danilo", "Welling", "Max"], "venue": "In NIPS", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Optimum follow the leader algorithm", "author": ["Kuzmin", "Dima", "Warmuth", "Manfred K"], "venue": "In COLT, pp", "citeRegEx": "Kuzmin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kuzmin et al\\.", "year": 2005}, {"title": "The MNIST database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher J.C"], "venue": "In NIPS", "citeRegEx": "LeCun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2014}, {"title": "The Concrete distribution: A continuous relaxation of discrete random variables", "author": ["Maddison", "Chris J", "Mnih", "Andriy", "Teh", "Yee Whye"], "venue": "CoRR, abs/1611.00712,", "citeRegEx": "Maddison et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2016}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In ICML, pp", "citeRegEx": "Nair and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Sided and symmetrized Bregman centroids", "author": ["Nielsen", "Frank", "Nock", "Richard"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Nielsen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nielsen et al\\.", "year": 2009}, {"title": "Patch matching with polynomial exponential families and projective divergences", "author": ["Nielsen", "Frank", "Nock", "Richard"], "venue": "In International Conference on Similarity Search and Applications (SISAP),", "citeRegEx": "Nielsen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nielsen et al\\.", "year": 2016}, {"title": "Guaranteed bounds on information-theoretic measures of univariate mixtures using piecewise log-sum-exp", "author": ["Nielsen", "Frank", "Sun", "Ke"], "venue": "inequalities. Entropy,", "citeRegEx": "Nielsen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nielsen et al\\.", "year": 2016}, {"title": "Markov chain Monte Carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diederik", "Welling", "Max"], "venue": "In ICML; JMLR W&CP", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Multi-modal variational encoder-decoders", "author": ["Serban", "II Iulian Vlad", "Alexander G. Ororbia", "Pineau", "Joelle", "Courville", "Aaron C"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Stochastic video prediction with conditional density estimation", "author": ["Shu", "Rui"], "venue": "In ECCV Workshop on Action and Anticipation for Visual Learning,", "citeRegEx": "Shu and Rui,? \\Q2016\\E", "shortCiteRegEx": "Shu and Rui", "year": 2016}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["Sohn", "Kihyuk", "Lee", "Honglak", "Yan", "Xinchen"], "venue": "In NIPS", "citeRegEx": "Sohn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 2015}, {"title": "Information geometry and minimum description length networks", "author": ["Sun", "Ke", "Wang", "Jun", "Kalousis", "Alexandros", "Marchand-Maillet", "St\u00e9phane"], "venue": "In ICML; JMLR W&CP", "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "An uncertain future: Forecasting from static images using variational autoencoders", "author": ["Walker", "Jacob", "Doersch", "Carl", "Gupta", "Abhinav", "Hebert", "Martial"], "venue": "In ECCV; LNCS 9911,", "citeRegEx": "Walker et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 27, "context": "1 Introduction Variational autoencoders (Kingma & Welling, 2014) and its variants (Rezende et al., 2014; Sohn et al., 2015; Salimans et al., 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al.", "startOffset": 82, "endOffset": 187}, {"referenceID": 24, "context": "1 Introduction Variational autoencoders (Kingma & Welling, 2014) and its variants (Rezende et al., 2014; Sohn et al., 2015; Salimans et al., 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al.", "startOffset": 82, "endOffset": 187}, {"referenceID": 2, "context": "1 Introduction Variational autoencoders (Kingma & Welling, 2014) and its variants (Rezende et al., 2014; Sohn et al., 2015; Salimans et al., 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al.", "startOffset": 82, "endOffset": 187}, {"referenceID": 25, "context": "1 Introduction Variational autoencoders (Kingma & Welling, 2014) and its variants (Rezende et al., 2014; Sohn et al., 2015; Salimans et al., 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al.", "startOffset": 82, "endOffset": 187}, {"referenceID": 12, "context": ", 2016) combine the two powers of variational Bayesian learning (Jordan et al., 1999) with strong generalization and a standard learning objective, and deep learning with flexible and scalable representations.", "startOffset": 64, "endOffset": 85}, {"referenceID": 13, "context": "They are attracting decent attentions, producing state-of-the-art performance in semi-supervised learning (Kingma et al., 2014) and image generation (Gregor et al.", "startOffset": 106, "endOffset": 127}, {"referenceID": 8, "context": ", 2014) and image generation (Gregor et al., 2015), and are getting applied in diverse areas such as deep generative modeling (Rezende et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 27, "context": ", 2014), image segmentation (Sohn et al., 2015), clustering (Dilokthanakul et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 6, "context": ", 2015), clustering (Dilokthanakul et al., 2017), and future prediction from images (Walker et al.", "startOffset": 20, "endOffset": 48}, {"referenceID": 29, "context": ", 2017), and future prediction from images (Walker et al., 2016).", "startOffset": 43, "endOffset": 64}, {"referenceID": 2, "context": ", 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al., 1999) with strong generalization and a standard learning objective, and deep learning with flexible and scalable representations. They are attracting decent attentions, producing state-of-the-art performance in semi-supervised learning (Kingma et al., 2014) and image generation (Gregor et al., 2015), and are getting applied in diverse areas such as deep generative modeling (Rezende et al., 2014), image segmentation (Sohn et al., 2015), clustering (Dilokthanakul et al., 2017), and future prediction from images (Walker et al., 2016). This paper discusses unsupervised learning with VAE which pipes an inference model q(z |x) with a generative model p(x | z), where x and z are observed and latent variables, respectively. A simple parameter-free prior p(z) combined with p(x | z) parameterized by a deep neural network results in arbitrarily flexible representations. However, its (very complex) posterior p(z |x) must be within the representation power of the inference machine q(z |x), so that the variational bound is tight and variational learning is effective. In the original VAE (Kingma & Welling, 2014), q(z |x) obeys a Gaussian distribution with a diagonal covariance matrix. This is a very simplified assumption, because Gaussian is the maximum entropy (least informative) distribution with respect to prescribed mean and variance and has one single mode, while human inference can be ambiguous and can have a bounded support when we exclude very unlikely cases (de Haan & Ferreira, 2006). Many recent works try to tackle this limitation. Jang et al. (2017) extended VAE to effectively use a discrete latent z following a category distribution (e.", "startOffset": 8, "endOffset": 1693}, {"referenceID": 2, "context": ", 2015; Burda et al., 2016; Serban et al., 2016) combine the two powers of variational Bayesian learning (Jordan et al., 1999) with strong generalization and a standard learning objective, and deep learning with flexible and scalable representations. They are attracting decent attentions, producing state-of-the-art performance in semi-supervised learning (Kingma et al., 2014) and image generation (Gregor et al., 2015), and are getting applied in diverse areas such as deep generative modeling (Rezende et al., 2014), image segmentation (Sohn et al., 2015), clustering (Dilokthanakul et al., 2017), and future prediction from images (Walker et al., 2016). This paper discusses unsupervised learning with VAE which pipes an inference model q(z |x) with a generative model p(x | z), where x and z are observed and latent variables, respectively. A simple parameter-free prior p(z) combined with p(x | z) parameterized by a deep neural network results in arbitrarily flexible representations. However, its (very complex) posterior p(z |x) must be within the representation power of the inference machine q(z |x), so that the variational bound is tight and variational learning is effective. In the original VAE (Kingma & Welling, 2014), q(z |x) obeys a Gaussian distribution with a diagonal covariance matrix. This is a very simplified assumption, because Gaussian is the maximum entropy (least informative) distribution with respect to prescribed mean and variance and has one single mode, while human inference can be ambiguous and can have a bounded support when we exclude very unlikely cases (de Haan & Ferreira, 2006). Many recent works try to tackle this limitation. Jang et al. (2017) extended VAE to effectively use a discrete latent z following a category distribution (e.g. Bernoulli distribution). Kingma et al. (2014) extended the latent structure with a combination of continuous and discrete latent variables (class labels)", "startOffset": 8, "endOffset": 1831}, {"referenceID": 11, "context": "Notably, we present a novel application scenario with new analysis on the Gumbel softmax trick (Jang et al., 2017; Maddison et al., 2016).", "startOffset": 95, "endOffset": 137}, {"referenceID": 18, "context": "Notably, we present a novel application scenario with new analysis on the Gumbel softmax trick (Jang et al., 2017; Maddison et al., 2016).", "startOffset": 95, "endOffset": 137}, {"referenceID": 11, "context": "We assemble these components into a machine CG-BPEF-VAE and present empirical results on unsupervised density estimation, showing improvements over vanilla VAE (Kingma & Welling, 2014) and category VAE (Jang et al., 2017).", "startOffset": 202, "endOffset": 221}, {"referenceID": 6, "context": "(2016) and Dilokthanakul et al. (2017) proposed to use a Gaussian mixture latent model in VAE.", "startOffset": 11, "endOffset": 39}, {"referenceID": 6, "context": "(2016) and Dilokthanakul et al. (2017) proposed to use a Gaussian mixture latent model in VAE. Serban et al. (2016) applied a piecewise constant distribution on z.", "startOffset": 11, "endOffset": 116}, {"referenceID": 3, "context": "1 Bounded Polynomial Exponential Family We try to model the latent z with a factorable polynomial exponential family (PEF) (Cobb et al., 1983; Nielsen & Nock, 2016) probability density function:", "startOffset": 123, "endOffset": 164}, {"referenceID": 3, "context": "This PEF family can be regarded as the most general parameterization, because with large enough M it can approximate arbitrary finely any given p(z) satisfying weak regularity conditions (Cobb et al., 1983).", "startOffset": 187, "endOffset": 206}, {"referenceID": 11, "context": "This reparameterization problem of category distribution is studied recently (Jang et al., 2017; Maddison et al., 2016) following earlier developments (Kuzmin & Warmuth, 2005; Maddison et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 18, "context": "This reparameterization problem of category distribution is studied recently (Jang et al., 2017; Maddison et al., 2016) following earlier developments (Kuzmin & Warmuth, 2005; Maddison et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 18, "context": "Based on these previous studies, we let yj follow a Concrete distribution (Maddison et al., 2016), which is a continuous relaxation of the category distribution, with the key advantage that Concrete samples can be easily drawn to be applied to VAE.", "startOffset": 74, "endOffset": 97}, {"referenceID": 18, "context": "Hence it can be considered as a relaxation (Maddison et al., 2016) of the category distribution.", "startOffset": 43, "endOffset": 66}, {"referenceID": 11, "context": "The study (Jang et al., 2017) implies that Tmin = 0.", "startOffset": 10, "endOffset": 29}, {"referenceID": 20, "context": "The SVHN dataset (Netzer et al., 2011) has around 100,000 gray-scale pictures (for simplicity the original 32\u00d7 32\u00d7 3 RGB images are reduced into 32\u00d7 32\u00d7 1 by averaging the 3 channels) of door numbers with a train/valid/test split of 10 : 1 : 3.", "startOffset": 17, "endOffset": 38}, {"referenceID": 11, "context": "We compare the proposed CG-BPEFVAE with vanilla VAE (Gauss-VAE) and Category VAE (Cat-VAE) (Jang et al., 2017).", "startOffset": 91, "endOffset": 110}, {"referenceID": 13, "context": "One has to incorporate supervised information (Kingma et al., 2014) to achieve better results.", "startOffset": 46, "endOffset": 67}, {"referenceID": 28, "context": "is essentially related to the theory of minimum description length (Hinton & Zemel, 1994; Sun et al., 2015).", "startOffset": 67, "endOffset": 107}, {"referenceID": 2, "context": "Among numerous variations of VAE (Burda et al., 2016; Jang et al., 2017), CG-BPEF-VAE is featured by using a universal BPEF density generator in the inference model, and providing a principled way to simulate continuous densities using discrete latent variables.", "startOffset": 33, "endOffset": 72}, {"referenceID": 11, "context": "Among numerous variations of VAE (Burda et al., 2016; Jang et al., 2017), CG-BPEF-VAE is featured by using a universal BPEF density generator in the inference model, and providing a principled way to simulate continuous densities using discrete latent variables.", "startOffset": 33, "endOffset": 72}, {"referenceID": 11, "context": "This study touches a fundamental problem in unsupervised learning: how to build a discrete latent structure to factor information in a continuous representation? We provide preliminary results on unsupervised density estimation, showing performance improvements over the original VAE and category VAE (Jang et al., 2017).", "startOffset": 301, "endOffset": 320}], "year": 2017, "abstractText": "Variational autoencoders (VAE) often use Gaussian or category distribution to model the inference process. This puts a limit on variational learning because this simplified assumption does not match the true posterior distribution, which is usually much more sophisticated. To break this limitation and apply arbitrary parametric distribution during inference, this paper derives a semi-continuous latent representation, which approximates a continuous density up to a prescribed precision, and is much easier to analyze than its continuous counterpart because it is fundamentally discrete. We showcase the proposition by applying polynomial exponential family distributions as the posterior, which are universal probability density function generators. Our experimental results show consistent improvements over commonly used VAE models.", "creator": "LaTeX with hyperref package"}}}