{"id": "1702.07998", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2017", "title": "Detecting (Un)Important Content for Single-Document News Summarization", "abstract": "We present a robust approach for detecting intrinsic sentence importance in news, by training on two corpora of document-summary pairs. When used for single-document summarization, our approach, combined with the \"beginning of document\" heuristic, outperforms a state-of-the-art summarizer and the beginning-of-article baseline in both automatic and manual evaluations. These results represent an important advance because in the absence of cross-document repetition, single document summarizers for news have not been able to consistently outperform the strong beginning-of-article baseline.", "histories": [["v1", "Sun, 26 Feb 2017 08:07:26 GMT  (27kb)", "http://arxiv.org/abs/1702.07998v1", "Accepted By EACL 2017"]], "COMMENTS": "Accepted By EACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yinfei yang", "forrest sheng bao", "ani nenkova"], "accepted": false, "id": "1702.07998"}, "pdf": {"name": "1702.07998.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["yangyin7@gmail.com", "forrest.bao@gmail.com", "nenkova@seas.upenn.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.07 998v 1 [cs.C L] 26 February 2017Intrinsic sentence meaning in messages by training two corpora of document summary pairs. When used to summarize individual documents, our approach, combined with the heuristic \"beginning of the document\" method, outperforms both automatic and manual summarizers and the start-end baseline. These results represent an important step forward as individual document summaries for messages without repetition of individual documents were not able to consistently outperform the strong start-start-article baseline."}, {"heading": "1 Introduction", "text": "With a few exceptions (Svore et al., 2007; Berg-Kirkpatrick et al., 2011; Kulesza and Taskar, 2011; Cao et al., 2015; Cheng and Lapata, 2016), modern summary methods are not supervised and rely on on on-the-fly analysis of the input text to compile the summary without using indicators of intrinsic meaning learned from previously seen document summary pairs. This state of the art is highly intuitive, as it is obvious that some aspects of importance can be learned. Recent work has shown that monitored systems can function even without sophisticated functions if sufficient training data is available (Cheng and Lapata, 2016). In this paper, we show that in the context of news it is possible to learn precise pre-content that is summary."}, {"heading": "2 Corpora", "text": "One of the most cited difficulties in using monitored methods for summarizing was the lack of suitable corpora of document summary pairs, in which each sentence is clearly marked as either important or not (Zhou and Hovy, 2003). We use two currently available resources: archive data from the Document Understanding Conferences (DUC) (Over et al., 2007) and the New York Times (NYT) corpus (https: / / catalog.ldc.upenn.edu / LDC2008T19). The DUC data contain document summary pairs, in which the summaries were produced for research purposes while preparing a common task for the summary. The NYT dataset contains thousands of such pairs, and the summaries were written by information scientists working for the study aper.DUC2002 is the latest dataset from the DUC series, in which annotators were produced extractive summaries consisting of sentences directly from input."}, {"heading": "3 Method", "text": "As already mentioned, existing data sets contain clear terms only for positive sentences. Due to the variability of human decisions in compiling a summary, unlabeled sentences cannot simply be treated as negative. In our monitored approach to detecting sentence meaning, a semi-monitored approach is first used to establish terms."}, {"heading": "3.1 Learning from Positive and Unlabeled Samples", "text": "If we learn from positive (e.g. important in this paper) and blank samples, with the methods proposed in (Lee and Liu, 2003; Elkan and Noto, 2008) we can train a detector for sentence meaning based on positive and blank examples. Let's specify the importance forecast for a sample where y = 1 is expected for each positive sample and y = 0 for each negative sample. Let's apply the soil truth forecast obtained with the method described in section 2, where o = 1 means the sentence is positive (important) and o = 0 is blank. In the first stage we build an estimator corresponding to the probability that a sample will be predicted as positive because it is actually positive, p (o = 1 | y = 1)."}, {"heading": "3.2 Features", "text": "The classifiers for both stages use dictionary-derived features that specify the types / characteristics of a word, along with several general features. MRC The psycholinguistic database MRC (Wilson, 1988) is a collection of word lists with associated word attributes according to assessments by multiple persons. The degree to which a word is associated with an attribute is given as a score within a range. We divide the score range into 230 intervals. The number of intervals was determined empirically on the basis of a small development set and was inspired by previous work of feature engineering for real ratings (Beigman Klebanov et al., 2013). Each interval corresponds to a feature; the value of the feature is the word fraction in a sentence whose rating belongs to that interval. Six attributes are selected: images, concreteness, familiarity, age of acquisition and two significant features. Overall, there are 1,380 positive WC attributes, the LIE is one category."}, {"heading": "4 Experiments on Importance Detection", "text": "The DUCmodel is trained on the basis of articles and summaries from the DUC2002 dataset, in which a total of 1,833 records appear in the summaries. Randomly, we sample 2,200 non-summary records as unmarked samples to balance the training set. According to the criteria described in the section on the NYT corpus, there are 22,459 (14.1%) positive records selected from a total of 158,892 records. Sets with Jacana alignment values of less than or equal to 10 form the unmarked record, including a total of 20,653 (12.9%) unmarked records. Liblinear (Fan et al., 2008) is used for training the two-level classifiers."}, {"heading": "4.1 Test Set", "text": "The test set consists of 1,000 randomly selected sentences from the 2007 NYT dataset, half of which are from the business area where the training data was taken, and the rest from the US Department of International Relations (or Politics for short) to test the stability of the prediction across subject areas. Three students at the University of Akron comment on whether the test sets contain important summary information.For each test (source) sentence from the original article, we first apply Jacana to match it with each sentence in the corresponding summary; the highest-match summary sentence is selected as the target sentence for the source set; each pair of source and target sentences is presented to students and they are asked to highlight whether the sentences share information. Sentries from the original article that contribute to the content of the most similar summary sentence are marked as positive; those that do not are marked as negative."}, {"heading": "4.2 Evaluation Results", "text": "In the above procedure, we have obtained a number of sets of articles that contribute to the summary (positive class) or not (negative class).Table 2 shows the evaluation results on the human-commented test set. The starting point assumes that all sentences are summarized. Although the insignificant class is the majority (see Table 1), predicting all test samples as not summarizing is less useful in real-world applications as we cannot output blank text as a summary. 1We assume that a set of articles that does not contribute to the summary does not contribute to the summary sentence that comes closest to the article set. Each line in Table 2 corresponds to a model that has been trained with a training set. We use dictionary features to create the models, i.e. NemNYT Model and DUCModel. We also evaluate the effectiveness of the general features by excluding it from the dictionary features, i.e. NYT w / o in general and DUC in general."}, {"heading": "4.3 NYT Model vs. DUC Model", "text": "Furthermore, we examine the agreement between the two models in terms of the prediction result. First, we compare the prediction result of the two models with the test set NYT2007. The correlation coefficients of the Spearman between the results of the two models are about 0.90, which shows that our model is very robust and independent of the training set. Then, we repeat the study with a much larger data set using articles from the DUC 2004 Multi-Documentation Task. This year, there are no summaries of individual documents, but that is not a problem, as we simply use the data to study the match between the two models, i.e. whether they predict the same summarized status for sets, not to measure the accuracy of the prediction. There are 12,444 sets in this data set. The match between the two models is very high for both sets of tests (87%). Consistent with the above observation, the DUC model predicts the intrinsic importance of more aggressive sets of positive sets, or only a handful of positive sets of negative sets for the two sets of models."}, {"heading": "5 Summarization", "text": "We propose two important approaches to improving the summary of individual documents. In the first approach, InfoRank, the summary is compiled solely from the predictions of the sentence meaning classifier. In the face of a document, we first apply the sentence meaning detector to each sentence to determine the likelihood that this sentence is important in itself. We then arrange the sentences based on the probability value to form a summary within the required length. The second approach, InfoFilter, uses the sentence meaning detector as a pre-processing step. We first apply the sentence meaning detector to each sentence, in the order in which it appears in the article. We retain only sentences that appear to be summarizable up to the length restriction. This combines the preference for sentences that appear at the beginning of the article, but filters sentences that appear early but are not informative."}, {"heading": "5.1 Results on Automatic Evaluation", "text": "The model being trained on the NYT corpus is used here in the experiments. Business and policy articles (100 each) with man-made summaries of NYT2007 are used for evaluation; summaries produced by summarists are limited to 100 words; the performance of the summary is measured by ROUGE-1 (R-1) and ROUGE-2 (R-2) results (Lin, 2004); several summary systems are used here for comparison, including LeadWords, which select the first 100 words as the summary; RandomRank, which randomly classifies the sentences and then selects the highest-rated sentences to form a 100-word summary; and Icsisumm (Gillick et al., 2009), a state-of-the-art summary of the documents (Hong et al., 2014)."}, {"heading": "5.2 Results on Human Evaluation", "text": "We also perform human evaluations to better compare the relative performance of the LeadWords and InfoFilter summaries. Judgements are made for each of the 116 articles in which at least one set of InfoFilters has been filtered out. For each article, we first read the notes from the NYT2007 data set, and then the two summaries generated by LeadWords and InfoFilter, respectively. Then, we ask the annotations if any of the summaries cover more of the information presented in the NYT2007 summary. Annotators are given the ability to indicate that the two summaries are equally informative with respect to the content of the NYT summary. We randomly sort the sentences in LeadWords and InfoFilter summaries when presented to the annotators.The tasks are published on Amazon Mechanical Turk (AMT) and each summary pair is assigned 8 annotators."}, {"heading": "6 Conclusion", "text": "In this paper, we introduced a sentence meaning detector and demonstrated that it is robust regardless of training data; the meaning detector far exceeds the baseline; we also tested the summary predictors on multiple sets of data; and in the summary of a single document, the ability to identify unimportant content allows us to significantly exceed the strong lead baseline."}], "references": [{"title": "Using pivot-based paraphrasing and sentiment profiles to improve a subjectivity lexicon for essay data", "author": ["Nitin Madnani", "Jill Burstein"], "venue": "Transactions of the Association", "citeRegEx": "Klebanov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Klebanov et al\\.", "year": 2013}, {"title": "Jointly learning to extract and compress", "author": ["Dan Gillick", "Dan Klein"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technolo-", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2011}, {"title": "Ranking with recursive neural networks and its application to multidocument summarization", "author": ["Cao et al.2015] Ziqiang Cao", "Furu Wei", "Li Dong", "Sujian Li", "Ming Zhou"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intel-", "citeRegEx": "Cao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "Neural summarization by extracting sentences and words. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages", "author": ["Cheng", "Lapata2016] Jianpeng Cheng", "Mirella Lapata"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Learning classifiers from only positive and unlabeled data", "author": ["Elkan", "Noto2008] Charles Elkan", "Keith Noto"], "venue": "In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Elkan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Elkan et al\\.", "year": 2008}, {"title": "Liblinear: A library for large linear classification", "author": ["Fan et al.2008] Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "The icsi/utd summarization system at tac", "author": ["Gillick et al.2009] Dan Gillick", "Benoit Favre", "Dilek Hakkani-tr", "Berndt Bohnet", "Yang Liu", "Shasha Xie"], "venue": "In Proceedings of the Second Text Analysis Conference (TAC 2009),", "citeRegEx": "Gillick et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2009}, {"title": "A repository of state of the art and competitive baseline summaries for generic news summarization", "author": ["Hong et al.2014] Kai Hong", "John Conroy", "Benoit Favre", "Alex Kulesza", "Hui Lin", "Ani Nenkova"], "venue": "In Proceedings of the Ninth International", "citeRegEx": "Hong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hong et al\\.", "year": 2014}, {"title": "Learning determinantal point processes", "author": ["Kulesza", "Taskar2011] Alex Kulesza", "Ben Taskar"], "venue": "UAI", "citeRegEx": "Kulesza et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulesza et al\\.", "year": 2011}, {"title": "Learning with positive and unlabeled examples using weighted logistic regression", "author": ["Lee", "Liu2003] Wee Sun Lee", "Bing Liu"], "venue": "In Proceedings of the Twentieth International Conference on International Conference on Machine Learning,", "citeRegEx": "Lee et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2003}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "Text Summarization Branches Out: Proceedings of the ACL04 Workshop,", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "The pyramid method: Incorporating human content selection variation in summarization evaluation", "author": ["Nenkova et al.2007] Ani Nenkova", "Rebecca Passonneau", "Kathleen McKeown"], "venue": "ACM Transactions on Speech and Language Processing,", "citeRegEx": "Nenkova et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nenkova et al\\.", "year": 2007}, {"title": "The general inquirer: A computer system for content analysis and retrieval based on the sentence as a unit of information", "author": ["Robert F. Bales", "J. Zvi Namenwirth", "Daniel M. Ogilvie"], "venue": "Behavioral Science,", "citeRegEx": "Stone et al\\.,? \\Q1962\\E", "shortCiteRegEx": "Stone et al\\.", "year": 1962}, {"title": "Enhancing singledocument summarization by combining RankNet and third-party sources", "author": ["Svore et al.2007] Krysta Svore", "Lucy Vanderwende", "Christopher Burges"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural", "citeRegEx": "Svore et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Svore et al\\.", "year": 2007}, {"title": "Mrc psycholinguistic database: Machine-usable", "author": ["Michael Wilson"], "venue": null, "citeRegEx": "Wilson.,? \\Q1988\\E", "shortCiteRegEx": "Wilson.", "year": 1988}, {"title": "Detecting information-dense texts in multiple news domains", "author": ["Yang", "Nenkova2014] Yinfei Yang", "Ani Nenkova"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "A lightweight and high performance monolingual word aligner", "author": ["Yao et al.2013] Xuchen Yao", "Benjamin Van Durme", "Chris Callison-Burch", "Peter Clark"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "A web-trained extraction summarization system", "author": ["Zhou", "Hovy2003] Liang Zhou", "Eduard Hovy"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language", "citeRegEx": "Zhou et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 13, "context": "With a handful of exceptions (Svore et al., 2007; Berg-Kirkpatrick et al., 2011; Kulesza and Taskar, 2011; Cao et al., 2015; Cheng and Lapata, 2016), modern summarization methods are unsupervised, relying on on-the-fly analysis of the input text to generate the summary,", "startOffset": 29, "endOffset": 148}, {"referenceID": 1, "context": "With a handful of exceptions (Svore et al., 2007; Berg-Kirkpatrick et al., 2011; Kulesza and Taskar, 2011; Cao et al., 2015; Cheng and Lapata, 2016), modern summarization methods are unsupervised, relying on on-the-fly analysis of the input text to generate the summary,", "startOffset": 29, "endOffset": 148}, {"referenceID": 2, "context": "With a handful of exceptions (Svore et al., 2007; Berg-Kirkpatrick et al., 2011; Kulesza and Taskar, 2011; Cao et al., 2015; Cheng and Lapata, 2016), modern summarization methods are unsupervised, relying on on-the-fly analysis of the input text to generate the summary,", "startOffset": 29, "endOffset": 148}, {"referenceID": 11, "context": "Unlabeled sentences could be truly not summary-worthy but also may be included into a summary by a different annotator (Nenkova et al., 2007).", "startOffset": 119, "endOffset": 141}, {"referenceID": 16, "context": "In order to label sentences in the input, we employee Jacana (Yao et al., 2013) for word alignment in mono-lingual setting for all pairs of article-summary sentences.", "startOffset": 61, "endOffset": 79}, {"referenceID": 14, "context": "MRC The MRC Psycholinguistic Database (Wilson, 1988) is a collection of word lists with associated word attributes according to judgements by multiple people.", "startOffset": 38, "endOffset": 52}, {"referenceID": 12, "context": "INQUIRER The General Inquirer (Stone et al., 1962) is another dictionary of 7,444 words, grouped in 182 general semantic categories.", "startOffset": 30, "endOffset": 50}, {"referenceID": 5, "context": "blinear (Fan et al., 2008) is used for training the two-stage classifiers.", "startOffset": 8, "endOffset": 26}, {"referenceID": 10, "context": "scores (Lin, 2004).", "startOffset": 7, "endOffset": 18}, {"referenceID": 6, "context": "picks the first 100 words as the summary; RandomRank, which ranks the sentences randomly and then picks the most highly ranked sentences to form a 100-word summary; and Icsisumm (Gillick et al., 2009), a state-of-the-art", "startOffset": 178, "endOffset": 200}, {"referenceID": 7, "context": "multi-document summarizer (Hong et al., 2014).", "startOffset": 26, "endOffset": 45}], "year": 2017, "abstractText": "We present a robust approach for detecting intrinsic sentence importance in news, by training on two corpora of documentsummary pairs. When used for singledocument summarization, our approach, combined with the \u201cbeginning of document\u201d heuristic, outperforms a state-ofthe-art summarizer and the beginning-ofarticle baseline in both automatic and manual evaluations. These results represent an important advance because in the absence of cross-document repetition, single document summarizers for news have not been able to consistently outperform the strong beginning-of-article baseline.", "creator": "LaTeX with hyperref package"}}}