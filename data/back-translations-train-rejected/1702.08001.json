{"id": "1702.08001", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2017", "title": "Bayesian Nonparametric Feature and Policy Learning for Decision-Making", "abstract": "Learning from demonstrations has gained increasing interest in the recent past, enabling an agent to learn how to make decisions by observing an experienced teacher. While many approaches have been proposed to solve this problem, there is only little work that focuses on reasoning about the observed behavior. We assume that, in many practical problems, an agent makes its decision based on latent features, indicating a certain action. Therefore, we propose a generative model for the states and actions. Inference reveals the number of features, the features, and the policies, allowing us to learn and to analyze the underlying structure of the observed behavior. Further, our approach enables prediction of actions for new states. Simulations are used to assess the performance of the algorithm based upon this model. Moreover, the problem of learning a driver's behavior is investigated, demonstrating the performance of the proposed model in a real-world scenario.", "histories": [["v1", "Sun, 26 Feb 2017 08:34:26 GMT  (418kb,D)", "http://arxiv.org/abs/1702.08001v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["j\\\"urgen hahn", "abdelhak m zoubir"], "accepted": false, "id": "1702.08001"}, "pdf": {"name": "1702.08001.pdf", "metadata": {"source": "CRF", "title": "Bayesian Nonparametric Feature and Policy Learning for Decision-Making", "authors": ["J\u00fcrgen Hahn", "Abdelhak M. Zoubir"], "emails": ["jhahn@spg.tu-darmstadt.de", "zoubir@spg.tu-darmstadt.de"], "sections": [{"heading": null, "text": "Although many approaches have been proposed to solve this problem, there is little work focused on arguing about observed behavior. We assume that an agent makes decisions on many practical issues based on latent characteristics indicating a particular action. Therefore, we propose a generative model for states and actions. Conclusions reveal the number of characteristics, characteristics and strategies that allow us to learn and analyze the underlying structure of observed behavior. In addition, our approach allows us to predict actions for new states. Simulations are used to evaluate the performance of the algorithm based on this model. Furthermore, the problem of learning the behavior of a driver is investigated, demonstrating the performance of the proposed model in a real scenario.Keywords: Bayesian non-parameters, decision-making, learning from demonstrations, feature learning, imitation learning"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "1.1 Problem formulation", "text": "The aim of this work is to introduce a function-based LFD framework. While this model can be used to mediate an agent, since observations of the desired behavior are available, the focus of this work is on analyzing the observed behavior based on characteristics. Since we are looking at a decision task, the problem under investigation can be modeled using a partially observable Markov decision process (POMDP). [21, 42], which is generated by \u2022 a series of observations, Z, \u2022 a series of states, X, \u2022 a finite series of Nu-actions, U, \u2022 a transition model that describes the likelihood of entering a state in the current state after taking an action, \u2022 an observation model that explains how the observations are generated from states, \u2022 a discount factor that penalizes long-term rewards, \u2022 and a reward function, R.Since the main objective of this work is to provide a means of understanding the observed behavior, we consider a imitated learning approach directly from the states, and the observations of the states."}, {"heading": "1.2 Relation to existing work", "text": "The following is an overview of the current state of the feature learning for LFD. Since IRL usually has to solve a Markov Decision Process (MDP), which is normally done using a Reinforcement Learning (RL) algorithm, we will start with an introduction to the feature learning for RL."}, {"heading": "1.2.1 Reinforcement Learning", "text": "Large spaces of state and action are particularly problematic for value-oriented RL algorithms such as value iteration [3] or Q-Learning [48], since the value function representing the expected cumulative reward must be approximated. In early approaches, a number of basic functions, often referred to as traits, are linearly weighted to represent this function [5]. However, there is little work to learn these basic functions. Riedmiller [38] has proposed the quality-adapted value allocation, in which the value function is approximated by means of a neural network, in which the traits are learned in the layers of the network. In [29] this approach has been expanded by replacing the neural network with a deep-layer object. Another concept for applying traits is proposed by Hutter with the framework Feature Reinforcement Learning [18]. Within this framework, the aim is to enable a characterization of the learning space based on the history of the DP (consisting of 31 actions) to learn a D."}, {"heading": "1.2.2 Inverse Reinforcement Learning", "text": "As in RL, characteristics are often used to parameterise the reward function linearly, e.g. in [30, 17]. Recent attempts have been made to consider a deep learning (DL) architecture [49] for IRL that provides funding for non-linear, hierarchical feature learning. [6] suggests a Bayesian non-parametric approach, using an Indian buffet process (IBP) to model feature activations. Since the characteristics of the reward function are assumed to be known, this approach can be understood more as feature selection than feature learning for IRL, where the number of characteristics is derived by the IBP. Different results on Bayesian non-parameterization for IRL that are indirectly related to feature learning are given in [27], where a division of state space is sought, or [45] where complex behaviours can be easily learned in more simple ways."}, {"heading": "1.2.3 Imitation Learning", "text": "Instead of estimating the reward as in the IRL, imitation acquisition aims to directly infer the underlying policy [2, 44]. As a rule, craft characteristics are used, e.g. in [37, 39]. Attempts to introduce new characteristics are made in [36] as an extension of the maximum margin planning algorithm proposed in [37]. As explained in [1], imitation acquisition can be considered a supervised learning task. Therefore, feature selection and learning techniques developed for classification and regression can also be used in imitation acquisition. An excellent overview is given in [16]. Although these models work well in practice, they may not be able to provide a deeper understanding of observed behavior, as they do not explicitly model states and actions."}, {"heading": "2 Choice of the Model", "text": "In the first part of this section, we propose a feature model for LFD. In the second part, we explain the relevance of the transition model for the proposed framework. As we assume a mixture of strategies in this framework, we briefly discuss the intuition behind this assumption in the third part. Alternative models for feature learning for LFD are discussed in the fourth part."}, {"heading": "2.1 Feature model for learning from demonstrations", "text": "We proceed from a linear latent feature model, similar to NMF [23] and PCA [20]. Therefore, it is assumed that the noisy observations, zn \u0435R1 \u00b7 D, n = 1,.., Nz, are composed of the latent features, F-FK \u00b7 D, and the coefficients of features, sn-S1 \u00b7 K, zn = snF + n, (1) where n represents Gaussian i.i.d. noise with variant attracting effect 2 z and the states are given as xn = snF. The number of features is K and the dimension of the observations is D. Clearly, the feature space, F, depends on the application. In the following, we assume that the features are positively evaluated, i.e. F = R +. In the following [22], the feature matrix is composed of a binary activation matrix, A-0, 1} K-D, whereby the feature A is implicit, W-K-FD, and the feature matrix is Ffinite."}, {"heading": "2.2 Transition model", "text": "Since we focus on deriving the latent causes of the observed actions based on characteristics, the transition model is less relevant because it only provides additional information. Moreover, the transition model is rarely known in practice. As we have observations, we can theoretically infer the transition model by using either a parametric or a non-parametric model. Defining a parametric model for the transitions is not trivial, since the dynamics in latent space can be highly non-linear. Alternatively, a non-parametric model can be adopted, but this requires a large number of observations to estimate the parameters that are often not available. Assuming that the noise in the observations is low, we argue that we can reliably infer the partial states from the corresponding observations, eliminating the need for a transition."}, {"heading": "2.3 Feature-based policy", "text": "Since each characteristic imposes its own policy, the probability that the agent will take a measure is u, in a sub-state, s, a mixture of the characteristic policies P (u | \u03c6k), P (u | s, \u03a6), K \u2211 k = 1 skP (u | \u03c6k), (3), where \u03c6k, k = 1,.., K, are the parameters of the characteristic policies, and 3). The mixture of policies can be interpreted as either stochastic or deterministic policies. In the first case, the measures to be taken should be sampled according to Equation (3). In the second case, simply the most likely measure is taken. Mixture policies have been investigated in multi-objective problems, where one agent aimed at achieving multiple goals, some of which may even be contradictory. [32, 46, 41] A stochastic policy is necessary to ensure that all objectives can be fulfilled. Similar problems arise when the agent is explained by the uncertainty of an individual state."}, {"heading": "2.4 Alternative Feature-based Models", "text": "We briefly discuss two alternatives with their potential advantages and disadvantages. For the sake of completeness, as mentioned in the introduction, in the most trivial setup one could simply group the states according to the observed actions and then learn the characteristics. However, this has the significant disadvantage that the characteristics cannot be divided between the clusters."}, {"heading": "2.4.1 Unique coefficient model", "text": "Instead of assuming, as in our model, that the characteristics determine the behavior of the agent, the characteristic coefficients can be bundled, with the clusters displaying the optimal actions taking into account the coefficients. Thus, the clusters can be interpreted as latent substates. Assuming that the elements of the characteristic coefficients are binary, we can convert any characteristic coefficient vector into a unique identifier representing the cluster. Thus, instead of applying costly clustering, a quick deterministic mapping of the binary coefficients to the cluster identifier is possible. However, since the identifiers must be unique for cluster assignments, we can have a maximum of 2K different clusters in this setup, i.e. the number of clusters (and thus possible actions) is strictly limited by the number of characteristics. One advantage of this model is that the relationship between substates and policies can be non-linear. A major disadvantage, however, is that this model is severely affected by the error associated with the particular element in the following."}, {"heading": "2.4.2 Clustering-based approach", "text": "A cluster-based approach assumes that similar states can be grouped and lead to the same behavior, which can also be understood as a single characteristic model, in which we assume that the observations can be described by a single characteristic that represents the clusters. Thus, the sub-states are reduced to cluster indicators, i.e. they indicate which characteristic best represents the observed state, and the number of latent states corresponds to the number of characteristics. One way to derive this number is to use a Chinese restaurant process (CRP), which leads to a Bayesian non-parametric model [27]. A similar model, in which the state space is grouped according to the actions played, is proposed in [43]."}, {"heading": "2.4.3 Relation to the proposed model", "text": "Compared to the cluster-based approach and the unique coefficient model, our model has the advantage that the characteristics are a means to understand the observed behavior. Unlike the unique coefficient model, we do not require binary coefficients or a clustering step. Compared to the cluster-based approach, our model is able to significantly reduce the number of latent characteristics and policies because the characteristics can be shared by different states. However, our model suffers from the assumption of a linear relationship between characteristics, sub-states and policies. This problem is mitigated in the other approaches because the characteristics are disconnected from the policies. Note that our model is similar to the cluster-based model at the cost of higher calculation costs if each sub-state is represented by only one characteristic."}, {"heading": "3 Bayesian Nonparametric Model for Feature Learning", "text": "In this section, we provide a general framework for Bayesian non-parametric feature learning for decision-making based on the model proposed in Section 2.1. To learn the structure, we assume that we obtain a set of observations consisting of state pairs D = {(z1, u1),... (zNz, uNz)}."}, {"heading": "3.1 Observation likelihood and noise variance", "text": "The observations, zn, n = 1,.., Nz, are assumed to be conditionally independent. As we proceed from Gaussian noise in Equation (1), the probability of state can be expressed with Z = [z1 T... zNz T] T and S = [s1 T... sNz T] T. The deviation of noise, \u03c32z, is assumed to be an inverse gamma distributed with hyperparameters \u03b1, \u03b2\u03c3. Furthermore, we place hyperpriors on \u03b2 and \u03b2, followed by gamma distributions with hyperparameters h (1), h (2) and h (2) \u03b2\u03c3."}, {"heading": "3.2 Prior for the feature weights", "text": "As explained above, the previous probability of feature weights W depends on the problem at hand. In general, we consider positively weighted feature weights wk, d with k = 1,..., K and d = 1,..., D and assume an exponential value, p (W | \u03b3w) = K \u0435k = 1 D, d = 1 Expwk, d (\u03b3w). The scaling factor \u03b3w is assumed to be an inverse gamma distribution with hyperparameters \u03b1\u03b3 and \u03b2\u03b3. If the features are assumed to be real, the previous one can be modeled with a Gaussian distribution with simple modifications."}, {"heading": "3.3 Prior for the feature activations", "text": "The activations of the feature are modeled using an IBP [13, 15], assuming an infinite number of characteristics. In the following, we will consider the generalization of two parameters [14], which makes it possible to capture both sparse and dense matrices. IBP is derived as follows: For a finite number of characteristics, K?, the sums are assumed over the rows of the feature activation matrix, A?, K? \u2212 \u00b7 \u00b7 D, to follow i.e. Binomial distributions. Placing a beta previously with hyperparameters, K? and \u03b2a over the parameter of binomial distribution and marginalization over this parameter results in a beta binomial distribution [13, 14]. Since we are interested in sampling an infinite number of characteristics, we consider the boundary for K?"}, {"heading": "3.4 Prior for the substates", "text": "As formulated in Equation (1), we assume that the observations consist of a mixture of characteristics weighted by the sub-states. Similar to an FMDP1, we limit the domain of the sub-states, sn, k, with n = 1,..., Nz and k = 1,.., K to take values from a finite set, S = {s, 1,... s, L, where L denotes the number of elements to simplify the conclusion. Convenience is that we assume equidistant elements in S and s, 1 = 0 and s, L = 1. Note that the limited range does not limit the model, as the characteristics can be scaled to the observations. Further, we assume that the sub-states are sparse, which means that any observation consists of 1A fundamental differences between our model and an FMDP, is that the sub-states in our model are theoretically not limited to a finite size, since we do not need to grasp a particular component."}, {"heading": "3.5 Mixture of policies and action likelihood", "text": "Since we assume a finite series of actions, we consider a categorical distribution for each mixture component, P (u | \u03c6k) = Catu (\u03c6k), so that the mixing model asP (u | \u03a6, s) = 1 To K \u2211 k = 1 skP (u | \u03c6k), (6) with normalization constant To = \u2211 u \u00b2 U \u00b2 K = 1 skP (u | \u0445 k) can be written. The parameters of the policy of each mixture component follow the Dirichlet distributions with identical hyperparameters, \u03b1\u03c6, p (\u03a6) = K \u00b2 k = 1 dirty lime (\u03b1), assuming independent strategies. We consider the hyperparameter as a gamma distribution variable with the parameters h (1) \u00b1 A and h (2) \u00b1 A. As explained, we need a data set containing observed actions, un, n = 1,."}, {"heading": "3.6 Joint posterior distribution", "text": "The complete posterior joint distribution can be factored as asp (W, A, S, \u03a6, \u03c32z, \u03b3w, \u03b1a, \u03b2a, \u03b1\u03c3, \u03b2\u03c3, \u03b1\u03c6 | Z, u) p (Z | W, A, S, \u03c32z) P (u | S, \u0445) p (\u03c32z | \u03b1\u03c3, \u03b2\u03c3) \u00b7 p (S) p (W | \u03b3w) P (A | \u03b1a, \u03b2a) p (\u03a6 | \u03b1\u03c6) \u00b7 p (\u03b3w | \u03b1\u0445), \u03b2\u03b3 (\u03b2\u03b3) p (\u03b2\u03c3) p (\u03b2a) p (\u03b1\u03c6). (7) The conditional independence in this model is evaluated in Section 4, where the conclusion based on this model is explained. The structure of the posterior model is shown in Fig. 1 as a graphic model."}, {"heading": "4 Inference", "text": "We consider the problem of learning the latent variables and predicting optimal actions for new observations to be a Bayesian inference problem. Therefore, we are interested in the common posterior distribution in equivalents (7). Since the common posterior variable is not directly tractable, we present it using samples generated using the Gibbs sample. Therefore, in the first part of this section we derive the conditional distributions of the variables. After explaining the sample scheme in the second part, we explain in detail how actions for new observations can be predicted using this model. For simplicity, we use the bar symbol (\u2212) in what follows to denote the set of conditional variables, i.e. all variables except the one to be sampled. The set of all latent variables is called B = {W, A, S, E \u00b2, D \u00b2, W, A, \u03b2a, D \u00b2, D \u00b2, where the common space of the latent is designated."}, {"heading": "4.1 Conditional distributions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Sampling the noise variance", "text": "The hyperpriors of \u03b1\u03c3 and \u03b2\u03c3 are conjugated to \u03c3 2. Therefore, the conditional p (\u03c3 2 z | \u2212) is also inverse gamma distributed, p (\u03c32z | \u2212) and IGa\u03c32z (\u03b1\u03c3 + NzD2 +, \u03b2\u03c3 + 12 Nz \u0445 n = 1 D \u0445 d = 1 (zn, d \u2212 K \u0445 k = 1 sn, kak, dwk, d) 2. We use a Metropolis-Hastings algorithm with a gamma proposal distribution to generate samples of the hyperparameters \u03b1\u03c3 and \u03b2\u03c3."}, {"heading": "4.1.2 Sampling the substates", "text": "The sample of the substates, S, is simple thanks to the assumption of a finite space of the elements. The condition consists of the probability of state and action as good as the (conditional) previous one, P (sn, k | \u2212), p (zn | W, A, sn, \u03c32z) P (un | sn, \u03a6) P (sn, k | S\\ sn, k), (8) for all sn, k \u00b2 S, where S\\ sn, k denotes the elements of S without sn, k. Since the previous one for the substates follows a beta binomial distribution, the condition is simply set asP (sn, k | S\\ sn, k): ms = 0 + \u03b1s = 0 for sn, k = 0ms6 = 0 for sn, k 6 = 0, where ms = 0 and ms 6 = 0 are defined in section 3.4."}, {"heading": "4.1.3 Sampling the feature weights", "text": "Since the probability is Gaussian and the weights i.i.d. follow an exponential distribution, the condition of the kest row of the weight matrix, wk, is an abbreviated Gaussian distribution, p (wk | \u2212) p (Z | W, A, S, \u03c32z) D \u0394d = 1 p (wk, d | \u03b3w) \u0435T Nwk (\u00b5wk, \u0394wk), where T Nwk (\u00b5wk, \u0394wk) denotes an abbreviated Gauss in which the elements of wk must necessarily be evaluated positively. Mean, \u00b5wk, and covariance, \u0441wk, are as\u03a3 \u2212 1wk = 1\u04412z Nz, \u0441wk = 1 s2n, kdiag (ak) (9) \u00b5wk = \u03a3 \u2212 1 wk = 1 sampling, k (zn \u2212 K) starting from the Gausk distribution, a neglect (we) in relation to the desired distribution is not taken into account."}, {"heading": "4.1.4 Sampling the feature activations", "text": "The sample from the IBP consists of two steps: For each row, (i) the active columns are updated and then (ii) new characteristics are proposed. In the first step, an element of the activation matrix, ak, d, with probability P (ak, d | \u03b2), p (zd | Sfd, \u03c32z) P (ak, d | ak\\ d), (11) is proposed with ak\\ d, which denotes the lowest row of A without the dste element and fd the lowest column of F. Assuming that A follows an IBP, the conditional attribute is in equation. (11) [13, 14] P (ak, d = 1 | ak\\ d) = mk\\ dD + \u03b2a \u2212 1, where mk\\ d is the sum of ak\\ d.In the second step, K + new characteristics are proposed in a metropolis step [10, 22] P (ak, d = 1 | ak\\ d) + the ability, where mk\\ d is the sum of ak\\ d over ak\\ d."}, {"heading": "4.1.5 Sampling the policies", "text": "An efficient approach is to introduce auxiliary variables, tn, n = 1,..., Nz, for each observation indicating the policy from which the observed action, un, was generated [25]. In view of the indicators, the mixing component, k = 1., K, is in equation. (6) become conditionally independent of the mixing weights, sn, n = 1,.., Nz, which makes scanning the components straightforward. Thus, the scanning algorithm consists of two steps. First, the indicators are divided into P (tn = k | un, S, \u03c6), skP (un | \u03c6k), n indicators. (13) To approximate the conditions for policy, we draw Nt indicators from Eq. (13)."}, {"heading": "4.2 Sampling algorithm", "text": "The Gibbs sampler is initialized with only one attribute and the variables are sampled from their previous distributions. After several iterations of the Gibbs sampler, samples from the target distribution are generated. Note that in each iteration of the Gibbs sampler, there is the possibility of generating new attribute. Especially in high-noise scenarios, different lines of the attribute matrix may converge to similar realizations, thereby unnecessarily increasing the number of attribute. We suggest merging characteristics that reduce the number of attribute if they exhibit a similarity greater than a given threshold, Tcorr, at which we maintain the activation of both attribute and average the policy. The similarity is measured by the estimated correlation between the attribute samples.A maximum a-posteriori (MAP) estimator can be used if we are interested in an estimate of the latent probability of the AP, the 35 predictive characteristics, among other things]."}, {"heading": "4.3 Prediction of actions", "text": "The proposed model can be used to learn the structure of the observed states as well as for predicting actions as new observed states. For predicting an optimal action, u?, in the face of a new observation, z?, we can evaluate the posterior predictive distribution, which leads to an MMSE estimator, P (u? | z?, D) = VP (u?, s?, Z?, E?) p (o? z?, D) d? ds?, (14) where we exploit that u? is independent of the dataset that contains the observations, D, since the observations,. Equation. (14) shows that this model depends on the new observations, z?. All variables would have to be derived for each prediction, which would be computationally expensive. To solve this problem, we assume that the observed data in D sufficiently represent the conditional distribution of the measures, so that we can ignore the dependence and simply derive from D."}, {"heading": "5 Experimental Results", "text": "In order to demonstrate the performance of the proposed method, we consider simulations as well as real data experiments. In this section, we evaluate the performance of the inference algorithm by simulating observations with different SNRs and different latent numbers of characteristics. To do this, we simulate the basic truth values by taking samples from distributions similar to the previous distributions of variables, described in detail below. The true hyperparameters of the characteristic weights are set to h-shaped values = h-shaped = 100. Thus, the true hyperparameters for the weights, g-w, and the true weights, W-shaped, are sampled from their previous distributions. An element of the true characteristic activation matrix, A-shaped, is activated with the probability P (a-shaped, d-1) = 0.5. The true particles, s-shaped, s-shaped, are simulated."}, {"heading": "5.1 Estimation of the features", "text": "The quality of the MAP estimates is evaluated with respect to the Mean Squared Error (MSE) for the elements of the characteristics, FMAP, the substates, SMAP and the reconstructions, XMAP = SMAPFMAP. Furthermore, we calculate the RMSEs over 20 Monte Carlo runs and obtain RMSE measurements for each estimated variable, F, S and X. The results are presented in Fig. 2 (a) - (c). We obtain good results with low errors, especially for a small latent number of characteristics, almost independently of the SNR. As K increases, we observe a sharp increase in the error. The error becomes even more significant with strong noise. It is noteworthy that the error of the estimated characteristic values and the coefficients of characteristics behave similar behavior. Due to the linear relationship between the characteristics and the substates, the errors of the reconstructions also grow with the number of characteristics."}, {"heading": "5.2 Action prediction", "text": "As shown in Fig. 2 (d), the error increases slightly as the number of characteristics increases. Here, too, the SNR has little impact on the accuracy of the measures compared to the number of characteristics. The accuracy of the action forecast with the estimators MAP and MMSE is shown in Fig. 2 (e). With few characteristics (K < 15) and low noise (SNR > 20 dB), we obtain high-precision predictions with an accuracy of over 90%. Only with heavy noise and many latent characteristics, the accuracy drops slightly below 70%. This observation can be explained by the relationship between the action and sub-states, which are difficult to conclude with many characteristics. Since we assume in the simulation experiments that there are a fixed set of parameters that explain the observations, the MMSE estimator shows only slight improvements compared to the predicted measures."}, {"heading": "5.3 Estimation of the number of features", "text": "Assuming that the observations were generated by a fixed number of characteristics, we will assess how accurately the algorithm can derive this number. Results of the simulations are shown in Fig. 2 (f) and show the MAD RMSE over the 20 Monte Carlo runs. As can be observed, the MAP estimator can reliably conclude the correct number of characteristics, especially with a few characteristics. If, on the one hand, the noise in the observations increases and the observations are based on many latent characteristics, the error increases significantly. On the other hand, if the SNR is relatively high, i.e. SNR > 15 dB, the results for 18 latent characteristics deviate on average by only four from the true number. The error in estimating the number of characteristics is probably due to the fact that some simulation examples can be explained by fewer characteristics than were used to generate them."}, {"heading": "6 Real Data Experiments", "text": "We are looking at the problem of analyzing the behavior of a driver, which is an important task for the User Adaptive Driver Assistance Systems, which we have roughly considered to demonstrate the performance of the proposed model in a real-world scenario. To this end, we are observing the environment of the vehicle and the actions taken by the driver aimed at learning what prompted the driver to make the observed decisions. To this end, we are looking at real data provided by the KITTI Vision Benchmark Suite [11], which includes several challenges in urban driving. We are using the data for tracking, as it contains temporal LIDAR measurements of different situations on public roads. A schematic graph of the device is shown in Figure 3.We are looking at Scene 11 and Benchmark 20, which are the suite detailed."}, {"heading": "6.1 Scene 11 - Traffic jam", "text": "In fact, it is the case that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process in which it comes, a process, a process, a process, a process, a process in which it comes, a process, a process, a process, a process, a process, a process, a process in which it comes, a process, a process, a process, a process, a process, a process, a process, a process in which it comes, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process,"}, {"heading": "6.2 Scene 20 - Lane change", "text": "Scene 20 shows a lane change manoeuvre on a two-lane road. As can be seen from the acceleration signals shown in Fig. 4 (b), the driver accelerates three times, depending on the current traffic situation. Lane change occurs between time frames 158 and 220. Application of the proposed algorithm reveals 17 features. Using these features to predict the actions of the test data set results in an accuracy of 73.33%. The confusion matrix in Fig. 3 shows that lane changes are reliably predicted. Acceleration manoeuvres are in some cases misinterpreted as moving at constant speed or changing lanes. A third of the movements at constant speed is misclassified as lane change. If the reconstructed states are compared with the loud observation, an MSE of 0.007 is obtained. To illustrate the derived features, Fig. 6 shows three of the 17 features, each of which indicates a different action. The first feature (a) shows a lane change (right)."}, {"heading": "7 Discussion", "text": "As shown in the simulation experiments, the algorithm based on the proposed model is able to reliably infer the number of features, characteristics and guidelines. In the case of heavy noise, the algorithm finds several, almost equally probable explanations for the observations, which leads to deviations in the MAP estimate. In particular, when the number of features is high compared to the dimensionality of the observations, the correct number of features can be relied on. Real data experiments show that the model is able to provide deeper insights into the observations that can provide new conclusions about the observed behavior. As explained, in high-dimensional observations, the observation probability is likely to dominate the posterior feature, resulting in only little impact on the likelihood of action and therefore poor predictive performance. As suggested in Section 4.3, the reweighting of the likelihood of action can be relied upon assuming an action variable in each entry, which can easily be structured for increased performance even during a clear observation."}, {"heading": "8 Conclusion", "text": "A key assumption of the proposed model is that the observations consist of latent features. Each feature imposes its own policy and contributes to the agent's decision. To learn the structure of the behavior and predict actions, we have considered a Bayesian non-parametric approach based on the Indian buffet process, which makes it possible to deduce the number of features and the characteristics themselves from the observed data. On the basis of this model, we are able to gain a deeper understanding of the observed behavior, since the characteristics and their guidelines allow us to argue about the decisions observed. Simulations show that the developed algorithm works well. It is only in scenarios with high noise and many latent characteristics that the conclusion will be challenging. Furthermore, we consider the task of learning the behavior of a driver. To this end, we have applied our algorithm to real data obtained from the KITTI benchmark suite to predict the conditions under which the drivers will be able to perform."}], "references": [{"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Robot learning from demonstration", "author": ["C.G. Atkeson", "S. Schaal"], "venue": "In Proceedings of the 14th International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1957}, {"title": "Exploiting structure in policy construction", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": "In Proceedings of the 14th International Joint Conference on Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["S.J. Bradtke", "A.G. Barto"], "venue": "Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Nonparametric Bayesian inverse reinforcement learning for multiple reward functions", "author": ["J. Choi", "K.-E. Kim"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Fast simulation of truncated Gaussian distributions", "author": ["N. Chopin"], "venue": "Statistics and Computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Feature reinforcement learning: State of the art", "author": ["M. Daswani", "P. Sunehag", "M. Hutter"], "venue": "Workshop on Sequential Decision-making with Big Data. Association for the Advancement of Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Learning the structure of factored Markov decision processes in reinforcement learning problems", "author": ["T. Degris", "O. Sigaud", "P.-H. Wuillemin"], "venue": "In Proceedings of the 23rd International Conference on Machine learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Accelerated sampling for the Indian buffet process", "author": ["F. Doshi-Velez", "Z. Ghahramani"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Are we ready for autonomous driving? The KITTI vision benchmark suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "In Proceedings of the 25th IEEE International Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Discovering latent causes in reinforcement learning", "author": ["S.J. Gershman", "K.A. Norman", "Y. Niv"], "venue": "Current Opinion in Behavioral Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["Z. Ghahramani", "T.L. Griffiths"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Bayesian nonparametric latent feature models", "author": ["Z. Ghahramani", "P. Sollich", "T.L. Griffiths"], "venue": "In Bayesian Statistics", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "The Indian buffet process: An introduction and review", "author": ["T.L. Griffiths", "Z. Ghahramani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Inverse reinforcement learning using expectation maximization in mixture models", "author": ["J. Hahn", "A.M. Zoubir"], "venue": "In Proceedings of the 40th IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Feature reinforcement learning: Part I", "author": ["M. Hutter"], "venue": "Unstructured MDPs. Journal of Artificial General Intelligence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Spike and slab variable selection: Frequentist and Bayesian strategies", "author": ["H. Ishwaran", "J.S. Rao"], "venue": "Annals of Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Principal component analysis", "author": ["I.T. Jolliffe"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Nonparametric Bayesian sparse factor models with application to gene expression modeling", "author": ["D. Knowles", "Z. Ghahramani"], "venue": "Annals of Applied Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "Driver behavior and situation aware brake assistance for intelligent vehicles", "author": ["J.C. McCall", "M.M. Trivedi"], "venue": "Proceedings of the IEEE,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Finite Mixture Models", "author": ["G. McLachlan", "D. Peel"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "Modeling dyadic data with binary latent factors", "author": ["E. Meeds", "Z. Ghahramani", "R.M. Neal", "S.T. Roweis"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Bayesian nonparametric inverse reinforcement learning", "author": ["B. Michini", "J.P. How"], "venue": "In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Bayesian variable selection in linear regression", "author": ["T.J. Mitchell", "J.J. Beauchamp"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1988}, {"title": "Humanlevel control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A.Y. Ng", "S.J. Russell"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}, {"title": "Online feature selection for model-based reinforcement learning", "author": ["T. Nguyen", "Z. Li", "T. Silander", "T.Y. Leong"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Policy gradient approaches for multi-objective sequential decision making", "author": ["S. Parisi", "M. Pirotta", "N. Smacchia", "L. Bascetta", "M. Restelli"], "venue": "In Proceedings of the International Joint Conference on Neural Networks,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Efficient training of artificial neural networks for autonomous navigation", "author": ["D.A. Pomerleau"], "venue": "Neural Computation,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1991}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1994}, {"title": "Beam search based map estimates for the Indian buffet process", "author": ["P. Rai", "H. Daume"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "Boosting structured prediction for imitation learning", "author": ["N. Ratliff", "D. Bradley", "J.A. Bagnell", "J. Chestnutt"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Maximum margin planning", "author": ["N.D. Ratliff", "J.A. Bagnell", "M.A. Zinkevich"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "Neural fitted Q iteration \u2013 first experiences with a data efficient neural reinforcement learning method", "author": ["M. Riedmiller"], "venue": "In Proceedings of the 16th European Conference on Machine Learning,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["S. Ross", "G.J. Gordon", "D. Bagnell"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}, {"title": "An MDP-based recommender system", "author": ["G. Shani", "D. Heckerman", "R.I. Brafman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2005}, {"title": "Importance sampling for reinforcement learning with multiple objectives", "author": ["C.R. Shelton"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2001}, {"title": "The optimal control of partially observable Markov processes over a finite horizon", "author": ["R.D. Smallwood", "E.J. Sondik"], "venue": "Operations Research,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1973}, {"title": "A Bayesian approach to policy recognition and state representation learning", "author": ["A. \u0160o\u0161i\u0107", "A.M. Zoubir", "H. Koeppl"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Policy recognition via expectation maximization", "author": ["A. Sosic", "A.M. Zoubir", "H. Koeppl"], "venue": "In Proceedings of the 41st IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Bayesian nonparametric inverse reinforcement learning for switched Markov decision processes", "author": ["A. Surana", "K. Srivastava"], "venue": "In Proceedings of the 13th International Conference on Machine Learning and Applications,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Constructing stochastic mixture policies for episodic multiobjective reinforcement learning tasks", "author": ["P. Vamplew", "R. Dazeley", "E. Barker", "A. Kelarev"], "venue": "In Proceedings of the 22nd Australasian Joint Conference on Advances in Artificial Intelligence,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2009}, {"title": "A forward collision warning algorithm with adaptation to driver behaviors", "author": ["J. Wang", "C. Yu", "S.E. Li", "L. Wang"], "venue": "IEEE Transactions on Intelligent Transportation Systems,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}, {"title": "Learning from Delayed Rewards", "author": ["C.J.C.H. Watkins"], "venue": "PhD thesis,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1989}, {"title": "Deep inverse reinforcement learning", "author": ["M. Wulfmeier", "P. Ondruska", "I. Posner"], "venue": "In NIPS Deep Reinforcement Learning Workshop,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2015}], "referenceMentions": [{"referenceID": 39, "context": "Decision-making plays a crucial role in many applications, such as robot learning, driver assistance systems, and recommender systems [40].", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "Therefore, Learning From Demonstrations (LFD) [1] has gained a lot of interest in the recent past.", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "According to [1], approaches for LFD can be grouped into (i) reward-based models and (ii) imitation learning.", "startOffset": 13, "endOffset": 16}, {"referenceID": 29, "context": "In reward-based models, it is assumed that the agent makes its decision based on a reward which is, in the context of LFD, learned from observations (as in Inverse Reinforcement Learning (IRL) [30]).", "startOffset": 193, "endOffset": 197}, {"referenceID": 2, "context": "Only in the case of finite state and action spaces and known rewards, learning optimal policies has been solved [3].", "startOffset": 112, "endOffset": 115}, {"referenceID": 32, "context": "[33, 5].", "startOffset": 0, "endOffset": 7}, {"referenceID": 4, "context": "[33, 5].", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "This idea has also been investigated from a psychological point of view by the concept of discovering latent causes in human behavior, which is related to learning state space representations [12].", "startOffset": 192, "endOffset": 196}, {"referenceID": 20, "context": "Since we consider a decisionmaking task, the investigated problem can be modeled by means of a Partially Observable Markov Decision Process (POMDP) [21, 42], which is defined by \u2022 a set of observations, Z, \u2022 a set of states, X , \u2022 a finite set of Nu actions, U , \u2022 a transition model, which describes the probability of entering a state after taking an action in the current state, \u2022 an observation model which explains how the observations are generated from the states, \u2022 a discount factor, which penalizes long-term rewards, \u2022 and a reward function, R.", "startOffset": 148, "endOffset": 156}, {"referenceID": 41, "context": "Since we consider a decisionmaking task, the investigated problem can be modeled by means of a Partially Observable Markov Decision Process (POMDP) [21, 42], which is defined by \u2022 a set of observations, Z, \u2022 a set of states, X , \u2022 a finite set of Nu actions, U , \u2022 a transition model, which describes the probability of entering a state after taking an action in the current state, \u2022 an observation model which explains how the observations are generated from the states, \u2022 a discount factor, which penalizes long-term rewards, \u2022 and a reward function, R.", "startOffset": 148, "endOffset": 156}, {"referenceID": 19, "context": "A simple approach to the considered problem would be the use of a feature extraction technique such as Principal Component Analysis (PCA) [20] or Non-negative Matrix Factorization (NMF) [23] to learn features from the observed states.", "startOffset": 138, "endOffset": 142}, {"referenceID": 22, "context": "A simple approach to the considered problem would be the use of a feature extraction technique such as Principal Component Analysis (PCA) [20] or Non-negative Matrix Factorization (NMF) [23] to learn features from the observed states.", "startOffset": 186, "endOffset": 190}, {"referenceID": 2, "context": "1 Reinforcement Learning Large state and action spaces are especially problematic for value-based RL algorithms such as value-iteration [3] or Q-learning [48] since the value function, representing the expected accumulated reward, needs to be approximated.", "startOffset": 136, "endOffset": 139}, {"referenceID": 47, "context": "1 Reinforcement Learning Large state and action spaces are especially problematic for value-based RL algorithms such as value-iteration [3] or Q-learning [48] since the value function, representing the expected accumulated reward, needs to be approximated.", "startOffset": 154, "endOffset": 158}, {"referenceID": 4, "context": "In early approaches, a set of basis functions, often referred to as features, is linearly weighted to represent the this function [5].", "startOffset": 130, "endOffset": 133}, {"referenceID": 37, "context": "Riedmiller [38] has proposed the Q-fitted value iteration where the value function is approximated by means of a neural network, where the features are learned in the layers of the network.", "startOffset": 11, "endOffset": 15}, {"referenceID": 28, "context": "In [29], this approach has been extended by replacing the neural network by a deep-layered counterpart.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "A different concept to employ features is proposed by Hutter with the Feature Reinforcement Learning framework [18].", "startOffset": 111, "endOffset": 115}, {"referenceID": 7, "context": "In this framework, the goal is to learn a feature mapping from the agent\u2019s history (comprised of actions, states, and rewards) to a MDP state, enabling decision learning for infinite state spaces [8].", "startOffset": 196, "endOffset": 199}, {"referenceID": 8, "context": "An alternative framework for learning the latent structure of the state space is proposed in [9] which is based on Factored Markov Decision Processs (FMDPs) [4].", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "An alternative framework for learning the latent structure of the state space is proposed in [9] which is based on Factored Markov Decision Processs (FMDPs) [4].", "startOffset": 157, "endOffset": 160}, {"referenceID": 30, "context": "This approach has been extended in [31] to an online approach, where the features are selected from a large set by means of Group LASSO.", "startOffset": 35, "endOffset": 39}, {"referenceID": 29, "context": "2 Inverse Reinforcement Learning IRL is concerned with the problem of learning the reward function from observed behavior [30].", "startOffset": 122, "endOffset": 126}, {"referenceID": 29, "context": ", in [30, 17].", "startOffset": 5, "endOffset": 13}, {"referenceID": 16, "context": ", in [30, 17].", "startOffset": 5, "endOffset": 13}, {"referenceID": 48, "context": "Recent attempts have been made to consider a Deep Learning (DL) architecture [49] for IRL, providing means for nonlinear, hierarchical feature learning.", "startOffset": 77, "endOffset": 81}, {"referenceID": 5, "context": "A Bayesian nonparametric approach is proposed in [6], utilizing an Indian Buffet Process (IBP) to model feature activations.", "startOffset": 49, "endOffset": 52}, {"referenceID": 26, "context": "Different results on Bayesian nonparametrics for IRL, which is indirectly related to feature learning, are given in [27], where a partitioning of the state space is sought for, or [45], where complex behavior is decomposed into several, simpler behaviors that can be easily learned.", "startOffset": 116, "endOffset": 120}, {"referenceID": 44, "context": "Different results on Bayesian nonparametrics for IRL, which is indirectly related to feature learning, are given in [27], where a partitioning of the state space is sought for, or [45], where complex behavior is decomposed into several, simpler behaviors that can be easily learned.", "startOffset": 180, "endOffset": 184}, {"referenceID": 1, "context": "3 Imitation Learning Instead of estimating the reward as in IRL, imitation learning aims at inferring the underlying policy directly [2, 44].", "startOffset": 133, "endOffset": 140}, {"referenceID": 43, "context": "3 Imitation Learning Instead of estimating the reward as in IRL, imitation learning aims at inferring the underlying policy directly [2, 44].", "startOffset": 133, "endOffset": 140}, {"referenceID": 36, "context": ", in [37, 39].", "startOffset": 5, "endOffset": 13}, {"referenceID": 38, "context": ", in [37, 39].", "startOffset": 5, "endOffset": 13}, {"referenceID": 35, "context": "Attempts to introduce new features are made in [36] as an extension of the maximum margin planning algorithm which is proposed in [37].", "startOffset": 47, "endOffset": 51}, {"referenceID": 36, "context": "Attempts to introduce new features are made in [36] as an extension of the maximum margin planning algorithm which is proposed in [37].", "startOffset": 130, "endOffset": 134}, {"referenceID": 0, "context": "As explained in [1], imitation learning can be considered", "startOffset": 16, "endOffset": 19}, {"referenceID": 15, "context": "An excellent overview is given in [16].", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": "1 Feature model for learning from demonstrations We assume a linear latent feature model, similar to NMF [23] and PCA [20].", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "1 Feature model for learning from demonstrations We assume a linear latent feature model, similar to NMF [23] and PCA [20].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "Following [22], the feature matrix is composed of a binary activation matrix, A \u2208 {0, 1}K\u00d7D, and a weighting matrix, W \u2208 FK\u00d7D, where the relation is given by the Hadamard product,", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "(2) can be easily extended to an infinite feature model by placing an IBP [13, 15] prior over A.", "startOffset": 74, "endOffset": 82}, {"referenceID": 14, "context": "(2) can be easily extended to an infinite feature model by placing an IBP [13, 15] prior over A.", "startOffset": 74, "endOffset": 82}, {"referenceID": 31, "context": "Mixture polices have been investigated in multiobjective problems, where an agent aims at reaching several objectives, some of which can even be conflicting [32, 46, 41].", "startOffset": 157, "endOffset": 169}, {"referenceID": 45, "context": "Mixture polices have been investigated in multiobjective problems, where an agent aims at reaching several objectives, some of which can even be conflicting [32, 46, 41].", "startOffset": 157, "endOffset": 169}, {"referenceID": 40, "context": "Mixture polices have been investigated in multiobjective problems, where an agent aims at reaching several objectives, some of which can even be conflicting [32, 46, 41].", "startOffset": 157, "endOffset": 169}, {"referenceID": 20, "context": "Acting according to a stochastic policy maximizes the expected return [21].", "startOffset": 70, "endOffset": 74}, {"referenceID": 33, "context": "In case of single agents, it has been shown that deterministic policies are optimal solutions for MDPs [34].", "startOffset": 103, "endOffset": 107}, {"referenceID": 26, "context": "One possibility to infer this number is to utilize a Chinese Restaurant Process (CRP), giving rise to a Bayesian nonparametric model [27].", "startOffset": 133, "endOffset": 137}, {"referenceID": 42, "context": "A similar model, where the state space is clustered according to the played actions, is proposed in [43].", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "3 Prior for the feature activations The feature activations are modeled by means of an IBP [13, 15], assuming an infinite number of features.", "startOffset": 91, "endOffset": 99}, {"referenceID": 14, "context": "3 Prior for the feature activations The feature activations are modeled by means of an IBP [13, 15], assuming an infinite number of features.", "startOffset": 91, "endOffset": 99}, {"referenceID": 13, "context": "In the following, we consider the two-parameter generalization [14] which allows to sample sparse as well as dense matrices.", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "Placing a Beta prior with hyperparameters \u03b1a\u03b2a K? and \u03b2a over the parameter of the Binomial distribution and marginalizing over this parameter yields a Beta-Binomial distribution [13, 14].", "startOffset": 179, "endOffset": 187}, {"referenceID": 13, "context": "Placing a Beta prior with hyperparameters \u03b1a\u03b2a K? and \u03b2a over the parameter of the Binomial distribution and marginalizing over this parameter yields a Beta-Binomial distribution [13, 14].", "startOffset": 179, "endOffset": 187}, {"referenceID": 12, "context": "Since we are interested in sampling from an infinite number of features, we consider the limit for K \u2192\u221e, resulting in the distribution of the activation matrix A [13, 14],", "startOffset": 162, "endOffset": 170}, {"referenceID": 13, "context": "Since we are interested in sampling from an infinite number of features, we consider the limit for K \u2192\u221e, resulting in the distribution of the activation matrix A [13, 14],", "startOffset": 162, "endOffset": 170}, {"referenceID": 12, "context": "Thus, we need to store only rows with active elements in memory, which can be understood as realizations of K features, where the average number of active rows is given by K\u0304 = \u03b1a \u2211D d=1 \u03b2a \u03b2a+d\u22121 [13, 10].", "startOffset": 197, "endOffset": 205}, {"referenceID": 9, "context": "Thus, we need to store only rows with active elements in memory, which can be understood as realizations of K features, where the average number of active rows is given by K\u0304 = \u03b1a \u2211D d=1 \u03b2a \u03b2a+d\u22121 [13, 10].", "startOffset": 197, "endOffset": 205}, {"referenceID": 13, "context": "The hyperparameters \u03b1a and \u03b2a reflect our prior belief about the number of features and the sparsity of the matrix [14].", "startOffset": 115, "endOffset": 119}, {"referenceID": 21, "context": ", \u03b1a \u223c Ga\u03b1a ( h (1) \u03b1A , h (2) \u03b1A ) and \u03b2a \u223c Ga\u03b2a ( h (1) \u03b2A , h (2) \u03b2A ) [22].", "startOffset": 74, "endOffset": 78}, {"referenceID": 27, "context": "In particular, we consider a sparsity-promoting mixture prior on the substates, similar to a spike and slab model [28, 19].", "startOffset": 114, "endOffset": 122}, {"referenceID": 18, "context": "In particular, we consider a sparsity-promoting mixture prior on the substates, similar to a spike and slab model [28, 19].", "startOffset": 114, "endOffset": 122}, {"referenceID": 6, "context": "We use the algorithm presented in [7] to sample from a truncated Gaussian distribution.", "startOffset": 34, "endOffset": 37}, {"referenceID": 12, "context": "(11) is [13, 14]", "startOffset": 8, "endOffset": 16}, {"referenceID": 13, "context": "(11) is [13, 14]", "startOffset": 8, "endOffset": 16}, {"referenceID": 9, "context": "In the second step, K new features are proposed in a Metropolis step [10, 22].", "startOffset": 69, "endOffset": 77}, {"referenceID": 21, "context": "In the second step, K new features are proposed in a Metropolis step [10, 22].", "startOffset": 69, "endOffset": 77}, {"referenceID": 12, "context": "The probability of adding K features is given as [13, 14]", "startOffset": 49, "endOffset": 57}, {"referenceID": 13, "context": "The probability of adding K features is given as [13, 14]", "startOffset": 49, "endOffset": 57}, {"referenceID": 25, "context": "As the proposal distribution is independent of the previous sample, the acceptance ratio, r, is equal to the likelihood ratio between the new and existing features [26],", "startOffset": 164, "endOffset": 168}, {"referenceID": 21, "context": "Since the IBP tends to mix slowly, we augment this ratio with probability P of accepting a single new feature, resulting in a modified acceptance ratio which is derived in [22].", "startOffset": 172, "endOffset": 176}, {"referenceID": 21, "context": "The hyperparameters \u03b1a and \u03b2a are sampled as described in [22].", "startOffset": 58, "endOffset": 62}, {"referenceID": 24, "context": ", Nz, for each observation, indicating from which policy the observed action, un, has been generated [25].", "startOffset": 101, "endOffset": 105}, {"referenceID": 34, "context": ", estimates of the features, F\u0302MAP [35], and the policies, \u03a6\u0302MAP.", "startOffset": 35, "endOffset": 39}, {"referenceID": 23, "context": "We consider the problem of analyzing a driver\u2019s behavior, which is an important task for useradaptive driver assistance systems [24, 47], in order to demonstrate the performance of the proposed model in a real-world scenario.", "startOffset": 128, "endOffset": 136}, {"referenceID": 46, "context": "We consider the problem of analyzing a driver\u2019s behavior, which is an important task for useradaptive driver assistance systems [24, 47], in order to demonstrate the performance of the proposed model in a real-world scenario.", "startOffset": 128, "endOffset": 136}, {"referenceID": 10, "context": "For this, we consider real data provided by the KITTI Vision Benchmark Suite [11] containing several challenges in urban driving.", "startOffset": 77, "endOffset": 81}], "year": 2017, "abstractText": "Learning from demonstrations has gained increasing interest in the recent past, enabling an agent to learn how to make decisions by observing an experienced teacher. While many approaches have been proposed to solve this problem, there is only little work that focuses on reasoning about the observed behavior. We assume that, in many practical problems, an agent makes its decision based on latent features, indicating a certain action. Therefore, we propose a generative model for the states and actions. Inference reveals the number of features, the features, and the policies, allowing us to learn and to analyze the underlying structure of the observed behavior. Further, our approach enables prediction of actions for new states. Simulations are used to assess the performance of the algorithm based upon this model. Moreover, the problem of learning a driver\u2019s behavior is investigated, demonstrating the performance of the proposed model in a real-world scenario.", "creator": "LaTeX with hyperref package"}}}