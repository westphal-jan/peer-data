{"id": "1702.08074", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2017", "title": "Learning Control for Air Hockey Striking using Deep Reinforcement Learning", "abstract": "We consider the task of learning control policies for a robotic mechanism striking a puck in an air hockey game. The control signal is a direct command to the robot's motors. We employ a model free deep reinforcement learning framework to learn the motoric skills of striking the puck accurately in order to score. We propose certain improvements to the standard learning scheme which make the deep Q-learning algorithm feasible when it might otherwise fail. Our improvements include integrating prior knowledge into the learning scheme, and accounting for the changing distribution of samples in the experience replay buffer. Finally we present our simulation results for aimed striking which demonstrate the successful learning of this task, and the improvement in algorithm stability due to the proposed modifications.", "histories": [["v1", "Sun, 26 Feb 2017 19:59:59 GMT  (1091kb,D)", "https://arxiv.org/abs/1702.08074v1", null], ["v2", "Tue, 25 Apr 2017 10:52:33 GMT  (1224kb,D)", "http://arxiv.org/abs/1702.08074v2", "Corrected typos Graphs added in results section"]], "reviews": [], "SUBJECTS": "cs.LG cs.RO", "authors": ["ayal taitler", "nahum shimkin"], "accepted": false, "id": "1702.08074"}, "pdf": {"name": "1702.08074.pdf", "metadata": {"source": "CRF", "title": "Learning Control for Air Hockey Striking using Deep Reinforcement Learning", "authors": ["Ayal Taitler"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The problem of learning a skill, a mapping between states and actions to achieve a goal in a continuous world lies at the heart of any interaction of an autonomous system with its environment. In this paper we consider the problem of a robot that effectively beats the puck in a multi-step process of planning and execution. First, the planning of a strategy based on the goal and the ability to calculate the best point of the collision in order to block and win it."}, {"heading": "2 Related Work", "text": "Research on learning in autonomous systems has been conducted in several directions. Our work has been mainly influenced by recent work on deep Q-networks [11, 12, 22, 24] and adaptation to continuous areas of deep deterministic political gradients [9]. Since the ground-breaking results that Deep Q-Learning has shown for playing games on the Atari 2600 arcade environment, there has been extensive research on deep reinforcement learning. In particular, Deep Q-Learning seeks to approximate Q-values [25] using deep networks, such as deep Convolutionary Neural Networks. There has also been work on better goal assessment [24], improving learning by prioritizing the experience buffer to maximize learning [19] and better gradient updates using parallel batch approaches [10, 14]. Some work on adaptation to the continuous control domain has also been done by [9]."}, {"heading": "3 Deep Q-Networks", "text": "We consider a standard amplification consisting of an agent interacting with the environment in discrete time steps. At each step, the agent receives an observation st * Rn representing the current physical state of the system, adopts an action that he applies to the environment, receives a scalar reward rt = r (st, at) and observes a new state st + 1 in which the environmental transitions are to be maximized. It is assumed that the next state is related to a stochastic transition model P (st + 1 | st, at). The objective of the agent is to maximize the sum of the rewards gained from interacting with the environment. Our problem is a limited horizon problem in which the game reaches a predefined time. We define the future return to time t as Rt = p T \u00b2 s, in which T is the time in which the game ends."}, {"heading": "4 Striking in Air Hockey", "text": "Next, we present the conspicuous problem and our approach to learning.4.1 The conspicuous problem generally deals with the interception of a moving puck and its impact in a controlled manner. We specialize here in the case in which the puck is stationary. We want to learn the control policy to hit the puck in such a way that after impact the puck orbit will have some desired characteristics. We focus on learning to bring the puck directly to the opponent's target. We also look at some other modes to hit the puck first. These are not presented here, but the same learning pattern fits them too. We refer to these modes as skills that a high-level agent can choose from full air hockey play. The learning goal is to learn these skills with the following desired characteristics \u2022 the puck velocity should be maximum according to the effect with the agent. \u2022 The end position of the puck on the opponent's side should be the center of the target."}, {"heading": "11 else", "text": "12 Probably select random actions at otherwise selectat = PA {argmax a Q (st, a; \u03b8) + nt} 13 Execute actions at around and observe reward rt, next state xt + 1 and if leadership dt + 1 14 Set new state st + 1 = st, xt + 1 15 Store transition < st, at, rt, st + 1 > in D 16 Sample random mini-batch of transition < sj, aj, rj, sj + 1 > from D17 set yt = rt + Q (sj + 1, argmax a Q (sj + 1, a; \u03b8);."}, {"heading": "5 Experiments", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live."}, {"heading": "6 Conclusions", "text": "We investigated the use of a stationary puck with a physical mechanism. We showed that the standard DQN algorithm did not produce satisfactory results. Therefore, we proposed two new improvements to this algorithm. 1. We used already existing knowledge during the learning process to steer the algorithm to interesting regions of the state and action spaces. 2. We used non-uniform target update periods with an expansion rate to stabilize the learning. We also expanded the commonly used - greedy exploration mechanism to include local exploration with time-correlated random processes to better adapt the physical environment. The modified algorithm shows that it achieves near-optimal performance in motion planning and control of air hockey hitting. Specifically, it completely solves the problem of point drop observed in Double DQN."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "In Proceedings of the twenty-first international conference on Machine learning", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and autonomous systems 57,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "A framework for learning from observation using primitives", "author": ["D.C. Bentivegna", "C.G. Atkeson"], "venue": "In Robot Soccer World Cup", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Catastrophic forgetting in connectionist networks", "author": ["R.M. French"], "venue": "Trends in cognitive sciences 3,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Truncated natural policy gradient", "author": ["S.M. Kakade"], "venue": "NIPS", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Reinforcement learning in robotics: A survey", "author": ["J. Kober", "J.A. Bagnell", "J. Peters"], "venue": "The International Journal of Robotics Research", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": "Nature 518,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Learning table tennis with a mixture of motor primitives", "author": ["K. Muelling", "J. Kober", "J. Peters"], "venue": "In 2010 10th IEEE-RAS International Conference on Humanoid Robots (2010),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "A. De Maria", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S Petersen"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Hierarchical processing architecture for an air-hockey robot system", "author": ["A. Namiki", "S. Matsushita", "T. Ozeki", "K. Nonami"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Control of planar rigid body sliding with impacts and friction", "author": ["C.B. Partridge", "M.W. Spong"], "venue": "The International Journal of Robotics Research 19,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Reinforcement learning by reward-weighted regression for operational space control", "author": ["J. Peters", "S. Schaal"], "venue": "In Proceedings of the 24th international conference on Machine learning (2007),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Learning from demonstration. Advances in neural information processing systems", "author": ["S. Schaal"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1997}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M Lanctot"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "On the theory of the brownian motion", "author": ["G.E. Uhlenbeck", "L.S. Ornstein"], "venue": "Physical review 36,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1930}, {"title": "Deep reinforcement learning with double q-learning", "author": ["H. Van Hasselt", "A. Guez", "D. Silver"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning 8,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1992}], "referenceMentions": [{"referenceID": 13, "context": ", calculating the best point of collision to achieve the goal, then planning a path and trajectory and finally executing the low level motoric control [15].", "startOffset": 151, "endOffset": 155}, {"referenceID": 21, "context": "Such problems include policy gradients [26] where a mapping between states and actions is learned with gradient ascent optimization on the accumulated reward, with or without keeping track of the value function.", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "Another popular approach is Learning from Demonstration (LfD) [2, 18] sometimes refereed as imitation learning [13] and apprenticeship learning [1].", "startOffset": 62, "endOffset": 69}, {"referenceID": 16, "context": "Another popular approach is Learning from Demonstration (LfD) [2, 18] sometimes refereed as imitation learning [13] and apprenticeship learning [1].", "startOffset": 62, "endOffset": 69}, {"referenceID": 11, "context": "Another popular approach is Learning from Demonstration (LfD) [2, 18] sometimes refereed as imitation learning [13] and apprenticeship learning [1].", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "Another popular approach is Learning from Demonstration (LfD) [2, 18] sometimes refereed as imitation learning [13] and apprenticeship learning [1].", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "Paper [3] used imitation learning to learn primitive behaviors for a humanoid robot in air hockey.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "-greedy exploration which is the most common one, is not highly efficient in such systems, since a dynamical system functions as a low pass filter [8] and once in a while using a random action might have little affect on the output of the system.", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "The algorithm combined -greedy exploration with a temporally correlated noise [9] for local exploration, which proved to be essential for effective learning.", "startOffset": 78, "endOffset": 81}, {"referenceID": 9, "context": "Our work has been influenced mainly from the recent work of deep Q-networks [11, 12, 22, 24], and the adaptation for continuous domains of deep deterministic policy gradients [9].", "startOffset": 76, "endOffset": 92}, {"referenceID": 10, "context": "Our work has been influenced mainly from the recent work of deep Q-networks [11, 12, 22, 24], and the adaptation for continuous domains of deep deterministic policy gradients [9].", "startOffset": 76, "endOffset": 92}, {"referenceID": 18, "context": "Our work has been influenced mainly from the recent work of deep Q-networks [11, 12, 22, 24], and the adaptation for continuous domains of deep deterministic policy gradients [9].", "startOffset": 76, "endOffset": 92}, {"referenceID": 20, "context": "Our work has been influenced mainly from the recent work of deep Q-networks [11, 12, 22, 24], and the adaptation for continuous domains of deep deterministic policy gradients [9].", "startOffset": 76, "endOffset": 92}, {"referenceID": 7, "context": "Our work has been influenced mainly from the recent work of deep Q-networks [11, 12, 22, 24], and the adaptation for continuous domains of deep deterministic policy gradients [9].", "startOffset": 175, "endOffset": 178}, {"referenceID": 20, "context": "There has also been work on better target estimation [24], improving the learning by prioritizing the experience replay buffer to maximize learning [19] and preforming better gradient updates with parallel batch approaches [10, 14].", "startOffset": 53, "endOffset": 57}, {"referenceID": 8, "context": "There has also been work on better target estimation [24], improving the learning by prioritizing the experience replay buffer to maximize learning [19] and preforming better gradient updates with parallel batch approaches [10, 14].", "startOffset": 223, "endOffset": 231}, {"referenceID": 12, "context": "There has also been work on better target estimation [24], improving the learning by prioritizing the experience replay buffer to maximize learning [19] and preforming better gradient updates with parallel batch approaches [10, 14].", "startOffset": 223, "endOffset": 231}, {"referenceID": 7, "context": "Some work on adaptation to the continuous control domain has been done also by [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "Policy gradients methods were traditionally used [7,17,26], but struggled as the number of parameters increased.", "startOffset": 49, "endOffset": 58}, {"referenceID": 15, "context": "Policy gradients methods were traditionally used [7,17,26], but struggled as the number of parameters increased.", "startOffset": 49, "endOffset": 58}, {"referenceID": 21, "context": "Policy gradients methods were traditionally used [7,17,26], but struggled as the number of parameters increased.", "startOffset": 49, "endOffset": 58}, {"referenceID": 17, "context": "Adaptation to the deep neural network framework has also been done in recent years [20, 21].", "startOffset": 83, "endOffset": 91}, {"referenceID": 3, "context": "Several benchmarks such as [5] have made comparisons between continuous control algorithms.", "startOffset": 27, "endOffset": 30}, {"referenceID": 14, "context": "We assume that f(\u00b7), the collision models and the table state constraints are hidden from the learning algorithm, The best known collision model is non-linear and hard to work with [16].", "startOffset": 181, "endOffset": 185}, {"referenceID": 7, "context": "Similarly to what was done in [9], we added a noise sampled from a noise process N to our currently learned policy.", "startOffset": 30, "endOffset": 33}, {"referenceID": 19, "context": "We used forN an Ornstein\u2013Uhlenbeck process [23] to generate temporally correlated exploration noise for exploring efficiently.", "startOffset": 43, "endOffset": 47}, {"referenceID": 4, "context": "might suffer from what is known as the catastrophic forgetting [6] of neural networks.", "startOffset": 63, "endOffset": 66}], "year": 2017, "abstractText": "We consider the task of learning control policies for a robotic mechanism striking a puck in an air hockey game. The control signal is a direct command to the robot\u2019s motors. We employ a model free deep reinforcement learning framework to learn the motoric skills of striking the puck accurately in order to score. We propose certain improvements to the standard learning scheme which make the deep Q-learning algorithm feasible when it might otherwise fail. Our improvements include integrating prior knowledge into the learning scheme, and accounting for the changing distribution of samples in the experience replay buffer. Finally we present our simulation results for aimed striking which demonstrate the successful learning of this task, and the improvement in algorithm stability due to the proposed modifications.", "creator": "LaTeX with hyperref package"}}}