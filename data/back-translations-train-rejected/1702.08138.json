{"id": "1702.08138", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Deceiving Google's Perspective API Built for Detecting Toxic Comments", "abstract": "Social media platforms provide an environment where people can freely engage in discussions. Unfortunately, they also enable several problems, such as online harassment. Recently, Google and Jigsaw started a project called Perspective, which uses machine learning to automatically detect toxic language. A demonstration website has been also launched, which allows anyone to type a phrase in the interface and instantaneously see the toxicity score [1]. In this paper, we propose an attack on the Perspective toxic detection system based on the adversarial examples. We show that an adversary can subtly modify a highly toxic phrase in a way that the system assigns significantly lower toxicity score to it. We apply the attack on the sample phrases provided in the Perspective website and show that we can consistently reduce the toxicity scores to the level of the non-toxic phrases. The existence of such adversarial examples is very harmful for toxic detection systems and seriously undermines their usability.", "histories": [["v1", "Mon, 27 Feb 2017 04:07:36 GMT  (9kb)", "http://arxiv.org/abs/1702.08138v1", "4 pages"]], "COMMENTS": "4 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CY cs.SI", "authors": ["hossein hosseini", "sreeram kannan", "baosen zhang", "radha poovendran"], "accepted": false, "id": "1702.08138"}, "pdf": {"name": "1702.08138.pdf", "metadata": {"source": "CRF", "title": "Deceiving Google\u2019s Perspective API Built for Detecting Toxic Comments", "authors": ["Hossein Hosseini", "Sreeram Kannan", "Baosen Zhang"], "emails": ["rp3}@uw.edu"], "sections": [{"heading": null, "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own."}, {"heading": "II. BACKGROUND", "text": "A. A Brief Description of Google's Perspective APIPerspective is an API developed by Jigsaw and Google's Counter Abuse Technology Team in Conversation AI. Conversation AI is collaborative research that examines ML as a tool for better online discussion [17]. The API uses machine learning models to assess the toxicity of an input text, defining toxic comments as \"a rude, disrespectful, or unreasonable comment likely to cause you to leave a discussion.\" Google and Jigsaw developed the measurement tool by taking millions of comments from various publishers and then asking panels of ten people to rate the comments on a scale of \"very toxic\" to \"very healthy.\" The resulting judgments provided a large number of training examples for the machine learning model. Jigsaw has teamed up with online communities and publishers to automatically implement the New York Times discussion system and its first-time editors."}, {"heading": "B. Adversarial Examples for Learning Systems", "text": "One of the weaknesses of machine learning algorithms is that an adversary can alter the algorithm's predictive value by easily disrupting the input, often unnoticed by humans. Such input is referred to as contrary examples [15]. Contrary examples have been applied to models for different tasks, such as classifying images [15], [18], [19], analyzing music content [20] and classifying malware [21]. In this work, we generate contrary examples on a real text classification system. In the context of assessing toxicity, contrary examples can be defined as modified phrases containing the same highly abusive language as the original, but with significantly lower toxicity. In a similar paper [22], contrary examples are used to maintain the gender classification of the text, while the gender classification method is presented as useless."}, {"heading": "III. THE PROPOSED ATTACKS", "text": "In fact, most of them are able to play by the rules."}, {"heading": "IV. OPEN PROBLEMS IN DEFENSE METHODS", "text": "The developers of Perspective have mentioned that the system is in the early days of research and development, and that the experiments, models and research data are published to explore the strengths and weaknesses of the use of machine learning as a tool for online discussion. In Section III, we have shown the vulnerability of the perspective system to adverse examples. Assessing the semantic toxicity of a formulation is clearly a very challenging task. Below, we briefly review some of the possible approaches to improving the robustness of toxic detection systems: \u2022 Adversarial Training: In this approach, we generate the adverse examples during the training phase and train the model to assign them the original designation [18]. In the context of toxic detection systems, we need to include various modified versions of toxic words in the training data. While this approach can improve the robustness of the system against adverse examples, it does not seem practical to train the model on all variants of each word."}, {"heading": "V. CONCLUSION", "text": "We showed that the system can be deceived by minor disturbance of abusive phrases to obtain very low toxicity values. [1] We also showed that the system exhibits a high false alarm rate in scoring high toxicity to harmless phrases. [1] We provided detailed examples of the cases investigated. Our future work includes developing countermeasures against such attacks. [1] \"The phrases used in Tables I and II are from the examples provided on the Perspective website. [1] For the purpose of demonstrating the results and not displaying the views or opinions of the authors or sponsoring agencies.XiXiXiXiXiXies.REFERENCES.\" \"https: / / www.perspectiveapi.com /,\" [2] M. Duggan, online harassment."}], "references": [{"title": "Online harassment", "author": ["M. Duggan"], "venue": "Pew Research Center,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Ex machina: Personal attacks seen at scale", "author": ["E. Wulczyn", "N. Thain", "L. Dixon"], "venue": "arXiv preprint arXiv:1610.08914, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pp. 1097\u20131105, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Context-dependent pretrained deep neural networks for large-vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, pp. 160\u2013 167, ACM, 2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Can machine learning be secure", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "venue": "Proceedings of the 2006 ACM Symposium on Information, computer and communications security, pp. 16\u201325, ACM, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "The security of machine learning", "author": ["M. Barreno", "B. Nelson", "A.D. Joseph", "J. Tygar"], "venue": "Machine Learning, vol. 81, no. 2, pp. 121\u2013148, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Adversarial machine learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B.I. Rubinstein", "J. Tygar"], "venue": "Proceedings of the 4th ACM workshop on Security and artificial intelligence, pp. 43\u201358, ACM, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "arXiv preprint arXiv:1312.6199, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "Z.B. Celik", "A. Swami"], "venue": "arXiv preprint arXiv:1602.02697, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "arXiv preprint arXiv:1412.6572, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. Papernot", "P. McDaniel", "S. Jha", "M. Fredrikson", "Z.B. Celik", "A. Swami"], "venue": "2016 IEEE European Symposium on Security and Privacy (EuroS&P), pp. 372\u2013387, IEEE, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning and music adversaries", "author": ["C. Kereliuk", "B.L. Sturm", "J. Larsen"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 11, pp. 2059\u2013 2071, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["K. Grosse", "N. Papernot", "P. Manoharan", "M. Backes", "P. McDaniel"], "venue": "arXiv preprint arXiv:1606.04435, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Obfuscating gender in social media writing", "author": ["S. Reddy", "M. Wellesley", "K. Knight", "C. Marina del Rey"], "venue": "NLP+ CSS 2016, p. 17, 2016. 4", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Unfortunately, the lack of a moderating entity in these platforms has caused several problems, ranging from the wide spread of fake news to online harassment [2].", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "As a result, there has been many calls for researchers to develop methods to automatically detect abusive or toxic context in the real time [6].", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "Recent advances in machine learning have transformed many domains such as computer vision [7], speech recognition [8], and language processing [9].", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "Recent advances in machine learning have transformed many domains such as computer vision [7], speech recognition [8], and language processing [9].", "startOffset": 114, "endOffset": 117}, {"referenceID": 4, "context": "Recent advances in machine learning have transformed many domains such as computer vision [7], speech recognition [8], and language processing [9].", "startOffset": 143, "endOffset": 146}, {"referenceID": 5, "context": "However, many works have pointed out their vulnerability in adversarial scenarios [12]\u2013 [14].", "startOffset": 82, "endOffset": 86}, {"referenceID": 7, "context": "However, many works have pointed out their vulnerability in adversarial scenarios [12]\u2013 [14].", "startOffset": 88, "endOffset": 92}, {"referenceID": 8, "context": "Such inputs are called adversarial examples [15], and have been shown to be effective against different machine learning algorithms even when the adversary has only a blackbox access to the target model [16].", "startOffset": 44, "endOffset": 48}, {"referenceID": 9, "context": "Such inputs are called adversarial examples [15], and have been shown to be effective against different machine learning algorithms even when the adversary has only a blackbox access to the target model [16].", "startOffset": 203, "endOffset": 207}, {"referenceID": 5, "context": "As a result, they are subject to attacks in adversarial scenarios [12]\u2013 [14].", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "As a result, they are subject to attacks in adversarial scenarios [12]\u2013 [14].", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "Such inputs are called adversarial examples [15].", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "Adversarial examples have been applied to models for different tasks, such as images classification [15], [18], [19], music content analysis [20] and malware classification [21].", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "Adversarial examples have been applied to models for different tasks, such as images classification [15], [18], [19], music content analysis [20] and malware classification [21].", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "Adversarial examples have been applied to models for different tasks, such as images classification [15], [18], [19], music content analysis [20] and malware classification [21].", "startOffset": 112, "endOffset": 116}, {"referenceID": 12, "context": "Adversarial examples have been applied to models for different tasks, such as images classification [15], [18], [19], music content analysis [20] and malware classification [21].", "startOffset": 141, "endOffset": 145}, {"referenceID": 13, "context": "Adversarial examples have been applied to models for different tasks, such as images classification [15], [18], [19], music content analysis [20] and malware classification [21].", "startOffset": 173, "endOffset": 177}, {"referenceID": 14, "context": "In a similar work [22], the authors presented a method for gender obfuscating in social media writing.", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "\u2022 Adversarial Training: In this approach, during the training phase, we generate the adversarial examples and train the model to assign the original label to them [18].", "startOffset": 163, "endOffset": 167}], "year": 2017, "abstractText": "Social media platforms provide an environment where people can freely engage in discussions. Unfortunately, they also enable several problems, such as online harassment. Recently, Google and Jigsaw started a project called Perspective, which uses machine learning to automatically detect toxic language. A demonstration website has been also launched, which allows anyone to type a phrase in the interface and instantaneously see the toxicity score [1]. In this paper, we propose an attack on the Perspective toxic detection system based on the adversarial examples. We show that an adversary can subtly modify a highly toxic phrase in a way that the system assigns significantly lower toxicity score to it. We apply the attack on the sample phrases provided in the Perspective website and show that we can consistently reduce the toxicity scores to the level of the non-toxic phrases. The existence of such adversarial examples is very harmful for toxic detection systems and seriously undermines their usability.", "creator": "LaTeX with hyperref package"}}}