{"id": "1702.08159", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "F2F: A Library For Fast Kernel Expansions", "abstract": "F2F is a C++ library for large-scale machine learning. It contains a CPU optimized implementation of the Fastfood algorithm, that allows the computation of approximated kernel expansions in loglinear time. The algorithm requires to compute the product of Walsh-Hadamard Transform (WHT) matrices. A cache friendly SIMD Fast Walsh-Hadamard Transform (FWHT) that achieves compelling speed and outperforms current state-of-the-art methods has been developed. F2F allows to obtain non-linear classification combining Fastfood and a linear classifier.", "histories": [["v1", "Mon, 27 Feb 2017 06:30:47 GMT  (13kb,D)", "http://arxiv.org/abs/1702.08159v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["joachim curto", "irene zarza", "feng yang", "alexander j smola", "luc van gool"], "accepted": false, "id": "1702.08159"}, "pdf": {"name": "1702.08159.pdf", "metadata": {"source": "CRF", "title": "F2F: A Library for Fast Kernel Expansions", "authors": ["Joachim Curto", "Irene Zarza", "Feng Yang", "Alexander J. Smola", "Luc Van Gool"], "emails": ["curto@vision.ee.ethz.ch", "zarza@vision.ee.ethz.ch", "fengyang@google.com", "alex@smola.org", "vangool@vision.ee.ethz.ch"], "sections": [{"heading": null, "text": "Keywords: Approximate kernel extensions, Fast Walsh-Hadamard Transform, SIMD, Gaussian RBF kernel, Matern kernel"}, {"heading": "1. Introduction", "text": "To solve this problem, Le et al. (2013) proposed an approach algorithm called fast food, based on Random Kitchen Sinks by Rahimi and Recht (2007), which accelerates the creation of a wide range of core functions and allows us to use them in large amounts of data. At its core, fast food requires scalar multiplications, a permutation, access to trigonometric functions, and two Walsh-Hadamard transformations (WHT) for implementation. The main computing bottlenecks here are the WHT. We offer fast, cache-friendly SIMD-oriented implementation, the SIMD functions and the SIMD classification."}, {"heading": "2. Fastfood and its Computational Complexity", "text": "It is the first time that we see ourselves in a position to go in search of a solution. (We are the first time that we see ourselves in a position to find a solution. (We are the second time that we find a solution.) (We are the second time that we find a solution.) We are the second time that we find a solution. (We are the second time that we find a solution.) (We are the second time that we find a solution.) We are the first time that we find a solution. (We are the second time that we find a solution.) We are the second time that we find a solution. (We are the second time that we find a solution.) We are the second time that we find a solution. (We are the second time that we find a solution.) \"We are the second time that we find a solution.\" (We are the second time that we find a solution.) We are the third time that we find a solution. (We are the second time that we find a solution.) We are the third time that we find a solution. (We are the second time that we find a solution.) We are the second time that we find a solution. (We are the second time that we find a solution.) We are the second time that we are the second time that we find a solution. (We are the second time that we find a solution.) We are the second time that we are the second time that we find a solution. (We are the second time that we find a solution.) We are the second time that we find a solution. (We are the second time that we find a solution.) We are the second time that we find a solution. \"We are the second time that we find a solution.\" (We are the second time that we find a solution. \"(We are the second time that we are the second time that we find a solution.) We are the third time that we find a solution. (We are the third time that we are the third time that we find a solution.) We are the third time that we are the third time that we find a solution. (We are the third time that we are the third time that we find a solution. (We are the third time that we find a solution.) We are the third time that we are the third time that we are the third time that we are the"}, {"heading": "3. Fast Walsh-Hadamard Transform Implementation", "text": "A central component of the library is an efficient implementation of the Fast Walsh-Hadamard transform. In particular, F2F offers considerable improvements over the Spiral2 library due to automatic code generation, the use of Intel SIMD intrinsics (SSE2 with 128-bit registers), and loop processing. This reduces memory overhead. F2F works with vectorized sums and subtractions iterative for the first n 2 kinput vector positions (where n is the length of the input matrix and k is the iteration starting from 1), calculates the intermediate operations of the Cooley-Tukey matrix algorithm until a small Hadamard routine fits into the cache. Then, the algorithm continues in the same way, but starting from the smallest length and doubling of the matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix mat"}, {"heading": "4. Software Description", "text": "The API for the implementation of fast food follows the pattern of the factory design. That is, while the fast food object is relatively general in terms of the calculation of properties, we have a factory that serves as a means of instantiating the parameters according to predefined parameter sets for e.g. a Gaussian RBF or a Mater kernel. The parameters chosen in this way are deterministic, given by the values of a hash function. The advantage of this approach is that there is no need to store the coefficients generated for fast food when using the functions. That is, we also provide a constructor that consumes B, VP, G and S to enable arbitrary cores."}, {"heading": "4.2 Design", "text": "The instance of the F2F class initializes the object by calculating and storing the B and \u0438 diagonal matrices. In the factory design, you can specify the creation of the matrix G and S by calling the correct kernel constructor. F2F objects contain the following methods: \u2022 ff _ fastfood (float * data, float * characteristics) calculates the characteristics using the real version of the complex characteristic map \u03c6 in Rahimi and Recht (2007), \u03c6 \u2032 (x) = n \u2212 1 2 {cos ([V x] j), sin ([V x] j)}. Vectorized instructions and cache locality of SIMD are used to increase the speed performance. To avoid a bottleneck in the basic trigonometric function calculation, an SSE2 sincos function 3 is used, which enables a speed improvement of 18 times for a 224-dimensional input matrix. \u2022 float calculates the weight (internal *) of the product."}, {"heading": "4.3 Practical Use", "text": "To illustrate how F2F can be embedded in the code header, definitions and examples are included in the appendix. B.F2F can be embedded in front of a linear classifier, for example, first by feature extraction, then by F2F and finally by Linear SVM. Another example is the embedding of F2F in a deep neural network architecture either as a non-linear mapping to the activation function or as a separate fast food layer."}, {"heading": "Appendix A: How to use FWHT", "text": "If you only want to use FWHT and no other functionality of this library, add # hpp / fast2food.hpp to your test file and follow the following usage guidelines."}, {"heading": "Description", "text": "fwht _ opt calculates the Fast Walsh Hadamard transform of a given input vector on the spot."}, {"heading": "Header", "text": "void fwht _ opt (float * data, unsigned long lgn)"}, {"heading": "Parameters", "text": "\u2022 data: input data vector \u2022 lgn: log2 input vector length"}, {"heading": "Usage", "text": "1 / / In \u2212 p lace FWHT input vec to r with dimension dim 2 fwht opt (data, l og2 (dim)); If you are interested in using the function for an n \u2212 input vector matrix, proceed as follows: 1 / / In \u2212 p lace FWHT n \u2212 input vec to r matrix with dimension dim 2 for (unsigned long i = 0; i < + + i) 3 fwht opt (data + i \u0445 dim, log2 (dim))); Appendix B: How to embed F2F2F in the code, insert # include \"hpp / fast2food _ factory.hpp\" into your test file and follow the following usage guidelines. F2F FactoryF2F Factory allows you to generate F2F objects according to your specific kernel choice."}, {"heading": "Standard version", "text": "The standard version is designed for a machine system. createFast2food initializes the Fast2food object by specifying the initialization parameters."}, {"heading": "Header", "text": "static Fast2food * createFast2food (Fast2foodType fast2foodType, float * data, const unsigned long nvec, const unsigned long dim, const unsigned long D, const float sigma = 1.0, const unsigned long t = 5);"}, {"heading": "Parameters", "text": "\u2022 fast2foodType: Fast2foodFactory:: RBF or Fast2foodFactory:: Matern \u2022 data: input data matrix \u2022 nvec: number of feature vectors \u2022 dim: feature dimension \u2022 D: number of base kernel extensions \u2022 sigma: RBF sigma parameter \u2022 t: experimental matern kernel parameter"}, {"heading": "Class atributes", "text": "\u2022 ff _ dim: feature dimension, type unsigned long. \u2022 ff _ dim: feature dimension, type unsigned long. \u2022 ff _ dimad: feature dimension, type unsigned long. \u2022 ff _ D: integer part of the quotient between the number of base extensions D and ff _ dimpad, type unsigned long. \u2022 ff _ dimD: number of kernel base extensions, defined as product ff _ D * ff _ dimpad, type unsigned long. \u2022 ff _ data: pointer input data matrix, type float. \u2022 ff _ dataout: pointer internal data matrix, type float. \u2022 diag _ Pi: random permutation diagonal matrix, type unsigned long vector. \u2022 diag _ G: random Gaussian diagonal matrix G, type float vector. \u2022 diag _ S: scaling diagonal matrix, type float vector, type long vector, type diag, type random vector: \u00b1 B."}, {"heading": "Methods", "text": "\u2022 ff _ fastfood (): calculates fast food and stores the result in ff _ dataout. \u2022 ff _ features (): calculates fast food features and returns the result in a float pointer."}, {"heading": "Usage", "text": "1 / / Fast2food with Gaussian RBF ke rne l 2 Fast2food \u0445 f a s t 2 f o od = 3 Fast2foodFactory:: c r ea t eFas t2 food (Fast2foodFactory:: RBF, datain, nvec, dim, D, sigma); 4 5 f loat, da t a f e a t u r e s; 6 f a s t2 f ood \u2212 > f f f f f f f f a s s (); / / complex map computation1 / / / Fast2food with Matern RBF ke rne l 2 Fast2food, f f f f f f f f a s utation (f), f / s utFactory: c r ea t eFas t2 food (Fast2foodFactory: Matern RBF ke rne l 2 Fast2food, f f f f f f f f f, f f); f f f / s utation (f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f, f); f f, f, f, f, f"}, {"heading": "Distributed oriented version", "text": "The distributed oriented version is to be embedded in a distributed system and enables the calculation of random numbers by means of hashes."}, {"heading": "Description", "text": "createFast2food initializes the Fast2food object by specifying the initialization parameters"}, {"heading": "Header", "text": "static Fast2food * createFast2food (Fast2foodType fast2foodType, float * data, const unsigned long nvec, const unsigned long dim, const unsigned long D, const unsigned long seed, const float sigma = 1.0, const unsigned long t = 5);"}, {"heading": "Parameters", "text": "\u2022 fast2foodType: Fast2foodFactory:: RBF or Fast2foodFactory:: Matern. \u2022 data: input data matrix \u2022 nvec: number of feature vectors \u2022 dim: feature dimension \u2022 D: number of base kernel extensions \u2022 seed: seed to generate random distributions \u2022 sigma: RBF sigma parameter \u2022 t: experimental Matern kernel parameter"}, {"heading": "Class atributes", "text": "\u2022 ff _ dim: feature dimension, type unsigned long. \u2022 ff _ dim: feature dimension, type unsigned long. \u2022 ff _ dimad: feature dimension, type unsigned long. \u2022 ff _ D: integer part of the quotient between the number of base extensions D and ff _ dimpad, type unsigned long. \u2022 ff _ dimD: number of kernel base extensions, defined as product ff _ D * ff _ dimpad, type unsigned long. \u2022 ff _ data: pointer to the input data matrix, type float. \u2022 ff _ dataout: pointer to internal data matrix, type float. \u2022 diag _ Pi: random permutation diagonal matrix, type unsigned long vector. \u2022 diag _ G: random gaussian diagonal matrix G, type float vector. \u2022 diag _ S: scaling diagonal matrix, type floveal, type 1., type random line diag, type B: random"}, {"heading": "Methods", "text": "\u2022 ff _ fastfood (): calculates fast food and stores the result in ff _ dataout. \u2022 ff _ features (): calculates fast food features and returns the result in a float pointer."}, {"heading": "Usage", "text": "1 s s s / s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s"}], "references": [{"title": "A Note on the Generation of Random Normal Deviates", "author": ["G.E.P. Box", "Mervin E. Muller"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Box and Muller.,? \\Q1958\\E", "shortCiteRegEx": "Box and Muller.", "year": 1958}, {"title": "In search of the optimal Walsh-Hadamard Transform", "author": ["Jeremy Johnson", "Markus Puschel"], "venue": null, "citeRegEx": "Johnson and Puschel.,? \\Q2000\\E", "shortCiteRegEx": "Johnson and Puschel.", "year": 2000}, {"title": "Fastfood - Approximating Kernel Expansions in Loglinear Time", "author": ["Quoc Le", "Tamas Sarlos", "Alex Smola"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Random Features for Large-Scale Kernel Machines", "author": ["Ali Rahimi", "Ben Recht"], "venue": null, "citeRegEx": "Rahimi and Recht.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2007}, {"title": "General cost functions for support vector regression", "author": ["Alexander J. Smola", "Bernhard Sch\u00f6kopf", "Klaus Robert M\u00fcller"], "venue": "Ninth Australian Conference on Neural Networks,", "citeRegEx": "Smola et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Smola et al\\.", "year": 1988}], "referenceMentions": [{"referenceID": 2, "context": "It contains a CPU optimized implementation of the Fastfood algorithm in Le et al. (2013), that allows the computation of approximated kernel expansions in loglinear time.", "startOffset": 72, "endOffset": 89}, {"referenceID": 2, "context": "In order to solve this problem, Le et al. (2013) proposed an approximation algorithm, called Fastfood, based on Random Kitchen Sinks by Rahimi and Recht (2007), that speeds up the computation of a large range of kernel functions, allowing us to use them in big data.", "startOffset": 32, "endOffset": 49}, {"referenceID": 2, "context": "In order to solve this problem, Le et al. (2013) proposed an approximation algorithm, called Fastfood, based on Random Kitchen Sinks by Rahimi and Recht (2007), that speeds up the computation of a large range of kernel functions, allowing us to use them in big data.", "startOffset": 32, "endOffset": 160}, {"referenceID": 3, "context": "Random Kitchen Sinks (Rahimi and Recht, 2007) approximate this feature mapping \u03c6 by a Fourier expansion in the case of radial basis function kernels, i.", "startOffset": 21, "endOffset": 45}, {"referenceID": 4, "context": "This is possible since the Fourier transform diagonalizes the corresponding integral operator (Smola et al., 1988).", "startOffset": 94, "endOffset": 114}, {"referenceID": 0, "context": "We generate the random variates using the Box-Muller transform Box and Muller (1958) while substituting the random number generator by calls to the hash function to allow us to recompute the values at any time without the need to store random numbers.", "startOffset": 63, "endOffset": 85}, {"referenceID": 3, "context": "\u2022 ff_fastfood(float* data, float* features) computes the features by using the real version of the complex feature map \u03c6 in Rahimi and Recht (2007), \u03c6\u2032(x) = n\u2212 1 2 {cos([V x]j), sin([V x]j)}.", "startOffset": 124, "endOffset": 148}], "year": 2017, "abstractText": "F2F is a C++ library for large-scale machine learning. It contains a CPU optimized implementation of the Fastfood algorithm in Le et al. (2013), that allows the computation of approximated kernel expansions in loglinear time. The algorithm requires to compute the product of Walsh-Hadamard Transform (WHT) matrices. A cache friendly SIMD Fast Walsh-Hadamard Transform (FWHT) that achieves compelling speed and outperforms current state-of-the-art methods has been developed. F2F allows to obtain non-linear classification combining Fastfood and a linear classifier.", "creator": "LaTeX with hyperref package"}}}