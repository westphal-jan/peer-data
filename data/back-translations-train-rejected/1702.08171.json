{"id": "1702.08171", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Fixed-point optimization of deep neural networks with adaptive step size retraining", "abstract": "Fixed-point optimization of deep neural networks plays an important role in hardware based design and low-power implementations. Many deep neural networks show fairly good performance even with 2- or 3-bit precision when quantized weights are fine-tuned by retraining. We propose an improved fixedpoint optimization algorithm that estimates the quantization step size dynamically during the retraining. In addition, a gradual quantization scheme is also tested, which sequentially applies fixed-point optimizations from high- to low-precision. The experiments are conducted for feed-forward deep neural networks (FFDNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs).", "histories": [["v1", "Mon, 27 Feb 2017 08:00:58 GMT  (2562kb)", "http://arxiv.org/abs/1702.08171v1", "This paper is accepted in ICASSP 2017"]], "COMMENTS": "This paper is accepted in ICASSP 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sungho shin", "yoonho boo", "wonyong sung"], "accepted": false, "id": "1702.08171"}, "pdf": {"name": "1702.08171.pdf", "metadata": {"source": "CRF", "title": "FIXED-POINT OPTIMIZATION OF DEEP NEURAL NETWORKS WITH ADAPTIVE STEP SIZE RETRAINING", "authors": ["Sungho Shin", "Yoonho Boo"], "emails": ["sungho.develop@gmail.com,", "yhboo.research@gmail.com,", "wysung@snu.ac.kr"], "sections": [{"heading": null, "text": "In fact, most of them will be able to play by the rules that they have shown in recent years, and they will be able to play by the rules."}, {"heading": "2. STEP SIZE ADAPTATION AND GRADUAL QUANTIZATION FOR RETRAINING OF DEEP NEURAL NETWORKS", "text": "In this section, we explain the conventional fixed-point optimization algorithm based on retraining and introduce adaptive retraining methods and step-by-step quantization methods."}, {"heading": "2.1. Retrain-based fixed-point quantization review", "text": "The original retraining-based fixed-point optimization algorithm can be briefly illustrated, as in Figure 1. Note that conventional algorithms [3, 4, 12] do not calculate new \u2206 at the stage of \"weight update.\" In this figure, after floating-point weights have been achieved through training, the size of the quantization step \u2206 is determined by minimizing the L2 error between floating-point weights and fixed-point weights. To simplify arithmetic, a uniform quantization is assumed. Two algorithms for quantization step size optimization have been developed. The first approach is an exhaustive search, in which the size of the original quantization step \u2206 is determined first taking into account the weight distribution, and then the best-functioning step size between \u2206 initial / 2 and 2 \u00b7 is developed initially by testing the quantized net with the weighting set [4]. The second approach consists of determining the quantity of the floating point weights and the mean step weighting."}, {"heading": "2.2. Step-size adaptation during retraining", "text": "As described in Section 2.1, the conventional method freezes the step size during the retraining, but in many cases the weight values change greatly due to retraining. Note that the amount of the change decreases as the retraining progresses. Therefore, to improve performance, it is advantageous to adjust the quantization step size during the retraining. In particular, the need to adjust the step size at the beginning of the retraining is greater. The proposed scheme adds the determination of new steps during the weighting update phase of Figure 1.We no longer perform an \"exhaustive search,\" but update the quantization step size during the retraining by using the L2 error minimization between floating point weights and fixed point weights. We consider two different quantization step size updates: the first is \"epoch update,\" and the other is \"epoch update,\" and the other is \"second epoch update\" for the first and third epoch."}, {"heading": "2.3. Gradual quantization scheme", "text": "We also propose another approach for adjusting the step size, similar to the curriculum. Curriculum learning is a training strategy to gradually shift the goal from a simple level to a more complex one [13]. One of the important points in the curriculum is the organization of the tasks from simple to complex tasks. We consider fixed point optimization with a small number of bits to be a more difficult problem than that with a large one. In the proposed scheme, we start with fixed point optimization with a relatively high precision, e.g. 6 bits, and then reduce the word length with retraining for each precision by one bit. In each retraining process with a specified precision, we also combine the proposed quantization step size adjustment scheme. The experiments are conducted for FFDNNs."}, {"heading": "3. EXPERIMENTAL RESULTS", "text": "The proposed step size adjustment is evaluated for three applications: We use FFDNNs for phoneme recognition, CNNs for house number recognition, and RNNs for voice modelling. To analyze the effects of step size adjustment, we change the size of each network and its word lengths."}, {"heading": "3.1. Phoneme recognition using feed-forward deep neural networks", "text": "In recent years, it has been shown that there have been exaggerations in the USA, in Europe, in Europe, in Europe, in Europe and throughout the world. (...) In the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in Europe, in the USA, in the USA, in Europe, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "3.2. Image classification using convolutional neural networks", "text": "Image classification experiments are performed on the SVHN dataset [19]. The dataset contains 600,000 labeled 32 x 32 three-channel images from real house numbers. We use the same method with [20] for data preprocessing. The output label has ten units, which have the numbers from 0 to 9. To evaluate the proposed scheme, we use three different structures. We call the networks \"L,\" \"C\" and \"V,\" which have the trainable parameters 60k, 84k and 435k, respectively. The \"L\" network is Lenet5 [21], \"C\" network comes from [22], and \"V\" network is constructed in the VGG style, which comes from [23]. We train the floating commands networks with SGD with Nesterov dynamics. The learning rate decreases from 2-e2 to 3.125e-4, if the development set does not show an improvement for 4 consecutive evaluations."}, {"heading": "3.3. Language modeling using recurrent neural networks", "text": "Since the input and output layers only take alphabets into account, the input and output complexity is much lower than the word-level language model. We use the English Wikipedia dataset to form character-level language modeling. The dataset contains 100 MB of English Wikipedia text. The input and output layers consist of 256 units for a uniformly encoded ASCII code. The RNN consists of three Long Short Term Memory (LSTM) layers with a different number of memory cells in the range of 64 to 256 [24]. We train the RNNs with AdaDelta-based SGD with 64 parallel input streams. The networks are rolled out 256 times and the weight update takes place for 128 forward steps. The learning rate starts at 5e-4 and decreases to 5e-8. For step size adjustment, \"1 Epoch Update & Small Strategy\" and the FFN results are optimized."}, {"heading": "4. CONCLUDING REMARKS", "text": "We have developed improved fixed-point weight optimization methods for deep neural networks; the first method is an adaptive method for determining the size of quantization steps by measuring the weight distribution during the retraining process; the second is a method for fixed-point optimization in the curriculum that progressively performs fixed-point optimization from high to low precision; and the proposed work leads to better quantization results in FFDNN, CNN, and RNN experiments; in particular, the effectiveness of the proposed techniques increases when the number of quantization levels is small and the network size is not large enough."}, {"heading": "5. REFERENCES", "text": "[1] Dario Amodei, Rishita Anubhai, Eric Battenberg, JinCase, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, et al., \"Deep speech 2: End-to-end speech recognition in English and Mandarin,\" in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. [3] Sungho Shin, Kyuyeon Hwang, and Wonyong Sung \"Fixed-point performance analysis of recurrent neural networks,\" in 2016 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), June 2016."}], "references": [{"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos"], "venue": "ICML 2016: 33rd International Conf. Machine Learning, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Fixed-point performance analysis of recurrent neural networks", "author": ["Sungho Shin", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 976\u2013980.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Fixed-point feedforward deep neural network design using weights +1, 0, and -1", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "2014 IEEE Workshop on Signal Processing Systems (SiPS). IEEE, 2014, pp. 1\u20136.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Fixed point quantization of deep convolutional networks", "author": ["Darryl D Lin", "Sachin S Talathi", "V Sreekanth Annapureddy"], "venue": "ICML 2016: 33rd International Conf. Machine Learning, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "CoRR, abs/1510.00149, vol. 2, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "X1000 real-time phoneme recognition vlsi using feedforward deep neural networks", "author": ["Jonghong Kim", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 7510\u20137514.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "FPGA based implementation of deep neural networks using on-chip memory only", "author": ["Jinhwan Park", "Wonyong Sung"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 1011\u20131015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "EIE: efficient inference engine on compressed deep neural network", "author": ["Song Han", "Xingyu Liu", "Huizi Mao", "Jing Pu", "Ardavan Pedram", "Mark A. Horowitz", "William J. Dally"], "venue": "43rd ACM/IEEE Annual International Symposium on Computer Architecture, ISCA 2016, Seoul, South Korea, June 18-22, 2016, 2016, pp. 243\u2013254.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "FPGA-based low-power speech recognition with recurrent neural networks", "author": ["Minjae Lee", "Kyuyeon Hwang", "Jinhwan Park", "Choi Sungwook", "Shin Sungho", "Wonyong Sung"], "venue": "2016 IEEE Workshop on Signal Processing Systems (SiPS). IEEE, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Scaling binarized neural networks on reconfigurable logic", "author": ["Nicholas J Fraser", "Yaman Umuroglu", "Giulio Gambardella", "Michaela Blott", "Philip Leong", "Magnus Jahre", "Kees Vissers"], "venue": "To appear in the PARMA-DITAM workshop at HiPEAC, 2017, vol. 2017.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 1131\u20131135.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Curriculum learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "Proceedings of the 26th annual international conference on machine learning. ACM, 2009, pp. 41\u201348.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "DARPA TIMIT acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1", "author": ["John S Garofolo", "Lori F Lamel", "William M Fisher", "Jonathon G Fiscus", "David S Pallett"], "venue": "NASA STI/Recon technical report n, vol. 93, 1993.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1993}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 6645\u2013 6649.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George E Dahl", "Geoffrey E Hinton"], "venue": "ICML (3), vol. 28, pp. 1139\u2013 1147, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "ICML, 2015, pp. 448\u2013456.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Resiliency of deep neural networks under quantization", "author": ["Wonyong Sung", "Sungho Shin", "Kyuyeon Hwang"], "venue": "arXiv preprint arXiv:1511.06488, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": "2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["Pierre Sermanet", "Soumith Chintala", "Yann LeCun"], "venue": "Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE, 2012, pp. 3288\u2013 3291.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel"], "venue": "Neural computation, vol. 1, no. 4, pp. 541\u2013551, 1989.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "cuda-convnet: High-performance c++/cuda implementation of convolutional neural networks", "author": ["Alex Krizhevsky"], "venue": "2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 3123\u2013 3131.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Felix A Gers", "Nicol N Schraudolph", "J\u00fcrgen Schmidhuber"], "venue": "Journal of machine learning research, vol. 3, no. Aug, pp. 115\u2013143, 2002.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNNs) show very high performance in various fields such as speech recognition [1] and image classification [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "Deep neural networks (DNNs) show very high performance in various fields such as speech recognition [1] and image classification [2].", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "Even ternary valued weights (+1, 0, and -1) for a DNN have yielded satisfactory performance [3, 4].", "startOffset": 92, "endOffset": 98}, {"referenceID": 3, "context": "Even ternary valued weights (+1, 0, and -1) for a DNN have yielded satisfactory performance [3, 4].", "startOffset": 92, "endOffset": 98}, {"referenceID": 4, "context": "Recently, several improved fixed-point optimization methods are developed by employing retraining based fine tuning [5, 6].", "startOffset": 116, "endOffset": 122}, {"referenceID": 5, "context": "Recently, several improved fixed-point optimization methods are developed by employing retraining based fine tuning [5, 6].", "startOffset": 116, "endOffset": 122}, {"referenceID": 6, "context": "Also, VLSI and FPGA based deep neural networks have been implemented using fixed-point weights [7, 8, 9, 10, 11].", "startOffset": 95, "endOffset": 112}, {"referenceID": 7, "context": "Also, VLSI and FPGA based deep neural networks have been implemented using fixed-point weights [7, 8, 9, 10, 11].", "startOffset": 95, "endOffset": 112}, {"referenceID": 8, "context": "Also, VLSI and FPGA based deep neural networks have been implemented using fixed-point weights [7, 8, 9, 10, 11].", "startOffset": 95, "endOffset": 112}, {"referenceID": 9, "context": "Also, VLSI and FPGA based deep neural networks have been implemented using fixed-point weights [7, 8, 9, 10, 11].", "startOffset": 95, "endOffset": 112}, {"referenceID": 10, "context": "Also, VLSI and FPGA based deep neural networks have been implemented using fixed-point weights [7, 8, 9, 10, 11].", "startOffset": 95, "endOffset": 112}, {"referenceID": 3, "context": "The previous works decide the optimum quantization step size based on the distribution of floating-point weights and freezes the stepsize during the retraining period [4, 5].", "startOffset": 167, "endOffset": 173}, {"referenceID": 4, "context": "The previous works decide the optimum quantization step size based on the distribution of floating-point weights and freezes the stepsize during the retraining period [4, 5].", "startOffset": 167, "endOffset": 173}, {"referenceID": 2, "context": "Note that, conventional algorithms [3, 4, 12] do not compute \u2206new at the \u2018weights update\u2019 stage.", "startOffset": 35, "endOffset": 45}, {"referenceID": 3, "context": "Note that, conventional algorithms [3, 4, 12] do not compute \u2206new at the \u2018weights update\u2019 stage.", "startOffset": 35, "endOffset": 45}, {"referenceID": 11, "context": "Note that, conventional algorithms [3, 4, 12] do not compute \u2206new at the \u2018weights update\u2019 stage.", "startOffset": 35, "endOffset": 45}, {"referenceID": 3, "context": "One is an exhaustive search, which decides the initial quantization step size \u2206initial by considering the weight distribution, and then searches the best performing step size between \u2206initial/2 and 2 \u00b7 \u2206initial by testing the quantized network with the evaluation set [4].", "startOffset": 268, "endOffset": 271}, {"referenceID": 4, "context": "The second approach is deciding the quantization step size by measuring the mean and the variance of the floating-point weights [5].", "startOffset": 128, "endOffset": 131}, {"referenceID": 12, "context": "The curriculum learning is a training strategy to move the goal from an easy level to more complex one gradually [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "The FFDNN is trained with the TIMIT corpus [14], and the detailed experimental condition for the data preprocessing is the same with [15].", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "The FFDNN is trained with the TIMIT corpus [14], and the detailed experimental condition for the data preprocessing is the same with [15].", "startOffset": 133, "endOffset": 137}, {"referenceID": 3, "context": "Note that \u2018conventional\u2019 is the baseline [4] and \u2018adaptive\u2019 is the proposed scheme.", "startOffset": 41, "endOffset": 44}, {"referenceID": 15, "context": "the stochastic gradient descent (SGD) with Nesterov momentum [16].", "startOffset": 61, "endOffset": 65}, {"referenceID": 16, "context": "The experiments also show the results with batch normalization (BN) [17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "When the unit size is large enough, the quantization scheme does not affect the performance much because a larger size network has a better resiliency to quantization [18].", "startOffset": 167, "endOffset": 171}, {"referenceID": 18, "context": "Image classification experiments are performed on the SVHN dataset [19].", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "For the data preprocessing, we employ the same method with [20].", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "The \u2018L\u2019 network is Lenet5 [21], \u2018C\u2019 network is from [22], and \u2018V\u2019 network is constructed as VGG style which is from [23].", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "The \u2018L\u2019 network is Lenet5 [21], \u2018C\u2019 network is from [22], and \u2018V\u2019 network is constructed as VGG style which is from [23].", "startOffset": 52, "endOffset": 56}, {"referenceID": 22, "context": "The \u2018L\u2019 network is Lenet5 [21], \u2018C\u2019 network is from [22], and \u2018V\u2019 network is constructed as VGG style which is from [23].", "startOffset": 116, "endOffset": 120}, {"referenceID": 23, "context": "The RNN consists of three Long Short-Term Memory (LSTM) layers with a different number of memory cells ranging from 64 to 256 [24].", "startOffset": 126, "endOffset": 130}], "year": 2017, "abstractText": "Fixed-point optimization of deep neural networks plays an important role in hardware based design and low-power implementations. Many deep neural networks show fairly good performance even with 2or 3-bit precision when quantized weights are fine-tuned by retraining. We propose an improved fixedpoint optimization algorithm that estimates the quantization step size dynamically during the retraining. In addition, a gradual quantization scheme is also tested, which sequentially applies fixed-point optimizations from highto low-precision. The experiments are conducted for feed-forward deep neural networks (FFDNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs).", "creator": "LaTeX with hyperref package"}}}