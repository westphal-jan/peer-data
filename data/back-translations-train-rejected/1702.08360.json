{"id": "1702.08360", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Neural Map: Structured Memory for Deep Reinforcement Learning", "abstract": "A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training.", "histories": [["v1", "Mon, 27 Feb 2017 16:32:27 GMT  (671kb,D)", "http://arxiv.org/abs/1702.08360v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["emilio parisotto", "ruslan salakhutdinov"], "accepted": false, "id": "1702.08360"}, "pdf": {"name": "1702.08360.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Emilio Parisotto"], "emails": ["eparisot@cs.cmu.edu", "rsalakhu@cs.cmu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year it is so far that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "2 BACKGROUND", "text": "A Markov Decision Process (MDP) is defined as a tuple (S, A, T, \u03b3, R) where S is a finite series of states, A is a finite series of acts, T (s, a) is the transition probability of arriving in state s \"when performing act a in initial state s, \u03b3 is a discount factor, and R (s, a, s\") is the reward function of performing act a in state s \"and ends at state s.\" We define a policy \u03c0 as an assignment from a state s to a distribution over actions where \u03c0 (ai | s) denotes the probability of act ai as we are in state s. The value of a policy V \u03c0 (s) is the expected discounted cumulative reward when we start from state s and sampling actions by state s, \"i.e.: V \u03c0 policy (s) = Esp."}, {"heading": "3 NEURAL MAP", "text": "In this section, we will describe the details of the neural map. We assume that we want our agent to act within a 2- or 3-dimensional environment; the neural map is the internal memory of the agent that can be read and written during interaction with its environment, but where the write operator is selectively limited to affecting only that part of the neural map (i.e. a 4D map with a 3D submap for each cardinal direction in which the agent is located); for simplicity, we assume that we are dealing with a 2-dimensional map; this can easily be extended to 3-dimensional or even higher-dimensional maps (i.e. a 4D map with a 3D submap for each cardinal direction that the agent can face); let us leave the position of the agent (x, y) with x-R and y-R and let the neural map M be a C-H-W attribute for each cardinal direction that the agent can face; let us leave the map (x, y) with x-R and y-R; let the neural map M be a C-H-W attribute block with the expansion of the mythical dimension, with the W attribute being the expansion of the mythical dimension and the dimension."}, {"heading": "3.1 GLOBAL READ OPERATION", "text": "The reading process guides the current neural map Mt through a deep winding network and generates a C-dimensional feature vector rt. The global read vector rt summarizes information about the entire map."}, {"heading": "3.2 CONTEXT READ OPERATION", "text": "The context operation performs a context-based addressing to verify that certain characteristics are stored in the map. It takes as input the current state in which st and the current global read vector rt are embedded, and first generates a query vector qt. The inner product of the query vector and each feature M (x, y) t in the neural map is then taken to obtain values at all positions (x, y) t. Values are then normalized to obtain a probability distribution \u03b1 (x, y) t over each position on the map, also known as \"soft attention\" (Bahdanau et al., 2015). This probability distribution is used to obtain a weight distribution ct over all characteristics M (x, y). In summary: qt = W [st, rt] (8) a (x, y) t = qt = qt = qt = the memory that (possibly) completes the operation."}, {"heading": "3.3 LOCAL WRITE OPERATION", "text": "Considering the current position of the agent (xt, yt) at time t, the write process enters the current state in which st is embedded, the global read output rt, the context read vector ct and the current feature at position (xt, yt) in the neural map M (xt, yt) t and generates a new C-dimensional vector w (xt, yt) t + 1 using a deep neural network f. This vector acts as a new local write candidate at the current position (xt, yt): w (xt, yt) t + 1 = f ([st, rt, ct, M (xt, yt) t] (12)."}, {"heading": "3.4 MAP UPDATE OPERATION", "text": "The new neural map Mt + 1 corresponds to the old neural map Mt, except at the current agent position (xt, yt), where the current write candidate vector w (xt, yt) t + 1 is stored: M (a, b) t + 1 = {w (xt, yt) t + 1, for (a, b) = (xt, yt) M (a, b) t, for (a, b) 6 = (xt, yt) (13)"}, {"heading": "3.5 OPERATION VARIANTS", "text": "There are several modifications that can be made to the standard operations defined above. In the following some variants are discussed."}, {"heading": "3.5.1 LOCALIZED READ OPERATION", "text": "Instead of guiding the entire neural map through a deep revolutionary network, a spatial subset of the map can instead be passed. For example, a Spatial Transformer Network (Jaderberg et al., 2015) can be used to study the neural map attentively at specific locations and scales, which can be helpful when the environment requires a large high-resolution map, which can be computationally expensive to process at each step."}, {"heading": "3.5.2 KEY-VALUE CONTEXT READ OPERATION", "text": "We can give context addressing a stronger bias by dividing each feature of the neural map into two parts M (x, y) t = [k (x, y) t, v (x, y) t, v (x, y) t], where k (x, y) t is the (C / 2) -dimensional \"key feature\" and v (x, y) t is the (C / 2) -dimensional \"value attribute\" Miller et al. (2016). Specifically: qt = W [st, rt] (14) M (x, y) t = [k (x, y) t, v (x, y) t [15) a (x, y) t = qt \u00b7 k (x, y) t = c \u00b7 k (x, y) t (x, y) t = c (c), c (c) t (b), c (c) t (b), c (c) t (c), c c (c) t (c), c c (c) t (c), c c (c) t (c), c (c) t (c), c (c) t (c), c (c) t (c (c), c (c) t (c (c), c (c) t (c (c), c (c) t (c), t (t, t (t, t (t, t, t, t, t (t, t, t, t (t), t (t, t, t (t), t (t, t, t, t, t (c), t (c), t (c), t (t (c), t, t (c), t (c), t (c), t (c (c (c), t (c), t (c), t (c (c), t (c (c), t (c), t (c), t (t (t (c), t (c), t (t (c), t (t (c), t (c), t (t (c), t (t, t, t (c), t (c), t, t (t (c), t, t"}, {"heading": "3.5.3 GRU-BASED LOCAL WRITE OPERATION", "text": "As before, the write process simply replaces the vector at the current position of the agent with a new feature produced by a deep network. Instead of this hard rewrite of the feature vector at the current position, we can use a gated write operation based on the recurring update equations of the gated recurrent unit (GRU) (Chung et al., 2014). Gated write operations have a long history in unstructured, recurring networks and they have a superior ability to maintain information for a long time, compared to ungated networks. GRU-based write is defined as: r (xt, yt) t + 1 = \u03c3 (Wr [st, rt, ct) t (xt, ct) t (xt, ct) t (xt, ct) t (xt, yt) t (Wz, yt) t (t) t (t) t (t) t (t), t (t) t, t, t, t, t, t, t, t, t, t (1, t (t), t (t), t (xt, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t (t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t (t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t (t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t (t, t, t, t, t, t, t, t, t, t, t (t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t,"}, {"heading": "4 EXPERIMENTS", "text": "To demonstrate the effectiveness of the Neural Map, we perform it in 2D and 3D labyrinth environments where memory is critical for optimal behavior, comparing it to previous memory-based DRL agents, namely a simple LSTM-based agent consisting of a single pre-output LSTM layer and MemNN agents (Oh et al., 2016). Of the agents presented in Oh et al. (2016), we use the MQN version, i.e. the standalone storage network without the LSTM layer."}, {"heading": "4.1 2D GOAL-SEARCH ENVIRONMENT", "text": "This year, it has come to the point where it can only take one year to move on to the next round."}, {"heading": "4.2 3D DOOM ENVIRONMENT", "text": "In fact, most of us are able to play by the rules we have set ourselves."}, {"heading": "5 EXTENSION: EGO-CENTRIC NEURAL MAP", "text": "A major disadvantage of the neural map is that it requires some oracles to provide the current (x, y) position of the agent (x, y). This is a difficult problem in and of itself, and although it is well studied, it is far from being solved. An alternative to using absolute positions within the map is the use of relative positions (x, y), which means that the map will be moved between time steps at a certain speed (u, v), the map will be transformed by (\u2212 u, \u2212 v), i.e. every feature in the map will be shifted in the H and W dimensions. This means that the map will be ego-centric, i.e. the position of the agent will remain in the center of the neural map, while the world as defined by the map will move around it. Therefore, in this constellation, we need only one way to extract the speed of the agent, which is ph, which is typically a simpler function that we assume is an environment."}, {"heading": "6 RELATED WORK", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave."}, {"heading": "7 CONCLUSION", "text": "In this work, we developed a neural memory architecture that organizes the spatial structure of its memory in the form of a 2D card and allows for sparse writing into that memory, with the memory address of the writing corresponding to the current position of the writer in the environment. We demonstrated his ability to learn, using a gain signal, how to behave within a challenging 2D labyrinth task that requires storing information over long periods of time. Results showed that our architecture outperforms the base memories used in previous work. They also showed that the GRU-based update equation we defined was critical to improving both the learning speed and training stability. Finally, to demonstrate that our method is applicable to more challenging 3D environments, we reimplemented the labyrinth environment in Doom. Using a hybrid model called Neural Map + LSTM, we were able to solve most scenarios and meet both LM and MemsagN."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K.H. Cho", "Y. Bengio"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Playing doom with slam-augmented deep reinforcement learning", "author": ["S. Bhatti", "A. Desmaison", "O. Miksik", "N. Nardelli", "N. Siddharth", "P.H.S. Torr"], "venue": "CoRR, abs/1612.00380,", "citeRegEx": "Bhatti et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bhatti et al\\.", "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Hybrid computing using a neural network", "author": ["A. Graves", "G. Wayne", "M. Reynolds", "T. Harley", "I. Danihelka", "A. Grabska-Barwiska", "S.G. Colmenarejo", "E. Grefenstette", "T. Ramalho", "J. Agapiou", "A.P. Badia", "K.M. Hermann", "Y. Zwols", "G. Ostrovski", "A. Cain", "H. King", "C. Summerfield", "P. Blunsom", "K. Kavukcuoglu", "D. Hassabis"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2016}, {"title": "Cognitive mapping and planning for visual navigation", "author": ["S. Gupta", "J. Davidson", "S. Levine", "R. Sukthankar", "J. Malik"], "venue": "CoRR, abs/1702.03920,", "citeRegEx": "Gupta et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2017}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["M. Hausknecht", "P. Stone"], "venue": "CoRR, abs/1507.06527,", "citeRegEx": "Hausknecht and Stone.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht and Stone.", "year": 2015}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman"], "venue": "In Advances in Neural Information Processing Systems, pp. 2017\u20132025,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Neural gpus learn algorithms", "author": ["L. Kaiser", "I. Sutskever"], "venue": "In Proceedings of the 4th International Conference on Learning Representations", "citeRegEx": "Kaiser and Sutskever.,? \\Q2016\\E", "shortCiteRegEx": "Kaiser and Sutskever.", "year": 2016}, {"title": "ViZDoom: A Doom-based AI research platform for visual reinforcement learning", "author": ["Micha\u0142 Kempka", "Marek Wydmuch", "Grzegorz Runc", "Jakub Toczek", "Wojciech Ja\u015bkowski"], "venue": "In IEEE Conference on Computational Intelligence and Games,", "citeRegEx": "Kempka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kempka et al\\.", "year": 2016}, {"title": "Neural random-access machines", "author": ["K. Kurach", "M. Andrychowicz", "I. Sutskever"], "venue": "In Proceedings of the 4th International Conference on Learning Representations", "citeRegEx": "Kurach et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2016}, {"title": "Playing fps games with deep reinforcement learning", "author": ["G. Lample", "D.S. Chaplot"], "venue": "CoRR, abs/1609.05521,", "citeRegEx": "Lample and Chaplot.,? \\Q2016\\E", "shortCiteRegEx": "Lample and Chaplot.", "year": 2016}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Key-value memory networks for directly reading", "author": ["A. Miller", "A. Fisch", "J. Dodge", "A. Karimi", "A. Bordes", "J. Weston"], "venue": "documents. CoRR,", "citeRegEx": "Miller et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518:529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T. Harley", "T.P. Lillicrap", "D. Silver", "K. Kavukcuoglu"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Control of memory, active perception, and action in minecraft", "author": ["J. Oh", "V. Chockalingam", "S. Singh", "H. Lee"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML),", "citeRegEx": "Oh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oh et al\\.", "year": 2016}, {"title": "Scaling memory-augmented neural networks with sparse reads and writes", "author": ["J.W. Rae", "J.J. Hunt", "T. Harley", "I. Danihelka", "A. Senior", "G. Wayne", "A. Graves", "T. Lillicrap"], "venue": "CoRR, abs/1610.09027,", "citeRegEx": "Rae et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rae et al\\.", "year": 2016}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["St\u00e9phane Ross", "Geoffrey J Gordon", "Drew Bagnell"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Reinforcement Learning: an Introduction", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Value iteration networks", "author": ["Aviv Tamar", "Yi Wu", "Garrett Thomas", "Sergey Levine", "Pieter Abbeel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tamar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tamar et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}], "referenceMentions": [{"referenceID": 15, "context": "More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames.", "startOffset": 17, "endOffset": 34}, {"referenceID": 13, "context": "Recently, Deep Reinforcement Learning agents have been capable of solving many challenging tasks such as Atari Arcade Games (Mnih et al., 2015), robot control (Levine et al.", "startOffset": 124, "endOffset": 143}, {"referenceID": 11, "context": ", 2015), robot control (Levine et al., 2016) and 3D games such as Doom (Lample & Chaplot, 2016), but successful behaviours in these tasks have often only been based on a relatively short-term temporal context or even just a single frame.", "startOffset": 23, "endOffset": 44}, {"referenceID": 18, "context": "Writeless external memory systems, often referred to as \u201cMemory Networks\u201d (Sukhbaatar et al., 2015; Oh et al., 2016), typically fix which memories are stored.", "startOffset": 74, "endOffset": 116}, {"referenceID": 15, "context": "Writeless external memory systems, often referred to as \u201cMemory Networks\u201d (Sukhbaatar et al., 2015; Oh et al., 2016), typically fix which memories are stored.", "startOffset": 74, "endOffset": 116}, {"referenceID": 18, "context": "The memory network approach has been successful in language modeling, question answering (Sukhbaatar et al., 2015) and was shown to be a sucessful memory for deep reinforcement learning agents in complex 3D environments (Oh et al.", "startOffset": 89, "endOffset": 114}, {"referenceID": 15, "context": ", 2015) and was shown to be a sucessful memory for deep reinforcement learning agents in complex 3D environments (Oh et al., 2016).", "startOffset": 113, "endOffset": 130}, {"referenceID": 9, "context": ", 2014), RAM (Kurach et al., 2016), and GPUs (Kaiser & Sutskever, 2016).", "startOffset": 13, "endOffset": 34}, {"referenceID": 3, "context": "One such model, the Differentiable Neural Computer (DNC) (Graves et al., 2016) and its predecessor the Neural Turing Machine (NTM) (Graves et al.", "startOffset": 57, "endOffset": 78}, {"referenceID": 21, "context": "The REINFORCE algorithm (Williams, 1992) iteratively updates a given policy \u03c0 in the direction of the optimal policy.", "startOffset": 24, "endOffset": 40}, {"referenceID": 14, "context": "In this paper, we utilize a modified Asynchronous Advantage Actor-Critic (A3C) (Mnih et al., 2016), which can be seen as a specialization of the actor-critic framework when using deep networks to", "startOffset": 79, "endOffset": 98}, {"referenceID": 14, "context": "Other than this modification, the algorithm is identical to the one used in A3C, including the methods used to update the value and policy networks (refer to (Mnih et al., 2016) for details).", "startOffset": 158, "endOffset": 177}, {"referenceID": 0, "context": "The scores are then normalized to get a probability distribution \u03b1 t over every position in the map, also known as \u201csoft attention\u201d (Bahdanau et al., 2015).", "startOffset": 132, "endOffset": 155}, {"referenceID": 6, "context": "For example, a Spatial Transformer Network (Jaderberg et al., 2015) can be used to attentively subsample the neural map at particular locations and scales.", "startOffset": 43, "endOffset": 67}, {"referenceID": 12, "context": "We can impose a stronger bias on the context addressing operation by splitting each feature of the neural map into two parts M (x,y) t = [k (x,y) t , v (x,y) t ], where k (x,y) t is the (C/2)-dimensional \u201ckey\u201d feature and v t is the (C/2)-dimensional \u201cvalue\u201d feature Miller et al. (2016). The key features are matched against the query vector (which is now a (C/2)-dimensional vector) to get the probability distribution \u03b1 t , and the weighted average is taken over the value features.", "startOffset": 267, "endOffset": 288}, {"referenceID": 2, "context": "Instead of this hard rewrite of the current position\u2019s feature vector, we can use a gated write operation based on the recurrent update equations of the Gated Recurrent Unit (GRU) (Chung et al., 2014).", "startOffset": 180, "endOffset": 200}, {"referenceID": 15, "context": "We compare to previous memory-based DRL agents, namely a simple LSTM-based agent which consists of a single pre-output LSTM layer as well as MemNN (Oh et al., 2016) agents.", "startOffset": 147, "endOffset": 164}, {"referenceID": 15, "context": "We compare to previous memory-based DRL agents, namely a simple LSTM-based agent which consists of a single pre-output LSTM layer as well as MemNN (Oh et al., 2016) agents. Of the agents presented in Oh et al. (2016), we use the MQN version, i.", "startOffset": 148, "endOffset": 217}, {"referenceID": 15, "context": "The \u201cGoal-Search\u201d environment is adapted from Oh et al. (2016). Here the agent starts in a fixed starting position within some randomly generated maze with two randomly positioned goal states.", "startOffset": 46, "endOffset": 63}, {"referenceID": 8, "context": "To demonstrate that our method can work in much more complicated 3D environments with longer time lags, we implemented the 2D maze environment in 3D using the ViZDoom (Kempka et al., 2016) environment and a random maze generator.", "startOffset": 167, "endOffset": 188}, {"referenceID": 14, "context": "Other than the straightforward architectures of combining an LSTM with Deep Reinforcement Learning (DRL) (Mnih et al., 2016; Hausknecht & Stone, 2015), there has also been work on using more advanced external memory systems with DRL agents to handle partial observability.", "startOffset": 105, "endOffset": 150}, {"referenceID": 13, "context": "Other than the straightforward architectures of combining an LSTM with Deep Reinforcement Learning (DRL) (Mnih et al., 2016; Hausknecht & Stone, 2015), there has also been work on using more advanced external memory systems with DRL agents to handle partial observability. Oh et al. (2016) used a memory network (MemNN) to solve maze-based environments similar to the ones presented in this paper.", "startOffset": 106, "endOffset": 290}, {"referenceID": 13, "context": "Other than the straightforward architectures of combining an LSTM with Deep Reinforcement Learning (DRL) (Mnih et al., 2016; Hausknecht & Stone, 2015), there has also been work on using more advanced external memory systems with DRL agents to handle partial observability. Oh et al. (2016) used a memory network (MemNN) to solve maze-based environments similar to the ones presented in this paper. MemNN keeps the last M states in memory and encodes them into (key, value) feature pairs. It then queries this memory using a soft attention mechanism similar to the context operation of the Neural Map, except in the Neural Map the key/value features were written by the agent and aren\u2019t just a stored representation of the last M frames seen. Oh et al. (2016) tested a few variants of this basic model, including ones which combined both LSTM and memory-network style memories.", "startOffset": 106, "endOffset": 759}, {"referenceID": 3, "context": "One recent model is similar to the Neural Map, called the Differentiable Neural Computer (DNC) (Graves et al., 2016), which combines a recurrent controller with an external memory system that allows several types of read/write access.", "startOffset": 95, "endOffset": 116}, {"referenceID": 16, "context": "Recently work has also been done toward sparsifying the read and write operations of the DNC (Rae et al., 2016).", "startOffset": 93, "endOffset": 111}, {"referenceID": 1, "context": "Additionally, a recent paper has used the idea of augmenting the state of an agent with an internal map when acting in 3D environments (Bhatti et al., 2016).", "startOffset": 135, "endOffset": 156}, {"referenceID": 17, "context": "Finally, their method used DAGGER (Ross et al., 2011), an imitation learning algorithm, to train their agent.", "startOffset": 34, "endOffset": 53}, {"referenceID": 20, "context": "An interesting addition they made was the use of a multi-scale map representation and a Value Iteration network (Tamar et al., 2016) to do better path planning.", "startOffset": 112, "endOffset": 132}, {"referenceID": 1, "context": "Additionally, a recent paper has used the idea of augmenting the state of an agent with an internal map when acting in 3D environments (Bhatti et al., 2016). Their approach uses a sophisticated pipeline of hard-coded sub-modules, such as SLAM (Simultaneous Localization And Mapping), image segmentation, etc., to augment the image inputs that are typically fed to DRL agents. In contrast, the Neural Map is trained fully end-to-end without even weak supervision and therefore it can learn by itself what currently relevant information it should store within in its internal knowledge map of the environment. A similar paper that also had a 2D map structured memory was recently made public concurrently with our submission. Gupta et al. (2017) designed a spatial memory that was used to do robot navigation in 3D environments.", "startOffset": 136, "endOffset": 744}, {"referenceID": 1, "context": "Additionally, a recent paper has used the idea of augmenting the state of an agent with an internal map when acting in 3D environments (Bhatti et al., 2016). Their approach uses a sophisticated pipeline of hard-coded sub-modules, such as SLAM (Simultaneous Localization And Mapping), image segmentation, etc., to augment the image inputs that are typically fed to DRL agents. In contrast, the Neural Map is trained fully end-to-end without even weak supervision and therefore it can learn by itself what currently relevant information it should store within in its internal knowledge map of the environment. A similar paper that also had a 2D map structured memory was recently made public concurrently with our submission. Gupta et al. (2017) designed a spatial memory that was used to do robot navigation in 3D environments. These environments were based off image scans of real office buildings, and they were preprocessed into a grid-world by quantizing the possible positions and orientations the agent could assume. In contrast to our paper, which presents the Neural Map more as a general memory architecture for DRL agents, Gupta et al. (2017) focuses mainly on solving the task of robot navigation.", "startOffset": 136, "endOffset": 1152}], "year": 2017, "abstractText": "A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training.", "creator": "LaTeX with hyperref package"}}}