{"id": "1702.08495", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Don't Fear the Reaper: Refuting Bostrom's Superintelligence Argument", "abstract": "In recent years prominent intellectuals have raised ethical concerns about the consequences of artificial intelligence. One concern is that an autonomous agent might modify itself to become \"superintelligent\" and, in supremely effective pursuit of poorly specified goals, destroy all of humanity. This paper considers and rejects the possibility of this outcome. We argue that this scenario depends on an agent's ability to rapidly improve its ability to predict its environment through self-modification. Using a Bayesian model of a reasoning agent, we show that there are important limitations to how an agent may improve its predictive ability through self-modification alone. We conclude that concern about this artificial intelligence outcome is misplaced and better directed at policy questions around data access and storage.", "histories": [["v1", "Mon, 27 Feb 2017 19:57:17 GMT  (14kb)", "https://arxiv.org/abs/1702.08495v1", null], ["v2", "Sat, 4 Mar 2017 20:43:32 GMT  (14kb)", "http://arxiv.org/abs/1702.08495v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["sebastian benthall"], "accepted": false, "id": "1702.08495"}, "pdf": {"name": "1702.08495.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 2.08 495v 2 [cs.A I] 4 Mar 201 7The appetite of the public and prominent intellectuals for studying the ethical implications of artificial intelligence has increased in recent years. An intriguing possibility is that research in the field of artificial intelligence could lead to a \"super-intelligence\" that threatens humanity. (Russell 2014) has called on AI researchers to seriously consider this possibility because, however unlikely it may be, its very possibility is grave. (Bostrom 2014) argues for the importance of considering the risks of artificial intelligence as a research agenda. For Bostrom, the potential risks of artificial intelligence are not just in the scale of industrial misfortunes or weapons of mass destruction. Rather, Bostrom argues that artificial intelligence has the potential to threaten humanity as a whole and determine the fate of the universe. We approach this great thesis with a degree of skepticism. Nevertheless, we hope that by getting a grip on the argument of faith, we can clarify the potential objections in good faith."}, {"heading": "1 Bostrom\u2019s core argument and definitions", "text": "Proposition 1: A system with sufficient intelligence in relation to other intelligent systems will have a \"decisive strategic advantage\" and will determine the fate of the world and the universe. Specifically, this means that Bostrom has the potential to dominate the world. In its implicit model of the world, these actors will compete with each other. By defining Bostrom, a \"decisive strategic advantage\" will be achieved that is sufficient to achieve complete world domination. It goes beyond the scope of this paper, to the nuances of Proposition 1: We will proclaim it and focus on the probability that a sufficiently intelligent system will emerge. Proposition 2: An intelligent system will probably achieve a decisive strategic advantage that has a decisive strategic advantage."}, {"heading": "2 Intelligence and instrumental tasks", "text": "This is regrettable and a consequence of the lack of clarity about the way in which it has taken place in recent years."}, {"heading": "3 Recalcitrance considered", "text": "Bostrom's model of intelligence change depends on two variables, optimisation power and unruliness. These are presented as components in a qualitative model. Optimisation power is the effort put into improving the intelligence of the system. Therefore, unruliness is the resistance of the system to improvement. Although it is desirable to have units in which intelligence, optimisation power and unruliness could be measured, none has been provided by Bostrom. Nevertheless, this model is a useful one to explain intuitions about self-modifying intelligence. Bostrom's original formulation of this model is: dI dt = O (I) RBostrom's claim is that for instrumental reasons an intelligent system is likely to invest part of its intelligence back into improving its intelligence. It introduces a linear self-improvement model that we will adapt here. By adopting it, we can change the model O (I) = \u03b2I + \u03b2 for some parameters, whereby both the external force and the optimisation force are positive and the contribution of the system itself."}, {"heading": "4 Recalcitrance of prediction", "text": "This year it has come to the point where it will be able to retaliate until it realizes that it will be able, that it is able to realize, that it is able, that it is able, that it is able, that it is able to unite."}, {"heading": "5 Discussion and directions for future work", "text": "We have explicitly set out the logic of an argument for worrying about the risks posed by artificial intelligence, which concerns the possibility that an autonomous intelligent system may modify itself, undergo an intelligence explosion, and take over the world in ways that run counter to human interests. In our analysis, we discover that the core of the argument are multiple claims that are much narrower than appear on the surface. In particular, we can address the problem of predicting the behavior of self-modifying intelligent systems by focusing on instrumental reasoning and their susceptibility to autonomous self-improvement. If we can show that the repetition of these tasks is predictably high, we can dismiss the likelihood of an intelligence explosion as negligible. To show how such an analysis might work, we analyze the inconsistency of prediction by showing a Bajesan model of a predictive agent that removes the obstacles to recursive self-improvement through algorithmic changes."}], "references": [{"title": "Of myths and moonshine. Contribution to the conversation on The Myth of AI on edge.org", "author": ["S. Russell"], "venue": null, "citeRegEx": "Russell,? \\Q2014\\E", "shortCiteRegEx": "Russell", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "(Russell 2014) has called for AI researchers to consider this possibility seriously because, however unlikely, its mere possibility is grave.", "startOffset": 0, "endOffset": 14}], "year": 2017, "abstractText": "In recent years prominent intellectuals have raised ethical concerns about the consequences of artificial intelligence. One concern is that an autonomous agent might modify itself to become \u201dsuperintelligent\u201d and, in supremely effective pursuit of poorly specified goals, destroy all of humanity. This paper considers and rejects the possibility of this outcome. We argue that this scenario depends on an agent\u2019s ability to rapidly improve its ability to predict its environment through self-modification. Using a Bayesian model of a reasoning agent, we show that there are important limitations to how an agent may improve its predictive ability through self-modification alone. We conclude that concern about this artificial intelligence outcome is misplaced and better directed at policy questions around data access and storage. The appetite of the public and prominent intellectuals for the study of the ethical implications of artificial intelligence has increased in recent years. One captivating possibility is that artificial intelligence research might result in a \u2018superintelligence\u2019 that puts humanity at risk. (Russell 2014) has called for AI researchers to consider this possibility seriously because, however unlikely, its mere possibility is grave. (Bostrom 2014) argues for the importance of considering the risks of artificial intelligence as a research agenda. For Bostrom, the potential risks of artificial intelligence are not just at the scale of industrial mishaps or weapons of mass destruction. Rather, Bostrom argues that artificial intelligence has the potential to threaten humanity as a whole and determine the fate of the universe. We approach this grand thesis with a measure of skepticism. Nevertheless, we hope that by elucidating the argument and considering potential objections in good faith, we can get a better grip on the realistic ethical implications of artificial intelligence. This paper is in that spirit. We consider the argument for this AI doomsday scenario proposed by Bostrom (Bostrom 2014). Section 1 summarizes Bostrom\u2019s argument and motivates the work of the rest of the paper. In focuses on the conditions of an \u201cintelligence explosion\u201d that would lead to a dominant machine intelligence averse to humanity. Section 2 argues that rather than speculating broadly about general artificial intelligence, we can predict outcomes of Copyright c \u00a9 2015, Sebastian Benthall. artificial intelligence by considering more narrowly a few tasks that are essential to instrumental reasoning. Section 3 considers recalcitrance, the resistance of a system to improvements to its own intelligence, and the ways it can limit intelligence explosion. Section 4 contains an analysis of the recalcitrance of prediction, using a Bayesian model of a predictive agent. We conclude that prediction is not something an agent can easily improve upon autonomously. Section 5 discusses the implication of these findings for further investigation into AI risk. 1 Bostrom\u2019s core argument and definitions Bostrom makes a number of claims in the course of his argument which I will outline here as distinct propositions. Proposition 1. A system with sufficient intelligence relative to other intelligent systems will have a \u2018decisive strategic advantage\u2019 and will determine the fate of the world and uni-", "creator": "LaTeX with hyperref package"}}}