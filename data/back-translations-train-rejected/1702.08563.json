{"id": "1702.08563", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "CIFT: Crowd-Informed Fine-Tuning to Improve Machine Learning Ability", "abstract": "Item Response Theory (IRT) allows for measuring ability of Machine Learning models as compared to a human population. However, it is difficult to create a large dataset to train the ability of deep neural network models (DNNs). We propose fine-tuning as a new training process, where a model pre-trained on a large dataset is fine-tuned with a small supplemental training set. Our results show that fine-tuning can improve the ability of a state-of-the-art DNN model for Recognizing Textual Entailment tasks.", "histories": [["v1", "Mon, 27 Feb 2017 22:25:45 GMT  (385kb,D)", "https://arxiv.org/abs/1702.08563v1", "10 pages, 2 tables, 2 figures"], ["v2", "Wed, 28 Jun 2017 23:59:15 GMT  (382kb,D)", "http://arxiv.org/abs/1702.08563v2", "8 pages plus references, 3 tables, 2 figures"]], "COMMENTS": "10 pages, 2 tables, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["john p lalor", "hao wu", "hong yu"], "accepted": false, "id": "1702.08563"}, "pdf": {"name": "1702.08563.pdf", "metadata": {"source": "CRF", "title": "CIFT: Crowd-Informed Fine-Tuning to Improve Machine Learning Ability", "authors": ["John P. Lalor", "Hao Wu", "Hong Yu"], "emails": ["lalor@cs.umass.edu,", "hao.wu.5@bc.edu,", "hong.yu@umassmed.edu"], "sections": [{"heading": "Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Item Response Theory", "text": "The question that has arisen in the last few years in the USA is whether it is in a position to surpass itself, and whether it is in a position to surpass itself, to surpass itself, to surpass itself, to surpass itself, to surpass itself, to surpass itself, to surpass itself, to surpass itself, to surpass itself, to surpass itself, to surpass itself, to surpass itself, to surpass itself, to surpass itself, to surpass itself, to surpass itself, to surpass itself, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload, to overload"}, {"heading": "Related Work", "text": "It is about the question of whether and in what form people are able to recognize themselves and understand what they have to do in order to understand and understand what they have to do. It is about the question if and how they should do it. It is about the question if and how they should do it. It is about the question if and how they should do it. It is about the question if and in what form they should do it. It is about the question if and in what form they should do it. It is about the question if and in what form they should do it. It is about the question if and how they should do it. It is about the question if and how they should do it. It is about the question if it is about the question if it is about the question and whether it is about the question, whether it is about the question and whether it is about the question."}, {"heading": "Crowd-Informed Fine-Tuning", "text": "We want to understand the effect of carefully selected items on a pre-trained learning model. Can small examples selected for a particular purpose be used to improve the overall performance of a model that has already been trained with a large training set? In our experiment, we refine a powerful neural network model with a selection of complementary data sets to determine where and how additional training data can improve performance, both in terms of accuracy and latent ability. We experiment with two loss rates based on the expected model performance in terms of fine-tuning the data."}, {"heading": "Model and Data", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "Experiments", "text": "To test our hypothesis, we conducted the following experiment: We trained the NSE DNN for a randomly selected subset of SNLI training (100, 1k, 2k, 10k, 100k, 200k, and the full 550k training), following the original NSE training parameters and hyperparameters (Munkhdalai and Yu 2017), the model is trained for 40 epochs on the training data and evaluated on the development set in each epoch. NSE is initialized with the 300-D GloVe 840B word embeddings (Pennington, Socher, and Manning 2014), model parameters where the development accuracy is highest and for the CIFT. These models were then trained with 4 of the 5 IRT test sets and 5 of categorical cross entropy, meaning quared error as loss functions."}, {"heading": "Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "IRT Ability", "text": "This year, it is time for us to set out to find a solution that paves the way to the future."}, {"heading": "Generalization with Fine-Tuning", "text": "We report on the accuracy of each fine-tuning loss function used with CIFT (MSE and CCE), as well as the baseline results. Figure 2 shows accuracy as a function of the training size (log scale). As Figure 2 shows, accuracy generally does not improve according to CIFT. However, the performance in terms of accuracy according to CIFT is very close to the pre-CIFT baseline. One explanation is that the models have already reached a strong point in terms of a local minimum, so there are only so many negative impacts that the additional overpass can have in terms of reducing accuracy. In fact, most of the results were achieved very early in the training process (around epoch 4 or so), suggesting that there is not too much overpass accuracy in terms of a local minimum."}, {"heading": "Discussion", "text": "By using CIFT on a powerful model, we can improve performance in terms of latent capabilities without diminishing accuracy due to overadjustments. In different subsets of test data, different fine-tuning methods lead to better performance.Our results show performance improvements in terms of capabilities as estimated with IRT. However, IRT capability measures how well an individual (or in our case, an ML model) performs in relation to a specific human population.While IRT performance is positively correlated with accuracy, it is not always true that higher accuracy implies a higher theta value. Items in an IRT test set have varying degrees of difficulty and discriminatory parameters that affect an ability estimation.Which points are correctly answered is more important than how many. Training a model to maximize capabilities directly is a difficult statement due to the small size of IRT data. However, we have shown that using CIFT with the data from CFT with a pre-signed DNN model can improve performance."}, {"heading": "Conclusion and Future Work", "text": "In this paper, we introduced CIFT, a novel fine-tuning approach to education that can improve performance for a state-of-the-art DNN by using item parameters modeled by a large collection of human response data. CIFT improves performance in terms of IRT capability without negatively impacting generalization due to overadjustments. By introducing specialized supplementary data, the model is able to update its representations to increase performance in model capability. CIFT exceeds a standard transfer learning method in terms of items that are more difficult than the test set. When testing for more difficult items, it is not helpful to identify with simpler items, as our results show that by introducing a small set of learning outcomes, the model improves performance in terms of items that are more difficult than the test set."}], "references": [{"title": "How to grade a test without knowing the answers \u2014 a bayesian graphical model for adaptive crowdsourcing and aptitude testing", "author": ["Bachrach"], "venue": "Proceedings of the 29th International Con-", "citeRegEx": "Bachrach,? \\Q2012\\E", "shortCiteRegEx": "Bachrach", "year": 2012}, {"title": "Item Response Theory: Parameter Estimation Techniques, Second Edition", "author": ["Baker", "F.B. Kim 2004] Baker", "Kim", "S.-H"], "venue": null, "citeRegEx": "Baker et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Baker et al\\.", "year": 2004}, {"title": "Curriculum learning", "author": ["Bengio"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Deep learners benefit more from out-of", "author": ["Bengio"], "venue": null, "citeRegEx": "Bengio,? \\Q2011\\E", "shortCiteRegEx": "Bengio", "year": 2011}, {"title": "Marginal maximum likelihood estimation of item parameters: Application of an em algorithm", "author": ["Bock", "R.D. Aitkin 1981] Bock", "M. Aitkin"], "venue": null, "citeRegEx": "Bock et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Bock et al\\.", "year": 1981}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Bowman"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Bowman,? \\Q2015\\E", "shortCiteRegEx": "Bowman", "year": 2015}, {"title": "Recognizing subjectivity: A case study in manual tagging", "author": ["Bruce", "R.F. Wiebe 1999] Bruce", "J.M. Wiebe"], "venue": null, "citeRegEx": "Bruce et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Bruce et al\\.", "year": 1999}, {"title": "The PASCAL Recognising Textual Entailment Challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment", "author": ["Glickman Dagan", "I. Magnini 2006] Dagan", "O. Glickman", "B. Magnini"], "venue": null, "citeRegEx": "Dagan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Maximum likelihood estimation of observer errorrates using the em algorithm", "author": ["Dawid", "A.P. Skene 1979] Dawid", "A.M. Skene"], "venue": "Journal of the Royal Statistical Society. Series C (Applied", "citeRegEx": "Dawid et al\\.,? \\Q1979\\E", "shortCiteRegEx": "Dawid et al\\.", "year": 1979}, {"title": "Building watson: An overview of the deepqa project. AI magazine 31(3):59\u201379", "author": ["Ferrucci"], "venue": null, "citeRegEx": "Ferrucci,? \\Q2010\\E", "shortCiteRegEx": "Ferrucci", "year": 2010}, {"title": "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting", "author": ["Freund", "Y. Schapire 1997] Freund", "R.E. Schapire"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "Learning whom to trust with mace", "author": ["Hovy"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Hovy,? \\Q2013\\E", "shortCiteRegEx": "Hovy", "year": 2013}, {"title": "Identifying and accounting for taskdependent bias in crowdsourcing", "author": ["Kapoor Kamar", "E. Horvitz 2015] Kamar", "A. Kapoor", "E. Horvitz"], "venue": "In Third AAAI Conference on Human Computation and Crowdsourcing", "citeRegEx": "Kamar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kamar et al\\.", "year": 2015}, {"title": "Bayesian classifier combination", "author": ["Kim", "Ghahramani 2012] Kim", "H.-C", "Z. Ghahramani"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2012}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Koller", "D. Friedman 2009] Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2009}, {"title": "Building machines that learn and think like people", "author": ["Lake"], "venue": "Behavioral and Brain Sciences", "citeRegEx": "Lake,? \\Q2016\\E", "shortCiteRegEx": "Lake", "year": 2016}, {"title": "Building an evaluation scale using item response theory", "author": ["Wu Lalor", "J.P. Yu 2016] Lalor", "H. Wu", "H. Yu"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lalor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lalor et al\\.", "year": 2016}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Marelli"], "venue": null, "citeRegEx": "Marelli,? \\Q2014\\E", "shortCiteRegEx": "Marelli", "year": 2014}, {"title": "Making sense of item response theory in machine learning", "author": ["Mart\u0131\u0301nez-Plumed"], "venue": "In ECAI,", "citeRegEx": "Mart\u0131\u0301nez.Plumed,? \\Q2016\\E", "shortCiteRegEx": "Mart\u0131\u0301nez.Plumed", "year": 2016}, {"title": "Neural semantic encoders", "author": ["Munkhdalai", "T. Yu 2017] Munkhdalai", "H. Yu"], "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association", "citeRegEx": "Munkhdalai et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Munkhdalai et al\\.", "year": 2017}, {"title": "Probabilistic modeling for crowdsourcing partially-subjective ratings", "author": ["Nguyen"], "venue": "In Proceedings of The Conference on Human Computation and Crowdsourcing (HCOMP),", "citeRegEx": "Nguyen,? \\Q2016\\E", "shortCiteRegEx": "Nguyen", "year": 2016}, {"title": "The benefits of a model of annotation", "author": ["Passonneau", "R.J. Carpenter 2014] Passonneau", "B. Carpenter"], "venue": null, "citeRegEx": "Passonneau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passonneau et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Socher Pennington", "J. Manning 2014] Pennington", "R. Socher", "C.D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Improved Boosting Algorithms Using Confidencerated Predictions", "author": ["Schapire", "R.E. Singer 1999] Schapire", "Y. Singer"], "venue": "Machine Learning", "citeRegEx": "Schapire et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1999}, {"title": "Boosting neural networks. Neural Computation 12(8):1869\u20131887", "author": ["Schwenk", "H. Bengio 2000] Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "Schwenk et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Schwenk et al\\.", "year": 2000}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Hassabis"], "venue": "Nature", "citeRegEx": "Hassabis,? \\Q2016\\E", "shortCiteRegEx": "Hassabis", "year": 2016}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["Levine Stadie", "B.C. Abbeel 2015] Stadie", "S. Levine", "P. Abbeel"], "venue": "CoRR abs/1507.00814", "citeRegEx": "Stadie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Stadie et al\\.", "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["Yosinski"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Yosinski,? \\Q2014\\E", "shortCiteRegEx": "Yosinski", "year": 2014}], "referenceMentions": [], "year": 2017, "abstractText": "Item Response Theory (IRT) allows for measuring ability of Machine Learning models as compared to a human population. However, it is difficult to create a large dataset to train the ability of deep neural network models (DNNs). We propose Crowd-Informed Fine-Tuning (CIFT) as a new training process, where a pre-trained model is fine-tuned with a specialized supplemental training set obtained via IRT modelfitting on a large set of crowdsourced response patterns. With CIFT we can leverage the specialized set of data obtained through IRT to inform parameter tuning in DNNs. We experiment with two loss functions in CIFT to represent (i) memorization of fine-tuning items and (ii) learning a probability distribution over potential labels that is similar to the crowdsourced distribution over labels to simulate crowd knowledge. Our results show that CIFT improves ability for a state-of-theart DNN model for Recognizing Textual Entailment (RTE) tasks and is generalizable to a large-scale RTE test set.", "creator": "LaTeX with hyperref package"}}}