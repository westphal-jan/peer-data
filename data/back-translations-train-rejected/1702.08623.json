{"id": "1702.08623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Progress Estimation and Phase Detection for Sequential Processes", "abstract": "Process modeling and understanding is fundamental for advanced human-computer interfaces and automation systems. Recent research focused on activity recognition, but little work has focused on process progress detection from sensor data. We introduce a real-time, sensor-based system for modeling, recognizing and estimating the completeness of a process. We implemented a multimodal CNN-LSTM structure to extract the spatio-temporal features from different sensory datatypes. We used a novel deep regression structure for overall completeness estimation. By combining process completeness estimation with a Gaussian mixture model, our system can predict the process phase using the estimated completeness. We also introduce the rectified hyperbolic tangent (rtanh) activation function and conditional loss to help the training process. Using the completeness estimation result and performance speed calculations, we also implemented an online estimator of remaining time. We tested this system using data obtained from a medical process (trauma resuscitation) and sport events (swim competition). Our system outperformed existing implementations for phase prediction during trauma resuscitation and achieved over 80% of process phase detection accuracy with less than 9% completeness estimation error and time remaining estimation error less than 18% of duration in both dataset.", "histories": [["v1", "Tue, 28 Feb 2017 03:11:33 GMT  (2027kb,D)", "http://arxiv.org/abs/1702.08623v1", "16 pages, 11 figures"], ["v2", "Thu, 29 Jun 2017 05:04:49 GMT  (2091kb,D)", "http://arxiv.org/abs/1702.08623v2", "Accepted by IMWUT/Ubicomp 2017"], ["v3", "Fri, 14 Jul 2017 20:01:31 GMT  (2024kb,D)", "http://arxiv.org/abs/1702.08623v3", "Accepted by IMWUT/Ubicomp 2017"]], "COMMENTS": "16 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.LG cs.HC", "authors": ["xinyu li", "yanyi zhang", "jianyu zhang", "yueyang chen", "shuhong chen", "yue gu", "moliang zhou", "richard a farneth", "ivan marsic", "randall s burd"], "accepted": false, "id": "1702.08623"}, "pdf": {"name": "1702.08623.pdf", "metadata": {"source": "CRF", "title": "Process Progress Estimation and Phase Detection", "authors": ["Xinyu Li", "Yanyi Zhang", "Jianyu Zhang", "Yueyang Chen", "Shuhong Chen", "Yue Gu", "Ivan Marsic", "Richard A. Farneth", "Randall S. Burd"], "emails": ["marsic}@rutgers.edu", "rburd}@childrensnational.org"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Problem Description and Related Work", "text": "Modeling the progress of a process can be viewed from three aspects: completeness, process phase and time remaining. Only a few previous studies have dealt with estimating process progress, and several challenges remain for process modelling and estimation using sensor data [2,12]. Classification models that can generate only discrete predictions are not so good for estimating process completeness as they are suitable for activity detection [7]. Without an overall assessment of completeness, the system could generate irrelevant phase predictions (such as the patient leaving the trauma room before his arrival). The duration of process performance depends on many factors and can vary significantly. For example, the duration of resuscitation of injuries may depend on the severity of existing conditions and the availability of different team roles. The speed of the process may change during the same event, so the remaining time estimates may need to be dynamically updated."}, {"heading": "3 System Structure", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Overall Structure", "text": "Our model consists of three steps (Figure 1): Feature extraction: Since we used different sensors for data acquisition in different applications, the first step consists of data representation and feature extraction. Instead of several manually created features, we applied CNN and LSTM to learn the spatial and temporal characteristics from the data [16]. We used a multimodal structure to merge the characteristics from different sensory data types. Completeness and phase estimation: We implemented a regression model based on deep learning that directly generates a single output value as regression output. We introduced an rtanh activation function for the output neuron and a conditional loss function to train the regression model. Furthermore, we used the GMM as a probable inference for phase detection and generated conditional loss for the model training.Estimation of time remaining function to train the system to update the residual process speed with the percentage of the process estimation we achieve."}, {"heading": "3.2 Feature Extraction", "text": "Similar to activity detection [7,13], the estimation of process progress is based on both temporal and spatial characteristics. Learnable filters in CNN are commonly used to extract spatial characteristics [23], and our trauma resuscitation dataset often includes depth images and audio from a Kinect [12]. Other types of sensors can be easily added and integrated into our system by adding additional multimodal branches. We chose the Kinect depth sensor for our medical application because it does not capture facial details and does not require active user involvement to support scanning. For relatively textureless depth images, we used an AlexeNet [9] (Figure 2, above)."}, {"heading": "3.3 Regression", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3.1 Regression and Completeness Estimation", "text": "Unlike image classification or activity detection, which uses the data collected at a given moment, the activation function is continuous and cannot be solved as a discrete classification problem. (For this reason, we propose a regression fit model that uses the extracted spatio-temporal characteristics to estimate the completeness of the process (Figure 3, Completeness Estimate part). Regression can be expressed as: an appropriate neural output model in which a deep neural network extracts properties from the input data and performs a regression by assigning a total completeness value (between zero and one). Regression can be expressed as: a positive output model that extracts applied features from the input data (wixi + bi)) and performs a regression by assigning a total completeness value (between zero and one)."}, {"heading": "3.3.2 Phase Detection and Conditional Loss", "text": "The regression model can intuitively estimate the pre-pre pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre-pre"}, {"heading": "3.4 Time Left Estimation", "text": "The speed of a process's performance may vary, so our remaining estimate requires a dynamic update strategy. We estimated the remaining time by calculating how long it takes this system to change by 1%. If we use p for the current completeness (in percent) and t for the time elapsed since startup, the remaining time can be estimated as follows: time remaining = (t / p) \u00b7 (1 \u2212 p) (7), where (t / p) means the time it takes the system to complete, and (1 \u2212 p) the remaining percentage until the process is complete."}, {"heading": "3.5 System Implementation", "text": "We implemented our model using the Keras framework and the TensorFlow backend. Since our proposed r\u03b2 activation and loss function does not exist in Keras, we developed these functions for this application. As suggested in previous research [9], we used the corrected linear unit (ReLU) as our CNN activation function. For the trauma resuscitation data, we initialized and trained the model with a GTX 1080 GPU. For the YouTube Olympic Swimming dataset, which has a larger input data format and a larger network structure, we used dual GTX 1080 GPUs for training. We directly used the trained weights in VGG Net [17] to initialize the network for RGB frame processing. An Adam Optimizer [8] was implemented with an initial learning rate of 0.01 and 10 \u2212 8 defect. We configured the system to stop automatically when performance did not change for three epochs with the callback function."}, {"heading": "4 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data Collection", "text": "Trauma Resuscitation Dataset: The Trauma Resuscitation Dataset was collected in a Trauma Room at Children's National Medical Center in Washington D.C. The data was collected via a Kinect depth sensor on the side wall of the Trauma Room [12,13] (Figure 5, left). Of the 150 trauma resuscations with severely injured patients, 50 had synchronized depth data and 35 of 50 had synchronized audio data. We used these 150 encoded cases to generate GMM phase distributions, and 35 cases of depth and audio data for model training and testing. Given the different patient conditions and times of day, the duration of resuscitation phases varied (Figure 5, right). Olympic Swimming Dataset: The Olympic Swimming Dataset includes 60 videos from 2004 to 2016 that were manually downloaded from YouTube. The videos were recorded by different devices and split at some competitions to ensure that not all videos were encrypted."}, {"heading": "4.2 Evaluation of Process Progress Estimation", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "4.3 Comparison of Phase Prediction to Previous Work", "text": "We compared the proposed system with our previous work. As we are the first to make completeness estimates and time estimates, we are only comparing the system performance of process phase detection with previous research. We used the same trauma resuscitation data set that was used in this previous study [12] and the EndoTube data set [22] that was previously used to evaluate EndoNet (with the same training and the same test data set). Finally, we used the TUM laptop data set that was used in previous processes. The proposed model achieved significantly higher performance compared to our previous model."}, {"heading": "5 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Application \u2014 Online Trauma Resuscitation Progress Detection", "text": "As shown (Figure 5, left), we mounted the entire system (a Kinect depth sensor and an Intel NUC mini-PC) to the side wall of the dream room. Since the mini-PC uses a low-power CPU with no dedicated GPU, we had to reduce the size of the input data to ensure it can operate an online feedback-forward network. Considering that real-time MFSC feature extraction is computationally expensive, we only used depth video to estimate the resuscitation progress. As the mini-PC has very limited computing resources at runtime, we reduced all the depth images captured online to 64 \u00d7 64 pixels before feeding them into the network. The use of depth scanning and scanning affects system performance (Figure 11). Fortunately, the problem can be solved by using more powerful computers than mini-PCs, and much more connected to the system based on our previous experiments."}, {"heading": "5.2 Limitations", "text": "The proposed system works well with several different sets of data recorded in real-world scenarios, but the completeness labels we use are based only on relative time. As previously argued, the time of a complex process can be affected by many factors (e.g., the availability of medical resources affects the waiting time of the post-secondary phase), and long waiting times do not always indicate a change in progress. Instead of labeling the data with time-based completeness, it may be useful to use activity combination indicators. For example, if both the left and right eye are checked, 5% of trauma resuscitation is complete. Labeling the detailed activity with associated completeness requires more manual coding work. In addition, for complex data sets such as medical activity sequences, it is difficult to determine the contribution of an activity combination to overall completeness under different patient conditions. More work and collaboration with the medical team is needed to design a phase recognition model based on both the progress of the process and the progress made."}, {"heading": "5.3 Extension \u2014 Smart Human Computer Interaction", "text": "The proposed system has great potential in many real world applications, such as an online trauma resuscitation system. Currently, the checklist is used in the trauma room to assist medical teams with resuscitation and to ensure that no necessary tasks are missed. The challenge is that trauma resuscitation is quick and doctors may forget to check certain items during resuscitation, and the online phase detection system can be used to remind the medical team to check certain items at the end of each stage of the process, and the future online activity detection system will provide more contextual information to make the checklist digital and smarter."}, {"heading": "6 Conclusion", "text": "We are implementing a system to estimate the progress of complex processes. We are providing two real sets of data collected from various commercially available sensors, and using several published sets of data to evaluate the system. The system has surpassed previous research and can be extended to many real-world applications that provide basic information for advanced human computer interaction. We introduced our use of the system in an actual trauma room to estimate the online resuscitation progress and potential expansion of the system. We hope that the paper can provide the following benefits to the community: 1. A regression-based system structure for estimating process completeness. 2. The GMM-based approach and conditional loss that can be used with the regression model for classification tasks. 3. The detailed system introductions that can be used as guidance for extending the system to other fields. 4. The trauma resuscitation data set and the Olympic swimming data set will be published with basic drawings that can be used for future studies."}, {"heading": "7 Acknowledgement", "text": "This research was supported by the National Institutes of Health under the reference number R01LM011834. The authors thank Colin Lea for providing the EndoTube dataset."}], "references": [{"title": "Convolutional neural networks for speech recognition", "author": ["O. Abdel-Hamid", "Mohamed", "A.-r", "H. Jiang", "L. Deng", "G. Penn", "D. Yu"], "venue": "IEEE/ACM Transactions on audio, speech, and language processing 22,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Phase recognition during surgical procedures using embedded and body-worn sensors", "author": ["J.E. Bardram", "A. Doryab", "R.M. Jensen", "P.M. Lange", "K.L. Nielsen", "S.T. Petersen"], "venue": "In Pervasive Computing and Communications (PerCom),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Modeling and online recognition of surgical phases using hidden markov models", "author": ["T. Blum", "N. Padoy", "H. Feu\u00dfner", "N. Navab"], "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Vision-based material recognition for automated monitoring of construction progress and generating building information modeling from unordered site image collections", "author": ["A. Dimitrov", "M. Golparvar-Fard"], "venue": "Advanced Engineering Informatics 28,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Automatic phase prediction from low-level surgical activities. International journal of computer assisted radiology and surgery", "author": ["G. Forestier", "L. Riffaud", "P. Jannin"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1506.02078", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Largescale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "An end-to-end generative framework for video segmentation and recognition", "author": ["H. Kuehne", "J. Gall", "T. Serre"], "venue": "In Applications of Computer Vision (WACV),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Learning convolutional action primitives for fine-grained action recognition", "author": ["C. Lea", "R. Vidal", "G.D. Hager"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Online process phase detection using multimodal deep learning", "author": ["X. Li", "Y. Zhang", "M. Li", "S. Chen", "F.R. Austin", "I. Marsic", "R.S. Burd"], "venue": "In Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON), IEEE Annual (2016),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Deep learning for rfid-based activity recognition", "author": ["X. Li", "Y. Zhang", "I. Marsic", "A. Sarcevic", "R.S. Burd"], "venue": "In Proceedings of the 14th ACM Conference on Embedded Network Sensor Systems CD-ROM (2016),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Statistical modeling and recognition of surgical workflow", "author": ["N. Padoy", "T. Blum", "Ahmadi", "S.-A", "H. Feussner", "Berger", "M.-O", "N. Navab"], "venue": "Medical image analysis 16,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Temporal segmentation of laparoscopic videos into surgical phases", "author": ["M.J. Primus", "K. Schoeffmann", "L. B\u00f6sz\u00f6rmenyi"], "venue": "In Content-Based Multimedia Indexing (CBMI),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Random forests for phase detection in surgical workflow analysis", "author": ["R. Stauder", "A. Okur", "L. Peter", "A. Schneider", "M. Kranzfelder", "H. Feussner", "N. Navab"], "venue": "In International Conference on Information Processing in Computer-Assisted Interventions", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "The tum lapchole dataset for the m2cai 2016 workflow challenge", "author": ["R. Stauder", "D. Ostler", "M. Kranzfelder", "S. Koller", "H. Feu\u00dfner", "N. Navab"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Combining embedded accelerometers with computer vision for recognizing food preparation activities", "author": ["S. Stein", "S.J. McKenna"], "venue": "In Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing (2013),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Endonet: A deep architecture for recognition tasks on laparoscopic videos", "author": ["A.P. Twinanda", "S. Shehata", "D. Mutter", "J. Marescaux", "M. de Mathelin", "N. Padoy"], "venue": "IEEE Transactions on Medical Imaging 36,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2017}, {"title": "Visualizing and understanding convolutional networks. In European conference on computer vision", "author": ["M.D. Zeiler", "R. Fergus"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Several successful systems have been introduced for image classification [1] and activity recognition [13].", "startOffset": 73, "endOffset": 76}, {"referenceID": 12, "context": "Several successful systems have been introduced for image classification [1] and activity recognition [13].", "startOffset": 102, "endOffset": 106}, {"referenceID": 2, "context": "Previous research tried to recognize medical process phase using Hidden Markov Models (HMM) or decision trees with sensor data [3,5,15], but they faced several problems.", "startOffset": 127, "endOffset": 135}, {"referenceID": 4, "context": "Previous research tried to recognize medical process phase using Hidden Markov Models (HMM) or decision trees with sensor data [3,5,15], but they faced several problems.", "startOffset": 127, "endOffset": 135}, {"referenceID": 14, "context": "Previous research tried to recognize medical process phase using Hidden Markov Models (HMM) or decision trees with sensor data [3,5,15], but they faced several problems.", "startOffset": 127, "endOffset": 135}, {"referenceID": 13, "context": "Previous approaches using domain knowledge based methods and certain instrument only worked for specific applications, such as a system specialized for the laparoscopic appendectomy [14].", "startOffset": 182, "endOffset": 186}, {"referenceID": 12, "context": "Features obtained via feature extraction do not generalize well and each sensory type may require arbitrary \u201cbest features\u201d [13].", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "These approaches do not address the associations between process phase and percentage completion, allowing the system to make phase predictions that do not always follow the logical order [12].", "startOffset": 188, "endOffset": 192}, {"referenceID": 11, "context": "We introduce and tested our model with two datasets: a medical dataset recorded in 35 actual trauma resuscitations at Children\u2019s National Medical Center (CNMC) using installed sensors (include depth camera and microphone array) [12], and an Olympic swimming dataset including 60 YouTube videos of Olympic swimming competitions in different swimming styles.", "startOffset": 228, "endOffset": 232}, {"referenceID": 20, "context": "Our system outperformed the previously proposed systems using the trauma resusciation dataset [21, 22].", "startOffset": 94, "endOffset": 102}, {"referenceID": 21, "context": "Our system outperformed the previously proposed systems using the trauma resusciation dataset [21, 22].", "startOffset": 94, "endOffset": 102}, {"referenceID": 1, "context": "Few previous studies have addressed process progress estimation and several challenges remain for the process modeling and estimation using sensor data [2,12].", "startOffset": 152, "endOffset": 158}, {"referenceID": 11, "context": "Few previous studies have addressed process progress estimation and several challenges remain for the process modeling and estimation using sensor data [2,12].", "startOffset": 152, "endOffset": 158}, {"referenceID": 6, "context": "Classification models,which can only generate discrete predictions, are not as suitable for process completeness estimation, which is a continuous variable, as they are suitable for activity recognition [7].", "startOffset": 203, "endOffset": 206}, {"referenceID": 18, "context": "The research initially tried to approach the process modeling using key activities estimated based on the output from surgical instruments [19].", "startOffset": 139, "endOffset": 143}, {"referenceID": 2, "context": "Endoscope video recordings were used for workflow analysis with HMM [3].", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "For economical reasons and ease of deployment, mobile sensors such as passive RFID and depth sensor have been used for trauma resuscitation process phase detection [2, 12].", "startOffset": 164, "endOffset": 171}, {"referenceID": 11, "context": "For economical reasons and ease of deployment, mobile sensors such as passive RFID and depth sensor have been used for trauma resuscitation process phase detection [2, 12].", "startOffset": 164, "endOffset": 171}, {"referenceID": 14, "context": "Deep Convolutional Neural Networks (CNNs) have been implemented for surgical phase detection with endoscope image [15, 22].", "startOffset": 114, "endOffset": 122}, {"referenceID": 21, "context": "Deep Convolutional Neural Networks (CNNs) have been implemented for surgical phase detection with endoscope image [15, 22].", "startOffset": 114, "endOffset": 122}, {"referenceID": 11, "context": "Our previous research used multimodal deep learning with low resolution depth images and microphone array audio for trauma resuscitation phase estimation [12], which is both privacy-preserving and easy to deploy.", "startOffset": 154, "endOffset": 158}, {"referenceID": 9, "context": "More recent work has used the temporal association for better process phase and activity modeling [10,11].", "startOffset": 98, "endOffset": 105}, {"referenceID": 10, "context": "More recent work has used the temporal association for better process phase and activity modeling [10,11].", "startOffset": 98, "endOffset": 105}, {"referenceID": 15, "context": "Instead of several types of manually crafted features, we applied CNN and LSTM to learn the spatio-temporal features from the data [16].", "startOffset": 131, "endOffset": 135}, {"referenceID": 6, "context": "2 Feature Extraction Similar to activity recognition [7,13], the estimation of process progress relies on both temporal and spatial features.", "startOffset": 53, "endOffset": 59}, {"referenceID": 12, "context": "2 Feature Extraction Similar to activity recognition [7,13], the estimation of process progress relies on both temporal and spatial features.", "startOffset": 53, "endOffset": 59}, {"referenceID": 22, "context": "The learnable filters in CNN are commonly used to extract the spatial features [23], and longshort-term memory networks (LSTM) are often used to model temporal dependencies of sequential data [6].", "startOffset": 79, "endOffset": 83}, {"referenceID": 5, "context": "The learnable filters in CNN are commonly used to extract the spatial features [23], and longshort-term memory networks (LSTM) are often used to model temporal dependencies of sequential data [6].", "startOffset": 192, "endOffset": 195}, {"referenceID": 11, "context": "Our dataset from trauma resuscitations contained low-resolution depth images and audio from a Kinect [12].", "startOffset": 101, "endOffset": 105}, {"referenceID": 8, "context": "For relatively texture-less depth images, we used an AlexNet [9] (Figure 2, top).", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "For audio data, we extracted MFSC feature maps [1] for every second, and fed it into an AlexNet [9].", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "For audio data, we extracted MFSC feature maps [1] for every second, and fed it into an AlexNet [9].", "startOffset": 96, "endOffset": 99}, {"referenceID": 11, "context": "The features extracted from different sensors were later combined in a fusion layer, based on a previous implementation [12] (Figure 2 top).", "startOffset": 120, "endOffset": 124}, {"referenceID": 12, "context": "We implemented a smaller neural network (using smaller input images) works on a mini PC (Intel NUC) mounted on a side wall in the trauma room using the same configuration as previously used [13].", "startOffset": 190, "endOffset": 194}, {"referenceID": 16, "context": "The weights from previous trained VGG Net were directly used for initialization [17].", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "A separate classification model using the same extracted features can be used for phase detection [12], but detecting phase independently from completeness ignores the associations between phase and the completeness (e.", "startOffset": 98, "endOffset": 102}, {"referenceID": 8, "context": "As proposed in previous research [9], we used the rectified linear unit (ReLU) as our CNN activation function.", "startOffset": 33, "endOffset": 36}, {"referenceID": 16, "context": "We directly used the trained weights in VGG Net [17] to initialize the network for RGB frame processing.", "startOffset": 48, "endOffset": 52}, {"referenceID": 7, "context": "An Adam optimizer [8] was implemented with initial learning rate 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 17, "context": "We adopted dropout [18] for the network to address this issue.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "We also partitioned the training and testing set by whole case instead of segmenting each case for evaluation (to minimize similarities between training and testing sets) as suggested in [12].", "startOffset": 187, "endOffset": 191}, {"referenceID": 11, "context": "The data was collected through a Kinect depth sensor mounted on the side wall of the trauma room [12,13] (Figure 5, left).", "startOffset": 97, "endOffset": 104}, {"referenceID": 12, "context": "The data was collected through a Kinect depth sensor mounted on the side wall of the trauma room [12,13] (Figure 5, left).", "startOffset": 97, "endOffset": 104}, {"referenceID": 3, "context": "In a building construction application [4], the processes are often labeled based on the availability of different construction components, and are generally invariantly sequential.", "startOffset": 39, "endOffset": 42}, {"referenceID": 11, "context": "We used the same trauma resuscitation dataset that was previously used in this previous study [12] and the EndoTube dataset [22], which has seven phases previously used to evaluate EndoNet (with the same training and testing split).", "startOffset": 94, "endOffset": 98}, {"referenceID": 21, "context": "We used the same trauma resuscitation dataset that was previously used in this previous study [12] and the EndoTube dataset [22], which has seven phases previously used to evaluate EndoNet (with the same training and testing split).", "startOffset": 124, "endOffset": 128}, {"referenceID": 19, "context": "Finally, we used the TUM LapChole dataset [20] which has been used in previous surgical process phase analysis [22].", "startOffset": 42, "endOffset": 46}, {"referenceID": 21, "context": "Finally, we used the TUM LapChole dataset [20] which has been used in previous surgical process phase analysis [22].", "startOffset": 111, "endOffset": 115}, {"referenceID": 11, "context": "The proposed model achieved significantly higher performance compared with our previous model [12] (Figure 10).", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "A major drawback of the previous work was its reliance on spatial information and ignorance of temporal associations [12].", "startOffset": 117, "endOffset": 121}, {"referenceID": 11, "context": "The previous model [12] could make incorrect predictions that do not follow common sense, e.", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "We next analyzed the EndoTube dataset, which was a part of the endoscope video challenge in 2015 and was recently used in EndoNet [22], and the TUM LapChole dataset used in the M2CAI workflow recognition challenge at the MICCAI conference 2016 [20].", "startOffset": 130, "endOffset": 134}, {"referenceID": 19, "context": "We next analyzed the EndoTube dataset, which was a part of the endoscope video challenge in 2015 and was recently used in EndoNet [22], and the TUM LapChole dataset used in the M2CAI workflow recognition challenge at the MICCAI conference 2016 [20].", "startOffset": 244, "endOffset": 248}, {"referenceID": 21, "context": "The experiments using both datasets again proved the importance of temporal associations in process phase detection, as our system again outperformed the previous purely CNN-based system [22].", "startOffset": 187, "endOffset": 191}, {"referenceID": 11, "context": "PA A P S PS PL [12] with softmax", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "[12] with constraint softmax", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "F-S Trauma Resuscitation Dataset Multimodal CNN [12] Yes Depth and audio Multimodal CNN 0.", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "50 Multimodal CNN [12] Yes Depth and audio Multimodal CNN + constraint softmax 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "Endovis Dataset Endonet [22] Yes Endoscope video CNN 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 19, "context": "TUM LapChole Dataset CNN based approach [20] Yes Laparoscopic video AlexNet + time window n/a 0.", "startOffset": 40, "endOffset": 44}, {"referenceID": 4, "context": "Other Previous Research Phase detection from lowlevel activities [5] No Activity log Decision Tree n/a 0.", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "Surgical phases detection [3] No Instrument signal HMM 0.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "Phase recognition using mobile sensors [2] No Wearable sensor data Decision Tree 0.", "startOffset": 39, "endOffset": 42}, {"referenceID": 12, "context": "Surgical workflow modeling [13] No Instrument signal HMM 0.", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "Food Preparation Activities [21] No RGB video, depth and accelerometer data Random Forest n/a 0.", "startOffset": 28, "endOffset": 32}, {"referenceID": 4, "context": "The system with best performance [5] can only predict high-level phase based on manually generated low-level activity logs.", "startOffset": 33, "endOffset": 36}], "year": 2017, "abstractText": "Process modeling and understanding is fundamental for advanced human-computer interfaces and automation systems. Recent research focused on activity recognition, but little work has focused on process progress detection from sensor data. We introduce a real-time, sensor-based system for modeling, recognizing and estimating the completeness of a process. We implemented a multimodal CNN-LSTM structure to extract the spatio-temporal features from different sensory datatypes. We used a novel deep regression structure for overall completeness estimation. By combining process completeness estimation with a Gaussian mixture model, our system can predict the process phase using the estimated completeness. We also introduce the rectified hyperbolic tangent (rtanh) activation function and conditional loss to help the training process. Using the completeness estimation result and performance speed calculations, we also implemented an online estimator of remaining time. We tested this system using data obtained from a medical process (trauma resuscitation) and sport events (swim competition). Our system outperformed existing implementations for phase prediction during trauma resuscitation and achieved over 80% of process phase detection accuracy with less than 9% completeness estimation error and time remaining estimation error less than 18% of duration in both dataset.", "creator": "LaTeX with hyperref package"}}}