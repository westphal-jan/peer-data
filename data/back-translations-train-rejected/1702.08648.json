{"id": "1702.08648", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Auto-clustering Output Layer: Automatic Learning of Latent Annotations in Neural Networks", "abstract": "In this paper, we propose a novel method to enrich the representation provided to the output layer of feedforward neural networks in the form of an auto-clustering output layer (ACOL) which enables the network to naturally create sub-clusters under the provided main class la- bels. In addition, a novel regularization term is introduced which allows ACOL to encourage the neural network to reveal its own explicit clustering objective. While the underlying process of finding the subclasses is completely unsupervised, semi-supervised learning is also possible based on the provided classification objective. The results show that ACOL can achieve a 99.2% clustering accuracy for the semi-supervised case when partial class labels are presented and a 96% accuracy for the unsupervised clustering case. These findings represent a paradigm shift especially when it comes to harnessing the power of deep networks for primary and secondary clustering applications in large datasets.", "histories": [["v1", "Tue, 28 Feb 2017 05:21:31 GMT  (11293kb,D)", "http://arxiv.org/abs/1702.08648v1", "Submitted to ICML 2017"], ["v2", "Wed, 9 Aug 2017 00:02:45 GMT  (1920kb,D)", "http://arxiv.org/abs/1702.08648v2", "Submitted to IEEE Transactions on Neural Networks and Learning Systems, 2017"]], "COMMENTS": "Submitted to ICML 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ozsel kilinc", "ismail uysal"], "accepted": false, "id": "1702.08648"}, "pdf": {"name": "1702.08648.pdf", "metadata": {"source": "META", "title": "Deep Clustering using Auto-Clustering Output Layer", "authors": ["Ozsel Kilinc", "Ismail Uysal"], "emails": ["<ozsel@mail.usf.edu>."], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to trump themselves, and they are also able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves."}, {"heading": "2. Auto-Clustering Output Layer", "text": "In fact, the fact is that most of them are able to move, to move and to move."}, {"heading": "2.1. Output Layer Modification", "text": "In fact, most people are able to move to another world, in which they are able to move, in which they are able to move, and in which they are able to move."}, {"heading": "2.2. Activity Regularization", "text": "(Z). \"(Z).\" (Z). \"(Z).\" (Z). \"(Z).\" (Z). \"(Z).\" (Z). \"(Z).\" (Z). \"(Z).\" (Z). \"(Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (Z). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4)."}, {"heading": "3. Analysis and Results", "text": "In order to observe the effects of defined regulatory conditions and the cluster performance of ACOL on a known benchmark dataset, we have created cluster problems related to MNIST (LeCun et al., 1998) for both semi-supervised and completely unsupervised uses."}, {"heading": "3.1. MNIST - Semi-supervised - 2 Class", "text": "As an example of a semi-supervised mode of use, we have relabeled MNIST in such a way that two class problems arise: t \u043a i = {0 if ti < 5 1 otherwise, where t is the original label vector for the entire data set, ti is the original label for sample i, so that 0 \u2264 ti \u2264 9 and t \u0445 i is its new label. As a classification target, we now specify whether a number is larger than 5 or not, and an unattended part of this problem will then become \"clusters of numbers 0, 1, 2, 3, 4\" and \"clusters of numbers 5, 6, 7, 8, 9.\" Before showing cluster performance, we will first show the effects of three regulatory terms that we have introduced."}, {"heading": "3.1.1. EFFECTS OF REGULARIZATION TERMS", "text": "This year it is more than ever before."}, {"heading": "3.1.2. CLUSTERING PERFORMANCE", "text": "In fact, most people who are able to survive themselves are able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think we will be able to change the world. \"He added:\" I don't think we will be able to change the world. \"He added:\" I don't think we will be able to change the world. \"He added:\" I don't think we will be able to change the world and that we are able to change the world. \""}, {"heading": "3.2. MNIST - Fully Unsupervised", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4. Conclusion", "text": "In this paper, we propose a modification at the output level of a neural network to enable unattended clustering of samples within provided class labels. We introduce and analyze a new regularization concept to facilitate and control this transformation by incorporating the distribution of neuron activations (affinity, equilibrium, and coactivity) into the cost function used for the supervised training of the modified neural network. We also propose the methodology to utilize deep neural networks in a completely unattended learning framework with a paradigm shift called deep clustering. The results of the MNIST dataset clearly show that not only are samples clustered during semi-supervised learning with visually similar features, but the improvement in unattended performance is very significant compared to conventional cluster algorithms."}], "references": [{"title": "Deep machine learning-a new frontier in artificial intelligence research [research frontier", "author": ["Arel", "Itamar", "Rose", "Derek C", "Karnowski", "Thomas P"], "venue": "Computational Intelligence Magazine, IEEE,", "citeRegEx": "Arel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Arel et al\\.", "year": 2010}, {"title": "k-means++: the advantages of careful seeding", "author": ["Arthur", "David", "Vassilvitskii", "Sergei"], "venue": "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Arthur et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2007}, {"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron C", "Vincent", "Pascal"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Fuzzy ART: fast stable learning and categorization of analog patterns by an adaptive resonance system", "author": ["Carpenter", "Gail A", "Grossberg", "Stephen", "Rosen", "David B"], "venue": "Neural Networks,", "citeRegEx": "Carpenter et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Carpenter et al\\.", "year": 1991}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["Coates", "Adam", "Ng", "Andrew Y"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Coates", "Adam", "Ng", "Andrew Y", "Lee", "Honglak"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "A practical guide to training restricted boltzmann machines. Momentum", "author": ["Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Hinton and Geoffrey.,? \\Q2010\\E", "shortCiteRegEx": "Hinton and Geoffrey.", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee-Whye"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "The self-organizing map", "author": ["Kohonen", "Teuvo"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Kohonen and Teuvo.,? \\Q1990\\E", "shortCiteRegEx": "Kohonen and Teuvo.", "year": 1990}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A review of unsupervised feature learning and deep learning for time-series modeling", "author": ["L\u00e4ngkvist", "Martin", "Karlsson", "Lars", "Loutfi", "Amy"], "venue": "Pattern Recognition Letters,", "citeRegEx": "L\u00e4ngkvist et al\\.,? \\Q2014\\E", "shortCiteRegEx": "L\u00e4ngkvist et al\\.", "year": 2014}, {"title": "The mnist database of handwritten digits", "author": ["LeCun", "Yann", "Cortes", "Corinna", "Burges", "Christopher JC"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Principal components analysis", "author": ["Pearson", "Karl"], "venue": "The London, Edinburgh and Dublin Philosophical Magazine and Journal,", "citeRegEx": "Pearson and Karl.,? \\Q1901\\E", "shortCiteRegEx": "Pearson and Karl.", "year": 1901}, {"title": "Learning internal representations by error propagation", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Rumelhart et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1985}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}], "referenceMentions": [{"referenceID": 14, "context": "Artificial neural networks, when first introduced as universal estimators, created wide spread enthusiasm especially with innovative and bio-inspired learning methodologies such as error back-propagation (Rumelhart et al., 1985).", "startOffset": 204, "endOffset": 228}, {"referenceID": 0, "context": "Deep learning mimics the operational and organizational behavior of the human brain, which works through abstraction (Arel et al., 2010) and relies on higher-level representations of features embedded in the data.", "startOffset": 117, "endOffset": 136}, {"referenceID": 10, "context": "For instance, the convolutional neural networks (ConvNet) has been applied to computer vision problems with remarkable success (Krizhevsky et al., 2012).", "startOffset": 127, "endOffset": 152}, {"referenceID": 2, "context": "Representation learning (Bengio et al., 2013) provides a way to perform unsupervised and semi-supervised learning.", "startOffset": 24, "endOffset": 45}, {"referenceID": 11, "context": "One of the applications of representation learning is unsupervised feature learning where unsupervised training is used as a pre-training stage for initializing the hidden layer parameters before they are fine-tuned by some form of supervised training using backpropogation (L\u00e4ngkvist et al., 2014).", "startOffset": 274, "endOffset": 298}, {"referenceID": 7, "context": "While unsupervised feature learning can be dated back to the invention of Principle Component Analysis (PCA) in 1901 (Pearson, 1901), ongoing studies mostly focus on architectures such as Deep Boltzmann Machines (DBM) (Salakhutdinov & Hinton, 2009; Hinton, 2010) Deep Belief Networks (DBN) (Hinton et al., 2006) and autoencoder variants (Coates & Ng, 2011; Coates et al.", "startOffset": 290, "endOffset": 311}, {"referenceID": 4, "context": ", 2006) and autoencoder variants (Coates & Ng, 2011; Coates et al., 2011).", "startOffset": 33, "endOffset": 73}, {"referenceID": 3, "context": "Unlike other existing neural network models used for clustering such as self-organizing maps (SOM) and adaptive resonance theory (ART) (Carpenter et al., 1991), ACOL does not apply competitive learning.", "startOffset": 135, "endOffset": 159}, {"referenceID": 12, "context": "In order to observe the effects of defined regularization terms and clustering performance of ACOL on a wellknown benchmark dataset, we have created clustering problems on MNIST (LeCun et al., 1998) for both semisupervised and fully unsupervised type of uses.", "startOffset": 178, "endOffset": 198}], "year": 2017, "abstractText": "In this paper, we propose a novel method to enrich the representation provided to the output layer of feedforward neural networks in the form of an auto-clustering output layer (ACOL) which enables the network to naturally create sub-clusters under the provided main class labels. In addition, a novel regularization term is introduced which allows ACOL to encourage the neural network to reveal its own explicit clustering objective. While the underlying process of finding the subclasses is completely unsupervised, semi-supervised learning is also possible based on the provided classification objective. The results show that ACOL can achieve a 99.2% clustering accuracy for the semi-supervised case when partial class labels are presented and a 96% accuracy for the unsupervised clustering case. These findings represent a paradigm shift especially when it comes to harnessing the power of deep networks for primary and secondary clustering applications in large datasets.", "creator": "LaTeX with hyperref package"}}}