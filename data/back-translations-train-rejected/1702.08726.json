{"id": "1702.08726", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Stacked Thompson Bandits", "abstract": "We introduce Stacked Thompson Bandits (STB) for efficiently generating plans that are likely to satisfy a given bounded temporal logic requirement. STB uses a simulation for evaluation of plans, and takes a Bayesian approach to using the resulting information to guide its search. In particular, we show that stacking multiarmed bandits and using Thompson sampling to guide the action selection process for each bandit enables STB to generate plans that satisfy requirements with a high probability while only searching a fraction of the search space.", "histories": [["v1", "Tue, 28 Feb 2017 10:19:30 GMT  (621kb,D)", "http://arxiv.org/abs/1702.08726v1", "Accepted at SEsCPS @ ICSE 2017"]], "COMMENTS": "Accepted at SEsCPS @ ICSE 2017", "reviews": [], "SUBJECTS": "cs.SE cs.AI cs.SY", "authors": ["lenz belzner", "thomas gabor"], "accepted": false, "id": "1702.08726"}, "pdf": {"name": "1702.08726.pdf", "metadata": {"source": "CRF", "title": "Stacked Thompson Bandits", "authors": ["Lenz Belzner"], "emails": ["belzner@ifi.lmu.de", "belzner@ifi.lmu.de"], "sections": [{"heading": null, "text": "In many cases, system requirements can be formalized in a limited temporal logic [1], [2]. For example, we see a mobile robot with planning capacities in an area with obstacles. Here, a requirement could be that there is a given sequential plan of ten actions less than e.g. three collisions with obstacles should give. Man could use this requirement as a temporal logical formula similar to the next formulating:??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "II. PRELIMINARIES", "text": "In this section we review open loop planning, multi-armed bandits and Thompson sampling."}, {"heading": "A. Open Loop Planning", "text": "In our environment, open-loop planning ([3], [4]) is an approach to finding plans that lead to the fulfillment of a particular goal without storing information about the intermediate states that occur in the execution of the plan. I.e., in the face of a series of measures A, we are only interested in finding a plan p-A *, and we only retain information about the action sequences to guide the planning process, in contrast to closed-loop planning, such as Monte Carlo tree search [7], where the selection of measures is typically conditioned by the history of previously encountered states and actions carried out."}, {"heading": "B. Multiarmed Bandits", "text": "A bandit consists of a number of weapons, each of which represents the choice of an agent. In our environment, each arm represents an action a-A. Each arm offers a specific benefit, and the agent's goal is to identify the preferred arm. He can explore the bandit by pulling one arm at a time, and observe the corresponding payout. An MAB can be interpreted as a simple Markov decision-making process with a single state. In its basic formulation, MABs already provide a clear framework for investigating the exploration and exploitation compromise inherent in decision-making amid uncertainty: should the agent select the arm that has previously proven most promising? Or should he continue to explore other options? For an up-to-date overview of MAB and its variants, see [8]."}, {"heading": "C. Thompson Sampling", "text": "Thompson sampling (TS) is a bayesian algorithm for solving an MAB. It was proposed decades ago [5], but only recently its astonishing effectiveness and universality was identified [9], [10], [11].In the case of Bernoulli rewards (as in the case of STB), the parameter of each arm to be estimated is a probability parameter Xiv: 170 2.08 726v 1 [cs.S E] 28 February 2017p [0; 1]. TS follows a posterior distribution via p based on the observed arm payouts and a previous assumption about the distribution of p. In general, the posterior arm is proportional to the probability of the observed data D (i.e. the observed payouts of an arm) multiplied by the previous distribution P (TB) over the parameters of interest, \u03b8 = p = p in our case (equation 1).P (D | massive) P (profile payout) P (1)."}, {"heading": "III. STACKED THOMPSON BANDITS", "text": "We present stacked Thompson Bandits (STB).STB works by stacking a number of MABs, treating them as a sequential decision problem.To construct a plan, STB sequentially selects an action from each MAB using Thompson samples. The resulting plan is tested for demand satisfaction using a simulation.M: Running a simulation M: Running a simulation for a given starting state S: a series of actions A: a given plan A: a given plan A and the number of given time formulas B: M: a conditional probability distribution as the following. M: P: P: P: P: S: running a simulation for a given starting state S: a given plan p: A: A: a given plan A: A: a given temporal formulas B: M: a conditional probability distribution as the following steps.M: P: P: P: P: P: P: P: P: P: A predetermined state: A: A predetermined state A: A: P: a predetermined probability distribution as the following steps.M: P: P: P: P: P: a predetermined state: A: A predetermined state: A: A: A predetermined state for a given time: P: A: P: A: A predetermined probability distribution as the following steps interprets a predetermined value."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "We implemented STB and observed whether it is able to generate plans with increasing satisfaction requirement when it performs more searchs.Algorithm 3 STB mode Plan selection 1: for i-0... h do 2: ai \u2190 arg maxa \u0445A sa, i + 1sa, i + fa, i + 2 3: p \u2190 p:: ai"}, {"heading": "A. Setup", "text": "The state s is formed by a 10 x 10 grid world, with the agent at the position (0, 0). Obstacles are randomly positioned, with an obstacle at a free position ratio of 0.2. Actions are movements in four directions up, down, left, right with obvious semantics. The agent has a Bernoulli probability for the failure of actions, which is evenly scanned from [0; 1]. Errors of action lead to the reverse movement (e.g. the failure up, results down).This represents domain noise in the simulation M, which is available to the agent. However, the task of the STB is to create a plan of length 10, which results in fewer than three collisions with obstacles when executed: \u03c6 = h \u2264 10 (collisions \u2264 2) This constellation results in a cardinality of the search space 410 = 1048576. Note, however, that due to the probability domain dynamics (i.e. the branching due to potential error of action plan), an adequate satisfaction must be obtained several times to obtain an estimation of probability."}, {"heading": "B. Results", "text": "In our test runs, we observed that STB is able to create plans with increasing probability to meet the given requirement. In particular, STB could potentially come close to optimal solutions by seeking only a fraction of the search space. Figure 1 shows an exemplary course of STB. As a reliability test, we also observed the average value of an arm (i.e. an action), as well as the coefficient of variation (CV) of the sampled plan arm distributions and the mode (i.e. the best) plan arm distributions. While the mode value is a rough estimate of the value of an arm (i.e. an action), the CV is defined as the ratio of the standard deviation to the mean value of a distribution. The CV is suitable for measuring the accuracy of a distribution when its mean value changes [12], as in the case of STB. We expect the average mode value for both the sampled and the best plans to increase in the long term, and we also expect the best plans to decrease."}, {"heading": "V. RELATED WORK", "text": "Our work on STB is heavily influenced by existing open-loop planners. Cross-entropy open-loop planning in particular is an approach to planning in large-scale continuous MDPs [4], but it is not applicable to discrete areas such as STB. Recently, cross-entropy planning has been used to search for sequences that meet a given temporal logical formula [13] in a setting for continuous motion planning. STB is subtly associated with statistical model verification (SMC) [14], [15] and, in particular, Bayesian statistical model verification [16], [17], [18]. In this setting, the aminimally required satisfaction probability for a particular, fixed sequence of actions is to be guaranteed. SMC approaches are able to provide such a result, potentially with quantifiable reliability. STB is unable to provide a once-only-quantifiability-dification of all."}, {"heading": "VI. CONCLUSION", "text": "We have presented Stacked Thompson Bandits (STB), an open loop planning algorithm for generating action sequences that are highly likely to fulfil a predetermined time logic. STB works by maintaining a stack of multi-armed bandits using Thompson sampling. We have evaluated the effectiveness of STB provisionally and empirically using a toy copy. There are different directions for future work. As mentioned above, it would be interesting to see if an ensemble approach with STB could reduce the likelihood of getting stuck in local minima. Another interesting venue would be the combination of temporal action abstraction with STB. See [19], [12] for earlier work by one of the authors on temporal abstraction in open-loop planning. Also, guiding the sampling process in a QoS-conscious manner based on required trust might prove worthwhile. See e.g. [20] for more detailed work on Sampling by the authors on multiple QB."}], "references": [{"title": "Principles of model checking", "author": ["C. Baier", "J.-P. Katoen", "K.G. Larsen"], "venue": "MIT press,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "PRISM 4.0: Verification of probabilistic real-time systems", "author": ["M. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "Proc. 23rd International Conference on Computer Aided Verification (CAV\u201911), ser. LNCS, G. Gopalakrishnan and S. Qadeer, Eds., vol. 6806. Springer, 2011, pp. 585\u2013591.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Open loop optimistic planning.", "author": ["S. Bubeck", "R. Munos"], "venue": "COLT,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Open-loop planning in large-scale stochastic domains.", "author": ["A. Weinstein", "M.L. Littman"], "venue": "in AAAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": "Biometrika, vol. 25, no. 3/4, pp. 285\u2013294, 1933.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1933}, {"title": "Analysis of thompson sampling for the multiarmed bandit problem.", "author": ["S. Agrawal", "N. Goyal"], "venue": "COLT,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Monte-carlo tree search", "author": ["G. Chaslot"], "venue": "Maastricht: Universiteit Maastricht, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Algorithms for multi-armed bandit problems", "author": ["V. Kuleshov", "D. Precup"], "venue": "arXiv preprint arXiv:1402.6028, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "A bayesian rule for adaptive control based on causal interventions", "author": ["P.A. Ortega", "D.A. Braun"], "venue": "arXiv preprint arXiv:0911.5104, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "An empirical evaluation of thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "Advances in neural information processing systems, 2011, pp. 2249\u2013 2257.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Thompson sampling: An asymptotically optimal finite-time analysis", "author": ["E. Kaufmann", "N. Korda", "R. Munos"], "venue": "International Conference on Algorithmic Learning Theory. Springer, 2012, pp. 199\u2013213.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Simulation-based autonomous systems in discrete and continuous domains", "author": ["L. Belzner"], "venue": "Ph.D. dissertation, LMU, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Cross-entropy temporal logic motion planning", "author": ["S.C. Livingston", "E.M. Wolff", "R.M. Murray"], "venue": "Proceedings of the 18th International Conference on Hybrid Systems: Computation and Control. ACM, 2015, pp. 269\u2013278.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Numerical vs. statistical probabilistic model checking", "author": ["H.L. Younes", "M. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "International Journal on Software Tools for Technology Transfer, vol. 8, no. 3, pp. 216\u2013228, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Statistical model checking: An overview", "author": ["A. Legay", "B. Delahaye", "S. Bensalem"], "venue": "International Conference on Runtime Verification. Springer, 2010, pp. 122\u2013135.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalized queries and bayesian statistical model checking in dynamic bayesian networks: Application to personalized medicine", "author": ["C.J. Langmead"], "venue": "2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "A bayesian approach to model checking biological systems", "author": ["S.K. Jha", "E.M. Clarke", "C.J. Langmead", "A. Legay", "A. Platzer", "P. Zuliani"], "venue": "International Conference on Computational Methods in Systems Biology. Springer, 2009, pp. 218\u2013234.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Bayesian statistical model checking with application to simulink/stateflow verification", "author": ["P. Zuliani", "A. Platzer", "E.M. Clarke"], "venue": "Proceedings of the 13th ACM international conference on Hybrid systems: computation and control. ACM, 2010, pp. 243\u2013252.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Time-adaptive cross entropy planning", "author": ["L. Belzner"], "venue": "Proceedings of the 31st Annual ACM Symposium on Applied Computing. ACM, 2016, pp. 254\u2013259.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Qos-aware multi-armed bandits", "author": ["L. Belzner", "T. Gabor"], "venue": "Foundations and Applications of Self* Systems, IEEE International Workshops on. IEEE, 2016, pp. 118\u2013119.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In many cases, system requirements can be formalized in a bounded temporal logic [1], [2].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "In many cases, system requirements can be formalized in a bounded temporal logic [1], [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "STB is an open loop planner [3], [4]: It does only search the space of plans.", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "STB is an open loop planner [3], [4]: It does only search the space of plans.", "startOffset": 33, "endOffset": 36}, {"referenceID": 4, "context": "Exploration of the search space and exploitation of already gathered results are balanced with Thompson sampling [5], [6].", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "Exploration of the search space and exploitation of already gathered results are balanced with Thompson sampling [5], [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 2, "context": "In our setting, open loop planning ([3], [4]) is an approach to find plans that result in satisfaction of a given goal without storing information about the intermediate states that are encountered while executing the plan.", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "In our setting, open loop planning ([3], [4]) is an approach to find plans that result in satisfaction of a given goal without storing information about the intermediate states that are encountered while executing the plan.", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "Monte Carlo Tree Search [7], where action selection is typically conditioned by the history of previously encountered states and executed actions.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "In their basic formulation, MABs already provide a clear framework for studying the exploration-exploitation tradeoff inherent to decision making under uncertainty: Should the agent select the arm that previously showed to be most promising? Or should it go on exploring other options? For a recent survey of MAB and its variants, see [8].", "startOffset": 335, "endOffset": 338}, {"referenceID": 4, "context": "It was proposed decades ago [5], but only recently its astonishing effectiveness and generality have been identified [9], [10], [11].", "startOffset": 28, "endOffset": 31}, {"referenceID": 8, "context": "It was proposed decades ago [5], but only recently its astonishing effectiveness and generality have been identified [9], [10], [11].", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "It was proposed decades ago [5], but only recently its astonishing effectiveness and generality have been identified [9], [10], [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 10, "context": "It was proposed decades ago [5], but only recently its astonishing effectiveness and generality have been identified [9], [10], [11].", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "CV is suitable to measure the accuracy of a distribution when its mean value changes [12], as is the case in STB.", "startOffset": 85, "endOffset": 89}, {"referenceID": 3, "context": "In particular, Cross Entropy Open Loop Planning is an approach for planning in large-scale continuous MDPs [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 12, "context": "Recently, cross entropy planning has been used for searching sequences that satisfy a given temporal logic formula [13] in a continuous motion planning setting.", "startOffset": 115, "endOffset": 119}, {"referenceID": 13, "context": "STB is subtly related to statistical model checking (SMC) [14], [15], and Bayesian statistical model checking in particular [16], [17], [18].", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "STB is subtly related to statistical model checking (SMC) [14], [15], and Bayesian statistical model checking in particular [16], [17], [18].", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "STB is subtly related to statistical model checking (SMC) [14], [15], and Bayesian statistical model checking in particular [16], [17], [18].", "startOffset": 124, "endOffset": 128}, {"referenceID": 16, "context": "STB is subtly related to statistical model checking (SMC) [14], [15], and Bayesian statistical model checking in particular [16], [17], [18].", "startOffset": 130, "endOffset": 134}, {"referenceID": 17, "context": "STB is subtly related to statistical model checking (SMC) [14], [15], and Bayesian statistical model checking in particular [16], [17], [18].", "startOffset": 136, "endOffset": 140}, {"referenceID": 18, "context": "See [19], [12] for previous work of one of the authors on temporal abstraction in open loop planning.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "See [19], [12] for previous work of one of the authors on temporal abstraction in open loop planning.", "startOffset": 10, "endOffset": 14}, {"referenceID": 19, "context": "[20] for previous work of the authors on QoS-aware sampling in multiarmed bandits.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "We introduce Stacked Thompson Bandits (STB) for efficiently generating plans that are likely to satisfy a given bounded temporal logic requirement. STB uses a simulation for evaluation of plans, and takes a Bayesian approach to using the resulting information to guide its search. In particular, we show that stacking multiarmed bandits and using Thompson sampling to guide the action selection process for each bandit enables STB to generate plans that satisfy requirements with a high probability while only searching a fraction of the search space.", "creator": "LaTeX with hyperref package"}}}