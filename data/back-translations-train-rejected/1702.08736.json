{"id": "1702.08736", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Analysing Congestion Problems in Multi-agent Reinforcement Learning", "abstract": "Congestion problems are omnipresent in today's complex networks and represent a challenge in many research domains. In the context of Multi-agent Reinforcement Learning (MARL), approaches like difference rewards and resource abstraction have shown promising results in tackling such problems. Resource abstraction was shown to be an ideal candidate for solving large-scale resource allocation problems in a fully decentralized manner. However, its performance and applicability strongly depends on some, until now, undocumented assumptions. Two of the main congestion benchmark problems considered in the literature are: the Beach Problem Domain and the Traffic Lane Domain. In both settings the highest system utility is achieved when overcrowding one resource and keeping the rest at optimum capacity. We analyse how abstract grouping can promote this behaviour and how feasible it is to apply this approach in a real-world domain (i.e., what assumptions need to be satisfied and what knowledge is necessary). We introduce a new test problem, the Road Network Domain (RND), where the resources are no longer independent, but rather part of a network (e.g., road network), thus choosing one path will also impact the load on other paths having common road segments. We demonstrate the application of state-of-the-art MARL methods for this new congestion model and analyse their performance. RND allows us to highlight an important limitation of resource abstraction and show that the difference rewards approach manages to better capture and inform the agents about the dynamics of the environment.", "histories": [["v1", "Tue, 28 Feb 2017 10:49:36 GMT  (1886kb,D)", "http://arxiv.org/abs/1702.08736v1", "8 pages, Adaptive Learning Agents (ALA) Workshop at AAMAS 2017 submission"], ["v2", "Thu, 30 Mar 2017 12:10:35 GMT  (1706kb,D)", "http://arxiv.org/abs/1702.08736v2", "Adaptive Learning Agents (ALA) Workshop at AAMAS 2017"]], "COMMENTS": "8 pages, Adaptive Learning Agents (ALA) Workshop at AAMAS 2017 submission", "reviews": [], "SUBJECTS": "cs.MA cs.AI", "authors": ["roxana r\\u{a}dulescu", "peter vrancx", "ann now\\'e"], "accepted": false, "id": "1702.08736"}, "pdf": {"name": "1702.08736.pdf", "metadata": {"source": "CRF", "title": "Analysing Congestion Problems in Multi-agent Reinforcement Learning", "authors": ["Roxana R\u0103dulescu", "Peter Vrancx", "Ann Now\u00e9"], "emails": ["rradules@vub.ac.be", "pvrancx@vub.ac.be", "anowe@vub.ac.be"], "sections": [{"heading": null, "text": "CCS concepts \u2022 Computing methods \u2192 Multi-agent systems; Multi-agent reinforcement learning; keywords Multi-agent reinforcement learning; Traffic jam problems; Resource abstraction. This paper extends our extended abstraction of AAMAS 2017 [9] by a detailed description of the RND problem area, a comprehensive analysis of the resource abstraction method and additional analysis of the experimental results. Published in: Proceedings of the 16th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2017), S. Das, E. Durfee, K. Larson, M. Winikoff (ed.), 8-12 May 2017, Sao Paulo, Brazil. Copyright c \u00a9 2017, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved."}, {"heading": "1. INTRODUCTION", "text": "Solving traffic jams is an important area of research, as they are present in a variety of areas such as traffic control [16], air traffic management [2], data transmission [14] and sensor networks [15]. We are interested here in the scenarios of independent learners where there is no direct communication between the actors. As the task is in hand, the number of actors exceeds the available capacity to achieve a high system benefit. Here, we consider two main scenarios of independent learners, whereby the task is to select the resources."}, {"heading": "2. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Reinforcement Learning", "text": "Reinforcement Learning (RL) [10] is a machine learning approach that allows an agent to learn how to solve a task by interacting with the environment by providing feedback in the form of a reward signal. The solution is to find a policy, i.e., a mapping between states and actions that maximize the rewards received. Q-Learning is a common RL value-based method in which a value function is iteratively updated to optimize the expected long-term reward. After a transition from the environmental state to s \u2032, Q-Learning performs the following update of its valued government action value function Q, which represents the quality of actions in certain states: Q (s, a) + \u03b1 [r] max a \u2032 Q (s, a) \u2212 Q (s, a) \u2212 Q (s, is the following update of the state action value function Q, which represents the quality of actions in certain states."}, {"heading": "2.2 Resource Selection Congestion Problems", "text": "A congestion problem from the perspective of multi-agent learning is defined by a set of n available resources. A certain capacity is defined for each resource. (...) Each resource is defined by three properties. (...) Each resource is defined by three properties. (...) Each resource is defined by three properties. (...) A resource is defined by the weighting of the resource. (...) The local utility of a resource is defined by its properties. (...) The local utility of a resource is defined by its properties. (...) The local utility of a resource is defined by its properties. (...) The first problem is defined as the problem of the beach problem. (...) In this paper we consider two resource selection problems that have become benchmark problems for the investigation of resource allocation in RL. (...) They differ mainly in relation to their local supply programs. (...) The first problem is defined as the problem of the beach issue. (...) [...]"}, {"heading": "2.3 Difference Rewards", "text": "Driven by the idea that the reward signals should allow actors to derive their individual contribution to the system, a reward signal that we are looking at here and that has been introduced in [17] is differential rewards (D). Under a global system utility program G, the differential rewards for Agent i are defined as: Di (z) = G (z) \u2212 G (z) \u2212 G (i) (5), where z refers to a general term for a state, action or pair of states, depending on the area under consideration, and G (z \u2212 i) is the global benefit of a system from which the contribution of Agent i is removed. The differential reward signal is constructed according to two main guidelines: (i) alignment of the reward received by the actor to the global system service program - factoredness, while (ii) the reduction of the influence of other actors on the reward signal - learning ability, whereby the differential reward problem is directly defined by a multi-agent in an attribute \u2212 12-7 setting \u2212 may be resolved by (ii)."}, {"heading": "2.4 Resource Abstraction", "text": "Resource abstraction [8] is an approach aimed at improving learning speed, solution quality, and scalability in major Marl congestion problems. Resource abstraction provides actors with a more informative reward that facilitates coordination in systems with up to 1000 actors [8]. Resource abstraction involves grouping a set of resources into subsets and modifying the local reward function after a resource bottleneck is reached, so that those who use it receive a higher penalty for overutilizing the resource. An abstract group is defined by aggregating the properties of the composite resources. In the bottleneck model defined above, an abstract group b has the following properties: Consumption Xb, t = assuming that it receives a higher penalty for overutilizing the resource."}, {"heading": "3. ANALYSING RESOURCE ABSTRACTION", "text": "Note that in order to apply the resource abstraction, information on the weight, capacity and consumption of resources, as well as the functioning of the system limiting its ease of application to a real area where such information is not available, is required. Although the paper [8] contains some guidelines and remarks on how the resource abstraction should be applied, clear insights and explanations on how to design the abstraction of the group are lacking. We believe that a deeper understanding of the method is beneficial in order to expand its usability and applicability. To better understand how the resource abstraction affects the collective behavior of the agents, we use Figure 1c. We map the abstract reward function of the various groups in the BPD, which differs from the size of the composing resources (from size 1 to size 5)."}, {"heading": "4. ROAD NETWORK DOMAIN", "text": "We propose the Road Network Domain (RND), a problem that introduces a scenario in which resources are not independent, since using one path also entails additional burdens for others. We model this problem as a network of paths (e.g. Figure 2), in which the actors have to choose between the paths of the network. Figure 2 is an example of a small network topology that can be explored, but which already serves the points we are aiming for. Each network of paths is modeled as a resource, according to the description in Section 2.2. The RND can be used with the utility functions of both the BPD and the TLD, the former creating a more difficult task, since the maximum value of the supply is only achieved at optimum capacity. The local reward of a network of paths P is then simply the sum of all local rewards of the composing road segments (e.g. Roads AB and BD for the path ABD): Lpath (P, t) = uncompromising L (fee)."}, {"heading": "4.1 Difference Rewards", "text": "Since the influence of Agent i on the system is limited to the composite road segments of its chosen path P, we can define the differential rewards for the RND as follows, where f is a local reward function (see Equation 1): Di (t) = Lpath (P, t) \u2212 Lpath (P \u2212 i, t) = Lpath (P, t) \u2212 conservation-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up-up)."}, {"heading": "4.2 Resource Abstraction", "text": "We will consider two approaches to defining the abstract group construction for the method of resource abstraction: via road segments or via network paths. As a road segment is a resource, the properties of an abstract group match those defined in Section 2.4 for a series of segments; the abstract reward for each road segment and its associated group b is defined as: A (b, \u0432, t) = {L (B, t), x\u0443, t \u2264 c\u0442 \u2212 f (Xb, t, Cb, Wb), x\u0443, t > c\u043a (10) with a local reward function (see Equation 1); finally, the abstract reward for choosing a path P at a given time is selected t to the sum of the abstract reward for each composing road segment: Apath (P, t) = Experience level P (b, t) = Experience level P: We will also expand the definition for an abstract group P via a series of paths P > b, we can have an Experience level xb = P, > B, Experience level xb = P."}, {"heading": "5. EXPERIMENTS", "text": "Each agent uses the Q learning algorithm with an exploration parameter = 0.05 and an exploration decay rate of 0.9999. Since an important aspect of the work is to understand and explore resource abstraction, the parameters for the experiment in Section 5.1 were chosen to match the setting used in the original paper [8]: learning rate \u03b1 = 0.1, decay rate for \u03b1 is 0.9999 and the discount factor \u03b3 = 1.0. As for the RND experiments (Section 5.2), the parameters chosen after several tests are for the local reward L, the global reward G, and the differential reward D: learning rate \u03b1 = 0.1, no decay, and the discount factor \u03b3 = 0.9."}, {"heading": "5.1 Beach Problem Domain", "text": "We borrow the original setting of 6 beach sections, each of which has a capacity of 6. There are 100 agents (who create a traffic jam scenario) and who maintain one position that completes the other. We run the scenario for 10 000 episodes and find that there is a deviation from the original position."}, {"heading": "5.2 Road Network Domain", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "6. DISCUSSION AND CONCLUSIONS", "text": "The contribution of this work is twofold: First, we introduce a new congestion problem, the Road Network Domain, where resources are no longer independent, and the choice of a route influences the load in other parts of the network. Second, we provide a thorough analysis of the resource abstraction approach to resource selection problems, along with its limitations and clear guidelines on how to best create the abstract groups according to the desired outcome. Road Network Domain represents a novel challenge to resource congestion problems and introduces the realistic aspect of interconnected resources, as we often find it in real-world applications such as power grids or transport networks. We note that the network topology used is small but sufficient to illustrate the additional challenge, and that more research is needed to assess scenarios that closely model real-world situations, closely model real-world situations."}, {"heading": "Acknowledgments", "text": "This work is supported by Flanders Innovation & Entrepreneurship (VLAIO), the SBO project 140047: Stable MultI-agent LEarnIng for neTworks (SMILE-IT)."}], "references": [{"title": "Analyzing and visualizing multiagent rewards in dynamic and stochastic domains", "author": ["A.K. Agogino", "K. Tumer"], "venue": "Autonomous Agents and Multi-Agent Systems, 17(2):320\u2013338", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "A multiagent approach to managing air traffic flow", "author": ["A.K. Agogino", "K. Tumer"], "venue": "Autonomous Agents and Multi-Agent Systems, 24(1):1\u201325", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Local Approximation of Difference Evaluation Functions", "author": ["M. Colby", "T. Duchow-Pressley", "J.J. Chung", "K. Tumer"], "venue": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pages 521\u2013529. International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Multiagent reinforcement learning in a distributed sensor network with indirect feedback", "author": ["M. Colby", "K. Tumer"], "venue": "Proceedings of the 2013 International Conference on Autonomous Agents and Multi-agent Systems, AAMAS \u201913, pages 941\u2013948, Richland, SC", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Potential-based difference rewards for multiagent reinforcement learning", "author": ["S. Devlin", "L. Yliniemi", "D. Kudenko", "K. Tumer"], "venue": "Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems, pages 165\u2013172. International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "The tragedy of the commons", "author": ["G. Hardin"], "venue": "Science, 162(3859):1243\u20131248", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1968}, {"title": "Intrusion response using difference rewards for scalability and online learning", "author": ["K. Malialis", "S. Devlin", "D. Kudenko"], "venue": "Workshop on Adaptive and Learning Agents at AAMAS (ALA-14)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Resource abstraction for reinforcement learning in multiagent congestion problems", "author": ["K. Malialis", "S. Devlin", "D. Kudenko"], "venue": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pages 503\u2013511. International  Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Analysing Congestion Problems in Multi-agent Reinforcement Learning", "author": ["R. R\u0103dulescu", "P. Vrancx", "A. Now\u00e9"], "venue": "Proceedings of the 2017 International Conference on Autonomous Agents & Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Traffic congestion management as a learning agent coordination problem", "author": ["K. Tumer", "A. Agogino", "Z. Welch", "A. Bazzan", "F. Kluegl"], "venue": "Multiagent Architectures for Traffic and Transportation Engineering, pages 261\u2013279", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Coordinating actions in congestion games: impact of top\u2013down and bottom\u2013up utilities", "author": ["K. Tumer", "S. Proper"], "venue": "Autonomous agents and multi-agent systems, 27(3):419\u2013443", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Aligning social welfare and agent preferences to alleviate traffic congestion", "author": ["K. Tumer", "Z.T. Welch", "A. Agogino"], "venue": "Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems-Volume 2, pages 655\u2013662. International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "TCP-like congestion control for layered multicast data transfer", "author": ["L. Vicisano", "J. Crowcroft", "L. Rizzo"], "venue": "INFOCOM\u201998. Seventeenth Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings. IEEE, volume 3, pages 996\u20131003. IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Coda: Congestion detection and avoidance in sensor networks", "author": ["C.-Y. Wan", "S.B. Eisenman", "A.T. Campbell"], "venue": "Proceedings of the 1st International Conference on Embedded Networked Sensor Systems, SenSys \u201903, pages 266\u2013279, New York, NY, USA", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Multi-agent reinforcement learning for traffic light control", "author": ["M. Wiering"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Optimal payoff functions for members of collectives", "author": ["D.H. Wolpert", "K. Tumer"], "venue": "Advances in Complex Systems, 4(2/3):265\u2013279", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 8, "context": "Multi-agent reinforcement learning; Congestion problems; Resource abstraction \u2217This paper expands our AAMAS 2017 extended abstract [9] with a detailed description of the RND problem domain, extensive analysis of the resource abstraction method, and additional analysis of the experimental results.", "startOffset": 131, "endOffset": 134}, {"referenceID": 15, "context": "Solving congestion problems is an important research area, as they are present in a wide variety of domains such as traffic control [16], air traffic management [2], data routing [14] and sensor networks [15].", "startOffset": 132, "endOffset": 136}, {"referenceID": 1, "context": "Solving congestion problems is an important research area, as they are present in a wide variety of domains such as traffic control [16], air traffic management [2], data routing [14] and sensor networks [15].", "startOffset": 161, "endOffset": 164}, {"referenceID": 13, "context": "Solving congestion problems is an important research area, as they are present in a wide variety of domains such as traffic control [16], air traffic management [2], data routing [14] and sensor networks [15].", "startOffset": 179, "endOffset": 183}, {"referenceID": 14, "context": "Solving congestion problems is an important research area, as they are present in a wide variety of domains such as traffic control [16], air traffic management [2], data routing [14] and sensor networks [15].", "startOffset": 204, "endOffset": 208}, {"referenceID": 12, "context": "Multi-agent reinforcement learning (MARL) has proven to be a suitable framework for such problems [13, 5, 8, 3], as it allows autonomous agents to learn in a decentralized manner, by interacting within a common environment.", "startOffset": 98, "endOffset": 111}, {"referenceID": 4, "context": "Multi-agent reinforcement learning (MARL) has proven to be a suitable framework for such problems [13, 5, 8, 3], as it allows autonomous agents to learn in a decentralized manner, by interacting within a common environment.", "startOffset": 98, "endOffset": 111}, {"referenceID": 7, "context": "Multi-agent reinforcement learning (MARL) has proven to be a suitable framework for such problems [13, 5, 8, 3], as it allows autonomous agents to learn in a decentralized manner, by interacting within a common environment.", "startOffset": 98, "endOffset": 111}, {"referenceID": 2, "context": "Multi-agent reinforcement learning (MARL) has proven to be a suitable framework for such problems [13, 5, 8, 3], as it allows autonomous agents to learn in a decentralized manner, by interacting within a common environment.", "startOffset": 98, "endOffset": 111}, {"referenceID": 16, "context": "We consider here two main approaches designed to achieve this goal: difference rewards [17] and resource abstraction [8].", "startOffset": 87, "endOffset": 91}, {"referenceID": 7, "context": "We consider here two main approaches designed to achieve this goal: difference rewards [17] and resource abstraction [8].", "startOffset": 117, "endOffset": 120}, {"referenceID": 7, "context": "In [8] the authors have shown to outperform difference rewards, however as we show in Section 5, we cannot confirm this conclusion.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "Reinforcement Learning (RL) [10] is a machine learning approach which allows an agent to learn how to solve a task by interacting with the environment, given feedback in the form of a reward signal.", "startOffset": 28, "endOffset": 32}, {"referenceID": 5, "context": "Allowing selfish entities to act in their own interest in a resource-sharing system can lead to a tragedy of the commons situation [6], which is a detrimental outcome for both the system and the agents.", "startOffset": 131, "endOffset": 134}, {"referenceID": 11, "context": "The first problem is defined as the beach problem domain (BPD) [12], where all the available resources are considered beach sections with the same weight equal to 1 and the same capacity c:", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "The second type of problem is the traffic lane domain (TLD) [11], where the agents have to select between several available lanes, each having a different capacity and weight (reflecting the importance or desirability of the lane):", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "For the BPD the congested resource can be any of the available beach sections, while for the TLD it should be the lane with the lowest weight and highest capacity combination [5].", "startOffset": 175, "endOffset": 178}, {"referenceID": 16, "context": "Driven by the idea that the reward signals should allow the agents to deduce their individual contribution to the system, a reward signal we consider here and which was introduced in [17] is difference rewards (D).", "startOffset": 183, "endOffset": 187}, {"referenceID": 10, "context": "These characteristics have proven to significantly improve learning performance [11, 7, 13], even in domains where G(z\u2212i) cannot be directly computed and should be estimated [2, 4, 3].", "startOffset": 80, "endOffset": 91}, {"referenceID": 6, "context": "These characteristics have proven to significantly improve learning performance [11, 7, 13], even in domains where G(z\u2212i) cannot be directly computed and should be estimated [2, 4, 3].", "startOffset": 80, "endOffset": 91}, {"referenceID": 12, "context": "These characteristics have proven to significantly improve learning performance [11, 7, 13], even in domains where G(z\u2212i) cannot be directly computed and should be estimated [2, 4, 3].", "startOffset": 80, "endOffset": 91}, {"referenceID": 1, "context": "These characteristics have proven to significantly improve learning performance [11, 7, 13], even in domains where G(z\u2212i) cannot be directly computed and should be estimated [2, 4, 3].", "startOffset": 174, "endOffset": 183}, {"referenceID": 3, "context": "These characteristics have proven to significantly improve learning performance [11, 7, 13], even in domains where G(z\u2212i) cannot be directly computed and should be estimated [2, 4, 3].", "startOffset": 174, "endOffset": 183}, {"referenceID": 2, "context": "These characteristics have proven to significantly improve learning performance [11, 7, 13], even in domains where G(z\u2212i) cannot be directly computed and should be estimated [2, 4, 3].", "startOffset": 174, "endOffset": 183}, {"referenceID": 7, "context": "Resource abstraction [8] is an approach that aims to improve learning speed, solution quality and scalability in large MARL congestion problems.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "Resource abstraction provides the agents with a more informative reward, facilitating coordination in systems with up to 1000 agents [8].", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "Even though the work [8] presents a few guidelines and remarks on how the resource abstraction should be applied, clear insights and explanations on how to create the group abstractions are not present.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "1 were chosen to match the setting used in the original work [8]: learning rate \u03b1 = 0.", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "We borrow the original setting from [8], with 6 beach sections, each with capacity 6, thus a total system capacity of 36.", "startOffset": 36, "endOffset": 39}, {"referenceID": 7, "context": "There are seven different resource abstraction configurations, just like in [8], composed of either two or three abstract groups.", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "In our experiment D ranks among the best performing methods, in contrast to the results obtained in [8], where D plateaued around the value of 8.", "startOffset": 100, "endOffset": 103}, {"referenceID": 7, "context": "We also note that the difference rewards application to the BPD described in Equation 14 of the work [8] does not correspond to the equation we consider to be the correct one (Equation 6).", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "Di(t) = L(\u03c8, t) \u2212 x\u03c8,te \u2212(x\u03c8,t\u22121) c [8], whereas the second term should be a function of (x\u03c8,t \u2212 1).", "startOffset": 36, "endOffset": 39}], "year": 2017, "abstractText": "Congestion problems are omnipresent in today\u2019s complex networks and represent a challenge in many research domains. In the context of Multi-agent Reinforcement Learning (MARL), approaches like difference rewards and resource abstraction have shown promising results in tackling such problems. Resource abstraction was shown to be an ideal candidate for solving large-scale resource allocation problems in a fully decentralized manner. However, its performance and applicability strongly depends on some, until now, undocumented assumptions. Two of the main congestion benchmark problems considered in the literature are: the Beach Problem Domain and the Traffic Lane Domain. In both settings the highest system utility is achieved when overcrowding one resource and keeping the rest at optimum capacity. We analyse how abstract grouping can promote this behaviour and how feasible it is to apply this approach in a real-world domain (i.e., what assumptions need to be satisfied and what knowledge is necessary). We introduce a new test problem, the Road Network Domain (RND), where the resources are no longer independent, but rather part of a network (e.g., road network), thus choosing one path will also impact the load on other paths having common road segments. We demonstrate the application of state-of-theart MARL methods for this new congestion model and analyse their performance. RND allows us to highlight an important limitation of resource abstraction and show that the difference rewards approach manages to better capture and inform the agents about the dynamics of the environment.", "creator": "LaTeX with hyperref package"}}}