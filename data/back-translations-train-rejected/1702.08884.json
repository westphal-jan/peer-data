{"id": "1702.08884", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "Low-rank Label Propagation for Semi-supervised Learning with 100 Millions Samples", "abstract": "The success of semi-supervised learning crucially relies on the scalability to a huge amount of unlabelled data that are needed to capture the underlying manifold structure for better classification. Since computing the pairwise similarity between the training data is prohibitively expensive in most kinds of input data, currently, there is no general ready-to-use semi-supervised learning method/tool available for learning with tens of millions or more data points. In this paper, we adopted the idea of two low-rank label propagation algorithms, GLNP (Global Linear Neighborhood Propagation) and Kernel Nystr\\\"om Approximation, and implemented the parallelized version of the two algorithms accelerated with Nesterov's accelerated projected gradient descent for Big-data Label Propagation (BigLP).", "histories": [["v1", "Tue, 28 Feb 2017 17:48:21 GMT  (622kb)", "http://arxiv.org/abs/1702.08884v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["raphael petegrosso", "wei zhang", "zhuliu li", "yousef saad", "rui kuang"], "accepted": false, "id": "1702.08884"}, "pdf": {"name": "1702.08884.pdf", "metadata": {"source": "CRF", "title": "Low-rank Label Propagation for Semi-supervised Learning with 100 Millions Samples", "authors": ["Raphael Petegrosso", "Wei Zhang", "Zhuliu Li", "Yousef Saad", "Rui Kuang"], "emails": ["kuang@cs.umn.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 2.08 884v 1 [cs.L G] 28 Feb 2017"}, {"heading": "1 Introduction", "text": "The unlabeled data is used to capture the underlying manifold structure and cluster by smoothness assumption, so that the information from the labeled data points can be propagated by the clusters along the manifold structure. By initializing the labels of the labeled data, the labels are propagated iteratively between adjacent labels, and the propagation process will eventually converge to the unique global optimum that minimizes a square criterion [18, 2]. To construct the similarity graph for labeling the labeled data, the labels are propagated iteratively between adjacent labels and propagated to the labels."}, {"heading": "2 Graph-based Semi-Supervised Learning", "text": "In this section we will first depict the graphene-based learning processes for the label propagation and then the two methods for approximating the equivalence of the data to the equivalence of the label matrix for the scalable label propagation matrix. (.) In a given Dataset X = x1,. (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (. (.). (.). (.). (.). (.). (.). (.). (.). (.). (. (.). (. (.). (. (.). (. (.). (.). (. (.). (. (.). (. (.). (.). (. (.). (. (.). (.). (. (.). (. (.). (. (.). (. (.). (.). (. (.). (.). (. (.). (.). (.). (. (.). (.). (. (.). (.). (.). (. (.). (.). (.).). (. (.). (.). (.). (.). (.). (. (.).). (. (.). (.). (.). (.).). (. (.). (. (.). (.). (.). (.). (. (.). (. (.).). (.). (.). (. (. (.).). (.). (. (.). (. (.). (.). (. (.).). (.). (. (.). (.).).). (. (.). (. (. (.).). (. (.).). (. (.). ("}, {"heading": "2.4 Global Linear Neighborhood Propagation", "text": "Another strategy for learning low rank representation has been suggested by the global linear neighbourhood [13], in which only one value is measured [13]. Global linear neighbourhood propagation (GLNP) has been suggested to maintain global cluster structures by studying both direct neighbours and indirect neighbours in [13]. It is shown that global linear neighbourhoods can be approximated by a low factorization of an unknown similarity curve (GLNPlearns a non-negative symmetric similarity curve by solving the following optimization problems: (2,7) minQ (F) = Jth dimension. Instead of constructing the similarity curve, GLNPlearns solves a non-negative symmetric similarity protocol by solving the following optimization problem."}, {"heading": "3 Parallel Implementation", "text": "The architecture of the parallel implementation of the low-rank label propagation algorithms is shown in Figure 1. (In this section, we first give a brief overview of the distributed memory and shared memory architecture, and linear algebra libraries used in the implementation, and then describe the parallel implementation of each algorithm. (In this section, we first give a brief overview of the distributed memory requirements on label propagation and Nystro \ufffd m low-rankmatrix computation with distributed memory architecture, and then the parallel implementation of each individual computer in a single computer with multi-threading.3.1.1 Distributed memory: The distributed memory architecture follows the SPMD (single program, multiple data) paradigm for parallelism. The same program runs on multiple CPUs according to the data decomposition. The processes communicate with each other to exchange data as required by the programs."}, {"heading": "4 Results", "text": "This year it is more than ever before."}, {"heading": "5 Discussion", "text": "In this paper, we applied a low matrix approximation and Nesterov's accelerated projected gradient descent with parallel implementation for Big Data Label Propagation (BigLP). BigLP was implemented and tested on datasets with enormous sample sizes for semi-supervised learning. Compared to sparsity-induced measures [4] for creating similarity diagrams, BigLP is better applicable to datasets with enormous sample sizes, with a relatively small number of features that need to be nuclearized for better classification in semi-supervised learning. Thrift-induced measures are based on the knowledge of all pairwise similarities and would not scale to datasets with more than hundreds of thousands of samples for better classification in semi-supervised learning due to the low scalability of the sample size and optimization on thrift. Moreover, compared to the sparsely induced measures and the local linear embedding method [12], the semitability of a semitability to construct data based on global structures could be selected locally between the LNP's neighbors."}, {"heading": "6 Funding", "text": "The research is supported by grants from the National Science Foundation (IIS 1149697) and the CAPES Foundation, Ministry of Education of Brazil (BEX 13250 / 13-2)."}, {"heading": "1 Algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Data Processing", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "In the first step in the parallel low-rank label propagation, each feature is individually shifted to contain only non-negative numbers. Given a low-rank matrix X \u2208 Rn\u00d7k, each process p will contain", "text": "Each process moves its data. Algorithm p.1 shows that each process first calculates the minimum value for each attribute on line 3, followed by an MPI All Reduce operation (line 4) to give each process the minimum value among all processes for each attribute. If the number obtained is negative, this attribute is then moved to avoid the negative number (line 5-7) The algorithm is outlined below: Algorithm 1 Parallel data shift for label multiplication 1: Procedure Par Shift (Xp, n, k) 2: for i = 0 \u2192 k \u2212 1 do 3: colMin = minj x p ji Minimum of column i 4: MPI Allreduce (colMin, 1, MPI MIN) 5: if colMin < 0 then 6: xpi = x p i \u2212 colMin 7: end if8: end for 9: return Xp10: end procedure"}, {"heading": "1.2 Data Normalization", "text": "The normalization was implemented in the MPI, also assuming that each process p-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp-Xp Xp Xp-Xp-Xp-Xp-Xp-Xp-Xp Xp-Xp Xp Xp-Xp Xp-Xp Xp Xp Xp-Xp Xp-Xp Xp Xp-Xp Xp-Xp Xp-Xp Xp Xp Xp Xp Xp Xp-Xp Xp Xp-Xp Xp Xp Xp-Xp Xp Xp Xp Xp Xp Xp Xp Xp Xp Xp-Xp-Xp Xp Xp Xp Xp Xp Xp Xp"}, {"heading": "1.3 Random sampling", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "In lines 2-6, process 0 first select k indices. This list is then sent to all processes (line 7). After, each process will look up and return the subset of the k indices which refers to data present in that process (lines 8-13). The algorithm is outlined below:", "text": "iSe rf\u00fc ide rf\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc for the R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die R\u00fc die"}, {"heading": "1.5 Parallel low-rank label propagation", "text": "In the algorithm, X and f0 are divided among the processes, so that each process has only one matrix Xp-R n p \u00b7 n and one vector f0 p-R n p \u00b7 1. The algorithm first initializes fp of size n p with an even distribution between-1 and 1 (line 2). Each process is responsible only for calculating the assigned portion of f. Lines 5-7, and lines 8-12 check for convergence. Each process returns the local vector fp. Algorithm 5 Parallel subordinate label propagation1: function Par LRLP (Xp, f0 p, n, k, \u03b1, maxIter, tol) 2: fp \u2190 Un p [\u2212 1, 1] 3: for t = 0 \u2192 maxitter \u2212 1 do 4: fptpold = f p \u2212 p & ltptmp (X p).fp \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p \u00b7 p (X p)."}], "references": [{"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM journal on imaging sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Using manifold stucture for partially labeled classification, in Advances in Neu-  ral Information", "author": ["M. Belkin", "P. Niyogi"], "venue": "Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Sparsity induced similarity measure for label propagation, in 2009", "author": ["H. Cheng", "Z. Liu", "J. Yang"], "venue": "IEEE 12th international conference on computer vision,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "A high-performance, portable implementation of the mpi message passing interface standard", "author": ["W. Gropp", "E. Lusk", "N. Doss", "A. Skjellum"], "venue": "Parallel computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Symmetric nonnegative matrix factorization for graph clustering", "author": ["D. Kuang", "H. Park", "C.H.Q. Ding"], "venue": "in SDM, SIAM / Omnipress,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["C.-J. Lin"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Training invariant support vector machines using selective sampling, Large scale kernel machines", "author": ["G. Loosli", "S. Canu", "L. Bottou"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "A method of solving a convex programming problem with convergence rate o (1/k2)", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1983}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Global linear neighborhoods for efficient label propagation", "author": ["Z. Tian", "R. Kuang"], "venue": "in SDM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines, in Proceedings of the 14th annual conference on neural information processing", "author": ["C. Williams", "M. Seeger"], "venue": "systems, no. EPFL-CONF-161322,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Inverting modified matrices", "author": ["M.A. Woodbury"], "venue": "Memorandum report,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1950}, {"title": "Improved nystr\u00f6m low-rank approximation and error analysis", "author": ["K. Zhang", "I.W. Tsang", "J.T. Kwok"], "venue": "Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "Graph-based semisupervised learning algorithms perform label propagation in a positively-weighted similarity graph between the data points [18, 2].", "startOffset": 139, "endOffset": 146}, {"referenceID": 9, "context": "We first adopted two low-rank label propagation algorithms, GLNP (Global Linear Neighborhood Propagation) [13] and Kernel Nystr\u00f6m Approximation [14], and implemented the parallelized algorithms.", "startOffset": 106, "endOffset": 110}, {"referenceID": 10, "context": "We first adopted two low-rank label propagation algorithms, GLNP (Global Linear Neighborhood Propagation) [13] and Kernel Nystr\u00f6m Approximation [14], and implemented the parallelized algorithms.", "startOffset": 144, "endOffset": 148}, {"referenceID": 10, "context": "Nystr\u00f6m Method [14] and Global Linear Neighborhood Propagation (GLNP) [13] were previously proposed to learn the low rank approximation for label propagation.", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": "Nystr\u00f6m Method [14] and Global Linear Neighborhood Propagation (GLNP) [13] were previously proposed to learn the low rank approximation for label propagation.", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "Taking advantage of the low-rank structure of In\u2212\u03b1F\u0304 F\u0304 , applying Matrix-Inversion Lemma [15] generates a simplified solution as", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "The Nystr\u00f6m method generates low-rank approximations of W using a subset of the samples in X [14].", "startOffset": 93, "endOffset": 97}, {"referenceID": 12, "context": "The k centroids obtained from the k-means were used as the landmark points to improve the approximation over random sampling [16].", "startOffset": 125, "endOffset": 129}, {"referenceID": 9, "context": "4 Global Linear Neighborhood Propagation Another strategy to learn the low rank representation is through global linear neighborhood [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 9, "context": "Global linear neighborhood propagation (GLNP) was proposed to preserve the global cluster structures by exploring both the direct neighbors and the indirect neighbors in [13].", "startOffset": 170, "endOffset": 174}, {"referenceID": 9, "context": "7), a multiplicative updating algorithm for nonnegative matrix factorization was proposed in [13].", "startOffset": 93, "endOffset": 97}, {"referenceID": 4, "context": "7) is a fourth order non-convex function of F similar to the symmetric NMF problem in [7].", "startOffset": 86, "endOffset": 89}, {"referenceID": 7, "context": "In [10], an optimal first order Nesterov\u2019s method was proposed to achieve O(1/r) convergence rate with f(a \u2212 a) \u2264 2L||a \u2212a|| r2 .", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "Since Nesterov\u2019s method is often used to accelerate the projected gradient descent to solve constraint optimization problems [1, 11].", "startOffset": 125, "endOffset": 132}, {"referenceID": 5, "context": "(\u2207Q(F ))ij , if Fij \u2265 0 min(0, (\u2207Q(F ))ij), otherwise The stopping condition ||\u2207PQ(F t)|| \u2264 \u01eb||\u2207Q(F 0)|| checks if a point F t is close to a stationary point in a bound-constrained optimization problem [8].", "startOffset": 202, "endOffset": 205}, {"referenceID": 5, "context": "The step size \u03b1t in the projected gradient descent is chosen by Backtracking line search [3, 8] as: Given 0 < \u03b2 < 1 and 0 < \u03c3 < 1, starting with \u03b11 = 1 and shrinking \u03b1 as \u03b1t+1 := \u03b2\u03b1t until the condition Q(Y t+1)\u2212Q(Y ) \u2264 \u03c3\u3008\u2207Q(Y ), (Y t+1\u2212Y t)\u3009 is satisfied.", "startOffset": 89, "endOffset": 95}, {"referenceID": 3, "context": "Message Passing Interface (MPI) [6] was used to implement the distributed memory architecture.", "startOffset": 32, "endOffset": 35}, {"referenceID": 0, "context": "Algorithm 3 Parallel GLNP - Multiplicative update rule 1: function Par GLNP MUL(X,m, n, k,maxIter, tol) 2: F \u2190 Um\u00d7k[0, 1] 3: for t = 0 \u2192 maxIter do 4: Fold = F 5: B = X(XF ) 6: D = F (FB) 7: G = B(FF ) 8: for i = 0 \u2192 m\u2212 1 do 9: for j = 0 \u2192 k \u2212 1 do 10: Fij = Fijsqrt(2Bij/(Dij +Gij)) 11: end for 12: end for 13: if max(abs(Fold \u2212 F )) < tol then 14: break 15: end if 16: end for 17: return F 18: end function", "startOffset": 115, "endOffset": 121}, {"referenceID": 0, "context": "The GLNP implementation with projected gradient Algorithm 4 Parallel GLNP - Projected Gradient Descent with Line Search 1: function Par GLNP APGD(X ,m,n,k,maxIter, ,maxInnerIter,beta,tol,roll) 2: F \u2190 Um\u00d7k[0, 1] 3: Y = F 4: for t = 0 \u2192 maxIter do 5: B = X(XY ) 6: D = Y (Y B) 7: G = B(Y Y ) 8: for i = 0 \u2192 m\u2212 1 do 9: for j = 0 \u2192 k \u2212 1 do 10: Gradij = 2Dij + 2Gij \u2212 4Bij 11: end for 12: end for 13: Grad0 = Grad 14: Grad = Grad/sqrt(sum(Grad)) 15: objold = obj 16: obj = ||X \u2212 Y Y X ||2 17: alpha = 1 18: for inner = 0 \u2192 maxInnerIter do 19: Y1 = max(Y \u2212 alpha.", "startOffset": 204, "endOffset": 210}, {"referenceID": 6, "context": "mnist8m is the handwritten digit data from [9] which contains digits 7 and 9 for classification.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "Compared with sparsity induced measures [4] to construct similarity graphs, BigLP is more applicable to the datasets of huge sample size with a relatively small number of features that need to be kernelized for better classification in semi-supervised learning.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "In addition, compared with the sparsity induced measures and local linear embedding method [12], in which the neighbors are selected \u201clocally\u201d, GLNP preserves the global structures among the data points, and construct more robust and reliable similarity graphs for graph-based semi-supervised learning.", "startOffset": 91, "endOffset": 95}], "year": 2017, "abstractText": "The success of semi-supervised learning crucially relies on the scalability to a huge amount of unlabelled data that are needed to capture the underlying manifold structure for better classification. Since computing the pairwise similarity between the training data is prohibitively expensive in most kinds of input data, currently, there is no general readyto-use semi-supervised learning method/tool available for learning with tens of millions or more data points. In this paper, we adopted the idea of two low-rank label propagation algorithms, GLNP (Global Linear Neighborhood Propagation) and Kernel Nystr\u00f6m Approximation, and implemented the parallelized version of the two algorithms accelerated with Nesterov\u2019s accelerated projected gradient descent for Big-data Label Propagation (BigLP). The parallel algorithms are tested on five real datasets ranging from 7000 to 10,000,000 in size and a simulation dataset of 100,000,000 samples. In the experiments, the implementation can scale up to datasets with 100,000,000 samples and hundreds of features and the algorithms also significantly improved the prediction accuracy when only a very small percentage of the data is labeled. The results demonstrate that the BigLP implementation is highly scalable to big data and effective in utilizing the unlabeled data for semi-supervised learning.", "creator": "LaTeX with hyperref package"}}}