{"id": "1703.00050", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2017", "title": "SceneSeer: 3D Scene Design with Natural Language", "abstract": "Designing 3D scenes is currently a creative task that requires significant expertise and effort in using complex 3D design interfaces. This effortful design process starts in stark contrast to the easiness with which people can use language to describe real and imaginary environments. We present SceneSeer: an interactive text to 3D scene generation system that allows a user to design 3D scenes using natural language. A user provides input text from which we extract explicit constraints on the objects that should appear in the scene. Given these explicit constraints, the system then uses a spatial knowledge base learned from an existing database of 3D scenes and 3D object models to infer an arrangement of the objects forming a natural scene matching the input description. Using textual commands the user can then iteratively refine the created scene by adding, removing, replacing, and manipulating objects. We evaluate the quality of 3D scenes generated by SceneSeer in a perceptual evaluation experiment where we compare against manually designed scenes and simpler baselines for 3D scene generation. We demonstrate how the generated scenes can be iteratively refined through simple natural language commands.", "histories": [["v1", "Tue, 28 Feb 2017 20:47:47 GMT  (7392kb,D)", "http://arxiv.org/abs/1703.00050v1", null]], "reviews": [], "SUBJECTS": "cs.GR cs.CL cs.HC", "authors": ["angel x chang", "mihail eric", "manolis savva", "christopher d manning"], "accepted": false, "id": "1703.00050"}, "pdf": {"name": "1703.00050.pdf", "metadata": {"source": "META", "title": "SceneSeer: 3D Scene Design with Natural Language", "authors": ["Angel X. Chang", "Mihail Eric", "Manolis Savva", "Christopher D. Manning"], "emails": ["manning}@cs.stanford.edu"], "sections": [{"heading": "INTRODUCTION", "text": "In fact, most people are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3D Design Interfaces", "text": "Current research typically focuses on low-level interfaces to 3D modeling. There is much work on input methods and schemes to facilitate 3D navigation [14, 15] and manipulation of primitives such as curves [17]. However, there is little work on manipulating 3D scenes at the semantic level. Recent work has explored the possibility of analyzing natural language for design problems in the context of mechanical computer-aided design software [5]. Our motivation is similar, but we are focusing on the more open setting of generating and manipulating 3D scenes in natural language. Another line of work focuses on defining semantic APIs at a higher level for scripting 3D animations and storyboards [7, 13]. This earlier work shows that people largely favor higher-level manipulation and specifications regarding semantic concepts such as \"front,\" \"\" left, \"and\" top-level manipulation at a lower level. \""}, {"heading": "Automatic Scene Layout", "text": "Recent research in computer graphics has focused on automatically creating 3D scenes from training data. Prior to working on the scene layout, much of the focus was on interior design and determining good furniture layouts by optimizing energy functions that capture the quality of a proposed layout. These energy functions are encoded in interior design guidelines [16] or were learned from scenery input data [10]. Knowledge of the occurrence of objects and spatial relationships is represented by simple models such as mixtures of casters into paired object positions and orientations. Methods for learning the scene structure have been demonstrated using various data sources, including simulation of human agents in 3D scenes [11, 12] and analysis of supporting contact points in scanned environments [18]. Although SCENESEER is also a system for creating 3D scenes, it focuses on providing an interactive natural language interface for this task."}, {"heading": "Text to Scene Systems", "text": "The SHRDLU [20] and PUT [6] systems were pioneers in parsing instructions in natural language, but generalizing their approach to more realistic scenarios that resemble the real world is challenging. Recent work on the WordsEye system [8, 9] and other similar approaches [19] has shown that more general text-based 3D scene creation can handle complex scenes. Authors convincingly demonstrate the usefulness of text for scene creation, but point out that their systems are limited by a lack of implicit spatial knowledge. As a result, unnatural language, such as \"the stool is 1 foot south of the table,\" must be used to fully specify the scene. More recently, Chang et al. [3] have shown how linguistic and non-linguistic spatial knowledge can be learned directly from existing scene data and how the scene can be dynamically paralyzed when natural scenery is interactive."}, {"heading": "APPROACH OVERVIEW", "text": "How could we create an interactive text to a 3D scenario system? The user should be able to describe and manipulate a scene with concise natural language. To make this possible, we need to analyze the input text on a set of explicit constraints on objects and their arrangement."}, {"heading": "Representation", "text": "There is a large gap between the representations typically used for 3D object and scene representations and the high-level semantics that people assign to scenes. Here, we define a scene template representation according to the approach of Chang et al. [3] to explicitly represent the information needed to link higher semantics to lower-level geometric representations. Since natural language usually expresses high-level semantics, we can consider the task of text to scene as a problem, first extracting a higher-level scene template representation and then mapping it into a concrete geometric representation."}, {"heading": "Scene Template", "text": "A scene template t = (O, C, Cs) consists of a series of object descriptions O = {o1,..., to} and constraints C = {c1,..., ck} on the relationships between objects. Also, a scene template has a scene type Cs. This provides a high-level but unambiguous representation of the scene structure. Each object oi, has associated properties such as category label, basic attributes such as color and material, and the number of occurrences in the scene. For constraints, we focus on spatial relationships between objects, expressed as predicates of the shape supported by (oi, oj) or left (oi, oj) where oi and oj are recognized objects."}, {"heading": "Geometric Scene", "text": ". A geometric scene consists of a series of 3D model examples {i1,.., in} in which each model instance ij = (mj, Tj) is represented by the model mj in the model database and the transformation matrix Tj. The model represents the physical appearance (geometry and texture) of the object, while the transformation matrix encodes the position, orientation and scaling of the object. Working directly with such low-level representations is unnatural for humans, which is a factor in the difficulty of learning current 3D scene design surfaces. We create a geometric scene from a scene template by selecting suitable models from a 3D model database and determining transformations that optimize its layout to meet spatial constraints."}, {"heading": "Model Formulation", "text": "We take a probabilistic view and model the task as the problem of estimating the distribution of possible scenes P (s | u). The distribution P (s | u) can be estimated from the initial statement u (s | u). This allows us to draw on previous knowledge to draw conclusions and use new advances in machine learning to learn from data. P (s | u) distribution can be selected from random samples to generate plausible scenes that are presented to the user. We can further decompose the P (t | u) model of a scene selection and select the scene selection in P (t | u), where P (t | u) is the probability of a scene template, and the scene selection is a ready-made template. In our pipelined system we assume that s is independent of t, u and that t is \"independent of getP (s | u) = P (t | u).\""}, {"heading": "SEMANTIC PARSING", "text": "In semantic parsing, we take the input text and create a scene template that identifies the objects and the relationships between them. We follow the approach of Chang et al. [3] to process the input text using the Pipeline1 of Stanford CoreNLP, identify visualizable objects, physical properties of the objects, and extract dependency patterns for spatial relationships between the objects. Then, the analyzed text is deterministically mapped to the scene template or scene operations. An interesting possibility for future research is to automatically learn how to map text using more advanced semantic analysis methods. SCENE INFERENCE Once we have received a scene template with the explicitly specified constraints, we expand it to include derived objects and implicit constraints. As an example, we use the support hierarchy before Psupport support support to find the superior object category for each object in the scene."}, {"heading": "SCENE GENERATION", "text": "Once we have a complete scene template, we need to select the objects we want to represent according to certain attributes and arrange the models in the scene based on constraints. During this step, we strive to find the most likely scene and give the user the opportunity to regenerate the scene with other models. This step can be extended to take into account correlations between objects (e.g. a lamp on a table should not be a floor model).Object Layout Given the selected models, scene objects and priors based on spatial relationships, we find an arrangement of objects within the scene that maximizes the likelihood of layout among the given scene elements."}, {"heading": "Scene Refinement", "text": "Figure 6 shows a sequence of operations to add a carpet to the living room that we have previously created. Combined with traditional mouse and keyboard navigation, these textual interactions enable high-level scene editing operations that are interpreted within the current viewing context (e.g. moving the carpet backwards from the viewer's perspective in the third panel).Without such high-level operations, the necessary interaction would involve several steps to find, place, orient and manually adjust its position."}, {"heading": "GENERATED SCENE EVALUATION", "text": "We evaluate the results of our system by asking people to judge how well generated scenes match the given input descriptions. This is an appropriate initial assessment, since in a practical application scenario, a scene that matches the input description would provide a good starting point for further refinements. We compare versions of our system against the advantages of different components. We also establish an upper baseline by asking people to manually design scenes that match the same descriptions, using a simple scene design interface used in previous work [10]."}, {"heading": "Conditions", "text": "We compare seven conditions: basic condition, + sup, + sup + spat, + sup + prior, full, full + inference and human. Basic condition is a simple layout approach that does not use any learned aids, spatial relations or placement priorities. conditions + sup, + sup + spat and + sup + prior and full (which include all three priorities) test the utility of adding these learned priorities to the system. full + inference finally implies the selection and arrangement of additional objects, while the human being consists of the manually designed 3D scenes created by humans based on the descriptions. For each of these conditions, we create a series of 50 scenes, one for each of the text descriptions entered. In total, we have 350 stimulus scenario description pairs that are evaluated by humans during our experiment (see Figure 8 for example descriptions and scenes)."}, {"heading": "Participants", "text": "Participants were recruited online on Amazon Mechanical Turk and had to speak fluent English. We recruited a total of 97 participants to evaluate the quality of the scenes generated in relation to the reference text descriptions. For the human condition, the scenes were created by another group of 50 participants who received the text descriptions and were asked to create a corresponding scene (see last column in Figure 8)."}, {"heading": "Procedure", "text": "Participants were asked to rate each pair on a 7-point Likert scale to indicate \"how well the scene matches the description,\" with a score of 1 indicating a very bad game and a score of 7 indicating a very good game. Participants were instructed to pay particular attention to three points: (1) Are the objects mentioned in the description included in the scene? (2) Are the relationships between the objects in the scene described correct? and (3) Does the scene as a whole match the description? Figure 7 shows a screenshot of the user interface we used to conduct the experiment."}, {"heading": "Design", "text": "The experiment was a factorial design within the subjects with the factors {basic, + sup, + sup + spat, + sup + prior, full, full + infolded, human}, description {1... 50} and participants {1... 97} as factors. The Likert rating for the correspondence between description and scene was the dependent measure."}, {"heading": "Results", "text": "In total, the 97 participants gave 2910 scene ratings for the 350 stimulus scene description pairs with an average of 8.39 ratings per pair (standard deviation 3.56 ratings)."}, {"heading": "Generated scene ratings", "text": "The average ratings for each condition in our experiment are summarized in Table 2, and the rating distributions are in Figure 9. We see that the basic condition receives the lowest average rating, while predictably the scenes designed by humans receive the highest rating. Adding learned support, spatial relation analysis, and priorities for relative position improves the rating for the scenes generated by our system, and the full combined condition receives the highest average rating. We note that the scenes generated with additional inferred objects (full + inference) actually receive a lower rating. We suspect that two factors may contribute to the lower rating for full + inference. First, adding additional objects makes the scene layout more complicated and tends to issue object selection or spatial relation errors. Second, inferred objects are not explicitly mentioned in the description, so that participants may not consider them to be significantly improving the quality of the scene (although they were instructed to view additional objects positively)."}, {"heading": "Statistical analysis", "text": "The statistical significance of the influence of the conditional factor on ratings was determined using a hybrid effect model, where the condition is considered a fixed effect and the participant and the description are considered random effects, since the latter two are randomly drawn from a large pool of potential participants and descriptions3. Most of the paired differences between the conditions for mean scene evaluation were significant in Wilcoxon-Ranksum tests with the Bonferroni-Holm correction (p < 0.05), with the exception of comparisons between + sup + spat, + sup + before and complete, which were not significant."}, {"heading": "Summary", "text": "The experimental results show that SCENESEER can generate plausible 3D scenes from input descriptions, and the various components of the system, which use learned support relationships, spatial parsing and relative position priorities, all contribute to improving the quality of the generated scenes. Implicitly inferred additional objects do not improve the human-given correspondences between scenes and descriptions."}, {"heading": "DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Limitations", "text": "The unoptimized implementation of our system takes several seconds to analyze, derive and generate a scene, depending on how many objects and relationships are expressed in the input. 3We used the lme4 R package and optimized it for maximum logarithms [1]. This mainly depends on the number of objects and relationships that are expressed in the input. Unfortunately, this makes interactive use in practical environments slow. However, the system can easily be optimized for real-time editing. A more important limitation is that we currently handle a limited domain of scenes. Analyzing scene descriptions using existing methods of natural language processing is error-prone even for simple, everyday language, and the publicly available 3D model datasets tend to be furniture and electronics, limiting the types of potential output scenes (e.g. \"the flower garden outside the window\" is a challenging case). Furthermore, generating scenes with organic, complex object geometry is a challenge for our simplistic approach to the layout, which we ignore."}, {"heading": "Future Work", "text": "A promising direction for future work is to integrate systems such as SCENESEER with language into text systems, enabling speech-driven and more natural dialogue interactions, which may be particularly relevant in the context of increasingly popular virtual and augmented reality systems, where text input is more cumbersome. Harnessing the interactive nature of the system to improve spatial knowledge is an exciting way to work in the future. For example, by observing where the user decides to place objects manually, we can improve our placement priorities. A natural way for the user to provide feedback and corrections to system errors is through dialogue-based interaction, which extends the mandatory text comment."}, {"heading": "CONCLUSION", "text": "We presented SCENESEER, a text-to-3D scene generation system with semantic-conscious textual interactions for iterative 3D scene design. We evaluated the scenes generated by our system through a human judgement experiment and confirmed that we can create scenes that match the input text descriptions and approach the quality of manually designed scenes. We demonstrated that automatically generated 3D scenes can be used as a starting point for iterative refinement through a series of text commands at the highest level. Natural language commands are a promising interaction method for 3D scene design tasks, as they abstract much of the tedious manipulation at a low level. In this way, we can bridge the semantic gap between high user intentions and low geometric operations for 3D scene design. We believe that the text-to-3D scene design domain will present many challenging research issues at the interface of computer graphics, enabling our scenario design system to step in a natural way."}, {"heading": "APPENDIX Scene Layout Score", "text": "The Scene Layout Score is obtained from L = \u03bbobjLobj + \u03bbrelLrel, a weighted sum of the object arrangement Lobj Score and Constraint Satisfaction Lrel Score, which results from the definition of Chang et al. [3]: Lobj = \u2211 oi Psurf (Sn | Coi) \u0445 oj \u0445 F (oi) Prelpos (\u00b7) Lrel = \u2211 ci Prel (ci), where F (oi) are the sibling and parent objects of oi."}, {"heading": "Spatial Knowledge Priors", "text": "We use similar definitions for spatial knowledge priors as Chang et al. [3] with updated support and binding interface priorities. Spatial knowledge priors are valued on the basis of scene observations. To cope with data sparseness, we use our category taxonomy. If there are fewer than k = 5 supportive observations, we retreat to a parent category in taxonomy for more informative priors."}, {"heading": "Object Occurrence Priors", "text": "Occurrences are given by simple Bayesian statistics of objects occurring in scenes: Pocc (Co | Cs) = number (Co in Cs) number (Cs)"}, {"heading": "Support Hierarchy Priors", "text": "We estimate the probability that a parent category Cp supports a given child category Cc as a simple conditional probability based on normalized observation numbers: Psupport (Cp | Cc) = count (Cc to Cp) count (Cc)"}, {"heading": "Support and Attachment Surface Priors", "text": "Similarly, the parental support surface is determined by the following values: Psurfsup (s | Cc) = Number (Cc on the surface with s) Number (Cc) The parental support surface becomes normal with the surface (top, bottom, horizontal) and whether the surface is inside (inwards) or outside (outwards). For example, a room has a floor that is an upward-facing support surface: The child support surfaces are prioritized by: Psurgrease (s | Cc) = Number (Cc on the surface s) Number (Cc) of object surfaces are characterized by the Bounding Box side: top, bottom, front, rear, left or right. For example, posters on their back are fixed to walls, carpets on their underside to floors. If no observations are available, we use the model geometry to determine the surfaces and surfaces."}, {"heading": "Relative Position Priors", "text": "We model the relative position of objects based on their object categories and the current scene type: i.e. the relative position of an object in the category Cobj is in relation to another object in the category Cref and for a scene type Cs. We condition the relationship R between the two objects, whether they are siblings (R = siblings) or child-parents (R = child parent), and define the relative position previously as: Prelpos (x, y, \u03b8 | Cobj, Cref, Cs, R), which we estimate by estimating the core density."}], "references": [{"title": "Mixed-effects modeling with crossed random effects for subjects and items", "author": ["R.H. Baayen", "D.J. Davidson", "D.M. Bates"], "venue": "Journal of memory and language", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Interactive learning of spatial knowledge for text to 3D scene generation", "author": ["A.X. Chang", "M. Savva", "C.D. Manning"], "venue": "In Proceedings of the ACL 2014 Workshop on Interactive Language Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning spatial knowledge for text to 3D scene generation", "author": ["A.X. Chang", "M. Savva", "C.D. Manning"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Content Creation with Semantic Attributes", "author": ["S. Chaudhuri", "E. Kalogerakis", "S. Giguere", "Funkhouser", "T. AttribIt"], "venue": "In UIST", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Natural language problem definition for computer-aided mechanical design", "author": ["H. Cheong", "W. Li", "L. Shu", "A. Tessier", "E. Bradner", "F. Iorio"], "venue": "In CHI-DSLI Workshop 2014 Conference Proceedings", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Language-based interactive manipulation of objects", "author": ["S.R. Clay", "Wilhelms", "J. Put"], "venue": "Computer Graphics and Applications", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1996}, {"title": "Alice: lessons learned from building a 3D system for novices", "author": ["M. Conway", "S. Audia", "T. Burnette", "D. Cosgrove", "K. Christiansen"], "venue": "In Proceedings of the SIGCHI conference on Human Factors in Computing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "an automatic text-to-scene conversion system", "author": ["B. Coyne", "Sproat", "R. WordsEye"], "venue": "In SIGGRAPH", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Example-based synthesis of 3D object arrangements", "author": ["M. Fisher", "D. Ritchie", "M. Savva", "T. Funkhouser", "P. Hanrahan"], "venue": "SIGGRAPH Asia", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Learning object arrangements in 3D scenes using human context", "author": ["Y. Jiang", "M. Lim", "A. Saxena"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Infinite latent conditional random fields for modeling environments through humans", "author": ["Y. Jiang", "A. Saxena"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Lessons learned from designing a programming system to support middle school girls creating animated stories", "author": ["C. Kelleher", "R. Pausch"], "venue": "In Visual Languages and Human-Centric Computing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Viewcube: a 3D orientation indicator and controller", "author": ["A. Khan", "I. Mordatch", "G. Fitzmaurice", "J. Matejka", "G. Kurtenbach"], "venue": "In Proceedings of the 2008 symposium on Interactive 3D graphics and games,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Multiscale 3D navigation", "author": ["J. McCrae", "I. Mordatch", "M. Glueck", "A. Khan"], "venue": "In Proceedings of the 2009 symposium on Interactive 3D graphics and games,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Interactive furniture layout using interior design guidelines", "author": ["P. Merrell", "E. Schkufza", "Z. Li", "M. Agrawala", "V. Koltun"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "When it gets more difficult, use both hands: exploring bimanual curve manipulation", "author": ["R. Owen", "G. Kurtenbach", "G. Fitzmaurice", "T. Baudel", "B. Buxton"], "venue": "In Proceedings of Graphics Interface", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Learning spatial relationships between objects", "author": ["B. Rosman", "S. Ramamoorthy"], "venue": "IJRR", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Real-time automatic 3D scene generation from natural language voice and text descriptions", "author": ["L.M. Seversky", "L. Yin"], "venue": "In Proceedings of the 14th annual ACM international conference on Multimedia", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}], "referenceMentions": [{"referenceID": 12, "context": "There is much work on input methodologies and schemes for easing navigation in 3D [14, 15] and manipulation of primitives such as curves [17].", "startOffset": 82, "endOffset": 90}, {"referenceID": 13, "context": "There is much work on input methodologies and schemes for easing navigation in 3D [14, 15] and manipulation of primitives such as curves [17].", "startOffset": 82, "endOffset": 90}, {"referenceID": 15, "context": "There is much work on input methodologies and schemes for easing navigation in 3D [14, 15] and manipulation of primitives such as curves [17].", "startOffset": 137, "endOffset": 141}, {"referenceID": 4, "context": "Recent work has explored the avenue of parsing natural language for design problem definitions in the context of mechanical computer-aided design software [5].", "startOffset": 155, "endOffset": 158}, {"referenceID": 6, "context": "Another line of work focuses on defining higher-level semantic APIs for scripting 3D animations and storyboards [7, 13].", "startOffset": 112, "endOffset": 119}, {"referenceID": 11, "context": "Another line of work focuses on defining higher-level semantic APIs for scripting 3D animations and storyboards [7, 13].", "startOffset": 112, "endOffset": 119}, {"referenceID": 5, "context": "We are closer to early seminal work in placing simple geometric objects in 3D through textual commands [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "[4] have demonstrated a novel 3D model design interface where users can control the desired values of high-level semantic attributes (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "These energy functions are encoded from interior design guidelines [16] or learned from input scene data [10].", "startOffset": 67, "endOffset": 71}, {"referenceID": 8, "context": "These energy functions are encoded from interior design guidelines [16] or learned from input scene data [10].", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "Methods to learn scene structure have been demonstrated using various data sources including simulation of human agents in 3D scenes [11, 12], and analysis of supporting contact points in scanned environments [18].", "startOffset": 133, "endOffset": 141}, {"referenceID": 10, "context": "Methods to learn scene structure have been demonstrated using various data sources including simulation of human agents in 3D scenes [11, 12], and analysis of supporting contact points in scanned environments [18].", "startOffset": 133, "endOffset": 141}, {"referenceID": 16, "context": "Methods to learn scene structure have been demonstrated using various data sources including simulation of human agents in 3D scenes [11, 12], and analysis of supporting contact points in scanned environments [18].", "startOffset": 209, "endOffset": 213}, {"referenceID": 5, "context": "The SHRDLU [20] and PUT [6] systems were pioneers in parsing natural language instructions, but generalization of their approach to more realistic scenarios similar to the real world is challenging.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "More recent work on the WordsEye system [8, 9] and other similar approaches [19] has demonstrated more general text-driven 3D scene generation that can handle complex scenes.", "startOffset": 40, "endOffset": 46}, {"referenceID": 17, "context": "More recent work on the WordsEye system [8, 9] and other similar approaches [19] has demonstrated more general text-driven 3D scene generation that can handle complex scenes.", "startOffset": 76, "endOffset": 80}, {"referenceID": 2, "context": "[3] have demonstrated how linguistic and non-linguistic spatial knowledge can be learned directly from existing scene data and leveraged when parsing natural language.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Based on the systems presented in [2, 3], we frame the interactive text-to-scene problem in a probabilistic formulation that covers both scene generation and interactive scene operations.", "startOffset": 34, "endOffset": 40}, {"referenceID": 2, "context": "Based on the systems presented in [2, 3], we frame the interactive text-to-scene problem in a probabilistic formulation that covers both scene generation and interactive scene operations.", "startOffset": 34, "endOffset": 40}, {"referenceID": 2, "context": "[3] and we use the same corpus of 3D scenes and component 3D models, consisting of about 12,490 mostly indoor objects with associated textual categories and tags.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3]\u2019s approach to make explicit the information necessary for connecting higher-level semantics to the lower-level geometric representations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] represents P (t|u) as a delta probability at the selected t.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] to process the input text using the Stanford CoreNLP pipeline1, identify visualizable objects, physical properties of the objects, and extract dependency patterns for spatial relations between the objects.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "We also establish an upperbound baseline by asking people to manually design scenes corresponding to the same descriptions, using a simple scene design interface used in prior work [10].", "startOffset": 181, "endOffset": 185}, {"referenceID": 0, "context": "We used the lme4 R package and optimized for maximum loglikelihood [1].", "startOffset": 67, "endOffset": 70}], "year": 2017, "abstractText": "Designing 3D scenes is currently a creative task that requires significant expertise and effort in using complex 3D design interfaces. This effortful design process starts in stark contrast to the easiness with which people can use language to describe real and imaginary environments. We present SCENESEER: an interactive text to 3D scene generation system that allows a user to design 3D scenes using natural language. A user provides input text from which we extract explicit constraints on the objects that should appear in the scene. Given these explicit constraints, the system then uses a spatial knowledge base learned from an existing database of 3D scenes and 3D object models to infer an arrangement of the objects forming a natural scene matching the input description. Using textual commands the user can then iteratively refine the created scene by adding, removing, replacing, and manipulating objects. We evaluate the quality of 3D scenes generated by SCENESEER in a perceptual evaluation experiment where we compare against manually designed scenes and simpler baselines for 3D scene generation. We demonstrate how the generated scenes can be iteratively refined through simple natural language commands. INTRODUCTION Designing 3D scenes is a challenging creative task. Expert users expend considerable effort in learning how to use complex 3D scene design tools. Still, immense manual effort is required, leading to high costs for producing 3D content in video games, films, interior design, and architectural visualization. Despite the conceptual simplicity of generating pictures from descriptions, systems for text-to-scene generation have only achieved limited success. How might we allow people to create 3D scenes using simple natural language? Current 3D design tools provide a great amount of control over the construction and precise positioning of geometry within 3D scenes. However, most of these tools do not allow for intuitively assembling a scene from existing objects which is critical for non-professional users. As an analogue, in real life few people are carpenters, but most of us have bought and arranged furniture. For the purposes of defining how to compose and arrange objects into scenes, natural language is an obvious interface. It is much easier to say \u201cPut a blue bowl on the dining table\u201d rather than retrieving, inserting and orienting a 3D model of a bowl. Text to 3D scene interfaces can empower a broader demographic to create 3D scenes for games, interior design, and virtual storyboarding. Text to 3D scene systems face several technical challenges. Firstly, natural language is typically terse and incomplete. People rarely mention many facts about the world since these facts can often be safely assumed. Most desks are upright and on the floor but few people would mention this explicitly. This implicit spatial knowledge is critical for scene generation but hard to extract. Secondly, people reason about the 1 ar X iv :1 70 3. 00 05 0v 1 [ cs .G R ] 2 8 Fe b 20 17 world at a much higher level than typical representations of 3D scenes (using the descriptive phrase table against wall vs a 3D transformation matrix). The semantics of objects and their approximate arrangement are typically more important than the precise and abstract properties of geometry. Most 3D scene design tools grew out of the traditions of Computer Aided Design and architecture where precision of control and specification is much more important than for casual users. Traditional interfaces allow for comprehensive control but are typically not concerned with high level semantics. SCENESEER allows users to generate and manipulate 3D scenes at the level of everyday semantics through simple natural language. It leverages spatial knowledge priors learned from existing 3D scene data to infer implicit, unmentioned constraints and resolve view-dependent spatial relations in a natural way. For instance, given the sentence \u201cthere is a dining table with a cake\u201d, we can infer that the cake is most likely on a plate and that the plate is most likely on the table. This elevation of 3D scene design to the level of everyday semantics is critical for enabling intuitive design interfaces, rapid prototyping, and coarse-to-fine refinements. In this paper, we present a framework for the text to 3D scene task and use it to motivate the design of the SCENESEER system. We demonstrate that SCENESEER can be used to generate 3D scenes from terse, natural language descriptions. We empirically evaluate the quality of the generated scenes with a human judgment experiment and find that SCENESEER can generate high quality scenes matching the input text. We show how textual commands can be used interactively in SCENESEER to manipulate generated 3D scenes.", "creator": "LaTeX with hyperref package"}}}