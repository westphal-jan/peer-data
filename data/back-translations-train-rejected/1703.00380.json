{"id": "1703.00380", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Personal Model Training under Privacy Constraints", "abstract": "Many current Internet services rely on inferences from models trained on user data. Commonly, both the training and inference tasks are carried out using cloud resources fed by personal data collected at scale from users. Holding and using such large collections of personal data in the cloud creates privacy risks to the data subjects, but is currently required for users to benefit from such services. We explore how to provide for model training and inference in a system where computation is moved to the data in preference to moving data to the cloud, obviating many current privacy risks. Specifically, we take an initial model learnt from a small set of users and retrain it locally using data from a single user. We evaluate on two tasks: one supervised learning task, using a neural network to recognise users' current activity from accelerometer traces; and one unsupervised learning task, identifying topics in a large set of documents. In both cases the accuracy is improved. We also demonstrate the feasibility of our approach by presenting a performance evaluation on a representative resource-constrained device (a Raspberry Pi).", "histories": [["v1", "Wed, 1 Mar 2017 16:50:44 GMT  (1162kb,D)", "https://arxiv.org/abs/1703.00380v1", "Databox Project Technical Report"], ["v2", "Wed, 21 Jun 2017 15:02:12 GMT  (440kb,D)", "http://arxiv.org/abs/1703.00380v2", "Databox Project Technical Report"]], "COMMENTS": "Databox Project Technical Report", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sandra servia-rodriguez", "liang wang", "jianxin r zhao", "richard mortier", "hamed haddadi"], "accepted": false, "id": "1703.00380"}, "pdf": {"name": "1703.00380.pdf", "metadata": {"source": "CRF", "title": "Personal Model Training under Privacy Constraints", "authors": ["Sandra Servia-Rodriguez", "Liang Wang", "Jianxin R. Zhao", "Richard Mortier", "Hamed Haddadi"], "emails": [], "sections": [{"heading": "1. INTRODUCTION", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is not a country, but a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which is a country, in which is a country, in which is a country,"}, {"heading": "2. METHODOLOGY", "text": "The current approach we want to avoid is for all users to send their personal data to the cloud to process it there. At the other extreme, we want to develop a model for a particular user that uses only their data, while at the same time offering the user more privacy, both for training and intuition."}, {"heading": "2.1 Architecture", "text": "After describing our approach, we will now outline a system architecture that could be used to implement it, divided into two parts: (i) staying in the cloud; the first part is responsible for building a common model using batch learning; and (ii) staying on the device of each individual user; the second part adjusts the model by using the locally available data, leading to a personal model. We will identify five components in this architecture: 1. The batch training module is located in the cloud and is responsible for building a common model using public or private but shared data sets, which it also maintains. As this component can support multiple applications, it will provide a collection of different machine learning algorithms to build different needed models."}, {"heading": "2.2 Typical Workflow", "text": "When the user activates the device for the first time, the device contacts the server and registers itself in order to join the system. The device notices that there is no local data to build the model, and sends a request to the server to receive the common model. After processing the registration, the server receives the download request. The common model has been trained on the basis of an initial set of data collected in an appropriate ethical and trustworthy manner, e.g. with informed consent, appropriate remuneration and duly anonymized. The server can either approve the download request or return a list of kindred spirits from whom the requesting user can retrieve the model. 3. After receiving the common model, the device can start processing inference requests. At the same time, the device continuously collects the user's personal data, in this case, its acceleration requests. Once enough local data is collected, the personalization phase begins to create a personal model."}, {"heading": "3. ACTIVITY RECOGNITION USING ACCELEROMETER TRACES", "text": "In this section, we validate our methodology for supervised learning using a neural network to detect the activity of users using acceleration tracks. Our evaluation for unattended learning is detailed in Section 4, where we use the task of identifying topics in documents as a case study.Here, we consider a scenario in which smartphone users want to train a motion-based activity classification without passing their data on to others.To test the algorithms, we use the WISDM Human Activity Recognition Dataset [36], which uses a collection of acceleration data on an Android phone by 35 subjects using 6 activities (walking, jogging, walking up, walking down, sitting and standing).These subjects carried an Android phone in their front pocket while they were asked to perform each of these activities for specific periods of time. Various time variables were extracted from the signal, and we consider the statistical measures achieved for all 10 seconds of acceleration in our model."}, {"heading": "3.1 Multi-Layer Perceptron", "text": "The output layer shows a graphical representation of an MLP with a single hidden layer. Each node in a layer is connected to all nodes in the previous layer and receives only the data, while both hidden and output layers actively process the data. The output layer also produces the results. Figure 2 shows a graphical representation of an MLP with a single hidden layer. Each node in a layer is connected to all nodes in the previous layer. Formation of this structure is equivalent to finding correct weights and biases for all layers that have a desired output for a corresponding input.The standard back propagation learning algorithms are used to train the MLP neural architecture."}, {"heading": "3.2 Experimental setup", "text": "We have set up a multilayer perceptron with 2 levels of activity detection, including a hidden layer of 128 nodes and a logistical regression layer, resulting in 6, 406 parameters to be determined during the training. We construct the input layer using the statistical measurements of the users \"accelerometers. Due to the sensitivity of the learning steps to scale [31], we normalize all statistical measurements to have a deviation between mean and unit. In the output layer, each unit corresponds to an activity inference class, so that the unit states can be interpreted as posterior probabilities. All training procedures have been implemented in Python, using the Theano-Deep-Learning library [8]. Training and tests have been carried out with a 5x cross-validation, using early stops as well as\" 2 regulations to prevent overmatching modules to the weights available in each of the 001 and 001 models."}, {"heading": "3.3 Results", "text": "The results show that the effect of training or retraining a model with few samples of the tested person provides worse predictions than the use of other people's samples (split model); that is, as the model adapts to the new scenario, the performance of the prediction decreases slightly. However, if more samples (an average of 20 or more) are used to retrain this split model, the accuracy of the prediction exceeds the accuracy achieved with the shared model itself. Specifically, the accuracy increases with steps in the number of samples used to retrain the model. That is, the more local samples are taken into account for retraining the model, the more personalized it becomes for the person under consideration. Although the improvement in accuracy is also shared with the local model by increasing the number of samples, more samples per person are required to train a model from scratch (local model) to obtain the same accuracy as when starting with a shared model (personal summary)."}, {"heading": "4. TOPIC MODELLING OF PERSONAL TEXT CORPUS", "text": "Text corpus, such as websites, documents, e-books and e-mail, is one of the most widely used media on the Internet and also dominates the devices of many users. Text classification therefore has a broad application to facilitate people's daily lives by grouping or filtering the elements in a text corpus based on specific topics. Since the personal text corpus often contains a significant amount of private information, uploading such a corpus to a public cloud is sure to violate users \"privacy. Let's consider a scenario in which users would like to classify the text documents on their computers without disclosing their contents to others. As we are more accurate, we are looking at researchers working on a confidential project within a company that does not want to disclose the publications they read, but would like to classify the documents according to their content.To simulate such a scenario, we use two sets of text in our assessment: the NIPS data model 4 and the Wikipedia data model [9]."}, {"heading": "4.1 Latent Dirichlet Allocation", "text": "A theme model is an effective tool for text mining. It aims to construct a statistical model to represent the abstract \"themes\" in a collection of documents. In this way, similar documents can be summarized for future queries. As each document consists of a sequence of words, it is often presented as a very high-dimensional and sparse vector using a bag-of-words model. Each dimension represents a unique vocabulary in the dictionary that is extracted from the corpus, and the size of each dimension is often calculated as a term frequency in the corresponding document. Therefore, the dimensionality of these vectors depends on the size of the dictionary, and it is common that the dimensionality of the themes extracted from the corpus is. LDA is a generative model that explicitly bases themes as latent variables in search for terms and documents."}, {"heading": "4.2 Experiment setup", "text": "In our topic modeling scenario, a user u owns a set of text documents, you, and wants to identify their topics without disclosing their contents. There is another set of publicly available documents, Dr, from which he can benefit. The set of u documents, you, is composed of the documents of the NIPS dataset, whereas Dr is composed of various random samples of documents from the Wikipedia dataset. We start by building a common LDA model called MS from the publicly available documents Dr, as we did in the previous case of activity detection. However, it is noteworthy that MS only includes the topic word distribution (as well as a dictionary for encrypting the documents), which is a very sparse matrix. The Document Topic parameter depends on the specific text corpus and is not useful for others to initialize a personal model, so it is not necessary to include the local model in MS."}, {"heading": "4.3 Improved accuracy and efficiency", "text": "Figure 4 shows the evolution of the log probability in each iteration, while the build-up of the personal (MP) and local models is very limited. NIPS data set is used in this first experiment, and the common model, MS, is formed with 50% of the data. A user's local data is generated by randomly selecting 300 documents from the rest of the datasets. The two lines in the figure correspond to the two possibilities of building the user's theme model: the red line is for using MS (personal model), while the blue line for building the model is from scratch by using only the user's local documents (local model). As we see, the personal model is able to achieve a higher probability in each iteration than the local one. Meanwhile, it also indicates that the model is able to convert much faster as a target accuracy is given. NIPS data sets are greater than the activity detection records, allowing us to raise the next question and answer the next one."}, {"heading": "4.4 Topic-based local dataset", "text": "The question is whether our method is still effective when the topics in the local data are only a subset of those contained in the documents used in the formation of the common model. To answer the question, we design another experiment in which we generate the local data set, and we randomly select the articles from a given topic, e.g. computer networking, architectural design, British history. We generate several local data sets with different topics. The x-axis in Figure 6 shows the topic indexes in the Wikipedia data set, and we present the results of 10 of these selected topics. The way of selecting the common data for the education of MS remains the same. In total, we pitch 1,000 articles from across Wikipedia as public or shared data, and 500 articles of a specific topic for each individual action 3.We initially improve 85%."}, {"heading": "5. PRACTICAL CONSIDERATIONS", "text": "In the following we discuss the data protection guarantees of our methodology and their robustness against enemy attacks (\u00a7 5.1), as well as empirically verify this robustness against the specific case of poisoning attacks (\u00a7 5.2) and then demonstrate the feasibility of their use by submitting a performance assessment on a representative resource-limited device (\u00a7 5.3)."}, {"heading": "5.1 Adversarial attacks", "text": "There are several potential attacks on each learning system [14, 33]. Here, we focus on how privacy and causal attacks could affect our system. In an attack on privacy, the opponent receives information from the learner, thereby affecting the secrecy or privacy of the users of the system. The goal of a causal attack is to alter the parameters of the target model by manipulating the training data set. An example of such attacks are poison attacks, in which an attacker can poison the training data by injecting carefully designed samples, ultimately endangering the entire learning process. The target model then updates with the poisoned data and gradually makes compromises. Below, we describe the potential effects of these attacks on our system. Our solution guarantees the confidentiality of the users \"data (potential users), as their devices are not compromised, as their personal data never leaves their devices."}, {"heading": "5.2 Poisoning attacks", "text": "In fact, it is in such a way that we are able to maneuver ourselves into a situation in which we see ourselves in a position, in which we are in a position, in which we are in a position, in which we are able to claim that we are in a position, in which we are, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we, in which we, in which we live, in which we live, in which we, in which we live, in which we live, in which we live, in which we, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we, in which we live, in which we live, in which we, in which we live, in which we, in which we live, in which we live, in which we, in which we live, in which we, in which we live, in which we, in which we, in which we live, in which we, in which we live, in which we live, in which we, in which we, in which we, in which we, in which we, in which we see, in which we, in which we, in which we see, in which we, in which we see, in which we see, in which we see, in which we see, in which we see, in which we, in which we see, in which we live, in which we are able to live, in which we are able to live, in which we are able, in which we are able, in which we are able, in which we are able to maintain, in which we are able to maintain, in which we are able to maintain, in which we are able to maintain, in which are able, in which are able to"}, {"heading": "5.3 Deployment Feasibility", "text": "The success of our privacy-preserving methodology for learning personal models depends on the ability to perform near-real-time model refinements on resource-limited personal devices that lack the capabilities of cloud-based servers, environments that are of growing interest for the use of such techniques as the availability of resources outside of data centers, which we are increasing with the creation of \"cloud computing\" environments using cheap and energy-efficient platforms such as those based on ARM processors [18]. To test the feasibility of our approach on resource-limited personal devices, we evaluate both tasks, the second phase of our approach, the testing of refinement and conclusions on a Raspberry Pi 3 Model B [6] as representatives of this type of environment.Figure 9 shows the time needed to develop a personal model (the refinement of the original common model), and the alternative approach of learning a local model using only one-time."}, {"heading": "6. RELATED WORK", "text": "Data-driven solutions are now ubiquitous in areas such as advertising, smart cities and eHealth [22, 51]. Almost everything we do in our daily lives is tracked by some means. Although careful analysis of this data can be extremely beneficial to us as individuals and to society in general, this approach usually involves a violation of privacy, a high price that more and more people are unwilling to pay. [19] Several data-preserving analytical solutions have been proposed to guarantee the confidentiality of personal data while extracting useful information, prominent among them those that rely on Dwork's differentiated data framework [45, 48, 21, 26, 11] which formalize the idea that a query over a sensitive database should not reveal whether a person is included in the dataset [23]. Most of these techniques are usually based on the addition of noise during training, leading to a challenging trade-off between privacy and accuracy."}, {"heading": "7. CONCLUSION", "text": "Our data protection methodology for learning analytics relies on the ability of current personal devices such as smartphones, tablets and small form factors such as the Raspberry Pi to perform traditionally resource-intensive tasks. However, by sharing model training between the cloud and the personal device, we avoid sending personal data to untrusted remote units in the cloud while retaining all rights to their personal data while retaining the benefits of learning-based services. We demonstrated our methodology for two learning tasks, one monitored and one unattended: activity detection from acceleration data and the identification of topics in text documents. Our experiments showed improvements in both accuracy and efficiency with this methodology compared to traditional cloud-based solutions, with the model using only the data we provide the service to."}, {"heading": "9. REFERENCES", "text": "[1] Amazon echo. https: / / www.amazon.com / Amazon-Echo-Bluetooth-Products / i-raspberry-3. \"http: / / www.amazon.com / Amazon-Bluetooth-Speaker-with-WiFi-Alexa / dp / B00X4WHP5E /. Accessed May 25, 2017. [2] Apple homekit. http: / / www.apple.com / ios / home /. Accessed May 25, 2017. [3] Google home. https: / / archive.ics.uci.edu / ml / machine-learning-databases / bag-of-words /. Google.com / home /. [5] Owl - an ocaml numerical library. https: / github.com / ryanrhymes / owl. Accessed. January 20, 2017."}], "references": [{"title": "Deep learning with differential privacy", "author": ["M. Abadi", "A. Chu", "I. Goodfellow", "H.B. McMahan", "I. Mironov", "K. Talwar", "L. Zhang"], "venue": "In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "A general survey of privacy-preserving data mining models and algorithms", "author": ["C.C. Aggarwal", "S.Y. Philip"], "venue": "In Privacy-preserving data mining,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Privacy-preserving data mining", "author": ["R. Agrawal", "R. Srikant"], "venue": "In Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "The security of machine learning", "author": ["M. Barreno", "B. Nelson", "A.D. Joseph", "J. Tygar"], "venue": "Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Securing private data sharing in multi-party analytics", "author": ["G. Bellala", "B. Huberman"], "venue": "First Monday,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "arXiv preprint arXiv:1206.6389,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Fog computing and its role in the internet of things", "author": ["F. Bonomi", "R. Milito", "J. Zhu", "S. Addepalli"], "venue": "In Proceedings of the First Edition of the MCC Workshop on Mobile Cloud Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Misplaced confidences", "author": ["L. Brandimarte", "A. Acquisti", "G. Loewenstein"], "venue": "Social Psychological and Personality Science,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Personal data: Thinking inside the box", "author": ["A. Chaudhry", "J. Crowcroft", "H. Howard", "A. Madhavapeddy", "R. Mortier", "H. Haddadi", "D. McAuley"], "venue": "In Proceedings of The Fifth Decennial Aarhus Conference on Critical Alternatives,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Differentially private empirical risk minimization", "author": ["K. Chaudhuri", "C. Monteleoni", "A.D. Sarwate"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Data-intensive applications, challenges, techniques and technologies: A survey on big data", "author": ["C.P. Chen", "C.-Y. Zhang"], "venue": "Information Sciences,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Differential privacy: A survey of results", "author": ["C. Dwork"], "venue": "In International Conference on Theory and Applications of Models of Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Privacy-preserving data aggregation in smart metering systems: an overview", "author": ["Z. Erkin", "J.R. Troncoso-pastoriza", "R.L. Lagendijk", "F. Perez-Gonzalez"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Tracking personal identifiers across the web", "author": ["M. Falahrastegar", "H. Haddadi", "S. Uhlig", "R. Mortier"], "venue": "In Passive and Active Measurement conference (PAM", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing", "author": ["M. Fredrikson", "E. Lantz", "S. Jha", "S. Lin", "D. Page", "T. Ristenpart"], "venue": "In USENIX Security,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Fast approximate nearest-neighbor search with k-nearest neighbor graph", "author": ["K. Hajebi", "Y. Abbasi-Yadkori", "H. Shahbazi", "H. Zhang"], "venue": "In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Volume Two,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Learning privately from multiparty data", "author": ["J. Hamm", "P. Cao", "M. Belkin"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Scalable similarity search with optimized kernel hashing", "author": ["J. He", "W. Liu", "S.-F. Chang"], "venue": "In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In Proceedings of the IEEE international conference on computer vision,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "A practical guide to training restricted boltzmann machines. Momentum", "author": ["G. Hinton"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "In Proceedings of the 22Nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1999}, {"title": "Adversarial machine learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B.I. Rubinstein", "J. Tygar"], "venue": "In Proceedings of the 4th ACM workshop on Security and artificial intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Fast nearest neighbor search through sparse random projections and voting", "author": ["V. Hyv\u00c3\u0171nen", "T. Pitk\u00c3d\u2019nen", "S. Tasoulis", "E. J\u00c3d\u2019\u00c3d\u2019saari", "R. Tuomainen", "L. Wang", "J. Corander", "T. Roos"], "venue": "IEEE International Conference on Big Data (Big Data),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Pria: A private intelligent assistant", "author": ["S. Jain", "V. Tiwari", "A. Balasubramanian", "N. Balasubramanian", "S. Chakraborty"], "venue": "In Proceedings of the 18th International Workshop on Mobile Computing Systems and Applications, HotMobile", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2017}, {"title": "Activity recognition using cell phone accelerometers", "author": ["J.R. Kwapisz", "G.M. Weiss", "S.A. Moore"], "venue": "ACM SigKDD Explorations Newsletter,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Communication efficient distributed machine learning with the parameter server", "author": ["M. Li", "D.G. Andersen", "A.J. Smola", "K. Yu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Distributed graphlab: A framework for machine learning and data mining in the cloud", "author": ["Y. Low", "D. Bickson", "J. Gonzalez", "C. Guestrin", "A. Kyrola", "J.M. Hellerstein"], "venue": "Proc. VLDB Endow.,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Communication-efficient learning of deep networks from decentralized data", "author": ["H.B. McMahan", "E. Moore", "D. Ramage", "S. Hampson"], "venue": "arXiv preprint arXiv:1602.05629,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Mllib: Machine learning in apache spark", "author": ["X. Meng", "J. Bradley", "B. Yavuz", "E. Sparks", "S. Venkataraman", "D. Liu", "J. Freeman", "D. Tsai", "M. Amde", "S. Owen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Personal data management wiht the Databox: What\u2019s inside the box", "author": ["R. Mortier", "J. Zhao", "J. Crowcroft", "L. Wang", "Q. Li", "H. Haddadi", "Y. Amar", "A. Crabtree", "J. Colley", "T. Lodge", "T. Brown", "D. McAuley", "C. Greenhalgh"], "venue": "In Proc. Cloud Assisted Networking workshop at ACM CoNEXT,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on knowledge and data engineering,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Semi-supervised knowledge transfer for deep learning from private training data", "author": ["N. Papernot", "M. Abadi", "\u00da. Erlingsson", "I. Goodfellow", "K. Talwar"], "venue": "In Proceedings of the 5th International Conference on Learning Representations,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2017}, {"title": "Signal processing and machine learning with differential privacy: Algorithms and challenges for continuous data", "author": ["A.D. Sarwate", "K. Chaudhuri"], "venue": "IEEE signal processing magazine,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}, {"title": "Privacy-preserving deep learning", "author": ["R. Shokri", "V. Shmatikov"], "venue": "In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "Membership inference attacks against machine learning models", "author": ["R. Shokri", "M. Stronati", "V. Shmatikov"], "venue": "In Proceedings of the 38th IEEE Symposium on Security and Privacy,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2017}, {"title": "Stochastic gradient descent with differentially private updates", "author": ["S. Song", "K. Chaudhuri", "A.D. Sarwate"], "venue": "In Global Conference on Signal and Information Processing (GlobalSIP), 2013 IEEE,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2013}, {"title": "Kvasir: Scalable provision of semantically relevant web content on big data framework", "author": ["L. Wang", "S. Tasoulis", "T. Roos", "J. Kangasharju"], "venue": "IEEE Transactions on Big Data,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "The impact of personalization on smartphone-based activity recognition", "author": ["G.M. Weiss", "J.W. Lockhart"], "venue": "In AAAI Workshop on Activity Context Representation: Techniques and Languages,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2012}, {"title": "Data mining with big data", "author": ["X. Wu", "X. Zhu", "G.Q. Wu", "W. Ding"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2014}, {"title": "Adversarial label flips attack on support vector machines", "author": ["H. Xiao", "C. Eckert"], "venue": "In Proceedings of the 20th European Conference on Artificial Intelligence,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2012}], "referenceMentions": [{"referenceID": 14, "context": "It is not only data from wearables that creates such risks: web queries, article reads and searches, visits to shopping sites and browsing online catalogues are also indexed, analysed, and traded by thousands of tracking services in order to build preference models [25].", "startOffset": 266, "endOffset": 270}, {"referenceID": 9, "context": "We are interested in an alternative approach where we reduce or remove the flow of user data to the cloud completely, instead moving computation to where the data already resides under the user\u2019s control [20, 41].", "startOffset": 204, "endOffset": 212}, {"referenceID": 30, "context": "We are interested in an alternative approach where we reduce or remove the flow of user data to the cloud completely, instead moving computation to where the data already resides under the user\u2019s control [20, 41].", "startOffset": 204, "endOffset": 212}, {"referenceID": 25, "context": "We evaluate this approach using (i) a neural network to recognise users\u2019 activity on the WISDM dataset [36] and (ii) the Latent Dirichlet Algorithm (LDA) [17] to identify topics in the Wikipedia and NIPS datasets [4, 9].", "startOffset": 103, "endOffset": 107}, {"referenceID": 6, "context": "We evaluate this approach using (i) a neural network to recognise users\u2019 activity on the WISDM dataset [36] and (ii) the Latent Dirichlet Algorithm (LDA) [17] to identify topics in the Wikipedia and NIPS datasets [4, 9].", "startOffset": 154, "endOffset": 158}, {"referenceID": 39, "context": ", activity recognition, it has been shown that a model trained solely using data from the individual concerned provides more accurate predictions for that individual than a model trained using data from other individuals [50].", "startOffset": 221, "endOffset": 225}, {"referenceID": 9, "context": "At the same time, this solution offers more privacy to the user as all computation, for both training and inference, can be done locally on the device [20].", "startOffset": 151, "endOffset": 155}, {"referenceID": 35, "context": "To assure the confidentiality of their data as well as their presence in the dataset, the shared model might be obtained using differentially private training [46, 39, 28, 44].", "startOffset": 159, "endOffset": 175}, {"referenceID": 28, "context": "To assure the confidentiality of their data as well as their presence in the dataset, the shared model might be obtained using differentially private training [46, 39, 28, 44].", "startOffset": 159, "endOffset": 175}, {"referenceID": 17, "context": "To assure the confidentiality of their data as well as their presence in the dataset, the shared model might be obtained using differentially private training [46, 39, 28, 44].", "startOffset": 159, "endOffset": 175}, {"referenceID": 33, "context": "To assure the confidentiality of their data as well as their presence in the dataset, the shared model might be obtained using differentially private training [46, 39, 28, 44].", "startOffset": 159, "endOffset": 175}, {"referenceID": 29, "context": "It may also need to perform more traditional, large scale processing, but can easily be built using modern data processing frameworks designed for datacenters such as Mllib [40] or GraphLab [38].", "startOffset": 173, "endOffset": 177}, {"referenceID": 27, "context": "It may also need to perform more traditional, large scale processing, but can easily be built using modern data processing frameworks designed for datacenters such as Mllib [40] or GraphLab [38].", "startOffset": 190, "endOffset": 194}, {"referenceID": 25, "context": "To test the algorithms, we use the WISDM Human Activity Recognition dataset [36], which is a collection of accelerometer data on an Android phone by 35 subjects performing 6 activities (walking, jogging, walking upstairs, walking downstairs, sitting and standing).", "startOffset": 76, "endOffset": 80}, {"referenceID": 25, "context": "Various time domain variables were extracted from the signal, and we consider the statistical measures obtained for every 10 seconds of accelerometer samples in [36] as the d = 43 dimensional features in our models.", "startOffset": 161, "endOffset": 165}, {"referenceID": 19, "context": "0/M ), being wij (wjk) the value of the connection weight between unit j (k) and unit i (j) in the previous layer, and N (M ) the number of input (hidden) units [30].", "startOffset": 161, "endOffset": 165}, {"referenceID": 31, "context": "The current recommendation is to use ReLU (Rectified Linear Unit) units [42, 30] as the activation function (\u03c6(x) = max(0, x)) though other options are possible.", "startOffset": 72, "endOffset": 80}, {"referenceID": 19, "context": "The current recommendation is to use ReLU (Rectified Linear Unit) units [42, 30] as the activation function (\u03c6(x) = max(0, x)) though other options are possible.", "startOffset": 72, "endOffset": 80}, {"referenceID": 19, "context": "For the shared model, MS , we initialise the weights to random small values and the bias to zero in both layers, as suggested by previous literature [30].", "startOffset": 149, "endOffset": 153}, {"referenceID": 20, "context": "Because of the sensitivity learning stages to feature scaling [31] we normalise all statistical measures to have zero mean and unit standard deviation.", "startOffset": 62, "endOffset": 66}, {"referenceID": 21, "context": "LDA is similar to pLSA [32] but replaces the maximum likelihood estimator with Bayesian estimator, hence it is sometimes referred to as the Bayesian version of pLSA.", "startOffset": 23, "endOffset": 27}, {"referenceID": 38, "context": "Many prior works [49, 27, 29, 34] focus on building compact and efficient data structure and search algorithm to speed up the queries to the models.", "startOffset": 17, "endOffset": 33}, {"referenceID": 16, "context": "Many prior works [49, 27, 29, 34] focus on building compact and efficient data structure and search algorithm to speed up the queries to the models.", "startOffset": 17, "endOffset": 33}, {"referenceID": 18, "context": "Many prior works [49, 27, 29, 34] focus on building compact and efficient data structure and search algorithm to speed up the queries to the models.", "startOffset": 17, "endOffset": 33}, {"referenceID": 23, "context": "Many prior works [49, 27, 29, 34] focus on building compact and efficient data structure and search algorithm to speed up the queries to the models.", "startOffset": 17, "endOffset": 33}, {"referenceID": 3, "context": "There are several potential attacks against any learning system [14, 33].", "startOffset": 64, "endOffset": 72}, {"referenceID": 22, "context": "There are several potential attacks against any learning system [14, 33].", "startOffset": 64, "endOffset": 72}, {"referenceID": 15, "context": "Since both the data and the personal model resides on the user\u2019s device, attacks such as model inversion [26] \u2013where an attacker, given the model and some auxiliary information about the user, can determine some user\u2019s raw data; and membership query [47], where, given a data record and black-box access to a model, an adversary could determine if the record was in the model\u00e2\u0102\u0179s training dataset, cannot affect our users.", "startOffset": 105, "endOffset": 109}, {"referenceID": 36, "context": "Since both the data and the personal model resides on the user\u2019s device, attacks such as model inversion [26] \u2013where an attacker, given the model and some auxiliary information about the user, can determine some user\u2019s raw data; and membership query [47], where, given a data record and black-box access to a model, an adversary could determine if the record was in the model\u00e2\u0102\u0179s training dataset, cannot affect our users.", "startOffset": 250, "endOffset": 254}, {"referenceID": 35, "context": "On the other hand, for applications such as face or speaker recognition, techniques based on differentially private training [46, 39, 28, 44] could be applied in order to, a priori, guarantee the confidentiality of the volunteers\u2019 data.", "startOffset": 125, "endOffset": 141}, {"referenceID": 28, "context": "On the other hand, for applications such as face or speaker recognition, techniques based on differentially private training [46, 39, 28, 44] could be applied in order to, a priori, guarantee the confidentiality of the volunteers\u2019 data.", "startOffset": 125, "endOffset": 141}, {"referenceID": 17, "context": "On the other hand, for applications such as face or speaker recognition, techniques based on differentially private training [46, 39, 28, 44] could be applied in order to, a priori, guarantee the confidentiality of the volunteers\u2019 data.", "startOffset": 125, "endOffset": 141}, {"referenceID": 33, "context": "On the other hand, for applications such as face or speaker recognition, techniques based on differentially private training [46, 39, 28, 44] could be applied in order to, a priori, guarantee the confidentiality of the volunteers\u2019 data.", "startOffset": 125, "endOffset": 141}, {"referenceID": 5, "context": "Some schemes have been proposed to conduct poisoning attacks against SVMs [16, 52], but we have barely seen any work about poisoning attacks against neural networks.", "startOffset": 74, "endOffset": 82}, {"referenceID": 41, "context": "Some schemes have been proposed to conduct poisoning attacks against SVMs [16, 52], but we have barely seen any work about poisoning attacks against neural networks.", "startOffset": 74, "endOffset": 82}, {"referenceID": 7, "context": "tion resources outside datacenters continues to increase with creation of \u201cfog computing\u201d environments using cheep and energy-efficient platforms such as those based on ARM processors [18].", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "RELATED WORK Data-driven solutions are now pervasive in areas such as advertising, smart cities and eHealth [22, 51].", "startOffset": 108, "endOffset": 116}, {"referenceID": 40, "context": "RELATED WORK Data-driven solutions are now pervasive in areas such as advertising, smart cities and eHealth [22, 51].", "startOffset": 108, "endOffset": 116}, {"referenceID": 8, "context": "Although careful analysis of these data can be highly beneficial for us as individuals and for society in general, this approach usually entails invasion of privacy, a high price that progressively more people are not willing to pay [19].", "startOffset": 233, "endOffset": 237}, {"referenceID": 2, "context": "Several privacy-preserving analytical solutions have been proposed to guarantee the confidentiality of personal data while extracting useful information [13, 12, 24, 15].", "startOffset": 153, "endOffset": 169}, {"referenceID": 1, "context": "Several privacy-preserving analytical solutions have been proposed to guarantee the confidentiality of personal data while extracting useful information [13, 12, 24, 15].", "startOffset": 153, "endOffset": 169}, {"referenceID": 13, "context": "Several privacy-preserving analytical solutions have been proposed to guarantee the confidentiality of personal data while extracting useful information [13, 12, 24, 15].", "startOffset": 153, "endOffset": 169}, {"referenceID": 4, "context": "Several privacy-preserving analytical solutions have been proposed to guarantee the confidentiality of personal data while extracting useful information [13, 12, 24, 15].", "startOffset": 153, "endOffset": 169}, {"referenceID": 34, "context": "Prominent among them are those that build on Dwork\u2019s differential privacyframework [45, 48, 21, 26, 11], which formalises the idea that a query over a sensitive database should not reveal", "startOffset": 83, "endOffset": 103}, {"referenceID": 37, "context": "Prominent among them are those that build on Dwork\u2019s differential privacyframework [45, 48, 21, 26, 11], which formalises the idea that a query over a sensitive database should not reveal", "startOffset": 83, "endOffset": 103}, {"referenceID": 10, "context": "Prominent among them are those that build on Dwork\u2019s differential privacyframework [45, 48, 21, 26, 11], which formalises the idea that a query over a sensitive database should not reveal", "startOffset": 83, "endOffset": 103}, {"referenceID": 15, "context": "Prominent among them are those that build on Dwork\u2019s differential privacyframework [45, 48, 21, 26, 11], which formalises the idea that a query over a sensitive database should not reveal", "startOffset": 83, "endOffset": 103}, {"referenceID": 0, "context": "Prominent among them are those that build on Dwork\u2019s differential privacyframework [45, 48, 21, 26, 11], which formalises the idea that a query over a sensitive database should not reveal", "startOffset": 83, "endOffset": 103}], "year": 2017, "abstractText": "Many current Internet services rely on inferences from models trained on user data. Commonly, both the training and inference tasks are carried out using cloud resources fed by personal data collected at scale from users. Holding and using such large collections of personal data in the cloud creates privacy risks to the data subjects, but is currently required for users to benefit from such services. We explore how to provide for model training and inference in a system where computation is moved to the data in preference to moving data to the cloud, obviating many current privacy risks. Specifically, we take an initial model learnt from a small set of users and retrain it locally using data from a single user. We evaluate on two tasks: one supervised learning task, using a neural network to recognise users\u2019 current activity from accelerometer traces; and one unsupervised learning task, identifying topics in a large set of documents. In both cases the accuracy is improved. We also demonstrate the robustness of our approach against adversarial attacks, as well as its feasibility by presenting a performance evaluation on a representative resource-constrained device (a Raspberry Pi).", "creator": "LaTeX with hyperref package"}}}