{"id": "1703.00420", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Virtual-to-real Deep Reinforcement Learning: Continuous Control of Mobile Robots for Mapless Navigation", "abstract": "Deep Reinforcement Learning has been successful in various virtual tasks, but it is still rarely used in real world applications especially for continuous control of mobile robots navigation. In this paper, we present a learning-based mapless motion planner by taking the 10-dimensional range findings and the target position as input and the continuous steering commands as output. Traditional motion planners for mobile ground robots with a laser range sensor mostly depend on the map of the navigation environment where both the highly precise laser sensor and the map building work of the environment are indispensable. We show that, through an asynchronous deep reinforcement learning method, a mapless motion planner can be trained end-to-end without any manually designed features and prior demonstrations. The trained planner can be directly applied in unseen virtual and real environments. We also evaluated this learning-based motion planner and compared it with the traditional motion planning method, both in virtual and real environments. The experiments show that the proposed mapless motion planner can navigate the nonholonomic mobile robot to the desired targets without colliding with any obstacles.", "histories": [["v1", "Wed, 1 Mar 2017 18:10:20 GMT  (5205kb,D)", "http://arxiv.org/abs/1703.00420v1", "8 pages, 9 figures, under review, video:this https URL"], ["v2", "Thu, 2 Mar 2017 08:09:19 GMT  (5230kb,D)", "http://arxiv.org/abs/1703.00420v2", "8 pages, 9 figures, under review, video:this https URL"], ["v3", "Tue, 4 Jul 2017 09:56:05 GMT  (5225kb,D)", "http://arxiv.org/abs/1703.00420v3", "video:this https URL, 6 pages, 9 figures, to appear in he 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2017), final submission version"], ["v4", "Fri, 21 Jul 2017 17:26:00 GMT  (5225kb,D)", "http://arxiv.org/abs/1703.00420v4", "video:this https URL, 6 pages, 9 figures, to appear in he 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2017), final submission version"]], "COMMENTS": "8 pages, 9 figures, under review, video:this https URL", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LG", "authors": ["lei tai", "giuseppe paolo", "ming liu"], "accepted": false, "id": "1703.00420"}, "pdf": {"name": "1703.00420.pdf", "metadata": {"source": "CRF", "title": "Virtual-to-real Deep Reinforcement Learning: Continuous Control of Mobile Robots for Mapless Navigation", "authors": ["Lei Tai", "Ming Liu"], "emails": ["lei.tai@my.cityu.edu.hk", "eelium@ust.hk", "giupaolo@student.ethz.ch"], "sections": [{"heading": null, "text": "I. INTRODUCTION"}, {"heading": "A. Motivation", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "B. Contribution", "text": "We list the most important contributions of this paper: \u2022 A mapless motion planner was proposed, using only 10-dimensional domain findings and relative target information as references. \u2022 The motion planner was trained from the ground up by an asynchronous learning method for depth enhancement without feature engineering or demonstrations. \u2022 The planner can output continuous linear and angular velocities directly. \u2022 The planner can generalize a true nontholonomic differential robot platform without fine-tuning to real examples. The rest of this paper is structured as follows: We present related mapless motion planning and depth enhancement learning in Section II. The revision of asynchronous learning for depth enhancement will be presented in Section III. We show the implementation of the approach in Section IV. Training and evaluation details are discussed in Section V. The analysis of the experiments will be discussed in Section VII."}, {"heading": "II. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Mapless Motion Planning", "text": "State-of-the-art 2D motion sensor-based motion planners are mostly map-based, such as Move Base1. Precise laser range1http: / / wiki.ros.org / move basesensors are often required to calculate the local cost map. Cardless motion planners perform navigational behavior without prior knowledge of the surroundings. McCarthy et al. [11] used optical flow to navigate a mobile robot in corridor-like environments, calculating the optical flow through Lucas and Kanades gradient-based method to terminate visual odometry. Guzel et al. [12] built a mapless motion planner based on a homing system. Such visionary methods often cost a lot of computational resources. Ulrich et al. [13] developed a vector field histogram (VFH) for distance sensor-based methods to avoid obstacles. Kamon et al. [14]"}, {"heading": "B. Deep-Learning-based navigation", "text": "Deep neural networks benefit from the improvement of high-performance computer hardware and have great potential to solve complex estimation problems. To avoid learning-based obstacles, deep neural networks have been successfully applied to monocular images [15] and depth images [16]. Chen et al. [17] used semantic information extracted from the image by deep neural networks to determine the behavior of the autonomous vehicle. However, their control commands are only discrete actions such as left and right turning that can lead to coarse navigational behavior. In terms of learning from demonstrations, Pfeiffer et al. [18] used a deep learning model to map the results of laser reach and target position on moving commands. Training data was collected by the output of the navigation behavior of Move Base. This model successfully navigated the robot in an invisible environment. Like the previously mentioned monitored learning methods, its effects are severely limited by the quality of training data sets."}, {"heading": "C. Deep Reinforcement Learning", "text": "Minh et al. [1] used deep neural networks to estimate the functioning of value-based reinforcement learning, known as deep cement learning or deep Q-network (DQN), and achieved a huge improvement in playing Atari games. The original DQN can only be used in tasks with a discrete action space. To extend it to continuous control, Lillicrap et al. [2] proposed to use deep deterministic policy trajectories (DDPG) to use deep neural networks on the actorcritical learning method of reinforcement, where both politics and the value of reinforcement learning were represented by hierarchical networks. Gu et al. [3] proposed continuous DQN based on the normalized advantage function (NAF). The successes of these deep reinforcement learning methods are mainly attributed to the memory replay strategy."}, {"heading": "III. ASYNCHRONOUS DEEP REINFORCEMENT LEARNING", "text": "In this section we ask the extension of the asynchronous Deep-RL in DDPG Q = Q Q = Q Q = Q = Q = Q = Q Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q"}, {"heading": "IV. MOTION PLANNER IMPLEMENTATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Problem Definition", "text": "We try to find such a translation function: vt = f (xt, pt, vt \u2212 1), where xt is the observation from the raw sensor information, pt is the relative position of the target, and vt \u2212 1 is the speed of the mobile robot in the last time step. They can be considered as the immediate state of the mobile robot. The model maps the state directly to the action that the next time the speed is vt, as shown in Figure 1. As an effective motion planner, the control frequency must be guaranteed so that the robot can react immediately to new observations. We focus on controlling a differential two-wheeled mobile robot that occupies the indispensable place among the Home Service robots."}, {"heading": "B. Network Structure", "text": "As already mentioned, the original DQN [1] can only be applied to a discrete action space. However, for a mobile robot motion planner, the continuous action space is obviously necessary. In this paper, we use DDPG [2] to train our model and expand it to an asynchronous version, as shown in Figure 1 and the definition function, the abstracted 10-dimensional laser range results, the previous action and the relative target position are merged as a 14-dimensional input vector. As we have already mentioned, the 10-dimensional laser range results from the raw laser results are scanned between -90 and 90 degrees in a three-dimensional and fixed angle distribution. The range information is normalized to (0.1).The 2-dimensional action each time includes the angular and linear speeds of the differential mobile robot."}, {"heading": "C. Reward Function Definition", "text": "There are three different conditions for the reward: r (st, at) = rarrive if dt < cdrcollision if minxt < co cr (dt \u2212 1 \u2212 dt) If the robot reaches the target by checking the distance threshold, a positive reward is ordered, but if the robot collides with the obstacle by checking the minimum range, a negative reward collision occurs. Both conditions end the training phase. Otherwise, the reward is the difference in distance from the target compared to the last step, dt \u2212 1 \u2212 dt, multiplied by a hyperparameter cr. This motivates the robot to get closer to the target position."}, {"heading": "V. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Training in simulation", "text": "The training procedure of our model was implemented in virtual environments simulated by V-REP [23]. In addition, we constructed two indoor environments to show the influence of the training environment on the motion planner, as shown in Fig. 4. The environments are surrounded by walls. Obstacles in Env-2 are more compact around the initial position of the robot. Both models of these two environments were learned from the ground up. A Kobuki-based turtlebot is used as the robotic platform, and the target is represented by a cylindrical object, as described in the figure, which is intended for visual purposes only, in fact it cannot be rendered by the laser sensor mounted on the turtlebot. In each episode, the target position was randomly initialized throughout the area and guaranteed to be collision-free with other obstacles. To add randomness, other objects were also initialized with a small random noise in each episode, so that the initial state of the episode was visualized."}, {"heading": "B. Evaluation", "text": "This year, we have reached the point where we are able to put ourselves at the top of the list in the way that we put ourselves at the top."}, {"heading": "VI. DISCUSSION", "text": "Experiments in the virtual and real world have shown that the deeply trained cardless motion planner can be directly transferred to invisible environments.The different navigation paths of the two training environments showed that the trained planner is to some extent influenced by the environment. Env-2 is much more aggressive when approaching obstacles, so the robot can navigate more successfully in the complex real environment.In this paper, we compared our deeply trained model with the original and low-dimensional map-based motion planner. Compared to the trajectories of Move Base, the path generated by our planner is more winding. One possible explanation is that the network has neither the memory of the previous observation nor the long-term predictive capability of the planner. Thus, LSTM and RNN [27] are possible solutions to this problem. We present this revision as future work."}, {"heading": "VII. CONCLUSION", "text": "In this paper, a mapless motion planner has been thoroughly trained by deep reinforcement learning. We have revised the state-of-the-art method of continuous reinforcement learning so that training and sampling can be carried out in parallel. By using 10-dimensional sensor findings and target position as input, the proposed motion planner can be used directly in invisible real environments without fine-tuning, even though it is only trained in a virtual environment. Compared to the low-dimensional, map-based motion planner, our approach has proven more robust for extremely complicated environments."}], "references": [{"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv preprint arXiv:1509.02971, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Continuous deep qlearning with model-based acceleration", "author": ["S. Gu", "T. Lillicrap", "I. Sutskever", "S. Levine"], "venue": "Proceedings of The 33rd International Conference on Machine Learning, 2016, pp. 2829\u20132838.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Y. Zhu", "R. Mottaghi", "E. Kolve", "J.J. Lim", "A. Gupta", "L. Fei-Fei", "A. Farhadi"], "venue": "arXiv preprint arXiv:1609.05143, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "cad) 2 rl: Real single-image flight without a single real image", "author": ["F. Sadeghi", "S. Levine"], "venue": "arXiv preprint arXiv:1611.04201, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards cognitive exploration through deep reinforcement learning for mobile robots", "author": ["L. Tai", "M. Liu"], "venue": "arXiv preprint arXiv:1610.01733, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Simultaneous localization and mapping: part i", "author": ["H. Durrant-Whyte", "T. Bailey"], "venue": "IEEE robotics & automation magazine, vol. 13, no. 2, pp. 99\u2013110, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "International Conference on Machine Learning, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Wifi signal strength-based robot indoor localization", "author": ["Y. Sun", "M. Liu", "M.Q.-H. Meng"], "venue": "Information and Automation (ICIA), 2014 IEEE International Conference on. IEEE, 2014, pp. 250\u2013256.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Let the light guide us: Vlc-based localization", "author": ["K. Qiu", "F. Zhang", "M. Liu"], "venue": "IEEE Robotics & Automation Magazine, vol. 23, no. 4, pp. 174\u2013183, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Performance of optical flow techniques for indoor navigation with a mobile robot", "author": ["C. McCarthy", "N. Bames"], "venue": "Robotics and Automation, 2004. Proceedings. ICRA\u201904. 2004 IEEE International Conference on, vol. 5. IEEE, 2004, pp. 5093\u20135098.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "A behaviour-based architecture for mapless navigation using vision", "author": ["M.S. Guzel", "R. Bicker"], "venue": "International Journal of Advanced Robotic Systems, vol. 9, no. 1, p. 18, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Vfh+: Reliable obstacle avoidance for fast mobile robots", "author": ["I. Ulrich", "J. Borenstein"], "venue": "Robotics and Automation, 1998. Proceedings. 1998 IEEE International Conference on, vol. 2. IEEE, 1998, pp. 1572\u20131577.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Range-sensor based navigation in three dimensions", "author": ["I. Kamon", "E. Rimon", "E. Rivlin"], "venue": "Robotics and Automation, 1999. Proceedings. 1999 IEEE International Conference on, vol. 1. IEEE, 1999, pp. 163\u2013169.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1999}, {"title": "Off-road obstacle avoidance through end-to-end learning", "author": ["Y. LeCun", "U. Muller", "J. Ben", "E. Cosatto", "B. Flepp"], "venue": "NIPS, 2005, pp. 739\u2013746.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "A Deep-network Solution Towords Modelless Obstacle Avoidence", "author": ["L. Tai", "S. Li", "M. Liu"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2016, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepdriving: Learning affordance for direct perception in autonomous driving", "author": ["C. Chen", "A. Seff", "A. Kornhauser", "J. Xiao"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 2722\u20132730.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "From perception to decision: A data-driven approach to end-toend motion planning for autonomous ground robots", "author": ["M. Pfeiffer", "M. Schaeuble", "J. Nieto", "R. Siegwart", "C. Cadena"], "venue": "arXiv preprint arXiv:1609.07910, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Socially compliant mobile robot navigation via inverse reinforcement learning", "author": ["H. Kretzschmar", "M. Spies", "C. Sprunk", "W. Burgard"], "venue": "The International Journal of Robotics Research, vol. 35, no. 11, pp. 1289\u20131307, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Reinforcement learning in robotics: A survey", "author": ["J. Kober", "J.A. Bagnell", "J. Peters"], "venue": "The International Journal of Robotics Research, vol. 32, no. 11, pp. 1238\u20131274, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates", "author": ["S. Gu", "E. Holly", "T. Lillicrap", "S. Levine"], "venue": "arXiv preprint arXiv:1610.00633, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to navigate in complex environments", "author": ["P. Mirowski", "R. Pascanu", "F. Viola", "H. Soyer", "A. Ballard", "A. Banino", "M. Denil", "R. Goroshin", "L. Sifre", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1611.03673, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "V-rep: A versatile and scalable robot simulation framework", "author": ["E. Rohmer", "S.P. Singh", "M. Freese"], "venue": "Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on. IEEE, 2013, pp. 1321\u20131326.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Ros: an open-source robot operating system", "author": ["M. Quigley", "K. Conley", "B. Gerkey", "J. Faust", "T. Foote", "J. Leibs", "R. Wheeler", "A.Y. Ng"], "venue": "ICRA workshop on open source software, vol. 3, no. 3.2. Kobe, 2009, p. 5.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Gaussian processes for machine learning", "author": ["C.E. Rasmussen"], "venue": "2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["M. Hausknecht", "P. Stone"], "venue": "arXiv preprint arXiv:1507.06527, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "1) Deep Reinforcement Learning in mobile robots: Deep Reinforcement Learning (deep-RL) methods achieve great success in many tasks including video games [1] and simulation control agents [2].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "1) Deep Reinforcement Learning in mobile robots: Deep Reinforcement Learning (deep-RL) methods achieve great success in many tasks including video games [1] and simulation control agents [2].", "startOffset": 187, "endOffset": 190}, {"referenceID": 2, "context": "The applications of deep reinforcement learning in robotics are mostly limited in manipulation [3] where the workspace is fully observable and stable.", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "In terms of mobile robots, deep-RL methods normally sampled the action from a discrete space to simplify the problem [4], [5], [6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 4, "context": "In terms of mobile robots, deep-RL methods normally sampled the action from a discrete space to simplify the problem [4], [5], [6].", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "In terms of mobile robots, deep-RL methods normally sampled the action from a discrete space to simplify the problem [4], [5], [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 6, "context": "problem through the prior map of the navigation environment [7] based on laser range findings.", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "In virtual environments, a nonholonomic differential drive robot was trained to learn how to arrive at the target position with obstacle avoidance through asynchronous deep reinforcement learning [8].", "startOffset": 196, "endOffset": 199}, {"referenceID": 8, "context": "Our planner also provides a planning solution for other low-cost localization methods like wifi localization [9] and visible-light communication [10].", "startOffset": 109, "endOffset": 112}, {"referenceID": 9, "context": "Our planner also provides a planning solution for other low-cost localization methods like wifi localization [9] and visible-light communication [10].", "startOffset": 145, "endOffset": 149}, {"referenceID": 10, "context": "[11] used optical flow to navigate a mobile robot in corridor-like environments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] built a mapless motion planner based on a homing system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] developed a vector field histogram (VFH) for obstacle avoidance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] used three-dimensional range data to plan a three-dimensional motion.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "For learning-based obstacle avoidance, deep neural networks have been successfully applied on monocular images [15] and depth images [16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "For learning-based obstacle avoidance, deep neural networks have been successfully applied on monocular images [15] and depth images [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "[17] used semantics information extracted from the image by deep neural networks to decide the behavior of the autonomous vehicle.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] used a deep learning model to map the laser range findings and target position to the moving commands.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] used inverse reinforcement learning methods to make robots interact with humans in a socially compliant way.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "tasks [20].", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "[1] utilized deep neural networks for the function estimation of value-based reinforcement learning which was called deep reinforcement learning or deep Q-network (DQN), and achieved a huge improvement in playing Atari games.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] proposed deep deterministic policy gradients (DDPG) to use deep neural networks on the actorcritic reinforcement learning method where both the policy and value of the reinforcement learning were represented", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] proposed continuous DQN based on the normalized advantage function (NAF).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Therefore, asynchronous deep reinforcement learning [8] used separated threads for training and sample collection (A3C).", "startOffset": 52, "endOffset": 55}, {"referenceID": 20, "context": "[21] proposed asynchronous NAF and trained the model with real-world samples where a door opening task was accomplished by a real robot arm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] trained a simulated agent to learn navigation in a virtual environment through raw images based on A3C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] trained an image-based planner where the robot learned to navigate to the referenced image place based on the instant view.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "In this section, we present the extension of asynchronous deep-RL in DDPG [2].", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "It has been successfully applied to many deep-RL methods like Advantage-Actor-Critic [8] and NAF [21].", "startOffset": 85, "endOffset": 88}, {"referenceID": 20, "context": "It has been successfully applied to many deep-RL methods like Advantage-Actor-Critic [8] and NAF [21].", "startOffset": 97, "endOffset": 101}, {"referenceID": 1, "context": "Both of the two target-networks are updated softly as [2].", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "DQN [1] can only be applied to a discrete action space.", "startOffset": 4, "endOffset": 7}, {"referenceID": 1, "context": "In this paper, we use DDPG [2] to train our model and extend it to an asynchronous version, as described in Section III.", "startOffset": 27, "endOffset": 30}, {"referenceID": 22, "context": "The virtual training environments were simulated by V-REP [23].", "startOffset": 58, "endOffset": 62}, {"referenceID": 22, "context": "The training procedure of our model was implemented in virtual environments simulated by V-REP [23] .", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "We trained the model from scratch with an Adam [24] optimizer on a single Nvidia GeForce GTX 1080 GPU3 for 800 k training steps which took almost 20 hours.", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "The robot operating system (ROS)[25] provided the communication interface between different agents.", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": "These 10-dimensional findings were extended to a 810-dimensional vector covering the field of view through an RBF kernel Gaussian process regression [26] for the local cost-map prediction that we called 10-dimensional Move Base in the following experiments.", "startOffset": 149, "endOffset": 153}, {"referenceID": 26, "context": "Thus LSTM and RNN [27] are possible solutions for that problem.", "startOffset": 18, "endOffset": 22}], "year": 2017, "abstractText": "Deep Reinforcement Learning has been successful in various virtual tasks, but it is still rarely used in real world applications especially for continuous control of mobile robots navigation. In this paper, we present a learning-based mapless motion planner by taking the 10-dimensional range findings and the target position as input and the continuous steering commands as output. Traditional motion planners for mobile ground robots with a laser range sensor mostly depend on the map of the navigation environment where both the highly precise laser sensor and the map building work of the environment are indispensable. We show that, through an asynchronous deep reinforcement learning method, a mapless motion planner can be trained end-to-end without any manually designed features and prior demonstrations. The trained planner can be directly applied in unseen virtual and real environments. We also evaluated this learning-based motion planner and compared it with the traditional motion planning method, both in virtual and real environments. The experiments show that the proposed mapless motion planner can navigate the nonholonomic mobile robot to the desired targets without colliding with any obstacles.", "creator": "LaTeX with hyperref package"}}}