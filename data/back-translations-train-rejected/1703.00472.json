{"id": "1703.00472", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Reinforcement Learning for Pivoting Task", "abstract": "In this work we propose an approach to learn a robust policy for solving the pivoting task. Recently, several model-free continuous control algorithms were shown to learn successful policies without prior knowledge of the dynamics of the task. However, obtaining successful policies required thousands to millions of training episodes, limiting the applicability of these approaches to real hardware. We developed a training procedure that allows us to use a simple custom simulator to learn policies robust to the mismatch of simulation vs robot. In our experiments, we demonstrate that the policy learned in the simulator is able to pivot the object to the desired target angle on the real robot. We also show generalization to an object with different inertia, shape, mass and friction properties than those used during training. This result is a step towards making model-free reinforcement learning available for solving robotics tasks via pre-training in simulators that offer only an imprecise match to the real-world dynamics.", "histories": [["v1", "Wed, 1 Mar 2017 19:25:55 GMT  (6528kb,D)", "http://arxiv.org/abs/1703.00472v1", "(Rika Antonova and Silvia Cruciani contributed equally)"]], "COMMENTS": "(Rika Antonova and Silvia Cruciani contributed equally)", "reviews": [], "SUBJECTS": "cs.RO cs.LG", "authors": ["rika antonova", "silvia cruciani", "christian smith", "danica kragic"], "accepted": false, "id": "1703.00472"}, "pdf": {"name": "1703.00472.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning for Pivoting Task", "authors": ["Rika Antonova", "Silvia Cruciani", "Christian Smith", "Danica Kragic"], "emails": ["dani}@kth.se"], "sections": [{"heading": null, "text": "This year, it has reached the point where there is only one person who is able to establish himself in the region."}, {"heading": "II. RELATED WORK", "text": "Extrinsic dexterity has been extensively researched in robotics and is still an open challenge. Pivoting is a type of extrinsic dexterity problem that has lately attracted the attention of the robotics research community. In the following section we give an overview of the work to date. As we use profound reinforcement learning in contrast to previous approaches, we also give a brief overview of this learning approach in the context of robotics."}, {"heading": "A. Previous Work in Pivoting", "text": "Existing solutions for panning use environmental influences, movements of the robotic arm to generate inertial forces and external forces such as gravity. In [6], the authors use gravity to rotate an object by using a contact surface between two stable poses. Panning occurs in an open loop and there is no control of the gripping force. Conversely, several other panning works focus heavily on controlling the torque that the gripper applies to the object: in [7], the authors focus on upwelling movements. They address the problem with an energy-based control and consider the gripper's ability to exert dissipative torques on the object, thanks to friction at the pivot point. In this case, the movement of the object seems to be limited to a vertical plane and the approach strongly depends on fast sensory feedback and rapid response time of the gripper. The adaptive control for panning the gripper uses gravity and controlled concretion parameter to set back-up the feedback strategy, therefore, with this feedback prefeedback of the object was also applied."}, {"heading": "B. Deep Reinforcement Learning in Robotics", "text": "This year is the highest in the history of the country."}, {"heading": "III. PROBLEM DESCRIPTION", "text": "The problem we are dealing with is the pivoting of the tool to a desired angle while holding it in the gripper. This can be achieved by moving the arm of the robot to generate inertial forces sufficient to move the tool and at the same time to open or close the fingers of the gripper to change the friction at the pivot point, thereby achieving a more precise control of the movement. We assume that the robot is able to use one of the usual planning approaches to first detect the tool, but the position of the tool between the fingers is initially at a random angle. However, the aim is to pivot the tool to a desired target angle. The first challenge is that the movement of the tool is affected by friction at the pivot point. This is dictated by the materials of the tool and the fingers, the deformation of the tool and the fingers, and potentially the air flow. It is difficult to accurately estimate all the coefficient of friction required to construct it."}, {"heading": "IV. MODELING FOR SIMULATION", "text": "As mentioned in the previous section, simulators that can effectively integrate global task dynamics information can be used to learn flexible control strategies. In this section, we describe the dynamic model of the system and the friction model that we used to simulate the pan task."}, {"heading": "A. Dynamic Model", "text": "Our system consists of a 1 DOF parallel gripper, which is attached to a link that can rotate around a single axis. This system is a 2-link planar arm that is actuated, in which the link that is actuated corresponds to the pivot point. We assume that we can control the desired acceleration on the first link. This system is shown in Figure 2. The dynamic model of the system is given by: (I + mr2 + mlr cos (\u03c6tl) \u03c6 \ufffd grp + (I + mr 2) \u03c6 \ufffd tl +...... + mlr sin (\u03c6tl) \u0445 2 grp + mgr cos (inspgrp + \u03c6tl) = \u03c4f, (1) where the variables are as follows: split and split are the angles of the first and second links respectively."}, {"heading": "B. Friction Model", "text": "Our pivot solution uses friction at the point of contact between the gripper and the tool to control rotation, which is controlled by increasing or tightening the gripper. If the tool does not move, this is the coefficient of static friction and fn is the normal force applied by the fingers of the gripper to the tool. If the tool moves, this is the friction torque \u0441tl 6 = 0, we model the friction torque \u0441f as viscous friction and the coulomb friction [17]: \u03c4f = \u2212 \u00b5v\u03c6 tl \u2212 \u00b5cfn sgn (?"}, {"heading": "V. LEARNING", "text": "As discussed in Section II-B, recent algorithmic advances in amplification learning indicate the possibility that model-free algorithms could be applied to robotic tasks. However, as these algorithms are often not sample-efficient enough to learn in real time on the hardware, our approach is to design a training process to learn robust strategies in a simple simulated environment and then deploy them on the robot. Below, we first describe the general training approach, then present the formalization of pivot assignment as a Markov decision process, and then give a brief summary of the model-free amplification learning algorithm we used for policy search."}, {"heading": "A. Learning Robust Policies using a Simulator", "text": "As described in Section III, our approach is to enable learning from simulated environments while being robust to the discrepancies between simulation and actual execution on the robot. To this end, we first built a simple custom simulator using the equations described in Section IV. Then, to simplify learning strategies that are robust to uncertainty, we injected up to 10% random delays for arm and finger actions into the simulation. In addition, we added 10% noise to the friction values estimated for the tool modeled by the simulator. Subsequently, we trained a model-free search algorithm for strategies to search for deep amplifications in our simulated environment. Finally, we implemented the resulting guidelines on the robot (Baxter) for evaluation, an approach that allowed us to keep the simulator simple and fast while at the same time enabling a learning policy that is robust to the discrepancy between simulated and real environments."}, {"heading": "B. Pivoting Task as a Markov Decision Process", "text": "We formulate the pivot task as a Markov decision-making process (MDP) - a tuple {S, A, P (s, s, a), R (s, a, s), \u03b1}. The state space S consists of states that are observed at all times. Step t: [\u03c6tl \u2212 \u03c6tgt, \u03c6 tl, \u03c6grp, \u03c6 grp, dfing], with notation as in Section IV-A. Using the given distance of the tool angle as the first component of the state vector allows us to facilitate generalization in learning, since in our setting the movements of the robot arm are symmetrical, e.g. the optimal policy for reaching the target angle 0 \u0445 starting from the tool at \u2212 45 \u0445 would be symmetrical in the case of beginning of 45 \u0445. For settings without symmetry, the representation of the state space could instead include the movement of the movement angle 100."}, {"heading": "C. Policy Search using Reinforcement Learning", "text": "As discussed in Section II-B, several continuous control model-free reinforcement learning algorithms would be suitable for learning optimal strategies for formulating our pivot task as MDP. In our experiments, we found that Trust Region Policy Optimization [12] was able to solve the problem without extensive parameter adjustments. In the following, we summarize the main ideas behind TRPO for a brief overview.TRPO is a method for optimizing large non-linear control measures such as those represented by neural networks. The algorithm aims to ensure monotonous improvement during training by calculating a safe region for exploration. The most important optimization performed by the algorithm is to solve iteratively a series of pre-reserved strategies, reserved strategies, reserved strategies, reserved strategies, reserved strategies, reserved strategies: maximizehabi It is an approach q (a | s) q (a) q (a) Q\u03b8old (s)]]] pre-reserved strategies, reserved strategies, reserved strategies, reserved research,"}, {"heading": "VI. EXPERIMENTS", "text": "In this section, we will first discuss the details of implementing the training in the simulator, then show initial evaluation results indicating the robustness potential of friction changes, and then describe experiments with the Baxter robot. We will discuss the performance of the tool with parameters similar to those used during the training, as well as results for manipulating the tool with different shapes, masses, inertial parameters and unknown friction properties. The results indicate that our training process is capable of developing robust strategies capable of solving the swing task on both known and unknown tools."}, {"heading": "A. Learning Robust Policies in Simulation", "text": "To facilitate experimentation with various RL algorithms, we implemented a custom environment in OpenAI Gym [19] for the pan task MDP (as defined in Section V-B). To experiment with TRPO [12] and DDPG [13] algorithms, we used the rllab implementation [14] as a starting point, then adjusted the behavior and parameters for different experiments for this project. Both TRPO and DPG papers documented the exploration and network training parameters used to obtain results for simulated control tasks. Based on these reported values, we experimented with several options such as exploration rate, batch size for neural network training and network size. We were unable to obtain robust learning progress with DPG (training progress would often come to a halt), so for the rest of the experiments we decided to use only TRO, but continued Pexting in our settings."}, {"heading": "B. Experiments on Hardware with Baxter", "text": "We implemented the proposed approach on a Baxter robot, using one of its 7 DOF arms. We selected slightly deformable fingertips for the gripper in order to be able to control the force applied to the tool by changing the distance between the fingers. As with many commercially available robots, Baxter's gripper cannot be controlled at the same high frequency as the joints. Specifically, we measured that the fingers of the gripper were only able to achieve the desired distance after allowing a delay of 80ms. This delay entails a longer time required to perform any control action on the hardware, hence the longer time it takes the tool to reach the target due to these limitations of the robot. To estimate the angle we used a colored segmentation to track a colored marker on the tool, the images were collected by a Kinect 2 RGB-D camera that was running at 30 fps while we were holding the gripper in a horizontal position."}, {"heading": "VII. CONCLUSIONS AND FUTURE WORK", "text": "In this thesis, we proposed to solve the pivoting task by building a simple custom task dynamics simulator and learning control policies using reinforcement learning. We presented the learning process that is able to manage the mismatch between the simulated and real settings, so it does not require accurate estimates of all tool parameters. We then demonstrated the ability of the learned policy to solve the pivoting task on the Baxter robot. However, in order to expand our work to a more general set of dextrosive manipulation problems, we can in the future combine pivoting with pushing, which would allow more possibilities for repositioning. In fact, pivoting allows us to change the orientation of the object around a single axis, while using pushing to translate the object, thus changing the contact point. The next step would be to incorporate existing approaches to modeling sliding into the simulator."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by the European Union Framework Programme H2020-645403 RobDREAM."}], "references": [{"title": "Extrinsic dexterity: In-hand manipulation with external forces", "author": ["N.C. Dafle", "A. Rodriguez", "R. Paolini", "B. Tang", "S.S. Srinivasa", "M. Erdmann", "M.T. Mason", "I. Lundberg", "H. Staab", "T. Fuhlbrigge"], "venue": "2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2014, pp. 1578\u20131585.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Regrasping", "author": ["P. Tournassoud", "T. Lozano-Perez", "E. Mazer"], "venue": "Robotics and Automation. Proceedings. 1987 IEEE International Conference on, vol. 4, Mar 1987, pp. 1924\u20131928.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1987}, {"title": "A framework for planning dexterous manipulation", "author": ["J.C. Trinkle", "J.J. Hunter"], "venue": "Robotics and Automation, 1991. Proceedings., 1991 IEEE International Conference on, Apr 1991, pp. 1245\u20131251 vol.2.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1991}, {"title": "Dynamic regrasping using a high-speed multifingered hand and a high-speed vision system", "author": ["N. Furukawa", "A. Namiki", "S. Taku", "M. Ishikawa"], "venue": "Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006., May 2006, pp. 181\u2013 187.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "A two-phase gripper to reorient and grasp", "author": ["N. Chavan-Dafle", "M.T. Mason", "H. Staab", "G. Rossano", "A. Rodriguez"], "venue": "2015 IEEE International Conference on Automation Science and Engineering (CASE), Aug 2015, pp. 1249\u20131255.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "A general framework for open-loop pivoting", "author": ["A. Holladay", "R. Paolini", "M.T. Mason"], "venue": "2015 IEEE International Conference on Robotics and Automation (ICRA), May 2015, pp. 3675\u20133681.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Swing-up regrasping algorithm using energy control", "author": ["A. Sintov", "A. Shapiro"], "venue": "2016 IEEE International Conference on Robotics and Automation (ICRA), May 2016, pp. 4888\u20134893.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "In-hand manipulation using gravity and controlled slip", "author": ["F.E. Vi\u00f1a", "Y. Karayiannidis", "K. Pauwels", "C. Smith", "D. Kragic"], "venue": "Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on, Sept 2015, pp. 5636\u20135641.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive control for pivoting with visual and tactile feedback", "author": ["F.E. Vi\u00f1a", "Y. Karayiannidis", "C. Smith", "D. Kragic"], "venue": "2016 IEEE International Conference on Robotics and Automation (ICRA), May 2016, pp. 399\u2013406.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Locally weighted learning for control", "author": ["C.G. Atkeson", "A.W. Moore", "S. Schaal"], "venue": "Lazy learning. Springer, 1997, pp. 75\u2013113.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["S. Levine", "P. Abbeel"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1071\u20131079.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel"], "venue": "CoRR, abs/1502.05477, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv preprint arXiv:1509.02971, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "arXiv preprint arXiv:1604.06778, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["A.A. Rusu", "M. Vecerik", "T. Roth\u00f6rl", "N. Heess", "R. Pascanu", "R. Hadsell"], "venue": "arXiv preprint arXiv:1610.04286, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Predicting slippage and learning manipulation affordances through gaussian process regression", "author": ["F.E. Vi\u00f1a", "Y. Bekiroglu", "C. Smith", "Y. Karayiannidis", "D. Kragic"], "venue": "IEEE-RAS International Conference on Humanoid Robots, Oct 2013, pp. 462\u2013468.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Friction models and friction compensation", "author": ["H. Olsson", "K.J. strm", "M. Gfvert", "C.C.D. Wit", "P. Lischinsky"], "venue": "Eur. J. Control, p. 176, 1998.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Computer simulation of stick-slip friction in mechanical dynamic systems.", "author": ["D. Karnopp"], "venue": "J. Dyn. Syst. Meas. Control.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1985}, {"title": "Openai gym", "author": ["G. Brockman", "V. Cheung", "L. Pettersson", "J. Schneider", "J. Schulman", "J. Tang", "W. Zaremba"], "venue": "2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "This problem falls in the general class of extrinsic dexterity problems formulated by [1].", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "Instead of relying on releasing the object and picking it up again [2], we focus on in-hand manipulation.", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "Successful strategies for this dexterous task often rely on multi-fingered robotic hands or customized grippers [3]\u2013[5].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "Successful strategies for this dexterous task often rely on multi-fingered robotic hands or customized grippers [3]\u2013[5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "In [6] the authors exploit gravity to rotate an object between two stable poses by using a contact surface.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "Conversely, several other works on pivoting strongly focus on controlling the torque applied by the gripper on the object: in [7] the authors focus on swing-up motions.", "startOffset": 126, "endOffset": 129}, {"referenceID": 7, "context": "The adaptive control for pivoting presented in [8] exploits gravity and controlled slip with only visual feedback.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "This approach has then been extended to consider also tactile feedback in [9].", "startOffset": 74, "endOffset": 77}, {"referenceID": 9, "context": "[10], [11]), such approaches might limit ability to incorporate prior knowledge \u2013 for example, forgoing the knowledge that can be easily incorporated into dynamics equations in a simulator.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[10], [11]), such approaches might limit ability to incorporate prior knowledge \u2013 for example, forgoing the knowledge that can be easily incorporated into dynamics equations in a simulator.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "reported to perform well on several simulated robotic tasks: Trust Region Policy Optimization (TRPO) [12] and Deep Deterministic Policy Gradient (DDPG) [13].", "startOffset": 101, "endOffset": 105}, {"referenceID": 12, "context": "reported to perform well on several simulated robotic tasks: Trust Region Policy Optimization (TRPO) [12] and Deep Deterministic Policy Gradient (DDPG) [13].", "startOffset": 152, "endOffset": 156}, {"referenceID": 13, "context": "For example, a recent benchmarking paper used up to 25 million steps when training [14].", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "The problem of \u201cforgetting\u201d in the context of learning in robotics is discussed at length in [15] \u2013 the work that attempted to resolve the issue by pre-training in simulation, then using Progressive Neural Network architecture to continue learning on the real hardware.", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "It is difficult to estimate precisely all the necessary coefficients to construct a high-fidelity friction model [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 16, "context": "When the tool moves with respect to the gripper, that is \u03c6\u0307tl 6= 0, we model the friction torque \u03c4f as viscous friction and Coulomb friction [17]:", "startOffset": 141, "endOffset": 145}, {"referenceID": 17, "context": "To alleviate this problem we follow the approach proposed in [18].", "startOffset": 61, "endOffset": 65}, {"referenceID": 7, "context": "Since most of the robots are not equipped with tactile sensors to measure the normal force fn at the contact point, we follow the approach proposed in [8] and express this force as a function of the distance dfing between the two fingers, assuming a linear deformation model:", "startOffset": 151, "endOffset": 154}, {"referenceID": 11, "context": "In our experiments, we found that Trust Region Policy Optimization [12] was able to solve the problem without extensive parameter adjustment.", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "where \u03b8old denotes the initial (or previous) set of policy parameters, \u03b8 denotes the updated policy parameters, \u03c0\u03b8 is the stochastic policy parameterized by \u03b8, q is a sampling distribution for exploration, Q\u03b8old is the Q function approximator estimated from previous samples, and the expectation E is taken over samples obtained using the policy from previous iteration (see [12] for details).", "startOffset": 375, "endOffset": 379}, {"referenceID": 13, "context": "TRPO has been shown to be competitive with (and sometimes outperform) other recent continuous state and action RL algorithms [14].", "startOffset": 125, "endOffset": 129}, {"referenceID": 18, "context": "To facilitate experimenting with various RL algorithms, we implemented a custom environment in OpenAI Gym [19] for the pivoting task MDP (as defined in Section V-B).", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "To experiment with TRPO [12] and DDPG [13] algorithms,", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "To experiment with TRPO [12] and DDPG [13] algorithms,", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "we used rllab implementation [14] as a starting point, then adjusted the behavior and parameters as needed for various experiments for this project.", "startOffset": 29, "endOffset": 33}], "year": 2017, "abstractText": "In this work we propose an approach to learn a robust policy for solving the pivoting task. Recently, several model-free continuous control algorithms were shown to learn successful policies without prior knowledge of the dynamics of the task. However, obtaining successful policies required thousands to millions of training episodes, limiting the applicability of these approaches to real hardware. We developed a training procedure that allows us to use a simple custom simulator to learn policies robust to the mismatch of simulation vs robot. In our experiments, we demonstrate that the policy learned in the simulator is able to pivot the object to the desired target angle on the real robot. We also show generalization to an object with different inertia, shape, mass and friction properties than those used during training. This result is a step towards making model-free reinforcement learning available for solving robotics tasks via pre-training in simulators that offer only an imprecise match to the real-world dynamics.", "creator": "LaTeX with hyperref package"}}}