{"id": "1703.00484", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Truth and Regret in Online Scheduling", "abstract": "We consider a scheduling problem where a cloud service provider has multiple units of a resource available over time. Selfish clients submit jobs, each with an arrival time, deadline, length, and value. The service provider's goal is to implement a truthful online mechanism for scheduling jobs so as to maximize the social welfare of the schedule. Recent work shows that under a stochastic assumption on job arrivals, there is a single-parameter family of mechanisms that achieves near-optimal social welfare. We show that given any such family of near-optimal online mechanisms, there exists an online mechanism that in the worst case performs nearly as well as the best of the given mechanisms. Our mechanism is truthful whenever the mechanisms in the given family are truthful and prompt, and achieves optimal (within constant factors) regret.", "histories": [["v1", "Wed, 1 Mar 2017 20:09:43 GMT  (135kb,D)", "http://arxiv.org/abs/1703.00484v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.AI cs.DS cs.LG", "authors": ["shuchi chawla", "nikhil devanur", "janardhan kulkarni", "rad niazadeh"], "accepted": false, "id": "1703.00484"}, "pdf": {"name": "1703.00484.pdf", "metadata": {"source": "CRF", "title": "Truth and Regret in Online Scheduling", "authors": ["Shuchi Chawla", "Nikhil Devanur", "Janardhan Kulkarni"], "emails": ["(shuchi@cs.wisc.edu)", "(nikdev@microsoft.com)", "(jakul@microsoft.com)", "(rad@cs.cornell.edu)"], "sections": [{"heading": null, "text": "We model the problem of competition against a family of online termination mechanisms as one of learning expert advice. A primary challenge is that all the appointment decisions we make affect not only the payout of the current step, but also the availability of resources and the payouts in future steps. In addition, the switch from one algorithm (also known as an expert) to another online is a challenge, both because it requires synchronization with the state of the latter algorithm and because it influences the incentive structure of the algorithms.We also show how we can adapt our algorithm to a non-clairvoyant environment in which job lengths are unknown until jobs are completed. Once again in this environment we get truthfulness together with asymptotically optimal regret (within polylogarithmic factors). ar Xiv: 170 3.00 484v 1 [cs.G T] 1M ar"}, {"heading": "1 Introduction", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves, and that they are able to abide by the rules that they have set themselves in order to achieve their goals. (...) In fact, it is the case that they are able to achieve their goals. (...) In fact, it is the case that they are able to achieve their goals. (...) In fact, it is the case that they are able to achieve their goals. (...) In fact, it is the case that they are able to achieve their goals. (...)"}, {"heading": "2 Model and definitions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 The online job scheduling problem", "text": "An example of the kind of job planning is the kind of job creation. (We assume that the number of jobs (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers per time unit (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number (the number of workers) per time unit (the number of workers) per time unit (the number of workers) per time unit (the number (the number of the number of workers) per time unit (the number of the number of workers) per time unit (the number of the number of workers) per time unit (the number (the number of the number of workers) per time unit (the number of workers) per time unit (the number of the number of the number of workers) per time unit (the number of workers) per time unit (the number of the number of the number of workers)"}, {"heading": "2.2 Learning from expert advice", "text": "We will reduce the repentance minimization problem for online termination to the problem of learning expert advice. In the latter, we will become n experts (indexed by i). In each time step, the online algorithm must select an (potentially random) expert to follow. An opponent then reveals a reward vector {r (i). We assume that the opponent is forgetful, that is, he cannot observe the internal coin rotations of the algorithm. The total remuneration of expert i is of [T] r (i). The remuneration of the algorithm is of E [T] r (ii)."}, {"heading": "3 The clairvoyant setting", "text": "In this section, we look at the online planning problem in the clairvoyant setting, where each order indicates its length (in addition to the rest of its type) at the time of its arrival. We get n online planning algorithms, ALG1,..., ALGn, and our goal is to design an online algorithm that minimizes regret in retrospect compared to the best of n algorithms. We start by showing how to switch between algorithms in such a way that the truthfulness of Section 3.1 is preserved. In Section 3.2, we present a reduction of this problem to the problem of learning from expert advice on switching costs. In Section 5, we prove that the repentance guarantee we receive from the reduction is optimal."}, {"heading": "3.1 Truthful switching", "text": "In this section, we will show how to switch between truthful mechanisms while preserving truthfulness and ensuring that the welfare loss is limited. We will consider the following setting. Let A and B be two sequences that respect truthful planning mechanisms. Our goal is to switch from mechanism A to mechanism B to time 0. (This is just a normalization of the time index to simple notation.) We will consider how to perform this switch in the clairvoyant setting and extend our algorithm to the nonclairvoyant setting in Section 4.2. The loss of prosperity from our switching algorithm is covered in the following lemma.Lemma 3.1. Considering the sequence of truthful mechanisms A and B, there is an order that respects the truthful mechanism C, which at least achieves prosperity, t \u2264 0 Wt (A) + assuming 1 Wt (B) - 2vmaxm.In particular, all the jobs are completed by the time of the mechanism A and will be completed."}, {"heading": "3.1.1 The online switching algorithm", "text": "Definition 3.1. the mechanism C (Lemma 3.1) is as follows: 1. For jobs that arrive with time 0, imitate mechanism A and return the same allocation, schedule and prices. Note that jobs that are scheduled in this step, by the time dmax.2. Mark the remaining time windows in [1, dmax] as unavailable. This means that for all jobs j that were not considered in the previous step (because aj > 0) and have deadline dj \u2264 dmax, we reject service and charge a price of 0.3. For all remaining jobs, i.e. jobs j with aj > 0 and dj > dmax, consider the jobs in the order of arrival and do the following: (a) If B rejects the work, then we reject j. (b) If there are not enough places to cover the length of j before its deadline, reject j. (c) Otherwise, we accept and schedule j in a manner."}, {"heading": "3.1.2 Truthfulness", "text": "We begin by proving that no early completion takes place.Lemma 3.3. Any replacement slot assigned in step (3) (c) is always at a later date relative to the unavailable slot it replaces. Proof Suppose one of the slots assigned to a job in mechanism B is not available in mechanism C. If t \u2264 dmax is then assigned later by building the replacement slot, the slot itself is a replacement slot for another job j \u2032. However, the arrival time of j \u2032, aj \u2032 is no greater than aj, because the jobs are processed in FIFO order. Since replacement slots are assigned in chronological order, all slots in [aj \u2032, t \u2212 1] must have been unavailable than when t \u2032 was assigned. Now, all slots in [aj, t] are not available when we consider job j, so its replacement for the slot can only be later.Lemma 3.4 mechanism is true."}, {"heading": "3.1.3 Welfare", "text": "Define a time slot t > dmax to be \"free\" if the mechanism C has fewer jobs in time t than mechanism B. The number of free slots in due time t is the difference, since it is not negative, and zero otherwise. We first argue that there are few free slots in C's Schedule.Lemma 3.5. All replacement slots occur before the first free slot. Proof Let t be the first free slot. Consider a job that arrives before t. This job is not assigned according to t, because t is free and therefore available, and replacement slots are assigned in chronological order. We will argue that jobs that do not have replacement slots after t, i.e., they get the same slots as in B. This is by induction in the order of arrival of these jobs. Consider the very first such job. All previous jobs arrive before t by definition and have no replacement slots after t, as already argued, therefore all slots are available for this job in 'B's."}, {"heading": "3.2 Reduction to experts with switching costs", "text": "Let Expert-ALG (C) designate an online algorithm for the problem of learning expert advice with switching costs C, which achieves the repentance guarantee of Theorem 2.1. Expert-ALG receives an instance of n experts, indexed by i-The-Switcher or FTS. It determines for each step t-T a random expert and then follows its advice, which expert, a.k.a. algorithm, should be executed at any time step. Definition 3.2. Given the n online scheduling algorithms, ALG1,.. ALGn, the Follow-The-Switcher, a.k.a. FTS, algorithm simulates the online algorithm, a.k.a. mDefinition 3.2 \u2212 n algorithm for ALG1. \u2212 ALGn, it follows the algorithm."}, {"heading": "4 Non-clairvoyant setting", "text": "In this section, we will consider minimizing regret in hiring without clairvoyants; we will remember the most important differences here. Each job reports all parts of its type except its length, and the algorithm only observes the length of a job when (and if) it is completed. Therefore, an algorithm without clairvoyants cannot plan for a full schedule in advance. The algorithm maintains a line of unfinished jobs and does not decide at any time which job it has planned from that line at that time. The deadline for a job is now the number of time windows the job is willing to wait. If a job passes its deadline, which means that the number of time windows a job has waited since its arrival exceeds a threshold, then the job is deleted from the queue. Therefore, the notion of speed is not entirely applicable to hiring without clairvoyants."}, {"heading": "4.1 Scheduling algorithms with random restarts", "text": "It is easy to provide examples that show that the welfare obtained from an online scheduling mechanism is very sensitive to timing in the opposite model of job planning, i.e. that we can drastically shift the start time of the welfare mechanism obtained, as demonstrated in Example 4.1.Example, how mechanisms to start FIFO scheduling with price uptake control at p = 1. Suppose we have three jobs J1, J2, J3, with v1 = v3 = 1. Suppose (a1, l1), (a2, l2) = (a3, l3) = (4, T \u2212 4). (All of them have immediate deadlines, which means that they must be scheduled when they arrive.) Normally, we plan jobs J1 and J3 and generate a welfare that is equal."}, {"heading": "4.2 Truthful switching in the non-clairvoyant setting", "text": "We repeat a subtle aspect of truthfulness in the max max setting: a job that tries to influence the mechanism by changing its position in the arrival order can be treated in the same way as a job that reports a later arrival time: it cannot be advantageous to do so because the mechanism is true. The random restart algorithm of definition 4.1 is an example of a mechanism that is both true and proper: all jobs whose values are less than a threshold price are rejected, and the rest of the jobs are included in the arrival description. We maintain that the random restart algorithm of definition 4.1 preserves the truthfulness of the underlying schema mechanism."}, {"heading": "4.3 Reduction to multi-armed bandits", "text": "In this section, we show how to design a truthful online learning algorithm that minimizes the regret associated with the random restart benchmark."}, {"heading": "5 Lower Bounds", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "A Deferred proofs for upper bound constructions", "text": "Lemma 4.1. If A is an order that respects the truthful mechanism, then A with (arbitrary) restarts is also an order that respects and is truthful. Proof We assume that A is restarted at the interval [1, dmax +'max], and that the decision to restart does not depend on what the job report reports. For a job that arrives with time 0, reporting an arrival time after 0 is not advantageous, as this would mean that this job can only be processed after the initial time dmax +' max + 1, and that the job would have passed its deadline by that time. If the job reports an arrival time before time 1, then the truthfulness of A guarantees that no false report is favored. Now, let us consider a job that arrives with time 0. If the deadline of this job is such that it must begin with time dmax +'max, then it does not matter what he reports that he is not scheduling jobs according to the other jobs where we are considering the truthfulness of all the jobs."}, {"heading": "B Doubling Trick In the Multi-Armed Bandit Problem", "text": "In this section, we briefly explain the doubling trick. This helps in the case when the algorithm Bandit-ALG1 (R, T) (which allows a repentance guarantee for O (R \u221a T log (T))), R and T must know in advance, and now we want to design an algorithm Bandit-ALG2 that does not need to know these parameters and still wants to achieve O (R \u221a T log (T)). Doubling trick for area R. For convenience, we assume that R = 2K for some integer K. Bandit-ALG2 does the following. It starts with a guess (initialized to 1) for area R and simulates bandit-ALG1 with this guess. Each time, it sees a reward that is not in the area."}, {"heading": "C Deferred proofs for lower bound constructions", "text": "There is an instance of clairvoyant planning process where the regret of each online algorithm has to be taken into account in relation to the problem of prediction with expert advice. Let's remember the lower limit of prediction for the problem of prediction with expert advice. In this case we have two experts. In each round, the opponent selects one of the two experts in uniform at random (and regardless of the previous rounds) and assigns a reward of 1. The opponent sets the reward of the other expert to zero. The expected reward of each online algorithm is T / 2. We suffer from a regret of the two experts, as the expected reward of the best expert is E (max."}], "references": [{"title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "Improved rates for the stochastic continuumarmed bandit problem", "author": ["Peter Auer", "Ronald Ortner", "Csaba Szepesv\u00e1ri"], "venue": "In International Conference on Computational Learning Theory,", "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Prompt mechanism for ad placement over time", "author": ["Yossi Azar", "Ety Khaitsin"], "venue": "In International Symposium on Algorithmic Game Theory,", "citeRegEx": "Azar and Khaitsin.,? \\Q2011\\E", "shortCiteRegEx": "Azar and Khaitsin.", "year": 2011}, {"title": "Truthful online scheduling with commitments", "author": ["Yossi Azar", "Inna Kalp-Shaltiel", "Brendan Lucier", "Ishai Menache", "Joseph Seffi Naor", "Jonathan Yaniv"], "venue": "In Proceedings of the Sixteenth ACM Conference on Economics and Computation,", "citeRegEx": "Azar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2015}, {"title": "On-line learning and the metrical task system problem", "author": ["Avrim Blum", "Carl Burch"], "venue": "Machine Learning,", "citeRegEx": "Blum and Burch.,? \\Q2000\\E", "shortCiteRegEx": "Blum and Burch.", "year": 2000}, {"title": "Static optimality and dynamic search-optimality in lists and trees", "author": ["Avrim Blum", "Shuchi Chawla", "Adam Kalai"], "venue": "In Proceedings of the Thirteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Blum et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2002}, {"title": "Bounding the power of preemption in randomized scheduling", "author": ["Ran Canetti", "Sandy Irani"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Canetti and Irani.,? \\Q1998\\E", "shortCiteRegEx": "Canetti and Irani.", "year": 1998}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": "Cambridge university press,", "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Stability of service under time-of-use pricing", "author": ["Shuchi Chawla", "Nikhil Devanur", "Alexander Holroyd", "Anna Karlin", "James Martin", "Balasubramanian Sivan"], "venue": "In STOC,", "citeRegEx": "Chawla et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chawla et al\\.", "year": 2017}, {"title": "Prompt mechanisms for online auctions", "author": ["Richard Cole", "Shahar Dobzinski", "Lisa Fleischer"], "venue": "In International Symposium on Algorithmic Game Theory,", "citeRegEx": "Cole et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cole et al\\.", "year": 2008}, {"title": "Bandits with switching costs: T2/3 regret", "author": ["Ofer Dekel", "Jian Ding", "Tomer Koren", "Yuval Peres"], "venue": "In Proceedings of the Forty-sixth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Dekel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2014}, {"title": "On the expectation of the maximum of iid geometric random variables", "author": ["Bennett Eisenberg"], "venue": "Statistics & Probability Letters,", "citeRegEx": "Eisenberg.,? \\Q2008\\E", "shortCiteRegEx": "Eisenberg.", "year": 2008}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "In European conference on computational learning theory,", "citeRegEx": "Freund and Schapire.,? \\Q1995\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1995}, {"title": "Online auctions with re-usable goods", "author": ["Mohammad T. Hajiaghayi", "Robert D. Kleinberg", "MohammadMahdian", "David C. Parkes"], "venue": "In Proceedings of the 6th ACM conference on Electronic commerce,", "citeRegEx": "Hajiaghayi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hajiaghayi et al\\.", "year": 2005}, {"title": "Optimal mechanism design and money burning", "author": ["J. Hartline", "T. Roughgarden"], "venue": "In Proc. 39th ACM Symp. on Theory of Computing,", "citeRegEx": "Hartline and Roughgarden.,? \\Q2008\\E", "shortCiteRegEx": "Hartline and Roughgarden.", "year": 2008}, {"title": "Paramils: an automatic algorithm configuration framework", "author": ["Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown", "Thomas St\u00fctzle"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hutter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2009}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "Online ascending auctions for gradually expiring items", "author": ["Ron Lavi", "Noam Nisan"], "venue": "Journal of Economic Theory,", "citeRegEx": "Lavi and Nisan.,? \\Q2015\\E", "shortCiteRegEx": "Lavi and Nisan.", "year": 2015}, {"title": "Randomized Algorithms", "author": ["Rajeev Motwani", "Prabhakar Raghavan"], "venue": null, "citeRegEx": "Motwani and Raghavan.,? \\Q1995\\E", "shortCiteRegEx": "Motwani and Raghavan.", "year": 1995}, {"title": "Satzilla: portfolio-based algorithm selection for sat", "author": ["Lin Xu", "Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Xu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2008}, {"title": "log(T )) regret bound (assuming R and T are finite). Doubling trick for time horizon T is fairly standard", "author": [], "venue": "e.g. see Auer et al", "citeRegEx": "\u221a,? \\Q2007\\E", "shortCiteRegEx": "\u221a", "year": 2007}], "referenceMentions": [{"referenceID": 4, "context": "Canetti and Irani (1998) showed, in particular, that no online algorithm can achieve a less than polylogarithmic competitive ratio for this problem in comparison to the hindsight optimal schedule.", "startOffset": 0, "endOffset": 25}, {"referenceID": 4, "context": "Canetti and Irani (1998) showed, in particular, that no online algorithm can achieve a less than polylogarithmic competitive ratio for this problem in comparison to the hindsight optimal schedule. On the other hand, Lavi and Nisan (2015) showed that no deterministic mechanism that is truthful with respect to all of the parameters can approximate social welfare better than a factor T in the worst case, where T is the time horizon, even for unit length jobs on a single machine.", "startOffset": 0, "endOffset": 238}, {"referenceID": 2, "context": "In the face of these strong negative results, a number of works have considered weakening various aspects of the model in order to obtain positive results, such as requiring a slackness condition on the jobs\u2019 deadlines Azar et al. (2015), allowing the algorithm to make tardy decisions Hajiaghayi et al.", "startOffset": 219, "endOffset": 238}, {"referenceID": 2, "context": "In the face of these strong negative results, a number of works have considered weakening various aspects of the model in order to obtain positive results, such as requiring a slackness condition on the jobs\u2019 deadlines Azar et al. (2015), allowing the algorithm to make tardy decisions Hajiaghayi et al. (2005), satisfying incentive compatibility with respect to only a few of the jobs\u2019 parameters Cole et al.", "startOffset": 219, "endOffset": 311}, {"referenceID": 2, "context": "In the face of these strong negative results, a number of works have considered weakening various aspects of the model in order to obtain positive results, such as requiring a slackness condition on the jobs\u2019 deadlines Azar et al. (2015), allowing the algorithm to make tardy decisions Hajiaghayi et al. (2005), satisfying incentive compatibility with respect to only a few of the jobs\u2019 parameters Cole et al. (2008); Azar and Khaitsin (2011), etc.", "startOffset": 219, "endOffset": 417}, {"referenceID": 2, "context": "(2008); Azar and Khaitsin (2011), etc.", "startOffset": 8, "endOffset": 33}, {"referenceID": 11, "context": "Hartline and Roughgarden (2008) advocate a general framework for generating an appropriate benchmark for such online problems\u2014determine the class of all mechanisms that are optimal for the problem in an appropriate stochastic setting; compete against the best of these Bayesian optimal mechanisms for a worst case instance.", "startOffset": 0, "endOffset": 32}, {"referenceID": 6, "context": "Our work is inspired by the recent work of Chawla et al. (2017) that shows that when jobs are drawn from an i.", "startOffset": 43, "endOffset": 64}, {"referenceID": 4, "context": ", Blum and Burch (2000); Blum et al.", "startOffset": 2, "endOffset": 24}, {"referenceID": 4, "context": ", Blum and Burch (2000); Blum et al. (2002).", "startOffset": 2, "endOffset": 44}, {"referenceID": 8, "context": "Chawla et al.\u2019s mechanism is a simple greedy best-effort mechanism based on posted prices. The mechanism announces a price per-unit of resource for each time period into the future. When a job arrives it gets scheduled in a best-effort FIFO manner in the cheapest slots that satisfy its requirements. Chawla et al. show that if the number of resources per time period is large enough, then for any underlying distribution over job types, there exists a set of prices such that this posted-pricing-FIFO mechanism achieves a 1 \u2212 approximation to expected social welfare. Unfortunately, finding the best price to offer requires knowing the fine details of the distribution over job types and solving a large linear program. Chawla et al.\u2019s mechanisms are parameterized by a single price. Machine learning techniques have been succesfully used to tune parameters of heuristics for a wide variety of problems Xu et al. (2008); Hutter et al.", "startOffset": 0, "endOffset": 921}, {"referenceID": 8, "context": "Chawla et al.\u2019s mechanism is a simple greedy best-effort mechanism based on posted prices. The mechanism announces a price per-unit of resource for each time period into the future. When a job arrives it gets scheduled in a best-effort FIFO manner in the cheapest slots that satisfy its requirements. Chawla et al. show that if the number of resources per time period is large enough, then for any underlying distribution over job types, there exists a set of prices such that this posted-pricing-FIFO mechanism achieves a 1 \u2212 approximation to expected social welfare. Unfortunately, finding the best price to offer requires knowing the fine details of the distribution over job types and solving a large linear program. Chawla et al.\u2019s mechanisms are parameterized by a single price. Machine learning techniques have been succesfully used to tune parameters of heuristics for a wide variety of problems Xu et al. (2008); Hutter et al. (2009). This is typically done in a batch setting, where past data is used to find a good setting of parameters for the heuristic.", "startOffset": 0, "endOffset": 943}, {"referenceID": 8, "context": "Fortunately, the benchmark suggested by Chawla et al. (2017), namely the optimal posted-pricing-FIFO mechanism, continues to obtain near-optimal welfare\u2014the mechanism does not need to know jobs\u2019 lengths in order to make scheduling decisions, although the existence of a good posted price assumes that lengths along with other job parameters are drawn from some i.", "startOffset": 40, "endOffset": 61}, {"referenceID": 11, "context": "Then, several different online algorithms are known to achieve a regret of O(R \u221a T log n), and this bound is tight Freund and Schapire (1995); Kalai and Vempala (2005); Cesa-Bianchi and Lugosi (2006).", "startOffset": 115, "endOffset": 142}, {"referenceID": 11, "context": "Then, several different online algorithms are known to achieve a regret of O(R \u221a T log n), and this bound is tight Freund and Schapire (1995); Kalai and Vempala (2005); Cesa-Bianchi and Lugosi (2006).", "startOffset": 115, "endOffset": 168}, {"referenceID": 7, "context": "Then, several different online algorithms are known to achieve a regret of O(R \u221a T log n), and this bound is tight Freund and Schapire (1995); Kalai and Vempala (2005); Cesa-Bianchi and Lugosi (2006).", "startOffset": 169, "endOffset": 200}, {"referenceID": 16, "context": "1 (Kalai and Vempala (2005)).", "startOffset": 3, "endOffset": 28}, {"referenceID": 0, "context": "2 (Auer et al. (1995)).", "startOffset": 3, "endOffset": 22}, {"referenceID": 10, "context": "For this problem, we mainly use that there is no algorithm with a regret of o(T 2/3) (Dekel et al., 2014), to get a similar lower bound for our problem.", "startOffset": 85, "endOffset": 105}, {"referenceID": 8, "context": "see Chawla et al. (2017)), the welfare loss due to independent (but infrequent) random restarts will easily be bounded.", "startOffset": 4, "endOffset": 25}, {"referenceID": 11, "context": "To bound the other term, we use the following fact, proved in Eisenberg (2008), about independent and identically distributed geometric random variables.", "startOffset": 62, "endOffset": 79}, {"referenceID": 11, "context": "To bound the other term, we use the following fact, proved in Eisenberg (2008), about independent and identically distributed geometric random variables. Lemma 4.6 (Eisenberg (2008)).", "startOffset": 62, "endOffset": 182}, {"referenceID": 10, "context": "Our lower bound follows by a reduction from the lower bound given in Dekel et al. (2014) for the multi-armed bandit problem with switching costs.", "startOffset": 69, "endOffset": 89}, {"referenceID": 10, "context": "Our lower bound follows by a reduction from the lower bound given in Dekel et al. (2014) for the multi-armed bandit problem with switching costs. For the bandit problem with switching costs with n actions, Dekel et al. (2014) show that there exists a sequence of loss functions `1, `2, .", "startOffset": 69, "endOffset": 226}, {"referenceID": 10, "context": "In our lower bound instance, we fix n = 2, and let `i(1) and `i(2) denote the losses of actions 1 and 2 in round i as defined in Dekel et al. (2014). We map each round of the game to 8 time steps; that is, round i corresponds to the time interval [8i, 8(i+ 1)\u2212 1].", "startOffset": 129, "endOffset": 149}, {"referenceID": 10, "context": "In our lower bound instance, we fix n = 2, and let `i(1) and `i(2) denote the losses of actions 1 and 2 in round i as defined in Dekel et al. (2014). We map each round of the game to 8 time steps; that is, round i corresponds to the time interval [8i, 8(i+ 1)\u2212 1]. Our instance has 4 sets of jobs J1, J2, J3, and J4, as shown in Figure 1. In each round, one job from each set arrives. Jobs in the set J1 arrive at the beginning of each round; that is, at time steps 8i for i = 0, 1, 2, . . . ..T . The processing length of a job j \u2208 J1 that arrives in the round i is 6 with probability pi(1) and 8 with probability (1\u2212 pt(1)), where pi(1) = 1/2 + `i(1)/2. Observe that processing lengths of the jobs in J1 depend on losses defined by Dekel et al. (2014). Further, the jobs in J1 have a value of 1 per unit length.", "startOffset": 129, "endOffset": 754}, {"referenceID": 10, "context": "In our lower bound instance, we fix n = 2, and let `i(1) and `i(2) denote the losses of actions 1 and 2 in round i as defined in Dekel et al. (2014). We map each round of the game to 8 time steps; that is, round i corresponds to the time interval [8i, 8(i+ 1)\u2212 1]. Our instance has 4 sets of jobs J1, J2, J3, and J4, as shown in Figure 1. In each round, one job from each set arrives. Jobs in the set J1 arrive at the beginning of each round; that is, at time steps 8i for i = 0, 1, 2, . . . ..T . The processing length of a job j \u2208 J1 that arrives in the round i is 6 with probability pi(1) and 8 with probability (1\u2212 pt(1)), where pi(1) = 1/2 + `i(1)/2. Observe that processing lengths of the jobs in J1 depend on losses defined by Dekel et al. (2014). Further, the jobs in J1 have a value of 1 per unit length. In round i, a job from J2 arrives at time 8(i+ 1)\u2212 2 for i = 0, 1, 2, 3, . . . ..T , and has a processing length of 2. The value per unit length of jobs in J2 is 3. The set J3 consists of jobs that arrive at time instants 8i\u2212 3 for i = 1, 2, . . . T , and have value per unit length of 2. The processing length of job j \u2208 J3 released in the round i is 4 with probability pi(2) and 2 with probability 1\u2212 pi(2), where pi(2) = `i(2). Similar to the jobs in J1, the processing lengths of jobs in J3 depend on the losses defined by the result of Dekel et al. (2014). Finally, the jobs in set J4 are released at time steps 8i\u2212 1 for i = 1, 2, .", "startOffset": 129, "endOffset": 1375}, {"referenceID": 10, "context": "The result of Dekel et al. (2014) shows that the problem has a minimax regret of least \u03a9\u0303(T 2/3), when there is a switching cost between any pair of actions.", "startOffset": 14, "endOffset": 34}, {"referenceID": 10, "context": "The result of Dekel et al. (2014) shows that the problem has a minimax regret of least \u03a9\u0303(T 2/3), when there is a switching cost between any pair of actions. However, it is easy to modify the proof in Dekel et al. (2014), where there is a switching cost only between action 2 to action 1, losing a factor of 2 in the regret bound Dekel et al.", "startOffset": 14, "endOffset": 221}, {"referenceID": 10, "context": "The result of Dekel et al. (2014) shows that the problem has a minimax regret of least \u03a9\u0303(T 2/3), when there is a switching cost between any pair of actions. However, it is easy to modify the proof in Dekel et al. (2014), where there is a switching cost only between action 2 to action 1, losing a factor of 2 in the regret bound Dekel et al. (2014). This completes our reduction.", "startOffset": 14, "endOffset": 350}, {"referenceID": 10, "context": "To extend the lower bound to random restarting benchmarks, we need the following theorem from Dekel et al. (2014) for the bandit with switching costs problem.", "startOffset": 94, "endOffset": 114}, {"referenceID": 10, "context": "6 (Dekel et al. (2014)).", "startOffset": 3, "endOffset": 23}], "year": 2017, "abstractText": "We consider a scheduling problem where a cloud service provider has multiple units of a resource available over time. Selfish clients submit jobs, each with an arrival time, deadline, length, and value. The service provider\u2019s goal is to implement a truthful online mechanism for scheduling jobs so as to maximize the social welfare of the schedule. Recent work shows that under a stochastic assumption on job arrivals, there is a single-parameter family of mechanisms that achieves near-optimal social welfare. We show that given any such family of near-optimal online mechanisms, there exists an online mechanism that in the worst case performs nearly as well as the best of the given mechanisms. Our mechanism is truthful whenever the mechanisms in the given family are truthful and prompt, and achieves optimal (within constant factors) regret. We model the problem of competing against a family of online scheduling mechanisms as one of learning from expert advice. A primary challenge is that any scheduling decisions we make affect not only the payoff at the current step, but also the resource availability and payoffs in future steps. Furthermore, switching from one algorithm (a.k.a. expert) to another in an online fashion is challenging both because it requires synchronization with the state of the latter algorithm as well as because it affects the incentive structure of the algorithms. We further show how to adapt our algorithm to a non-clairvoyant setting where job lengths are unknown until jobs are run to completion. Once again, in this setting, we obtain truthfulness along with asymptotically optimal regret (within polylogarithmic factors). 1 ar X iv :1 70 3. 00 48 4v 1 [ cs .G T ] 1 M ar 2 01 7", "creator": "LaTeX with hyperref package"}}}