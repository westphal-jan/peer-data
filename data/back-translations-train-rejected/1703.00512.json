{"id": "1703.00512", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "PMLB: A Large Benchmark Suite for Machine Learning Evaluation and Comparison", "abstract": "The selection, development, or comparison of machine learning methods in data mining can be a difficult task based on the target problem and goals of a particular study. Numerous publicly available real-world and simulated benchmark datasets have emerged from different sources, but their organization and adoption as standards have been inconsistent. As such, selecting and curating specific benchmarks remains an unnecessary burden on machine learning practitioners and data scientists. The present study introduces an accessible, curated, and developing public benchmark resource to facilitate identification of the strengths and weaknesses of different machine learning methodologies. We compare meta-features among the current set of benchmark datasets in this resource to characterize the diversity of available data. Finally, we apply a number of established machine learning methods to the entire benchmark suite and analyze how datasets and algorithms cluster in terms of performance. This work is an important first step towards understanding the limitations of popular benchmarking suites and developing a resource that connects existing benchmarking standards to more diverse and efficient standards in the future.", "histories": [["v1", "Wed, 1 Mar 2017 21:20:11 GMT  (697kb,D)", "http://arxiv.org/abs/1703.00512v1", "14 pages, 5 figures, submitted for review to JMLR"]], "COMMENTS": "14 pages, 5 figures, submitted for review to JMLR", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["randal s olson", "william la cava", "patryk orzechowski", "ryan j urbanowicz", "jason h moore"], "accepted": false, "id": "1703.00512"}, "pdf": {"name": "1703.00512.pdf", "metadata": {"source": "CRF", "title": "PMLB: A Large Benchmark Suite for Machine Learning Evaluation and Comparison", "authors": ["Randal S. Olson", "William La Cava", "Patryk Orzechowski", "Ryan J. Urbanowicz", "Jason H. Moore"], "emails": ["olsonran@upenn.edu", "lacava@upenn.edu", "patryk@upenn.edu", "ryanurb@upenn.edu", "jhmoore@upenn.edu"], "sections": [{"heading": null, "text": "Keywords: machine learning, benchmarking, data storage, classification"}, {"heading": "1. Introduction", "text": "The term benchmarking is used in machine learning (ML) to refer to the evaluation and comparison of ML methods with respect to their ability to learn patterns in \"benchmark\" data sets used as \"standards.\" Benchmarking could be seen as an approach to identifying the respective strengths, to confirm that a new method is working successfully as expected and can reliably find simple patterns that identify existing methods used by Hastie et al. (2009). A rigorous way to look at benchmarking is as an approach to identifying the respective strengths. Olson, La Cava, Orzechowski, Urbanowicz, and Moore.License: CC-BY 4.0, see https: / creativecommons.org / licenses / by. Attribution requests are provided at http: / / jmlr.org / papers / x.html.ar Xiv: 170 3.00 512v 1and weaknesses of a given metology in contrast to other Caruzil and Nicaraguana 2006."}, {"heading": "2. Penn Machine Learning Benchmark (PMLB)", "text": "We created the Penn Machine Learning Benchmark (PMLB) datasets from a wide range of existing ML benchmark suites, including the UCI ML Repository Lichman (2013), Kaggle Goldbloom (2010), KEEL Alcala \u0301 et al. (2010) and Meta-Learning Benchmark Mature (2012). As such, PMLB includes most of the real-world benchmark datasets used in ML benchmarking studies. To make it easier to use PMLB, we pre-processed all datasets to follow a standard column format where the characteristics correspond to the columns in the dataset and each instance in the dataset is a row. All categorical characteristics and labels with non-numerical codes have been replaced by numerical equivalents (e.g. \"Low,\" \"\" Medium, \"\" Visualization \"and\" High \"dataset.\""}, {"heading": "2.1 PMLB Meta-Features", "text": "In the current release, PMLB comprises 165 datasets. Meta-characteristics of these datasets are summarized in Figure 1. These meta-characteristics are defined as follows: \u2022 # Instances: The number of instances in each dataset. \u2022 # Features: The number of characteristics in each dataset with > 2 levels. \u2022 # Continuous characteristics: The number of continuously evaluated characteristics in each dataset with only two levels. \u2022 # Category and Ordinary characteristics: The number of discrete characteristics in each dataset with > 2 levels. \u2022 # Continuous characteristics: The number of continuously evaluated characteristics in each dataset. \u2212 The distinction of categorical and ordinary characteristics from continuous characteristics was automatically determined on the basis of whether a variable was considered a \"float\" in a pandas datafram pan. \u2022 Endpoint type: Whether each dataset is a binary data class or a multicultural data class."}, {"heading": "2.2 PMLB Python Interface", "text": "To facilitate access to PMLB datasets, we have released an open-source Python interface for PMLB on PyPi1. This interface provides a simple fetch data function that returns each dataset in PMLB as a pandas-pandas dataFrame. For example, to fetch the clean2 dataset: import pmlb c l ean2 data = pmlb. f e t c h d a t a (\"c lean2\") The clean2 data variable then contains a dataset frame of the clean2 dataset, with the class column corresponding to the class names and the remaining columns being the characteristics. The fetch data function has several caching and preprocessing options, all of which are available in the PMLB repository2.To get a complete list of all datasets available in PMLB, users can access the dataset names used prior to the analysis: 1. https: 2. / datasepyp.pythb / httppb / enn / pplb."}, {"heading": "3. Evaluating Machine Learning Methods", "text": "To provide a basis for comparison, we evaluated 13 ML classification methods from scikit-LB Pedregosa et al. (2011) on the 165 data sets in PMLB. These methods include: \u2022 Gaussian Na \ufffd \u0131ve Bayes (NB) \u2022 Bernoulli Na \ufffd \u0131ve Bayes \u2022 Multinomial Na \ufffd ve Bayes \u2022 Logistic Regression \u2022 Linear Classifier trained on stochastic gradient descent (SGD) \u2022 Support Vector Classifier (SVC) with a linear, polynomial, sigmoid or RBF kernel \u2022 Passive Aggressive Classifier \u2022 K-Nearest (KNN) \u2022 Decision Tree \u2022 Extra Random Forest (a.k.a.)"}, {"heading": "4. Results", "text": "To characterize the data sets in PMLB, they are clustered by their metafeatures in Section 4.1. Then, we analyze the data sets based on ML performance in Section 4.2, which determines which data sets can be solved with high or low accuracy, as well as which data sets appear universally simple or difficult for the set of different ML algorithms to model accurately, compared with those that seem to be particularly useful for highlighting differential ML algorithm performance."}, {"heading": "4.1 Dataset Meta-Features", "text": "We used k-means to group the normalized meta-characteristics of the datasets into 5 clusters visualized along the first two main component axes in Figure 2 (note that the first two components of the PCA explain 49% of the variance, so we assume that there will be some overlap of the clusters in the visualization).The number of clusters was chosen to close the trade-off between the interpretability of the clusters and the proper separation of the bundled datasets as defined by the silhouette score. Figure 2 contains two clusters centered on failed datasets (Cluster 2 and 4).All clusters are compared in more detail according to the mean values of the dataset meta characteristics in each cluster in Figure 3. Clusters 0 and 1 contain most of the datasets and are separated by their endpoint type, i.e. Cluster 0 consists of binary cluster classification profiles, while Data Classes 1 consists of relatively high datasets and Data Classes 1 consists of nearly high datasets."}, {"heading": "4.2 Model-Dataset Biclustering", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "5. Discussion and Conclusion", "text": "The primary objective of this paper is to introduce an ongoing research project on the benchmarking of LB methods. Specifically, we have collected and curated 165 data sets from the most popular data repositories and introduced PMLB, a new set of benchmark standards for comparing and evaluating different ML methods. Aside from the repository itself, we have conducted a comprehensive analysis of the performance of numerous standard ML methods that can serve as a basis for evaluating and comparing newly developed ML methods, and have evaluated the diversity of these existing benchmark data sets to identify deficits that will be addressed by the subsequent addition of further benchmarks in a future relation."}, {"heading": "Acknowledgments", "text": "We thank Dr. Andreas C. Mueller for his valuable contribution to the development of this project and Penn Medicine Academic Computing Services for using their computing resources. This work was supported by National Institutes of Health grants AI116794, DK112217, ES013508, EY022300, HL134015, LM009012, LM010098, LM011360, TR001263 and the Warren Center for Network and Data Science."}], "references": [{"title": "Spectral biclustering", "author": ["Yuval Kluger", "Ronen Basri", "Joseph T Chang", "Mark Gerstein"], "venue": null, "citeRegEx": "Kluger et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kluger et al\\.", "year": 2009}, {"title": "Natural Selection", "author": ["William La Cava", "Kourosh Danai", "Lee Spector"], "venue": null, "citeRegEx": "Cava et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Cava et al\\.", "year": 1992}, {"title": "A flexible computational framework for detecting, characterizing, and interpreting statistical patterns of epistasis in genetic studies of human disease susceptibility", "author": ["Jason H Moore", "Joshua C Gilbert", "Chia-Ti Tsai", "Fu-Tien Chiang", "Todd Holden", "Nate Barney", "Bill C White"], "venue": "Journal of Theoretical Biology,", "citeRegEx": "Moore et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Moore et al\\.", "year": 2006}, {"title": "Open issues in genetic programming", "author": ["Michael O\u2019Neill", "Leonardo Vanneschi", "Steven Gustafson", "Wolfgang Banzhaf"], "venue": "Genetic Programming and Evolvable Machines,", "citeRegEx": "O.Neill et al\\.,? \\Q2010\\E", "shortCiteRegEx": "O.Neill et al\\.", "year": 2010}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "A comprehensive dataset for evaluating approaches of various meta-learning tasks", "author": ["Matthias Reif"], "venue": "In First International Conference on Pattern Recognition and Methods (ICPRAM),", "citeRegEx": "Reif.,? \\Q2012\\E", "shortCiteRegEx": "Reif.", "year": 2012}, {"title": "Creating and benchmarking a new dataset for physical activity monitoring", "author": ["Attila Reiss", "Didier Stricker"], "venue": "In Proceedings of the 5th International Conference on Pervasive Technologies Related to Assistive Environments,", "citeRegEx": "Reiss and Stricker.,? \\Q2012\\E", "shortCiteRegEx": "Reiss and Stricker.", "year": 2012}, {"title": "Machine learning benchmarks and random forest regression", "author": ["Mark R Segal"], "venue": "Center for Bioinformatics & Molecular Biostatistics,", "citeRegEx": "Segal.,? \\Q2004\\E", "shortCiteRegEx": "Segal.", "year": 2004}, {"title": "Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition", "author": ["Johannes Stallkamp", "Marc Schlipsing", "Jan Salmen", "Christian Igel"], "venue": "Neural Networks,", "citeRegEx": "Stallkamp et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stallkamp et al\\.", "year": 2012}, {"title": "Exstracs 2.0: description and evaluation of a scalable learning classifier system", "author": ["Ryan J Urbanowicz", "Jason H Moore"], "venue": "Evolutionary Intelligence,", "citeRegEx": "Urbanowicz and Moore.,? \\Q2015\\E", "shortCiteRegEx": "Urbanowicz and Moore.", "year": 2015}, {"title": "Predicting the difficulty of pure, strict, epistatic models: metrics for simulated model selection", "author": ["Ryan J Urbanowicz", "Jeff Kiralis", "Jonathan M Fisher", "Jason H Moore"], "venue": "BioData Mining,", "citeRegEx": "Urbanowicz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Urbanowicz et al\\.", "year": 2012}, {"title": "Gametes: a fast, direct algorithm for generating pure, strict, epistatic models with random architectures", "author": ["Ryan J Urbanowicz", "Jeff Kiralis", "Nicholas A Sinnott-Armstrong", "Tamra Heberling", "Jonathan M Fisher", "Jason H Moore"], "venue": "BioData Mining,", "citeRegEx": "Urbanowicz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Urbanowicz et al\\.", "year": 2012}, {"title": "A balanced accuracy function for epistasis modeling in imbalanced datasets using multifactor dimensionality reduction", "author": ["Digna R Velez"], "venue": "Genetic Epidemiology,", "citeRegEx": "Velez,? \\Q2007\\E", "shortCiteRegEx": "Velez", "year": 2007}, {"title": "Better gp benchmarks: community survey results and proposals", "author": ["David R. White", "James McDermott", "Mauro Castelli", "Luca Manzoni", "Brian W. Goldman", "Gabriel Kronberger", "Wojciech Ja\u015bkowski", "Una-May O\u2019Reilly", "Sean Luke"], "venue": "Genetic Programming and Evolvable Machines,", "citeRegEx": "White et al\\.,? \\Q2013\\E", "shortCiteRegEx": "White et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "For example, the GAMETES genetic-data simulation software generates epistatic patterns of association in \u2018mock\u2019 single nucleotide polymorphism (SNP) data Urbanowicz et al. (2012b,a). The third form is toy data, which we will define here as data that is also artificially generated with a known embedded pattern but without an emphasis on representing real-world data, e.g., the parity or multiplexer problems Blum et al. (2003); Koza (1992).", "startOffset": 154, "endOffset": 428}, {"referenceID": 6, "context": "For example, the GAMETES genetic-data simulation software generates epistatic patterns of association in \u2018mock\u2019 single nucleotide polymorphism (SNP) data Urbanowicz et al. (2012b,a). The third form is toy data, which we will define here as data that is also artificially generated with a known embedded pattern but without an emphasis on representing real-world data, e.g., the parity or multiplexer problems Blum et al. (2003); Koza (1992). It is worth noting that the term \u2018toy dataset\u2019 has often been used to describe a small and simple dataset such as the examples included with algorithm software.", "startOffset": 154, "endOffset": 441}, {"referenceID": 2, "context": "The genetic programming (GP) community has also previously discussed appropriate benchmarking when comparing GP methodologies O\u2019Neill et al. (2010); McDermott et al.", "startOffset": 126, "endOffset": 148}, {"referenceID": 2, "context": "The genetic programming (GP) community has also previously discussed appropriate benchmarking when comparing GP methodologies O\u2019Neill et al. (2010); McDermott et al. (2012); White et al.", "startOffset": 126, "endOffset": 173}, {"referenceID": 2, "context": "The genetic programming (GP) community has also previously discussed appropriate benchmarking when comparing GP methodologies O\u2019Neill et al. (2010); McDermott et al. (2012); White et al. (2013). Benchmarking efforts may focus on a specific application of interest, e.", "startOffset": 126, "endOffset": 194}, {"referenceID": 2, "context": "The genetic programming (GP) community has also previously discussed appropriate benchmarking when comparing GP methodologies O\u2019Neill et al. (2010); McDermott et al. (2012); White et al. (2013). Benchmarking efforts may focus on a specific application of interest, e.g. traffic sign detection Stallkamp et al. (2012), or a more narrowly defined ML problem type, e.", "startOffset": 126, "endOffset": 317}, {"referenceID": 2, "context": "classification of 2-way epistatic interactions Moore et al. (2006); Li et al.", "startOffset": 47, "endOffset": 67}, {"referenceID": 2, "context": "classification of 2-way epistatic interactions Moore et al. (2006); Li et al. (2016). The scope of benchmarking may also be limited by practical computational requirements.", "startOffset": 47, "endOffset": 85}, {"referenceID": 2, "context": "classification of 2-way epistatic interactions Moore et al. (2006); Li et al. (2016). The scope of benchmarking may also be limited by practical computational requirements. There are currently a number of challenges that make it difficult to benchmark ML methods in a useful and globally accepted manner. For one, there are an overwhelming number of publications that reference the use of benchmark datasets, however there are surprisingly few publications that discuss the topic of appropriate ML benchmarking in general. Additionally, collecting and curating real-world benchmark datasets remains a challenge for many researchers. Although repositories such as the UCI ML repository Lichman (2013) and Kaggle Goldbloom (2010) provide dozens of real-world datasets to download for free, these datasets come in myriad formats and require considerable preprocessing before ML methods can be applied to them.", "startOffset": 47, "endOffset": 700}, {"referenceID": 2, "context": "classification of 2-way epistatic interactions Moore et al. (2006); Li et al. (2016). The scope of benchmarking may also be limited by practical computational requirements. There are currently a number of challenges that make it difficult to benchmark ML methods in a useful and globally accepted manner. For one, there are an overwhelming number of publications that reference the use of benchmark datasets, however there are surprisingly few publications that discuss the topic of appropriate ML benchmarking in general. Additionally, collecting and curating real-world benchmark datasets remains a challenge for many researchers. Although repositories such as the UCI ML repository Lichman (2013) and Kaggle Goldbloom (2010) provide dozens of real-world datasets to download for free, these datasets come in myriad formats and require considerable preprocessing before ML methods can be applied to them.", "startOffset": 47, "endOffset": 728}, {"referenceID": 2, "context": "classification of 2-way epistatic interactions Moore et al. (2006); Li et al. (2016). The scope of benchmarking may also be limited by practical computational requirements. There are currently a number of challenges that make it difficult to benchmark ML methods in a useful and globally accepted manner. For one, there are an overwhelming number of publications that reference the use of benchmark datasets, however there are surprisingly few publications that discuss the topic of appropriate ML benchmarking in general. Additionally, collecting and curating real-world benchmark datasets remains a challenge for many researchers. Although repositories such as the UCI ML repository Lichman (2013) and Kaggle Goldbloom (2010) provide dozens of real-world datasets to download for free, these datasets come in myriad formats and require considerable preprocessing before ML methods can be applied to them. As a result, many benchmark datasets go unused simply because they are too difficult to preprocess. Further, while real-world benchmarks can be derived from many different problem domains, from a strict data science perspective, many of the benchmarks in repositories can have very similar meta-features (e.g. the number of instances, number of features, number of classes, presence of missing data, and similar signal to noise ratios, etc.), such that while they are representative of different real-world problems, they may not represent a diverse assembly of data science problems. This issue has been raised previously; when applying UCI datasets as benchmarks, it was noted that the scope of included datasets limited method evaluation, and suggested that repositories such as UCI should be expanded Segal (2004).", "startOffset": 47, "endOffset": 1725}, {"referenceID": 4, "context": "physical activity monitoring data Reiss and Stricker (2012) or dynamical systems simulation La Cava et al.", "startOffset": 34, "endOffset": 60}, {"referenceID": 1, "context": "physical activity monitoring data Reiss and Stricker (2012) or dynamical systems simulation La Cava et al. (2016), it is vital for the bioinformatics and ML community to have a comprehensive benchmark suite with which to compare and contrast ML methods.", "startOffset": 95, "endOffset": 114}, {"referenceID": 1, "context": "physical activity monitoring data Reiss and Stricker (2012) or dynamical systems simulation La Cava et al. (2016), it is vital for the bioinformatics and ML community to have a comprehensive benchmark suite with which to compare and contrast ML methods. Towards this goal, the present study introduces the Penn Machine Learning Benchmark (PMLB), a publicly available dataset suite (accessibly hosted on GitHub) initialized with 165 real-world, simulated, and toy benchmark datasets for evaluating supervised classification methods. PMLB includes datasets from many of the most-used ML benchmark suites, such as KEEL Alcal\u00e1 et al. (2010) and the UCI ML repository Lichman (2013).", "startOffset": 95, "endOffset": 637}, {"referenceID": 1, "context": "physical activity monitoring data Reiss and Stricker (2012) or dynamical systems simulation La Cava et al. (2016), it is vital for the bioinformatics and ML community to have a comprehensive benchmark suite with which to compare and contrast ML methods. Towards this goal, the present study introduces the Penn Machine Learning Benchmark (PMLB), a publicly available dataset suite (accessibly hosted on GitHub) initialized with 165 real-world, simulated, and toy benchmark datasets for evaluating supervised classification methods. PMLB includes datasets from many of the most-used ML benchmark suites, such as KEEL Alcal\u00e1 et al. (2010) and the UCI ML repository Lichman (2013). In addition to collecting data from these resources, PMLB standardizes the format of these data and provides useful interfaces for fetching datasets directly from the web.", "startOffset": 95, "endOffset": 678}, {"referenceID": 1, "context": "physical activity monitoring data Reiss and Stricker (2012) or dynamical systems simulation La Cava et al. (2016), it is vital for the bioinformatics and ML community to have a comprehensive benchmark suite with which to compare and contrast ML methods. Towards this goal, the present study introduces the Penn Machine Learning Benchmark (PMLB), a publicly available dataset suite (accessibly hosted on GitHub) initialized with 165 real-world, simulated, and toy benchmark datasets for evaluating supervised classification methods. PMLB includes datasets from many of the most-used ML benchmark suites, such as KEEL Alcal\u00e1 et al. (2010) and the UCI ML repository Lichman (2013). In addition to collecting data from these resources, PMLB standardizes the format of these data and provides useful interfaces for fetching datasets directly from the web. This initial PMLB repository is not meant to be comprehensive; it includes mainly real-world datasets and excludes regression datasets (i.e. those with a continuous-valued dependent variable), as well as any datasets with missing values. We have chosen to focus our initial assessment on available datasets in classification. This paper includes a highlevel analysis of the properties (i.e. meta-features) of the founding PMLB datasets, such as feature counts, class imbalance, etc. Further, we evaluate the performance of 13 standard statistical ML methods from scikit-learn Pedregosa et al. (2011) over the full set of PMLB datasets.", "startOffset": 95, "endOffset": 1451}, {"referenceID": 5, "context": "(2010), and meta-learning benchmark Reif (2012). As such, the PMLB includes most of the real-world benchmark datasets commonly used in ML benchmarking studies.", "startOffset": 36, "endOffset": 48}, {"referenceID": 4, "context": "Evaluating Machine Learning Methods To provide a basis for comparison, we evaluated 13 supervised ML classification methods from scikit-learn Pedregosa et al. (2011) on the 165 datasets in PMLB.", "startOffset": 142, "endOffset": 166}, {"referenceID": 10, "context": "ML methods were evaluated using balanced accuracy Velez et al. (2007); Urbanowicz and Moore (2015) as the scoring metric, which is a normalized version of accuracy that accounts for class imbalance by calculating accuracy on a per-class basis then averaging the per-class accuracies.", "startOffset": 50, "endOffset": 70}, {"referenceID": 8, "context": "(2007); Urbanowicz and Moore (2015) as the scoring metric, which is a normalized version of accuracy that accounts for class imbalance by calculating accuracy on a per-class basis then averaging the per-class accuracies.", "startOffset": 8, "endOffset": 36}, {"referenceID": 8, "context": "(2007); Urbanowicz and Moore (2015) as the scoring metric, which is a normalized version of accuracy that accounts for class imbalance by calculating accuracy on a per-class basis then averaging the per-class accuracies. For more information on these ML methods, see Hastie et al. (2009) and the scikit-learn documentation Pedregosa et al.", "startOffset": 8, "endOffset": 288}, {"referenceID": 4, "context": "(2009) and the scikit-learn documentation Pedregosa et al. (2011). When we evaluated each ML method, we first scaled the features of the datasets by subtracting the mean and scaling the features to unit variance.", "startOffset": 42, "endOffset": 66}, {"referenceID": 0, "context": "2 Model-Dataset Biclustering Figure 4 summarizes the results of biclustering the balanced accuracy of the tuned models according to the ML method and dataset using a spectral biclustering algorithm Kluger et al. (2003). The methods and datasets are grouped into 40 contiguous biclusters (4 MLwise clusters by 10 data-wise clusters) in order to expose relationships between models and datasets.", "startOffset": 198, "endOffset": 219}], "year": 2017, "abstractText": "The selection, development, or comparison of machine learning methods in data mining can be a difficult task based on the target problem and goals of a particular study. Numerous publicly available real-world and simulated benchmark datasets have emerged from different sources, but their organization and adoption as standards have been inconsistent. As such, selecting and curating specific benchmarks remains an unnecessary burden on machine learning practitioners and data scientists. The present study introduces an accessible, curated, and developing public benchmark resource to facilitate identification of the strengths and weaknesses of different machine learning methodologies. We compare meta-features among the current set of benchmark datasets in this resource to characterize the diversity of available data. Finally, we apply a number of established machine learning methods to the entire benchmark suite and analyze how datasets and algorithms cluster in terms of performance. This work is an important first step towards understanding the limitations of popular benchmarking suites and developing a resource that connects existing benchmarking standards to more diverse and efficient standards in the future.", "creator": "LaTeX with hyperref package"}}}