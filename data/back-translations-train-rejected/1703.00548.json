{"id": "1703.00548", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Evolving Deep Neural Networks", "abstract": "The success of deep learning depends on finding an architecture to fit the task. As deep learning has scaled up to more challenging tasks, the architectures have become difficult to design by hand. This paper proposes an automated method, CoDeepNEAT, for optimizing deep learning architectures through evolution. By extending existing neuroevolution methods to topology, components, and hyperparameters, this method achieves results comparable to best human designs in standard benchmarks in object recognition and language modeling. It also supports building a real-world application of automated image captioning on a magazine website. Given the anticipated increases in available computing power, evolution of deep networks is promising approach to constructing deep learning applications in the future.", "histories": [["v1", "Wed, 1 Mar 2017 23:40:42 GMT  (2540kb,D)", "http://arxiv.org/abs/1703.00548v1", null], ["v2", "Sat, 4 Mar 2017 23:13:05 GMT  (2540kb,D)", "http://arxiv.org/abs/1703.00548v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["risto miikkulainen", "jason liang", "elliot meyerson", "aditya rawal", "dan fink", "olivier francon", "bala raju", "hormoz shahrzad", "arshak navruzyan", "nigel duffy", "babak hodjat"], "accepted": false, "id": "1703.00548"}, "pdf": {"name": "1703.00548.pdf", "metadata": {"source": "META", "title": "Evolving Deep Neural Networks", "authors": ["Risto Miikkulainen", "Jason Liang", "Elliot Meyerson", "Aditya Rawal", "Dan Fink", "Olivier Francon", "Bala Raju", "Arshak Navruzyan", "Nigel Du\u0082y", "Babak Hodjat"], "emails": [], "sections": [{"heading": null, "text": "KEYWORDS Neural Networks, Deep Learning, LSTMs, Bilevel Optimization, Coevolution, Design"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is a case of a reactionary party that is able to assert itself, that it is able to assert itself, that it is able to assert itself, and that it is able to assert itself."}, {"heading": "2 BACKGROUND AND RELATEDWORK", "text": "In fact, most of us are able to move to another world, to move to another world, to move to another world, to move to another world."}, {"heading": "3 EVOLUTION OF DEEP LEARNING ARCHITECTURES", "text": "The neuroevolution method NEAT [43] is first extended to the developing network topology and hyperparameters of deep neural networks in DeepNEAT and then to the co-evolution of modules and blueprints for their combination in CoDeepNEAT. e-approach is tested in the standard CIFAR-10 benchmark for object recognition and found to be comparable to the state of the art."}, {"heading": "3.1 Extending NEAT to Deep Networks", "text": "nrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "3.2 Cooperative Coevolution of Modules and Blueprints", "text": "Many of the most successful DNNs, such as GoogLeNet and ResNet, consist of modules that repeat themselves several times [20, 45]. These modules themselves have a complicated structure with branching and merging of different layers. Inspired by this observation, a variant of DeepNEAT is proposed called Coevolution DeepNEAT (CoDeepNEAT). The algorithm behind CoDeepNEAT is mainly inspired by hierarchical SANE [37], but is also influenced by component evolution approaches ESP [13] and CoSyNE [14]. In CoDeepNEAT, two populations of modules and blueplates are developed separately, using the same methods as described above for DeepNEAT. e Blueprint chromosome is a graph in which each node contains a pointer to a specific module type."}, {"heading": "3.3 Evolving DNNs in the CIFAR-10 Benchmark", "text": "In this experiment, CoDeepNEAT was used to develop the topology of a Convolutionary Neural Network (CNN) to maximize its classification performance on the CIFAR-10 dataset, a common image classification benchmark. e dataset consists of 50,000 training images and 10,000 test images. E images consist of 32x32 color pixels and belong to one of 10 classes. For comparison, the neural network layer types were limited to those used by Snoek et al. [41] in their Bayean optimization of CNN hyperparameters. Also, the following Snoek et al., data augmentation consisted of converting images from RGB to HSV color space by adding random perturbations, and crops, and converting them back to RGB color space. CoDeepNEAT was initialized with populations of 25 blueprints and 45 modules."}, {"heading": "4 EVOLUTION OF LSTM ARCHITECTURES", "text": "Recurring neural networks, especially those that use LSTM nodes, are another powerful approach for DNN. Much of the energy comes from the repetition of LSTM modules and the connectivity between them. In this section, CoDeepNEAT is expanded to include mutations that enable the search for such connectivity, and the approach is evaluated in the standard benchmark task of voice modeling."}, {"heading": "4.1 Extending CoDeepNEAT to LSTMs", "text": "Long-term short-term memory (LSTM) consists of controlled memory cells that can integrate information over longer time scales (compared to simply recurring connections in a neural network).LSTMs have recently proven to be powerful in monitored sequence processing tasks such as speech recognition [16] and machine translation [3].Recent research on LSTMs has focused on two directions: the search for variations in individual LSTM memory units [4, 7, 25, 28] and the discovery of new ways to sew LSTM layers into a network [8, 26, 53].Both approaches have improved performance over vanilla LSTMs, with the best recent results achieved through network design.The CoDeepNEAT method takes both approaches into account: neuroevolution seeks both new LSTM units and multi-layer connectivity at the same time."}, {"heading": "4.2 Evolving DNNs in the Language Modeling Benchmark", "text": "A standard benchmark task for the LSTM network is voice modeling, i.e. predicting the next word in a large body of text. The benchmark uses the Penn Tree Bank (PTB) dataset [34], which consists of 929k training words, 73k validation words, and 82k test words. It has 10k words in its vocabulary. A population of 50 LSTM networks was initialized with uniformly random initial weights within [-0.05, 0.05]. Each network consisted of two recurring layers (Vanilla LSTM or its variants) with 650 hidden nodes in each layer. The network was unrolled up to 35 steps. e hidden states were initialized to zero. e hidden states of the current minibatch was used as the initial hidden state of the subsequent minibatch (Vanilla LSTM) is the end of the training set."}, {"heading": "5 APPLICATION CASE STUDY: IMAGE CAPTIONING FOR THE BLIND", "text": "In a real-life case study, CoDeepNEAT's visual and speech capabilities were combined into a real-time online captioning system. In this application, CoDeepNEAT searches for architectures that learn to integrate image and text representations to produce captions that blind users can access via existing screen readers, and the application was implemented for a major online magazine website (web address for double-blind verification removed), and developed networks were trained with the open source captioning data set MSCOCO [6], along with a new data set collected for this website."}, {"heading": "5.1 Evolving DNNs for Image Captioning", "text": "Deep Learning has recently provided state-of-the-art captioning performance, and several different architectures have been proposed that are fully connected to an architecture [27, 46, 47, 49, 51]. Input to a captioning system is a raw image, and the output is a text caption that is intended to describe the content of the image. In deep learning approaches, a revolutionary network is usually used to process the image, and recurring units, or LSTMs, to generate coherent sentences with far-reaching dependencies. As is common in existing approaches, the developed system uses a pre-formed ImageNet model [45] to generate initial image embeddings. e evolved network takes an image embedding as input, along a one-hot text input."}, {"heading": "5.2 Building the Application", "text": "The Internet as a whole, and the online magazine's website in particular, contain many images that cannot be classified as \"common objects in context.\" Other types of images from the magazine include staged portraits of people, infographics, cartoons, abstract designs, and iconic images, i.e. images of one or more objects that can be taken out of context, such as on a white or dark background. Therefore, an additional data set of 17,000 pairs of captions was constructed for the case study, targeting iconic images, especially iconic images. Four thousand images were first scraped off the magazine's website, and 1,000 of them became iconic. de, 16,000 images that were visually similar to these 1,000 were automatically retrieved from a large image store."}, {"heading": "5.3 Image Captioning Results", "text": "Most t architecture was discovered in Generation 37 (Figure 5). Is architecture more powerful than the 1h ps: / / mty.ai / computer-vision / hand-tuned baseline [47] when trained solely on the basis of MSCOCO data (Table 3).However, a more important result is the performance of this network on the magazine's website. As there are no suitable automatic metrics for captions collected for the magazine's website (and existing metrics are very loud if there is only one caption), the captions generated by the developed model on all 3100 endured images were evaluated manually on a scale from 1 to 4 (Figure 6).Figure 7 shows some examples of good and bad captions for these images. The model is not perfect, but the results are promising."}, {"heading": "6 DISCUSSION AND FUTUREWORK", "text": "The results of this paper show that the evolutionary approach to optimizing deep neural networks is feasible: E-results are comparable hand-designed architectures in benchmark tasks, and it is possible to develop applications in the real world on the basis of this approach. It is important to note that the approach has not yet reached its full potential. It takes a few days to train each deep neural network on a state-of-the-art GPU, and over the course of evolution thousands of them need to be trained. Therefore, the results are limited by the computing power available. Interestingly, evolution tends to train networks only partially during evolution, to detect rapid learners rather than to achieve excellence. This is an interesting result in itself: evolution can be guided with goals other than simply with accuracy, including training time, execution time or storage requirements of the network. Significantly more computing resources will likely be available in the near future. Already cloud-based services such as Amazon's Web services are currently available to us at a lower cost, such as Amazon's services worldwide."}, {"heading": "7 CONCLUSION", "text": "Evolutionary optimization makes it possible to construct more complex deep-learning architectures than is possible by hand. E-topology, components, and hyperparameters of the architecture can all be optimized simultaneously to the requirements of the task, resulting in superior performance. Automated design can enable new deep-learning applications in vision, speech, speech, and other areas. Currently, such designs are comparable to the best human designs; with the expected increase in computing power, they should soon surpass them, increasing performance for good use."}], "references": [{"title": "An Application of Reinforcement Learning to Aerobatic Helicopter Flight", "author": ["Pieter Abbeel", "Adam Coates", "Morgan \u008bigley", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Autonomous Helicopter Control Using Reinforcement Learning Policy Search Methods", "author": ["James Bagnell", "Je\u0082 Schneider"], "venue": "In Proceedings of the International Conference on Robotics and Automation", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Evolving memory cell structures for sequence learning", "author": ["J. Bayer", "D. Wierstra", "J. Togelius", "J. Schmidhuber"], "venue": "In In Arti\u0080cial Neural Networks ICANN", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "author": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "venue": "CoRR abs/1606.01865", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Microso\u0089 COCO captions: Data collection and evaluation", "author": ["X. Chen", "H. Fang", "T.Y. Lin", "R. Vedantam", "S. Gupta", "P. Dollar", "C.L. Zitnick"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A uni\u0080ed architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Convolution by Evolution: Di\u0082erentiable Pa\u008aern Producing Networks", "author": ["Chrisantha Fernando", "Dylan Banarse", "Frederic Besse", "Max Jaderberg", "David Pfau", "Malcolm Reynolds", "Marc Lactot", "Daan Wierstra"], "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Neuroevolution: From Architectures to Learning", "author": ["Dario Floreano", "Peter D\u00fcrr", "Claudio Ma\u008aiussi"], "venue": "Evolutionary Intelligence", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Incremental Evolution of Complex General Behavior", "author": ["Faustino Gomez", "Risto Miikkulainen"], "venue": "Adaptive Behavior", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Solving Non-Markovian Control Tasks with Neuroevolution", "author": ["Faustino Gomez", "Risto Miikkulainen"], "venue": "In Proceedings of the 16th International Joint Conference on Arti\u0080cial Intelligence. Morgan Kaufmann,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Accelerated Neural Evolution \u008crough Cooperatively Coevolved Synapses", "author": ["Faustino Gomez", "J\u00fcrgen Schmidhuber", "Risto Miikkulainen"], "venue": "Journal of Machine Learning Research", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "GenProg: Automatic Bug Correction in Real Programs", "author": ["C. Le Goues", "T. Nguyen", "S. Forrest", "W. Weimer"], "venue": "ACM Transactions on So\u0087ware Engineering", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In In Proc", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geo\u0082rey Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Adding Learning to the Cellular Development of Neural Networks: Evolution and the Baldwin E\u0082ect", "author": ["Frederic Gruau", "Darrell Whitley"], "venue": "Evolutionary Computation", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR abs/1512.03385", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Identity Mappings in Deep Residual Networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR abs/1603.05027", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "How Learning Can Guide Evolution", "author": ["Geo\u0082rey E. Hinton", "Steven J. Nowlan"], "venue": "Complex Systems", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1987}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Programming by Optimization", "author": ["Holger Hoos"], "venue": "Commun. ACM", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Neuroevolution for Reinforcement Learning Using Evolution Strategies", "author": ["Christian Igel"], "venue": "In Proceedings of the 2003 Congress on Evolutionary Computation. IEEE Press, Piscataway,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "An empirical exploration of recurrent network architectures", "author": ["R. Jozefowicz", "W. Zaremba", "I. Sutskever"], "venue": "Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Grid Long Short-Term Memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "CoRR abs/1507.01526", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In Proc. of CVPR", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "LSTM: A search space odyssey", "author": ["G. Klaus", "R. Srivastava", "J. Koutnk", "R. Steunebrink", "J. Schmidhuber"], "venue": "arXiv preprint arxiv/1503.04069", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Neuroevolutionary reinforcement learning for generalized control of simulated helicopters", "author": ["Rogier Koppejan", "Shimon Whiteson"], "venue": "Evolutionary Intelligence", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Gradientbased learning applied to document recognition", "author": ["Yann LeCun", "Leon Bo\u008aou", "Yoshua Bengio", "Patrick Ha\u0082ner"], "venue": "Proc. IEEE", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1998}, {"title": "Evolutionary Bilevel Optimization for Complex Control Tasks", "author": ["Jason Zhi Liang", "Risto Miikkulainen"], "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks", "author": ["Ilya Loshchilov", "Frank Hu\u008aer"], "venue": "CoRR abs/1604.07269", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Building a large annotated corpus of english: \u008ce penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1993}, {"title": "Conversion Rate Optimization through Evolutionary Computation", "author": ["Risto Miikkulainen", "Neil Iscoe"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2017}, {"title": "Training Feedforward Neural Networks Using Genetic Algorithms", "author": ["David J. Montana", "Lawrence Davis"], "venue": "In Proceedings of the 11th International Joint Conference on Arti\u0080cial Intelligence", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1989}, {"title": "Forming Neural Networks \u008crough E\u0081cient and Adaptive Co-Evolution", "author": ["David E. Moriarty", "Risto Miikkulainen"], "venue": "Evolutionary Computation", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1997}, {"title": "Autonomous Helicopter Flight via Reinforcement Learning", "author": ["Andrew Y. Ng", "H. Jin Kim", "Michael Jordan", "Shankar Sastry"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}, {"title": "Beyond Short Snippets: Deep Networks for Video Classi\u0080cation", "author": ["Joe Yue-Hei Ng", "Ma\u008ahew J. Hausknecht", "Sudheendra Vijayanarasimhan", "Oriol Vinyals", "Rajat Monga", "George Toderici"], "venue": "CoRR abs/1503.08909", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "A bilevel optimization approach to automated parameter tuning", "author": ["Ankur Sinha", "Pekka Malo", "Peng Xu", "Kalyanmoy Deb"], "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Scalable Bayesian Optimization Using Deep Neural Networks", "author": ["J. Snoek", "O. Rippel", "K. Swersky", "R. Kiros", "N. Satish", "N. Sundaram", "M.M.A. Patwary", "M. Prabhat", "R.P. Adams"], "venue": "In Proc. of ICML", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Compositional Pa\u008aern Producing Networks: A Novel Abstraction of Development", "author": ["Kenneth Stanley"], "venue": "Genetic Programming and Evolvable Machines", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "Evolving Neural Networks Through Augmenting Topologies", "author": ["Kenneth O. Stanley", "Risto Miikkulainen"], "venue": "Evolutionary Computation", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2002}, {"title": "Inception-v4, inceptionresnet and the impact of residual connections on learning", "author": ["C. Szegedy", "S. Io\u0082e", "V. Vanhoucke", "A. Alemi"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Rethinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Io\u0082e", "J. Shlens", "Z. Wojna"], "venue": "In Proc. of CVPR", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "Context-aware Captions from Context-agnostic Supervision", "author": ["R. Vedantam", "S. Bengio", "K. Murphy", "D. Parikh", "G. Chechik"], "venue": "arXiv preprint arxiv/1701.02870", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2017}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In Proc. of CVPR", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Show and tell: Lessons learned from the 2015 mscoco image captioning challenge", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Trans. on Pa\u0088ern Analysis and Machine Intelligence", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}, {"title": "Show, A\u008aend and Tell: Neural Image Caption Generation with Visual A\u008aention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salkhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "In Proc. of ICML", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2015}, {"title": "Evolving Arti\u0080cial Neural Networks", "author": ["Xin Yao"], "venue": "Proc. IEEE 87,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1999}, {"title": "Image captioning with semantic a\u008aention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "In Proc. of CVPR", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arxiv/1409.2329", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2014}, {"title": "Neural Architecture Search with Reinforcement Learning", "author": ["Barret Zoph", "\u008boc V. Le"], "venue": "CoRR abs/1611.01578", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2016}], "referenceMentions": [{"referenceID": 28, "context": "convolutional neural networks [30] and recurrent neural networks (in particular long short-term memory, or LSTM [22]), which have existed since the 1990s, have improved state-of-the-art signi\u0080cantly", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "convolutional neural networks [30] and recurrent neural networks (in particular long short-term memory, or LSTM [22]), which have existed since the 1990s, have improved state-of-the-art signi\u0080cantly", "startOffset": 112, "endOffset": 116}, {"referenceID": 7, "context": "in computer vision, speech, language processing, and many other areas [9, 17, 45].", "startOffset": 70, "endOffset": 81}, {"referenceID": 15, "context": "in computer vision, speech, language processing, and many other areas [9, 17, 45].", "startOffset": 70, "endOffset": 81}, {"referenceID": 42, "context": "in computer vision, speech, language processing, and many other areas [9, 17, 45].", "startOffset": 70, "endOffset": 81}, {"referenceID": 4, "context": "hand-designed architectures on new problems [5, 20, 39, 45].", "startOffset": 44, "endOffset": 59}, {"referenceID": 18, "context": "hand-designed architectures on new problems [5, 20, 39, 45].", "startOffset": 44, "endOffset": 59}, {"referenceID": 36, "context": "hand-designed architectures on new problems [5, 20, 39, 45].", "startOffset": 44, "endOffset": 59}, {"referenceID": 42, "context": "hand-designed architectures on new problems [5, 20, 39, 45].", "startOffset": 44, "endOffset": 59}, {"referenceID": 21, "context": "stance, humans write the overall design of a so\u0089ware system, and the parameters and low-level code is optimized automatically [23]; humans write imperfect versions of programs, and evolutionary algorithms are then used to repair them [15]; humans de\u0080ne the space of possible web designs, and evolution is used to \u0080nd e\u0082ective", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": "stance, humans write the overall design of a so\u0089ware system, and the parameters and low-level code is optimized automatically [23]; humans write imperfect versions of programs, and evolutionary algorithms are then used to repair them [15]; humans de\u0080ne the space of possible web designs, and evolution is used to \u0080nd e\u0082ective", "startOffset": 234, "endOffset": 238}, {"referenceID": 32, "context": "ones [35].", "startOffset": 5, "endOffset": 9}, {"referenceID": 40, "context": "It is based on the existing neuroevolution technique of NEAT [43], which has been successful in evolving topologies and weights of relatively small recurrent networks in the past.", "startOffset": 61, "endOffset": 65}, {"referenceID": 9, "context": "Neuroevolution techniques have been applied successfully to sequential decision tasks for three decades [11, 31, 36, 50].", "startOffset": 104, "endOffset": 120}, {"referenceID": 33, "context": "Neuroevolution techniques have been applied successfully to sequential decision tasks for three decades [11, 31, 36, 50].", "startOffset": 104, "endOffset": 120}, {"referenceID": 47, "context": "Neuroevolution techniques have been applied successfully to sequential decision tasks for three decades [11, 31, 36, 50].", "startOffset": 104, "endOffset": 120}, {"referenceID": 22, "context": "CMA-ES [24], a technique for continuous optimization, works well on optimizing the ar X iv :1 70 3.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "Other approaches such as SANE, ESP, and CoSyNE evolve partial neural networks and combine them into fully functional networks [12, 14, 37].", "startOffset": 126, "endOffset": 138}, {"referenceID": 12, "context": "Other approaches such as SANE, ESP, and CoSyNE evolve partial neural networks and combine them into fully functional networks [12, 14, 37].", "startOffset": 126, "endOffset": 138}, {"referenceID": 34, "context": "Other approaches such as SANE, ESP, and CoSyNE evolve partial neural networks and combine them into fully functional networks [12, 14, 37].", "startOffset": 126, "endOffset": 138}, {"referenceID": 16, "context": "Further, techniques such as Cellular Encoding [18] and NEAT [43] have been developed to evolve the topology of the", "startOffset": 46, "endOffset": 50}, {"referenceID": 40, "context": "Further, techniques such as Cellular Encoding [18] and NEAT [43] have been developed to evolve the topology of the", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Much of the work is based on utilizing the Baldwin e\u0082ect, where learning only a\u0082ects the selection [21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "encode the learned weight changes back into the genome [18].", "startOffset": 55, "endOffset": 59}, {"referenceID": 37, "context": "Deep neuroevolution is thus more closely related to bilevel (or multilevel) optimization techniques [40].", "startOffset": 100, "endOffset": 104}, {"referenceID": 0, "context": "\u008cis is a challenging benchmark from the 2000s for which various reinforcement learning approaches have been developed [1, 2, 38].", "startOffset": 118, "endOffset": 128}, {"referenceID": 1, "context": "\u008cis is a challenging benchmark from the 2000s for which various reinforcement learning approaches have been developed [1, 2, 38].", "startOffset": 118, "endOffset": 128}, {"referenceID": 35, "context": "\u008cis is a challenging benchmark from the 2000s for which various reinforcement learning approaches have been developed [1, 2, 38].", "startOffset": 118, "endOffset": 128}, {"referenceID": 27, "context": "One of the most successful ones is single-level neuroevolution, where the helicopter is controlled by a neural network that is evolved through genetic algorithms [29].", "startOffset": 162, "endOffset": 166}, {"referenceID": 29, "context": "However, a bilevel approach, where a high-level evolutionary process is employed to optimize these parameters, can search this space more e\u0082ectively [32].", "startOffset": 149, "endOffset": 153}, {"referenceID": 30, "context": "[33] used CMA-ES to optimize the hyperparameters of existing DNNs obtaining state-of-the-art results on e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] evolved a CPPN (compositional pa\u008aern-producing network [42]) to output the weights of an auto-encoder neural network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[10] evolved a CPPN (compositional pa\u008aern-producing network [42]) to output the weights of an auto-encoder neural network.", "startOffset": 60, "endOffset": 64}, {"referenceID": 50, "context": "A related approach was proposed by Zoph and Le [54]: the topology and hyperparameters of a deep network and LSTM network were modi\u0080ed through policy iteration.", "startOffset": 47, "endOffset": 51}, {"referenceID": 40, "context": "NEAT neuroevolution method [43] is \u0080rst extended to evolving", "startOffset": 27, "endOffset": 31}, {"referenceID": 18, "context": "are composed of modules that are repeated multiple times [20, 45].", "startOffset": 57, "endOffset": 65}, {"referenceID": 42, "context": "are composed of modules that are repeated multiple times [20, 45].", "startOffset": 57, "endOffset": 65}, {"referenceID": 34, "context": "\u008ce algorithm behind CoDeepNEAT is inspired mainly by Hierarchical SANE [37] but is also in\u0083uenced by component-evolution approaches ESP [13] and CoSyNE [14].", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "\u008ce algorithm behind CoDeepNEAT is inspired mainly by Hierarchical SANE [37] but is also in\u0083uenced by component-evolution approaches ESP [13] and CoSyNE [14].", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "\u008ce algorithm behind CoDeepNEAT is inspired mainly by Hierarchical SANE [37] but is also in\u0083uenced by component-evolution approaches ESP [13] and CoSyNE [14].", "startOffset": 152, "endOffset": 156}, {"referenceID": 29, "context": "Number of Filters [32, 256] Dropout Rate [0, 0.", "startOffset": 18, "endOffset": 27}, {"referenceID": 42, "context": "99] Hue Shi\u0089 [0, 45] Saturation/Value Shi\u0089 [0, 0.", "startOffset": 13, "endOffset": 20}, {"referenceID": 24, "context": "5] Cropped Image Size [26, 32] Spatial Scaling [0, 0.", "startOffset": 22, "endOffset": 30}, {"referenceID": 29, "context": "5] Cropped Image Size [26, 32] Spatial Scaling [0, 0.", "startOffset": 22, "endOffset": 30}, {"referenceID": 38, "context": "[41] in their Bayesian optimization of CNN hyperparameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[41].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "LSTMs have recently been shown powerful in supervised sequence processing tasks such as speech recognition [16] and machine translation [3].", "startOffset": 107, "endOffset": 111}, {"referenceID": 2, "context": "LSTMs have recently been shown powerful in supervised sequence processing tasks such as speech recognition [16] and machine translation [3].", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": "variations of individual LSTM memory unit architecture [4, 7, 25, 28], and discovering new ways of stitching LSTM layers into a network [8, 26, 53].", "startOffset": 55, "endOffset": 69}, {"referenceID": 6, "context": "variations of individual LSTM memory unit architecture [4, 7, 25, 28], and discovering new ways of stitching LSTM layers into a network [8, 26, 53].", "startOffset": 55, "endOffset": 69}, {"referenceID": 23, "context": "variations of individual LSTM memory unit architecture [4, 7, 25, 28], and discovering new ways of stitching LSTM layers into a network [8, 26, 53].", "startOffset": 55, "endOffset": 69}, {"referenceID": 26, "context": "variations of individual LSTM memory unit architecture [4, 7, 25, 28], and discovering new ways of stitching LSTM layers into a network [8, 26, 53].", "startOffset": 55, "endOffset": 69}, {"referenceID": 24, "context": "variations of individual LSTM memory unit architecture [4, 7, 25, 28], and discovering new ways of stitching LSTM layers into a network [8, 26, 53].", "startOffset": 136, "endOffset": 147}, {"referenceID": 31, "context": "\u008ce benchmark utilizes the Penn Tree Bank (PTB) dataset [34], which consists of 929k training words, 73k validation words, and 82k test words.", "startOffset": 55, "endOffset": 59}, {"referenceID": 49, "context": "78) as compared to the vanilla LSTM [52].", "startOffset": 36, "endOffset": 40}, {"referenceID": 5, "context": "Evolved networks were trained with the open source MSCOCO image captioning dataset [6], along with a new dataset collected for this website.", "startOffset": 83, "endOffset": 86}, {"referenceID": 25, "context": "Deep learning has recently provided state-of-the-art performance in image captioning, and several diverse architectures have been suggested [27, 46, 47, 49, 51].", "startOffset": 140, "endOffset": 160}, {"referenceID": 43, "context": "Deep learning has recently provided state-of-the-art performance in image captioning, and several diverse architectures have been suggested [27, 46, 47, 49, 51].", "startOffset": 140, "endOffset": 160}, {"referenceID": 44, "context": "Deep learning has recently provided state-of-the-art performance in image captioning, and several diverse architectures have been suggested [27, 46, 47, 49, 51].", "startOffset": 140, "endOffset": 160}, {"referenceID": 46, "context": "Deep learning has recently provided state-of-the-art performance in image captioning, and several diverse architectures have been suggested [27, 46, 47, 49, 51].", "startOffset": 140, "endOffset": 160}, {"referenceID": 48, "context": "Deep learning has recently provided state-of-the-art performance in image captioning, and several diverse architectures have been suggested [27, 46, 47, 49, 51].", "startOffset": 140, "endOffset": 160}, {"referenceID": 42, "context": "As is common in existing approaches, the evolved system uses a pretrained ImageNet model [45] to produce initial image embeddings.", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "As usual, in training the text input contains the previous word of the ground truth caption; in inference it contains the previous word generated by the model [27, 47].", "startOffset": 159, "endOffset": 167}, {"referenceID": 44, "context": "As usual, in training the text input contains the previous word of the ground truth caption; in inference it contains the previous word generated by the model [27, 47].", "startOffset": 159, "endOffset": 167}, {"referenceID": 44, "context": "In particular, the well-known Show and Tell image captioning architecture [47] is in this search space, providing a baseline with which evolution results can be compared.", "startOffset": 74, "endOffset": 78}, {"referenceID": 5, "context": "Since there is no single best accepted metric for evaluating captions, the \u0080tness function is the mean across three metrics (BLEU, METEOR, and CIDEr; [6]) normalized by their baseline values.", "startOffset": 150, "endOffset": 153}, {"referenceID": 44, "context": "As a result, there is evolutionary pressure towards networks that converge quickly: \u008ce best resulting architectures train to near convergence 6 times faster than the baseline Show and Tell model [47].", "startOffset": 195, "endOffset": 199}, {"referenceID": 17, "context": "\u0087e motif of skip connections with a summing merge is similar to residual architectures that are currently popular in deep learning [19, 44].", "startOffset": 131, "endOffset": 139}, {"referenceID": 41, "context": "\u0087e motif of skip connections with a summing merge is similar to residual architectures that are currently popular in deep learning [19, 44].", "startOffset": 131, "endOffset": 139}, {"referenceID": 38, "context": "DNGO [41] 26.", "startOffset": 5, "endOffset": 9}, {"referenceID": 44, "context": "Baseline [47] 27.", "startOffset": 9, "endOffset": 13}, {"referenceID": 44, "context": "hand-tuned baseline [47] when trained on the MSCOCO data alone (Table 3).", "startOffset": 20, "endOffset": 24}, {"referenceID": 45, "context": "performing beam search or scheduled sampling during training [48].", "startOffset": 61, "endOffset": 65}], "year": 2017, "abstractText": "\u008ce success of deep learning depends on \u0080nding an architecture to \u0080t the task. As deep learning has scaled up to more challenging tasks, the architectures have become di\u0081cult to design by hand. \u008cis paper proposes an automated method, CoDeepNEAT, for optimizing deep learning architectures through evolution. By extending existing neuroevolution methods to topology, components, and hyperparameters, this method achieves results comparable to best human designs in standard benchmarks in object recognition and language modeling. It also supports building a real-world application of automated image captioning on a magazine website. Given the anticipated increases in available computing power, evolution of deep networks is promising approach to constructing deep learning applications in the future.", "creator": "LaTeX with hyperref package"}}}