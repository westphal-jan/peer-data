{"id": "1703.00729", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Mixing Complexity and its Applications to Neural Networks", "abstract": "We suggest analyzing neural networks through the prism of space constraints. We observe that most training algorithms applied in practice use bounded memory, which enables us to use a new notion introduced in the study of space-time tradeoffs that we call mixing complexity. This notion was devised in order to measure the (in)ability to learn using a bounded-memory algorithm. In this paper we describe how we use mixing complexity to obtain new results on what can and cannot be learned using neural networks.", "histories": [["v1", "Thu, 2 Mar 2017 11:34:38 GMT  (86kb,D)", "http://arxiv.org/abs/1703.00729v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michal moshkovitz", "naftali tishby"], "accepted": false, "id": "1703.00729"}, "pdf": {"name": "1703.00729.pdf", "metadata": {"source": "META", "title": "Mixing Complexity and its Applications to Neural Networks", "authors": ["Michal Moshkovitz", "Naftali Tishby"], "emails": ["<michal.moshkovitz@mail.huji.ac.il>,", "<tishby@cs.huji.ac.il>."], "sections": [{"heading": "1. Introduction", "text": "Understanding neural learning is an active field of research in machine learning and neuroscience (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Arora et al., 2014; Tishby & Zaslavsky, 2015; Kadmon & Sompolinsky, 2016) In this paper, we show this problem most often through another lens - that of spatial limitations. We observe that learning with neural networks, either artificial or biological, is almost always done with a bound memory algorithm. In the setting of machine learning, artificial neural networks, we often use the stochastic gradient descent (SGD) algorithm, an example of a bound memory algorithm."}, {"heading": "1.1. Paper overview", "text": "In Section 3, we show that artificial and biological neural networks each use an algorithm with limited memory; in Section 4, we define and illustrate the mixing of complexity; in Section 5, we repeat the main theorem proven in (Moshkovitz & Moshkovitz, 2017), which asserts that the limited memory algorithm cannot learn hypotheses classes that are \"mixing\"; in Section 6, we examine a number of natural classes and prove that they do not mix; i.e., an algorithm with limited memory can learn these classes; in Section 7, we describe the work done in (Zhang et al., 2017) and its connection to mixing complexity; in Section 8, we demonstrate some desirable properties of mixing complexity; Section 9 summarizes the results and leaves some open problems for future work."}, {"heading": "2. Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Learning", "text": "Learning is the process of transforming experience into expertise. A learner receives successively labeled examples (x, b) as well as examples (X \u00b7 {0, 1} as experience and after sufficient examples, the learner issues a hypothesis h: X \u2192 {0, 1}. The aim is to return h, which minimizes the test error defined as probability, a different answer from the true underlying hypothesis fL (D, f) (h) = Pr x \u0445 D [h (x) 6 = f (x)]]] The examples in the learning process are drawn independently of an unknown distribution D and an underlying hypothesis f. Definition 1 (PAC learnable, (Valiant, 1984). A hypothesis class H is learnable if there is a function mH: (0, 1) 2 \u2192 N and a learning algorithm A with the following property: For each, xi (1), for each distribution D over X, and for each underlying hypothesis."}, {"heading": "2.2. The VC-dimension Complexity", "text": "The VC dimension is a measure of the complexity of hypotheses classes, as demonstrated in the Basic Theorem of Statistical Learning. Definition 2 (constraint): Let H be a hypotheses class for binary classification over domain X. Let S = {x1,.., xn} be a series of examples. The constraint of H in S is setHS = {(h (x1),. h (xn): h (H). Definition 3 (shattering): A hypotheses class H shatters a lot of S'X if | HS | = 2 | S |. Definition 4 (VC dimension): The VC dimension of a hypotheses class H is V Cdim (H) = sup {S |: H shatters S} Theorem 5 (Fundamental Theorem of Statistical Learning)."}, {"heading": "2.3. Bounded-Memory Learning Algorithm", "text": "One approach to developing a learning algorithm is to store all the examples obtained and return a hypothesis with a minimal training error. Note that this approach does not use bound memory. However, in this paper we focus on bound storage algorithms. A bound storage algorithm is a Turing machine with a bound size, with each cell on the tape being either \"0\" or \"1.\" It is useful to imagine such an algorithm as a graph on the corners of \"2 s.\" Each vertex is a possible memory state. In each step, the algorithm is in a memory state. When we are confronted with a new example, the algorithm switches to a different memory state. In the last step, the algorithm issues a hypothesis depending on the memory state in which it ended. In this paper, we examine hypotheses classes that are unlearnable with a bound storage algorithm."}, {"heading": "3. Neural Networks and Bounded-Memory Algorithms", "text": "From the perspective of machine learning, neural networks define a class of hypotheses. The algorithm that is almost always used to find a hypothesis from this class is a bounded memory algorithm (i.e. the stochastic gradient descent algorithm), as explained below. From the perspective of neuroscience, we show that among the accepted assumptions, any calculation performed by the nervous system must be a bounded memory algorithm."}, {"heading": "3.1. Neural Networks and Machine Learning", "text": "Artificial neural networks have drastically improved the state of the art in many areas (see LeCun et al., 2015) and referenced it. However, learning a neural network is generally difficult (Blum & Rivest, 1988), which has led many researchers to attempt to understand the reasons for its astonishing success despite its proven hardness (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Livni et al., 2014). In this section we note a property of the widely used algorithm for learning neural networks: it is an algorithm of boundary memory. An advanced artificial neural consists of layers of neurons and directed edges between successive layers, known as the architecture of the neural network."}, {"heading": "3.2. Neural Networks and Neuroscience", "text": "The nervous system is responsible for processing all the information that an organism receives and acts accordingly. It consists of a large number of neurons that are interconnected and together form a biological neuronal network. Two neurons are connected by a synapse, a structure that enables the transmission of an electrical or chemical signal from one neuron (the so-called pre-synaptic neuron) to another (the so-called post-synaptic neuron).Each pre-synaptic neuron can influence the post-synaptic neuron differently depending on different biological parameters (e.g. the amount of neurotransmitter released into the synapse, the myelination).In the standard model of a neural network, each synapse that connects a pre-synaptic neuron i and a post-synaptic neuron is represented by some weight and weight changes."}, {"heading": "4. Mixing Complexity", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Hypotheses Graphs", "text": "A hypothesis class can be regarded as a split graph in which on one side there is a vertex for each hypothesis h and on the other side a vertex for each example x, and there is an edge (h, x) if and only if h (x) = 1. To illustrate this, focus on the class Hth of discrete threshold functions in [0, 1]: \u2022 the examples are the numbers X = 0 = 0 | X | - 1,.., 2 (| X | - 1 | X | - 1 = 1} \u2022 the hypotheses correspond to | X | - 1 = 1} \u2022 the hypotheses correspond to the | X | - 1 triangular number b. The number of hypotheses is equal to the hypotheses of a hypotheses that represent a hypotheses."}, {"heading": "4.2. Mixing Graphs", "text": "In Section 5, we will use this new view of the hypotheses categories as a graph to deduce the unlearnability of some classes when the learning algorithm has a limited memory. This result will apply to classes with hypotheses that are \"close\" to the randomness principle. There are many ways to be close to the randomness principle. A natural way is to use the edge distribution as discussed."}, {"heading": "5. Unlearnability of Mixing Classes with Bounded-Memory Algorithm", "text": "In this section we will present the main theories we have considered in (Moshkovitz & Moshkovitz, 2017) from the perspective of interest. (This theorem shows that the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the constants of the"}, {"heading": "5.1. The Inability of a Neural Network to Learn Most Classes", "text": "In Section 3.1, we have shown that the most commonly used training algorithm for learning a neural network is the SGD, an algorithm with limited memory. In Section 3.2, we have explained why any calculation performed by the brain (i.e. a biologically plausible calculation) must be an algorithm with limited memory. Theorem 8 presented in this section concludes that classes that are O (\u221a | X |) mixtures cannot be learned by neural networks. In Section 4, we also learn that most hypotheses classes cannot be learned by neural networks."}, {"heading": "6. What Can be Learned", "text": "In the previous section, we explained why most categories of hypotheses cannot be learned with an algorithm with limited memory, especially through a neural network. One might wonder which (and how) classes can be learned with an algorithm with limited memory. (In Section 5, for example, we cited the class H of discrete threshold functions in [0, 1], which represent a \"symmetry\" that exists in the class.) We demonstrate that such classes can be learned d-mixed with an algorithm with limited memory. In this section, we consider other natural classes that have sufficient partitions (are formally defined later, but now we think of them as \"symmetries\" that exist in the class. We then cite evidence that these classes are mixed with capital d. (empirically and theoretically) that these problems can be learned with an algorithm with limited memory."}, {"heading": "7. Rethinking Generalization to Understand Deep Learning", "text": "This year it has come to the point that it is a purely reactionary project, which is a reactionary, reactionary and reactionary party."}, {"heading": "8. Hardness and Robustness of Mixing Classes", "text": "In this section we prove that the complexity of mixing has several desirable properties. First, we prove that each class H that mixes has a VC dimension (log | H |). Since the VCDimension of each class H is at most log2 | H |, we get that such classes (up to a constant factor) are the most difficult to learn without memory limitations. Furthermore, we prove that the complexity of mixing is robust under small disturbances. That is, if a small number of labels are changed, the mixing complexity is approximately unchanged."}, {"heading": "8.1. Mixing Hypothesis Classes and VC-dimension", "text": "In this section, we focus on hypotheses categories that mix and we try to understand the hardness of learning these classes without memory limitations. (As discussed in Section 2.2, the VC dimension is used to measure the complexity of learning a class (without memory limitations). We prove that the mixture classes H have V Cdim (H) = 1 (logH) = 2 (H) = 2 (H) = 2 (H) = 2 (H) different vectors of length k. To find these k examples, we point out that the first example x1 divides the set of hypotheses into H: all hypotheses h (x1) = 1 hypotheses h (x1) = 0. The idea of the proof is that the size of the two groups is almost the same (see Claim 10). The second example x2 splits each of these groups into two again, depending on whether h (x1) or not."}, {"heading": "8.2. Small Perturbation of Mixing Classes", "text": "In our study of mixing complexity, we would like to know how a small disturbance of a class can change the mixing property. Specifically, we would like to know whether a small change to a class that is d mixing with a small d is a d mixing with a small d. The next claim answers this question.Claim 12. If a hypothesis is class H, then by changing the labeling of at most b examples, the class d + \u221a b mixing results. Proof. For each T'H, S'X by e \"(S, T) the number of edges between S and T in the hypotheses graphically changes the b designations. Fix T'H, S'X by | = s and | T | = t. Our goal is to show that the left side b's (S, T) \u2212 st2 that the left side b's (S, T) and the left side b's (S, T) and the left side s (S, T) \u2212 stst."}, {"heading": "9. Conclusions and Open Problems", "text": "One possible explanation for this apparent contradiction is that the data processed by neural networks in practice might not generally be the same. Another possible explanation is that in many artificial neural network applications the same example is repeated many times. Therefore, it would be interesting to generalize the results in this data acquisition setting (Raz, 2017; Moshkovitz & Moshkovitz, 2017). Another possible explanation is that problems of interest do not mix and have a lot of \"structure.\" We suggested to use the notion of sufficient partitions to formalize the notion of \"structure.\" We showed that classes that have such a partition do not mix. It would be interesting to prove that these classes can be learned with this hypothesis that these classes are the most difficult to learn their algorithms."}, {"heading": "Acknowledgements", "text": "This work is supported in part by the Gatsby Charitable Foundation, the Israel Science Foundation, and the Intel ICRICI Center. M.M. is grateful to the Harry and Sylvia Hoffman Leadership and Responsibility Program."}], "references": [{"title": "Training a 3-node neural network is NP-complete", "author": ["Blum", "Avrim", "Rivest", "Ronald L"], "venue": "In Proceedings of the 1st International Conference on Neural Information Processing Systems,", "citeRegEx": "Blum et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Blum et al\\.", "year": 1988}, {"title": "Stability and generalization", "author": ["Bousquet", "Olivier", "Elisseeff", "Andr\u00e9"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bousquet et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bousquet et al\\.", "year": 2002}, {"title": "Group equivariant convolutional networks", "author": ["Cohen", "Taco S", "Welling", "Max"], "venue": "In ICML,", "citeRegEx": "Cohen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity", "author": ["Daniely", "Amit", "Frostig", "Roy", "Singer", "Yoram"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Daniely et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2016}, {"title": "Exploiting cyclic symmetry in convolutional neural networks", "author": ["Dieleman", "Sander", "De Fauw", "Jeffrey", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1602.02660,", "citeRegEx": "Dieleman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2016}, {"title": "The power of depth for feedforward neural networks", "author": ["Eldan", "Ronen", "Shamir", "Ohad"], "venue": "In COLT,", "citeRegEx": "Eldan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Eldan et al\\.", "year": 2016}, {"title": "Deep symmetry networks", "author": ["Gens", "Robert", "Domingos", "Pedro M"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Gens et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gens et al\\.", "year": 2014}, {"title": "Rigorous learning curve bounds from statistical mechanics", "author": ["Haussler", "David", "Kearns", "Michael", "Seung", "H Sebastian", "Tishby", "Naftali"], "venue": "Machine Learning,", "citeRegEx": "Haussler et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Haussler et al\\.", "year": 1996}, {"title": "Optimal architectures in a solvable model of deep networks", "author": ["Kadmon", "Jonathan", "Sompolinsky", "Haim"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kadmon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadmon et al\\.", "year": 2016}, {"title": "Algorithmic stability and sanity-check bounds for leave-one-out cross-validation", "author": ["Kearns", "Michael", "Ron", "Dana"], "venue": "Neural computation,", "citeRegEx": "Kearns et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1999}, {"title": "Time-space hardness of learning sparse parities", "author": ["G. Kol", "R. Raz", "A. Tal"], "venue": "Electronic Colloquium on Computational Complexity (ECCC),", "citeRegEx": "Kol et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kol et al\\.", "year": 2016}, {"title": "Pseudo-random graphs. In More sets, graphs and numbers, pp. 199\u2013262", "author": ["Krivelevich", "Michael", "Sudakov", "Benny"], "venue": null, "citeRegEx": "Krivelevich et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Krivelevich et al\\.", "year": 2006}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Computable shell decomposition bounds", "author": ["Langford", "John", "McAllester", "David A"], "venue": "In COLT, pp", "citeRegEx": "Langford et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2000}, {"title": "On the computational efficiency of training neural networks", "author": ["Livni", "Roi", "Shalev-Shwartz", "Shai", "Shamir", "Ohad"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Livni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Livni et al\\.", "year": 2014}, {"title": "Mixing implies lower bounds for space bounded learning", "author": ["Moshkovitz", "Dana", "Michal"], "venue": "ECCC,", "citeRegEx": "Moshkovitz et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Moshkovitz et al\\.", "year": 2017}, {"title": "On the expressive power of deep neural networks", "author": ["Raghu", "Maithra", "Poole", "Ben", "Kleinberg", "Jon", "Ganguli", "Surya", "Sohl-Dickstein", "Jascha"], "venue": "arXiv preprint arXiv:1606.05336,", "citeRegEx": "Raghu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Raghu et al\\.", "year": 2016}, {"title": "Fast learning requires good memory: A timespace lower bound for parity learning", "author": ["Raz", "Ran"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Raz and Ran.,? \\Q2016\\E", "shortCiteRegEx": "Raz and Ran.", "year": 2016}, {"title": "A time-space lower bound for a large class of learning problems", "author": ["Raz", "Ran"], "venue": "ECCC,", "citeRegEx": "Raz and Ran.,? \\Q2017\\E", "shortCiteRegEx": "Raz and Ran.", "year": 2017}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["Rosenblatt", "Frank"], "venue": "Psychological review,", "citeRegEx": "Rosenblatt and Frank.,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt and Frank.", "year": 1958}, {"title": "Depth separation in relu networks for approximating smooth non-linear functions", "author": ["Safran", "Itay", "Shamir", "Ohad"], "venue": "arXiv preprint arXiv:1610.09887,", "citeRegEx": "Safran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Safran et al\\.", "year": 2016}, {"title": "Understanding machine learning: From theory to algorithms", "author": ["Shalev-Shwartz", "Shai", "Ben-David"], "venue": "Cambridge university press,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2014}, {"title": "Fundamental limits of online and distributed algorithms for statistical learning and estimation", "author": ["O. Shamir"], "venue": "In Proceedings of the 27th International Conference on Neural Information Processing Systems,", "citeRegEx": "Shamir,? \\Q2014\\E", "shortCiteRegEx": "Shamir", "year": 2014}, {"title": "Distribution-specific hardness of learning neural networks", "author": ["Shamir", "Ohad"], "venue": "arXiv preprint arXiv:1609.01037,", "citeRegEx": "Shamir and Ohad.,? \\Q2016\\E", "shortCiteRegEx": "Shamir and Ohad.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Deep learning and the information bottleneck principle", "author": ["Tishby", "Naftali", "Zaslavsky", "Noga"], "venue": "In Information Theory Workshop (ITW),", "citeRegEx": "Tishby et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tishby et al\\.", "year": 2015}, {"title": "A theory of the learnable", "author": ["Valiant", "Leslie G"], "venue": "Communications of the ACM,", "citeRegEx": "Valiant and G.,? \\Q1984\\E", "shortCiteRegEx": "Valiant and G.", "year": 1984}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Zhang", "Chiyuan", "Bengio", "Samy", "Hardt", "Moritz", "Recht", "Benjamin", "Vinyals", "Oriol"], "venue": "In Internation Conference on Learning Representations (oral presentation),", "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 3, "context": "Understanding neural network learning is an active research area in machine learning and neuroscience (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Livni et al., 2014; Tishby & Zaslavsky, 2015; Kadmon & Sompolinsky, 2016).", "startOffset": 102, "endOffset": 297}, {"referenceID": 17, "context": "Understanding neural network learning is an active research area in machine learning and neuroscience (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Livni et al., 2014; Tishby & Zaslavsky, 2015; Kadmon & Sompolinsky, 2016).", "startOffset": 102, "endOffset": 297}, {"referenceID": 15, "context": "Understanding neural network learning is an active research area in machine learning and neuroscience (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Livni et al., 2014; Tishby & Zaslavsky, 2015; Kadmon & Sompolinsky, 2016).", "startOffset": 102, "endOffset": 297}, {"referenceID": 23, "context": "In recent years, several works have shown that under memory constraints numerous examples are needed in order to learn certain hypothesis classes (Shamir, 2014; Raz, 2016; Kol et al., 2016; Moshkovitz & Moshkovitz, 2017; Raz, 2017).", "startOffset": 146, "endOffset": 231}, {"referenceID": 10, "context": "In recent years, several works have shown that under memory constraints numerous examples are needed in order to learn certain hypothesis classes (Shamir, 2014; Raz, 2016; Kol et al., 2016; Moshkovitz & Moshkovitz, 2017; Raz, 2017).", "startOffset": 146, "endOffset": 231}, {"referenceID": 13, "context": "The above might seem to contradict the fact that, empirically, neural networks do indeed learn (Krizhevsky et al., 2012; LeCun et al., 2015).", "startOffset": 95, "endOffset": 140}, {"referenceID": 28, "context": "In addition we discuss an application of mixing complexity as a response to a question raised by (Zhang et al., 2017).", "startOffset": 97, "endOffset": 117}, {"referenceID": 28, "context": "In Section 7 we describe the work done in (Zhang et al., 2017) and its connection to mixing complexity.", "startOffset": 42, "endOffset": 62}, {"referenceID": 3, "context": "This has led many researchers to attempt to understand the reasons for the astonishing success despite the proven hardness (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Livni et al., 2014).", "startOffset": 123, "endOffset": 264}, {"referenceID": 17, "context": "This has led many researchers to attempt to understand the reasons for the astonishing success despite the proven hardness (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Livni et al., 2014).", "startOffset": 123, "endOffset": 264}, {"referenceID": 15, "context": "This has led many researchers to attempt to understand the reasons for the astonishing success despite the proven hardness (Shamir, 2016; Safran & Shamir, 2016; Eldan & Shamir, 2016; Daniely et al., 2016; Raghu et al., 2016; Arora et al., 2014; Livni et al., 2014).", "startOffset": 123, "endOffset": 264}, {"referenceID": 13, "context": "It is a powerful model to solve problems in machine learning and computer vision (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; Szegedy et al., 2015).", "startOffset": 81, "endOffset": 156}, {"referenceID": 4, "context": "Recently this idea has been generalized to other symmetries (Gens & Domingos, 2014; Dieleman et al., 2016; Cohen & Welling, 2016).", "startOffset": 60, "endOffset": 129}, {"referenceID": 28, "context": "In this section we suggest an answer to the open problem presented in (Zhang et al., 2017):", "startOffset": 70, "endOffset": 90}, {"referenceID": 28, "context": "Elegantly (Zhang et al., 2017) put their finger on a tremendous gap in current research on deep learning: the inability to know when the test and training errors are close.", "startOffset": 10, "endOffset": 30}, {"referenceID": 28, "context": "These authors (Zhang et al., 2017) illustrated this gap nicely in the following set of experiments.", "startOffset": 14, "endOffset": 34}, {"referenceID": 28, "context": "Unlike other measures mentioned in (Zhang et al., 2017), the mixing complexity is able to distinguish between a random hypothesis class and class with natural images.", "startOffset": 35, "endOffset": 55}, {"referenceID": 13, "context": ", (Krizhevsky et al., 2012; Gens & Domingos, 2014; Dieleman et al., 2016; Cohen & Welling, 2016))", "startOffset": 2, "endOffset": 96}, {"referenceID": 4, "context": ", (Krizhevsky et al., 2012; Gens & Domingos, 2014; Dieleman et al., 2016; Cohen & Welling, 2016))", "startOffset": 2, "endOffset": 96}, {"referenceID": 28, "context": "However, in (Zhang et al., 2017), there was excessive use of the training data; i.", "startOffset": 12, "endOffset": 32}, {"referenceID": 28, "context": "This begs the question as to whether the proof of (Raz, 2017; Moshkovitz & Moshkovitz, 2017) can be generalized to the setting in (Zhang et al., 2017) as well.", "startOffset": 130, "endOffset": 150}, {"referenceID": 7, "context": "In (Haussler et al., 1996; Langford & McAllester, 2000) it was suggested that VC-dimension is too crude to be a measure of the number of examples needed to learn.", "startOffset": 3, "endOffset": 55}], "year": 2017, "abstractText": "We suggest analyzing neural networks through the prism of space constraints. We observe that most training algorithms applied in practice use bounded memory, which enables us to use a new notion introduced in the study of spacetime tradeoffs that we call mixing complexity. This notion was devised in order to measure the (in)ability to learn using a bounded-memory algorithm. In this paper we describe how we use mixing complexity to obtain new results on what can and cannot be learned using neural networks.", "creator": "LaTeX with hyperref package"}}}