{"id": "1703.00782", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Lock-Free Parallel Perceptron for Graph-based Dependency Parsing", "abstract": "Dependency parsing is an important NLP task. A popular approach for dependency parsing is structured perceptron. Still, graph-based dependency parsing has the time complexity of $O(n^3)$, and it suffers from slow training. To deal with this problem, we propose a parallel algorithm called parallel perceptron. The parallel algorithm can make full use of a multi-core computer which saves a lot of training time. Based on experiments we observe that dependency parsing with parallel perceptron can achieve 8-fold faster training speed than traditional structured perceptron methods when using 10 threads, and with no loss at all in accuracy.", "histories": [["v1", "Thu, 2 Mar 2017 13:49:23 GMT  (29kb)", "http://arxiv.org/abs/1703.00782v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xu sun", "shuming ma"], "accepted": false, "id": "1703.00782"}, "pdf": {"name": "1703.00782.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["shumingma}@pku.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.00 782v 1 [cs.C L] 2M ar2 017task. A popular approach to parsing dependencies is the structured perceptron. However, graph-based parsing of dependencies has the temporal complexity of O (n3) and suffers from slow training. To solve this problem, we propose a parallel algorithm called parallel perceptron. The parallel algorithm can take full advantage of a multicore computer, which saves a lot of training time. Based on experiments, we observe that parsing dependencies with parallel perceptron can achieve an eight times faster training speed than conventional structured perceptron methods when using 10 threads, without any loss of accuracy."}, {"heading": "1 Introduction", "text": "Previous researchers have proposed several models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006).Although structured perceptron is one of the most popular approaches to graph-based dependency analysis, it is first applied to dependency sparing by Collins (2002) and McDonald et al. (2005).McDonald's model is decoded using an efficient algorithm proposed by Eisner (1996) and they trained the structured perceptron model and its variant Margin Infused Relaxed Algorithm al. (MIRA) initially on dependency sparsence.McDonald's model is decoded using an efficient algorithm proposed by Eisner (1996) and they trained the structured perceptron model and its variant Margin Infused Relaxed Algorithm."}, {"heading": "2 Lock-Free Parallel Perceptron for Dependency Parsing", "text": "The dataset can be called {(xi, yi)} n i = 1, while xi is the input point and yi is the correct output. GEN is a function that enumerates a series of candidates GEN (x) for input x. \u03a6 (x, y) is the characteristic vector corresponding to the input pair (x, y). Finally, the parameter vector is called \u03b1.In the structured perceptron, the score of an input pair is calculated as follows: s (x, y) = \u03a6 (x, y) \u00b7 \u03b1 (1) The output of the structured perceptron is the generation of the structure y \u2032 with the highest score in the candidate set GEN (x). In the dependency parse, the input x is a sentence, while the output y is a dependency tree. An edge is called (i, j) with a head i and its child j. Each edge has a character representation that can be written as Epi, j and izi."}, {"heading": "3 Convergence Analysis of Lock-Free Parallel Perceptron", "text": "For completion-free parallel learning, it is very important to analyze the convergence characteristics, because in most cases non-completion leads to a divergence of training (i.e., the training fails).Here, we prove that non-completion parallel perception is convergent even under the worst assumption.The challenge is that several threads can simultaneously update and override the parameter vector, so we have to prove convergence. We follow the definition in Collins's work (Collins, 2002).We write GEN (x) as all false candidates generated by input x. We define that a training example is separable with the margin \u03b4 > 0 if U is convergent (x), U \u00b7 \u0445 (x, y) \u2212 delay (x, z)."}, {"heading": "3.1 Worst Case Convergence", "text": "Suppose we have k threads and use j threads to mark the j-th thread, then each thread updates the parameter vector as follows: y threads = argmax z threads (x) \u0445j (x, y) \u00b7 \u03b1 (6) Let us remember that the update is as follows: \u03b1i + 1 = \u03b1i + \u03a6j (x, y) \u2212 \u03a6j (x, y \"j) (7) Here, y threads (x, y) both correspond to the jth thread, whereas \u03b1i is the parameter vector after the ith timestample.Since we use a loose-free parallel setting, we assume that there are k perceptron updates in parallel in each timestamp. Then, after a time step, the total parameters are updated as follows: \u03b1t + 1 = \u03b1t + k + k + k + k threads j = 1 (x, y) \u2212 p threads that we have the same threads (j-j),\" x-j-8 threads (x \u2212 x)."}, {"heading": "3.2 Optimal Case Convergence", "text": "In practice, the worst-case scenario of an extremely delayed update is probably not, or at least not always, the case. Therefore, we expect the real convergence rate to be much faster than this worst-case limit. The optimal limit is: t \u2264 R2 / (k\u03b42) (13) This limit is derived if the parallel update is not delayed (i.e. the update of each thread is based on a current parameter vector). As we can see, in the optimal case, we can become k times faster by using k-threads for a block-free parallel perceptron training, which can achieve the full acceleration of the training through parallel learning."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "After previous work, we use the English Penn TreeBank (PTB) (Marcus et al., 1993) to evaluate our proposed approach. We follow the standard division of the corpus, using sections 2-21 as the training kit, section 22 as the development kit and section 23 as the final test kit. We implement two popular models of graph-based dependency analysis: the first-order model and the second-order model. We coordinate all the hyperparameters of the development kit. The features of our model are found in McDonald et al. (2005; 2006). Our principles are traditional perceptron, MST parser (McDonald et al., 2005) 1 and the locked version parallel perceptron. All experiments are performed on a computer with the Intel (R) Xeon (R) 3.0 GHz CPU."}, {"heading": "4.2 Results", "text": "Table 2 shows that our non-blocking method is eight times faster than the baseline system, which is better speed compared to locked parallel perceptrons. In both the real 1www.seas.upenn.edu / s-trctlrn / MSTParser / MSTParser.htmlworld applications, the results show that the locked parallel pectron in the real 1www.seas.upenn.edu / s-trctlrn / MSTParser / MSTParser.htmlworld applications are close to the optimal case theory analysis of low delays, rather than the theoretical analysis of high delays in the worst case. The experimental results of accuracy are in Table 1. The base line MSTParser (McDonald et al., 2005) is a popular system for parsing dependencies. Table 1 shows that our 10-thread method outperforms the system with only one thread. Our non-blocking system is slightly better than the MST parser system, mainly because we use more distance-based features, including greater distance-loss on the first and the second-line features."}, {"heading": "5 Conclusions", "text": "Our experiments show that when using 10 running threads, it can be more than 8 times faster than the baseline without loss of accuracy. We have also performed convergence analyses for non-blocking parallel perceptrons and show that they are convergent in the loose-free learning environment. Non-blocking parallel perceptrons can be used directly for other structured prediction NLP tasks."}, {"heading": "6 Acknowledgements", "text": "This work was partially supported by the National Natural Science Foundation of China (No. 61673028) and the National High Technology Research and Development Program of China (No. 863 Program, No. 2015AA015404)."}], "references": [{"title": "Very high accuracy and fast dependency parsing is not a contradiction", "author": ["Bernd Bohnet."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics. pages 89\u201397.", "citeRegEx": "Bohnet.,? 2010", "shortCiteRegEx": "Bohnet.", "year": 2010}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins."], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Associa-", "citeRegEx": "Collins.,? 2002", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["Koby Crammer", "Yoram Singer."], "venue": "The Journal of Machine Learning Research 2:265\u2013292.", "citeRegEx": "Crammer and Singer.,? 2002", "shortCiteRegEx": "Crammer and Singer.", "year": 2002}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason Eisner."], "venue": "Proceedings of the 16th conference on Computational linguistics. pages 340\u2013345.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "Efficient thirdorder dependency parsers", "author": ["Terry Koo", "Michael Collins."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. pages 1\u201311.", "citeRegEx": "Koo and Collins.,? 2010", "shortCiteRegEx": "Koo and Collins.", "year": 2010}, {"title": "The insideoutside recursive neural network model for dependency parsing", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 729\u2013739.", "citeRegEx": "Le and Zuidema.,? 2014", "shortCiteRegEx": "Le and Zuidema.", "year": 2014}, {"title": "Building a large annotated", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Online large-margin training of dependency parsers", "author": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."], "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics. pages 91\u201398.", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Distributed training strategies for the structured perceptron", "author": ["Ryan McDonald", "Keith Hall", "Gideon Mann."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "McDonald et al\\.,? 2010", "shortCiteRegEx": "McDonald et al\\.", "year": 2010}, {"title": "Online learning of approximate dependency parsing algorithms", "author": ["Ryan T. McDonald", "Fernando C.N. Pereira."], "venue": "11st Conference of the European Chapter of the Association for Computational Linguistics.", "citeRegEx": "McDonald and Pereira.,? 2006", "shortCiteRegEx": "McDonald and Pereira.", "year": 2006}, {"title": "Towards shockingly easy structured classification: A search-based probabilistic online learning framework", "author": ["Xu Sun."], "venue": "Technical report, arXiv:1503.08381 .", "citeRegEx": "Sun.,? 2015", "shortCiteRegEx": "Sun.", "year": 2015}, {"title": "Asynchronous parallel learning for neural networks and structured models with dense features", "author": ["Xu Sun."], "venue": "COLING.", "citeRegEx": "Sun.,? 2016", "shortCiteRegEx": "Sun.", "year": 2016}, {"title": "Latent structured perceptrons for large-scale learning with hidden information", "author": ["Xu Sun", "Takuya Matsuzaki", "Wenjie Li."], "venue": "IEEE Trans. Knowl. Data Eng. 25(9):2063\u20132075.", "citeRegEx": "Sun et al\\.,? 2013", "shortCiteRegEx": "Sun et al\\.", "year": 2013}, {"title": "Latent variable perceptron algorithm for structured classification", "author": ["Xu Sun", "Takuya Matsuzaki", "Daisuke Okanohara", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "Sun et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2009}, {"title": "Maxmargin parsing", "author": ["Ben Taskar", "Dan Klein", "Michael Collins", "Daphne Koller", "Christopher D Manning."], "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP). volume 1, page 3.", "citeRegEx": "Taskar et al\\.,? 2004", "shortCiteRegEx": "Taskar et al\\.", "year": 2004}, {"title": "A latent variable model for generative dependency parsing", "author": ["Ivan Titov", "James Henderson."], "venue": "Proceedings of the 10th International Conference on Parsing Technologies, pages 144\u2013155.", "citeRegEx": "Titov and Henderson.,? 2007", "shortCiteRegEx": "Titov and Henderson.", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006).", "startOffset": 74, "endOffset": 116}, {"referenceID": 10, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006).", "startOffset": 74, "endOffset": 116}, {"referenceID": 3, "context": "The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004).", "startOffset": 204, "endOffset": 251}, {"referenceID": 15, "context": "The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004).", "startOffset": 204, "endOffset": 251}, {"referenceID": 12, "context": "However, those deep learning methods are very slow during training (Sun, 2016).", "startOffset": 67, "endOffset": 78}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al.", "startOffset": 75, "endOffset": 253}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing.", "startOffset": 75, "endOffset": 280}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing. The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al.", "startOffset": 75, "endOffset": 407}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing. The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004). It proves that MIRA and structured perceptron are effective algorithms for graph-based dependency parsing. McDonald and Pereira (2006) extended it to a second-order model while Koo and Collins (2010) developed a third-order model.", "startOffset": 75, "endOffset": 708}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing. The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004). It proves that MIRA and structured perceptron are effective algorithms for graph-based dependency parsing. McDonald and Pereira (2006) extended it to a second-order model while Koo and Collins (2010) developed a third-order model.", "startOffset": 75, "endOffset": 773}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing. The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004). It proves that MIRA and structured perceptron are effective algorithms for graph-based dependency parsing. McDonald and Pereira (2006) extended it to a second-order model while Koo and Collins (2010) developed a third-order model. They all used perceptron style methods to learn the parameters. Recently, many models applied deep learning to dependency parsing. Titov and Henderson (2007) first proposed a neural network model for transition-based dependency parsing.", "startOffset": 75, "endOffset": 962}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing. The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004). It proves that MIRA and structured perceptron are effective algorithms for graph-based dependency parsing. McDonald and Pereira (2006) extended it to a second-order model while Koo and Collins (2010) developed a third-order model. They all used perceptron style methods to learn the parameters. Recently, many models applied deep learning to dependency parsing. Titov and Henderson (2007) first proposed a neural network model for transition-based dependency parsing. Chen and Manning (2014) improved the performance of neural network dependency parsing algorithm while Le and Zuidema (2014) improved the parser with Inside-Outside Recursive Neural Network.", "startOffset": 75, "endOffset": 1065}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing. The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004). It proves that MIRA and structured perceptron are effective algorithms for graph-based dependency parsing. McDonald and Pereira (2006) extended it to a second-order model while Koo and Collins (2010) developed a third-order model. They all used perceptron style methods to learn the parameters. Recently, many models applied deep learning to dependency parsing. Titov and Henderson (2007) first proposed a neural network model for transition-based dependency parsing. Chen and Manning (2014) improved the performance of neural network dependency parsing algorithm while Le and Zuidema (2014) improved the parser with Inside-Outside Recursive Neural Network.", "startOffset": 75, "endOffset": 1165}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing. The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004). It proves that MIRA and structured perceptron are effective algorithms for graph-based dependency parsing. McDonald and Pereira (2006) extended it to a second-order model while Koo and Collins (2010) developed a third-order model. They all used perceptron style methods to learn the parameters. Recently, many models applied deep learning to dependency parsing. Titov and Henderson (2007) first proposed a neural network model for transition-based dependency parsing. Chen and Manning (2014) improved the performance of neural network dependency parsing algorithm while Le and Zuidema (2014) improved the parser with Inside-Outside Recursive Neural Network. However, those deep learning methods are very slow during training (Sun, 2016). To address those issues, we hope to implement a simple and very fast dependency parser, which can at the same time achieve state-of-the-art accuracies. To reach this target, we propose a lock-free parallel algorithm called lock-free parallel perceptron. We use lock-free parallel perceptron to train the parameters for dependency parsing. Although lots of studies implemented perceptron for dependency parsing, rare studies try to implement lockfree parallel algorithms. McDonald et al. (2010) proposed a distributed perceptron algorithm.", "startOffset": 75, "endOffset": 1805}, {"referenceID": 11, "context": "The proposed lock-free parallel perceptron is a variant of structured perceptron (Sun et al., 2009, 2013; Sun, 2015).", "startOffset": 81, "endOffset": 116}, {"referenceID": 8, "context": "This is substantially different compared with the setting of McDonald et al. (2010), in which it is not lock-free parallel learning.", "startOffset": 61, "endOffset": 84}, {"referenceID": 2, "context": "We follow the definition in Collins\u2019s work (Collins, 2002).", "startOffset": 43, "endOffset": 58}, {"referenceID": 2, "context": "where \u03b4 is the separable margin of data, following the same definition of Collins (2002). Since the initial parameter \u03b1 = 0, we will have that U \u00b7 \u03b1t+1 \u2265 tk\u03b4 after t time steps.", "startOffset": 74, "endOffset": 89}, {"referenceID": 2, "context": "where R is the same definition following Collins (2002) such that \u03a6(x, y) \u2212 \u03a6(x, y j) \u2264 R.", "startOffset": 41, "endOffset": 56}, {"referenceID": 7, "context": "Following prior work, we use English Penn TreeBank (PTB) (Marcus et al., 1993) to evaluate our", "startOffset": 57, "endOffset": 78}, {"referenceID": 8, "context": "Our baselines are traditional perceptron, MST-Parser (McDonald et al., 2005), and the locked version of parallel perceptron.", "startOffset": 53, "endOffset": 76}, {"referenceID": 8, "context": "The baseline MSTParser (McDonald et al., 2005) is a popular system for dependency parsing.", "startOffset": 23, "endOffset": 46}], "year": 2017, "abstractText": "Dependency parsing is an important NLP task. A popular approach for dependency parsing is structured perceptron. Still, graph-based dependency parsing has the time complexity of O(n3), and it suffers from slow training. To deal with this problem, we propose a parallel algorithm called parallel perceptron. The parallel algorithm can make full use of a multicore computer which saves a lot of training time. Based on experiments we observe that dependency parsing with parallel perceptron can achieve 8-fold faster training speed than traditional structured perceptron methods when using 10 threads, and with no loss at all in accuracy.", "creator": "LaTeX with hyperref package"}}}