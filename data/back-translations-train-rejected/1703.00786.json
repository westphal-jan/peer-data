{"id": "1703.00786", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "A Generic Online Parallel Learning Framework for Large Margin Models", "abstract": "To speed up the training process, many existing systems use parallel technology for online learning algorithms. However, most research mainly focus on stochastic gradient descent (SGD) instead of other algorithms. We propose a generic online parallel learning framework for large margin models, and also analyze our framework on popular large margin algorithms, including MIRA and Structured Perceptron. Our framework is lock-free and easy to implement on existing systems. Experiments show that systems with our framework can gain near linear speed up by increasing running threads, and with no loss in accuracy.", "histories": [["v1", "Thu, 2 Mar 2017 13:52:47 GMT  (73kb)", "http://arxiv.org/abs/1703.00786v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["shuming ma", "xu sun"], "accepted": false, "id": "1703.00786"}, "pdf": {"name": "1703.00786.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["xusun}@pku.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.00 786v 1 [cs.C L] 2M ar2 017To speed up the training process, many existing systems use parallel technology for online learning algorithms. However, most research mainly focuses on stochastic gradient descent (SGD) rather than other algorithms. We propose a general online framework for parallel learning for large margin models and also analyze our framework using popular algorithms for large margins, including MIRA and Structured Perceptron. Our framework is latch-free and easy to implement on existing systems. Experiments show that systems using our framework can gain near-linear speed by increasing the runtime of threads without loss of accuracy."}, {"heading": "1 Introduction", "text": "However, the algorithms can still suffer from slower training time if the training examples are extremely massive, the weight vector is large, or the inference process is slow. With parallel algorithms, we can make better use of our multi-core machine and reduce the time cost of the training process. Unfortunately, most studies on parallel algorithms focus mainly on SGD. Recht et.al (2011) initially proposed a lockless parallel SGD algorithm called HOGWILD. It is a simple and effective algorithm that does not exceed parallel algorithms in the order of magnitude. Lian et.al (2015) offers a theoretical analysis of asynchronous parallel SGD systems for non-convective optimization and a more precise description for lock-free implementation on the shared storage system. Zinkevich et.al (2010) proposed a parallel algorithm for multi-machine called Stoally."}, {"heading": "2 Generic Parallel Learning Framework", "text": "In fact, it is the case that they will be able to be in the position in which they are, and in which they will be able to be in the position in which they are."}, {"heading": "3 Large Margin Models", "text": "In this section, we will present some popular large-margin algorithms and analyze their performance in our parallel framework."}, {"heading": "3.1 MIRA", "text": "Crammer and Singer (2003) developed a large margin algorithm called MIRA, which was later extended by Taskar et.al (?), an algorithm widely used in many popular models (McDonald et al., 2005), which attempts to minimize the gap between the output score s (x, z) and the correct score s (x, y) in such a way that the gap between the output score s (x, y) and the correct score s (x, y) is greater than the loss of the output structure: Minimize the loss of the output structure."}, {"heading": "3.2 Structured Perceptron", "text": "Structured perceptron is first proposed by Collins (2002) and proves to be an effective and efficient structured prediction algorithm with convergence guarantee for separable data (Sun et al., 2009, 2013; Sun, 2015). We refer to the binary representation of sequence x and structure y as f (x, y). The set of structural candidates for input sequence x is called GEN (x). Structured perceptron searches the space of GEN (x) and finds the output z with the highest score f (x, z) \u00b7 w. The weight vector w is then updated with the output. In our parallel framework, the inference and updating of various samples runs in parallel. Inference and updating for structured perceptron can be described as z = argmaxt: GEN (x) f (x, t) \u00b7 w (1) w = w + f (x, y) \u2212 f (2)."}, {"heading": "4 Experiments", "text": "We compare our parallel framework and non-parallel algorithms for large margins across multiple benchmark datasets."}, {"heading": "4.1 Experiment Tasks", "text": "After the previous work (Collins, 2002), we derive the data set from the Penn Wall Street Journal Treebank (Marcus et al., 1993). We use sections 0-18 of the tree bank as a training set, while sections 19-21 are development set and sections 22-24 are test set. The selected feature includes unigrams and bigrams of adjacent words as well as lexical patterns of current words (Tsuruoka et al., 2011). We report on the accuracy of the output date as a benchmark. Phrase Chunking (Chunking): In the phrase chunking task, we highlight the words in the B, I or O sequence to identify the noun phrases. The data set is extracted from the CoNLL-2000 model to detect ambiguities."}, {"heading": "4.2 Experiment Setting", "text": "We implement our parallel framework using large margin algorithms, including structured perceptron and MIRA, on the above benchmark datasets. We use a development set to adjust the learning rate \u03b10 and the L2 regularization. The final learning rate \u03b10 is set to 0.02, 0.05, 0.005 for the above three tasks, and the L2 regularization is 1, 0.5, 5. We implement a one-thread framework as a starting point. The baseline definition is completely identical to our proposed framework. To achieve fair comm parameters, we also intersect the baseline parameters. We run our parallel framework up to 10 threads after previous work (Recht et al., 2011). We compare our parallel framework with the baseline in terms of accuracy / F score and time cost. Experiments are performed on a computer with Intel (R) Xeon (R) 3.0 GHz PU."}, {"heading": "4.3 Experiment Result", "text": "Figure 1 shows that our parallel algorithm can gain near-linear speed. At 10 threads, our framework delivers a speed four to six times faster than that of a single thread. Table 2 shows the acceleration in our benchmark records of 1.4 and 10 threads. Figure 2 also shows that our parallel framework has no loss of accuracy / F score or convergence rate compared to the single-thread baseline. Table 1 shows that our framework does not harm large margin algorithms because the difference in results is very small. In other words, there is little interference between threads, and the strong robustness of a large margin algorithm does not guarantee performance loss under the parallel framework."}, {"heading": "5 Conclusions", "text": "We propose a generic online framework for parallel learning for large margins. Our experiment concludes that the proposed framework shows no loss of performance compared to baseline, while the training speed is almost linear with increasing yarns."}, {"heading": "6 Acknowledgements", "text": "This work was partially supported by the National Natural Science Foundation of China (No.61673028) and the National High Technology Research and Development Program of China (863 Program, No.2015AA015404). Xu Sun is the corresponding author of this paper."}], "references": [{"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins."], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Associa-", "citeRegEx": "Collins.,? 2002", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["Koby Crammer", "Yoram Singer."], "venue": "The Journal of Machine Learning Research 3:951\u2013 991.", "citeRegEx": "Crammer and Singer.,? 2003", "shortCiteRegEx": "Crammer and Singer.", "year": 2003}, {"title": "Structured perceptron with inexact search", "author": ["Liang Huang", "Suphan Fayong", "Yang Guo."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Asso-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["Xiangru Lian", "Yijun Huang", "Yuncheng Li", "Ji Liu."], "venue": "Advances in Neural Information Processing Systems. pages 2719\u20132727.", "citeRegEx": "Lian et al\\.,? 2015", "shortCiteRegEx": "Lian et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Online large-margin training of dependency parsers", "author": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."], "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics. Association for Computational Linguistics,", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Distributed training strategies for the structured perceptron", "author": ["Ryan McDonald", "Keith Hall", "Gideon Mann."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "McDonald et al\\.,? 2010", "shortCiteRegEx": "McDonald et al\\.", "year": 2010}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu."], "venue": "Advances in Neural Information Processing Systems. pages 693\u2013701.", "citeRegEx": "Recht et al\\.,? 2011", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Towards shockingly easy structured classification: A search-based probabilistic online learning framework", "author": ["Xu Sun."], "venue": "Technical report, arXiv:1503.08381 .", "citeRegEx": "Sun.,? 2015", "shortCiteRegEx": "Sun.", "year": 2015}, {"title": "Asynchronous parallel learning for neural networks and structured models with dense features", "author": ["Xu Sun."], "venue": "COLING.", "citeRegEx": "Sun.,? 2016", "shortCiteRegEx": "Sun.", "year": 2016}, {"title": "Latent variable perceptron", "author": ["Jun\u2019ichi Tsujii"], "venue": null, "citeRegEx": "Tsujii.,? \\Q2009\\E", "shortCiteRegEx": "Tsujii.", "year": 2009}, {"title": "Parallelized stochastic gradient", "author": ["Alex J Smola"], "venue": null, "citeRegEx": "Smola.,? \\Q2010\\E", "shortCiteRegEx": "Smola.", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "Popular approach to speed up inference process is to take approximate inference instead of exact inference (Huang et al., 2012).", "startOffset": 107, "endOffset": 127}, {"referenceID": 9, "context": "However, we can parallel the inference process of several samples and update asynchronously to further accelerate the training process (Sun, 2016).", "startOffset": 135, "endOffset": 146}, {"referenceID": 1, "context": "Popular approach to speed up inference process is to take approximate inference instead of exact inference (Huang et al., 2012). However, we can parallel the inference process of several samples and update asynchronously to further accelerate the training process (Sun, 2016). In our parallel framework, we split the dataset into k parts and then assign these split datasets to k threads. Each thread updates independently with a shared memory system. After that, we average the weight vector by the number of iterations (not the number of threads as some distributed parallel algorithms). We can see that our approach has no more computation than large margin algorithms, so it is a simple parallel framework. Since the whole framework runs on a shared memory system, there are mainly two problems about this framework: First, several threads update at the same time so it may be closer to minibatch algorithm instead of online learning algorithm intuitively. Whether the parallel framework will affect the convergence rate of online learning algorithm is a problem. Second, when a thread is working, the weight vector may be overwritten by other threads. Whether it will lead to divergence is another problem need to be analyzed. For the first problem, Lian et.al (2015) proved that the convergence rate will not be affected under the parallel SGD framework.", "startOffset": 108, "endOffset": 1273}, {"referenceID": 1, "context": "Popular approach to speed up inference process is to take approximate inference instead of exact inference (Huang et al., 2012). However, we can parallel the inference process of several samples and update asynchronously to further accelerate the training process (Sun, 2016). In our parallel framework, we split the dataset into k parts and then assign these split datasets to k threads. Each thread updates independently with a shared memory system. After that, we average the weight vector by the number of iterations (not the number of threads as some distributed parallel algorithms). We can see that our approach has no more computation than large margin algorithms, so it is a simple parallel framework. Since the whole framework runs on a shared memory system, there are mainly two problems about this framework: First, several threads update at the same time so it may be closer to minibatch algorithm instead of online learning algorithm intuitively. Whether the parallel framework will affect the convergence rate of online learning algorithm is a problem. Second, when a thread is working, the weight vector may be overwritten by other threads. Whether it will lead to divergence is another problem need to be analyzed. For the first problem, Lian et.al (2015) proved that the convergence rate will not be affected under the parallel SGD framework. Our experiments also support that the convergence rate of large margin algorithms is still the same in our parallel framework. For the second problem, Recht et.al (2011) shows that individual SGD steps only modify a small part of the decision variable so memory overwrites are rare and barely any error will be made into the computation.", "startOffset": 108, "endOffset": 1531}, {"referenceID": 0, "context": "One reason is that Collins (2002) explains that averaging parameter helps advoid overfitting.", "startOffset": 19, "endOffset": 34}, {"referenceID": 5, "context": "The algorithm has been widely used in many popular models (McDonald et al., 2005).", "startOffset": 58, "endOffset": 81}, {"referenceID": 8, "context": "It proves to be an effective and efficient structured prediction algorithm with convergence guarantee for separable data (Sun et al., 2009, 2013; Sun, 2015).", "startOffset": 121, "endOffset": 156}, {"referenceID": 0, "context": "Structured perceptron is first proposed by Collins (2002). It proves to be an effective and efficient structured prediction algorithm with convergence guarantee for separable data (Sun et al.", "startOffset": 43, "endOffset": 58}, {"referenceID": 0, "context": "Following the prior work (Collins, 2002), we derives the dataset from Penn Wall Street Journal Treebank (Marcus et al.", "startOffset": 25, "endOffset": 40}, {"referenceID": 4, "context": "Following the prior work (Collins, 2002), we derives the dataset from Penn Wall Street Journal Treebank (Marcus et al., 1993).", "startOffset": 104, "endOffset": 125}, {"referenceID": 7, "context": "We run our parallel framework up to 10 threads following prior work (Recht et al., 2011).", "startOffset": 68, "endOffset": 88}], "year": 2017, "abstractText": "To speed up the training process, many existing systems use parallel technology for online learning algorithms. However, most research mainly focus on stochastic gradient descent (SGD) instead of other algorithms. We propose a generic online parallel learning framework for large margin models, and also analyze our framework on popular large margin algorithms, including MIRA and Structured Perceptron. Our framework is lock-free and easy to implement on existing systems. Experiments show that systems with our framework can gain near linear speed up by increasing running threads, and with no loss in accuracy.", "creator": "LaTeX with hyperref package"}}}