{"id": "1703.00788", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "A Robust Adaptive Stochastic Gradient Method for Deep Learning", "abstract": "Stochastic gradient algorithms are the main focus of large-scale optimization problems and led to important successes in the recent advancement of the deep learning algorithms. The convergence of SGD depends on the careful choice of learning rate and the amount of the noise in stochastic estimates of the gradients. In this paper, we propose an adaptive learning rate algorithm, which utilizes stochastic curvature information of the loss function for automatically tuning the learning rates. The information about the element-wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms.", "histories": [["v1", "Thu, 2 Mar 2017 14:03:48 GMT  (655kb,D)", "http://arxiv.org/abs/1703.00788v1", "IJCNN 2017 Accepted Paper, An extension of our paper, \"ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient\""]], "COMMENTS": "IJCNN 2017 Accepted Paper, An extension of our paper, \"ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient\"", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["caglar gulcehre", "jose sotelo", "marcin moczulski", "yoshua bengio"], "accepted": false, "id": "1703.00788"}, "pdf": {"name": "1703.00788.pdf", "metadata": {"source": "CRF", "title": "A Robust Adaptive Stochastic Gradient Method for Deep Learning", "authors": ["Caglar Gulcehre", "Jose Sotelo", "Marcin Moczulski", "Yoshua Bengio"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTIONWe develop an automatic stochastic gradient algorithm that reduces the burden of extensive search for hyperparameters for the optimizer. Our proposed algorithm uses a lower estimate of variance in the curvature of the cost function and uses it to achieve an automatically matched adaptive learning rate for each parameter.In deep learning and numerical optimization literature, several papers suggest using a diagonal approximation of Hessian (second derived matrix of the cost function in terms of parameters) to calculate optimal learning rates for stochastic gradient descent over high-dimensional parameter spaces [2], [3] a fundamental advantage of using such an approximation is that such an approximation can be a trivial and cheap operation."}, {"heading": "II. DIRECTIONAL SECANT APPROXIMATION", "text": "The advantage of the directional Newton method proposed in [7] compared to the Newton method is that it does not require matrix inversion and still maintains a quadratic rate of convergence. In this thesis we develop a second directional Newton method for nonlinear optimization. The step quantity tk of the update k for step k can be written as if it were a diagonal matrix. (3) In this thesis we develop a second directional Newton method for nonlinear optimization. (1) = \u2212 diag of the update k for step k (Hdk) \u2212 1 of the diagonal matrix (Hdk). (3) where the parameter vector at update k is f, f is the objective function and dk is a unit vector of the direction that the optimization algorithm should follow."}, {"heading": "III. RELATIONSHIP TO THE DIAGONAL APPROXIMATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "TO THE HESSIAN", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "V. BLOCKWISE GRADIENT NORMALIZATION", "text": "It is well known that the repeated application of nonlinearity can cause the gradients to disappear [12], [13]. To address this problem, we normalize the gradients that come into each block / layer to have the norm 1. Assuming that the normalized gradient can be called g, it can be calculated as g = g | | E [g] | | 2. We estimate E [g] over moving averages. Block-by-block normalization of the gradient adds noise to the gradient, but in practice we have not observed any negative effects of it. We suspect that this is due to the fact that the angle between the stochastic gradient and the blocked normalized gradient is still less than 90 degrees."}, {"heading": "VI. ADAPTIVE STEP-SIZE IN STOCHASTIC CASE", "text": "In the case of stochastic gradients, the step size of the directional section can be calculated by applying an expectation via the minibatches: Ek [ti] = Ek [\u2206 ki \u0442\u0442i f (\u03b8 k + \u0445 k) \u2212 \u03b8i f (\u03b8k)]. (20) The Ek [\u00b7] used to calculate the Secant update is calculated via the minibatches at the past values of the parameters. The calculation of the expectation in Eq.20 was numerically unstable in stochastic setting. We opted for a more stable second order Taylor approximation of the equation 20 around the values Ek [(\u03b1ki) 2], \u221a Ek [(\u0441ki) 2], and Ek [\u0441ki) 2]), with the expectation in equation f (\u0442k + \u0441\u0442k) \u2212 \u0441i f (\u0432)."}, {"heading": "VII. ALGORITHMIC DETAILS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Approximate Variability", "text": "In order to calculate the moving averages, as adopted by [3], we used an algorithm to dynamically determine the time constant based on the step size undertaken. Consequently, the algorithm we use will assign greater weights to updates with a large step size and a smaller weight. Assuming that the rule for moving average updates with a larger step size and a smaller step size can be written as follows, the rule for moving average updates with a larger step size and a smaller step size can read as follows. (24) This rule for each update assigns a different weight to each element of the gradient vector. For each iteration, a scalar multiplication with a factor \u2212 1i is performed and adjusted accordingly with the following equation: i [k] [k] [k] (1K] (1) [K) (K) [K) [K) [K) (K) [K) [K) [K) [K) [K)."}, {"heading": "B. Outlier Gradient Detection", "text": "We are very similar to [6], but instead of incrementing (t + 1) when an outlier is detected, the time constant is reset to 2.2. Note that when generalization and optimization occur, this allows approximately the same amount of weight for the current and average of the previous observations. This mechanism has made learning more stable, because without it, the generalization and optimization methods can have advantages both in terms of generalization and in terms of optimization, introducing an exploration and exploitation that can be controlled by an upper limit of values."}, {"heading": "IX. EXPERIMENTS", "text": "We have conducted PTB character level experiments with GRU units, MNIST with Maxout Networks [16], and handwriting synthesis with the IAM OnDB dataset [17]. We compare AdaSecant with popular stochastic gradient learning algorithms: Adagrad, RMSProp [18], Adadelta [19], Adam [20], and SGD + impulse (with linear decreasing learning rate). AdaSecant performs all these different tasks as well or better than carefully tuned algorithms."}, {"heading": "A. Ablation Study", "text": "In this section, we decompile the different parts of the algorithm to measure the effect they have on performance. For this comparison, we trained a model to learn handwriting synthesis on IAM-OnDB datasets. Our model closely follows the architecture introduced in [18] with two modifications. First, we used a recurring layer of size 400 instead of three. Second, we used GRU [21] units instead of LSTM [22] units. In addition, we used a different symbol for each of the 87 different characters in the dataset. The code for this experiment is available online. 2We tested various configurations, including the waiver of variance reduction (VR), Adagrad (AG), block standardization (BN) and outlier recognition (OD). Furthermore, we compared ADAM [20] with different learning rates in Figure 1. We found that the adasekant in Figure 1 performed less reliably than Adam with a carefully crafted learning algorithm."}, {"heading": "B. PTB Character-level LM", "text": "We performed experiments with GRU-RNN [21] on the PTB dataset for modeling character-level language over the subset defined in [23]. For this task, we use 400 GRU units with a minibatch size of 20. We train the model on the sequences of length 150. For AdaSecant, we did not perform a hyperparameter search, but for Adam, we perform a hyperparameter search by learning rate and gradient section. Learning rates are determined by the logically even distribution between 1e \u2212 1 and 6e \u2212 5. Grade section threshold is consistently sampled between 1.2 and 20. We evaluated 20 different pairs of randomly recorded learning rates and gradient section thresholds. The rest of the hyperparameters are set to their default values. We use the model with the best validation error for Adam. For AdaSecant algorithm, we correct all hyperparameters to their default values. The learning curves for the two algorithms are shown in Figure 3."}, {"heading": "C. MNIST with Maxout Networks", "text": "Results are summarized in Figure 4 and we show that AdaSecant converges as fast or faster than other techniques, including the use of hand-tuned global learning rate and impulse for SGD, RMS prop and Adagrad. In our experiments with the AdaSecant algorithm, the term adaptive impulse was shortened to 1.8. In two-tiered maxout network experiments for SGD impulse experiments, we used the best hyperparameters reported by [16], for RMSProp and Adagrad, we cross-validated the learning rate for 15 different learning rates consistently sampled from the logspace. We crossed 30 different pairs of impulse and learning rate for SGD + impulse, for RMSProp and Adagrad, we cross-validated 15 different learning rates from the logspace uniformly for deep maximum experiments."}, {"heading": "X. CONCLUSION", "text": "We have described a new stochastic gradient algorithm with adaptive learning rates that is relatively insensitive to adjustment of hyperparameters and does not require adjustment of learning rates. Furthermore, the method we propose to reduce variance improves convergence when stochastic gradients exhibit high variance. Our algorithm works just as well or better than other popular, carefully tuned stochastic gradient algorithms. We also present a comprehensive ablation study in which we demonstrate the effects and significance of each element of our algorithm. In the future, we should try to find theoretical convergence properties of the algorithm to better understand it analytically."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank the developers of Theano [24], Pylearn2 [25] and Blocks [26] and the computing resources of Compute Canada and Calcul Que \ufffd bec. This work was partially supported by NSERC, CIFAR and Canada Research Chairs, Project TIN2013-41751, grant 2014-SGR-221. Jose Sotelo also thanks the Consejo Nacional de Ciencia y Tecnolog\u0131'a (CONACyT) and the Secretar\u00ed'a de Educacio \ufffd n Pu \ufffd blica (SEP) for their support. We thank Tom Schaul for the valuable discussions and Kyunghyun Cho and Orhan Firat for the proofreading and feedback on the work."}, {"heading": "A. Derivation of Equation 18", "text": "\u2202 E [(\u03b2igi + (1 \u2212 \u03b2i) E [gi] \u2212 g \u2032 i) 2] \u2202 \u03b2i + \u03bb\u03b22i = 0E [(\u03b2igi + (1 \u2212 \u03b2i) E [gi] \u2212 g \u2032 i) \u2202 (\u03b2igi + (1 \u2212 \u03b2i) E [gi] \u2212 g \u2032 i) \u2202 \u03b2i] + \u03bb\u03b2i = 0E [(\u03b2igi + (1 \u2212 \u03b2i) E [gi] \u2212 g \u2032 i) (gi \u2212 E [gi])) (gi \u2212 E [gi])) + \u03bb\u03b2i = 0E [(gi \u2212 E [gi]) (\u03b2igi (1 \u2212 \u03b2i) E [gi] (gi \u2212 E [gi]) \u2212 g \u2032 i (gi \u2212 E [gi]) + \u03bb\u03b2i = 0\u03b2i = E [gi \u2212 E [gi]) (g \u2032 i \u2212 E [gi])) (gi \u2212 E [gi \u2212 E])))"}, {"heading": "B. Further Experimental Details", "text": "In Figure 5, we analyzed the effects of using different minibatch sizes for AdaSecant and compared their convergence with Adadelta in wall clock time. AdaSecant was able to achieve almost the same negative training probability as Adadelta in minibatch size 100 after the same time, but their convergence took much longer. AdaSecant was able to converge faster to better local minimums in wall clock time with minibatches of size 500."}, {"heading": "C. More decomposition experiments", "text": "We have conducted experiments with the various combinations of the components of the algorithm and show these results in handwriting synthesis with IAM-OnDB datasets. The results can be observed from Figure 6, Figure 7, Figure 8 and Figure 9 when the components are deactivated, resulting in a more unstable training curve in most scenarios."}], "references": [{"title": "Improving the convergence of backpropagation learning with second order methods", "author": ["S. Becker", "Y. Le Cun"], "venue": "Proceedings of the 1988 connectionist models summer school. San Matteo, CA: Morgan Kaufmann, 1988, pp. 29\u201337.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1988}, {"title": "No more pesky learning rates", "author": ["T. Schaul", "S. Zhang", "Y. LeCun"], "venue": "arXiv preprint arXiv:1206.1106, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic learning rate maximization by on-line estimation of the hessians eigenvectors", "author": ["Y. LeCun", "P.Y. Simard", "B. Pearlmutter"], "venue": "Advances in neural information processing systems, vol. 5, pp. 156\u2013163, 1993.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1993}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural networks: Tricks of the trade. Springer, 2012, pp. 9\u201348.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients", "author": ["T. Schaul", "Y. LeCun"], "venue": "arXiv preprint arXiv:1301.3764, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Directional newton methods in n variables", "author": ["Y. Levin", "A. Ben-Israel"], "venue": "Mathematics of Computation, vol. 71, no. 237, pp. 251\u2013262, 2002.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Directional secant method for nonlinear equations", "author": ["H.-B. An", "Z.-Z. Bai"], "venue": "Journal of computational and applied mathematics, vol. 175, no. 2, pp. 291\u2013304, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast curvature matrix-vector products for second-  order gradient descent", "author": ["N.N. Schraudolph"], "venue": "Neural computation, vol. 14, no. 7, pp. 1723\u2013 1738, 2002.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Variance reduction for stochastic gradient optimization", "author": ["C. Wang", "X. Chen", "A. Smola", "E. Xing"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 181\u2013189.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 315\u2013323.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE transactions on neural networks, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "2001.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics, pp. 400\u2013407, 1951.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1951}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1302.4389, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Iam-ondb - an on-line english sentence database acquired from handwritten text on a whiteboard.\u201d in ICDAR", "author": ["M. Liwicki", "H. Bunke"], "venue": "IEEE Computer Society,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Adadelta: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "International Conference on Learning Representations, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997. [Online]. Available: http://dx.doi.org/10.1162/neco.1997.9.8.1735", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Subword language modeling with neural networks", "author": ["T. Mikolov", "I. Sutskever", "A. Deoras", "H. Le", "S. Kombrink", "J. Cernocky"], "venue": "preprint, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Pylearn2: a machine learning research library", "author": ["I.J. Goodfellow", "D. Warde-Farley", "P. Lamblin", "V. Dumoulin", "M. Mirza", "R. Pascanu", "J. Bergstra", "F. Bastien", "Y. Bengio"], "venue": "arXiv preprint arXiv:1308.4214, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Blocks and Fuel: Frameworks for deep learning", "author": ["B. van Merri\u00ebnboer", "D. Bahdanau", "V. Dumoulin", "D. Serdyuk", "D. Warde- Farley", "J. Chorowski", "Y. Bengio"], "venue": "ArXiv e-prints, jun 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In deep learning and numerical optimization literature, several papers suggest using a diagonal approximation of the Hessian (second derivative matrix of the cost function with respect to parameters), in order to estimate optimal learning rates for stochastic gradient descent over high dimensional parameter spaces [2], [3], [4].", "startOffset": 316, "endOffset": 319}, {"referenceID": 1, "context": "In deep learning and numerical optimization literature, several papers suggest using a diagonal approximation of the Hessian (second derivative matrix of the cost function with respect to parameters), in order to estimate optimal learning rates for stochastic gradient descent over high dimensional parameter spaces [2], [3], [4].", "startOffset": 321, "endOffset": 324}, {"referenceID": 2, "context": "In deep learning and numerical optimization literature, several papers suggest using a diagonal approximation of the Hessian (second derivative matrix of the cost function with respect to parameters), in order to estimate optimal learning rates for stochastic gradient descent over high dimensional parameter spaces [2], [3], [4].", "startOffset": 326, "endOffset": 329}, {"referenceID": 3, "context": "For example, obtaining a diagonal approximation of Hessian are the Gauss-Newton matrix [5] or by finite differences [6].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "For example, obtaining a diagonal approximation of Hessian are the Gauss-Newton matrix [5] or by finite differences [6].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "[3] suggested a reliable way to estimate the local curvature in the stochastic setting by keeping track of the variance and average of the gradients.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "We keep track of the estimation of curvature using a technique similar to that proposed by [3], which uses the variability of the expected loss.", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "Directional Newton is a method proposed for solving equations with multiple variables[7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "The advantage of directional Newton method proposed in[7], compared to Newton\u2019s method is that, it does not require a matrix inversion and still maintains a quadratic rate of convergence.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "We can approximate the per-parameter learning rate ti following [8] using finite differences:", "startOffset": 64, "endOffset": 67}, {"referenceID": 7, "context": "Let us note that alternatively one might use the R-op to compute the Hessian-vector product for the denominator in Equation 7 [9].", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "Both [10] and [11] proposed new ways of dealing with this problem.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "Both [10] and [11] proposed new ways of dealing with this problem.", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "BLOCKWISE GRADIENT NORMALIZATION It is very well-known that the repeated application of the non-linearities can cause the gradients to vanish [12], [13].", "startOffset": 142, "endOffset": 146}, {"referenceID": 11, "context": "BLOCKWISE GRADIENT NORMALIZATION It is very well-known that the repeated application of the non-linearities can cause the gradients to vanish [12], [13].", "startOffset": 148, "endOffset": 152}, {"referenceID": 1, "context": "Approximate Variability To compute the moving averages as also adopted by [3], we used an algorithm to dynamically decide the time constant based on the step size being taken.", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "Our algorithm is very similar to [6], but instead of incrementing \u03c4i[t+1] when an outlier is detected, the time-constant is reset to 2.", "startOffset": 33, "endOffset": 36}, {"referenceID": 12, "context": "such that the learning rate \u03b7 should decrease [14].", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "To ensure it, we developed a new variant of Adagrad [15] with thresholding, such that each scaling factor is lower bounded by 1.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "EXPERIMENTS We have run experiments on character-level PTB with GRU units, on MNIST with Maxout Networks [16] and on handwriting synthesis using the IAM-OnDB dataset [17].", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "EXPERIMENTS We have run experiments on character-level PTB with GRU units, on MNIST with Maxout Networks [16] and on handwriting synthesis using the IAM-OnDB dataset [17].", "startOffset": 166, "endOffset": 170}, {"referenceID": 16, "context": "We compare AdaSecant with popular stochastic gradient learning algorithms: Adagrad, RMSProp [18], Adadelta [19], Adam [20] and SGD+momentum (with linearly decaying learning rate).", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "We compare AdaSecant with popular stochastic gradient learning algorithms: Adagrad, RMSProp [18], Adadelta [19], Adam [20] and SGD+momentum (with linearly decaying learning rate).", "startOffset": 107, "endOffset": 111}, {"referenceID": 18, "context": "We compare AdaSecant with popular stochastic gradient learning algorithms: Adagrad, RMSProp [18], Adadelta [19], Adam [20] and SGD+momentum (with linearly decaying learning rate).", "startOffset": 118, "endOffset": 122}, {"referenceID": 16, "context": "Our model follows closely the architecture introduced in [18] with two modifications.", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "Second, we use GRU [21] units instead of LSTM [22] units.", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "Second, we use GRU [21] units instead of LSTM [22] units.", "startOffset": 46, "endOffset": 50}, {"referenceID": 18, "context": "Also, we compared against ADAM [20] with different learning rates in Figure 1.", "startOffset": 31, "endOffset": 35}, {"referenceID": 19, "context": "We have run experiments with GRU-RNN[21] on PTB dataset for character-level language modeling over the subset defined in [23].", "startOffset": 36, "endOffset": 40}, {"referenceID": 21, "context": "We have run experiments with GRU-RNN[21] on PTB dataset for character-level language modeling over the subset defined in [23].", "startOffset": 121, "endOffset": 125}, {"referenceID": 14, "context": "In 2-layer Maxout network experiments for SGD-momentum experiments, we used the best hyper-parameters reported by [16], for RMSProp and Adagrad, we crossvalidated learning rate for 15 different learning rates sampled uniformly from the log-space.", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "We thank the developers of Theano [24], Pylearn2 [25] and Blocks [26] and the computational resources provided by Compute Canada and Calcul Qu\u00e9bec.", "startOffset": 34, "endOffset": 38}, {"referenceID": 23, "context": "We thank the developers of Theano [24], Pylearn2 [25] and Blocks [26] and the computational resources provided by Compute Canada and Calcul Qu\u00e9bec.", "startOffset": 49, "endOffset": 53}, {"referenceID": 24, "context": "We thank the developers of Theano [24], Pylearn2 [25] and Blocks [26] and the computational resources provided by Compute Canada and Calcul Qu\u00e9bec.", "startOffset": 65, "endOffset": 69}], "year": 2017, "abstractText": "Stochastic gradient algorithms are the main focus of large-scale optimization problems and led to important successes in the recent advancement of the deep learning algorithms. The convergence of SGD depends on the careful choice of learning rate and the amount of the noise in stochastic estimates of the gradients. In this paper, we propose an adaptive learning rate algorithm, which utilizes stochastic curvature information of the loss function for automatically tuning the learning rates. The information about the element-wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms. 1", "creator": "LaTeX with hyperref package"}}}