{"id": "1703.00810", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Opening the Black Box of Deep Neural Networks via Information", "abstract": "Despite their great success, there is still no com- prehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their in- ner organization. Previous work [Tishby &amp; Zaslavsky (2015)] proposed to analyze DNNs in the Information Plane; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the In- formation Bottleneck (IB) tradeoff between com- pression and prediction, successively, for each layer.", "histories": [["v1", "Thu, 2 Mar 2017 14:53:14 GMT  (3499kb,D)", "http://arxiv.org/abs/1703.00810v1", "9 pages, 7 figures"], ["v2", "Thu, 9 Mar 2017 10:00:24 GMT  (3499kb,D)", "http://arxiv.org/abs/1703.00810v2", "9 pages, 7 figures"], ["v3", "Sat, 29 Apr 2017 17:32:47 GMT  (6334kb,D)", "http://arxiv.org/abs/1703.00810v3", "19 pages, 8 figures"]], "COMMENTS": "9 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ravid shwartz-ziv", "naftali tishby"], "accepted": false, "id": "1703.00810"}, "pdf": {"name": "1703.00810.pdf", "metadata": {"source": "META", "title": "Opening the Black Box of Deep Neural Networks via Information", "authors": ["Ravid Shwartz-Ziv", "Naftali Tishby"], "emails": ["<tishby@cs.huji.ac.il>."], "sections": [{"heading": null, "text": "In this paper, we explore this idea and demonstrate the effectiveness of the information-level visualization of DNNs. First, we show that the stochastic gradient reduction (SGD) has two distinct phases: rapid empirical error minimization followed by slow compression of the representation for each layer. Then, we argue that the DNN layers end very close to the theoretical limit of the IB, and present a new theoretical argument for the computational utility of the hidden layers."}, {"heading": "1. Introduction", "text": "This year it has come to the point that it is a purely reactionary project, in which it is a reactionary project, in which it is a reactionary, reactionary and reactionary project."}, {"heading": "2. Information Theory of Deep Learning", "text": "In supervised learning, we are interested in good representations, T (X), the input patterns X (X, Y), which allow a good prediction of the label Y (Y). Furthermore, we want to learn such representations efficiently from an empirical sample of the (unknown) common distribution P (X, Y), in a way that allows a good generalization. DNNs and deep learning generate a Markov chain of such representations, the hidden layers, by minimizing the empirical error over the weights of the network layer by layer. This optimization is done using stochastic gradient derivation (SGD) using a noisy estimate of the gradient of the empirical error at each weight by backpropagation. Our first important finding is to treat the entire layer, T, as a single random variable characterized by its encoder, P (T | X), and the deformation, Y | T representations."}, {"heading": "2.1. Mutual Information", "text": "Given any two random variables, X and Y, with a common distribution p (x, y), their mutual information is defined as: I (X; Y) = DKL [p (x, y) | | p (x) p (y)]] (1) = \u2211 x-X, y-Yp (x, y) log (p (x, y) p (x) p (x) p (y)) (2) = \u2211 x-X, y-Yp (x, y) log (p (x))) (3) = H (X (X) \u2212 H (X | Y), (4) where DKL [p | q] is the Kullback Liebler divergence of distributions p and q, and H (X | Y) and H (X | Y) is the entropy and conditional entropy of X and Y, i.e. the mutual information quantifies the number of relevant bits contained in the input variable X."}, {"heading": "2.2. The Information Plane", "text": "Each representation variable, T, defined as a (possibly stochastic) map of input X, is characterized by its common distributions with X and Y or by its encoder and decoder distributions P (T | X) or P (Y | T), respectively. If P (X; Y) is specified, T is uniquely mapped to a point in the information plane with coordinates (I (X; T), I (T; Y)). When applied to the Markov chain of a K layer DNN, where Ti denotes the ith-hidden layer as a single multivariate variable (Figure 1), the layers are mapped to K-monotonic connecting points in the plane - henceforth a unique information path - that fulfills the following DPI chains: I (X; Y) \u2265 I (T1; Y) \u2265 I (T2; Y) \u2265 I (Y) \u2265 I (Y) \u2265 I (Y) \u2265 I (Y; Y).I (X) \u2265 I (X; X (X; Y) \u2265 I (X; Y) \u2265 I (X) \u2265 I (X) \u2265 (Y)."}, {"heading": "2.3. The Information Bottleneck optimal bound", "text": "What characterizes the optimal representations of X w.r.t. Y? The classic notion of minimal, sufficient statistics provides good candidates for optimal representations. Sufficient statistics in our context are maps or partitions of X, S (X), which capture all the information X has on Y. Namely I (S); Y) = I (X; Y) = I (X; Y) [Cover & Thomas (2006)].Minimal, sufficient statistics, T (X), are the simplest, sufficient statistics and induce the coarsest, sufficient partition on X. In other words, they are functions of any other sufficient statistic. A simple way to formulate this is through the Markov chain: Y \u2192 X \u2192 S (X) \u2192 T (X), which for a minimal, sufficient statistic T (X) with every other sufficient statistic S (X).With the help of the DPI, we can throw it into a limited optimization problem: T (X) = arg S (X)."}, {"heading": "2.4. Visualizing DNNs in the Information Plane", "text": "As proposed by Tishby & Zaslavsky (2015), we study the information pathways of DNNs at the information level, which can happen when the underlying distribution, P (X, Y), is known and the distributions of encoders and decoders P (T | X) and P (Y | T) can be calculated directly. For \"big real\" problems, these distributions and mutual information values should be estimated using random samples or using other modelling assumptions. This goes beyond the scope of this work, but we believe that our analyses and observations are general and expect the dynamic phase transitions for larger networks to become even sharper, as they are inherently based on statistical ensemble characteristics. Good overviews of mutual information estimation methods can be found in Paninski (2003) and Kraskov et al. (2004). Our two order parameters I (T; X) and I (T; Y) allow us to examine different network architectures in terms of efficiency."}, {"heading": "3. Numerical Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Experimental Setup", "text": "The activation function of all neurons was the hyperbolic tangent function, which was shifted to a sigmoidal function in the last layer. Networks were trained using SGD and the cross entropy loss function (see Figure 4), without any other explicit regulation. Unless otherwise noted, the DNNs used had up to 7 completely hidden layers, with widths: 12-10-7-5-4-3-2 neurons (see Figure 4). In our results below, layer 1 is the hidden layer closest to input and the highest is the initial layer. To simplify our analysis, the tasks were selected as binary decision rules invariant under O (3). Rotations of sphere x, with 12 binary inputs representing 12 evenly distributed points on a 2D sphere."}, {"heading": "3.2. Estimating the Mutual Information of the Layers", "text": "As mentioned above, we consider each of the layers 1 \u2264 i \u2264 K in the network as a single variable Ti and calculate the mutual information between each layer with the input and with the labels. To calculate the mutual information of the network layers with the input and output variables, we summarized the arctane output activations of the neuron in 30 equal intervals between -1 and 1. Then we used these discredited values for each neuron in the layer, t-Ti, to calculate the joint distributions directly via the 4096 equally probable input patterns x-X, P (Ti, X) and P (Ti, Y) = x-P (Ti | x), using the Markov chain Y-X-Ti for each hidden layer. Using these discrete joint distributions, we calculated the decoder and coded the opposite information I (Ti; Y) selectively."}, {"heading": "3.3. The dynamics of the optimization process", "text": "In order to understand the dynamics of the network, we have to deal with the problems that have arisen in recent years."}, {"heading": "3.4. The two phases of SGD optimization", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "3.5. The benefit of the hidden layers", "text": "We now turn to one of the fundamental questions about deep learning - what is the benefit of the hidden layers? To solve this, we trained 6 different architectures with 1 to 6 hidden layers (with layers as shown in Figure 4), trained on 80% of the patterns randomly sampled from Equation (8). As before, we repeated each training 50 times with random starting weights and training samples. Figure 5 shows the information levels for these 6 architectures during training periods, averaged over the random networks. There are several important results from this experiment:"}, {"heading": "1. Adding hidden layers dramatically reduces the number", "text": "To see this, compare the color of the paths on the top layers of Figure 5 (with 1 and 2 hidden layers) with the colors on the bottom layers (with 5 and 6 hidden layers).While the network with a hidden layer could not achieve good IY values even after 104 epochs, it reached the full relevant information on the output level with 6 hidden layers within 400 epochs."}, {"heading": "2. The compression phase of each layer is shorter when it starts from a previous compressed layer.", "text": "Comparing the time with a good generalization with 4 and 5 hidden layers, the yellow above indicates a much slower convergence with 4 layers than with 5 or 6 layers, where they reach the endpoints with half of the epochs."}, {"heading": "3. The compression is faster for the deeper (narrower and closer to the output) layers.", "text": "While in the drift phase the lower layers move first (due to DPI), in the diffusion phase the upper layers compress first and \"pull\" the lower layers thereafter. Adding more layers seems to add intermediate representations that accelerate compression. In the discussion, we outline a simple explanation for the dramatic arithmetic advantage of the hidden layers, which is based on the dynamics of the diffusion processes during the compression phase."}, {"heading": "3.6. The IB optimality of the layers", "text": "Finally, in order to quantify the IB optimality of the layers, we tested whether the converged layers measured the encoder-decoder relationships of Eq. (7) for a certain value of the Lagrange multiplier \u03b2.For each converged layer, we used the encoder and decoder distributions based on the quantified values of the layer neurons pi (t | x) and pi (t | x), with which we calculated the information values (IiX, I i Y). To test the IB optimality of the layer coder decoders, we calculated the optimal IB encoder pi, \u03b2 (t | x) using the ith layer decoder pi (y | t) by Eq. (7) This can be done for each value of \u03b2 (X, Y), with the known P (X, Y) encoder being the optimum \u03b2i value for each layer by calculating the mean co\u03b2-\u03b2-\u03b2 co\u03b2 information between the co\u03b2-\u03b2 of the co\u03b2-\u03b2 and the co\u03b2-\u03b2 of the co\u03b2 of the co\u03b2-beta co\u03b2-beta."}, {"heading": "3.7. Evolution of the layers with training size", "text": "Another fundamental problem in machine learning, which we only briefly address in this paper, is the dependence on the size of the training data [Cho et al. (2015)]. It is useful to visualize the convergent positions of the hidden layers for different training data sizes at the information level (Figure 7).We trained networks with 6 hidden layers as before, but with different sample sizes ranging from 3% to 85% of the patterns. As expected, the convergent layers are on a smooth line for each layer as the training size increases, with remarkable regularity. We maintain that the layers converge to specific points on the finite sample information curves and move closer to the theoretical IB limit for the control distribution. Despite the randomization, the convergent layers for different training sizes are on a smooth line, with remarkable regularity. We maintain that the sample layers can converge to specific points on the curve information curves."}, {"heading": "3.8. The computational benefits of layered diffusion", "text": "Diffusion processes are regulated by the diffusion equation or by the Focker-Planck equation, although there is also a drift or a limiting potential. In simple diffusion, the initial distribution develops through a convolution with a Gaussian nucleus whose width increases with time, in each dimension (D - a diffusion constant). Such convolutions result in an entropy increase that grows like an H-log (Dt). Thus, entropy growth is logarithmic in the number of time steps, or the number of steps is exponential in entropy growth. If there is a potential or empirical error limitation, this process asymptotically converges with the maximum entropy distribution, which is exponential in the limited potential or training error. This exponential distribution meets the IB equations Eq. (7), as we have seen in the previous section."}, {"heading": "4. Discussion and conclusions", "text": "We have shown that the visualization of the layers at the information level reveals many - hitherto unknown - details about the inner workings of deep learning and deep neural networks, revealing the different phases of SGD optimization, drift and diffusion that explain the ERM and the display compression paths of the layers. Stochasticity of SGD methods is usually motivated as a way to escape local minimums of the training error. In this paper, we give it a new, perhaps much more important role: it generates highly efficient internal representations through compression through diffusion. We also argue that SGD is an overkill during the diffusion phase that consumes most training periods, and that much simpler optimization algorithms, such as Monte Carlo relations through diffusion, can be more efficient."}, {"heading": "If our findings hold for general networks and tasks, the compression phase of the SGD and the convergence of the layers to the IB bound can explain the phenomenal success", "text": "by Deep Learning."}, {"heading": "Acknowledgments", "text": "This work is supported in part by the Gatsby Charitable Foundation, the Israel Science Foundation and the Intel ICRICI Center."}], "references": [{"title": "Understanding intermediate layers using linear classifier probes, 2016", "author": ["Alain", "Guillaume", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Alain et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alain et al\\.", "year": 2016}, {"title": "How much data is needed to train a medical image deep learning system to achieve necessary high accuracy", "author": ["Cho", "Junghwan", "Lee", "Kyewook", "Shin", "Ellie", "Choy", "Garry", "Do", "Synho"], "venue": "arXiv preprint arXiv:1511.06348,", "citeRegEx": "Cho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["German", "Stuart", "Donald"], "venue": "neurocomputing: foundations of research,", "citeRegEx": "German et al\\.,? \\Q1988\\E", "shortCiteRegEx": "German et al\\.", "year": 1988}, {"title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey"], "venue": "(icassp), 2013 ieee international conference on,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Rotation invariant spherical harmonic representation of 3d shape descriptors", "author": ["Kazhdan", "Michael", "Funkhouser", "Thomas", "Rusinkiewicz", "Szymon"], "venue": "Eurographics Symposium on Geometry Processing,", "citeRegEx": "Kazhdan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kazhdan et al\\.", "year": 2003}, {"title": "Estimating mutual information", "author": ["Kraskov", "Alexander", "St\u00f6gbauer", "Harald", "Grassberger", "Peter"], "venue": "Phys. Rev. E,", "citeRegEx": "Kraskov et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kraskov et al\\.", "year": 2004}, {"title": "Exploring strategies for training deep neural networks", "author": ["Larochelle", "Hugo", "Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me", "Lamblin", "Pascal"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Larochelle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2009}, {"title": "Estimation of entropy and mutual information", "author": ["Paninski", "Liam"], "venue": "Neural Comput.,", "citeRegEx": "Paninski and Liam.,? \\Q2003\\E", "shortCiteRegEx": "Paninski and Liam.", "year": 2003}, {"title": "The Fokker-Planck Equation: Methods of Solution and Applications. Number isbn9780387504988, lccn=89004059 in Springer series in synergetics", "author": ["H. Risken"], "venue": null, "citeRegEx": "Risken,? \\Q1989\\E", "shortCiteRegEx": "Risken", "year": 1989}, {"title": "Deep learning and the information bottleneck principle", "author": ["Tishby", "Naftali", "Zaslavsky", "Noga"], "venue": "In Information Theory Workshop (ITW),", "citeRegEx": "Tishby et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tishby et al\\.", "year": 2015}, {"title": "The information bottleneck method", "author": ["Tishby", "Naftali", "Pereira", "Fernando C", "Bialek", "William"], "venue": "In Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing,", "citeRegEx": "Tishby et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Tishby et al\\.", "year": 1999}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Zhang", "Chiyuan", "Bengio", "Samy", "Hardt", "Moritz", "Recht", "Benjamin", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1611.03530,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Text understanding from scratch", "author": ["Zhang", "Xiang", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1502.01710,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "In the last decade, deep learning algorithms have made remarkable progress on numerous machine learning tasks and dramatically improved the state-of-the-art in many practical areas [Graves et al. (2013); Zhang & LeCun (2015); Hinton et al.", "startOffset": 182, "endOffset": 203}, {"referenceID": 3, "context": "In the last decade, deep learning algorithms have made remarkable progress on numerous machine learning tasks and dramatically improved the state-of-the-art in many practical areas [Graves et al. (2013); Zhang & LeCun (2015); Hinton et al.", "startOffset": 182, "endOffset": 225}, {"referenceID": 3, "context": "In the last decade, deep learning algorithms have made remarkable progress on numerous machine learning tasks and dramatically improved the state-of-the-art in many practical areas [Graves et al. (2013); Zhang & LeCun (2015); Hinton et al. (2012); He et al.", "startOffset": 182, "endOffset": 247}, {"referenceID": 3, "context": "In the last decade, deep learning algorithms have made remarkable progress on numerous machine learning tasks and dramatically improved the state-of-the-art in many practical areas [Graves et al. (2013); Zhang & LeCun (2015); Hinton et al. (2012); He et al. (2015); LeCun et al.", "startOffset": 182, "endOffset": 265}, {"referenceID": 3, "context": "In the last decade, deep learning algorithms have made remarkable progress on numerous machine learning tasks and dramatically improved the state-of-the-art in many practical areas [Graves et al. (2013); Zhang & LeCun (2015); Hinton et al. (2012); He et al. (2015); LeCun et al. (2015)].", "startOffset": 182, "endOffset": 286}, {"referenceID": 11, "context": "Moreover, they suggested that optimized DNNs layers should approach the Information Bottleneck (IB) bound [Tishby et al. (1999)] of the optimal achievable representations of the input X .", "startOffset": 107, "endOffset": 128}, {"referenceID": 11, "context": ", exponential families), Tishby et al. (1999) relaxed this optimization problem by first allowing the map to be stochastic, defined as an encoder P (T |X), and then, by allowing the map to capture as much as possible of I(X;Y ), not necessarily all of it.", "startOffset": 25, "endOffset": 46}, {"referenceID": 11, "context": "This leads to the Information Bottleneck (IB) tradeoff [Tishby et al. (1999)], which provides a computational framework for finding approximate minimal sufficient statistics, or the optimal tradeoff between compression of X and prediction of Y .", "startOffset": 56, "endOffset": 77}, {"referenceID": 7, "context": "Good overviews on methods for mutual information estimation can be found in Paninski (2003) and Kraskov et al. (2004).", "startOffset": 96, "endOffset": 118}, {"referenceID": 6, "context": "These orbits form a minimal sufficient partition/statistics for spherically symmetric rules [Kazhdan et al. (2003)].", "startOffset": 93, "endOffset": 115}, {"referenceID": 6, "context": "To generate the input-output distribution, P (X,Y ), we calculated a spherically symmetric real valued function of the pattern f(x) (evaluated through its spherical harmonics power spectrum [Kazhdan et al. (2003)]) and compared it to a threshold, \u03b8, and apply a step \u0398 function to obtain a {0, 1} label: y(x) = \u0398(f(x) \u2212 \u03b8).", "startOffset": 191, "endOffset": 213}, {"referenceID": 8, "context": "This looks very much like overfitting the small sample noise, which can be avoided with early stopping methods [Larochelle et al. (2009)].", "startOffset": 112, "endOffset": 137}, {"referenceID": 10, "context": "Risken (1989)], whose stationary distribution maximizes the entropy of the weights distribution, under the training error constraint.", "startOffset": 0, "endOffset": 14}, {"referenceID": 13, "context": "This is consistent with previous works which showed that explicit forms of regularization, such as weight decay, dropout, and data augmentation, do not adequately explain the generalization error of DNNs [Zhang et al. (2016)].", "startOffset": 205, "endOffset": 225}, {"referenceID": 1, "context": "Another fundamental issue in machine learning, which we only deal with briefly in this paper, is the dependence on the training data size [Cho et al. (2015)].", "startOffset": 139, "endOffset": 157}], "year": 2017, "abstractText": "Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work [Tishby & Zaslavsky (2015)] proposed to analyze DNNs in the Information Plane; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the InformationPlane visualization of DNNs. We first show that the stochastic gradient descent (SGD) epochs have two distinct phases: fast empirical error minimization followed by slow representation compression, for each layer. We then argue that the DNN layers end up very close to the IB theoretical bound, and present a new theoretical argument for the computational benefit of the hidden layers.", "creator": "LaTeX with hyperref package"}}}