{"id": "1703.00830", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "General and Robust Communication-Efficient Algorithms for Distributed Clustering", "abstract": "As datasets become larger and more distributed, algorithms for distributed clustering have become more and more important. In this work, we present a general framework for designing distributed clustering algorithms that are robust to outliers. Using our framework, we give a distributed approximation algorithm for k-means, k-median, or generally any L_p objective, with z outliers and/or balance constraints, using O(m(k+z)(d+log n)) bits of communication, where m is the number of machines, n is the size of the point set, and d is the dimension. This generalizes and improves over previous work of Bateni et al. and Malkomes et al. As a special case, we achieve the first distributed algorithm for k-median with outliers, answering an open question posed by Malkomes et al. For distributed k-means clustering, we provide the first dimension-dependent communication complexity lower bound for finding the optimal clustering. This improves over the lower bound from Chen et al. which is dimension-agnostic.", "histories": [["v1", "Thu, 2 Mar 2017 15:27:14 GMT  (167kb,D)", "http://arxiv.org/abs/1703.00830v1", null], ["v2", "Thu, 12 Oct 2017 19:08:04 GMT  (167kb,D)", "http://arxiv.org/abs/1703.00830v2", null]], "reviews": [], "SUBJECTS": "cs.DS cs.DC cs.LG", "authors": ["pranjal awasthi", "maria-florina balcan", "colin white"], "accepted": false, "id": "1703.00830"}, "pdf": {"name": "1703.00830.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Pranjal Awasthi", "Maria-Florina Balcan", "Colin White"], "emails": ["pranjal.awasthi@rutgers.edu,", "ninamf@cs.cmu.edu,", "crwhite@cs.cmu.edu."], "sections": [{"heading": null, "text": "In addition, we provide distributed cluster algorithms that provide near-optimal solutions, provided the data meet the state of approximation stability of Balcan et al. [8] or the state of spectral stability of Kumar and Kannan [27]. In certain cluster applications, where each machine only needs to find a cluster that is consistent with the global optimum, we show that no communication is necessary if the data meets the approximate stability. Authors \"addresses: pranjal.awasthi @ rutgers.edu, ninamf @ cs.cmu.edu, crwhite @ cs.cmu.edu. This work has been partially supported by NSF grants CCF-1422910, CCF-1535967, IIS-1618714, a Sloan Research Fellowship, a Microsoft Research Faculty Fellowship, a Google Research Award, and a National Defense Science & Engineering Graduate (NDSEG) fellowship."}, {"heading": "1 Introduction", "text": "This year, it will be able to put itself at the top of the group."}, {"heading": "1.1 Our contributions", "text": "In this publication, we provide conceptually simple clustering algorithms, combined with a new analysis, to draw a unified picture for distributed clustering that generalizes and improves several current results. A distributed clustering instance consists of a set of n points arbitrarily distributed over the machine, with a distance metric (if points are in Rd, the metric is a Euclidean distance) and the problem is to approximate the \"p clustering objective, while the amount of communication across machines is arbitrarily distributed. For clustering with capacities or outliers, the number of outliers and the capacity limitation are L inputs to the algorithms (as it is).General Robust Distributed Clustering objective in Section 3, we show a general distributed algorithm for balanced k clustering in d dimensions with outliers that use O (m + log n) communication."}, {"heading": "1.2 Related Work", "text": "The first constant factor approximation algorithm for k-medians was given by Charikar et al. [16], and the current best approximation ratio is 2.675 by Byrka et al. [15]. For k-center, there is a narrow 2-approximation algorithm [23]. For k-means, the best approximation ratio is 6.357 [2], and Makarychev et al. recently showed a bicriteria algorithm with strong guarantees [30]. For clustering with outliers, there is a 3-approximation algorithm for kcenter with z-outliers, as well as a bicriteria 4 (1 + 1 /) -approximation algorithm for k-medians, the outlier (1 +) z outlier [17]. Chen found a true constant factor approximation algorithm for k-median communication (the constant is not calculated explicitly)."}, {"heading": "2 Preliminaries", "text": "We assume that the communication processes between the centers are separated by a factor of no more than 2, if the communication processes are separated by 2, 2 or 3. The \"p-costs\" of C iscost (C) = \u2264 \"Xi d\" (V, xi) p 1p. \"We will create the optimal cluster of a point in.\" p. \"p.\" p. \"p\" p \"p.\" p \"p\" p \"p\" p. V \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p\" p. \"p.\" p. \"p.\" p. \"p.\" p. \"p.\" p \"p.\" p. \"p.\" p. \"p\" p. \"p.\" p. \"p\" p. \"p\" p. \"p\" p. \"p.\" p \"p.\" p \"p\" p. \"p\" p \"p.\" p \"p\" p. \"p\" p. \"p\" p. \"p\" p. \"p\" p \"p\" p. \"p\" p. \"p\" p \"p.\" p \"p.\" p \"p\" p \"p.\" p \"p.\" p. \"p.\" p. \"p\" p. \"p.\" p. \"p.\" p. \"p.\" p. \""}, {"heading": "3 General Robust Distributed Clustering", "text": "In this section, we give a general algorithm for distributed clustering with the \"p target,\" with or without equilibrium constraints and with or without outliers. This generalizes previous distributed clustering results [12, 31] and answers an open question by Malkomes et al. [31] We give a simple algorithmic framework, along with a careful analysis, to prove strong guarantees in various constellations. Each machine performs a k clustering on its own data, and the centers, along with the size of their respective clusters, are sent to a central machine, which then performs a weighted clustering ratio to the mk centers (see Figure 1). In the case of clustering with outliers, each machine performs a -clustering, and the central machine performs a clustering algorithm that acts outliers. Theorem 3. Faced with a sequential algorithm approach algorithm A 1 for balanced clustering with the \"outlier\" target."}, {"heading": "4 Communication Complexity Lower Bounds", "text": "This generalizes the result of Chen et al. [18], which has a lower limit regardless of dimension. [18] Our lower limit applies even if the data include (1 + \u03b1,) approximate stability or \u03b3 spectral stability for arbitrary alpha values or \u03b3 spectral stability. [18]. For any c instance, and z-0, the communication complexity of calculating a c approximation for k clustering in \"p with z outliers is a corollary of Chen et al. [18] This follows from Theorem 4.1 in [18], because k clusters with z outliers are a k + z imputable function: it values to 0 if it is at most k + z points, otherwise it is greater than 0."}, {"heading": "5 Distributed Clustering under Approximation Stability", "text": "In this section, we improve the results of section 3 if it is known in advance that the data is approximately stable. We give modified algorithms that use this structure to output a clustering very close to the optimum, using no more communication than the approximation threshold from section 3. We focus on k-median, but we append the details for k-center and \"p\" for p < log n. We give a two-phase algorithm for k-median with approximate stability. The high structure of the algorithm is similar to algorithm 1: first, each machine clusters its local score and sends the weighted centers to the coordinator. Then, the coordinator performs a weighted clustering algorithm to output the algorithms, which is executed by the machines and the coordinator."}, {"heading": "6 Distributed Clustering under Spectral Stability", "text": "In this section, we specify a distributed clustering algorithm under spectral stability (logarithm). Recall the definition of matrices A and C from Section 2. Our distributed algorithm for spectral stability works as follows: \u2212 Logarithm for k-means developed in Section 3. Finally, we perform a naturally distributed version of the popular Lloyd's heuristics for a few rounds of convergence to near-optimal centers. We achieve the following theorem 10. Let A be a data matrix satisfying - spectral stability. Then, Algorithm 4 on A outputs centers 1, 2,."}, {"heading": "7 Locally Consistent Clustering", "text": "In this section, we show how to output a near-optimal local cluster without communication, provided that the data is roughly in line with global stability. First, we provide a few motivating examples. In view of different hospitals, each of which has data on their own patients, each patient is at low or high risk for certain diseases, and there is a (unknown) reason for truth clustering of patients, corresponding to their risk for the diseases. We want to conduct a representative survey, with at least one patient from each cluster and each hospital. One solution is to perform a distributed clustering algorithm to approximate the basic truth clustering using (mkd) communication, and then randomly pick the patients from each cluster. However, in Theorem 12, we show that if the data is globally structured, we can perform a clustering algorithm on each hospital, then we stab a patient from each cluster in each hospital. Another example is when each hospital wants to decide whether to donate blood to the others."}, {"heading": "8 Conclusion", "text": "We present a simple and general framework for distributed clustering with outliers. We give an algorithm for k clustering for each \"p target with z-outliers using O-bit (m (k + z) (d + log n)) communication, answer an open question [31] and improve on the previous best approximation ratio [12]. For distributed k-mean clustering, we give the first dimensional communication complexity that is lower to find optimal clustering, and improve on the lower limit of Chen et al. [18]. Our lower limit applies even to stable data. An interesting open question is to extend this result to any\" p object. \"We show how to improve the quality of clustering produced, provided the data meet certain natural notions of stability, in particular approximation stability and spectral stability [8, 27]. In certain applications that only require locally consistent clustering, we show that approximation communication is not necessary."}, {"heading": "A Proof from Section 4", "text": "To prove Theorem 6, we use the following direct sum theory.Theorem 13 = ky. [22] 2 Suppose we calculate the mean value of m data points in X dimensions, with each machine containing k points. Then, the communication costs are to be calculated even if the clustering value is promised to satisfy the stability of communication (1 + \u03b1,) approximate values for all \u03b1 points in d dimensions, where each machine contains k points. (mkd log (md)))), even if the clustering values promise to achieve (1 + \u03b1,) approximate stability for all \u03b1 points, or \u03b3 spectral stability for all x dimensions. Proof. Given a protocol used for calculating the optimal clustering instance with B bits of communication, and an instance X of the mean estimation of Yint problems with m machines, each with a kd-dimensional datapint, we solve the average problem."}, {"heading": "B Proofs from Section 5", "text": "Lemma 14 [8] Given a graph G over good clusters X1,. Xk and bad points B, with the following properties: 1. For all u, v in the same Xi, edge (u, v) is in E (G).2 The theorem is in relation to calculating the true mean of a Gaussian distribution, but the proof applies to any function f: [\u2212 1] d that is linear across dimensions (such as the mean function).3This problem is achieved by making Lemma 3.6 and Theorem 3.9 [8].2 For u, Xi, v, Xj, so that i = j, then (u, v) / E (Xi), beyond that, u and v do not share a common neighbor in G.Then we let C (v1),., C (vk) denote the output of the running algorithm 2 on G."}], "references": [{"title": "k-means++ under approximation stability", "author": ["Manu Agarwal", "Ragesh Jaiswal", "Arindam Pal"], "venue": "Theoretical Computer Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Better guarantees for k-means and euclidean k-median by primal-dual algorithms", "author": ["Sara Ahmadian", "Ashkan Norouzi-Fard", "Ola Svensson", "Justin Ward"], "venue": "arXiv preprint arXiv:1612.07925,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Local search heuristics for k-median and facility location problems", "author": ["Vijay Arya", "Naveen Garg", "Rohit Khandekar", "Adam Meyerson", "Kamesh Munagala", "Vinayaka Pandit"], "venue": "SIAM Journal on Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Center based clustering: A foundational perspective", "author": ["Pranjal Awasthi", "Maria-Florina Balcan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Center-based clustering under perturbation stability", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "Information Processing Letters,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Spectral embedding of k-cliques, graph partitioning and k-means", "author": ["Pranjal Awasthi", "Moses Charikar", "Ravishankar Krishnaswamy", "Ali Kemal Sinop"], "venue": "In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Improved spectral-norm bounds for clustering. In Approximation, Randomization, and Combinatorial Optimization", "author": ["Pranjal Awasthi", "Or Sheffet"], "venue": "Algorithms and Techniques,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Clustering under approximation stability", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "k-center clustering under perturbation resilience", "author": ["Maria-Florina Balcan", "Nika Haghtalab", "Colin White"], "venue": "In Proceedings of the Annual International Colloquium on Automata, Languages, and Programming (ICALP),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Clustering under perturbation resilience", "author": ["Maria Florina Balcan", "Yingyu Liang"], "venue": "SIAM Journal on Computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Distributed k-means and kmedian clustering on general topologies", "author": ["Maria-Florina F Balcan", "Steven Ehrlich", "Yingyu Liang"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Distributed balanced clustering via mapping coresets", "author": ["Mohammadhossein Bateni", "Aditya Bhaskara", "Silvio Lattanzi", "Vahab Mirrokni"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Are stable instances easy? Combinatorics", "author": ["Yonatan Bilu", "Nathan Linial"], "venue": "Probability and Computing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "A tight bound for set disjointness in the message-passing model", "author": ["Mark Braverman", "Faith Ellen", "Rotem Oshman", "Toniann Pitassi", "Vinod Vaikuntanathan"], "venue": "In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "An improved approximation for k-median, and positive correlation in budgeted optimization", "author": ["Jaros  law Byrka", "Thomas Pensyl", "Bartosz Rybicki", "Aravind Srinivasan", "Khoa Trinh"], "venue": "In Proceedings of the Annual Symposium on Discrete Algorithms (SODA),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "A constant-factor approximation algorithm for the k-median problem", "author": ["Moses Charikar", "Sudipto Guha", "\u00c9va Tardos", "David B Shmoys"], "venue": "In Proceedings of the Annual Symposium on Theory of Computing (STOC),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Algorithms for facility location problems with outliers", "author": ["Moses Charikar", "Samir Khuller", "David M Mount", "Giri Narasimhan"], "venue": "In Proceedings of the Annual Symposium on Discrete Algorithms (SODA),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Communication-optimal distributed clustering", "author": ["Jiecao Chen", "He Sun", "David Woodruff", "Qin Zhang"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "A constant factor approximation algorithm for k-median clustering with outliers", "author": ["Ke Chen"], "venue": "In Proceedings of the Annual Symposium on Discrete Algorithms (SODA),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "A probabilistic analysis of em for mixtures of separated, spherical gaussians", "author": ["Sanjoy Dasgupta", "Leonard Schulman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Data driven resource allocation for distributed learning", "author": ["Travis Dick", "Mu Li", "Venkata Krishna Pillutla", "Colin White", "Maria Florina Balcan", "Alex Smola"], "venue": "In \u201dProceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)\u201d,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "On communication cost of distributed statistical estimation and dimensionality", "author": ["Ankit Garg", "Tengyu Ma", "Huy Nguyen"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Clustering to minimize the maximum intercluster distance", "author": ["Teofilo F Gonzalez"], "venue": "Theoretical Computer Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1985}, {"title": "Simpler analyses of local search algorithms for facility location", "author": ["Anupam Gupta", "Kanat Tangwongsan"], "venue": "arXiv preprint arXiv:0809.2554,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Decompositions of triangle-dense graphs", "author": ["Rishi Gupta", "Tim Roughgarden", "C Seshadhri"], "venue": "In Proceedings of the 5th conference on Innovations in theoretical computer science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "The spectral method for general mixture models", "author": ["Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala"], "venue": "SIAM Journal on Computing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Clustering with spectral norm and the k-means algorithm", "author": ["Amit Kumar", "Ravindran Kannan"], "venue": "In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Improved distributed principal component analysis", "author": ["Yingyu Liang", "Maria-Florina F Balcan", "Vandana Kanchanapally", "David Woodruff"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Approximation algorithms for geometric median problems", "author": ["Jyh-Han Lin", "Jeffrey Scott Vitter"], "venue": "Information Processing Letters,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1992}, {"title": "A bi-criteria approximation algorithm for k means", "author": ["Konstantin Makarychev", "Yury Makarychev", "Maxim Sviridenko", "Justin Ward"], "venue": "In APPROX,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Fast distributed k-center clustering with outliers on massive data", "author": ["Gustavo Malkomes", "Matt J Kusner", "Wenlin Chen", "Kilian Q Weinberger", "Benjamin Moseley"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Spectral partitioning of random graphs", "author": ["Frank McSherry"], "venue": "In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2001}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J Schulman", "Chaitanya Swamy"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "How to round subspaces: A new spectral clustering algorithm", "author": ["Ali Kemal Sinop"], "venue": "In Proceedings of the Annual Symposium on Discrete Algorithms (SODA),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "On lloyds algorithm: new theoretical insights for clustering in practice", "author": ["Cheng Tang", "Claire Monteleoni"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "The communication complexity of addition", "author": ["Emanuele Viola"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "[12] and Malkomes et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "which is dimension-agnostic [18].", "startOffset": 28, "endOffset": 32}, {"referenceID": 7, "context": "[8] or the spectral stability condition of Kumar and Kannan [27].", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[8] or the spectral stability condition of Kumar and Kannan [27].", "startOffset": 60, "endOffset": 64}, {"referenceID": 2, "context": "Finding approximation algorithms to different clustering objectives and variants has attracted significant attention in the computer science community [3, 15, 16, 17, 19, 23, 30].", "startOffset": 151, "endOffset": 178}, {"referenceID": 14, "context": "Finding approximation algorithms to different clustering objectives and variants has attracted significant attention in the computer science community [3, 15, 16, 17, 19, 23, 30].", "startOffset": 151, "endOffset": 178}, {"referenceID": 15, "context": "Finding approximation algorithms to different clustering objectives and variants has attracted significant attention in the computer science community [3, 15, 16, 17, 19, 23, 30].", "startOffset": 151, "endOffset": 178}, {"referenceID": 16, "context": "Finding approximation algorithms to different clustering objectives and variants has attracted significant attention in the computer science community [3, 15, 16, 17, 19, 23, 30].", "startOffset": 151, "endOffset": 178}, {"referenceID": 18, "context": "Finding approximation algorithms to different clustering objectives and variants has attracted significant attention in the computer science community [3, 15, 16, 17, 19, 23, 30].", "startOffset": 151, "endOffset": 178}, {"referenceID": 22, "context": "Finding approximation algorithms to different clustering objectives and variants has attracted significant attention in the computer science community [3, 15, 16, 17, 19, 23, 30].", "startOffset": 151, "endOffset": 178}, {"referenceID": 29, "context": "Finding approximation algorithms to different clustering objectives and variants has attracted significant attention in the computer science community [3, 15, 16, 17, 19, 23, 30].", "startOffset": 151, "endOffset": 178}, {"referenceID": 10, "context": "Therefore, distributed clustering algorithms have gained popularity over the past few years [11, 12, 31].", "startOffset": 92, "endOffset": 104}, {"referenceID": 11, "context": "Therefore, distributed clustering algorithms have gained popularity over the past few years [11, 12, 31].", "startOffset": 92, "endOffset": 104}, {"referenceID": 30, "context": "Therefore, distributed clustering algorithms have gained popularity over the past few years [11, 12, 31].", "startOffset": 92, "endOffset": 104}, {"referenceID": 10, "context": "Recent work in the theoretical machine learning community establishes guarantees on the clusterings produced in distributed settings for certain problems [11, 12, 31].", "startOffset": 154, "endOffset": 166}, {"referenceID": 11, "context": "Recent work in the theoretical machine learning community establishes guarantees on the clusterings produced in distributed settings for certain problems [11, 12, 31].", "startOffset": 154, "endOffset": 166}, {"referenceID": 30, "context": "Recent work in the theoretical machine learning community establishes guarantees on the clusterings produced in distributed settings for certain problems [11, 12, 31].", "startOffset": 154, "endOffset": 166}, {"referenceID": 30, "context": "provide distributed algorithms for k-center and k-center with outliers [31], and Bateni et al.", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "introduce distributed algorithms for capacitated k-clustering under any `p objective [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "However, these approximation ratios are unavoidable in the worst case due to existing lower bounds, for example, k-center cannot be approximated to a factor smaller than 2\u2212 even on a single machine [23].", "startOffset": 198, "endOffset": 202}, {"referenceID": 4, "context": "To go beyond these worst-case results, many recent works have studied natural structure that exists in real-world instances, and shown that algorithms can output a clustering very close to optimal under these natural stability assumptions [5, 7, 8, 10, 13, 21, 27, 37].", "startOffset": 239, "endOffset": 268}, {"referenceID": 6, "context": "To go beyond these worst-case results, many recent works have studied natural structure that exists in real-world instances, and shown that algorithms can output a clustering very close to optimal under these natural stability assumptions [5, 7, 8, 10, 13, 21, 27, 37].", "startOffset": 239, "endOffset": 268}, {"referenceID": 7, "context": "To go beyond these worst-case results, many recent works have studied natural structure that exists in real-world instances, and shown that algorithms can output a clustering very close to optimal under these natural stability assumptions [5, 7, 8, 10, 13, 21, 27, 37].", "startOffset": 239, "endOffset": 268}, {"referenceID": 9, "context": "To go beyond these worst-case results, many recent works have studied natural structure that exists in real-world instances, and shown that algorithms can output a clustering very close to optimal under these natural stability assumptions [5, 7, 8, 10, 13, 21, 27, 37].", "startOffset": 239, "endOffset": 268}, {"referenceID": 12, "context": "To go beyond these worst-case results, many recent works have studied natural structure that exists in real-world instances, and shown that algorithms can output a clustering very close to optimal under these natural stability assumptions [5, 7, 8, 10, 13, 21, 27, 37].", "startOffset": 239, "endOffset": 268}, {"referenceID": 20, "context": "To go beyond these worst-case results, many recent works have studied natural structure that exists in real-world instances, and shown that algorithms can output a clustering very close to optimal under these natural stability assumptions [5, 7, 8, 10, 13, 21, 27, 37].", "startOffset": 239, "endOffset": 268}, {"referenceID": 26, "context": "To go beyond these worst-case results, many recent works have studied natural structure that exists in real-world instances, and shown that algorithms can output a clustering very close to optimal under these natural stability assumptions [5, 7, 8, 10, 13, 21, 27, 37].", "startOffset": 239, "endOffset": 268}, {"referenceID": 0, "context": "states that any c-approximation to the clustering objective is -close to the target clustering [1, 8, 25], and the spectral stability condition of Kumar and Kannan is a deterministic generalization of many generative clustering models [27, 34, 35].", "startOffset": 95, "endOffset": 105}, {"referenceID": 7, "context": "states that any c-approximation to the clustering objective is -close to the target clustering [1, 8, 25], and the spectral stability condition of Kumar and Kannan is a deterministic generalization of many generative clustering models [27, 34, 35].", "startOffset": 95, "endOffset": 105}, {"referenceID": 24, "context": "states that any c-approximation to the clustering objective is -close to the target clustering [1, 8, 25], and the spectral stability condition of Kumar and Kannan is a deterministic generalization of many generative clustering models [27, 34, 35].", "startOffset": 95, "endOffset": 105}, {"referenceID": 26, "context": "states that any c-approximation to the clustering objective is -close to the target clustering [1, 8, 25], and the spectral stability condition of Kumar and Kannan is a deterministic generalization of many generative clustering models [27, 34, 35].", "startOffset": 235, "endOffset": 247}, {"referenceID": 33, "context": "states that any c-approximation to the clustering objective is -close to the target clustering [1, 8, 25], and the spectral stability condition of Kumar and Kannan is a deterministic generalization of many generative clustering models [27, 34, 35].", "startOffset": 235, "endOffset": 247}, {"referenceID": 34, "context": "states that any c-approximation to the clustering objective is -close to the target clustering [1, 8, 25], and the spectral stability condition of Kumar and Kannan is a deterministic generalization of many generative clustering models [27, 34, 35].", "startOffset": 235, "endOffset": 247}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "For example, for k-median, we achieve a (6\u03b1 + 2 + )-approximation by plugging in the bicriteria algorithm of Lin and Vitter [29] (adding an O(log n) factor to the communication cost), as opposed to a 32\u03b1approximation from Bateni et al.", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "By plugging in the approximation algorithm for k-median with outliers [19], our algorithm achieves the first constant approximation for distributed k-median with outliers, answering an open question posed by Malkomes et al.", "startOffset": 70, "endOffset": 74}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "gave a communication complexity lower bound of \u03a9(mk) for obtaining any c-approximation for distributed clustering [18].", "startOffset": 114, "endOffset": 118}, {"referenceID": 21, "context": "We close this gap using a directsum reduction from a recent result on distributed mean estimation [22], proving an \u03a9\u0303(mkd) lower bound for finding the optimal k-means centers.", "startOffset": 98, "endOffset": 102}, {"referenceID": 8, "context": "[9], this result is optimal with respect to the value of \u03b1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16], and the current best approximation ratio is 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "For k-center, there is a tight 2-approximation algorithm [23].", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "357 [2], and Makarychev et al.", "startOffset": 4, "endOffset": 7}, {"referenceID": 29, "context": "recently showed a bicriteria algorithm with strong guarantees [30].", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "For clustering with outliers, there is a 3-approximation algorithm for kcenter with z outliers, as well as a bicriteria 4(1 + 1/ )-approximation algorithm for k-median that picks (1+ )z outliers [17].", "startOffset": 195, "endOffset": 199}, {"referenceID": 18, "context": "Chen found a true constant factor approximation algorithm for k-median (the constant is not explicitly computed) [19].", "startOffset": 113, "endOffset": 117}, {"referenceID": 10, "context": "showed a coreset construction for k-median and k-means, which leads to a clustering algorithm with \u00d5(mkd) communication, and also studied more general graph topologies for distributed computing [11].", "startOffset": 194, "endOffset": 198}, {"referenceID": 11, "context": "indroduced a construction for mapping coresets, which admits a distributed clustering algorithm that can handle balance constraints with communication cost \u00d5(mk) [12].", "startOffset": 162, "endOffset": 166}, {"referenceID": 30, "context": "showed a distributed 13- and 4- approximation algorithm for k-center with and without outliers, respectively [31].", "startOffset": 109, "endOffset": 113}, {"referenceID": 17, "context": "studied clustering under the broadcast model of distributed computing, and also proved a communication complexity lower bound of \u03a9(mk) for distributed clustering [18], building on a recent lower bound for setdisjointness in the message-passing model [14].", "startOffset": 162, "endOffset": 166}, {"referenceID": 13, "context": "studied clustering under the broadcast model of distributed computing, and also proved a communication complexity lower bound of \u03a9(mk) for distributed clustering [18], building on a recent lower bound for setdisjointness in the message-passing model [14].", "startOffset": 250, "endOffset": 254}, {"referenceID": 21, "context": "showed a communication complexity lower bound for computing the mean of d-dimensional points [22].", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "who showed an algorithm which utilizes the structure to output a nearly optimal clustering [8].", "startOffset": 91, "endOffset": 94}, {"referenceID": 8, "context": "stability, and they proved a matching lower bound, namely that (2\u2212 \u03b4, 0)-approximation stability is NP-hard for any \u03b4 unless NP = RP [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 26, "context": "Kumar and Kannan introduced a spectral stability condition which generalized many generative models [27], including Gaussian mixture-models [20, 26], the Planted Partition model [32], as well as deterministic conditions [33].", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "Kumar and Kannan introduced a spectral stability condition which generalized many generative models [27], including Gaussian mixture-models [20, 26], the Planted Partition model [32], as well as deterministic conditions [33].", "startOffset": 140, "endOffset": 148}, {"referenceID": 25, "context": "Kumar and Kannan introduced a spectral stability condition which generalized many generative models [27], including Gaussian mixture-models [20, 26], the Planted Partition model [32], as well as deterministic conditions [33].", "startOffset": 140, "endOffset": 148}, {"referenceID": 31, "context": "Kumar and Kannan introduced a spectral stability condition which generalized many generative models [27], including Gaussian mixture-models [20, 26], the Planted Partition model [32], as well as deterministic conditions [33].", "startOffset": 178, "endOffset": 182}, {"referenceID": 32, "context": "Kumar and Kannan introduced a spectral stability condition which generalized many generative models [27], including Gaussian mixture-models [20, 26], the Planted Partition model [32], as well as deterministic conditions [33].", "startOffset": 220, "endOffset": 224}, {"referenceID": 6, "context": "This work was later improved along several axes, including the dependence on k in the condition, by Awasthi and Sheffet [7].", "startOffset": 120, "endOffset": 123}, {"referenceID": 17, "context": "study distributed algorithms for graph partitioning when the graphs satisfy a notion of stability relating internal expansion of the k pieces to the external expansion [18].", "startOffset": 168, "endOffset": 172}, {"referenceID": 5, "context": "It is known that such graphs also satisfy spectral stability under a suitable Euclidean embedding [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "In fact, this distinction only changes the cost of the optimal clustering by at most a factor of 2 when p = 1, 2, or \u221e [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 13, "context": "In fact, the two models are equivalent up to small factors in the communication complexity [14].", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "For more details, see [12, 31].", "startOffset": 22, "endOffset": 30}, {"referenceID": 30, "context": "For more details, see [12, 31].", "startOffset": 22, "endOffset": 30}, {"referenceID": 11, "context": "This generalizes previous distributed clustering results [12, 31], and answers an open question of Malkomes et al.", "startOffset": 57, "endOffset": 65}, {"referenceID": 30, "context": "This generalizes previous distributed clustering results [12, 31], and answers an open question of Malkomes et al.", "startOffset": 57, "endOffset": 65}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "It has been pointed out that every clustering algorithm we are aware of has this property [12].", "startOffset": 90, "endOffset": 94}, {"referenceID": 28, "context": "Setting B to be the ( logn , 1 + )-bicriteria approximation algorithm for k-median [29], the approximation ratio becomes 6\u03b1+ 2 + , which improves over the 32\u03b1 approximation ratio of Bateni et al.", "startOffset": 83, "endOffset": 87}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "If we set A as the current best k-median algorithm [15], we achieve a distributed (18.", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "If instead we plug in the sequential approximation algorithm for k-median with z outliers [19], we obtain the first constant-factor approximation algorithm for k-median with outliers, answering an open question from Malkomes et al.", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "We can also use the results from Gupta and Tangwongsan [24] to obtain an O(1)-approximation algorithm for 1 < p < log n.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "First we bound the sum of the local optimal (k+ z)-clustering on each machine by the global clustering with outliers in the following lemma (a non-outlier version of this lemma appears in [12]).", "startOffset": 188, "endOffset": 192}, {"referenceID": 17, "context": "[18], who showed a lower bound independent of the dimension.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "1 in [18] because k-clustering with z outliers is a k + z eligible function: it evaluates to 0 if there are at most k + z points, otherwise it is greater than 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 35, "context": "The communication complexity needed to compute the sum of m numbers, each on a different machine, is \u03a9(m) [36].", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "Now we use a direct-sum theorem to generalize this result to d-dimensional numbers in Euclidean space [22].", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "Then we add offsets 4i \u00b7 [1]d to each vector Xi.", "startOffset": 25, "endOffset": 28}, {"referenceID": 21, "context": "[22] (included in", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Appendix A) and Viola [36].", "startOffset": 22, "endOffset": 26}, {"referenceID": 7, "context": "[8] Given a (1+\u03b1, )-approximation stable clustering instance (V, d) for k-median, then Property 1: For all x, for \u2264 x n \u03b1 points v, d(v, cv) \u2265 \u03b1wavg/(x ).", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Even in the single machine setting, it is NP-hard to find the exact solution to k-center under (2 \u2212 , 0)-approximation stability unless NP = RP [9], therefore our result is tight with respect to the level of stability.", "startOffset": 144, "endOffset": 147}, {"referenceID": 27, "context": "This can be done in a distributed manner [28].", "startOffset": 41, "endOffset": 45}, {"referenceID": 6, "context": "The correctness of the algorithm then follows from previous work [7, 27] that shows that in T = log(\u2016A\u2016 ) steps the cluster centers will be recovered to accuracy.", "startOffset": 65, "endOffset": 72}, {"referenceID": 26, "context": "The correctness of the algorithm then follows from previous work [7, 27] that shows that in T = log(\u2016A\u2016 ) steps the cluster centers will be recovered to accuracy.", "startOffset": 65, "endOffset": 72}, {"referenceID": 27, "context": "[28] bounds the communication cost of computing k-SVD to be O(mkd).", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "1: Run the distributed algorithm from [28] to compute \u00c2i\u2019s, i.", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "A notion related to spectral stability has been recently studied [18] for distributed graph partitioning.", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "design communication efficient distributed algorithms to cluster such stable graphs [18].", "startOffset": 84, "endOffset": 88}, {"referenceID": 5, "context": "In fact, it was shown that stable graphs in the above sense also satisfy spectral stability under an appropriate Euclidean embedding of the nodes of the graph [6].", "startOffset": 159, "endOffset": 162}, {"referenceID": 17, "context": "study a setting where the edges are partitioned across machines and hence their result is formally incomparable to ours [18].", "startOffset": 120, "endOffset": 124}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "We give an algorithm for k-clustering for any `p objective with z outliers using \u00d5(m(k + z)(d + log n)) bits of communication, answering an open question [31] and improving over the previous best approximation ratio [12].", "startOffset": 154, "endOffset": 158}, {"referenceID": 11, "context": "We give an algorithm for k-clustering for any `p objective with z outliers using \u00d5(m(k + z)(d + log n)) bits of communication, answering an open question [31] and improving over the previous best approximation ratio [12].", "startOffset": 216, "endOffset": 220}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "We show how to improve the quality of the clustering produced, provided the data satisfies certain natural notions of stability, specifically, approximation stability and spectral stability [8, 27].", "startOffset": 190, "endOffset": 197}, {"referenceID": 26, "context": "We show how to improve the quality of the clustering produced, provided the data satisfies certain natural notions of stability, specifically, approximation stability and spectral stability [8, 27].", "startOffset": 190, "endOffset": 197}], "year": 2017, "abstractText": "As datasets become larger and more distributed, algorithms for distributed clustering have become more and more important. In this work, we present a general framework for designing distributed clustering algorithms that are robust to outliers. Using our framework, we give a distributed approximation algorithm for k-means, k-median, or generally any `p objective, with z outliers and/or balance constraints, using O(m(k+ z)(d+ log n)) bits of communication, where m is the number of machines, n is the size of the point set, and d is the dimension. This generalizes and improves over the previous work of Bateni et al. [12] and Malkomes et al. [31]. As a special case, we achieve the first distributed algorithm for k-median with outliers, answering an open question posed by Malkomes et al. [31]. For distributed k-means clustering, we provide the first dimension-dependent communication complexity lower bound for finding the optimal clustering. This improves over the lower bound of Chen et al. which is dimension-agnostic [18]. Furthermore, we give distributed clustering algorithms which return nearly optimal solutions, provided the data satisfies the approximation stability condition of Balcan et al. [8] or the spectral stability condition of Kumar and Kannan [27]. In certain clustering applications where each machine only needs to find a clustering consistent with the global optimum, we show that no communication is necessary if the data satisfies approximation stability. \u2217Authors\u2019 addresses: pranjal.awasthi@rutgers.edu, ninamf@cs.cmu.edu, crwhite@cs.cmu.edu. This work was supported in part by NSF grants CCF-1422910, CCF-1535967, IIS-1618714, a Sloan Research Fellowship, a Microsoft Research Faculty Fellowship, a Google Research Award, and a National Defense Science & Engineering Graduate (NDSEG) fellowship. ar X iv :1 70 3. 00 83 0v 1 [ cs .D S] 2 M ar 2 01 7", "creator": "LaTeX with hyperref package"}}}