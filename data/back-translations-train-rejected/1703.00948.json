{"id": "1703.00948", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "DAWT: Densely Annotated Wikipedia Texts across multiple languages", "abstract": "In this work, we open up the DAWT dataset - Densely Annotated Wikipedia Texts across multiple languages. The annotations include labeled text mentions mapping to entities (represented by their Freebase machine ids) as well as the type of the entity. The data set contains total of 13.6M articles, 5.0B tokens, 13.8M mention entity co-occurrences. DAWT contains 4.8 times more anchor text to entity links than originally present in the Wikipedia markup. Moreover, it spans several languages including English, Spanish, Italian, German, French and Arabic. We also present the methodology used to generate the dataset which enriches Wikipedia markup in order to increase number of links. In addition to the main dataset, we open up several derived datasets including mention entity co-occurrence counts and entity embeddings, as well as mappings between Freebase ids and Wikidata item ids. We also discuss two applications of these datasets and hope that opening them up would prove useful for the Natural Language Processing and Information Retrieval communities, as well as facilitate multi-lingual research.", "histories": [["v1", "Thu, 2 Mar 2017 20:55:20 GMT  (4036kb,D)", "http://arxiv.org/abs/1703.00948v1", "8 pages, 3 figures, 7 tables, WWW2017, WWW 2017 Companion proceedings"]], "COMMENTS": "8 pages, 3 figures, 7 tables, WWW2017, WWW 2017 Companion proceedings", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL cs.SI", "authors": ["nemanja spasojevic", "preeti bhargava", "guoning hu"], "accepted": false, "id": "1703.00948"}, "pdf": {"name": "1703.00948.pdf", "metadata": {"source": "CRF", "title": "DAWT: Densely Annotated Wikipedia Texts across multiple languages", "authors": ["Nemanja Spasojevic", "Preeti Bhargava", "Guoning Hu"], "emails": ["guoning.hu}@lithium.com"], "sections": [{"heading": null, "text": "Keywords Wiki, Wikipedia, Freebase, Freebase notes, Wikipedia notes, Wikification, Named Entity Recognition, Entity Disambiguation, Entity Linking"}, {"heading": "1. INTRODUCTION", "text": "Over the past decade, the amount of data available to businesses has grown exponentially, but much of this data is unstructured or in free form, also known as Dark Data1. These data hold challenges for Natural Language Processing (NLP) and Information Retrieval (IR) tasks unless the text is labeled semantically. Two tasks that are particularly important for the IR community are: 1https: / en.wikipedia.org / wiki / Dark _ data."}, {"heading": "2. PROBLEM STATEMENT", "text": "The Wikifixing problem was introduced by Mihalcea et al. [13], where the task was to introduce hyperlinks to the correct Wikipedia articles for a specific mention. In Wikipedia, only the first mention is linked or commented on. In our task, we focus on the condensation of the annotations, i.e. denser hyperlinks from references in Wikipedia articles to other Wikipedia articles. The ultimate goal is to have high-precision hyperlinks with relatively high memory value that could be used further as a basis for other NLP tasks. For most of the monitored machine learning or NLP tasks, one of the challenges is to collect basic truths on a large scale. In this work, we try to solve the problem of generating a marked data set on a large scale with the following limitations: \u2022 The associated entity IDs must be unified across different languages. Freebase is therefore the machine-summarizing with Wikipedia and the machine-summarizing with multiple IDs."}, {"heading": "3. CONTRIBUTIONS", "text": "Our contributions in this work are: \u2022 We extract a comprehensive inventory of mentions from multiple domains. \u2022 We condense the linkages of entities in Wikipedia documents 4.8-fold. \u2022 The DAWT dataset includes, in addition to English, several other languages such as Arabic, French, German, Italian, and Spanish. \u2022 Finally, we open this dataset and several other entrenched datasets (such as mention counts, entity counts, mention codes, entity word2vec, and mappings between freebase IDs and Wikidata IDs) for the benefit of the IR and NLP communities."}, {"heading": "4. KNOWLEDGE BASE", "text": "We prefer Freebase as our KB, because in Freebase the same ID is a unique entity in multiple languages. For more general use, we have also provided the mapping of Freebase ID to Wikipedia shortcuts and Wikidata IDs (see Section 6.4). Due to the limited resources and usefulness of the entities, our KB contains about 1 million of the most important entities from all Freebase entities. This gives us a good balance between the reach and relevance of the entities for processing popular social media text. To this end, we calculate an entity importance score [2] using linear regression with properties that capture popularity within Wikipedia shortcuts, and the importance of the entity within the freebase shortcuts."}, {"heading": "5. DAWT DATA SET GENERATION", "text": "Indeed, in recent years, the number of those able to establish themselves in the US has multiplied, both in the US and in Europe."}, {"heading": "6. DERIVED DATASETS", "text": "We also derive several other datasets from the DAWT dataset that we are discussing here."}, {"heading": "6.1 Mention Occurrences", "text": "This dataset contains the raw data for mentioning Mi in our corpus and KB. Table 2 shows the raw data for mentioning \"Apple,\" \"Apple\" and \"Tesla.\""}, {"heading": "6.2 Entity Occurrences", "text": "This dataset contains the raw deposit counts for one unit Ej in our corpus and KB. Table 3 shows the raw data for units Apple Inc., Apple (fruit) and Nikola Tesla. We also create separate dictionaries for each language. Table 4 shows the different surface shape variations and occurrence counts of the same unit in different languages."}, {"heading": "6.3 Mention To Entity Co-occurrences", "text": "This data set contains the number of mentions and entities that occur simultaneously, which is particularly useful for estimating the previous probability of mentioning Mi, which refers to a candidate unit Ej in relation to our KB and corpora. Table 5 shows the raw and normalized mentions entities for mentions \"Apple\" and \"Apple\" and various candidate units. As is evident, the probability that \"Apple\" refers to the entity Apple Inc. is higher than the entity Apple (fruit). However, \"Apple\" most likely refers to the entity Apple (fruit). Likewise, the mention \"Tesla\" most likely refers to the entity Nikola Tesla."}, {"heading": "6.4 Freebase id to Wikidata id Mappings", "text": "In this dataset, we use the Freebase ID to represent a unit. To facilitate studies with Wikidata IDs, which are also widely used in literature, we provide a dataset that maps individual Freebase IDs to Wikidata IDs. This dataset contains twice as many mappings as Google3. A summary comparison between these two mapping IDs is presented in Table 6, which lists the total number of mappings in 4 groups: \u2022 Same: A Freebase Id maps to the same Wikidata Id. \u2022 Differently: A Freebase Id maps to different Wikidata Ids. \u2022 DAWT Only: A Freebase Id maps to a Wikidataid only to a Wikidataid in DAWT.3https: / / developopers.google.com / freebase # freebase-wikidata mappings \u2022 Google Only: A Freebase Id to a Wikidata Id only to a Wikidataid in DA.3https: / / freebase Id maps to the same Wikidata Id in DAWT.3https."}, {"heading": "6.5 Entity Embeddings", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "7. APPLICATIONS OF THE DATASET", "text": "As already mentioned, the DAWT and other derivative datasets we have described in this post have several applications for the NLP and IR communities, including: 6The data is available at http: / / nlp.stanford.edu / data / glove.6B.zip 7The data set is available at http: / / www.fit.vutbr.cz / ~ imikolov / rnnlm / word-test.v1.txt."}, {"heading": "7.1 Named Entity Recognition (NER)", "text": "For this purpose, the Mention Occurcence, Entity Occurcence and Mention To Entity Co-occurcence datasets described in sections 6.1, 6.2 and 6.3 are extremely useful. For example, the number of mentions and probabilities in a dictionary can be stored and used to extract the mentions in a document. In addition, the number of mentions of a mentioning event Mi and its number of mentions with an entity Ej can be used to calculate the previous probability of mentioning the entity: number (Mi \u2192 Ej) number (Mi) This can be used to determine the candidates for a mentioning from our KB."}, {"heading": "7.2 Entity Disambiguation and Linking (EDL)", "text": "This task involves linking the mention with its correct KB entity. For each mention, there may be several candidates with previous probabilities as calculated in the NER. In addition, other characteristics derived from these data sets may include entity coincidences, entity word2vec similarity and lexical similarity between mention and entity surface shape. These characteristics can be used to train a supervised learning algorithm to associate mention with the correct unique entity among all candidates, as happened in [1]. The approach applied in [1] uses several such context-dependent and independent characteristics and has an accuracy of 63%, a memory of 87% and an F-score of 73%."}, {"heading": "8. ACCESSING THE DATASET", "text": "The DAWT dataset and derivative datasets are available for download from this page: https: / / github.com / klout / opendata / tree / master / wiki _ annotation. The DAWT dataset was generated using Wikipedia Data Dumps dated 20 January 2017. Statistics on the dataset are shown in Table 1."}, {"heading": "9. RELATED WORK", "text": "While much work has focused on the construction and opening of such records, very few have addressed all the challenges and constraints we set out in Section 2. Spitkovsky and Chang [22] opened a linguistic dictionary (the English Wikipedia article) with 175,100,788 mentions that included links to 7,560,141 units. This record, though extremely valuable, represents mention - entity mappings across a mixture of all languages that makes it more difficult to be used for a particular language. Furthermore, this work used raw data that is useful but does not mention context (such as previous and successful tokens, etc.) that have a bigoted effect while performing EDL. The freebase annotation of the ClueWeb dataset 8 contains 647 million English websites with an average of 13 entities commented per document and 456 million documents that comment on at least one entity."}, {"heading": "10. CONCLUSION AND FUTURE WORK", "text": "In this thesis, we opened the DAWT dataset - Density Annotated Wikipedia Texts across multiple languages. The annotations include labeled text mentions mapping to entities (represented by their freebase machine ids) as well as the type of entity. The dataset comprises a total of 13.6M articles, 5.0B tokens, 13.8M mention entity co-occurrences. DAWT contains 4.8 times more anchor text to entity links than originally included in the Wikipedia markup. In addition, it includes multiple languages, including English, Spanish, Italian, German, French and Arabic. We also presented the methodology used to generate the dataset, which enriched the Wikipedia markup to increase the number of links. In addition to the main dataset, we opened several derivative datasets for mentioning occurrences, entity-occurrence, entity-co-occurence, entity-occurence, and data that we hope will be included in this dataset."}, {"heading": "11. REFERENCES", "text": "In Proceedings of the 26th International Conference Companion on on World Wide Web, page to appear. International World Wide Web Conferences Steering Committee, 2017. [2] P. Bhattacharyya and N. Spasojevic. Global Entity Ranking across multiple languages. In Proceedings of the 26th International Conference on World Wide Web Conferences Steering Committee, 2017. [3] Z. Cai, K. Zhao, K. Q. Zhu, and H. Wang. Wikification via link co-occurrence. In Proceedings of the 22nd ACM international Conference on Conference on Information and Knowledge Management, CIKM '13, pp. 1087-1096, New York, NY, USA, 2013."}, {"heading": "A. SAMPLE ANNOTATED WIKIPEDIA TEXTS FROM DAWT", "text": "Figure 2 shows examples of the heavily commented Wikipediapages of the companies Nikola Tesla and Tesla Motors in English and Arabic."}], "references": [{"title": "High-throughput and language-agnostic entity disambiguation and linking on user generated data", "author": ["P. Bhargava", "N. Spasojevic", "G. Hu"], "venue": "In Proceedings of the 26th International Conference Companion on World Wide Web. International World Wide Web Conferences Steering Committee,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Global entity ranking across multiple languages", "author": ["P. Bhattacharyya", "N. Spasojevic"], "venue": "In Proceedings of the 26th International Conference on World Wide Web, page to appear. International World Wide Web Conferences Steering Committee,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Wikification via link co-occurrence", "author": ["Z. Cai", "K. Zhao", "K.Q. Zhu", "H. Wang"], "venue": "In Proceedings of the 22nd ACM international conference on Conference on information and knowledge management,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Relational inference for wikification", "author": ["X. Cheng", "D. Roth"], "venue": "Urbana, 51:61801,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Large-scale named entity disambiguation based on wikipedia data", "author": ["S. Cucerzan"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American society for information science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "Tagme: on-the-fly annotation of short text fragments (by wikipedia entities)", "author": ["P. Ferragina", "U. Scaiella"], "venue": "In Proceedings of the 19th ACM international conference on Information and knowledge management,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Collective annotation of wikipedia entities in web text", "author": ["S. Kulkarni", "A. Singh", "G. Ramakrishnan", "S. Chakrabarti"], "venue": "In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Dbpedia-a large-scale, multilingual knowledge base extracted from wikipedia", "author": ["J. Lehmann", "R. Isele", "M. Jakob", "A. Jentzsch", "D. Kontokostas", "P.N. Mendes", "S. Hellmann", "M. Morsey", "P. van Kleef", "S. Auer"], "venue": "Semantic Web,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Cross-language entity linking", "author": ["P. McNamee", "J. Mayfield", "D. Lawrie", "D.W. Oard", "D.S. Doermann"], "venue": "In IJCNLP,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Mining meaning from wikipedia", "author": ["O. Medelyan", "D. Milne", "C. Legg", "I.H. Witten"], "venue": "International Journal of Human-Computer Studies,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Wikify!: Linking documents to encyclopedic knowledge", "author": ["R. Mihalcea", "A. Csomai"], "venue": "In Proceedings of the Sixteenth ACM Conference on  Conference on Information and Knowledge Management,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In Proceedings of NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Learning to link with wikipedia", "author": ["D. Milne", "I.H. Witten"], "venue": "In Proceedings of the 17th ACM Conference on Information and Knowledge Management,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Local and global algorithms for disambiguation to wikipedia", "author": ["L. Ratinov", "D. Roth", "D. Downey", "M. Anderson"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Topic modeling for wikipedia link disambiguation", "author": ["B. Skaggs", "L. Getoor"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Mining half a billion topical experts across multiple social networks", "author": ["N. Spasojevic", "P. Bhattacharyya", "A. Rao"], "venue": "Social Network Analysis and Mining,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Lasta: Large scale topic assignment on multiple social networks", "author": ["N. Spasojevic", "J. Yan", "A. Rao", "P. Bhattacharyya"], "venue": "In Proc. of ACM Conference on Knowledge Discovery and Data Mining (KDD), KDD", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "A cross-lingual dictionary for english wikipedia", "author": ["V.I. Spitkovsky", "A.X. Chang"], "venue": "Language Resources and Evaluation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}], "referenceMentions": [{"referenceID": 19, "context": "These tasks play a critical role in the construction of a high quality information network which can be further leveraged for a variety of IR and NLP tasks such as text categorization, topical interest and expertise modeling of users [20, 21].", "startOffset": 234, "endOffset": 242}, {"referenceID": 20, "context": "These tasks play a critical role in the construction of a high quality information network which can be further leveraged for a variety of IR and NLP tasks such as text categorization, topical interest and expertise modeling of users [20, 21].", "startOffset": 234, "endOffset": 242}, {"referenceID": 7, "context": "Wikipedia precedes other OpenData projects like Freebase [8] and DBpedia [10] which were built on the foundation of Wikipedia.", "startOffset": 57, "endOffset": 60}, {"referenceID": 9, "context": "Wikipedia precedes other OpenData projects like Freebase [8] and DBpedia [10] which were built on the foundation of Wikipedia.", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "[13], where task was to introduce hyperlinks to the correct wikipedia articles for a given mention.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "To this end, we calculate an entity importance score [2] using linear regression with features capturing popularity within Wikipedia links, and importance of the entity within Freebase.", "startOffset": 53, "endOffset": 56}, {"referenceID": 5, "context": ", vector space representations of words [6, 14, 15].", "startOffset": 40, "endOffset": 51}, {"referenceID": 13, "context": ", vector space representations of words [6, 14, 15].", "startOffset": 40, "endOffset": 51}, {"referenceID": 14, "context": ", vector space representations of words [6, 14, 15].", "startOffset": 40, "endOffset": 51}, {"referenceID": 16, "context": "[17] proposed a model, GloVe, which learns word embeddings with both global matrix factorization and local context windowing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "This task is commonly used to evaluate embeddings [14, 15, 17] by answering the following question: Given word/entity X, Y, and Z, what is the word/entity that is similar to Z in the same sense as Y is similar to X? For example, given word \"Athens\", \"Greece\", and \"Paris\", the right answer is \"France\".", "startOffset": 50, "endOffset": 62}, {"referenceID": 14, "context": "This task is commonly used to evaluate embeddings [14, 15, 17] by answering the following question: Given word/entity X, Y, and Z, what is the word/entity that is similar to Z in the same sense as Y is similar to X? For example, given word \"Athens\", \"Greece\", and \"Paris\", the right answer is \"France\".", "startOffset": 50, "endOffset": 62}, {"referenceID": 16, "context": "This task is commonly used to evaluate embeddings [14, 15, 17] by answering the following question: Given word/entity X, Y, and Z, what is the word/entity that is similar to Z in the same sense as Y is similar to X? For example, given word \"Athens\", \"Greece\", and \"Paris\", the right answer is \"France\".", "startOffset": 50, "endOffset": 62}, {"referenceID": 13, "context": "Here we used the test data provided by [14].", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "These features can be used to train a supervised learning algorithm to link the mention to the correct disambiguated entity among all the candidate entities as done in [1].", "startOffset": 168, "endOffset": 171}, {"referenceID": 0, "context": "The approach used in [1] employs several such context dependent and independent features and has a precision of 63%, recall of 87% and an F-score of 73%.", "startOffset": 21, "endOffset": 24}, {"referenceID": 21, "context": "Spitkovsky and Chang [22] opened a cross-lingual dictionary (of English Wikipedia Articles) containing 175,100,788 mentions linking to 7,560,141 entities.", "startOffset": 21, "endOffset": 25}, {"referenceID": 11, "context": "Another related technique for generating such dictionaries is Wikification [12, 16, 4] where mentions in Wikipedia pages are linked to the disambiguated entities\u2019 Wikipedia pages.", "startOffset": 75, "endOffset": 86}, {"referenceID": 15, "context": "Another related technique for generating such dictionaries is Wikification [12, 16, 4] where mentions in Wikipedia pages are linked to the disambiguated entities\u2019 Wikipedia pages.", "startOffset": 75, "endOffset": 86}, {"referenceID": 3, "context": "Another related technique for generating such dictionaries is Wikification [12, 16, 4] where mentions in Wikipedia pages are linked to the disambiguated entities\u2019 Wikipedia pages.", "startOffset": 75, "endOffset": 86}, {"referenceID": 4, "context": "by comparing the relatedness of candidate Wiki articles with the mentions [5, 7, 19] while in global approach entities across the entire document are disambiguated together using document context, thus, ensuring consistency of entities across the document [9, 18].", "startOffset": 74, "endOffset": 84}, {"referenceID": 6, "context": "by comparing the relatedness of candidate Wiki articles with the mentions [5, 7, 19] while in global approach entities across the entire document are disambiguated together using document context, thus, ensuring consistency of entities across the document [9, 18].", "startOffset": 74, "endOffset": 84}, {"referenceID": 18, "context": "by comparing the relatedness of candidate Wiki articles with the mentions [5, 7, 19] while in global approach entities across the entire document are disambiguated together using document context, thus, ensuring consistency of entities across the document [9, 18].", "startOffset": 74, "endOffset": 84}, {"referenceID": 8, "context": "by comparing the relatedness of candidate Wiki articles with the mentions [5, 7, 19] while in global approach entities across the entire document are disambiguated together using document context, thus, ensuring consistency of entities across the document [9, 18].", "startOffset": 256, "endOffset": 263}, {"referenceID": 17, "context": "by comparing the relatedness of candidate Wiki articles with the mentions [5, 7, 19] while in global approach entities across the entire document are disambiguated together using document context, thus, ensuring consistency of entities across the document [9, 18].", "startOffset": 256, "endOffset": 263}, {"referenceID": 2, "context": "[3] achieved 89.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "[11] introduced the problem of cross-language entity linking.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "In this work, we open up the DAWT dataset Densely Annotated Wikipedia Texts across multiple languages. The annotations include labeled text mentions mapping to entities (represented by their Freebase machine ids) as well as the type of the entity. The data set contains total of 13.6M articles, 5.0B tokens, 13.8M mention entity co-occurrences. DAWT contains 4.8 times more anchor text to entity links than originally present in the Wikipedia markup. Moreover, it spans several languages including English, Spanish, Italian, German, French and Arabic. We also present the methodology used to generate the dataset which enriches Wikipedia markup in order to increase number of links. In addition to the main dataset, we open up several derived datasets including mention entity co-occurrence counts and entity embeddings, as well as mappings between Freebase ids and Wikidata item ids. We also discuss two applications of these datasets and hope that opening them up would prove useful for the Natural Language Processing and Information Retrieval communities, as well as facilitate multi-lingual research.", "creator": "TeX"}}}