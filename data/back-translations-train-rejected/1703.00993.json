{"id": "1703.00993", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "A Comparative Study of Word Embeddings for Reading Comprehension", "abstract": "The focus of past machine learning research for Reading Comprehension tasks has been primarily on the design of novel deep learning architectures. Here we show that seemingly minor choices made on (1) the use of pre-trained word embeddings, and (2) the representation of out-of-vocabulary tokens at test time, can turn out to have a larger impact than architectural choices on the final performance. We systematically explore several options for these choices, and provide recommendations to researchers working in this area.", "histories": [["v1", "Thu, 2 Mar 2017 23:58:54 GMT  (698kb,D)", "http://arxiv.org/abs/1703.00993v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bhuwan dhingra", "hanxiao liu", "ruslan salakhutdinov", "william w cohen"], "accepted": false, "id": "1703.00993"}, "pdf": {"name": "1703.00993.pdf", "metadata": {"source": "CRF", "title": "A Comparative Study of Word Embeddings for Reading Comprehension", "authors": ["Bhuwan Dhingra", "Hanxiao Liu", "Ruslan Salakhutdinov", "William W. Cohen"], "emails": ["wcohen}@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Systems that can read documents and answer questions about their contents are a key language technology. The field known as reading comprehension has attracted enormous interest over the past two years, mainly due to the introduction of large-format annotated datasets, such as CNN (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016). Strong statistical models, including deep learning models (also referred to as readers), have been proposed for RC, most of which use the following recipe: (1) Tokens in the document and in the question are represented using word vectors derived from a lookup table (either initialized randomly or from a pre-formed source such as GloVe). (2) A sequence model such as LSTM (Hochreiter and Schmidhuber, 1997), supplemented with an attention mechanism."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 RC Datasets & Models", "text": "Many data sets aimed at measuring the performance of RC have been proposed (Nguyen et al., 2016; Trischler et al., 2016). For our purposes, we select two of these benchmarks from different areas - Who-Did-What (WDW) (Onishi et al., 2016), constructed from news, and the Children's Book Test (CBT et al., 2015), constructed from children's books. For CBT, we consider only those questions where the answer is a designated entity (CBT-NE et al., 2016). Several RC models based on deep learning have been proposed (Cui et al., 2016; Munkhdalai et al., 2016; Sordoni et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang et al., 2016."}, {"heading": "2.2 Word Embedding methods", "text": "The two most popular methods of inducing text embedding from text corpora are GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013). These packages also offer non-help embedding (OTS), which is trained on large corporations.3 While the GloVe package offers embedding of different sizes (50-300), word2vec only offers embedding of size 300. This is an important difference, which we will discuss in more detail later. We also train three additional embedding specified in Table 1, including those trained on the target datasets.2 In summary, we test with two in-domain corpora for WDW: one large (OTS) and one small (WDW) and two in-domain corpora for CBT: one large (BT) and one small (CBT) embedding for the target datasets themselves, which means that both CBT / 2BT / 3BT / 3BT are converted into capital letters."}, {"heading": "3 Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Comparison of Word Embeddings", "text": "rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfteeteerteerrrrrrrrrrrrrrrrrrrrrsrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rf\u00fc the rfu the rfu the rrrf\u00fc the rrrf\u00fc the rrrrrreteeteeeteeeeeeeeeeeeeeeeeeSrteeeSrteeeeSrteerrrrrrteeeeeerrrrrrrrrrteeeeeeeeeeeSrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "3.2 Handling OOV tokens", "text": "rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rtef\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the r"}, {"heading": "4 Conclusions", "text": "We have shown that the choice of pre-formed embeddings to initialize word vectors has a significant impact on the performance of neural models for reading comprehension. The same applies to the method of handling OOV tokens at test time. We argue that different architectures can only be compared if these decisions are controlled. Based on our experiments, we recommend using commercially available GloVe embeddings and mapping pre-formed GloVe vectors, if available, or random but unique vectors at test dates."}, {"heading": "Acknowledgments", "text": "This work was funded by NSF as part of CCF14030 and Google Research."}, {"heading": "A Answer Selection for Stanford AR", "text": "Using the notation from (Chen et al., 2016), we omit p \u04411, p \u04412,.., p \u0441m to be the contextual embedding of the tokens in the document, and let us be the attention-weighted document representation, then we calculate the probability that tokens i will answer the question as follows: P (a = di | d, q) = si = softmax (p \u0447Ti o) (1) The probability of a particular candidate c \u0441C as an answer is then calculated by aggregating the probabilities of all tokens appearing in c and renormalizing them over the candidates: Pr (c | d, q), where I (c, d) si (2) I (c, d) is the set of positions at which a token appears in c in document d."}, {"heading": "B Hyperparameter Details", "text": "For the WDW dataset, we use the hidden state variable d = 128 for the GRU and the dropout with p = 0.3. For the CBT-NE dataset, we use d = 128 and the dropout with p = 0.4. Stanford AR has only one layer as suggested in the original paper, while the GA reader has 3 layers. For the Stanford AR dropout, we use the input of the layer, and for the GA reader, it is applied between layers. The embedding sizes for the word vectors have been set to dw = 100 for all experiments, except those that use standard word2vec embedding. To make a fair comparison, we do not use the qe-comm function for Stanford AR, which was used in the implementation of the GA reader. Since our purpose is to study the effect of word vectors, we do not use the validation with Volograms in our experiments."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Embracing data abundance: Booktest dataset for reading comprehension", "author": ["Ondrej Bajgar", "Rudolf Kadlec", "Jan Kleindienst."], "venue": "arXiv preprint arXiv:1610.00956 .", "citeRegEx": "Bajgar et al\\.,? 2016", "shortCiteRegEx": "Bajgar et al\\.", "year": 2016}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1606.02858 .", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Attention-overattention neural networks for reading comprehension", "author": ["Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu."], "venue": "arXiv preprint arXiv:1607.04423 .", "citeRegEx": "Cui et al\\.,? 2016", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William W Cohen", "Ruslan Salakhutdinov."], "venue": "arXiv preprint arXiv:1606.01549 .", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Tracking the world state with recurrent entity networks", "author": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun."], "venue": "arXiv preprint arXiv:1612.03969 .", "citeRegEx": "Henaff et al\\.,? 2016", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1684\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1511.02301 .", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "arXiv preprint arXiv:1603.01547 .", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Dynamic entity representations with max-pooling improves machine reading", "author": ["Sosuke Kobayashi", "Ran Tian", "Naoaki Okazaki", "Kentaro Inui."], "venue": "NAACL-HLT .", "citeRegEx": "Kobayashi et al\\.,? 2016", "shortCiteRegEx": "Kobayashi et al\\.", "year": 2016}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Neural semantic encoders", "author": ["Tsendsuren Munkhdalai", "Hong Yu."], "venue": "arXiv preprint arXiv:1607.04315 .", "citeRegEx": "Munkhdalai and Yu.,? 2016", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "Ms marco: A human generated machine reading comprehension dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "arXiv preprint arXiv:1611.09268 .", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Who did what: A large-scale person-centered cloze dataset", "author": ["Takeshi Onishi", "Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester."], "venue": "EMNLP .", "citeRegEx": "Onishi et al\\.,? 2016", "shortCiteRegEx": "Onishi et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "arXiv preprint arXiv:1606.05250 .", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "arXiv preprint arXiv:1611.01603 .", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "author": ["Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen."], "venue": "arXiv preprint arXiv:1609.05284 .", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Iterative alternating neural attention for machine reading", "author": ["Alessandro Sordoni", "Phillip Bachman", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1606.02245 .", "citeRegEx": "Sordoni et al\\.,? 2016", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "Newsqa: A machine comprehension dataset", "author": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1611.09830 .", "citeRegEx": "Trischler et al\\.,? 2016", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "arXiv preprint arXiv:1608.07905 .", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Multi-perspective context matching for machine comprehension", "author": ["Zhiguo Wang", "Haitao Mi", "Wael Hamza", "Radu Florian."], "venue": "arXiv preprint arXiv:1612.04211 .", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "arXiv preprint arXiv:1611.01604 .", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "End-to-end answer chunk extraction and ranking for reading comprehension", "author": ["Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou."], "venue": "arXiv preprint arXiv:1610.09996 .", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "years, primarily due to the introduction of largescale annotated datasets, such as CNN (Hermann et al., 2015) and SQuAD (Rajpurkar et al.", "startOffset": 87, "endOffset": 109}, {"referenceID": 19, "context": ", 2015) and SQuAD (Rajpurkar et al., 2016).", "startOffset": 18, "endOffset": 42}, {"referenceID": 18, "context": "Powerful statistical models, including deep learning models (also termed as readers), have been proposed for RC, most of which employ the following recipe: (1) Tokens in the document and question are represented using word vectors obtained from a lookup table (either initialized randomly, or from a pre-trained source such as GloVe (Pennington et al., 2014)).", "startOffset": 333, "endOffset": 358}, {"referenceID": 9, "context": "model such as LSTM (Hochreiter and Schmidhuber, 1997), augmented with an attention mechanism (Bahdanau et al.", "startOffset": 19, "endOffset": 53}, {"referenceID": 0, "context": "model such as LSTM (Hochreiter and Schmidhuber, 1997), augmented with an attention mechanism (Bahdanau et al., 2014), updates these vectors to produce contextual representations.", "startOffset": 93, "endOffset": 116}, {"referenceID": 1, "context": "Corpus 1: BookTest dataset (Bajgar et al., 2016), Corpus 2: Wikipedia + Gigaword.", "startOffset": 27, "endOffset": 48}, {"referenceID": 2, "context": "As a concrete example, in Figure 1 we compare the performance of two RC models\u2014Stanford Attentive Reader (AR) (Chen et al., 2016) and Gated Attention (GA) Reader (Dhingra et al.", "startOffset": 110, "endOffset": 129}, {"referenceID": 5, "context": ", 2016) and Gated Attention (GA) Reader (Dhingra et al., 2016)\u2014on the Who-Did-What dataset (Onishi et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 17, "context": ", 2016)\u2014on the Who-Did-What dataset (Onishi et al., 2016), initialized with word embeddings trained on different corpora.", "startOffset": 36, "endOffset": 57}, {"referenceID": 2, "context": "(Chen et al., 2016; Shen et al., 2016)) is to replace infrequent words during training with a special token UNK, and use this to-", "startOffset": 0, "endOffset": 38}, {"referenceID": 21, "context": "(Chen et al., 2016; Shen et al., 2016)) is to replace infrequent words during training with a special token UNK, and use this to-", "startOffset": 0, "endOffset": 38}, {"referenceID": 16, "context": "Many datasets aimed at measuring the performance of RC have been proposed (Nguyen et al., 2016; Trischler et al., 2016).", "startOffset": 74, "endOffset": 119}, {"referenceID": 23, "context": "Many datasets aimed at measuring the performance of RC have been proposed (Nguyen et al., 2016; Trischler et al., 2016).", "startOffset": 74, "endOffset": 119}, {"referenceID": 8, "context": ", 2016) constructed from news stories, and the Children\u2019s Book Test (CBT) (Hill et al., 2015) constructed from children\u2019s books.", "startOffset": 74, "endOffset": 93}, {"referenceID": 4, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 15, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 22, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 21, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 12, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 6, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 24, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 25, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 20, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 26, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 27, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 10, "context": "Hence, we instead select the answer from the document representation itself, followed by an attention sum mechanism (Kadlec et al., 2016).", "startOffset": 116, "endOffset": 137}, {"referenceID": 3, "context": "architecture which updates the representation of document tokens through multiple bidirectional GRU layers (Cho et al., 2014).", "startOffset": 107, "endOffset": 125}, {"referenceID": 5, "context": "We use the publicly available code2 with the default hyperparameter settings of (Dhingra et al., 2016), detailed in Appendix B.", "startOffset": 80, "endOffset": 102}, {"referenceID": 18, "context": "The two most popular methods for inducing word embeddings from text corpora are GloVe (Pennington et al., 2014) and word2vec (Mikolov et al.", "startOffset": 86, "endOffset": 111}, {"referenceID": 14, "context": ", 2014) and word2vec (Mikolov et al., 2013).", "startOffset": 21, "endOffset": 43}, {"referenceID": 13, "context": "This is by no means an optimal choice, in fact previous studies (Levy et al., 2015) have shown that hyperparameter choices may have a significant impact on downstream performance.", "startOffset": 64, "endOffset": 83}, {"referenceID": 13, "context": "It is difficult to claim that one method is better than the other, since previous studies (Levy et al., 2015) have shown that these methods are sensitive to hyperparameter tuning.", "startOffset": 90, "endOffset": 109}, {"referenceID": 2, "context": "(Chen et al., 2016; Shen et al., 2016)) for constructing this vocabulary is to decide on a minimum frequency threshold n (typically 510) and set V = V T n .", "startOffset": 0, "endOffset": 38}, {"referenceID": 21, "context": "(Chen et al., 2016; Shen et al., 2016)) for constructing this vocabulary is to decide on a minimum frequency threshold n (typically 510) and set V = V T n .", "startOffset": 0, "endOffset": 38}, {"referenceID": 5, "context": "A third approach, used in (Dhingra et al., 2016), is motivated by the fact that many of the RC models rely on computing fine-grained similarity between document and query tokens.", "startOffset": 26, "endOffset": 48}], "year": 2017, "abstractText": "The focus of past machine learning research for Reading Comprehension tasks has been primarily on the design of novel deep learning architectures. Here we show that seemingly minor choices made on (1) the use of pre-trained word embeddings, and (2) the representation of outof-vocabulary tokens at test time, can turn out to have a larger impact than architectural choices on the final performance. We systematically explore several options for these choices, and provide recommendations to researchers working in this area.", "creator": "LaTeX with hyperref package"}}}