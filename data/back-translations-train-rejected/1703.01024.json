{"id": "1703.01024", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Exponential Moving Average Model in Parallel Speech Recognition Training", "abstract": "As training data rapid growth, large-scale parallel training with multi-GPUs cluster is widely applied in the neural network model learning currently.We present a new approach that applies exponential moving average method in large-scale parallel training of neural network model. It is a non-interference strategy that the exponential moving average model is not broadcasted to distributed workers to update their local models after model synchronization in the training process, and it is implemented as the final model of the training system. Fully-connected feed-forward neural networks (DNNs) and deep unidirectional Long short-term memory (LSTM) recurrent neural networks (RNNs) are successfully trained with proposed method for large vocabulary continuous speech recognition on Shenma voice search data in Mandarin. The character error rate (CER) of Mandarin speech recognition further degrades than state-of-the-art approaches of parallel training.", "histories": [["v1", "Fri, 3 Mar 2017 03:14:28 GMT  (119kb)", "http://arxiv.org/abs/1703.01024v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xu tian", "jun zhang", "zejun ma", "yi he", "juan wei"], "accepted": false, "id": "1703.01024"}, "pdf": {"name": "1703.01024.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["wj80290}@alibaba-inc.com"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.01 024v 1 [cs.C L] 3M ar2 01with multi-GPUs cluster is widely applied in the neural network model learning now. We present a new approach that applies exponentially sliding average methods in the large-scale parallel training of the neural network model. It is a non-disruptive strategy that the exponentially sliding average model is not transferred to distributed workers to update their local models after model synchronization in the training process, and it is implemented as the final model of the training system. Fully networked feed-forward neural networks (DNNs) and deep unidirectional long-term short-term memory (LSTM) recursive neural networks (RNNNs) are successfully trained with proposed methods for large continuous speech recognition on Shenma language search data in Mandarin. Character error rate (CER) of Mandarin language recognition continues to deteriorate as advanced approaches of parallel training."}, {"heading": "1. INTRODUCTION", "text": "In recent years, the number of new arrivals in Germany has multiplied, so that the number of new arrivals has slightly increased compared to previous years."}, {"heading": "2. EXPONENTIAL MOVING AVERAGE MODEL", "text": "In recognition applications, neural mesh parameters are trained for classification. It is also an optimization problem: Argmin \u03b81tt \u2211 i = 1 (L (f\u03b8 (x), y). Where t are the number of data points, (x, y) the input data and the corresponding target, L is the loss function and f\u03b8 the network. Let us mention the parameters that minimize empirical costs. Recognition training on a large scale has to deal with the optimization problem of billions of training data and makes it difficult to find \u03b8. SGD and its variants are presented promising learning results for a large-scale optimization problem and are becoming the most popular methods of deep learning."}, {"heading": "2.1. Model averaging and block-wise model updating filter", "text": "The complete training data set is divided into N-splits without overlapping and is distributed over N-GPU. Each GPU optimizes the local model in parallel to a split of the training data set. After a mini-batch training, the global model is updated, and it is calculated using model mean or BMUF, and then transferred to GPU to update its local models. For the model averaging method, all local models are synchronized and averaged, and then the aggregated model \u03b8 (t) is sent back to GPU [14, 15]. For the BMUF method, the global model \u0445g (t) is used in the block averaging method instead of the model averaging method. The synchronization and updating process of the model \u0442g (t) in the BMUF is updated as follows:"}, {"heading": "2.2. Moving Average and Exponential Moving Average", "text": "The averaged SGD is proposed to further accelerate the convergence rate of the SGD. The averaged SGD uses the moving average (MA) \u03b8 as an estimate of the SGD. [17] It is shown that the sufficiently large training data set can be well converged within the framework of a single GPU training. It can be considered a non-interference strategy that \u03b8 does not participate in the main optimization process and takes effect only after the end of the entire optimization. However, for the parallel implementation of the training, each of this data set is calculated by modelling and BMUF with multiple models, and the movement of the average model is not well converged compared to individual GPU training sessions."}, {"heading": "3. EXPERIMENTS AND RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Training Data", "text": "In order to demonstrate the performance of our proposed method, we have trained an acoustic model for LVCSR. A large amount of marked data is required to train a more precise acoustic model. We collect 17,000 hours of marked data from Shenma voice search, which is one of the most popular mobile search engines in China. The data set is compiled from anonymous searches by online users in Mandarin, and the sampling rate of all audio files is 16 kHz recorded by mobile phones. This data set consists of many different conditions, such as varied noise, even low signal-to-noise, chatter, dialects, accents, hesitation, etc. The data set is divided into training set, validation set, and test set, and the amount of which is displayed in Table 1.The three sets are divided by speaker to avoid utterances of the same speaker appearing in three sentences at the same time."}, {"heading": "3.2. Experimental setup", "text": "The LSTM RNNs exceed conventional RNNs for voice recognition systems, especially deep LSTM RNNNs, due to their extensive dependencies more accurately for timing conditions [19, 20]. Shenma Voice Search is a streaming service that displays intermedia detection results while users are still speaking. So, when it comes to online real-time detection, we prefer unidirectional LSTM models rather than bidirectional ones. Therefore, the parallel training process is unidirectional LSTM-Based.A 28-dimensional filter base function is extracted for each frame, and is associated with the difference of the first and second order of the server as the final input of the network. The architecture we have trained consists of two LSTM layers with sigmoid activation function followed by a full connectivity layer. The out CE layer is a Softmax layer with 11088 hidden Markov model as a cross-state (MI) bound class (MI)."}, {"heading": "3.3. Results", "text": "The test set of approximately 9000 samples contains various real-world conditions and simulates the majority of user scenarios and could also evaluate the performance of a trained model. Therefore, the BMUF-based approach, which does not perform worse than the single GPU training method, is the baseline of the experiments. Therefore, the results of the MA and EMA methods based on the BMUF are calculated according to each epoch, and we call them MA-based methods. As the EMA is a non-interference method, the performance cannot be evaluated using the real-time FER. Therefore, the FER is calculated on validation sets according to each epoch. In order to present the decoding performance of the MAbased methods, we extract 4 temporary models from each epoch to visualize the degradation of the CER. FER curves of the LSTM models trained using BMUF, MA and EMA methods are shown in Figure 1 that the framework accuracy of the MA is significantly higher."}, {"heading": "4. CONCLUSION", "text": "It is shown that unidirectional LSTM and DNN models trained with the EMA method have better decoding results than those of the BMUF, and traditional moving average methods for large vocabulary continue to use Mandarin language recognition. Our future work includes 1) applying this method to CNN, Connectionist Temporal Classification (CTC), attention-based neural networks and other hybrid deep neural network architectures; 2) extending this method from framework discriminatory training to discriminatory training such as maximum mutual information (MMI) and segmental minimum Bayes risk (sMBR); 3) developing further approaches to parallel training with better performance."}, {"heading": "5. REFERENCES", "text": "[1] Alex Graves and Navdeep Jaitly et, \"Towards end-to-endspeech networks with recurrent neural networks with second neural networks,\" in ICML, 2014, vol. 14, pp. 1764-1772. [2] Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo-hamed, \"Hybrid speech recognition with deep bidirectional lstm,\" in Automatic Speech Recognition and Understanding (ASRU), 2013, pp. 273-278. [3] Dario Amodei, Rishita Anubhai, Eric Battenberg, CarlCase, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, et al. \"Deep speech 2: End-to-end speech recognition in English and Mandarin.\""}], "references": [{"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "ICML, 2014, vol. 14, pp. 1764\u20131772.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos"], "venue": "arXiv preprint arXiv:1512.02595, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770\u2013778.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four  research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "Acoustics, speech and signal processing (icassp), 2013 ieee international conference on. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "AndrewW Senior", "Fran\u00e7oise Beaufays"], "venue": "Interspeech, 2014, pp. 338\u2013342.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Highway long short-term memory rnns for distant speech recognition", "author": ["Yu Zhang", "Guoguo Chen", "Dong Yu", "Kaisheng Yaco", "Sanjeev Khudanpur", "James Glass"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5755\u20135759.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "Advances in neural information processing systems, 2012, pp. 1223\u20131231.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Asynchronous stochastic gradient descent for dnn training", "author": ["Shanshan Zhang", "Ce Zhang", "Zhao You", "Rong Zheng", "Bo Xu"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 6660\u20136663.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Revisiting distributed synchronous sgd", "author": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "venue": "arXiv preprint arXiv:1604.00981, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed training strategies for the structured perceptron", "author": ["Ryan McDonald", "Keith Hall", "Gideon Mann"], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2010, pp. 456\u2013464.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Parallelized stochastic gradient descent", "author": ["Martin Zinkevich", "Markus Weimer", "Lihong Li", "Alex J Smola"], "venue": "Advances in neural information processing systems, 2010, pp. 2595\u20132603.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise modelupdate filtering", "author": ["Kai Chen", "Qiang Huo"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5880\u20135884.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["Boris T Polyak", "Anatoli B Juditsky"], "venue": "SIAM Journal  on Control and Optimization, vol. 30, no. 4, pp. 838\u2013 855, 1992.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1992}, {"title": "Towards optimal one pass large scale learning with averaged stochastic gradient descent", "author": ["Wei Xu"], "venue": "arXiv preprint arXiv:1107.2490, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Michiel Hermans", "Benjamin Schrauwen"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 190\u2013198.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Acoustic modelling with cd-ctcsmbr lstm rnns", "author": ["Ha\u015fim Sak", "F\u00e9lix de Chaumont Quitry", "Tara Sainath", "Kanishka Rao"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on. IEEE, 2015, pp. 604\u2013609.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Fran\u00e7oise Beaufays"], "venue": "arXiv preprint arXiv:1507.06947, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].", "startOffset": 142, "endOffset": 151}, {"referenceID": 1, "context": "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].", "startOffset": 142, "endOffset": 151}, {"referenceID": 2, "context": "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].", "startOffset": 142, "endOffset": 151}, {"referenceID": 3, "context": "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 4, "context": "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 5, "context": "Over the past few years, neural networks has been widely used in some domains, such as large vocabulary continuous speech recognition (LVCSR) [1, 2, 3], image recognition [4, 5] and neural machine translation [6].", "startOffset": 209, "endOffset": 212}, {"referenceID": 6, "context": "Fully-connected feed-forward deep neural networks (DNNs) and Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs have shown the effective performance for LVCSR [7, 8, 9, 10].", "startOffset": 188, "endOffset": 201}, {"referenceID": 7, "context": "Fully-connected feed-forward deep neural networks (DNNs) and Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs have shown the effective performance for LVCSR [7, 8, 9, 10].", "startOffset": 188, "endOffset": 201}, {"referenceID": 8, "context": "Fully-connected feed-forward deep neural networks (DNNs) and Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs have shown the effective performance for LVCSR [7, 8, 9, 10].", "startOffset": 188, "endOffset": 201}, {"referenceID": 9, "context": "Fully-connected feed-forward deep neural networks (DNNs) and Recurrent neural networks (RNNs), especially long short-term memory (LSTM) RNNs have shown the effective performance for LVCSR [7, 8, 9, 10].", "startOffset": 188, "endOffset": 201}, {"referenceID": 10, "context": "Asynchronous SGD is a successful attempt [11, 12], and it is shown that parallel training with asynchronous SGD can many times speedup without lowering the accuracy.", "startOffset": 41, "endOffset": 49}, {"referenceID": 11, "context": "Asynchronous SGD is a successful attempt [11, 12], and it is shown that parallel training with asynchronous SGD can many times speedup without lowering the accuracy.", "startOffset": 41, "endOffset": 49}, {"referenceID": 12, "context": "Besides, synchronous SGD is another positive effort, where the parameter server waits for every workers to finish their computation and send their local models to it, and then it sends updated model back to all workers [13].", "startOffset": 219, "endOffset": 223}, {"referenceID": 13, "context": "Model averaging is a method for large-scale parallel training, which the final model is averaged from all parameters of separated models [14, 15].", "startOffset": 137, "endOffset": 145}, {"referenceID": 14, "context": "Model averaging is a method for large-scale parallel training, which the final model is averaged from all parameters of separated models [14, 15].", "startOffset": 137, "endOffset": 145}, {"referenceID": 15, "context": "It can achieve improvement or nodegradation of recognition performance compared with minibatch SGD on single GPU [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 16, "context": "It is demonstrated that the performance of moving average of the parameters obtained by SGD is as good as that of the parameters which minimize the empirical cost, and moving average parameters can be used as the estimator of them, if there are enough training samples [17].", "startOffset": 269, "endOffset": 273}, {"referenceID": 17, "context": "One pass learning is then proposed, and it is the combination of learning rate schedule and averaged SGD using moving average [18].", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": "For model averaging method, all local models are synchronized and averaged, and then aggregated model \u03b8\u0304(t) is sent back to GPUs [14, 15].", "startOffset": 129, "endOffset": 137}, {"referenceID": 14, "context": "For model averaging method, all local models are synchronized and averaged, and then aggregated model \u03b8\u0304(t) is sent back to GPUs [14, 15].", "startOffset": 129, "endOffset": 137}, {"referenceID": 16, "context": "Averaged SGD leverages the moving average (MA) \u03b8\u0304 as the estimator of \u03b8\u2217 [17]:", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [19, 20].", "startOffset": 185, "endOffset": 193}, {"referenceID": 19, "context": "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [19, 20].", "startOffset": 185, "endOffset": 193}, {"referenceID": 20, "context": "Besides, frame stacking leads to reduce the computation and training time dramatically [21].", "startOffset": 87, "endOffset": 91}], "year": 2017, "abstractText": "As training data rapid growth, large-scale parallel training with multi-GPUs cluster is widely applied in the neural network model learning currently. We present a new approach that applies exponential moving average method in large-scale parallel training of neural network model. It is a non-interference strategy that the exponential moving average model is not broadcasted to distributed workers to update their local models after model synchronization in the training process, and it is implemented as the final model of the training system. Fully-connected feed-forward neural networks (DNNs) and deep unidirectional Long short-term memory (LSTM) recurrent neural networks (RNNs) are successfully trained with proposed method for large vocabulary continuous speech recognition on Shenma voice search data in Mandarin. The character error rate (CER) of Mandarin speech recognition further degrades than state-of-the-art approaches of parallel training.", "creator": "LaTeX with hyperref package"}}}