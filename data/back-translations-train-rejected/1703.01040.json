{"id": "1703.01040", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Learning Robot Activities from First-Person Human Videos Using Convolutional Future Regression", "abstract": "We design a new approach that allows robot learning of new activities from unlabeled human example videos. Given videos of humans executing the same activity from a human's viewpoint (i.e., first-person videos), our objective is to make the robot learn the temporal structure of the activity as its future regression network, and learn to transfer such model for its own motor execution. We present a new deep learning model: We extend the state-of-the-art convolutional object detection network for the detection of human hands in training videos based on image information, and newly introduce the concept of using a fully convolutional network to regress (i.e., predict) the intermediate scene representation corresponding to the future frame (e.g., 1-2 seconds later). Combining these allows direct prediction of future locations of human hands and objects, which enables the robot to infer the motor control plan using our manipulation network. We experimentally confirm that our approach makes learning of robot activities from unlabeled human interaction videos possible, and demonstrate that our robot is able to execute the learned collaborative activities in real-time directly based on its camera input.", "histories": [["v1", "Fri, 3 Mar 2017 05:27:50 GMT  (3229kb,D)", "https://arxiv.org/abs/1703.01040v1", null], ["v2", "Mon, 24 Jul 2017 08:02:11 GMT  (3108kb,D)", "http://arxiv.org/abs/1703.01040v2", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV cs.LG", "authors": ["jangwon lee", "michael s ryoo"], "accepted": false, "id": "1703.01040"}, "pdf": {"name": "1703.01040.pdf", "metadata": {"source": "CRF", "title": "Learning Robot Activities from First-Person Human Videos Using Convolutional Future Regression", "authors": ["Jangwon Lee", "Michael S. Ryoo"], "emails": ["mryoo}@indiana.edu"], "sections": [{"heading": null, "text": "In fact, most people who have lived and worked in the US in recent years are not able to outdo themselves, but are able to outlive themselves, \"he said in an interview with The New York Times, in which he addressed the question of how the situation came about.\" I don't think it will come to that, \"he said,\" but I think it will. \""}, {"heading": "II. RELATED WORK", "text": "In fact, most people who are able to survive themselves are able to survive themselves, most of them are not able to survive themselves, but they are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, most of them are not able to survive themselves, most of them are able to survive themselves, most of them are able to survive themselves, and most of them are able to survive themselves, and most of them are not able to survive themselves."}, {"heading": "III. APPROACH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. System Overview", "text": "Based on a sequence of current images, our goal is to (i) predict future hand locations and all interactive objects in front of the robot, and then (ii) generate robot control commands to move the robot's hands to the predicted hand locations. To achieve this goal, we use two components: the first component is a perception component consisting of two completely revolutionary neural networks: (1) an advanced version of the Single Shot MultiBox Detector (SSD) [6] to create a hand-based scene representation and estimate boundary fields, and (2) a future regression network to model how such intermediate scene representations should (should) change in future images; and the second component is a manipulation component that maps 2-D hand locations in the image with the actual motor control using fully connected layers. The key idea of our approach is that the motion component proposed by the camera can be moved later on (1)."}, {"heading": "B. Perception Component", "text": "The target of our perception component is the prediction of future hand-held locations Y-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T"}, {"heading": "Hand representa,on network: t", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Hand representa,on network: t+\u0394", "text": "We are formulating this problem as a regression problem."}, {"heading": "C. Manipulation Component", "text": "Although our perception component is able to predict the future hand locations of humans in first-person animation videos, it is insufficient for robot manipulation, where we construct another regression network (m) to match the predicted 2-D hand locations in the image coordinate to the actual motor control commands. The main assumption is that a video image from a robot's camera will have a similar angle to our training data (human first person videos), which allows us to use the learned model for future robot hand locations (Z-T + T) assuming: Y-Rt'Y-T (7), where Y-Rt represents robot hand locations. Our manipulation component (m) predicts future robot hand locations (Z-T + T) in the face of current robot hand locations (Z-T), robot hand locations (Y-Rt), Y-hand locations, and future hand locations (T) to say the hands (T) where to move."}, {"heading": "IV. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Datasets", "text": "Our approach consists of three different types of networks (within the two components), and we use three different types of data sets for training each model. EgoHands [23]: This is a public data set of 48 first-person videos of people engaging in four types of activities (playing cards, playing chess, solving a puzzle, and playing Jenga). It has 4,800 frames with 15,053 ground truth labels. Here, we have added 466 frames with 1,267 basic truths annotations to the original data set to cover more postures. We use this data set to learn our hand representation, which is trained to locate handboxes in a video frame. Unlabeled Human-Human Interaction Videos: We collected a total of 47 first-person videos of human-collaboration scenarios, with each video clip ranging from 4 to 10 seconds."}, {"heading": "B. Baselines", "text": "To make quantitative comparisons, we compared our perceptual component with four different baselines: (i) Handmade representation uses a handmade state representation based on explicit object recognition and hand recognition. It encodes the relative distances between all interactive objects in our two scenarios and uses them to predict future hand position using neural network-based regression. More specifically, it detects objects using KAZE features [24] and hands using CNN-based hand detector in [23], then calculates relative distances between all objects and hands for creating the state representation, which is a 20-dimensional vector. Then we have built a new network in which five fully connected levels are trained to train using the state representations on the same interaction dataset we use. (ii) Hands only uses hand locations for future regression, hand-based visualization, and we train hand-based locations exclusively on this hand-based prediction of other hand-held locations."}, {"heading": "EVALUATION OF FUTURE HAND PREDICTION", "text": "(iii) SSD with future annotations1 is a baseline that uses the original SSD model [6], which was trained on the basis of the EgoHands dataset. Instead of training the model to derive the current hand positions based on the input frame, we refined this model on the EgoHands dataset after changing the annotations of the dataset to have \"future\" hand positions instead of switching them to current hand positions. We also used 466 frames for this fine-tuning, as the original EgoHands dataset was insufficient for this training (too many repetitive hand movements). (iv) SSD with future annotations2 is a baseline that also uses the original SSD model, but we retrained this model from scratch. This time we changed all the annotations of the EgoHands dataset and then trained the same model for the SSD."}, {"heading": "C. Evaluation of our future hand prediction", "text": "We first evaluated the perceptual component of our approach in terms of precision, recall, and F measurement and compared it to the above baselines. In the first evaluation, we made our approach to predicting boundaries of human hands in the future framework given the current frame of the image. We measured the \"intersection between the ranges of each predicted box and the (future) ground truth. Only when the ratio was greater than 0.5, the predicted box was accepted as truly positive. In this experiment, we randomly divided the set of our human-human interaction videos into the training and test sets, so that 32 videos were used for training sets and the remaining 15 videos for test sets totaling 47 videos.Table I shows quantitative results of our future hand prediction. In this experiment, the plus-minus sign (\u00b1) standard deviation indicates, and K represents the number of frames we used as input for our regression network. Our approach was clearly observing the SSD, our 30 frame (i.e., the 3-D) location of the SSD."}, {"heading": "MEAN PIXEL DISTANCE BETWEEN GROUND TRUTH AND PREDICTED POSITIONS OF ALL HANDS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "MEAN PIXEL DISTANCE BETWEEN GROUND TRUTH AND PREDICTED POSITION OF RIGHT HAND", "text": "The size of the image plane was 1280 * 720. We measured this mean pixel pitch only when both the ground truths and the predictions are present in the same image. Table II shows the average pixel pitch errors for all four types of hands (my left, my right, your left and your right hand). Once again, we can confirm that our approaches far exceed the performance of all baseline lines. The average total distance was slightly high due to changes in human hand shapes and their variations, but they were sufficient to generate robot motions.We also compared the accuracy of these methods taking into account my predictions in the right hand, as the position of my right hand is more important for manipulating the robot than the positions of other hand types. This is because in our test scenarios the activities of the robot are very much focused on its right hand movement. Table III shows the mean pixel pitch between the ground truth and the predicted \"visual position of my right hand\" are superior to the results of our four examples."}, {"heading": "D. Real-time robot experiments", "text": "Finally, we conducted a user study to evaluate the level of success of the robot activities carried out on the basis of our proposed approach with human subjects. A total of 12 participants (5 students and 7 doctoral students) were recruited from the campus and asked to perform one of the two activities (clearing the table for a partner and preparing a saucer for a pan) together with our robot. After such interactions, participants were asked to complete a questionnaire about the robot's behavior for each task. The questionnaire contained two statements (one statement for each activity) with scales from 1 (absolutely do not match) to 5 (completely agree) to express their impression of the robot's behavior: \"I think the robot cleared the table to create a space for me.\" For task 1 and \"I think the robot passed a saucer closer to me so that I can place the sauceptor on top of it.\" For task 2. In addition to our approach (i.e., we use the following component + the angle of perception)."}, {"heading": "THE SUCCESS LEVEL OF OUR HUMAN-ROBOT COLLABORATION", "text": "Our method works in slow real-time with our unoptimized C + + code. It requires approximately 100 ms per frame with an Nvidia Pascal Titan X GPU, and we were able to conduct experiments on real-time collaboration between humans and robots."}, {"heading": "V. CONCLUSION", "text": "In this paper, we proposed a new robotic activity learning model that uses a fully revolutionary network for future representational regression. The main idea was to let the robot learn the temporal structure of a human activity as a future regression network and learn to transfer such a model for its own motor execution using our manipulation network. We show that our approach allows the robot to derive motor control commands based on prediction of future human hand movements in real time. Experimental results confirm that our approach not only predicts the future positions of human-robot hands more reliably, but is also capable of carrying out activities based on predictions. The paper focuses on robot learning of location-based hand movements (i.e. translations and natural rotations) and dealing with more dynamic hand movements remains one of our future challenges."}], "references": [{"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "Journal of Machine Learning Research, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning for physical interaction through video prediction", "author": ["C. Finn", "I. Goodfellow", "S. Levine"], "venue": "Advances In Neural Information Processing Systems (NIPS), 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "A syntactic approach to robot imitation learning using probabilistic activity grammars", "author": ["K. Lee", "Y. Su", "T.-K. Kim", "Y. Demiris"], "venue": "Robotics and Autonomous Systems, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Robot learning manipulation action plans by\u201d watching\u201d unconstrained videos from the world wide web.", "author": ["Y. Yang", "Y. Li", "C. Ferm\u00fcller", "Y. Aloimonos"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and Autonomous Systems, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "SSD: Single shot multibox detector", "author": ["W. Liu", "D. Anguelov", "D. Erhan", "C. Szegedy", "S. Reed", "C. Fu", "A. Berg"], "venue": "European Conference on Computer Vision (ECCV), 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Anticipating visual representations with unlabeled video", "author": ["C. Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Robot programming by demonstration", "author": ["A. Billard", "S. Calinon", "R. Dillmann", "S. Schaal"], "venue": "Springer handbook of robotics, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning dexterous manipulation for a soft robotic hand from human demonstrations", "author": ["A. Gupta", "C. Eppner", "S. Levine", "P. Abbeel"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning about objects with human teachers", "author": ["A.L. Thomaz", "M. Cakmak"], "venue": "ACM/IEEE International Conference on Human-Robot Interaction (HRI), 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning to select and generalize striking movements in robot table tennis", "author": ["K. M\u00fclling", "J. Kober", "O. Kroemer", "J. Peters"], "venue": "The International Journal of Robotics Research, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning social affordance for human-robot interaction", "author": ["T. Shu", "M.S. Ryoo", "S.-C. Zhu"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI), 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Physically-grounded spatio-temporal object affordances", "author": ["H. Koppula", "A. Saxena"], "venue": "European Conference on Computer Vision (ECCV), 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Patch to the future: Unsupervised visual prediction", "author": ["J. Walker", "A. Gupta", "M. Hebert"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep predictive coding networks for video prediction and unsupervised learning", "author": ["W. Lotter", "G. Kreiman", "D. Cox"], "venue": "arXiv preprint arXiv:1605.08104, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual foresight for planning robot motion", "author": ["C. Finn", "S. Levine"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), 2017.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Fast unsupervised ego-action learning for first-person sports videos", "author": ["K.M. Kitani", "T. Okabe", "Y. Sato", "A. Sugimoto"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Understanding egocentric activities", "author": ["A. Fathi", "A. Farhadi", "J.M. Rehg"], "venue": "International Conference on Computer Vision (ICCV), 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Detecting activities of daily living in first-person camera views", "author": ["H. Pirsiavash", "D. Ramanan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Pooled motion features for first-person videos", "author": ["M.S. Ryoo", "B. Rothrock", "L. Matthies"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Robot-centric activity prediction from first-person videos: What will they do to me?", "author": ["M.S. Ryoo", "T.J. Fuchs", "L. Xia", "J.K. Aggarwal", "L. Matthies"], "venue": "in ACM/IEEE International Conference on Human- Robot Interaction (HRI),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Multi-type activity recognition in robot-centric scenarios", "author": ["I. Gori", "J.K. Aggarwal", "L. Matthies", "M.S. Ryoo"], "venue": "IEEE Robotics and Automation Letters (RA-L), 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions", "author": ["S. Bambach", "S. Lee", "D.J. Crandall", "C. Yu"], "venue": "IEEE International Conference on Computer Vision (ICCV), 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Kaze features", "author": ["P.F. Alcantarilla", "A. Bartoli", "A.J. Davison"], "venue": "European Conference on Computer Vision (ECCV), 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": ", images and videos) [1].", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "proaches showed very promising results on learning video prediction [2] and actual motor control policy [1], they have been limited to relatively simple actions such as object grasping and pushing.", "startOffset": 68, "endOffset": 71}, {"referenceID": 0, "context": "proaches showed very promising results on learning video prediction [2] and actual motor control policy [1], they have been limited to relatively simple actions such as object grasping and pushing.", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "samples of humans (or the robot itself) motor controlling the robot is necessary for generating training data [1], and this is", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "There have been previous works on robot activity learning from human videos [3], [4], extending the previous concept of \u2018robot learning from demonstration\u2019 [5] which was mostly done with direct motor control data.", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "There have been previous works on robot activity learning from human videos [3], [4], extending the previous concept of \u2018robot learning from demonstration\u2019 [5] which was mostly done with direct motor control data.", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "There have been previous works on robot activity learning from human videos [3], [4], extending the previous concept of \u2018robot learning from demonstration\u2019 [5] which was mostly done with direct motor control data.", "startOffset": 156, "endOffset": 159}, {"referenceID": 5, "context": "detection network (SSD [6]) for the representation of human hand-object information in a video frame, and newly introduce the concept of using a fully convolutional network to regress (i.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "Overview of our perception component: Our perception component consists of two fully convolutional neural networks: The first network is an extended version of the state-of-the-art convolutional object detection network (SSD [6]) for the representation of human hands and estimation of the bounding boxes (top).", "startOffset": 225, "endOffset": 228}, {"referenceID": 6, "context": "That is, not only feature-level prediction of future representations (similar to [7]) but also semantic-level prediction of explicit future hand locations of humans and robots during the learned activity is being jointly performed in our new network.", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "a) Robot learning from humans: There have been a considerable amount of previous efforts on robot learning from demonstration (LfD) [5], [8], [9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 7, "context": "a) Robot learning from humans: There have been a considerable amount of previous efforts on robot learning from demonstration (LfD) [5], [8], [9].", "startOffset": 137, "endOffset": 140}, {"referenceID": 8, "context": "a) Robot learning from humans: There have been a considerable amount of previous efforts on robot learning from demonstration (LfD) [5], [8], [9].", "startOffset": 142, "endOffset": 145}, {"referenceID": 9, "context": "However, there are limitations since most of these approaches focused on making robots learn motor control polices from human data, which usually was in the form of direct control sequences obtained with actual robots or simulation softwares [10].", "startOffset": 242, "endOffset": 246}, {"referenceID": 10, "context": "Moreover, it often requires a knowledge about all primitive actions for teaching high-level tasks [11].", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "There also have been previous works on robot activity learning from visual data [3], [4], [12], extending the previ-", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "There also have been previous works on robot activity learning from visual data [3], [4], [12], extending the previ-", "startOffset": 85, "endOffset": 88}, {"referenceID": 11, "context": "There also have been previous works on robot activity learning from visual data [3], [4], [12], extending the previ-", "startOffset": 90, "endOffset": 94}, {"referenceID": 12, "context": "[13] studied an approach to directly learn object manipulation trajectories", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "There have been previous works on the prediction of future frames from the computer vision community [7], [14], [15].", "startOffset": 101, "endOffset": 104}, {"referenceID": 13, "context": "There have been previous works on the prediction of future frames from the computer vision community [7], [14], [15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "There have been previous works on the prediction of future frames from the computer vision community [7], [14], [15].", "startOffset": 112, "endOffset": 116}, {"referenceID": 15, "context": "There exists a recent robotics work that attempted applying visual prediction for generating robot control actions [16].", "startOffset": 115, "endOffset": 119}, {"referenceID": 16, "context": "Recognition of human/robot activities from such first-person videos has been actively studied particularly in the past 5 years, including recognition of human actions from wearable cameras [17]\u2013[20] and human-robot interactions from robot cameras [21], [22].", "startOffset": 189, "endOffset": 193}, {"referenceID": 19, "context": "Recognition of human/robot activities from such first-person videos has been actively studied particularly in the past 5 years, including recognition of human actions from wearable cameras [17]\u2013[20] and human-robot interactions from robot cameras [21], [22].", "startOffset": 194, "endOffset": 198}, {"referenceID": 20, "context": "Recognition of human/robot activities from such first-person videos has been actively studied particularly in the past 5 years, including recognition of human actions from wearable cameras [17]\u2013[20] and human-robot interactions from robot cameras [21], [22].", "startOffset": 247, "endOffset": 251}, {"referenceID": 21, "context": "Recognition of human/robot activities from such first-person videos has been actively studied particularly in the past 5 years, including recognition of human actions from wearable cameras [17]\u2013[20] and human-robot interactions from robot cameras [21], [22].", "startOffset": 253, "endOffset": 257}, {"referenceID": 5, "context": "The first component is a perception component that consists of two fully convolutional neural networks: (1) an extended version of the Single Shot MultiBox Detector (SSD) [6] to create a hand-based scene representation and estimate bounding boxes, and (2) a future regression network to model how such intermediate scene representation (should) change in future frames.", "startOffset": 171, "endOffset": 174}, {"referenceID": 22, "context": "EgoHands [23]: This is a public dataset containing 48 first-person videos of people interacting in four types of activities (playing cards, playing chess, solving a puzzle, and playing Jenga).", "startOffset": 9, "endOffset": 13}, {"referenceID": 23, "context": "More specifically, it detects objects using KAZE features [24] and hands using CNN based hand detector in [23], then computes relative distances between all objects and hands for building the state representation which is a 20 dimensional vector.", "startOffset": 58, "endOffset": 62}, {"referenceID": 22, "context": "More specifically, it detects objects using KAZE features [24] and hands using CNN based hand detector in [23], then computes relative distances between all objects and hands for building the state representation which is a 20 dimensional vector.", "startOffset": 106, "endOffset": 110}, {"referenceID": 5, "context": "(iii) SSD with future annotations is a baseline that uses the original SSD model [6] trained based on EgoHands dataset.", "startOffset": 81, "endOffset": 84}], "year": 2017, "abstractText": "We design a new approach that allows robot learning of new activities from unlabeled human example videos. Given videos of humans executing the same activity from a human\u2019s viewpoint (i.e., first-person videos), our objective is to make the robot learn the temporal structure of the activity as its future regression network, and learn to transfer such model for its own motor execution. We present a new deep learning model: We extend the state-of-the-art convolutional object detection network for the representation/estimation of human hands in training videos, and newly introduce the concept of using a fully convolutional network to regress (i.e., predict) the intermediate scene representation corresponding to the future frame (e.g., 1-2 seconds later). Combining these allows direct prediction of future locations of human hands and objects, which enables the robot to infer the motor control plan using our manipulation network. We experimentally confirm that our approach makes learning of robot activities from unlabeled human interaction videos possible, and demonstrate that our robot is able to execute the learned collaborative activities in real-time directly based on its camera input.", "creator": "LaTeX with hyperref package"}}}