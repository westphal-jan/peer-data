{"id": "1703.01141", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Dynamic State Warping", "abstract": "The ubiquity of sequences in many domains enhances significant recent interest in sequence learning, for which a basic problem is how to measure the distance between sequences. Dynamic time warping (DTW) aligns two sequences by nonlinear local warping and returns a distance value. DTW shows superior ability in many applications, e.g. video, image, etc. However, in DTW, two points are paired essentially based on point-to-point Euclidean distance (ED) without considering the autocorrelation of sequences. Thus, points with different semantic meanings, e.g. peaks and valleys, may be matched providing their coordinate values are similar. As a result, DTW is sensitive to noise and poorly interpretable. This paper proposes an efficient and flexible sequence alignment algorithm, dynamic state warping (DSW). DSW converts each time point into a latent state, which endows point-wise autocorrelation information. Alignment is performed by using the state sequences. Thus DSW is able to yield alignment that is semantically more interpretable than that of DTW. Using one nearest neighbor classifier, DSW shows significant improvement on classification accuracy in comparison to ED (70/85 wins) and DTW (74/85 wins). We also empirically demonstrate that DSW is more robust and scales better to long sequences than ED and DTW.", "histories": [["v1", "Fri, 3 Mar 2017 13:01:38 GMT  (2571kb)", "http://arxiv.org/abs/1703.01141v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhichen gong", "huanhuan chen"], "accepted": false, "id": "1703.01141"}, "pdf": {"name": "1703.01141.pdf", "metadata": {"source": "CRF", "title": "Dynamic State Warping", "authors": ["Zhichen Gong"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 3.01 141v 1 [cs.L G] 3M ar2 017 1Problem is how to measure the distance between sequences. Dynamic time warping (DTW) aligns two sequences by non-linear local warping and returns a distance value. DTW shows superior capabilities in many applications, such as video, image, etc. However, in DTW two points are essentially paired based on point-to-point euclidean distance (ED), without taking into account the autocorrelation of the sequences. Therefore, points with different semantic meanings, such as peaks and valleys, can be matched if their coordinate values are similar. As a result, DTW is sensitive to noise and difficult to interpret. This paper proposes an efficient and flexible sequence algorithm, Dynamic state warping (DSW). DSW converts any moment into a latent state, which enables point-by-point autocorrelation."}, {"heading": "1 INTRODUCTION", "text": "In fact, it is such that most of them will be able to move to another world, in which they will be able to move to another world, in which they will be able to move to another world, in which they will be able to move, in which they will move, in which they will move, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live in which they will live, in which they will live, in which they will live, in which they will live, that they will live, that they will live, that they will live, that they will live, that they will live, that they will live, that they will live, that they will live, that they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live, in which they will live in which they will live, in which they will live, in which they will live, in which they will live, in which"}, {"heading": "2 BACKGROUND AND RELATED WORK", "text": "In this section we will first explain some notations and basic knowledge of sequence learning. Then we will introduce the DTW algorithm in detail. As our method is based on relapsed neural networks, we will finally introduce relapsing neural networks."}, {"heading": "2.1 Sequence Distance", "text": "A sequence is a series of observations for at least one variable. We use a MatrixX = [x1, x2, \u00b7 \u00b7, xLX] T-RLX \u00b7 d3 10 20 30 40 50 60 Alignment Path Point Distance Matrix DFig. 4. Figure of the search for optimal alignment using DTW. The two sequences are demonstrated left and bottom. The point distance matrix and the optimal alignment path are available. Each sequence is associated with a label y. The task of the classification is to learn a function mapping from X to Y.For two sequences Q and C, if they are of the same length, their sequence can be easily calculated using Euclidean Distance (ED) or p-Distance (ED)."}, {"heading": "2.1.1 Dynamic Time Warping", "text": "Since two sequences Q and C may be of different lengths, DTW extends or condenses two sequences to the same length. DTW allows a nonlinear local warping to take into account possible distortions. It finds an optimal alignment between Q and C so that the accumulative euclidean distance is minimized [12], returning a distance value that measures the similarity between the two sequences. Denotating the distance matrix of the time points of Q and C. DTW algorithm migrates from position [1, 1] to position [LQ, LC] of D by a sequence of indices A and B of the same length l \u2212 max. LQ, LC} so that the cumulative distance between l = 1 DAi, Bi = 1 DAi is minimized. In this way, the point A (i) of the sequence Q with the point B (i) of sequence C. See Figure 4 for an illustration."}, {"heading": "2.1.2 Related Work", "text": "Dre rf\u00fc ide nlrf\u00dc\u00fceaeGtn nvo one rf\u00fc ide nlrf\u00dc\u00fceaeGtn nvo a rf\u00fc \u00fceerBnlrf\u00fc ni rde rf\u00fc \u00fc\u00fceeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "2.2 Recurrent Neural Network and Reservoir Computing", "text": "However, unlike HMM, which has ignored earlier points beyond a threshold, RNNs do not restrict the storage time. [24], sequencing process [4] and video prediction [3], etc. The much earlier inputs give less impact on the representation of the current input. The reservoir network is a type of RNN sequence. It has a high-dimensional recursive network that is fixed during training, called the reservoir. The reservoir provides general dynamic characteristics for input sequences that it properly designs. In [2], the initial weights of the reservoir model are likely. It proves to be able to extract discriminatory features from the entire sequence. We assume that the generation of the mechanism of time sequences is invariable."}, {"heading": "3 DYNAMIC STATE WARPING", "text": "It is about the question of whether and how one sees oneself in a position to surpass oneself, and whether one sees oneself in a position to surpass oneself. (...) It is about the question of whether and how one is able to surpass oneself. (...) It is about the question of whether and how one is able to surpass oneself. (...) It is about the question of whether and how one is able to surpass oneself. (...) It is about the question of whether and how one is able to surpass oneself. (...) It is about the question of whether and how one is able to surpass oneself. (...) It is about the question of whether one has to surpass oneself to surpass oneself to surpass oneself. (...) It is about the question of whether and how one is able to surpass oneself. (...) It is about the question of whether one has to surpass oneself to surpass oneself to surpass oneself to surpass oneself. (...) It is about the question of whether one has to surpass oneself to surpass oneself to surpass oneself to surpass oneself. (...) It is about the question of whether one has to surpass oneself to surpass oneself to surpass oneself to surpass oneself. (...) It is about the question of whether one has to surpass oneself to surpass oneself to surpass oneself. (...) It is about the question of whether one has to surpass oneself to surpass oneself to surpass oneself to surpass oneself to surpass oneself. (...) It is about the question of whether one has to surpass oneself to surpass oneself to surpass oneself to surpass oneself to surpass oneself. (...) It is about the question of whether one has to surpass oneself to surpass oneself is about the world, if one has to surpass oneself to surpass oneself is about the world, world, the world, world, world, world, world of the world of the world of the world, world of the world of the world"}, {"heading": "4 EXPERIMENT", "text": "In this section, we conduct extensive experiments to evaluate our method. As all the recent improvements for DTW also work in DSW, our strategy is to compare DSW primarily with DTW and ED. We also compare DSW with more advanced algorithms for classification performance. This section is divided into four parts in particular: First, we evaluate our method using synthetic data sets to show their effectiveness as a distance measurement measure (Section 4.2); second, the robustness of synthetic noise data is evaluated; the scalability of long series is tested by rigorous sequence lengthening (Section 4.3); third, we evaluate DSW using the classification results of 85 UCR time series data [18] compared to ED, DTW and other algorithms (Section 4.4 and 4.5); and finally, we analyze the therelizing properties of our method to provide more insight into DSW (Section 4.6)."}, {"heading": "4.1 Experimental Setup", "text": "We compare DSW with Euclidean distances and DTW. For dynamic programming processes, the size of the warping window is not optimized. However, since the second stage of our method is per se DTW, more advanced amplification techniques for DTW, as proposed in [9] and [19], can be easily integrated. Without explicit mention, the DSW reservoir is randomly generated."}, {"heading": "4.2 Distance Measurement", "text": "To show that DSW is a distance measurement that can approximate the true similarity structure of sequences, we compare DSW with the Euclidean distance and DTW on synthetic data. In particular, we look at the shapes of polygons [10], see Figure 8. The number of endpoints of a shape is taken as a label. For each polygon, the original shape is converted into a dimensional series by calculating the distance from the center to the edge points. The resulting series is demonstrated in Figure 7. Figure 6, which shows the distance between different shapes based on the Euclidean distance, DTW or DSW. According to Figure 6 compared to ED and DTW, DSW captures the similarity between different shapes. Ideally, the distance between two shapes should increase with the difference in complexity. Euclidean distance cannot approximate the true similarity relationship between polygons."}, {"heading": "4.3 Robustness and Scalability", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1 Robustness", "text": "To evaluate the robustness of different distance measurements, we perform experiments that vary the noise in the dataset. In detail, we select a benchmark dataset coffee [18]. On this dataset, the DTW and the Euclidean distance achieve zero error rate if no additional noise is included. We add a Gaussian white noise to the coffee dataset with zero mean and standard deviation that vary within the set {0,1,0,0,3,0,5 0,7 0,9}. The standard deviation is set this way to clearly represent the tendency. On mind7 Noise Eevel 1 2 3 4 5 6 G en er al iz at io n E rr or 0 0,2 0,3 0,4 0,5 0,7 DSW DTW Euclidean DistanceFig. 9. The generalization power of Euclidean, DTW and DSW distance with 1NN classifier is the same order of magnitude."}, {"heading": "4.3.2 Scalability to Long Sequences", "text": "This year we have the opportunity to put ourselves at the top of the list, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "4.4 Classification Performance on Benchmark Datasets", "text": "This year it is more than ever before."}, {"heading": "4.5 Texas Sharpshooter Plot", "text": "The result is encouraging. Note that it does not make sense if we cannot know in advance whether DSW will perform well on a given dataset [10]. To this end, we use the Texas Sharpshooter Diagram to visualize whether DSW is useful by predicting generalizing capability based on the classification results on the training set. Let's take, for example, algorithm A and algorithm B. In detail, we use the LOOCV diagram on the training set of algorithms A and B to calculate the expected performance gain: training accuracy of A / training accuracy of B; let's use the accuracy on the test set to calculate the actual performance gain: test accuracy of A / test accuracy of B. The result for Euclidean distance, DTW and DSW data is shown in Figure 14. The true positive (TP) region represents datasets on which we predict performance increase and correctly."}, {"heading": "4.6 What is happening in the reservoir?", "text": "Sceptical readers may wonder how a randomly generated reservoir network can achieve excellent classification performance. In this subsection, we will empirically uncover some insights into the DSW reservoir network. In particular, we will analyze the spectral radius scaling of the reservoir (4.6.1), the input connection weights (4.6.2), the predictability of the reservoir for input sequences (4.6.3), the size of the reservoir (4.6.4) and the input dimensionality (4.6.5). Our strategy for evaluating these properties is to set the other parameters and only vary the target characteristic.10"}, {"heading": "4.6.1 Spectral Radius Scaling", "text": "In fact, most of them are able to set out in search of a new home."}, {"heading": "4.6.2 Input Weight", "text": "The reservoir receives the current input and changes into a new state by linking this input to the previous reservoir state. There is a trade-off between the current input and the previous input history. We equate the reservoir parameters for the previous subsection. The input weight varies from 0.1 to 1.9 step by 0.2. Figure 17 illustrates the results. It shows that small input weights are helpful for short-term memory and large input weights for short-term memory. This observation is consistent with the result of spectral scaling."}, {"heading": "4.6.3 The Predictability of Reservoir Model", "text": "We use the predictability of the readout layer as a proxy for how well the representation of the state space approximates the original sequences.The reservoir model is designed to approximate the function mapping of input sequences to output sequences.Let's define the predictability of a reservoir as the difference between empirical performance and actual performance. To gain further insight into the reservoir model in DSW, we analyze the relationship between the predictability of the training set and the classification performance on the test set. We train the reservoir model using a one-step prediction. In detail, we perform 10 repetitions for each of the 85 UCR datasets. The reservoir network is randomly regenerated each time. The predictability and generalization error of each network is a fivefold cross validation [2]."}, {"heading": "4.6.4 The Size of Dynamic Reservoir", "text": "The number of neurons in the reservoir has an influence on the storage capacity of the reservoir models [26]. Large reservoir provides more nonlinearity and dynamics. To study how the reservoir size affects DSW, we experiment with 42 old UCR time series datasets with different reservoir sizes, ranging in {5,10,15,20,25,30,40,50,60,70}. We fix the reservoir parameter as ri = 0.2, rc = 0.3, rj = 0.4, N = 5, and Jumplength = 2. On each dataset we get a series of generalization error rates corresponding to different reservoir sizes. We then sort the error rates so that each value is associated with a rank. The algorithm with the minimum generalization error gets the rank of 1, the second minimum gets the rank of 2, and so on."}, {"heading": "4.6.5 Input Dimensionality", "text": "The dimensionality of the input affects the resulting state sequences. For the input dimensionality, this means that we enter n consecutive times as input into the reservoir to obtain an updated state. We extend the original sequence by repeating the end of the sequences n times so that the state sequences have the same length as the original sequences. We conduct experiments on 42 old UCR datasets using different input dimensions. On each dataset, the input dimensionality is selected as 1,2,3 and 4. Based on the selected input dimensionality, we then learn the state sequences of the reservoir. We specified the reservoir parameters during the experiment to ensure that only the input dimensionality is varied. The parameters are the same as in subsection 4.6.1. Figure 20 illustrates the average rank of each dimensionality across all 42 datasets. It clearly shows that 2 input dimensionality achieves the best performance followed by 1 dimensionality."}, {"heading": "5 CONCLUSION", "text": "In this thesis, we propose a novel algorithm, DSW, to calculate the distance between the sequences. DSW uses a reservoir network as a nonlinear time filter for general purposes. Initially, the original sequences are converted into reservoir state sequences to capture autocorrelation structure information. DSW's state path sequence provides versatile discriminatory features for classification, and then dynamic programming to find alignment between two state sequences. Therefore, points are compared with similar states. DSW's temporal complexity is on the same level as DTW's (O (LQLC). We have conducted extensive experiments to evaluate DSW using both synthetic data sets and benchmark data sets, compared with DTW and its variants. The experimental results show the competitiveness of DSW. DSW achieves in particular a state-of-the-art classification performance on 85 UCR time-range data. DSW is also demonstrably equipped with DSW's adaptation methods."}], "references": [{"title": "Time series analysis of nursing notes for mortality prediction via a state transition topic model", "author": ["Y. Jo", "N. Loghmanpour", "C.P. Ros\u00e9"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pp. 1171\u20131180, ACM, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning in the model space for cognitive fault diagnosis", "author": ["H. Chen", "P. Tino", "A. Rodan", "X. Yao"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 25, no. 1, pp. 124\u2013136, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised learning of spatiotemporally coherent metrics", "author": ["R. Goroshin", "J. Bruna", "J. Tompson", "D. Eigen", "Y. LeCun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pp. 4086\u20134093, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling time series similarity with siamese recurrent networks", "author": ["W. Pei", "D.M. Tax", "L. van der Maaten"], "venue": "arXiv preprint arXiv:1603.04713, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Model-based kernel for efficient time series analysis", "author": ["H. Chen", "F. Tang", "P. Tino", "X. Yao"], "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 392\u2013400, ACM, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Model metric co-learning for time series classification", "author": ["H. Chen", "F. Tang", "P. Tino", "A.G. Cohn", "X. Yao"], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, pp. 3387\u20133394, AAAI Press, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "The great time series classification bake off: An experimental evaluation of recently proposed algorithms. extended version", "author": ["A. Bagnall", "A. Bostrom", "J. Large", "J. Lines"], "venue": "arXiv preprint arXiv:1602.01711, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Time series shapelets: a new primitive for data mining", "author": ["L. Ye", "E. Keogh"], "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 947\u2013956, ACM, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Searching and mining trillions of time series subsequences under dynamic time warping", "author": ["T. Rakthanmanon", "B. Campana", "A. Mueen", "G. Batista", "B. Westover", "Q. Zhu", "J. Zakaria", "E. Keogh"], "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 262\u2013270, ACM, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Cid: an efficient complexity-invariant distance for time series", "author": ["G.E. Batista", "E.J. Keogh", "O.M. Tataw", "V.M. de Souza"], "venue": "Data Mining and Knowledge Discovery, vol. 28, no. 3, pp. 634\u2013669, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Using dynamic time warping to find patterns in time series", "author": ["D.J. Berndt", "J. Clifford"], "venue": "AAAI workshop on KDD, vol. 10, pp. 359\u2013370, Seattle, WA, 1994.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1994}, {"title": "Derivative dynamic time warping", "author": ["E.J. Keogh", "M.J. Pazzani"], "venue": "SDM, vol. 1, pp. 5\u20137, SIAM, 2001.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Generalized canonical time warping", "author": ["F. Zhou", "F. De la Torre"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, no. 2, pp. 279\u2013294, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Automated recognition of bird song elements from continuous recordings using dynamic time warping and hidden markov models: A comparative study", "author": ["J.A. Kogan", "D. Margoliash"], "venue": "The Journal of the Acoustical Society of America, vol. 103, no. 4, pp. 2185\u2013 2196, 1998.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Querying and mining of time series data: experimental comparison of representations and distance measures", "author": ["H. Ding", "G. Trajcevski", "P. Scheuermann", "X. Wang", "E. Keogh"], "venue": "Proceedings of the VLDB Endowment, vol. 1, no. 2, pp. 1542\u20131552, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Accelerating dynamic time warping clustering with a novel admissible pruning strategy", "author": ["N. Begum", "L. Ulanova", "J. Wang", "E. Keogh"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 49\u201358, ACM, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Time series shapelets: a novel technique that allows accurate, interpretable and fast classification", "author": ["L. Ye", "E. Keogh"], "venue": "Data mining and knowledge discovery, vol. 22, no. 1-2, pp. 149\u2013182, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "The ucr time series classification archive", "author": ["Y. Chen", "E. Keogh", "B. Hu", "N. Begum", "A. Bagnall", "A. Mueen", "G. Batista"], "venue": "July 2015. www.cs.ucr.edu/\u223ceamonn/time series data/.  14", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Weighted dynamic time warping for time series classification", "author": ["Y.-S. Jeong", "M.K. Jeong", "O.A. Omitaomu"], "venue": "Pattern Recognition, vol. 44, no. 9, pp. 2231\u20132240, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast subsequence matching in time-series databases", "author": ["C. Faloutsos", "M. Ranganathan", "Y. Manolopoulos"], "venue": "ACM SIGMOD Record, pp. 419\u2013429, ACM, 1994.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1994}, {"title": "Metric learning for temporal sequence alignment", "author": ["D. Garreau", "R. Lajugie", "S. Arlot", "F. Bach"], "venue": "Advances in Neural Information Processing Systems, pp. 1817\u20131825, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1817}, {"title": "Dynamic time warping averaging of time series allows faster and more accurate classification", "author": ["F. Petitjean", "G. Forestier", "G.I. Webb", "A.E. Nicholson", "Y. Chen", "E. Keogh"], "venue": "2014 IEEE International Conference on Data Mining, pp. 470\u2013479, IEEE, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems, pp. 3111\u20133119, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning discriminative fisher kernels", "author": ["L. Maaten"], "venue": "Proceedings of the 28th International Conference on Machine Learning, pp. 217\u2013224, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Reservoir computing approaches to recurrent neural network training", "author": ["M. Luko\u0161Evi\u010dIus", "H. Jaeger"], "venue": "Computer Science Review, vol. 3, no. 3, pp. 127\u2013149, 2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "S EQUENCES are generated and analyzed in almost every domain of human society such as medical [1], engineering [2], entertainment [3], etc.", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "S EQUENCES are generated and analyzed in almost every domain of human society such as medical [1], engineering [2], entertainment [3], etc.", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "S EQUENCES are generated and analyzed in almost every domain of human society such as medical [1], engineering [2], entertainment [3], etc.", "startOffset": 130, "endOffset": 133}, {"referenceID": 3, "context": "Computing the distance between sequences is critical for classification and has attracted significant research interest [4], [5], [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 4, "context": "Computing the distance between sequences is critical for classification and has attracted significant research interest [4], [5], [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "Computing the distance between sequences is critical for classification and has attracted significant research interest [4], [5], [6].", "startOffset": 130, "endOffset": 133}, {"referenceID": 6, "context": "For sequence classification, one nearest neighbor (1NN) classifier has been empirically shown to be a strong solution with proper distance measurements [7], [8], [9].", "startOffset": 152, "endOffset": 155}, {"referenceID": 7, "context": "For sequence classification, one nearest neighbor (1NN) classifier has been empirically shown to be a strong solution with proper distance measurements [7], [8], [9].", "startOffset": 157, "endOffset": 160}, {"referenceID": 8, "context": "For sequence classification, one nearest neighbor (1NN) classifier has been empirically shown to be a strong solution with proper distance measurements [7], [8], [9].", "startOffset": 162, "endOffset": 165}, {"referenceID": 9, "context": "In this case, the only concern is how to measure the distance between sequences properly [10].", "startOffset": 89, "endOffset": 93}, {"referenceID": 9, "context": "A more comprehensive review can be found in [10].", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "Dynamic time warping (DTW) [11], [12] is to compute the distance between two sequences by warping them locally to the same length.", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": "Dynamic time warping (DTW) [11], [12] is to compute the distance between two sequences by warping them locally to the same length.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "In this way, DTW is naturally compatible with phase distortion invariance of sequential data [10].", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "[9], [13], [14].", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[9], [13], [14].", "startOffset": 5, "endOffset": 9}, {"referenceID": 13, "context": "[9], [13], [14].", "startOffset": 11, "endOffset": 15}, {"referenceID": 14, "context": "It has been a consensus that DTW may be the strongest distance measurements for sequences [15], [16].", "startOffset": 90, "endOffset": 94}, {"referenceID": 15, "context": "It has been a consensus that DTW may be the strongest distance measurements for sequences [15], [16].", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "DTW aligns two sequences by taking the point-to-point comparison of coordinate values as a fundamental unit [16].", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "such that the aligned sequences yields a globally minimum Euclidean distance [11], which is returned as the distance between the original sequences.", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "Thus, DTW does align globally but is unable to take into consideration the auto-correlated structure information properly [8].", "startOffset": 122, "endOffset": 125}, {"referenceID": 11, "context": "This makes DTW fragile to noise [12], which is also demonstrated in our experiment (Section 4).", "startOffset": 32, "endOffset": 36}, {"referenceID": 7, "context": "As pointed out in [8], [17], DTW usually has weak interpretability of its alignment.", "startOffset": 18, "endOffset": 21}, {"referenceID": 16, "context": "As pointed out in [8], [17], DTW usually has weak interpretability of its alignment.", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": "grade the classification performance [2].", "startOffset": 37, "endOffset": 40}, {"referenceID": 17, "context": "Figure 1 illustrates the DTW match result of GunPoint dataset from UCR time series archive [18].", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "(6) DSW shows more robustness to noise and more suitable for long sequences than compared methods; (7) After obtaining the state sequences, the problem is again DTW, thus advanced techniques [9], [19] for improving the effectiveness and efficiency of DTW is also compatible with DSW.", "startOffset": 191, "endOffset": 194}, {"referenceID": 18, "context": "(6) DSW shows more robustness to noise and more suitable for long sequences than compared methods; (7) After obtaining the state sequences, the problem is again DTW, thus advanced techniques [9], [19] for improving the effectiveness and efficiency of DTW is also compatible with DSW.", "startOffset": 196, "endOffset": 200}, {"referenceID": 19, "context": "Given two sequences Q and C, if they are of equal length, one can easily compute their distance using Euclidean distance (ED) or p-norm distance [20], such as", "startOffset": 145, "endOffset": 149}, {"referenceID": 14, "context": "In this case, elastic distance, such as DTW and longest common subsequence [15] etc.", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "It finds an optimal alignment between Q and C such that the accumulative Euclidean distance is minimized [12].", "startOffset": 105, "endOffset": 109}, {"referenceID": 0, "context": "DTW algorithm travels from the position [1, 1] to position [LQ, LC ] of D to find a sequence of indices A and B of the same length l \u2265 max{LQ, LC} such that the cumulative distance \u2211l i=1 DAi,Bi is minimized.", "startOffset": 40, "endOffset": 46}, {"referenceID": 0, "context": "DTW algorithm travels from the position [1, 1] to position [LQ, LC ] of D to find a sequence of indices A and B of the same length l \u2265 max{LQ, LC} such that the cumulative distance \u2211l i=1 DAi,Bi is minimized.", "startOffset": 40, "endOffset": 46}, {"referenceID": 11, "context": "[12], [13], [21]) and heuristically constraining the DTW (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12], [13], [21]) and heuristically constraining the DTW (e.", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "[12], [13], [21]) and heuristically constraining the DTW (e.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "[10], [19]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[10], [19]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "[21] proposed to learn a distance metric for measuring the similarity between two points.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] combined DTW with canonical correlation analysis (CCA), termed canonical time warping (CTA).", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "In [22], Petitjean et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "Keogh and Pazzani [12] proposed to replace the coordinator distances by the derivative distance (DDTW).", "startOffset": 18, "endOffset": 22}, {"referenceID": 18, "context": "[19] proposed to weight the match of two points by the length of stretch such that the long shift is penalized (WDTW).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] proposed a similar penalty-based distance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Other studies focus on improving the efficiency of DTW [9] but do not improve the alignment results of DTW.", "startOffset": 55, "endOffset": 58}, {"referenceID": 22, "context": "This implicit fading memory has enabled RNN a powerful tool in language modeling [24], sequence process [4] and video prediction [3] etc.", "startOffset": 81, "endOffset": 85}, {"referenceID": 3, "context": "This implicit fading memory has enabled RNN a powerful tool in language modeling [24], sequence process [4] and video prediction [3] etc.", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "This implicit fading memory has enabled RNN a powerful tool in language modeling [24], sequence process [4] and video prediction [3] etc.", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "In [2], the output weights of the reservoir model is proved to be able to extract discriminative features from the whole sequence.", "startOffset": 3, "endOffset": 6}, {"referenceID": 23, "context": "Similarly, Fisher kernel [25] learns vectorial fisher score as a representations for the whole sequence using a probabilistic generative model.", "startOffset": 25, "endOffset": 29}, {"referenceID": 24, "context": "For more information about RNN and reservoir, we refer the readers to [26].", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "DTW is commonly used comparing time points in the time domain in previous sequence classification methods, which makes it sensitive to noise and weakly interpretable [8].", "startOffset": 166, "endOffset": 169}, {"referenceID": 24, "context": "That is, the noise level is reduced in the state sequence by adjusting ri (usually ri \u2264 1 in the typical ESN setting [26]).", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "Thirdly, we evaluate DSW by the classification results on 85 UCR time series datasets [18] in comparison with ED, DTW and other algorithms (subsection 4.", "startOffset": 86, "endOffset": 90}, {"referenceID": 8, "context": "However, since the second stage of our method is intrinsically DTW, more advanced strengthening techniques for DTW, such as those proposed in [9] and [19], can be easily incorporated.", "startOffset": 142, "endOffset": 145}, {"referenceID": 18, "context": "However, since the second stage of our method is intrinsically DTW, more advanced strengthening techniques for DTW, such as those proposed in [9] and [19], can be easily incorporated.", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "In particular, we consider the shapes of polygons [10], see Figure 8 for an illustration.", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "In detail, we choose a benchmark dataset Coffee [18].", "startOffset": 48, "endOffset": 52}, {"referenceID": 6, "context": "This result explains why DTW is so successful in sequence processing domains [7], [9].", "startOffset": 77, "endOffset": 80}, {"referenceID": 8, "context": "This result explains why DTW is so successful in sequence processing domains [7], [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 17, "context": "Datasets Our experiments are performed using the UCR time series datasets [18].", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "The 85 UCR datasets [18] are collected from different domains such as insect recognition, medicine, engineering, motion tracking, image and synthetic data etc.", "startOffset": 20, "endOffset": 24}, {"referenceID": 17, "context": "Detailed information about the datasets is available on the website [18].", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "(3) DDTW: The first order distance is calculated for DTW as in [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "Following [19], we employ a logistic function for the weight, i.", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "(5) WDDTW: A combination of DDTW and WDTW, which considers the derivative and weight penalty simultaneously [19].", "startOffset": 108, "endOffset": 112}, {"referenceID": 9, "context": "(6) CID-DTW: As indicated in [10], a complexity penalty is multiplied on the original DTW distance to take into account the difference in the complexity of two series.", "startOffset": 29, "endOffset": 33}, {"referenceID": 9, "context": "Note that, it would make no sense if we cannot know ahead of time whether DSW will perform well on a given dataset [10].", "startOffset": 115, "endOffset": 119}, {"referenceID": 0, "context": "In FP region, most datasets are close to [1, 1], which indicates the reduction is very small.", "startOffset": 41, "endOffset": 47}, {"referenceID": 0, "context": "In FP region, most datasets are close to [1, 1], which indicates the reduction is very small.", "startOffset": 41, "endOffset": 47}, {"referenceID": 24, "context": "Previous studies have revealed that the spectral radius scaling should be less than one to guarantee echo state property [26].", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "In particular, we have R = scaling \u00d7 R/max(eig(R)), where R is the reservoir weight matrix [26].", "startOffset": 91, "endOffset": 95}, {"referenceID": 24, "context": "In particular, smaller scaling parameter weights the previous state less and gives more importance to the current input, resulting in short short-term memory [26].", "startOffset": 158, "endOffset": 162}, {"referenceID": 1, "context": "Ridge regression is employed to learn the output weights [2], [26].", "startOffset": 57, "endOffset": 60}, {"referenceID": 24, "context": "Ridge regression is employed to learn the output weights [2], [26].", "startOffset": 62, "endOffset": 66}, {"referenceID": 1, "context": "The ridge regression parameter is selected in {10, 10, \u00b7 \u00b7 \u00b7 , 10} by 5-fold cross validation [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 24, "context": "The number of neurons in the reservoir has an influence on the memory capacity of reservoir models [26].", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "Note that in previous literatures [26], [27], the size of reservoir is usually set as hundreds of neurons to capture the generating mechanism.", "startOffset": 34, "endOffset": 38}], "year": 2017, "abstractText": "The ubiquity of sequences in many domains enhances significant recent interest in sequence learning, for which a basic problem is how to measure the distance between sequences. Dynamic time warping (DTW) aligns two sequences by nonlinear local warping and returns a distance value. DTW shows superior ability in many applications, e.g. video, image, etc. However, in DTW, two points are paired essentially based on point-to-point Euclidean distance (ED) without considering the autocorrelation of sequences. Thus, points with different semantic meanings, e.g. peaks and valleys, may be matched providing their coordinate values are similar. As a result, DTW is sensitive to noise and poorly interpretable. This paper proposes an efficient and flexible sequence alignment algorithm, dynamic state warping (DSW). DSW converts each time point into a latent state, which endows point-wise autocorrelation information. Alignment is performed by using the state sequences. Thus DSW is able to yield alignment that is semantically more interpretable than that of DTW. Using one nearest neighbor classifier, DSW shows significant improvement on classification accuracy in comparison to ED (70/85 wins) and DTW (74/85 wins). We also empirically demonstrate that DSW is more robust and scales better to long sequences than ED and DTW.", "creator": "LaTeX with hyperref package"}}}