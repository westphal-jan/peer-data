{"id": "1703.01203", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Stochastic Separation Theorems", "abstract": "A set $S$ is linearly separable if each $x\\in S$ can be separated from the rest of $S$ by a linear functional. We study random $N$-element sets in $\\mathbb{R}^n$ for large $n$ and demonstrate that for $N&lt;a\\exp(b{n})$ they are linearly separable with probability $p$, $p&gt;1-\\vartheta$, for a given (small) $\\vartheta&gt;0$. Constants $a,b&gt;0$ depend on the probability distribution and the constant $\\vartheta$. The results are important for machine learning in high dimension, especially for correction of unavoidable mistakes of legacy Artificial Intelligence systems.", "histories": [["v1", "Fri, 3 Mar 2017 15:27:38 GMT  (390kb,D)", "https://arxiv.org/abs/1703.01203v1", null], ["v2", "Sat, 1 Apr 2017 16:47:51 GMT  (97kb)", "http://arxiv.org/abs/1703.01203v2", "5 pages, added numerical experiments, extended bibliography, corrected misprints"], ["v3", "Thu, 3 Aug 2017 17:37:06 GMT  (97kb)", "http://arxiv.org/abs/1703.01203v3", "6 pages, accepted for publication in Neural Networks (Letter section)"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["a n gorban", "i y tyukin"], "accepted": false, "id": "1703.01203"}, "pdf": {"name": "1703.01203.pdf", "metadata": {"source": "CRF", "title": "Stochastic Separation Theorems", "authors": ["A.N. Gorbana", "I.Y. Tyukin"], "emails": ["ag153@le.ac.uk", "it37@le.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.01 203v 3 [cs.L G] 3A ug2 017The problem of non-iterative and non-destructive correction of unavoidable errors arises in all applications of artificial intelligence in the real world. Their solution requires a robust separation of samples with errors from samples where the system is functioning properly. We show that this separation in (moderately) high dimensions could be achieved with a probability close to one by linear discriminants. Based on basic properties of the measurement concentration, we show that for M < a exp (bn) random quantities of melt in Rn are linearly separable with the probability p, p > 1 \u2212 \u03d1, where 1 > 0 is a given small constant. Exact values of a, b > 0 depend on the probability distribution that determines how the random M element sets are drawn and on constant differentiation. These stostic separation theories represent a new tool for evaluating algorithms and high dimension."}, {"heading": "1. Introduction", "text": "The problem of non-destructive correction arises in many areas of research and development, from AI to mathematical neuroscience, where the reverse engineering of the brain's ability to learn on-the-fly remains a major challenge. It is highly desirable that the correction of errors is non-iterative (one-shot), because the iterative retraining of a large system requires a great deal of time and resources and cannot be done immediately without affecting the capabilities of the brain. Non-destructive correction requires separation of situations (samples) with errors from the samples that correspond to the correct behavior of a simple and robust classifier. Linear discriminators introduced by Fisher are simple, robust, require only the inverse coordination matrix of data and can be easily modified for assimilation of new data. Rosenblatt (1962) has revived the common interest in linear classifiers."}, {"heading": "2. Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Notation", "text": "Throughout the text, Rn is the n-dimensional linear real vector space. Unless otherwise specified, the symbols xi = (xi, 1,.., xi, n) denote elements of Rn, and (xi, x j) = \u2211 k xi, kx j, k is the inner product of xi and x j in R n. Symbol Bn stands for the unified sphere in Rn centered at the origin: Bn = {x-Rn | (x, x) \u2264 1}."}, {"heading": "2.2. Linear Separability of Sets", "text": "Definition A set of S-Rn is linearly separable if for each x-compact K there is a linear functional l, so that l (x) > l (y) is linearly separable for all y-S, y, x.Remember that x-Rn is an extreme point of a convex compact K when there are no points y, z-K, y, z, so that x = (y + z) / 2. The basic examples of linearly separable quantities are extremes of a convex compact: vertible polyhedra vertibles or points on the n-dimensional sphere. The sets of extreme points of a compact unit may not be linearly separable, as simple 2D examples show (Simon, 2011).Proposition 1. Each compact linear separable set S-Rn is a series of extreme points of a convex compact K = vexact S-points. The proof follows directly from the previous definitions, the Krein-Milman, Simon, 2011 (Theorem)."}, {"heading": "3. Main Results", "text": "The probability that a random point belongs to a layer Bn / rBn (0 < r < r < r < 1) between the spheres of radius 1 and radius r is p = 1 \u2212 rn. Let us take a unit of measurement v. The probability that the projection of a random vector x on v, (x, v) > r, is more than half the volume of balls of radii 1 \u2212 r2 and 1 (see Fig. 1): P (x, v) > r, more than half the volume of balls of radii 1 \u2212 r2 and 1 (see Fig. 1): P (x, v) > r, 0. Theorem 1. Let {x1, xM} a series of M i.e. random points from the equivalent in the unit Bn, 0 < r < r < r < 1. ThenP (xi, xM, xM)."}, {"heading": "4. Conclusion", "text": "Statistical separation theories describe a random structure of these thin layers: the random points are not only concentrated in a thin layer, but they are all linearly separable from the rest of the group, even if they represent exponentially large random sets. Estimates are produced for two classes of distributions in high dimensions: for equibutions in balls or ellipsoids, or for product distribution with compact support (i.e. in the case that coordinates are independent random variables). Countless generalizations are possible, for example: Relax the requirement of independent coordinates in Theorem 2 to those of weakly dependent vector-weighted vari-ables; \u2022 Instead of equibutions, we look at distributions with strongly logical-conceptual probabilities."}, {"heading": "Acknowledgement", "text": "The authors thank M. Gromov and S. Utev for groundbreaking questions and comments. The work was partly supported by Innovate UK (KTP009890 and KTP010522)."}], "references": [{"title": "An elementary introduction to modern convex geometry", "author": ["K. Ball"], "venue": "Flavors of geometry", "citeRegEx": "Ball,? \\Q1997\\E", "shortCiteRegEx": "Ball", "year": 1997}, {"title": "Stability and generalization", "author": ["O. Bousquet", "A. Elisseeff"], "venue": "Journal of Machine Learning Research ,", "citeRegEx": "Bousquet and Elisseeff,? \\Q2002\\E", "shortCiteRegEx": "Bousquet and Elisseeff", "year": 2002}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["R.A. Fisher"], "venue": "Machine Learning", "citeRegEx": "Fisher,? \\Q1936\\E", "shortCiteRegEx": "Fisher", "year": 1936}, {"title": "Counterfactual DataFusion for Online Reinforcement Learners. A talk at the 1st Workshop on Transfer in Reinforcement Learning at AAMAS 2017", "author": ["A. Forney", "J. Pearl", "E. Bareinboim"], "venue": "May 8\u20139,", "citeRegEx": "Forney et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Forney et al\\.", "year": 2017}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Y. Freund", "R. Schapire"], "venue": "Machine Learning", "citeRegEx": "Freund and Schapire,? \\Q1999\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1999}, {"title": "2016a. Onetrial correction of legacy AI systems and stochastic separation theorems", "author": ["A.N. Gorban", "I. Romanenko", "R. Burton", "I. Tyukin"], "venue": null, "citeRegEx": "Gorban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gorban et al\\.", "year": 2016}, {"title": "2016b. Approximation with random bases: Pro et contra", "author": ["A.N. Gorban", "I. Tyukin", "D. Prokhorov", "K. Sofeikov"], "venue": "Information Sciences", "citeRegEx": "Gorban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gorban et al\\.", "year": 2016}, {"title": "The blessing of dimensionality: Separation theorems in the thermodynamic limit", "author": ["A.N. Gorban", "I.Y. Tyukin", "I. Romanenko"], "venue": "IFACPapersOnLine, 49,", "citeRegEx": "Gorban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gorban et al\\.", "year": 2016}, {"title": "Isoperimetry of waists and concentration of maps. GAFA", "author": ["M. Gromov"], "venue": "Geomteric and Functional Analysis", "citeRegEx": "Gromov,? \\Q2003\\E", "shortCiteRegEx": "Gromov", "year": 2003}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "J. Amer. Statist. Assoc", "citeRegEx": "Hoeffding,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding", "year": 1963}, {"title": "Rapid encoding of new memories by individual neurons in the human", "author": ["M. Ison", "R. Quian Quiroga", "I. Fried"], "venue": "brain. Neuron", "citeRegEx": "Ison et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ison et al\\.", "year": 2015}, {"title": "Quasiorthogonal dimension of euclidian spaces", "author": ["P. Kainen", "V. K\u016frkov\u00e1"], "venue": "Appl. Math. Lett", "citeRegEx": "Kainen and K\u016frkov\u00e1,? \\Q1993\\E", "shortCiteRegEx": "Kainen and K\u016frkov\u00e1", "year": 1993}, {"title": "Estimates of covering numbers of convex sets with slowly decaying orthogonal subsets", "author": ["V. K\u016frkov\u00e1", "M. Sanguineti"], "venue": "Discrete Applied Mathematics", "citeRegEx": "K\u016frkov\u00e1 and Sanguineti,? \\Q2007\\E", "shortCiteRegEx": "K\u016frkov\u00e1 and Sanguineti", "year": 2007}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. Le Cun", "T. Bengio"], "venue": null, "citeRegEx": "Cun and Bengio,? \\Q1995\\E", "shortCiteRegEx": "Cun and Bengio", "year": 1995}, {"title": "On learning sets and functions", "author": ["B. Natarajan"], "venue": "Machine Learning", "citeRegEx": "Natarajan,? \\Q1989\\E", "shortCiteRegEx": "Natarajan", "year": 1989}, {"title": "Invariant visual representation by single neurons in the human brain", "author": ["R. Quian Quiroga", "L. Reddy", "G. Kreiman", "C. Koch", "I. Fried"], "venue": "Nature 435,", "citeRegEx": "Quiroga et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Quiroga et al\\.", "year": 2005}, {"title": "Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms", "author": ["F. Rosenblatt"], "venue": "Spartan Books", "citeRegEx": "Rosenblatt,? \\Q1962\\E", "shortCiteRegEx": "Rosenblatt", "year": 1962}, {"title": "Learning internal representations by error propagation, in: Rumelhart, D.E. McClelland, J., the PDP research group (Eds.), Parallel distributed processing: Explorations", "author": ["D.E. Rumelhart", "G.E. Hinton", "R. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Randomness in neural networks: an overview", "author": ["S. Scardapane", "D. Wang"], "venue": "WIREs Data Mining Knowl. Discov. 7,", "citeRegEx": "Scardapane and Wang,? \\Q2017\\E", "shortCiteRegEx": "Scardapane and Wang", "year": 2017}, {"title": "Convexity, An Analytic Viewpoint, Cambridge Tracts in Mathematics, V. 187", "author": ["B. Simon"], "venue": null, "citeRegEx": "Simon,? \\Q2011\\E", "shortCiteRegEx": "Simon", "year": 2011}, {"title": "The Nature of Statistical Learning Theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q2000\\E", "shortCiteRegEx": "Vapnik", "year": 2000}, {"title": "Estimation of Dependences Based on Empirical Data", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1982\\E", "shortCiteRegEx": "Vapnik", "year": 1982}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V.N. Vapnik", "A.Y. Chervonenkis"], "venue": "Theory of Probability and Its Applications", "citeRegEx": "Vapnik and Chervonenkis,? \\Q1971\\E", "shortCiteRegEx": "Vapnik and Chervonenkis", "year": 1971}, {"title": "Human medial temporal lobe neurons respond preferentially to personally relevant images", "author": ["I. Viskontasa", "R. Quian Quiroga", "I. Fried"], "venue": "Proc. Nat. Acad. Sci. 120,", "citeRegEx": "Viskontasa et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Viskontasa et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 22, "context": "Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971), learnability (Natarajan, 1989), and generalization capabilities of neural networks (Vapnik, 2000), (Bousquet and Elisseeff, 2002).", "startOffset": 27, "endOffset": 58}, {"referenceID": 14, "context": "Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971), learnability (Natarajan, 1989), and generalization capabilities of neural networks (Vapnik, 2000), (Bousquet and Elisseeff, 2002).", "startOffset": 73, "endOffset": 90}, {"referenceID": 20, "context": "Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971), learnability (Natarajan, 1989), and generalization capabilities of neural networks (Vapnik, 2000), (Bousquet and Elisseeff, 2002).", "startOffset": 143, "endOffset": 157}, {"referenceID": 1, "context": "Vapnik-Chervonenkis theory (Vapnik and Chervonenkis, 1971), learnability (Natarajan, 1989), and generalization capabilities of neural networks (Vapnik, 2000), (Bousquet and Elisseeff, 2002).", "startOffset": 159, "endOffset": 189}, {"referenceID": 17, "context": "multilayer perceptrons, (Rumelhart et al., 1986), Convolutional Neural Networks (Le Cun and Bengio, 1995), (LeCun et al.", "startOffset": 24, "endOffset": 48}, {"referenceID": 1, "context": "Linear discriminants introduced by Fisher (1936) are simple, robust, require just the inverse covariance matrix of data, and may be easily modified for assimilation of new data.", "startOffset": 35, "endOffset": 49}, {"referenceID": 1, "context": "Linear discriminants introduced by Fisher (1936) are simple, robust, require just the inverse covariance matrix of data, and may be easily modified for assimilation of new data. Rosenblatt (1962) revived the common interest in linear classifiers.", "startOffset": 35, "endOffset": 196}, {"referenceID": 4, "context": "Tyukin) as \u201cstand-alone\u201d learning machines, including their generalization margins (Freund and Schapire, 1999), (Vapnik, 2000) and numerous methods for their construction: linear discriminants and regression, perceptron learning, and Support Vector Machines (Vapnik, 1982) among others.", "startOffset": 83, "endOffset": 110}, {"referenceID": 20, "context": "Tyukin) as \u201cstand-alone\u201d learning machines, including their generalization margins (Freund and Schapire, 1999), (Vapnik, 2000) and numerous methods for their construction: linear discriminants and regression, perceptron learning, and Support Vector Machines (Vapnik, 1982) among others.", "startOffset": 112, "endOffset": 126}, {"referenceID": 21, "context": "Tyukin) as \u201cstand-alone\u201d learning machines, including their generalization margins (Freund and Schapire, 1999), (Vapnik, 2000) and numerous methods for their construction: linear discriminants and regression, perceptron learning, and Support Vector Machines (Vapnik, 1982) among others.", "startOffset": 258, "endOffset": 272}, {"referenceID": 3, "context": "According to Forney et al. (2017), data collected by different agents may not be naively combined due to changes in the context, and special procedures for their assimilation without damage of gained skills are needed.", "startOffset": 13, "endOffset": 34}, {"referenceID": 19, "context": "The sets of extreme points of a compact may be not linearly separable as is demonstrated by simple 2D examples (Simon, 2011).", "startOffset": 111, "endOffset": 124}, {"referenceID": 19, "context": "The proof follows immediately from the previous definitions, the Krein-Milman theorem (Simon, 2011) (its finitedimensional form was known to Minkovsky) and classical theorems about separation of a point from a convex set.", "startOffset": 86, "endOffset": 99}, {"referenceID": 0, "context": "Ball (1997) provides more geometric details of concentration of the volume of high-dimensional balls.", "startOffset": 0, "endOffset": 12}, {"referenceID": 0, "context": "Ball (1997) provides more geometric details of concentration of the volume of high-dimensional balls. In (3) we estimate the probability that the cosine of the angles between xi and x j does not exceed r. Gorban et al. (2016b) analyzed the asymptotic behavior of these estimations for small r.", "startOffset": 0, "endOffset": 227}, {"referenceID": 0, "context": "Ball (1997) provides more geometric details of concentration of the volume of high-dimensional balls. In (3) we estimate the probability that the cosine of the angles between xi and x j does not exceed r. Gorban et al. (2016b) analyzed the asymptotic behavior of these estimations for small r. The idea of almost orthogonal bases was introduced by Kainen and K\u016frkov\u00e1 (1993) and used efficiently by K\u016frkov\u00e1 and Sanguineti (2007) for estimation of the cardinality of \u03b5-nets in compact convex subsets of Hilbert spaces including the sets of functions computable by perceptrons.", "startOffset": 0, "endOffset": 374}, {"referenceID": 0, "context": "Ball (1997) provides more geometric details of concentration of the volume of high-dimensional balls. In (3) we estimate the probability that the cosine of the angles between xi and x j does not exceed r. Gorban et al. (2016b) analyzed the asymptotic behavior of these estimations for small r. The idea of almost orthogonal bases was introduced by Kainen and K\u016frkov\u00e1 (1993) and used efficiently by K\u016frkov\u00e1 and Sanguineti (2007) for estimation of the cardinality of \u03b5-nets in compact convex subsets of Hilbert spaces including the sets of functions computable by perceptrons.", "startOffset": 0, "endOffset": 428}, {"referenceID": 8, "context": "Concentration near the spheres with different centres implies concentration in the vicinity of their intersection (an example of the \u2018waist concentration\u2019 (Gromov, 2003)).", "startOffset": 155, "endOffset": 169}, {"referenceID": 9, "context": "The vicinity of the spheres, where the distribution is concentrated, can be estimated by the Hoeffding inequality (Hoeffding, 1963).", "startOffset": 114, "endOffset": 131}, {"referenceID": 18, "context": "Such measure concentration effects reveal the hidden geometric background of the reported success of randomized neural networks models (Scardapane and Wang, 2017).", "startOffset": 135, "endOffset": 162}, {"referenceID": 23, "context": ", 2005), (Viskontasa et al., 2009).", "startOffset": 9, "endOffset": 34}, {"referenceID": 10, "context": "Strikingly, not only is the brain able to respond selectively to \u201crare\u201d individual stimuli but also such selectivity can be learnt very rapidly from a limited number of experiences (Ison et al., 2015).", "startOffset": 181, "endOffset": 200}], "year": 2017, "abstractText": "The problem of non-iterative one-shot and non-destructive correction of unavoidable mistakes arises in all Artificial Intelligence applications in the real world. Its solution requires robust separation of samples with errors from samples where the system works properly. We demonstrate that in (moderately) high dimension this separation could be achieved with probability close to one by linear discriminants. Based on fundamental properties of measure concentration, we show that for M < a exp(bn) random Melement sets in R are linearly separable with probability p, p > 1 \u2212 \u03b8, where 1 > \u03b8 > 0 is a given small constant. Exact values of a, b > 0 depend on the probability distribution that determines how the random M-element sets are drawn, and on the constant \u03b8. These stochastic separation theorems provide a new instrument for the development, analysis, and assessment of machine learning methods and algorithms in high dimension. Theoretical statements are illustrated with numerical examples.", "creator": "LaTeX with hyperref package"}}}