{"id": "1703.01274", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Actor-Critic Reinforcement Learning with Simultaneous Human Control and Feedback", "abstract": "This paper contributes a first study into how different human users deliver simultaneous control and feedback signals during human-robot interaction. As part of this work, we formalize and present a general interactive learning framework for online cooperation between humans and reinforcement learning agents. In many human-machine interaction settings, there is a growing gap between the degrees-of-freedom of complex semi-autonomous systems and the number of human control channels. Simple human control and feedback mechanisms are required to close this gap and allow for better collaboration between humans and machines on complex tasks. To better inform the design of concurrent control and feedback interfaces, we present experimental results from a human-robot collaborative domain wherein the human must simultaneously deliver both control and feedback signals to interactively train an actor-critic reinforcement learning robot. We compare three experimental conditions: 1) human delivered control signals, 2) reward-shaping feedback signals, and 3) simultaneous control and feedback. Our results suggest that subjects provide less feedback when simultaneously delivering feedback and control signals and that control signal quality is not significantly diminished. Our data suggest that subjects may also modify when and how they provide feedback. Through algorithmic development and tuning informed by this study, we expect semi-autonomous actions of robotic agents can be better shaped by human feedback, allowing for seamless collaboration and improved performance in difficult interactive domains.", "histories": [["v1", "Fri, 3 Mar 2017 18:15:32 GMT  (3297kb,D)", "https://arxiv.org/abs/1703.01274v1", "10 pages, 2 pages of references, 8 figures. Under review for the 34th International Conference on Machine Learning,Sydney, Australia, 2017. Copyright 2017 by the authors"], ["v2", "Wed, 15 Mar 2017 15:15:14 GMT  (3297kb,D)", "http://arxiv.org/abs/1703.01274v2", "10 pages, 2 pages of references, 8 figures. Under review for the 34th International Conference on Machine Learning, Sydney, Australia, 2017. Copyright 2017 by the authors"]], "COMMENTS": "10 pages, 2 pages of references, 8 figures. Under review for the 34th International Conference on Machine Learning,Sydney, Australia, 2017. Copyright 2017 by the authors", "reviews": [], "SUBJECTS": "cs.AI cs.HC cs.RO", "authors": ["kory w mathewson", "patrick m pilarski"], "accepted": false, "id": "1703.01274"}, "pdf": {"name": "1703.01274.pdf", "metadata": {"source": "META", "title": "Actor-Critic Reinforcement Learning with  Simultaneous Human Control and Feedback", "authors": ["Kory W. Mathewson", "Patrick M. Pilarski"], "emails": ["<korym@ualberta.ca>."], "sections": [{"heading": null, "text": "1University of Alberta, Dep. of Computing Science, Edmonton, Canada 2University of Alberta, Deps. of Medicine and Computing Science, Edmonton, Alberta, Canada. Correspondence to: Kory Mathewson < korym @ ualberta.ca >. Checked at the 34th International Conference on Machine Learning, Sydney, Australia, 2017. JMLR: W & CP. Author Copyright 2017. Figure 1. Experimental configuration: One of the study participants with the myo band on his right arm provides a control signal while simultaneously providing feedback signals with his left hand.The Aldebaran Nao robot simulation is visible on the screen next to the experimental logging."}, {"heading": "1. Introduction", "text": "In this context, it has to be noted that this is a process, which is a process, which is a process, which is a process, which is a process, which is a process, which is a process, which is a process, which is a process, which is a process, which is a process, which is a process, which is a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, a process,"}, {"heading": "2. Background", "text": "RL is a learning framework inspired by behaviors (Skinner, 1938) that describes how the feedback of the actors improves over time by taking action in an environment that maximizes the expected return, the cumulative future reward signal received by the agent (Sutton & Barto, 1998).The agent's control policy is iteratively improved by selecting measures that maximize the return. RL problems are often described as sequential decision problems modeled as Markov Decision Processes (MDPs) that define tuples: (State, Action, Transitions, \u03b3, Reward), full details of the MDPs are left out for space and can be found in (Sutton & Barto, 1998).The ultimate goal of an RL agent is to determine a policy that maps a given current state to the right measures to maximize the expected return. AC In this work we use an algorithm (1) similar to a continuous actuator."}, {"heading": "2.1. General Interactive Learning Framework", "text": "The system consists of the classical RL learning diagram (Sutton & Barto, 1998), as shown in the dotted red field surrounding the agent, the environment, and the lines for action a, state s, and reward r. Both the agent and the environment have interactive components that humans can directly influence through ah. Humans can take actions that affect the dense feedback channel F or the dense state signal S. Humans perceive the dense indication signal D, which can take many different forms, including, but not limited to, the visual perception of a screen. The feedback interface serves as a filter through which human feedback actions are translated into a real valuable human signal h, and similarly, the state interface translates human state signals into state information to the agent sh. Finally, the agent provides meta information about the action that is invisible from the environment in a +, and the environment can be directly observed by humans through a state signal in a meta field for an experiment."}, {"heading": "3. Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Experimental configuration: Aldebaran Nao and Myo Electromyography (EMG) Data", "text": "When this configuration is compared to the Aldebaran Nao simulation platform, it is easy to see how the components fit together. Man delivers control signals via a state interface, in this case an 8-channel wireless myo-electromyography (EMG), and feedback to the feedback interface (keyboard) connected to the learning system, in this case a MacBook Air (Apple, 2.2 GHz Intel Core i7, 8GB RAM), which provides the user with a view of the simulated robot and logs data for future analysis. All the experiments in this paper are conducted using a simulated Nao platform. The performance of this experimental setup is comparable, albeit with less environmental noise."}, {"heading": "3.2. Actor-Critic Reinforcement Learning", "text": "In this paper, we use a continuous actor-critic algorithm (AC) (algorithm 1) similar to that described in previous work (Pilarski et al., 2011; 2013; Mathewson & Pilarski, 2016; 2017).AC methods can reduce the variance in gradient estimation by using two learning systems: a policy-focused actor (selects the best action) and a critic (estimation of value function, criticizes the actor) (Sutton & Barto, 1998).Actor-critic algorithms are a subset of gradient-based algorithms. In these algorithms, control politics (a | s) is a function that defines the probability that the system uses to select an action in a state."}, {"heading": "4. Experiments", "text": "We present the first user study (N = 13) of human-robot interaction, in which humans simultaneously provide control and feedback signals. Our experimental configuration is similar to that of Mathewson & Pilarski (2016; 2017).We aim to compare a human's conditions with control signals, feedback signals, and both control and feedback signals.By studying the effects of actual human trainers, simultaneous control and feedback signals to the RL system during the performance of a self-mirrored motion control task, we can correctly explain the differences between the conditions.The task is designed so that humans require a multiple task but cannot be excessively cognitively demanding.First, the right arm of the Nao system is pre-programmed to move in a periodic pattern of flexion and expansion at the elbow connection between two stable setpoints within the permissible joint range."}, {"heading": "5. Results", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...)"}, {"heading": "6. Discussion", "text": "This work represents the first human user study to examine combinations of human and environmental feedback in a robotic system with human-derived EMG control signals. Simulation and physical experimentation have shown consistency in past studies. Some performance decreases can be expected with physical robot systems due to heat, inertia and mechanical jitter.Simulation provides safety, controlled repeatability and batch processing (Mathewson & Pilarski, 2016).The question of optimal outcome measurements for measuring human-machine interaction remains open. In this study, we investigate the effects of tightly coupled control signals on feedback and the effects of fast-delivered feedback signals on control signals. In optimal collaboration, humans should not have to provide signals (control and feedback) when the system does not benefit them. Likewise, the system should begin to model the signals delivered by humans (Knox & Stone, 2009; Vien al et al, 2013, a limited number of human feedback may be available)."}, {"heading": "7. Conclusions", "text": "The main contributions of this paper are twofold: First, we present the first human user study on a collaborative task between humans and robots, in which humans simultaneously provide control and feedback signals in the training of an actor and critic of a robot agent. Second, we present a General Interactive Learning Framework (Fig. 2) that is useful for clearly formulating and understanding communication channels in IML systems.Our findings suggest that feedback changes when control signals are given simultaneously, and that feedback can be given without compromising the quality of control signals. Likewise, we show results of how the feedback provided changes over the course of the training.This work provides novel results and a new perspective on the human training of a semi-autonomous robot system, and therefore takes important steps towards IML, where collaborative human and machine intelligence will be able to solve complex problems of the future."}], "references": [{"title": "Interactive machine", "author": ["S00568ED1V01Y201402AIM028. Fails", "Jerry Alan", "Olsen Jr.", "Dan R"], "venue": null, "citeRegEx": "Fails et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fails et al\\.", "year": 2014}, {"title": "Interactively Shaping", "author": ["Knox", "W. Bradley", "Stone", "Peter"], "venue": null, "citeRegEx": "Knox et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Knox et al\\.", "year": 1984}, {"title": "Reinforcement learning from human reward: Discounting in episodic tasks", "author": ["Knox", "W Bradley", "Stone", "Peter"], "venue": "In The 21st IEEE International Symposium on Robot and Human Interactive Communication,", "citeRegEx": "Knox et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Knox et al\\.", "year": 2012}, {"title": "Framing reinforcement learning from human reward: Reward positivity, temporal discounting, episodicity, and performance", "author": ["Knox", "W. Bradley", "Stone", "Peter"], "venue": "Artificial Intelligence,", "citeRegEx": "Knox et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Knox et al\\.", "year": 2015}, {"title": "Design principles for creating human-shapable agents", "author": ["Knox", "W. Bradley", "Fasel", "Ian", "Stone", "Peter"], "venue": "In AAAI Spring 2009 Symposium on Agents that Learn from Human Teachers,", "citeRegEx": "Knox et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Knox et al\\.", "year": 2009}, {"title": "How humans teach agents", "author": ["Knox", "W Bradley", "Glass", "Brian D", "Love", "Bradley C", "Maddox", "W Todd", "Stone", "Peter"], "venue": "International Journal of Social Robotics,", "citeRegEx": "Knox et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Knox et al\\.", "year": 2012}, {"title": "Convergent Actor Critic by Humans", "author": ["Macglashan", "James", "Littman", "Michael L", "Roberts", "David L", "Loftin", "Robert", "Peng", "Bei", "Taylor", "Matthew E"], "venue": "In International Conference on Intelligent Robots and Systems,", "citeRegEx": "Macglashan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Macglashan et al\\.", "year": 2016}, {"title": "Interactive learning from policydependent human", "author": ["MacGlashan", "James", "Ho", "Mark K", "Loftin", "Robert Tyler", "Peng", "Bei", "Roberts", "David L", "Taylor", "Matthew E", "Littman", "Michael L"], "venue": "feedback. CoRR,", "citeRegEx": "MacGlashan et al\\.,? \\Q2017\\E", "shortCiteRegEx": "MacGlashan et al\\.", "year": 2017}, {"title": "Reinforcement learning based embodied agents modelling human users through interaction and multi-sensory perception", "author": ["Mathewson", "Kory Wallace", "Pilarski", "Patrick M"], "venue": "Accepted to the 2017 AAAI Spring Symposium", "citeRegEx": "Mathewson et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mathewson et al\\.", "year": 2017}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Ng", "Andrew Y", "Harada", "Daishi", "Russell", "Stuart J"], "venue": "In Proceedings of the 16th International Conference on Machine Learning,", "citeRegEx": "Ng et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Between instruction and reward: Human-prompted switching", "author": ["Pilarski", "Patrick", "Sutton", "Richard"], "venue": "In AAAI Fall Symposium Series: Robots Learning Interactively from Human Teachers,", "citeRegEx": "Pilarski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pilarski et al\\.", "year": 2012}, {"title": "Real-time Prediction Learning for the Simultaneous Actuation of Multiple Prosthetic Joints", "author": ["Pilarski", "Patrick M", "Dick", "Travis B", "Sutton", "Richard S"], "venue": "In IEEE International Conference on Rehabilitation Robotics,", "citeRegEx": "Pilarski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pilarski et al\\.", "year": 2013}, {"title": "Prosthetic devices as goal-seeking agents", "author": ["Pilarski", "Patrick M", "Sutton", "Richard S", "Mathewson", "Kory W"], "venue": "In 2nd Workshop on Present and Future of Non-Invasive Peripheral-Nervous-System Machine Interfaces, Singapore,", "citeRegEx": "Pilarski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pilarski et al\\.", "year": 2015}, {"title": "The Behavior of Organisms: An experimental analysis", "author": ["Skinner", "Burrhus Frederic"], "venue": "Appleton-Century, Oxford,", "citeRegEx": "Skinner and Frederic.,? \\Q1938\\E", "shortCiteRegEx": "Skinner and Frederic.", "year": 1938}, {"title": "Summary of levels of driving automation for on-road vehicles", "author": ["Smith", "BW"], "venue": "Center for Internet and Society, Stanford Law School,", "citeRegEx": "Smith and BW.,? \\Q2013\\E", "shortCiteRegEx": "Smith and BW.", "year": 2013}, {"title": "Reinforcement learning: An introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners", "author": ["Thomaz", "Andrea L", "Breazeal", "Cynthia"], "venue": "Artificial Intelligence,", "citeRegEx": "Thomaz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Thomaz et al\\.", "year": 2008}, {"title": "Real-time interactive reinforcement learning for robots", "author": ["Thomaz", "Andrea Lockerd", "Hoffman", "Guy", "Breazeal", "Cynthia"], "venue": "In AAAI 2005 Workshop on Human Comprehensible Machine Learning.,", "citeRegEx": "Thomaz et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Thomaz et al\\.", "year": 2005}, {"title": "Teaching on a budget: Agents advising agents in reinforcement learning", "author": ["Torrey", "Lisa", "Taylor", "Matthew"], "venue": "In Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Torrey et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Torrey et al\\.", "year": 2013}, {"title": "Learning via human feedback in continuous state and action spaces", "author": ["Vien", "Ngo Anh", "Ertel", "Wolfgang", "Chung", "Tae Choong"], "venue": "Applied Intelligence,", "citeRegEx": "Vien et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vien et al\\.", "year": 2013}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine Learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}], "referenceMentions": [{"referenceID": 17, "context": "In this paper we focus specifically on humans interacting with reinforcement learning (RL) systems (Thomaz et al., 2005; Knox & Stone, 2015).", "startOffset": 99, "endOffset": 140}, {"referenceID": 11, "context": "Of note, Actor-Critic Reinforcement Learning (ACRL) is a family of RL algorithms which have shown promise in past studies as a way for humans and autonomous agents to collaborate in tightly coupled HMI like prosthetic limbs and other robots (Pilarski et al., 2013; 2011).", "startOffset": 241, "endOffset": 270}, {"referenceID": 6, "context": "show that human feedback may be better modelled as an advantage function to handle changes in a feedback strategy over time (Macglashan et al., 2016) and that these techniques may be extended to the policy space (MacGlashan et al.", "startOffset": 124, "endOffset": 149}, {"referenceID": 7, "context": ", 2016) and that these techniques may be extended to the policy space (MacGlashan et al., 2017).", "startOffset": 70, "endOffset": 95}, {"referenceID": 2, "context": "It has been shown that the limited human feedback can be applied across near-optimal state-action pairs, and support the agent learning an optimal solution (Pilarski et al., 2011; Knox et al., 2012).", "startOffset": 156, "endOffset": 198}, {"referenceID": 4, "context": "This reaction delay is about 20 time steps, which corresponds to 660ms, which is well within the human reaction time window (Knox et al., 2009; Hockley, 1984).", "startOffset": 124, "endOffset": 158}, {"referenceID": 9, "context": "More feedback can provide additional shaping toward optimal goal states thereby improving performance, but has a higher probability of creating positive reward cycles (Ng et al., 1999).", "startOffset": 167, "endOffset": 184}, {"referenceID": 19, "context": "Similarly, the system should start to model the human delivered signals (Knox & Stone, 2009; Vien et al., 2013).", "startOffset": 72, "endOffset": 111}, {"referenceID": 12, "context": "Bandwidth and explicitness in human signals delivered to learning agents has been explored previously (Pilarski & Sutton, 2012; Pilarski et al., 2015).", "startOffset": 102, "endOffset": 150}, {"referenceID": 6, "context": "8, as has been previously explored in related work (Loftin et al., 2016b; Macglashan et al., 2016).", "startOffset": 51, "endOffset": 98}, {"referenceID": 6, "context": "Future algorithmic improvements should exploit predicted teacher style to build beliefs about the provided feedback (Loftin et al., 2016b; Macglashan et al., 2016).", "startOffset": 116, "endOffset": 163}], "year": 2017, "abstractText": "This paper contributes a first study into how different human users deliver simultaneous control and feedback signals during human-robot interaction. As part of this work, we formalize and present a general interactive learning framework for online cooperation between humans and reinforcement learning agents. In many humanmachine interaction settings, there is a growing gap between the degrees-of-freedom of complex semi-autonomous systems and the number of human control channels. Simple human control and feedback mechanisms are required to close this gap and allow for better collaboration between humans and machines on complex tasks. To better inform the design of concurrent control and feedback interfaces, we present experimental results from a human-robot collaborative domain wherein the human must simultaneously deliver both control and feedback signals to interactively train an actor-critic reinforcement learning robot. We compare three experimental conditions: 1) human delivered control signals, 2) reward-shaping feedback signals, and 3) simultaneous control and feedback. Our results suggest that subjects provide less feedback when simultaneously delivering feedback and control signals and that control signal quality is not significantly diminished. Our data suggest that subjects may also modify when and how they provide feedback. Through algorithmic development and tuning informed by this study, we expect semi-autonomous actions of robotic agents can be better shaped by human feedback, allowing for seamless collaboration and improved performance in difficult interactive domains. University of Alberta, Dep. of Computing Science, Edmonton, Canada University of Alberta, Deps. of Medicine and Computing Science, Edmonton, Alberta, Canada. Correspondence to: Kory Mathewson <korym@ualberta.ca>. Under review for the 34 th International Conference on Machine Learning, Sydney, Australia, 2017. JMLR: W&CP. Copyright 2017 by the authors. Figure 1. Experimental configuration. One of the study participants with the Myo band on their right arm providing a control signal, while simultaneously providing feedback signals with their left hand. The Aldebaran Nao robot simulation is visible on the screen alongside experimental logging.", "creator": "LaTeX with hyperref package"}}}