{"id": "1703.01327", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Multi-step Reinforcement Learning: A Unifying Algorithm", "abstract": "Unifying seemingly disparate algorithmic ideas to produce better performing algorithms has been a longstanding goal in reinforcement learning. As a primary example, TD($\\lambda$) elegantly unifies one-step TD prediction with Monte Carlo methods through the use of eligibility traces and the trace-decay parameter $\\lambda$. Currently, there are a multitude of algorithms that can be used to perform TD control, including Sarsa, $Q$-learning, and Expected Sarsa. These methods are often studied in the one-step case, but they can be extended across multiple time steps to achieve better performance. Each of these algorithms is seemingly distinct, and no one dominates the others for all problems. In this paper, we study a new multi-step action-value algorithm called $Q(\\sigma)$ which unifies and generalizes these existing algorithms, while subsuming them as special cases. A new parameter, $\\sigma$, is introduced to allow the degree of sampling performed by the algorithm at each step during its backup to be continuously varied, with Sarsa existing at one extreme (full sampling), and Expected Sarsa existing at the other (pure expectation). $Q(\\sigma)$ is generally applicable to both on- and off-policy learning, but in this work we focus on experiments in the on-policy case. Our results show that an intermediate value of $\\sigma$, which results in a mixture of the existing algorithms, performs better than either extreme. The mixture can also be varied dynamically which can result in even greater performance.", "histories": [["v1", "Fri, 3 Mar 2017 20:19:08 GMT  (452kb,D)", "http://arxiv.org/abs/1703.01327v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["kristopher de asis", "j fernando hernandez-garcia", "g zacharias holland", "richard s sutton"], "accepted": false, "id": "1703.01327"}, "pdf": {"name": "1703.01327.pdf", "metadata": {"source": "META", "title": "Multi-step Reinforcement Learning: A Unifying Algorithm", "authors": ["Kristopher De Asis", "Fernando Hernandez-Garcia", "Zacharias Holland", "Richard S. Sutton"], "emails": ["KLDEASIS@UALBERTA.CA", "JFHERNAN@UALBERTA.CA", "GHOLLAND@UALBERTA.CA", "RSUTTON@UALBERTA.CA"], "sections": [{"heading": "1. The Landscape of TD Algorithms", "text": "Temporary Difference (TD) Methods (Sutton, 1988) are an important concept in the field of reinforcing learning (RL) that combines ideas from Monte Carlo and dynamic programming methods. TD methods allow learning directly from the raw experience, while the model of environmental dynamics, as with Monte Carlo methods, can be fully updated without waiting for a final result, as with dynamic programming. The core concepts of TD methods provide a flexible framework for creating a variety of powerful algorithms that can be used both for prediction and for control. There are a number of TD control methods that have been proposed. Q-Learning (Watkins, 1989) is probably the most popular method and is considered an off-policy method that generalizes behavior (behavioral policy), and the policies that are learned (target policy) are different. Sarsa (Rummery & Niranjan, 1994) is the classical method of governance, where policy and behavior are the goals."}, {"heading": "2. MDPs and One-step Solution Methods", "text": "The sequential decision problem encountered in RL is often modeled as a Markov decision process (MDP). Under this framework, an agent and the environment interact through a sequence of discrete time steps. At each step, the agent receives information about the state of the environment, where S is the set of all possible states. The agent uses this information to select an action in which all possible measures are taken. On the basis of the behavior of the agent and the state of the environment, the agent receives a reward, Rt + 1, R, and moves to another state, St + 1, with a state transitional probability p (s, a) = P (St + 1 = s), for a reward. The agent behaves according to a policy procedure (a) that is a probability distribution over the set S \u00d7 A. Through the process of policy iteration (SutBarto, 1998), we learn the optimal distribution of probability."}, {"heading": "3. Atomic Multi-Step Algorithms", "text": "The TD methods described in the previous section can be generalized even further by applying them over longer time intervals, which has shown that the bias of updating is reduced at the expense of increasing variance (Jaakkola, Jordan, & Singh, 1994).However, in many cases it is possible to achieve better performance by selecting a value for the backup length parameter, n, greater than one that generates an atomic multi-step algorithm (Sutton & Barto, 1998).1 Just as entry methods are defined by their TD error, each atomic multi-step algorithm is characterized by its n-step return. However, for Sarsa atomic multi-step algorithm policy, n-step return is not possible: G (n) t = Rt + 1 + 2 + Qgorithms 2Rt + 3 + 3 + QQalgorithms n + n + n +."}, {"heading": "4. Tree-backup", "text": "As shown in (11), the TD yield of n-step Expected Sarsa is calculated by assuming an expectation of the measures taken in the last step of the backup (Precup et al., 2000).The resulting algorithm is a multi-level generalization of Expected Sarsa, known as tree backup due to its characteristic backup diagram (Figure 1).Since Expected Sarsa subsumes Q-Learning, loyalty backup can also be considered a multi-level generalization of Q-Learning if the target policy is the greedy policy regarding the action value feature. Tree backup has several advantages over n-step Expected Sarsa. Tree backup has the ability to learn outside of politics without the need for a weight test. This has the effect of reducing variance and speeding up learning."}, {"heading": "5. TheQ(\u03c3) Algorithm", "text": "In the previous sections, we have introduced incrementally several generalizations for the TD control methods Sarsa and Expected Sarsa, and in this section, we introduce an algorithm that refers to them as Q (\u03c3). Sarsa can therefore be generalized to an atomic multi-step algorithm by using an n-step return and n-step algorithms: Tree Backup and nstep Expected algorithms by using Importance Sampling. In contrast, all algorithms presented so far can be divided into two families: those that back up their actions as examples such as Sarsa; and those that consider an expectation of all actions, such as Expected Sarsa and Tree Backup.2 In this section, we introduce a method for unifying both algorithms by introducing a new parameter."}, {"heading": "6. Experiments", "text": "In this section, we examine the performance of the various atomic multi-level algorithms that we have presented. First, we evaluate their performance against a predictive task that motivates the unification of Sarsa and Expected Sarsa, and we also introduce the idea of dynamically varying the characteristics of each algorithm in order to exploit the particular performance characteristics. Then, we show a navigation problem in the network world that it is possible to improve performance with Q (\u03c3). 3For an example of the psuedocode for general non-political Q (\u03c3) see Sutton and Barto (2017). Algorithm 1 In-step of the network world Q (\u03c3) to estimate q\u03c0 Initiate S0 6 = Terminal; select A0 according to Yankee (. | S0) Save S0, A0 and Q (S0, A0) for t = 0, 1, 2,..., T + n \u2212 1 Doif t < St \u2212 Note an endangered value (."}, {"heading": "6.1. 19-State Random Walk", "text": "The 19-State Random Walk is a 1-dimensional environment in which an agent randomly makes transitions to one of two neighboring states. There is a terminal state at each end of the environment, the transition to one of them gives a reward of -1, and the transition to the other gives a reward of 1. To compare algorithms that include an expectation based on its policy, the task is formulated in such a way that each state has two actions. Each action leads deterministically to one of the two neighboring states, and the agent learns within the framework of an equivalent random behavior policy, which differs from typical random walk settings in which each state has an action that randomly merges into one of the two neighboring states (Sutton & Barto, 1998), but the resulting state values are identical. This environment has been treated as a predictive task, with a learning algorithm consisting of assessing a value function among its behavioral policies."}, {"heading": "6.2. Stochastic Windy Gridworld", "text": "The Windy Web World is a tabular navigation task in a standard Web world described in (Sutton & Barto, 1998). There is a start and a destination state, and there are several possible movements: to the right, left, up and down. As the agent moves into one of the middle columns of the Web World, it is influenced by a \"wind\" upward that shifts the resulting next state by a number of cells up and varies from column to column. Figure 4 shows the layout of the Windy Web World along with the strengths of the wind in each column. If the agent stands at the edge of the world and selects a move that would cause him to leave the Web or be pushed out of the world, he is simply replaced in the next state at the edge of the world. At each step, the agent receives a constant reward of -1 until the goal is reached."}, {"heading": "6.3. Mountain Cliff", "text": "We implemented a variant of the classic episodic task, Mountain Car, as described by Sutton and Barto Q (1998). For this implementation, the rewards, measures and objectives remained the same. However, if the agent ever ventured beyond the top of the left mountain, he fell off a cliff, but was rewarded -100 and returned to a random starting point in the valley between the two hills. We called this environment Mountain Cliff. Both environments were tested and showed the same trend in the results. However, the results obtained in the mountain cliff were more pronounced and more suitable for demonstration purposes. Since the state space is continuous, we approached q\u03c0 using tile coding functions. Specifically, we used version 3 of Sutton's tile coding software (n.d.) with 8 slopes, an asymmetrical compensation by successive odd numbers, and each tile having over 1 / 8 fraction of the feature space, which has a resolution of about 1.6% of each of each algorithm."}, {"heading": "7. Discussion", "text": "Our experiments show that it makes sense to unify the space of the algorithms with Q (\u03c3), whereas the effectiveness of the results with Q (\u03c3) is lower. In prediction tasks such as the 19-state random response, which varies the degree of scanning, there is a trade-off between initial and asymptotic performance. In control tasks such as the stochastic windy grid, a right-centered moving average with a window of 100 consecutive episodes is capable of smoothing out the results. Q (\u03c3) with dynamic development had the best performance of all algorithms. Age yield as either extreme, depending on the number of episodes elapsed. These results also extend to tasks with continuous states, such as the Berg Cliff."}, {"heading": "8. Conclusions", "text": "In this paper, we examined Q (\u03c3) -397.97 -399.80 -396.39-396.69 Q 404.71 Q. Q (\u03c3), by using the sampling parameter \u03c3, allows continuous variation between updating based on complete sampling andTable 1. Summaries of initial and final performance for all atomic multi-stage algorithms around the mountain cliff. Standard error (SE) and lower (LB) 95% confidence interval problems are provided to validate the results. Q (0.5) had the best initial performance, whereas dynamic Q (\u03c3) had the best final performance. Average yield per episode after 50 episode algorithm Mean SE LB UBQ (1), Sarsa -447.27 1.15 -449.17 -445.37 Q (0), tree backup -429.15 1.77 -426.24 Q (0.5 -397.39-396.71 Dynamics."}, {"heading": "Acknowledgements", "text": "The authors thank Vincent Zhang, Harm van Seijen, Doina Precup, and Pierre-luc Bacon for their insights and discussions that contributed to the results presented in this paper, and the entire Reinforcement Learning and Artificial Intelligence research group for providing the environment to promote and support this research, and we thank Alberta Innovates - Technology Futures, Google Deepmind, and the Natural Sciences and Engineering Research Council of Canada for funding."}], "references": [{"title": "On the convergence of stochastic iterative dynamic programming algorithms", "author": ["T. Jaakkola", "M.I. Jordan", "S.P. Singh"], "venue": "Neural Computation", "citeRegEx": "Jaakkola et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1994}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "Precup et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Precup et al\\.", "year": 2000}, {"title": "Problem Solving with Reinforcement Learning", "author": ["G.A. Rummery"], "venue": "PhD Thesis,", "citeRegEx": "Rummery,? \\Q1995\\E", "shortCiteRegEx": "Rummery", "year": 1995}, {"title": "On-line qlearning using connectionist systems", "author": ["G.A. Rummery", "M. Niranjan"], "venue": "Technical Report CUED/F-INFENG/TR 166,", "citeRegEx": "Rummery and Niranjan,? \\Q1994\\E", "shortCiteRegEx": "Rummery and Niranjan", "year": 1994}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning", "citeRegEx": "Sutton,? \\Q1988\\E", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["R.S. Sutton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sutton,? \\Q1996\\E", "shortCiteRegEx": "Sutton", "year": 1996}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Reinforcement Learning: An Introduction (2nd ed.). Manuscript in preparation", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q2017\\E", "shortCiteRegEx": "Sutton and Barto", "year": 2017}, {"title": "A theoretical and empirical analysis of expected sarsa", "author": ["H. van Seijen", "H. van Hasselt", "S. Whiteson", "M. Wiering"], "venue": "In Proceedings of the IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning,", "citeRegEx": "Seijen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Seijen et al\\.", "year": 2009}, {"title": "Learning from Delayed Rewards", "author": ["Watkins", "C.J.C. H"], "venue": null, "citeRegEx": "Watkins and H.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and H.", "year": 1989}], "referenceMentions": [{"referenceID": 4, "context": "Temporal-difference (TD) methods (Sutton, 1988) are an important concept in reinforcement learning (RL) which combines ideas from Monte Carlo and dynamic programming methods.", "startOffset": 33, "endOffset": 47}, {"referenceID": 5, "context": "Sarsa (Rummery & Niranjan, 1994; Sutton, 1996) is the classical on-policy control method, where the behaviour and target policies are the same.", "startOffset": 6, "endOffset": 46}, {"referenceID": 4, "context": "The TD(\u03bb) algorithm unifies one-step TD learning with Monte Carlo methods (Sutton, 1988).", "startOffset": 74, "endOffset": 88}, {"referenceID": 2, "context": "The concept of eligibility traces can also be applied to TD control methods such as Sarsa and Q-learning, which can create more efficient learning and produce better performance (Rummery, 1995).", "startOffset": 178, "endOffset": 193}, {"referenceID": 1, "context": "The Tree-backup algorithm was originally presented as a method to perform off-policy evaluation when the behaviour policy is non-Markov, non-stationary or completely unknown (Precup et al., 2000).", "startOffset": 174, "endOffset": 195}, {"referenceID": 4, "context": "Q(\u03c3) is an algorithm that was first proposed by Sutton and Barto (2017) which unifies and generalizes the existing multi-step TD control methods.", "startOffset": 48, "endOffset": 72}, {"referenceID": 1, "context": "n-step Sarsa can be adapted for off-policy learning by introducing an importance sampling ratio term (Precup et al., 2000):", "startOffset": 101, "endOffset": 122}, {"referenceID": 1, "context": "A drawback to using importance sampling to learn offpolicy is that it can create high variance which must be compensated for by using small step sizes; this can slow learning (Precup et al., 2000).", "startOffset": 175, "endOffset": 196}, {"referenceID": 1, "context": "However, it is possible to extend this idea to every time step of the backup by taking an expectation at every step (Precup et al., 2000).", "startOffset": 116, "endOffset": 137}, {"referenceID": 1, "context": "Additionally, because an importance sampling ratio does not need to be computed, the behavior policy does not need to be stationary, Markov, or even known (Precup et al., 2000).", "startOffset": 155, "endOffset": 176}, {"referenceID": 4, "context": "This atomic version of multi-step Tree-backup was first presented by Sutton and Barto (2017).", "startOffset": 69, "endOffset": 93}, {"referenceID": 1, "context": "The possibility of unifying Sarsa and Tree-backup was first suggested by Precup et al. (2000), and the first formulation of Q(\u03c3) was presented by Sutton and Barto (2017).", "startOffset": 73, "endOffset": 94}, {"referenceID": 1, "context": "The possibility of unifying Sarsa and Tree-backup was first suggested by Precup et al. (2000), and the first formulation of Q(\u03c3) was presented by Sutton and Barto (2017).", "startOffset": 73, "endOffset": 170}, {"referenceID": 4, "context": "For an example of the psuedocode for general off-policy Q(\u03c3), see Sutton and Barto (2017). Algorithm 1 On-policy n-step Q(\u03c3) for estimating q\u03c0 Initialize S0 6= terminal; select A0 according to \u03c0(.", "startOffset": 66, "endOffset": 90}, {"referenceID": 4, "context": "The windy gridworld as described by Sutton and Barto (1998). The start and goal states are denoted by S and G respectively.", "startOffset": 36, "endOffset": 60}, {"referenceID": 4, "context": "We implemented a variant of the classical episodic task, mountain car, as described by Sutton and Barto (1998). For this implementation, the rewards, actions and goal remained the same.", "startOffset": 87, "endOffset": 111}], "year": 2017, "abstractText": "Unifying seemingly disparate algorithmic ideas to produce better performing algorithms has been a longstanding goal in reinforcement learning. As a primary example, TD(\u03bb) elegantly unifies one-step TD prediction with Monte Carlo methods through the use of eligibility traces and the trace-decay parameter \u03bb. Currently, there are a multitude of algorithms that can be used to perform TD control, including Sarsa, Q-learning, and Expected Sarsa. These methods are often studied in the one-step case, but they can be extended across multiple time steps to achieve better performance. Each of these algorithms is seemingly distinct, and no one dominates the others for all problems. In this paper, we study a new multi-step action-value algorithm called Q(\u03c3) which unifies and generalizes these existing algorithms, while subsuming them as special cases. A new parameter, \u03c3, is introduced to allow the degree of sampling performed by the algorithm at each step during its backup to be continuously varied, with Sarsa existing at one extreme (full sampling), and Expected Sarsa existing at the other (pure expectation). Q(\u03c3) is generally applicable to both onand off-policy learning, but in this work we focus on experiments in the on-policy case. Our results show that an intermediate value of \u03c3, which results in a mixture of the existing algorithms, performs better than either extreme. The mixture can also be varied dynamically which can result in even greater performance. 1. The Landscape of TD Algorithms Temporal-difference (TD) methods (Sutton, 1988) are an important concept in reinforcement learning (RL) which combines ideas from Monte Carlo and dynamic programming methods. TD methods allow learning to occur directly from raw experience in the absence of a model of the environment\u2019s dynamics, like with Monte Carlo methods, while also allowing estimates to be updated based on other learned estimates without waiting for a final result, like with dynamic programming. The core concepts of TD methods provide a flexible framework for creating a variety of powerful algorithms that can be used for both prediction and control. There are a number of TD control methods that have been proposed. Q-learning (Watkins, 1989) is arguably the most popular, and is considered an off-policy method because the policy generating the behaviour (the behaviour policy), and the policy that is being learned (the target policy) are different. Sarsa (Rummery & Niranjan, 1994; Sutton, 1996) is the classical on-policy control method, where the behaviour and target policies are the same. However, Sarsa can be extended to learn off-policy with the use of importance sampling (Precup, Sutton, & Singh, 2000). Expected Sarsa is an extension of Sarsa that, instead of using the action-value of the next state to update the value of the current state, uses the expectation of all the subsequent action-values of the current state with respect to the target policy. Expected Sarsa has been studied as a strictly on-policy method (van Seijen, van Hasselt, Whiteson, & Wiering, 2009), but in this paper we present a more general version that can be used for both onand off-policy learning and that also subsumes Q-learning. All of these methods are often described in the simple one-step case, but they can also be extended across multiple time steps. The TD(\u03bb) algorithm unifies one-step TD learning with Monte Carlo methods (Sutton, 1988). Through the use of eligibility traces, and the trace-decay parameter, \u03bb \u2208 [0, 1], ar X iv :1 70 3. 01 32 7v 1 [ cs .A I] 3 M ar 2 01 7 Multi-step Reinforcement Learning: A Unifying Algorithm a spectrum of algorithms is created. At one end, \u03bb = 1, exists Monte Carlo methods, and at the other, \u03bb = 0, exists one-step TD learning. In the middle of the spectrum are intermediate methods which can perform better than the methods at either extreme (Sutton & Barto, 1998). The concept of eligibility traces can also be applied to TD control methods such as Sarsa and Q-learning, which can create more efficient learning and produce better performance (Rummery, 1995). Multi-step TD methods are usually thought of in terms of an average of many multi-step returns of differing lengths and are often associated with eligibility traces, as is the case with TD(\u03bb). However, it is also natural to think of them in terms of individual n-step returns with their associated nstep backup (Sutton & Barto, 1998). We refer to each of these individual backups as atomic backups, whereas the combination of several atomic backups of different lengths creates a compound backup. In the existing literature, it is not clear how best to extend one-step Expected Sarsa to a multi-step algorithm. The Tree-backup algorithm was originally presented as a method to perform off-policy evaluation when the behaviour policy is non-Markov, non-stationary or completely unknown (Precup et al., 2000). In this paper, we re-present Tree-backup as a natural multi-step extension of Expected Sarsa. Instead of performing the updates with entirely sampled transitions as with multi-step Sarsa, Treebackup performs the update using the expected values of all the actions at each transition. Q(\u03c3) is an algorithm that was first proposed by Sutton and Barto (2017) which unifies and generalizes the existing multi-step TD control methods. The degree of sampling performed by the algorithm is controlled by the sampling parameter, \u03c3. At one extreme (\u03c3 = 1) exists Sarsa (full sampling), and at the other (\u03c3 = 0) exists Tree-backup (pure expectation). Intermediate values of \u03c3 create algorithms with a mixture of sampling and expectation. In this work we show that an intermediate value of \u03c3 can outperform the algorithms that exist at either extreme. In addition, we show that \u03c3 can be varied dynamically to produce even greater performance. We limit our discussion of Q(\u03c3) to the atomic multi-step case without eligibility traces, but a natural extension is to make use of compound backups and is an avenue for future research. Furthermore, Q(\u03c3) is generally applicable to both onand off-policy learning, but for our initial empirical study we examined only on-policy prediction and control problems. 2. MDPs and One-step Solution Methods The sequential decision problem encountered in RL is often modeled as a Markov decision process (MDP). Under this framework, an agent and the environment interact over a sequence of discrete time steps t. At every time step, the agent receives information about the environment\u2019s state, St \u2208 S , where S is the set of all possible states. The agent uses this information to select an action, At, from the set of all possible actions A. Based on the behavior of the agent and the state of the environment, the agent receives a reward, Rt+1 \u2208 R \u2282 R, and moves to another state, St+1 \u2208 S, with a state-transition probability p(s\u2032|s, a) = P (St+1 = s |St = s,At = a), for a \u2208 A and s, s\u2032 \u2208 S. The agent behaves according to a policy \u03c0(a|s), which is a probability distribution over the set S \u00d7 A. Through the process of policy iteration (Sutton & Barto, 1998), the agent learns the optimal policy, \u03c0\u2217, that maximizes the expected discounted return: Gt = Rt+1+\u03b3Rt+2+\u03b3 Rt+3+... = T\u22121 \u2211", "creator": "LaTeX with hyperref package"}}}