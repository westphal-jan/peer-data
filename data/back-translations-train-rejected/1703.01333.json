{"id": "1703.01333", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Towards Monetary Incentives in Social Q&A Services", "abstract": "Community based question answering (CQA) services receive a large volume of questions today. It is increasingly challenging to motivate domain experts to give timely answers. Recently, payment-based CQA services explore new incentive models to engage real-world experts and celebrities by allowing them to set a price on their answers. In this paper, we perform a data-driven analysis on Fenda, a payment-based CQA service that has gained initial success with this incentive model. Using a large dataset of 220K paid questions (worth 1 million USD) over two months, we examine how monetary incentives affect different players in the system and their over-time engagement. Our study reveals several key findings: while monetary incentive enables quick answers from experts, it also drives certain users to aggressively game the systems for profits. In addition, this incentive model turns CQA service into a supplier-driven marketplace where users need to proactively adjust their price as needed. We find famous people are unwilling to lower their price, which in turn hurts their income and engagement-level over time. Based on our results, we discuss the implications to future payment-based CQA design.", "histories": [["v1", "Fri, 3 Mar 2017 20:36:38 GMT  (1140kb)", "https://arxiv.org/abs/1703.01333v1", null], ["v2", "Fri, 28 Apr 2017 01:48:18 GMT  (1840kb)", "http://arxiv.org/abs/1703.01333v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.HC", "authors": ["steve t k jan", "chun wang", "qing zhang", "gang wang"], "accepted": false, "id": "1703.01333"}, "pdf": {"name": "1703.01333.pdf", "metadata": {"source": "CRF", "title": "Towards Monetary Incentives in Social Q&A Services", "authors": ["Steve T.K. Jan", "Chun Wang", "Qing Zhang", "Gang Wang"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 3.01 333v 2 [cs.A I] 2 8A pr2 017Community-based Question Answering (CQA) Services face key challenges in motivating domain experts to provide timely answers. Recently, CQA Services has been exploring new incentive models to engage experts and celebrities by allowing them to set a price for their answers. In this post, we are conducting a data-driven analysis of two emerging payment-based CQA systems: Fenda (China) and Whale (USA). By analyzing a large dataset of 220K questions (worth $1 million collectively), we are examining how monetary incentives affect different actors in the system. We find that while monetary incentives enable quick responses from experts, they also induce certain users to aggressively gamble the system for profits. In addition, in this provider-driven market, users need to proactively adjust their prices in order to make future profits, which in turn will affect their incomes."}, {"heading": "INTRODUCTION", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution that meets the needs of the individual."}, {"heading": "RELATED WORK", "text": "In recent years, researchers have examined CQA services from various angles [32]; early studies have focused on identifying domain experts in a CQA community [26, 8] and directing user questions to the right experts [21, 27]; other studies have focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting inferior (or even abusive) content [16]; and finally, researchers have also examined Q & A activity in online social networks [25, 7]. As the size of CQA systems increases rapidly, it will be difficult to engage with experts in timely and high-quality responses [33]."}, {"heading": "Asker Answerer", "text": "Most crowdsourced marketplaces (e.g. Mechanical Turk) are now \"customer-oriented,\" where customers post their tasks and set the task price. [17] In such marketplaces, pricing strategy can affect work quality and response time [15, 23, 14]. Fenda and Whale, on the other hand, are \"vendor-oriented\" marketplaces where experts (suppliers) set the price of their answers (products). Part of our analysis is to understand users \"pricing strategies and their impact on their questions and financial returns."}, {"heading": "RESEARCH QUESTIONS AND METHOD", "text": "In fact, most people who stand up for people's rights are not in a position to defend their rights and benefits, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "DATA COLLECTION", "text": "We begin by collecting a large dataset of Fenda and Whale through their mobile APIs. Our dataset focused on user profiles, which included a complete list of historical questions answered by the user. In order to obtain a large number of active users, we examined various options (some of which did not work). First, we found that there was no central list to search all registered users. Second, a user's follow-up list is not public (only the total number is visible), and therefore Social Graph crawling is not practical. To this end, we started our crawling from the list of experts. For each expert, we collected their questions and the questioners \"profiles to obtain their answer list and extract new questioners. We repeated this process until no new users appeared. In this way, we gathered a large number of active users who asked or answered at least one dataset."}, {"heading": "ENGAGING WITH DOMAIN EXPERTS", "text": "As a CQA system run by real-world experts, we first examine the role and influence of domain experts within the system. More specifically, we examine domain experts \"contributions to the community in terms of generating content and promoting financial redress. Fenda maintains a list of verified experts and celebrities who are usually already known in their respective domains. At the time of data collection, there were 4370 verified experts who were classified by Fenda administrators into 44 categories. We refer to these 4,370 users as experts and the remaining 84,170 users as normal users. Whale has a similar list of experts (118 experts) and we refer to the remaining 1,301 whale users as normal users."}, {"heading": "Question Answering", "text": "Of the 212K responses in the Fenda dataset, 171K (81%) came from experts. Using this dataset, we can briefly estimate the contribution of experts in the context of the entire network. On June 27, 2016, Fenda officially announced a total of 500K responses and 10 million users [39]. By the same date, our dataset shows that the 4,370 experts (0.44% of the population) contributed 122K responses (24.4% of the total numbers). Individual experts in Fenda (FD) also received IRB approval: Protocol 16-1143.00.20.40.60.810 5 10 10 10 15 20C DF from A nsw ersPrice ($) WH-Normal WH-Expert FD-Normal FD-ExpertFigure 4th prize for each answer 0.20.40.60.81100 101 102 103 104C DF from A nsw Listers # Normal WH-5% of similar WH questions as 4.4% of users."}, {"heading": "Money", "text": "Experts play an important role in managing revenue. In total, the questions in the Fenda data set were worth $1,169,994, including payments from questioners and listeners. 4 The answers from the experts generated $1,106,561, accounting for 95% of the total revenue in our data set. To assess the contribution of the experts in the context of the entire network, we once again made an estimate: Fenda achieved revenue of $2 million as of June 27, 2016 [39]. By that same date (June 27), expert responses attracted $909,876 in our data set, accounting for a significant 45% of the $2 million revenue. Figure 4 and Figure 5 show that experts attract, on average, higher costs ($2.9 vs. $1.0) and more listeners (27 vs. 5) than normal users. Individually, experts also earn more money than normal users. Figure 3 shows the total income of users who answered at least one question."}, {"heading": "Expert Categories", "text": "Experts of different categories have different earning patterns. Table 2 shows the top 10 categories according to total income per category. In Fenda, the most popular experts are related to professional counseling. The top category is health, followed by career, business and relationship. In the health category, many experts are doctors and pediatricians from the real world. They give Fenda users medical advice on various (non-life threatening) topics such as headaches and flu at a cost of several dollars. Other popular categories such as movies and entertainment include questions to celebrities about their insider experience, gossip and opinions on Trending Events.Whale, on the other hand, has fewer experts. The highest earning experts refer to start-ups and technology. Note that whale experts can belong to several categories, so the percentage of cumulative income exceeds 100%. In Figure 6, we further illustrate the different earning patterns of Fenda experts."}, {"heading": "IMPACT OF MONETARY INCENTIVES", "text": "In this section, we conduct comprehensive analyses of the monetary incentive model to understand its impact on user behavior. Remarkably, Fenda and Whale use money to reward both questioners and questioners. To this end, we first analyze respondents to understand how they rate their responses and whether payments lead to on-demand responses. Second, we focus on questioners who analyze whether and how users make money by asking the right questions. Finally, we try to identify abnormal users such as \"bounty hunters\" who aggressively or strategically deprive the system of profits."}, {"heading": "Answerers", "text": "In order to motivate users (especially domain experts), both Fenda and Whale allow users to quickly determine the price of their answers. In the following, we examine how money affects the way users answer questions. In particular, we examine whether monetary incentives really do enable on-demand quick answers. Setting the Answer Price. To understand how users set a price for their answers, we calculate the Pearson correlation [31] between a user's price and different behavioral measures. In Table 3, we observe that the price has positive and significant correlations with the number of followers, listeners and questions. One possible explanation is that users with many followers and listeners are real-world celebrities who have the confidence to set a higher price. The higher price can also motivate them to answer more questions."}, {"heading": "Askers", "text": "The reason for this is that most people who are able to give a positive opinion of themselves only have a positive opinion of themselves. (...) The result shows that respondents are motivated to ask good questions that appeal to a wide range of interests. (...) As shown in Figure 8, the questioner's income is half of the listener's pay, with Fanda's commission fee and the initial question fee deducted. (...) Our result shows that questioners are motivated to ask good questions that attract broad interest. (...) As shown in Figure 8, 40% of respondents have successfully attracted enough listeners to return a positive profit to the questioner. (...) Figure 9 shows that 40% of them have a positive overall income. (...) However, the vast majority of questioners do not earn money."}, {"heading": "Abnormal Users", "text": "In fact, most of them are able to play by the rules."}, {"heading": "DYNAMIC PRICING AND USER ENGAGEMENT", "text": "In this section, we turn to the dynamic aspect to analyze how users adjust their response prices over time and how different pricing strategies affect their engagement. Understanding this question is critical because user involvement (especially experts) is key to building a sustainable CQA service. In the following, we first identify common pricing strategies by applying uncontrolled clusters to track user price changes, then we analyze the identified clusters to understand what type of users they represent and how their engagement changes over time."}, {"heading": "Identifying Distinct Pricing Strategies", "text": "In order to characterize the dynamic price change of users, we construct a list of characteristics for group users with similar price change patterns. Key characteristics. For each user, we model their price change as a sequence of events. In view of the user i, our record contains the complete list of their responses and the price for each response. We use Pi to mark the user's price sequence Pi = [pi, 1, pi, 2,..., pi, Ni] where Ni is the total number of responses of the user i. However, a price change event occurs when pi, j \u2212 1, pi, j for each price sequence [2, Ni]. We refer to the price change sequence asCi = [ci, ci, 2, Wed] where Mi] is a number of times for price changes and ci, j is a price change event (price-up, price-down, or equal price-price)."}, {"heading": "Clustering Results", "text": "Our method produces 3 clusters for both Fenda (modularity 0.59) and Whales (modularity 0.62). To understand the pricing strategy of each cluster, we present their characteristic value distributions in Figure 12. Due to space constraints, we present 4 (of 9) most distinctive features, which show the largest discrepancy between the 3 clusters statistically selected by Chi-Squared [31]. The three clusters on Fenda are: \u2022 CL.1 (33%): Frequent price increases up and down. 687 users (76% are experts), which exhibit a high frequency of price changes. Price increases up and down are almost as common. \u2022 CL.2 (43%): Rare price changes down. 908 users (76% are experts), who rarely change their price. \u2022 CL.3 (24%): Frequent price increases up. 499 users (74% are experts), who raise the price but rarely lower their prices."}, {"heading": "Impact to User Engagement", "text": "Next, we analyze how price adjustments affect a user's engagement level over time. Price is a key parameter within a user's control, and price adjustment is a way to test the value of their responses in the marketplace. Intuitively, this price can influence a user's incoming questions, outcome, and social interactions, which are key incentive factors to keep users going. To measure changes over time, we divide a user's listening span (time between their first and last answers in our data set) into two equal parts. Then, we calculate the differences for the average price and engagement level between the later half and the first half. Similarly, we measure changes in income (Figure 13 (b)) and audience (Figure 13 (c)), which represent the differences for the average price and engagement level between the later half and the first half."}, {"heading": "DISCUSSION", "text": "Our analysis shows both the positive impact of monetary incentives and some long-term issues. Below, we discuss the key implications of future CQA concepts. First, answering questions on demand. Fenda and Whale adopt a vendor-driven model in which experts set a price for their answers. This model is suitable for targeted questions (users know who to ask), but may have a longer delay compared to crowdsourcing (where anyone can be a potential caller). Fenda and Whale achieve faster answers than most CQA services, but are still not as fast as the crowdsourcing-based Yahoo answers. Recently, Fenda added a new crowdsourcing channel for \"medical\" and \"legal\" questions. This channel is customer-driven: users post their questions with a cash reward, and all the experts give their answers to compete for reward."}, {"heading": "FUTURE WORK AND LIMITATIONS", "text": "Fenda andWhale are among the latest wave of QQA systems exploring a new design space for payment-based knowledge sharing communities, offering not only valuable lessons for other services, but also raising new questions. QQA communication mechanisms. Fenda and Whale let users record their answers in audio / video to avoid the inconvenience of typing text on the phone. Audio / video is expected to provide a more intimate experience for users, but it also makes it harder to provide longer answers. Future research could explore the right communication channels (text, audio, video) for different QA contexts. It is noteworthy that real-time streaming can be a promising option for CQA, given the huge success of Periscope and Facebook Live [36].Prevention of abuse and manipulation. Abuse activities are likely to be a common problem for payment-based QA services, as money is an incentive to work in QA."}, {"heading": "CONCLUSION", "text": "In this paper, we discuss the lessons learned from the first vendor-based payment-based CQA systems. By analyzing a large empirical dataset, we demonstrate the benefits of applying monetary incentives to CQA systems (rapid response, high-quality questions) as well as potential concerns (bounty hunters and long-term engagement). The more payment-based CQA systems emerge (Campfire.fm, Quora Knowledge Prize, Zhihu Live), the better our research can help system designers make more informed design decisions."}], "references": [{"title": "Knowledge sharing and yahoo answers: everyone knows something", "author": ["Lada A Adamic", "Jun Zhang", "Eytan Bakshy", "Mark S Ackerman"], "venue": "In Proc. of WWW", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Knowledge market design: A field experiment at Google Answers", "author": ["Yan Chen", "Tech-Hua Ho", "Yong-Mi Kim"], "venue": "Journal of Public Economic Theory 12,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Earnings And Ratings At Google Answers", "author": ["Benjamin Edelman"], "venue": "Economic Inquiry 50,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Community detection in graphs", "author": ["Santo Fortunato"], "venue": "Physics Reports", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Understanding Malicious Behavior in Crowdsourcing Platforms: The Case of Online Surveys", "author": ["Ujwal Gadiraju", "Ricardo Kawase", "Stefan Dietze", "Gianluca Demartini"], "venue": "In Proc. of CHI", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Encouraging user behaviour with achievements: an empirical study", "author": ["Scott Grant", "Buddy Betts"], "venue": "In Proc. of MSR", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Who Wants to Know?: Question-asking and Answering Practices Among Facebook Users", "author": ["Rebecca Gray", "Nicole B. Ellison", "Jessica Vitak", "Cliff Lampe"], "venue": "In Proc. of CSCW", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Modeling Problem Difficulty and Expertise in Stackoverflow", "author": ["Benjamin V. Hanrahan", "Gregorio Convertino", "Les Nelson"], "venue": "In Proc. of CSCW", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Predictors of answer quality in online Q&A sites", "author": ["F Maxwell Harper", "Daphne Raban", "Sheizaf Rafaeli", "Joseph A Konstan"], "venue": "In Proc. of CHI", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "mimir: A market-based real-time question and answer service", "author": ["Gary Hsieh", "Scott Counts"], "venue": "In Proc. of CHI", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Why pay?: exploring how financial incentives are used for question & answer", "author": ["Gary Hsieh", "Robert E Kraut", "Scott E Hudson"], "venue": "In Proc. of CHI", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Re-examining price as a predictor of answer quality in an online Q&A site", "author": ["Grace YoungJoo Jeon", "Yong-Mi Kim", "Yan Chen"], "venue": "In Proc. of CHI", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Why users keep answering questions in online question answering communities: A theoretical and empirical investigation", "author": ["Xiao-Ling Jin", "Zhongyun Zhou", "Matthew KO Lee", "Christy MK Cheung"], "venue": "IJIM 33,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Evaluating the Crowd with Confidence", "author": ["Manas Joglekar", "Hector Garcia-Molina", "Aditya G. Parameswaran"], "venue": "In Proc. of SIGKDD", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Incentive mechanisms for crowdsourcing platforms", "author": ["Aikaterini Katmada", "Anna Satsiou", "Ioannis Kompatsiaris"], "venue": "In Proc. of INSCI", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "The Social World of Content Abusers in Community Question Answering", "author": ["Imrul Kayes", "Nicolas Kourtellis", "Daniele Quercia", "Adriana Iamnitchi", "Francesco Bonchi"], "venue": "In Proc. of WWW", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Crowdsourcing user studies with Mechanical Turk", "author": ["Aniket Kittur", "H. Chi", "Bongwon Suh"], "venue": "In Proc. of CHI", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "China\u2019s online question-and-answer platform Fenda raises US$25 million from investors. http://www.scmp.com/business/companies/article/ 1982524/chinas-online-question-and-answerplatform-fenda-raises-us25", "author": ["Phoenix Kwong"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Understanding mobile Q&A usage: an exploratory study", "author": ["Uichin Lee", "Hyanghong Kang", "Eunhee Yi", "Mun Yi", "Jussi Kantola"], "venue": "In Proc. of CHI", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Analyzing crowd workers in mobile pay-for-answer q&a", "author": ["Uichin Lee", "Jihyoung Kim", "Eunhee Yi", "Juyup Sung", "Mario Gerla"], "venue": "In Proc. of CHI", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Routing questions to appropriate answerers in community question answering services", "author": ["Baichuan Li", "Irwin King"], "venue": "In Proc. of CIKM", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Design lessons from the fastest Q&A site in the west", "author": ["Lena Mamykina", "Bella Manoim", "Manas Mittal", "George Hripcsak", "Bj\u00f6rn Hartmann"], "venue": "In Proc. of CHI", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Financial incentives and the performance of crowds", "author": ["Winter Mason", "Duncan J Watts"], "venue": "ACM SigKDD Explorations Newsletter 11,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Questions in, knowledge in?: a study of naver\u2019s question answering community", "author": ["Kevin Kyung Nam", "Mark S Ackerman", "Lada A Adamic"], "venue": "In Proc. of CHI", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Analyzing the Quality of Information Solicited from Targeted Strangers on Social Media", "author": ["Jeffrey Nichols", "Michelle Zhou", "Huahai Yang", "Jeon-Hyung Kang", "Xiao Hua Sun"], "venue": "In Proc. of CSCW", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Evolution of Experts in Question Answering Communities", "author": ["Aditya Pal", "Shuo Chang", "Joseph A. Konstan"], "venue": "In Proc. of ICWSM", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Question routing to user communities", "author": ["Aditya Pal", "Fei Wang", "Michelle X. Zhou", "Jeffrey Nichols", "Barton A. Smith"], "venue": "In Proc. of CIKM", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "The incentive structure in an online information market", "author": ["Daphne Ruth Raban"], "venue": "Journal of the American Society for Information Science and Technology 59,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Great Question! Question Quality in Community Q&A", "author": ["Sujith Ravi", "Bo Pang", "Vibhor Rastogi", "Ravi Kumar"], "venue": "In Proc. of ICWSM", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Evaluating and predicting answer quality in community QA", "author": ["Chirag Shah", "Jefferey Pomerantz"], "venue": "In Proc. of SIGIR", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Handbook of Parametric and Nonparametric Statistical Procedures", "author": ["David J. Sheskin"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "2016a. A Comprehensive Survey and Classification of Approaches for Community Question Answering", "author": ["Ivan Srba", "Maria Bielikova"], "venue": "ACM TWeb 10,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Why is Stack Overflow Failing? Preserving Sustainability in Community Question Answering", "author": ["I. Srba", "M. Bielikova"], "venue": "IEEE Software 33,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Internet-scale Collection of Human-reviewed Data", "author": ["Qi Su", "Dmitry Pavlov", "Jyh-Herng Chow", "Wendell C. Baker"], "venue": "In Proc. of WWW", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Towards Predicting the Best Answers in Community-based Question-Answering Services", "author": ["Qiongjie Tian", "Peng Zhang", "Baoxin Li"], "venue": "In Proc. of ICWSM", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Anatomy of a Personalized Livestreaming System", "author": ["Bolun Wang", "Xinyi Zhang", "Gang Wang", "Haitao Zheng", "Ben Y. Zhao"], "venue": "In Proc. of IMC", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Unsupervised Clickstream Clustering For User Behavior Analysis", "author": ["Gang Wang", "Xinyi Zhang", "Shiliang Tang", "Haitao Zheng", "Ben Y. Zhao"], "venue": "In Proc. of CHI", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Comparing IPL2 and Yahoo! Answers: A Case Study of Digital Reference and Community Based Question Answering", "author": ["Dan Wu", "Daqing He"], "venue": "In Proc. of IConf", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Putting a price on knowledge. http://www.globaltimes.cn/content/997510.shtml", "author": ["Li Xuanmin"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Predicting Long-term Impact of CQA Posts: A Comprehensive Viewpoint", "author": ["Yuan Yao", "Hanghang Tong", "Feng Xu", "Jian Lu"], "venue": "In Proc. of KDD", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Quora now has 100 million monthly visitors, up from 80 million in January. VentureBeat", "author": ["Ken Yeung"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Knowledge Contribution in Problem Solving Virtual Communities: The Mediating Role of Individual Motivations", "author": ["Jie Yu", "Zhenhui Jiang", "Hock Chuan Chan"], "venue": "In Proc. of SIGMIS CPR", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "A Market in Your Social Network: The Effects of Extrinsic Rewards on Friendsourcing and Relationships", "author": ["Haiyi Zhu", "Sauvik Das", "Yiqun Cao", "Shuang Yu", "Aniket Kittur", "Robert Kraut"], "venue": "In Proc. of CHI", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}], "referenceMentions": [{"referenceID": 40, "context": "With highly engaging experts, services like Quora and StackOverflow attract hundreds of millions of visitors worldwide [41].", "startOffset": 119, "endOffset": 123}, {"referenceID": 32, "context": "As the question volume keeps growing, it becomes difficult to draw experts\u2019 attention to a particular question, let alone getting answers on-demand [33].", "startOffset": 148, "endOffset": 152}, {"referenceID": 9, "context": "To motivate experts, one possible direction is to introduce monetary incentives [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 38, "context": "Launched in May 2016, Fenda quickly gained 10 million registered users, 500K paid questions, and 2 million US dollar revenue in just two months [39].", "startOffset": 144, "endOffset": 148}, {"referenceID": 1, "context": "The involvement of verified experts differs them from earlier payment-based CQA services (most defunct now) that were built on an anonymous crowd such as Mahalo Answers and ChaCha [2, 11, 20].", "startOffset": 180, "endOffset": 191}, {"referenceID": 10, "context": "The involvement of verified experts differs them from earlier payment-based CQA services (most defunct now) that were built on an anonymous crowd such as Mahalo Answers and ChaCha [2, 11, 20].", "startOffset": 180, "endOffset": 191}, {"referenceID": 19, "context": "The involvement of verified experts differs them from earlier payment-based CQA services (most defunct now) that were built on an anonymous crowd such as Mahalo Answers and ChaCha [2, 11, 20].", "startOffset": 180, "endOffset": 191}, {"referenceID": 31, "context": "In recent years, researchers have studied CQA services from various aspects [32].", "startOffset": 76, "endOffset": 80}, {"referenceID": 25, "context": "Early studies have focused on identifying domain experts in a CQA community [26, 8] and routing user questions to the right experts [21, 27].", "startOffset": 76, "endOffset": 83}, {"referenceID": 7, "context": "Early studies have focused on identifying domain experts in a CQA community [26, 8] and routing user questions to the right experts [21, 27].", "startOffset": 76, "endOffset": 83}, {"referenceID": 20, "context": "Early studies have focused on identifying domain experts in a CQA community [26, 8] and routing user questions to the right experts [21, 27].", "startOffset": 132, "endOffset": 140}, {"referenceID": 26, "context": "Early studies have focused on identifying domain experts in a CQA community [26, 8] and routing user questions to the right experts [21, 27].", "startOffset": 132, "endOffset": 140}, {"referenceID": 28, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 79, "endOffset": 105}, {"referenceID": 39, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 79, "endOffset": 105}, {"referenceID": 29, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 79, "endOffset": 105}, {"referenceID": 34, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 79, "endOffset": 105}, {"referenceID": 0, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 79, "endOffset": 105}, {"referenceID": 8, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 79, "endOffset": 105}, {"referenceID": 33, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 79, "endOffset": 105}, {"referenceID": 15, "context": "Other works focused on assessing the quality of existing questions and answers [29, 40, 30, 35, 1, 9, 34] and detecting low quality (or even abusive) content [16].", "startOffset": 158, "endOffset": 162}, {"referenceID": 24, "context": "Finally, researchers also studied Q&A activities in online social networks [25, 7].", "startOffset": 75, "endOffset": 82}, {"referenceID": 6, "context": "Finally, researchers also studied Q&A activities in online social networks [25, 7].", "startOffset": 75, "endOffset": 82}, {"referenceID": 32, "context": "As the sizes of CQA systems rapidly grow, it becomes challenging to engage with experts for timely and high-quality answers [33].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "Prior works have summarized three main user motivations to answer questions online: \u201cintrinsic\u201d, \u201csocial\u201d and \u201cextrinsic\u201d [13].", "startOffset": 122, "endOffset": 126}, {"referenceID": 41, "context": ", enjoyment) that users gain through helping others [42, 24].", "startOffset": 52, "endOffset": 60}, {"referenceID": 23, "context": ", enjoyment) that users gain through helping others [42, 24].", "startOffset": 52, "endOffset": 60}, {"referenceID": 12, "context": "Intrinsic and social factors are critical incentives for non-payment based CQA services [13].", "startOffset": 88, "endOffset": 92}, {"referenceID": 23, "context": ", badges and credit points) [24, 6].", "startOffset": 28, "endOffset": 35}, {"referenceID": 5, "context": ", badges and credit points) [24, 6].", "startOffset": 28, "endOffset": 35}, {"referenceID": 1, "context": "Monetary incentive is an extrinsic factor implemented in earlier payment-based CQA services such as Google Answers, Mahalo, ChaCha and Jisiklog [2, 11, 20, 19].", "startOffset": 144, "endOffset": 159}, {"referenceID": 10, "context": "Monetary incentive is an extrinsic factor implemented in earlier payment-based CQA services such as Google Answers, Mahalo, ChaCha and Jisiklog [2, 11, 20, 19].", "startOffset": 144, "endOffset": 159}, {"referenceID": 19, "context": "Monetary incentive is an extrinsic factor implemented in earlier payment-based CQA services such as Google Answers, Mahalo, ChaCha and Jisiklog [2, 11, 20, 19].", "startOffset": 144, "endOffset": 159}, {"referenceID": 18, "context": "Monetary incentive is an extrinsic factor implemented in earlier payment-based CQA services such as Google Answers, Mahalo, ChaCha and Jisiklog [2, 11, 20, 19].", "startOffset": 144, "endOffset": 159}, {"referenceID": 19, "context": "Users are primarily driven by financial incentives without a strong sense of community [20, 10].", "startOffset": 87, "endOffset": 95}, {"referenceID": 9, "context": "Users are primarily driven by financial incentives without a strong sense of community [20, 10].", "startOffset": 87, "endOffset": 95}, {"referenceID": 27, "context": "This is concerning since research shows monetary incentive plays an important role in getting users started in CQA, but it is the social factors that contribute to the persistent participation [28].", "startOffset": 193, "endOffset": 197}, {"referenceID": 8, "context": "Some researchers find that monetary incentives improves the answer quality [9] and response rate in social Q&A [43].", "startOffset": 75, "endOffset": 78}, {"referenceID": 42, "context": "Some researchers find that monetary incentives improves the answer quality [9] and response rate in social Q&A [43].", "startOffset": 111, "endOffset": 115}, {"referenceID": 1, "context": "Others suggest that payments merely reduce the response delay but have no significant impact on the answer quality [2, 12, 11].", "startOffset": 115, "endOffset": 126}, {"referenceID": 11, "context": "Others suggest that payments merely reduce the response delay but have no significant impact on the answer quality [2, 12, 11].", "startOffset": 115, "endOffset": 126}, {"referenceID": 10, "context": "Others suggest that payments merely reduce the response delay but have no significant impact on the answer quality [2, 12, 11].", "startOffset": 115, "endOffset": 126}, {"referenceID": 9, "context": "based Q&A can reduce low-quality questions since users are more selective regarding what to ask [10, 11].", "startOffset": 96, "endOffset": 104}, {"referenceID": 10, "context": "based Q&A can reduce low-quality questions since users are more selective regarding what to ask [10, 11].", "startOffset": 96, "endOffset": 104}, {"referenceID": 16, "context": "Mechanical Turk) are \u201ccustomer-driven\u201d where customers post their tasks and set the task price [17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "In such marketplaces, pricing strategy can affect the work quality and response time [15, 23, 14].", "startOffset": 85, "endOffset": 97}, {"referenceID": 22, "context": "In such marketplaces, pricing strategy can affect the work quality and response time [15, 23, 14].", "startOffset": 85, "endOffset": 97}, {"referenceID": 13, "context": "In such marketplaces, pricing strategy can affect the work quality and response time [15, 23, 14].", "startOffset": 85, "endOffset": 97}, {"referenceID": 38, "context": "Launched in May 2016, Fenda quickly gained 10 million registered users and over 2 million US dollars worth of questions answers in the first two months [39, 18].", "startOffset": 152, "endOffset": 160}, {"referenceID": 17, "context": "Launched in May 2016, Fenda quickly gained 10 million registered users and over 2 million US dollars worth of questions answers in the first two months [39, 18].", "startOffset": 152, "endOffset": 160}, {"referenceID": 38, "context": "Fenda announced that they had 500,000 answers as of June 27, 2016 [39].", "startOffset": 66, "endOffset": 70}, {"referenceID": 38, "context": "On June 27 2016, Fenda officially announced total 500K answers and 10 million users [39].", "startOffset": 84, "endOffset": 88}, {"referenceID": 38, "context": "To gauge experts\u2019 contribution in the context of the entire network, we again performed an estimation: Fenda reached 2 million revenue as of June 27 in 2016 [39].", "startOffset": 157, "endOffset": 161}, {"referenceID": 30, "context": "To understand how users set a price for their answers, we calculate the Pearson correlation [31] between a user\u2019s price and different behaviormetrics.", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": "This is different from existing results on customer-driven CQA markets, where an asker can use a higher payment to collect answers more quickly [15, 23, 11].", "startOffset": 144, "endOffset": 156}, {"referenceID": 22, "context": "This is different from existing results on customer-driven CQA markets, where an asker can use a higher payment to collect answers more quickly [15, 23, 11].", "startOffset": 144, "endOffset": 156}, {"referenceID": 10, "context": "This is different from existing results on customer-driven CQA markets, where an asker can use a higher payment to collect answers more quickly [15, 23, 11].", "startOffset": 144, "endOffset": 156}, {"referenceID": 37, "context": "We compare Fenda and Whale with different CQA sites including Yahoo Answers [38], Google Answers [3] and StackOverflow [22].", "startOffset": 76, "endOffset": 80}, {"referenceID": 2, "context": "We compare Fenda and Whale with different CQA sites including Yahoo Answers [38], Google Answers [3] and StackOverflow [22].", "startOffset": 97, "endOffset": 100}, {"referenceID": 21, "context": "We compare Fenda and Whale with different CQA sites including Yahoo Answers [38], Google Answers [3] and StackOverflow [22].", "startOffset": 119, "endOffset": 123}, {"referenceID": 30, "context": "A two-sample t-Test [31] shows the significance of the differences between the two groups of askers.", "startOffset": 20, "endOffset": 24}, {"referenceID": 36, "context": "This produces a fully connected similarity graph [37] where each node is a user and edges are weighted by distance.", "startOffset": 49, "endOffset": 53}, {"referenceID": 3, "context": "Then, we apply hierarchical clustering algorithm [4] to detect groups of users with similar price change patterns.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "To determine the number of clusters, we use modularity, a well-known metric to measure clustering quality [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 30, "context": "Due to space limitation, we plot 4 (out of 9) most distinguishing features that have the largest variance among the 3 clusters selected by Chi-Squared statistic [31].", "startOffset": 161, "endOffset": 165}, {"referenceID": 30, "context": "We validate the statistical significance of the results by calculating the Pearson correlation [31] between the price change (x) and behavior metrics (y) for all three clusters in Figure 13.", "startOffset": 95, "endOffset": 99}, {"referenceID": 4, "context": "Note that this is different from the traditional cheating behavior in crowdsourcing platforms like MTurk, where cheaters often produce low-quality work [5].", "startOffset": 152, "endOffset": 155}, {"referenceID": 35, "context": "Noticeably, real-time streaming can be a promising option for CQA, given the huge success of Periscope and Facebook Live [36].", "startOffset": 121, "endOffset": 125}], "year": 2017, "abstractText": "Community-based question answering (CQA) services are facing key challenges to motivate domain experts to provide timely answers. Recently, CQA services are exploring new incentive models to engage experts and celebrities by allowing them to set a price on their answers. In this paper, we perform a data-driven analysis on two emerging payment-based CQA systems: Fenda (China) and Whale (US). By analyzing a large dataset of 220K questions (worth 1 million USD collectively), we examine how monetary incentives affect different players in the system. We find that, while monetary incentive enables quick answers from experts, it also drives certain users to aggressively game the system for profits. In addition, in this supplier-driven marketplace, users need to proactively adjust their price to make profits. Famous people are unwilling to lower their price, which in turn hurts their income and engagement over time. Finally, we discuss the key implications to future CQA design. INTRODUCTION The success of community based question answering (CQA) services depends on high-quality content from users, particularly from domain experts. With highly engaging experts, services like Quora and StackOverflow attract hundreds of millions of visitors worldwide [41]. However, for most CQA systems, domain experts are answering questions voluntarily for free. As the question volume keeps growing, it becomes difficult to draw experts\u2019 attention to a particular question, let alone getting answers on-demand [33]. To motivate experts, one possible direction is to introduce monetary incentives [10]. Recently, Quora started a beta test on \u201cknowledge prize\u201d, which allows users to put cash rewards on their questions.While Quora is slowly accumulating interested users (less than 10 paid answers per month), another payment-based service called Fenda is rising quickly in China. Fenda is a social network app that connects users to well-known domain experts and celebrities to ask questions with payments. Launched in May 2016, Fenda quickly gained 10 million registered users, 500K paid questions, and 2 million US dollar revenue in just two months [39]. Fenda leads a new wave of payment-based CQA services that socially engage users with real-world domain experts. Similar services are emerging in China (Zhihu, Weibo QA) and US (Whale, Campfire.fm). The involvement of verified experts differs them from earlier payment-based CQA services (most defunct now) that were built on an anonymous crowd such as Mahalo Answers and ChaCha [2, 11, 20]. 1http://fd.zaih.com/fenda So, is monetary incentive the solution to strong expert engagement in CQA systems? How does monetary incentive affect the behavior of different players in the system and their overtime engagement? These questions are critical for paymentbased CQA design, and platforms like Fenda provide a unique opportunity to study them. First, Fenda is the first supplierdriven CQA marketplace, where answerers (experts) set their own price. Users ask questions to a specific person instead of an anonymous crowd using payments. In addition, Fenda\u2019s incentive model not only rewards answerers, but also those who ask good questions. After a question is answered, other users need to pay a small amount ($0.14) to listen to the answer. This money is split evenly between the asker and the answerer (Figure 1). A good question may attract enough listeners to compensate the initial question fee. In this paper, we describe our experience and findings in an effort to understand the impact of monetary incentives on CQA systems, through a detailed measurement of Fenda (China) and a similar system Whale (US). We collected a dataset of 88,540 users and 212,082 answers from Fenda (two months in 2016), and 1,419 users and 9,199 answers from Whale (6 months during 2016\u20132017), involving more than 1 million USD transactions. Given the drastic differences between payment-based CQA systems and mainstream systems such as Quora and StackOverflow, our study has significant implications for the future direction of CQA design. Our study has a number of key findings. \u2022 First, we seek to understand the effectiveness of monetary incentives to engage domain experts. Our result shows this attempt is successful. Both Fenda and Whale attract a small group of high-profile experts and celebrities who make significant contributions to the CQA community. For example, Fenda experts count for 0.5% of the user population, but have contributed a quarter of all answers and driven nearly half of the financial revenue. \u2022 Second, we analyze how the incentive model affects user behavior, and find a mixed effect. On the positive side, monetary incentive enables quick answers (average delay 10\u2013 23 hours) and motivates users to ask good questions (40% of the Fenda questions successfully drew enough interested audience to cover the askers\u2019 cost). However, we did find a small number of manipulative users including \u201cbounty hunters\u201d who aggressively asked questions to make money from listeners, and \u201ccollusive users\u201d who work together to manipulate their perceived popularity. 2https://askwhale.com/ \u2022 Third, we study the dynamics between money and user engagement over time. In a supplier-driven CQA marketplace, users need to set the price of their answers. We find different pricing strategies of users have distinct impacts on their own engagement level. Users who proactively adjust their price are more likely to increase income and engagement level. Certain highly famous people, however, are unwilling to lower their price, which in turn hurts their income and social engagement. To the best of our knowledge, this is the first empirical study on supplier-driven CQA marketplaces. Our study provides practical guidelines for other arising payment-based CQA services (Quora knowledge prize, Zhihu, Campfire.fm) and reveals key implications for future CQA system design. We believe this is a first step towards understanding the economy of community-based knowledge sharing.", "creator": "gnuplot 5.0 patchlevel 4"}}}