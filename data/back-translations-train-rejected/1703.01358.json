{"id": "1703.01358", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Generalised Discount Functions applied to a Monte-Carlo AImu Implementation", "abstract": "In recent years, work has been done to develop the theory of General Reinforcement Learning (GRL). However, there are few examples demonstrating these results in a concrete way. In particular, there are no examples demonstrating the known results regarding gener- alised discounting. We have added to the GRL simulation platform AIXIjs the functionality to assign an agent arbitrary discount functions, and an environment which can be used to determine the effect of discounting on an agent's policy. Using this, we investigate how geometric, hyperbolic and power discounting affect an informed agent in a simple MDP. We experimentally reproduce a number of theoretical results, and discuss some related subtleties. It was found that the agent's behaviour followed what is expected theoretically, assuming appropriate parameters were chosen for the Monte-Carlo Tree Search (MCTS) planning algorithm.", "histories": [["v1", "Fri, 3 Mar 2017 23:25:38 GMT  (17kb)", "http://arxiv.org/abs/1703.01358v1", "12 pages, 4 figures"]], "COMMENTS": "12 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["sean lamont", "john aslanides", "jan leike", "marcus hutter"], "accepted": false, "id": "1703.01358"}, "pdf": {"name": "1703.01358.pdf", "metadata": {"source": "CRF", "title": "Generalised Discount Functions applied to a Monte-Carlo AI\u03bc Implementation", "authors": ["Sean Lamont", "John Aslanides", "Jan Leike", "Marcus Hutter"], "emails": ["sean.a.lamont@outlook.com", "john.stewart.aslanides@gmail.com", "leike@google.com", "marcus.hutter@anu.edu.au"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.01 358v 1 [cs.A I] Keywords - Reinforcement Learning, Discount Function, Time Consistency, Monte Carlo"}, {"heading": "1 Introduction", "text": "Most RL methods focus on a specific area, such as the Google Deepmind Alpha-Go program, which focuses on the Go board game [12]. General Reinforcement Learning (GRL) deals with the design of agents that are effective in a wide range of environments. RL agents use a discount function in the selection of their future actions, which controls how heavily they burden future rewards. Several theoretical results have been proven for arbitrary rebate functions associated with GRL agents [8]. We present some contributions to the AIXIjs1 platform [1], which allows simulation of GRL agents for Gridworld problems. Web-based makes it possible to use this platform as an alternative to other environments."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Reinforcement Learning Setup", "text": "The characteristic approach of RL is to link rewards to the desired goal and allow the agent to learn the best strategy to reward himself through trial and error. [14] The agent interacts with an environment by producing an action a, and the environment reacts with an observation and reward pair (o, r) = e, which we call perception. The story up to the interaction cycle k is given by the string of all actions and perceptions, a1e1..... ak \u2212 1. To simplify notation, this is written as \u00e6 < k. Mathematically, the policy of an agent is a stochastic function that assigns a story to an action: (A \u00d7 E) A, while an environment becomes a stochastic map of a story and an action to a perception."}, {"heading": "2.2 AI\u00b5", "text": "The GRL active ingredient AI\u00b5 [4] is used to find the optimal reward in a known environment. There are no other assumptions about the environment, so this active ingredient extends to only partially observable cases. AI\u00b5 is simply defined as the active ingredient that maximizes the value function given by (1). Specifically for each environment \u00b5, \u03c0AI\u00b5 and Argmax \u03c0 V \u03c0\u00b5 (2) Since there is normally no way to know the true environment, the main purpose of AI\u00b5 is to create a theoretical upper limit for the performance of an active ingredient in a given environment. Since we wish to isolate the effect of discounting, AI\u00b5 is the active ingredient used in our experiments to eliminate the uncertainty in the active ingredient model."}, {"heading": "2.3 Generalised Discounting", "text": "There are several motivations for using a discount function to determine the use value, as opposed to an unchanged sum of rewards. In practice, a discount function allows the agent's designer to decide how he wants to evaluate the agent's behavior based on the sum of the discounted future rewards. A discount function also serves to prevent the benefit from deviating from the divergence to infinity, as is the case when using undiscounted reward sums. Samuelson [11] initially introduced the discounted use value model at a time given by the sum of discounted future rewards: Vk = prediction model, which is most commonly used in both RL and other disciplines but has several problems. These include that the discount function cannot be changed over time, and that the value of an action is independent of the story. Hutter and Lattimore [8] define multiple choices to allow the agent to address the past issues with this model."}, {"heading": "2.4 Monte-Carlo Tree Search with \u03c1UCT", "text": "Monte-Carlo Tree Search (MCTS) is a planning algorithm designed to approximate the search tree generated by (1) with maximum probability, which normally cannot be fully enumerated. UCT [7] is an MCTS algorithm that is effective for Markovian settings. Veness et al. [16] extend this algorithm to general environments using the \u03c1UCT algorithm. The algorithm generates a tree consisting of two types of nodes, \"decision nodes\" and \"random nodes.\" A decision node reflects the possible actions of the actors, while random nodes represent the possible environmental reactions. A summary of the algorithm reads as follows: Plan forward using the standard Monte-Carlo simulation. Then select an action in the tree using the UCB action policy; define a search horizon m, maximum and minimum reward, and an estimate \u03b1 \u2032 of the value of a visit node (the number of ha)."}, {"heading": "2.5 AIXIjs", "text": "We are implementing our experiments with AIXIjs, a JavaScript platform designed to demonstrate GRL results. AIXIjs is structured as follows: There are currently several GRL agents that have been implemented to work in different (toy) Gridworld and MDP environments. Using these, there is a collection of demos, each designed to present some theoretical results in GRL and presented on the website. Once a demo version has been selected, the user can choose to change some default parameters and then run the demo version, starting a batch simulation with the specified agent and the environment for the selected number of time cycles (a batch simulation runs the entire simulation as a job, without interference), and the data collected during the simulation is then used to visualize the interaction. The API allows everyone to design their own demos and environments based on updated environments and agents."}, {"heading": "3 Technical Details", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 AI\u00b5 Implementation", "text": "The active ingredient used for experiments is an MCTS-approximated version of AI\u00b5. By using AI\u00b5, we eliminate any potential uncertainty in the model of the active ingredient, allowing a more accurate analysis of the impact of discounting, which knows the true environment, so that at a fixed discount, its policy (s) will remain the same for each particular state, provided it is a Markovian environment. Although we will not use a very large tree depth, the listing of expectation by solving Equation (1) is generally not feasible. Instead, we use MCTS to approximate the search tree, in particular the UKT algorithm introduced in the background. Although UCT would suffice in our deterministic setting, UKT is the default search algorithm integrated into AIXIjs and used as such without modification."}, {"heading": "3.2 Agent Plan and Time Inconsistency Heuristic", "text": "We determine the agent's plan at the appropriate time step k by traversing the tree created by \u03c1UCT, first selecting the highest quality decision node and then selecting the corresponding random node with the highest number of visits. In the case of the environment used here, each decision node has only one random child, as it is deterministic, and the process is then repeated to the maximum horizon that the search reaches, and the sequence of actions taken is recorded as the agent's plan. The plan is recorded as a numeric string representing the sequence of actions the agent plans to take. For example, a recorded plan of 000111 indicates that the agent plans to act first three times in a row and then \"1\" three times. If the action in the cycle k does not correspond to the action predicted by the plan at the time k \u2212 1, then we consider that time to be contradictory. Formally, this time, if the following equation is fulfilled, then the action is contradictory to that time."}, {"heading": "3.3 Environment Setup", "text": "The environment we use is a deterministic, fully observable finite MDP, represented by Figure 1. This environment is structured in such a way that it provides a simple means of distinguishing short-sighted and far-sighted strategies; the idea behind the environment is to allow the agent to receive an immediate reward at any time, which he will receive if he is sufficiently short-sighted; the other option gives the agent a very high reward only after a second action for N-steps; if the agent is far-sighted enough, he ignores the low immediate reward and plans in advance to achieve the very large reward in N-steps. Formally, the agent has two actions by the state of Si: the first is to go to S0 and receive a small immediate reward rI; the other leads the agent to Si + 1 (where i-Z / (N + 1) Z) Z and gives a very low reward r0 < 1 and a large action rI = 1 while the second action represents Ni r1."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Overview", "text": "In this section, we will present the discount function experiments performed with the AIXIjs experiment API mentioned in the background. In particular, we will examine the effects of geometric, hyperbolic and performance-related discounting on the \u03c1UCT AI\u00b5model, which was the case in Figure 1, which was parameterized by N = 6, rI = 4, r0 = 0, rL = 1000. We will use the average reward as a measure of the agent's performance. We will avoid using the overall reward as it increases monotonously over time in this environment, which would affect the scale of diagrams that could obscure an agent's behavior. We will now present the MCTS parameters, according to which we will detail two specific guidelines before the experiments that cover the rest of the section. We will introduce these guidelines to avoid unnecessary overlaps in the analysis of geometric and hyperbolic disconfiguration, as they exhibit very similar behaviors."}, {"heading": "4.2 MCTS Parameters", "text": "A high exploration constant would overwhelm the UCB expression in (5) and lead to unpredictable measures as there are no clear better measures. Given the large number of samples, it was also necessary to narrow the horizon to shorten the depth of the tree. 7 is the minimum required to look far enough into the future to notice the delayed reward."}, {"heading": "4.3 Far-Sighted Policy", "text": "The total reward for farsighted policy in 200 time cycles is 33 000, with a delayed reward of 1000 and a reward interval of 6 time steps. Figure 3 shows the average reward of this policy in our environment.Average reward versus time for farsighted policyThe periodic nature of the delayed reward is reflected in the zig-zag shape of the average reward chart. As this policy consistently takes a1, the time between peaks is constant."}, {"heading": "4.4 Short-Sighted (Myopic) Agent", "text": "The second policy takes action a1 (solid arrow in Figure 1) for each time step. The total reward for this policy is 792, with an immediate reward of 4. Figure 4 shows a graph of the average reward for this policy in our environment. The graph reflects the initial reward of 0 at the beginning of the agent, and then the constant reward of 4 in each subsequent time cycle."}, {"heading": "4.5 Geometric Discounting", "text": "We found that in all trials, the number of time-contradictory actions given by our heurists was 0. We found that at \u03b3 \u2264 0.4, the active ingredient followed exactly the short-sighted policy of the previous subsection, averaging a reward against time for short-sighted policy, which received a total reward of 792. At \u03b3 \u2265 0.6, the active ingredient behaved as described in the far-sighted subsection, reaching the optimal reward of 33,000. At \u03b3 = 0.5, the active ingredient behaved somewhat erratically and occasionally changed its behavior between the two strategies. Since this value lies between the threshold that causes severe far-sightedness / short-sightedness, there would be a small difference in the weighted rewards between the two strategies. It is therefore likely that the erratic behavior is caused by the MCTS struggling to find the best decision, as there is a degree of inaccuracy in tree search."}, {"heading": "4.6 Hyperbolic Discounting", "text": "We found that only \u0445 = 1.8 yielded inconsistent actions over time, with the total number of these actions recorded at 200. We found that the active substance followed exactly the behavior of the myopic political subsection and received a total reward of 792. For \u0432 > 1.8, the active substance behaved as described in the far-sighted political subsection and achieved the optimal reward of 33,000. The plan for \u0432 = 1.8 was 0111111000 at each time step. For \u0432 > 1.8, the plan remained as 11111111, and \u0432 > 1.8 gave a constant plan of 000000000000.In the interest of reproducibility, the experiments for hyperbolic discounting at Commit 3911d73 were performed on the intended Github linkage. Results can also be replicated with recentAverage Revs Time for Power Discounting (Beta: 1.01) and future versions, with the CTS parameters possibly requiring adjustment."}, {"heading": "4.7 Power Discounting", "text": "In this case, we only used a single \u03b2-value, \u03b2 = 1.01. We found that any change in \u03b2 would lead to similar behavior, with only the length and time between these stages changing (so we only need to present the results of a \u03b2-value), and no time inconsistencies were found for this function. Overall, the pathogen reward received was 15412. Behavior in this case follows three stages: In about 100 time steps, the pathogen behaves completely short-sighted, which is reflected in the small continuous increase in the first half of the diagram. The discount function then reaches a stage where the distant rewards are weighted high enough so that the pathogen decides to act farsighted. In several time steps, the pathogen then collects the delayed reward for a few cycles at a moment, gradually decreasing the number of cycles it remains there until it strictly adheres to the far-sighted policy."}, {"heading": "5 Discussion", "text": "In terms of agent timing, the results were consistent with theoretical predictions. Geometric discounting was, in all cases of \u03b3, consistent over time as expected. Somewhat surprisingly, hyperbolic discounting was measured for all but 1.8, when it was continuously inconsistent; the results of power discounting also lacked expected, time-inconsistent actions; the hyperbolic agent schedule of 01111000 for \u0432 = 1.8 reflects interesting behavior; we can see that the agent plans to stay with immediate reward for the next time step and then set off to collect a delayed reward; but since this plan is the same for all time steps, the agent continuously focuses on immediate reward planning in order to take the better long-term actions later. In fact, the agent is eternally hesitant; the fact that this behavior can be induced with this function, may subvert the hyperbolic claim that hyperbolic behavior is possible."}, {"heading": "6 Summary", "text": "We have customized the AIXIjs platform to include arbitrary discount functionality, thereby isolating temporal inconsistency behavior and illustrating the impact of the discount function on an agent's farsightedness; we have demonstrated that it is possible to use power discounts in a concrete environment to observe the effects of a growing effective horizon that has influenced the time an agent decides to collect remote rewards; and we have demonstrated that hyperbolic discounting can lead to delayed agent behavior; our current framework now allows for a larger class of experiments and demos with general discounting, which will be useful for future research in this field."}], "references": [{"title": "AIXIjs: A software demo for general reinforcement learning", "author": ["J Aslanides"], "venue": "Australian National University", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "General Reinforcement Learning Algorithms: Survey and Experiments, 2016", "author": ["J Aslanides", "M. Hutter", "J. Leike"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Dynamic programming", "author": ["R Bellman"], "venue": "Princeton, NJ: Princeton University Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1957}, {"title": "A theory of universal artificial intelligence based on algorithmic complexity", "author": ["M. Hutter"], "venue": "ISDIA-14-00, ISDIA, arXiv:cs.AI/0004001", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Universal artificial intelligence: Sequential decisions based on algorithmic probability", "author": ["M. Hutter"], "venue": "Springer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "https://cs.stanford.edu/people/karpathy/reinforcejs /index.html", "author": ["A. Karpathy"], "venue": "Reinforcejs,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Bandit based Monte-Carlo planning", "author": ["L. Kocsis", "C. Szepesvari"], "venue": "Euro. Conf. Mach. Learn. Berlin, Germany : Springer, pp. 282-293.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "General time consistent discounting", "author": ["T. Lattimore", "M. Hutter"], "venue": "Theoretical Computer Science, 519:140-154", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "What is AIXI? - An Introduction to General Reinforcement Learning, 2015. https://jan.leike.name/AIXI.html", "author": ["J. Leike"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "On-line Q-learning using connectionist systems", "author": ["G.A. Rummery andM. Niranjan"], "venue": "Technical Report CUED/F-INFENG/TR 166,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "A note on measurement of utility", "author": ["P. Samuelson"], "venue": "The Review of Economic Studies, 4(2) : 155-161", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1937}, {"title": "G", "author": ["D. Sliver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre"], "venue": "van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, N. Kalchbrenner, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of go with deep neural networks and tree search. Nature 529, 484-489", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R. Sutton"], "venue": "Machine Learning, 3, 9-44.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1988}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Some empirical evidence on dynamic inconsistency", "author": ["R. Thaler"], "venue": "Economics Letters, 8(3) : 201 - 207", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1981}, {"title": "A Monte-Carlo AIXI Approximation", "author": ["J. Veness", "M. Hutter", "W. Uther", "D. Silver", "K.S. Ng"], "venue": "Journal of Artificial Intelligence Research 40: 95-142", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Q-learning", "author": ["C. Watkins", "P. Dayan"], "venue": "Machine Learning, 8, 279-292", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 11, "context": "Most RL methods focus on one specialised area, for example the Alpha-Go program from Google Deepmind which is targeted towards the board game Go [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 7, "context": "Several theoretical results have been proven for arbitrary discount functions relating to GRL agents [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "We present some contributions to the platform AIXIjs [1][2], which enables the simulation of GRL agents for gridworld problems.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "We present some contributions to the platform AIXIjs [1][2], which enables the simulation of GRL agents for gridworld problems.", "startOffset": 56, "endOffset": 59}, {"referenceID": 13, "context": "The characteristic approach of RL is to associate rewards with the desired goal and allow the agent to learn the best strategy for gaining rewards itself through trial and error [14].", "startOffset": 178, "endOffset": 182}, {"referenceID": 2, "context": "In the context of adaptive control, Bellman [3] first introduced equations for expressing optimal policies in both deterministic and stochastic environments, including infinite state spaces.", "startOffset": 44, "endOffset": 47}, {"referenceID": 8, "context": "Where r is the reward and \u03b3 is a discount function [9].", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "If we are in a MDP, then we can replace the history by the current state, and rewrite this as a Bellman Equation [3].", "startOffset": 113, "endOffset": 116}, {"referenceID": 3, "context": "2 AI\u03bc The GRL agent AI\u03bc [4] is purposed to find the optimal reward in a known environment.", "startOffset": 24, "endOffset": 27}, {"referenceID": 10, "context": "Samuelson [11] first introduced the model of discounted utility, with the utility at time k given by the sum of discounted future rewards:", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "Hutter and Lattimore [8] address several issues with this model first by using the GRL framwork to allow decisions which consider the agent\u2019s history.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "Using this model, Hutter and Lattimore [8] provide a general classification of time inconsistent discounting.", "startOffset": 39, "endOffset": 42}, {"referenceID": 14, "context": "Hyperbolic discounting has been thought to accurately model human behaviour, with some research suggesting humans discount this way when deciding actions [15].", "startOffset": 154, "endOffset": 158}, {"referenceID": 7, "context": "Hutter and Lattimore [8] point out that this function is time consistent, which combined with the growing effective horizon makes it an effective means of agent discounting.", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "UCT [7] is a MCTS algorithm which is effective for Markovian settings.", "startOffset": 4, "endOffset": 7}, {"referenceID": 15, "context": "[16] extend this to general environments with the \u03c1UCT algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] remark that high values of C lead to \u2019bushy\u2019 and short trees, compared to low values yielding longer and more discerning trees.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "In particular, the Monte-Carlo AIXI approximation [16] successfully implemented a AIXI model using the aforementioned \u03c1UCT algorithm.", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "Related to the AIXIjs platform is the REINFORCEjs web demo by Karpathy [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 16, "context": "This demo implements Q-Learning [17] and SARSA [10] RL methods in a grid world scenario, as well as deep Q-Learning for two continuous state settings.", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "This demo implements Q-Learning [17] and SARSA [10] RL methods in a grid world scenario, as well as deep Q-Learning for two continuous state settings.", "startOffset": 47, "endOffset": 51}], "year": 2017, "abstractText": "In recent years, work has been done to develop the theory of General Reinforcement Learning (GRL). However, there are few examples demonstrating the known results regarding generalised discounting. We have added to the GRL simulation platform AIXIjs the functionality to assign an agent arbitrary discount functions, and an environment which can be used to determine the effect of discounting on an agent\u2019s policy. Using this, we investigate how geometric, hyperbolic and power discounting affect an informed agent in a simple MDP. We experimentally reproduce a number of theoretical results, and discuss some related subtleties. It was found that the agent\u2019s behaviour followed what is expected theoretically, assuming appropriate parameters were chosen for the Monte-Carlo Tree Search (MCTS) planning algorithm. Keywords\u2014 Reinforcement Learning, Discount Function, Time Consistency, Monte Carlo", "creator": "LaTeX with hyperref package"}}}