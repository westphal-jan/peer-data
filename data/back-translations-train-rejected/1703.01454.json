{"id": "1703.01454", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2017", "title": "Matrix-centric Neural Networks", "abstract": "We present a new distributed representation in deep neural nets wherein the information is represented in native form as a matrix. This differs from current neural architectures that rely on vector representations. We consider matrices as central to the architecture and they compose the input, hidden and output layers. The model representation is more compact and elegant - the number of parameters grows only with the largest dimension of the incoming layer rather than the number of hidden units. We derive feed-forward nets that map an input matrix into an output matrix, and recurrent nets which map a sequence of input matrices into a sequence of output matrices. Experiments on handwritten digits recognition, face reconstruction, sequence to sequence learning and EEG classification demonstrate the efficacy and compactness of the matrix-centric architectures.", "histories": [["v1", "Sat, 4 Mar 2017 13:35:49 GMT  (1149kb,D)", "http://arxiv.org/abs/1703.01454v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kien do", "truyen tran", "svetha venkatesh"], "accepted": false, "id": "1703.01454"}, "pdf": {"name": "1703.01454.pdf", "metadata": {"source": "CRF", "title": "Matrix-Centric Neural Nets", "authors": ["Kien Do", "Truyen Tran", "Svetha Venkatesh"], "emails": ["svetha.venkatesh}@deakin.edu.au"], "sections": [{"heading": "1 Introduction", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "2 Methods", "text": "We present our main contribution, the matrixcentric neural architectures."}, {"heading": "2.1 Matrix-Matrix Mapping", "text": "In conventional vector representation, the mapping of an input vector x to an output vector y has the shape y = \u03c3 (Wx + b), where b is a presetting vector, W is an imaging matrix, and \u03c3 is an elementary nonlinear transformation. Similarly, in our new matrix representation, the block is a mapping of an input matrix X of size c1 \u00b7 r1 to an output matrix Y of size c2 \u00b7 r2, as follows: Y = \u03c3 (UXV + B) (1), where U Rc1 \u00b7 c2 is the row mapping matrix, V Rr1 \u00b7 r2 is the column mapping matrix, and B Rc2 \u00b7 r2 is the presetting matrix. In this mapping, the construction of a deep forward network is simple by stacking the matrix layers on top of each other."}, {"heading": "2.2 Matrix-Matrix Mapping as Low-Rank Factorization", "text": "Suppose we vectorize X and Y into the vector x of length c1r1 and y of length c2r2 and use the standard vector mapping between them: y = \u03c3 (W x + b), where W is the parameter matrix of form c1r1 \u00b7 c2r2. Thus, each element yk can be calculated with k {1,..., c2r2} as follows: yk = \u03c3 (W:, kx + bk) = \u03c3 (all elements (W:, k x) + bk) (2). On the other hand, we can slightly modify the mapping of the whole matrix X to an element Yi, j of the matrix Y: Yi, j = \u03c3 (U:, iXV:, j + Bij) to describe the mapping of the whole matrix X."}, {"heading": "2.3 Matrix Recurrent Networks", "text": "We now introduce the matrix extension of recursive networks. Here, the input, output, and hidden state are all matrices. To simplify the development, we insert the notation mat (X, H; \u03b8), defined as: mat (X, H; \u03b8): = UxXVx + U hHVh + B (5), with \u03b8 = {Ux, Vx, Vx, Uh, Vh, B}. The next state in Vanillematrix-RNs is simply: Ht = \u03c3 (mat (Xt, Ht \u2212 1; \u03b8i)))). In the case of the matrix Long-Term Memory (matrix), \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 matrixLSTM, the formula of a block (without a peep-hole connection) can be specified step t as follows: It is the matrix (mat (Xt, Ht \u2212 1; \u03b8i))) Ft = \u03c3 (Xt \u2212 1; tanf)))))) Octgate (that Hector (that) matrix (that) matrix is the matrix extension of recursive networks."}, {"heading": "2.4 Tensor-Matrix Mapping", "text": "For spatial input, revolutionary layers are indispensable for learning translation-invariant characteristics. Output of a revolutionary layer is usually a set of 2D attribute cards - or in other words, a 3D tensor. The neural matrix networks defined above can easily connect to CNNs using a tensor matrix mapping. As these attribute cards are structured, their flattening in vectors can lose information just like in conventional CNNs. We suggest treating these 2D cards as matrices. Think of X as an output of the revolutionary layer. X is a 3D tensor of form (m, c1, r1), where m is the number of attribute cards, with each form having c1 \u00b7 r1. We want to map this tensor to a 2D matrix Y of form (c2, r2)."}, {"heading": "3 Experiments", "text": "In this section, the matrixcentric architectures described in Section 2 are experimentally validated. For all the experiments presented below, our networks use the activation function ReLU and are trained with Adam [Kingma and Ba, 2014]."}, {"heading": "3.1 Feedforward Nets on MNIST", "text": "The MNIST dataset consists of 70K handwritten digits of size 28 \u00d7 28, with 50K images for training, 10K for testing, and 10K for validation. For matrix networks, images are of course matrices, but for traditional vector networks, images are vectorized into vectors of length 784. To test the ability to accommodate very deep networks without skip connections [Srivastava et al., 2015b; Pham et al., 2016] we create vector and matrix feed-forward networks with increasing depth. The top layers are softmax as usual for both vector and matrix networks. We compare matrix networks with the hidden form of 20 \u00d7 20 and 50 \u00d7 50 against vector networks that have 50 and 200 hidden units.We observe that without batch standard (BN) [Ioffe and Szegedy, 2015, vector learning goes beyond depth]."}, {"heading": "3.2 Autoencoders for Corrupted Face Reconstruction", "text": "To evaluate the ability of learning structural information in images of matrix-centric networks, we conduct experiments on the Frey Face Dataset1, consisting of 1,965 face images from 1http: / / www.cs.nyu.edu / \u02dc roweis / data.htmlsize 28 \u00d7 20, taken from sequential frames of a video. We randomly select 70% data for training, 20% for testing and 10% for validation. Test images are corrupted with 5 \u00d7 5 black square patches at random positions. Auto-encoders (AEs) are used for this reconstruction task. We build deep AEs consisting of 20 and 40 layers. For each depth, we select vector networks with 50, 100, 200 and 500 hidden units and matrix networks with hidden shapes of 20 x 20, 50 x 50, 100 x 100 and 150 x 150."}, {"heading": "3.3 Sequence to Sequence Learning with Moving MNIST", "text": "In this experiment we compare the performance of matrix and vector recursively in a sequence to sequence (seq2seq), which has a length of 20 digits moving in 64 sequences."}, {"heading": "3.4 Sequence Classification with EEG", "text": "We use the alcoholic EEG data 6 of 122 subjects, which are divided into two groups: alcoholics and control groups. Each subject has completed about 100 studies and the data contains about 12K studies in total. For each study, the subject was presented with three different types of stimuli in 1 second. However, EEG signals have 64 channels, which are sampled at the rate of 256 Hz. Thus, each study consists of 64 x 256 samples in total. We convert the signals into spectrograms, which use half the frequency of the bins. We also exclude the first bin, which corresponds to a frequency of 64 and 56 overlapping samples. The signals were detralized by removing averages along the time axis. The signals are taken only half the frequency, which corresponds to the frequency. These results in a tensor of the form 64 x 32 x 25, where the dimensions are channel, frequency and time."}, {"heading": "4 Related Work", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5 Discussion", "text": "We have introduced a new concept of distributed representation in neural networks, in which information is distributed about neurons arranged in a matrix, which differs from the existing representation by vectors. Matrix-centric representation is consistent with newer memory-enlarged RNNNs, where external memory is an array of storage locations - essentially a matrix arrangement. However, in our treatment matrices are first-class citizens: neurons in a layer are always arranged in a matrix. We derive matrix-centric forward and recursive networks and propose a method to convert revolutionary output into a matrix. We show theoretically that matrix factorization of weights can replace a traditional vector network with an equivalent matrix network when modeling matrix data, resulting in a much more compact matrix network than structure-centric matrix networks, benefiting from structural and structural regulation."}], "references": [{"title": "In Advances in neural information processing systems", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber. Offline handwriting recognition with multidimensional recurrent neural networks"], "venue": "pages 545\u2013552,", "citeRegEx": "Graves and Schmidhuber. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["Graves et al", "2016] Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka GrabskaBarwi\u0144ska", "Sergio G\u00f3mez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361(10):1995,", "citeRegEx": "LeCun et al.. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Nature", "author": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton. Deep learning"], "venue": "521(7553):436\u2013444,", "citeRegEx": "LeCun et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In AAAI", "author": ["Tu Dinh Nguyen", "Truyen Tran", "Dinh Q Phung", "Svetha Venkatesh. Tensor-variate restricted boltzmann machines"], "venue": "pages 2887\u20132893,", "citeRegEx": "Nguyen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Alexander Novikov", "Dmitrii Podoprikhin", "Anton Osokin", "Dmitry P Vetrov. Tensorizing neural networks"], "venue": "pages 442\u2013450,", "citeRegEx": "Novikov et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "SIAM Journal on Scientific Computing", "author": ["Ivan V Oseledets. Tensor-train decomposition"], "venue": "33(5):2295\u2013 2317,", "citeRegEx": "Oseledets. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Faster training of very deep networks via p-norm gates", "author": ["Trang Pham", "Truyen Tran", "Dinh Phung", "Svetha Venkatesh"], "venue": "ICPR,", "citeRegEx": "Pham et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Neural Networks (IJCNN)", "author": ["Guanglei Qi", "Yanfeng Sun", "Junbin Gao", "Yongli Hu", "Jinghua Li. Matrix variate restricted boltzmann machine"], "venue": "2016 International Joint Conference on, pages 389\u2013395. IEEE,", "citeRegEx": "Qi et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1502.04681,", "citeRegEx": "Srivastava et al.. 2015a", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in neural information processing systems", "author": ["Rupesh K Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber. Training very deep networks"], "venue": "pages 2377\u20132385,", "citeRegEx": "Srivastava et al.. 2015b", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le. Sequence to sequence learning with neural networks"], "venue": "pages 3104\u20133112,", "citeRegEx": "Sutskever et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In ICML (3)", "author": ["Yichuan Tang", "Ruslan Salakhutdinov", "Geoffrey E Hinton. Tensor analyzers"], "venue": "pages 163\u2013171,", "citeRegEx": "Tang et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes"], "venue": "arXiv preprint arXiv:1410.3916,", "citeRegEx": "Weston et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "IEEE transactions on neural networks", "author": ["Xianchao Xie", "Shuicheng Yan", "James T Kwok", "Thomas S Huang. Matrix-variate factor analysis", "its applications"], "venue": "19(10):1821\u20131826,", "citeRegEx": "Xie et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "author": ["Xingjian et al", "2015] SHI Xingjian", "Zhourong Chen", "Hao Wang", "Dit-Yan Yeung", "Wai-Kin Wong", "Wang-chun Woo"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Two-dimensional PCA: a new approach to appearance-based face representation and recognition", "author": ["Jian Yang", "David Zhang", "Alejandro F Frangi", "Jing-yu Yang"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 26(1):131\u2013137,", "citeRegEx": "Yang et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "In Advances in neural information processing systems", "author": ["Jieping Ye", "Ravi Janardan", "Qi Li. Twodimensional linear discriminant analysis"], "venue": "pages 1569\u20131576,", "citeRegEx": "Ye et al.. 2004", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 8, "context": "Recent advances in deep learning have generated a constant stream of new neural architectures, ranging from variants of feed-forward nets to differentiable Turing machines [LeCun et al., 2015; Graves et al., 2016].", "startOffset": 172, "endOffset": 213}, {"referenceID": 1, "context": "For recurrent nets, each time step has a matrix memory where rows/columns can be considered as memory slots and matrix mapping works like the attentive reading operator in [Graves et al., 2014].", "startOffset": 172, "endOffset": 193}, {"referenceID": 7, "context": "Convolutional nets [LeCun et al., 1995] whose output is typically a set of 2D feature maps can be combined seamlessly with matrix-centric networks using tensor-matrix mapping, and this preserves spatial structure of feature maps.", "startOffset": 19, "endOffset": 39}, {"referenceID": 1, "context": "(5) bears some similarity with the external memory bank in a Neural Turing Machine [Graves et al., 2014; Graves et al., 2016].", "startOffset": 83, "endOffset": 125}, {"referenceID": 6, "context": "For all experiments presented below, our networks use ReLU activation function and are trained using Adam [Kingma and Ba, 2014].", "startOffset": 106, "endOffset": 127}, {"referenceID": 15, "context": "To test the ability to accommodate very deep nets without skip-connections [Srivastava et al., 2015b; Pham et al., 2016], we create vector and matrix feed-forward nets with increasing depths.", "startOffset": 75, "endOffset": 120}, {"referenceID": 12, "context": "To test the ability to accommodate very deep nets without skip-connections [Srivastava et al., 2015b; Pham et al., 2016], we create vector and matrix feed-forward nets with increasing depths.", "startOffset": 75, "endOffset": 120}, {"referenceID": 4, "context": "We observe that without Batch-Norm (BN) [Ioffe and Szegedy, 2015], vector nets struggle to learn when the depth goes beyond 20, as evidenced in Fig.", "startOffset": 40, "endOffset": 65}, {"referenceID": 16, "context": "In this experiment, we compare the performance of matrix and vector recurrent nets in a sequence-to-sequence (seq2seq) learning task [Sutskever et al., 2014; Srivastava et al., 2015a].", "startOffset": 133, "endOffset": 183}, {"referenceID": 14, "context": "In this experiment, we compare the performance of matrix and vector recurrent nets in a sequence-to-sequence (seq2seq) learning task [Sutskever et al., 2014; Srivastava et al., 2015a].", "startOffset": 133, "endOffset": 183}, {"referenceID": 14, "context": "Different from [Srivastava et al., 2015a], the decoder do not have readout connections4 for simplicity.", "startOffset": 15, "endOffset": 41}, {"referenceID": 3, "context": "To model the frequency change of all channels over time, we use LSTMs [Hochreiter and Schmidhuber, 1997].", "startOffset": 70, "endOffset": 104}, {"referenceID": 21, "context": "Matrix data modeling has been well studied in shallow or linear settings, such as 2DPCA [Yang et al., 2004], 2DLDA [Ye et al.", "startOffset": 88, "endOffset": 107}, {"referenceID": 22, "context": ", 2004], 2DLDA [Ye et al., 2004], Matrix-variate Factor Analysis [Xie et al.", "startOffset": 15, "endOffset": 32}, {"referenceID": 19, "context": ", 2004], Matrix-variate Factor Analysis [Xie et al., 2008], Tensor analyzer [Tang et al.", "startOffset": 40, "endOffset": 58}, {"referenceID": 17, "context": ", 2008], Tensor analyzer [Tang et al., 2013], Matrix/tensor RBM [Nguyen et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 9, "context": ", 2013], Matrix/tensor RBM [Nguyen et al., 2015; Qi et al., 2016].", "startOffset": 27, "endOffset": 65}, {"referenceID": 13, "context": ", 2013], Matrix/tensor RBM [Nguyen et al., 2015; Qi et al., 2016].", "startOffset": 27, "endOffset": 65}, {"referenceID": 0, "context": "There have been several original deep architectures recently proposed to handle multidimensional data such as Multidimensional RNNs [Graves and Schmidhuber, 2009], Grid LSTMs [Kalchbrenner et al.", "startOffset": 132, "endOffset": 162}, {"referenceID": 5, "context": "There have been several original deep architectures recently proposed to handle multidimensional data such as Multidimensional RNNs [Graves and Schmidhuber, 2009], Grid LSTMs [Kalchbrenner et al., 2015] and Convolutional LSTMs [Xingjian et al.", "startOffset": 175, "endOffset": 202}, {"referenceID": 10, "context": "We are also aware of another work [Novikov et al., 2015] that proposes matrix decomposition of the weight matrix in vector-vector mapping similar to ours.", "startOffset": 34, "endOffset": 56}, {"referenceID": 11, "context": "They show that this kind of matrix decomposition is, indeed, a specific case of a more general Tensor Train (TT) transformation [Oseledets, 2011] when the tensor is a 2D matrix.", "startOffset": 128, "endOffset": 145}, {"referenceID": 1, "context": ", those in NTM [Graves et al., 2014; Graves et al., 2016] and Memory Networks [Weston et al.", "startOffset": 15, "endOffset": 57}, {"referenceID": 18, "context": ", 2016] and Memory Networks [Weston et al., 2014]).", "startOffset": 28, "endOffset": 49}], "year": 2017, "abstractText": "We present a new distributed representation in deep neural nets wherein the information is represented in native form as a matrix. This differs from current neural architectures that rely on vector representations. We consider matrices as central to the architecture and they compose the input, hidden and output layers. The model representation is more compact and elegant \u2013 the number of parameters grows only with the largest dimension of the incoming layer rather than the number of hidden units. We derive feed-forward nets that map an input matrix into an output matrix, and recurrent nets which map a sequence of input matrices into a sequence of output matrices. Experiments on handwritten digits recognition, face reconstruction, sequence to sequence learning and EEG classification demonstrate the efficacy and compactness of the matrix-centric architectures.", "creator": "LaTeX with hyperref package"}}}