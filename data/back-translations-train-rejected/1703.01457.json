{"id": "1703.01457", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2017", "title": "Chain-NN: An Energy-Efficient 1D Chain Architecture for Accelerating Deep Convolutional Neural Networks", "abstract": "Deep convolutional neural networks (CNN) have shown their good performances in many computer vision tasks. However, the high computational complexity of CNN involves a huge amount of data movements between the computational processor core and memory hierarchy which occupies the major of the power consumption. This paper presents Chain-NN, a novel energy-efficient 1D chain architecture for accelerating deep CNNs. Chain-NN consists of the dedicated dual-channel process engines (PE). In Chain-NN, convolutions are done by the 1D systolic primitives composed of a group of adjacent PEs. These systolic primitives, together with the proposed column-wise scan input pattern, can fully reuse input operand to reduce the memory bandwidth requirement for energy saving. Moreover, the 1D chain architecture allows the systolic primitives to be easily reconfigured according to specific CNN parameters with fewer design complexity. The synthesis and layout of Chain-NN is under TSMC 28nm process. It costs 3751k logic gates and 352KB on-chip memory. The results show a 576-PE Chain-NN can be scaled up to 700MHz. This achieves a peak throughput of 806.4GOPS with 567.5mW and is able to accelerate the five convolutional layers in AlexNet at a frame rate of 326.2fps. 1421.0GOPS/W power efficiency is at least 2.5 to 4.1x times better than the state-of-the-art works.", "histories": [["v1", "Sat, 4 Mar 2017 14:14:14 GMT  (949kb)", "http://arxiv.org/abs/1703.01457v1", "DATE 2017"]], "COMMENTS": "DATE 2017", "reviews": [], "SUBJECTS": "cs.AR cs.LG", "authors": ["shihao wang", "dajiang zhou", "xushen han", "takeshi yoshimura"], "accepted": false, "id": "1703.01457"}, "pdf": {"name": "1703.01457.pdf", "metadata": {"source": "CRF", "title": "Chain-NN: An Energy-Efficient 1D Chain Architecture for Accelerating Deep Convolutional Neural Networks", "authors": ["Shihao Wang", "Dajiang Zhou", "Xushen Han", "Takeshi Yoshimura"], "emails": ["wshh1216@moegi.waseda.jp"], "sections": [{"heading": null, "text": "INTRODUCTION Convolutional Neural Networks (CNN) have been a hot topic recently, as they have led to major breakthroughs in many computer vision tasks. Many CNN-based algorithms such as AlexNet [1] and VGG [2] have proven to be stronger than traditional algorithms, and among these networks, ResNet [3] has recently proven that a network with more than a thousand layers can further enhance performance compared to flatter networking. Increasing network scalability leads to more computational complexity. It usually takes quite a long time to train deep CNNs and also calls into question the design of real CNN-specific accelerators."}, {"heading": "A. Taxonomy of Existing CNN Architectures", "text": "The way data is used affects not only the reconfigurability, but also the required memory bandwidths (the amount of data movements). According to [16], the data movements of Convolutionary Operands can be more expensive than ALU operations under existing techniques. We give a detailed description and conclusion of the advantages and disadvantages of these two categories, followed by the introduction of how Chain-NN can improve over existing works in Sec. II.B.1) Memory-centric architectures As shown in Fig. 2, these architectures are based on the addressability of memory to achieve their reconfigurability for various CNN parameters. The processor core is simply a stack of PEs. There is no data storage or data reuse within the processor."}, {"heading": "B. High-level Description of Chain-NN", "text": "In fact, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own."}, {"heading": "C. Dual-channel PE Architecture", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "D. Dataflow and Memory Hierarchy", "text": "A recent paper [7] discusses a detailed analysis of CNN memoryefficient dataflow and on-chip memory hierarchy, whose strategies we are using to test our proposed 1D Chain Architecture. Data flow is modified to support the requirement for column-by-column scan input patterns discussed in Sec. IV.C. The detailed data flow is demonstrated by a loop structure in Fig. 7. Chain-NN parallelism unfolds the loop of maps and windings marked as ParaTile. Meanwhile, two separate memories, iMemory and oMemory, form the storage hierarchy for data reuse under InnerTile.V. ERIMENTAL RESULTS"}, {"heading": "A. Methodology", "text": "Chain-NN was encoded with SystemVerilog HDL and synthesized with TSMC 28nm HPC library under slow operating conditions (0.81V, 125oC) using the Synopsys Design Compiler. Layout was performed by Encounter, as shown in Fig. 8. We implemented a floating-point-to-fix simulator integrated in MatConvnet to verify the test data set including the revolutionary layers of pre-trained networks for MNIST, Cifar-10, AlexNet and VGG-16. Verification is performed with ModelSim both through function simulation and post-synthesis simulation. The results of the hardware are verified with the simulation results on-the-fly. Power Compiler analyzes the performance of the Chain-NN under typical operating conditions (0.9V, 25oC)."}, {"heading": "B. Performance", "text": "In theory, this design can achieve a peak throughput of 806.4GOPS.AlexNet to evaluate realistic performance. We are using a total of 352KB of on-chip memory, including kMemory in each PE, to facilitate data reuse in AlexNet based on the data flow in Fig. 7. In detail, we are implementing 32KB for iMemory, 295KB for kMemory, and 25KB for oMemory. In terms of kMemory, 295KB is distributed on average across 576 PEs, resulting in a capacity of 256 kernel weights per PE.AlexNet, which includes five revolutionary layers, including a total of 666 million MACs per 227x227 input image. Fig. 9 shows the time distribution of five revolutionary layers in AlexNet. AlexNet contains five revolutionary layers, from which we benefit only once per batch and 3252ps per batch input image."}, {"heading": "C. Energy Efficiency", "text": "Energy efficiency is measured by throughput per watt. Fig. 10 shows that Chain-NN consumes 567.5 mW and contributes 806.4 GOPS, resulting in an energy efficiency of 1421 GOPS / W. In detail, about 90% of the power consumption comes from the 1D chain architecture including kMemory, while the memory hierarchy causes only 10.55% of the cost. In Chain-NN, the power consumption of memory is significantly reduced in two ways. First, we have shortened the length of most data movements from SRAM level to on-chip level to PE level. Specifically, dedicated PEs with column-wide scan input patterns guarantee that ifmaps K2 times on average are reused within systolic primitives, which can reduce the data movements of ifmaps from SRAM to only (2K-1) / K times. Furthermore, cores are stored within PEs, so that all movements take place within systolic primitives only."}, {"heading": "D. Comparison with the State-of-the-art Works", "text": "DaDianNao [10] belongs to memory-centered architectures, while Eyeriss [12] represents the spatial 2D structure.Table V shows the comparison results. Our chain NN is at least 2.5 times energy efficient. Fig. 10 provides the comparison with [10]. We have roughly divided the designs into two parts, processor core and memory hierarchy. Looking at processor cores only, [10] about 3.0TOPS / W can be achieved, while ChainNN is about 1.7TOPS / W. This is reasonable since we have reduced the energy costs of data movements due to the analysis in sec.V.C. at the expense of increasing the complexity of the processor core. Therefore, Chain-NN can actually reach 1421,000 GOPS / W when we measure the entire chip (without the energy of off-chip DRAM access in Table IV)."}, {"heading": "ACKNOWLEDGMENT", "text": "This work is supported by the Waseda University Graduate Program for Embodiment Informatics (FY2013-FY2019)."}], "references": [{"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Neural Information Processing Systems, pp. 1097\u20131105, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Very Deep Convolutional Networks for LargeScale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Do deep nets really need to be deep?", "author": ["J. Ba", "R. Caruana"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "cuDNN: Efficient Primitives for Deep Learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": "CoRR, vol. abs/1410.0759, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "The neuro vector engine: Flexibility to improve convolutional net efficiency for wearable vision", "author": ["M. Peemen", "R. Shi", "S. Lal", "B. Juurlink", "B. Mesman", "H. Corporaal"], "venue": "Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 1604-1609, 2016", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Origami: A Convolutional Network Accelerator", "author": ["L. Cavigelli", "D. Gschwend", "C. Mayer", "S. Willi", "B. Muheim", "L. Benini"], "venue": "Proceedings of GLSVLSI-25, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning", "author": ["T. Chen", "Z. Du", "N. Sun", "J. Wang", "C. Wu", "Y. Chen", "O. Temam"], "venue": "ASPLOS, pp. 269-284, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "DaDianNao: A Machine-Learning Supercomputer", "author": ["Y. Chen"], "venue": "47th International Symposium on Microarchitecture, pp. 609-622, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "ShiDianNao: Shifting vision processing closer to the sensor", "author": ["Z. Du"], "venue": "42nd Annual International Symposium on Computer Architecture (ISCA), pp. 92-104, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "14.5 Eyeriss: An energyefficient reconfigurable accelerator for deep convolutional neural networks", "author": ["Y.H. Chen", "T. Krishna", "J. Emer", "V. Sze"], "venue": "2016 IEEE International Solid-State Circuits Conference (ISSCC), San Francisco, CA, 2016, pp. 262-263.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Cnvlutin: Ineffectualneuron-free deep neural network computin", "author": ["J. Albericio", "T. Aamodt", "T.H. Hetherington"], "venue": "42nd Annual International Symposium on Computer Architecture (ISCA), pp. 1-13, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "EIE: Efficient Inference Engine on compressed deep neural network", "author": ["S. Han", "X. Liu", "H. Mao", "J. Pu", "A. Pedram", "M.A. Horowitz", "W.J. Dally"], "venue": "2016 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA), pp. 243-254, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "A ultra-low-energy convolution engine for fast brain-inspired vision in multicore clusters", "author": ["F. Conti", "L. Benini"], "venue": "2015 Design, Automation & Test in Europe Conference & Exhibition (DATE), 2015, pp. 683-688.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "One-dimensional systolic arrays for multidimensional convolution and resampling", "author": ["HT Kung", "RL Picard"], "venue": "kMemory", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1984}], "referenceMentions": [{"referenceID": 0, "context": "Many CNN-based algorithms like AlexNet [1] and VGG [2] have proved to be stronger than conventional algorithms.", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "Many CNN-based algorithms like AlexNet [1] and VGG [2] have proved to be stronger than conventional algorithms.", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "Further, there is a trend that CNN is towards deeper and larger for a better performance [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "For example, ResNet [3] has recently proved a network with more than a thousand layers can further enhance the performance compared to a shallower network.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "The designs on CPU or GPU like [5] can achieve a high reconfigurability at the expense of high energy costs and usually suffer from the Von-Neumann bottleneck which limits their performance.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 22, "endOffset": 27}, {"referenceID": 6, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 7, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 8, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 9, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 10, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 11, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 12, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 13, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 10, "context": "For example, [12] builds a 2D PE array, which has an on-chip network and inter-PE communications.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "According to [16], the data movements of convolutional operands can be more expensive than ALU operations under the existing techniques.", "startOffset": 13, "endOffset": 17}, {"referenceID": 7, "context": "They highly rely on memories for huge amount of data movements like [9,10,13].", "startOffset": 68, "endOffset": 77}, {"referenceID": 8, "context": "They highly rely on memories for huge amount of data movements like [9,10,13].", "startOffset": 68, "endOffset": 77}, {"referenceID": 11, "context": "They highly rely on memories for huge amount of data movements like [9,10,13].", "startOffset": 68, "endOffset": 77}, {"referenceID": 6, "context": "Some achieve their reconfigurability by utilizing fewer PEs with fewer feeding data from memories, resulting in a waste of computing resources [8,15].", "startOffset": 143, "endOffset": 149}, {"referenceID": 13, "context": "Some achieve their reconfigurability by utilizing fewer PEs with fewer feeding data from memories, resulting in a waste of computing resources [8,15].", "startOffset": 143, "endOffset": 149}, {"referenceID": 9, "context": "This solution can reduce the amount of data movements and have been employed in many works like [11,12].", "startOffset": 96, "endOffset": 103}, {"referenceID": 10, "context": "This solution can reduce the amount of data movements and have been employed in many works like [11,12].", "startOffset": 96, "endOffset": 103}, {"referenceID": 14, "context": "4 to form a systolic architecture like [16], which we call 1D systolic primitive.", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "Comparison with the State-of-the-art Works Two ASIC-based works [10][12] are chosen for comparison.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "Comparison with the State-of-the-art Works Two ASIC-based works [10][12] are chosen for comparison.", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "DaDianNao [10] belongs to memory-centric architectures while Eyeriss [12] represents the 2D spatial ones.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "DaDianNao [10] belongs to memory-centric architectures while Eyeriss [12] represents the 2D spatial ones.", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "10 gives the comparison with [10].", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "If only processor cores are considered, [10] can achieve around 3.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "51k/PE logic gates while [12] is 11.", "startOffset": 25, "endOffset": 29}], "year": 2017, "abstractText": "Deep convolutional neural networks (CNN) have shown their good performances in many computer vision tasks. However, the high computational complexity of CNN involves a huge amount of data movements between the computational processor core and memory hierarchy which occupies the major of the power consumption. This paper presents Chain-NN, a novel energy-efficient 1D chain architecture for accelerating deep CNNs. Chain-NN consists of the dedicated dual-channel process engines (PE). In Chain-NN, convolutions are done by the 1D systolic primitives composed of a group of adjacent PEs. These systolic primitives, together with the proposed column-wise scan input pattern, can fully reuse input operand to reduce the memory bandwidth requirement for energy saving. Moreover, the 1D chain architecture allows the systolic primitives to be easily reconfigured according to specific CNN parameters with fewer design complexity. The synthesis and layout of Chain-NN is under TSMC 28nm process. It costs 3751k logic gates and 352KB on-chip memory. The results show a 576-PE Chain-NN can be scaled up to 700MHz. This achieves a peak throughput of 806.4GOPS with 567.5mW and is able to accelerate the five convolutional layers in AlexNet at a frame rate of 326.2fps. 1421.0GOPS/W power efficiency is at least 2.5 to 4.1x times better than the state-of-the-art works. Keywords\u2014convolutional neural networks; CNN; accelerator; ASIC; power efficiency; memory bandwidth", "creator": "Microsoft\u00ae Word 2016"}}}