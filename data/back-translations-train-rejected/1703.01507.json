{"id": "1703.01507", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2017", "title": "Machine Learning Friendly Set Version of Johnson-Lindenstrauss Lemma", "abstract": "In this paper we make a novel use of the Johnson-Lindenstrauss Lemma. The Lemma has an existential form saying that there exists a JL transformation $f$ of the data points into lower dimensional space such that all of them fall into predefined error range $\\delta$. We formulate in this paper a theorem stating that we can choose the target dimensionality in a random projection type JL linear transformation in such a way that with probability $1-\\epsilon$ all of them fall into predefined error range $\\delta$ for any user-predefined failure probability $\\epsilon$. This result is important for applications such a data clustering where we want to have a priori dimensionality reducing transformation instead of trying out a (large) number of them, as with traditional Johnson-Lindenstrauss Lemma.", "histories": [["v1", "Sat, 4 Mar 2017 19:08:22 GMT  (32kb,D)", "https://arxiv.org/abs/1703.01507v1", null], ["v2", "Mon, 15 May 2017 17:56:29 GMT  (52kb,D)", "http://arxiv.org/abs/1703.01507v2", "34 pages, 6 Figures"], ["v3", "Thu, 7 Sep 2017 08:06:18 GMT  (54kb,D)", "http://arxiv.org/abs/1703.01507v3", "38 pages, 6 Figures"], ["v4", "Mon, 18 Sep 2017 08:35:17 GMT  (55kb,D)", "http://arxiv.org/abs/1703.01507v4", "38 pages, 6 Figures"]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["mieczys{\\l}aw a k{\\l}opotek"], "accepted": false, "id": "1703.01507"}, "pdf": {"name": "1703.01507.pdf", "metadata": {"source": "CRF", "title": "Machine Learning Friendly Set Version of Johnson-Lindenstrauss Lemma", "authors": ["Mieczys\u0142aw A. K\u0142opotek"], "emails": ["(klopotek@ipipan.waw.pl)"], "sections": [{"heading": null, "text": "In this paper, we use the Johnson-Lindenstrauss Lemma. The Lemma has an existential form, which states that there is a JL transformation for the data points in the underdimensional space, so that they all fall within a predefined error range. In this paper, we formulate a theorem that states that we can select the target dimensionality in a random projection of type JL linear transformation in such a way that with probability 1 - all of them fall within a predefined error range for each custom error probability. This result is important for applications such as data clustering, where we want to have a a a priori dimensionality-reducing transformation, rather than try a (large) number of them, as with conventional Johnson-Lindenstrauss Lemma. In particular, we look more closely at the k-means algorithm and prove that a good solution in the projected space is also a good solution in the original space. Furthermore, local optimizations in the original space form that projected space in the Lemma."}, {"heading": "1 Introduction", "text": "Dimensionality of reduction plays an important role in many areas of data processing and in particular in the field of machine learning (cluster analysis, classification, validation, visualisation of data, etc.).Normally it is associated with multifaceted learning, i.e. with a belief that the data is in fact located in a low dimensional subspace that needs to be identified and the data projected onto it, so that the number of degrees of freedom is reduced and as a result sample sizes can be smaller without loss of reliability. Techniques such as reduced k means [17], PCA (Principal Component Analysis), Kernel PCA, LLE (Locally Linear Embedding), LEM (Laplacian eigenmaps), MDS (Metric Multidimensional Scaling), Isomap, SDE (Semidefinite Embedding), just to mention another way of approaching dimensionality, especially when data cannot be localized."}, {"heading": "2 Derivation of the Set-Friendly", "text": "Johnson-Lindenstrauss LemmaLet us explain the process of searching for the assignment f of theorem 1 in more detail, so that we can then switch to our goal of selecting the size of the subspace to ensure that the projected distances maintain their proportionality in the required range. First, let us consider a single vector x = (x1,.x2,..., xn) n (xn) n (x2, xn) n (xn) n (x2, xn) n (xn) n (xn) n \u00b2 (x2, x2) n (n) n \u00b2 (n) n \u00b2 (n) n \u00b2 (n) n \u00b2 (n) n \u00b2 (n) n \u00b2 (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n n (n) n n n (n) n n (n) n n (n) n n n (n) n (n n n n) n (n n n n (n) n (n n n) n (n n n n n (n) n (n n) n (n n n n n (n) n n (n) n (n n n n n (n) n (n) n (n n n n n) n (n (n) n n n n n (n) n (n n) n (n n n n n) n (n n n n n n n n) n (n n n n) n (n) n (n (n) n n n n n (n n) n (n) n (n n n) n n (n n n) n (n (n) n n n (n) n n n n n (n n n n n (n) n n n (n) n n n (n) n (n n (n) n n n n (n) n n n (n) n n n (n n n n n n n (n n) n (n) n (n) n n n (n n n n (n n (n) n n n n n (n) n (n) n (n n) n n n n n n n n (n n (n) n n n"}, {"heading": "3 Proofs of theorems 2-5", "text": "The permissible error will certainly depend on the target application in this paper. First, let's look at the context of Q averages. First, we claim that the JL value is applied not only to data points but also to cluster centers. 5 We have replaced the denominator with a smaller positive number and the nominator with a larger positive number, so a higher n value is required than is actually needed. 6 Although a similar result in Lemma 5,3 http: / / math.mit.edu / ~ bandeira / 2015 _ 18.S096 _ 5 _ Johnson _ Lindenstrauss.pdf, although without explicit evidence. They suggest that a similar result in Lemma 5,3 http: / / math.mit.edu / ~ bandeira / 2015 _ 18.S096 _ 5 _ 5 _ Johnson _ 5 _ Johnson _ Lindenstrauss.pdf, although there is no explicit evidence."}, {"heading": "4 Clusterability and the dimensionality re-", "text": "In the literature, a number of terms of so-called clustering capability have been introduced. < J < Q > Q < J < J < Q < Q < Q > Q > Q < J < J < J < M < M < M < M < M < M < M < M < M < M < M < M < M < M < M < M < M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M; M"}, {"heading": "5 Numerical Experiments on Some Aspects", "text": "The value of reduced dimensionality n \"in the former depends on n\" on margin of error delta Dependence of reduced dimensionality n \"on original dimensionality nDiscretion between original and projected square dimensionality n\" while it is explicitly in n. \"However, the value of n\" in the former depends on n \"but can only iteratively.Let us examine the differences between the n\" calculation in both cases. Let's examine the effects of the following parameters: n - the original dimensionality (see Table 4 and Figure 4), \u03b4 - the limitation of the deviation of the distances between the data in the original and the reduced space. (see Table 3 and Figure 3), m - the explicit formula (see Table 1 and Figure 1), which we consider \"probable.\""}, {"heading": "6 Previous work", "text": "\u00b1 that if we set (close) to 1 and by Taylor method extended the ln function to the denominator of inequality (4) to up to three terms, then we get the value of n \u00b2 from the equation (2,1) from the paper [11]: n \u00b2 4 lnm \u03b42 \u2212 \u03b43Note, however, this setting to a value close to 1 makes no sense since we want to keep the event that the data do not fit the interval we impose rare \u2212 although one may be tempted to view our results as formally similar to those of Dasgupta and Gupta, there is a big difference. Let us first remember that the original evidence of Johnson and Lindenstrauss [13] is probable and shows that the projection of the m point onto a random subspace of O (lnm / 2) dimensions changes only the (square) deviations between the points by a maximum of 1 \u2212 and 2 with positive probability. Dasgupta and Gupta showed that this probability is at least 1."}, {"heading": "7 Conclusions", "text": "In this paper, we examined a novel aspect of the well-known and widely researched and exploited Johnson-Lindenstrauss problem of the possibility of reducing dimensionality by projecting onto a random subspace. In practice, the original formulation means that we must verify that we have found a correct transformation f that leads to error limits within the required range for all pairs of points, and if necessary (and it is theoretically very often necessary) to repeat the random projection process over and over again. We have shown here that it is possible to determine the choice of dimensionality in the random projection process in advance to ensure with the desired certainty that none of the points in the dataset violates limitations of error boundaries. This new formulation may be relevant for many data mining applications, such as clustering, where distortion of distances subtly affects the results (e.g. k means clusters) based on the number of thousands of projections we have pointed out on some examples of these dimensions starting from the number of thousands."}], "references": [{"title": "Database-friendly random projections: Johnsonlindenstrauss with binary coins", "author": ["Dimitris Achlioptas"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Clusterability: A theoretical study", "author": ["Margareta Ackerman", "Shai Ben-David"], "venue": "Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Approximate nearest neighbors and the fast johnson-lindenstrauss transform", "author": ["Nir Ailon", "Bernard Chazelle"], "venue": "In Proceedings of the Thirtyeighth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "k-means++: the advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proc. of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Stability yields a ptas for k-median and k-means clustering", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Center-based clustering under perturbation stability", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "Inf. Process. Lett.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Approximate clustering without the approximation", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Computational feasibility of clustering under clusterability", "author": ["Shai Ben-David"], "venue": "assumptions. https://arxiv.org/abs/1501.00437,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Are stable instances easy", "author": ["Yonatan Bilu", "Nathan Linial"], "venue": "Comb. Probab. Comput.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Random projection estimation of discrete-choice models with large choice sets, arxiv:1604.06036, 2016", "author": ["Khai X. Chiong", "Matthew Shum"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "An elementary proof of a theorem of johnson and lindenstrauss", "author": ["Sanjoy Dasgupta", "Anupam Gupta"], "venue": "Random Struct. Algorithms,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Nearest-neighbor-preserving embeddings", "author": ["Piotr Indyk", "Assaf Naor"], "venue": "ACM Trans. Algorithms,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Extensions of lipschitz mappings into a hilbert space. In Conference in modern analysis and probability (New Haven, Conn., 1982)", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "Also appeared in volume 26 of Contemp. Math.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1984}, {"title": "Optimality of the johnsonlindenstrauss", "author": ["Kasper Green Larsen", "Jelani Nelson"], "venue": "lemma. CoRR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "On variants of the johnson-lindenstrauss lemma", "author": ["Ji\u0155\u0131 Matousek"], "venue": "Random Struct. Algorithms,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "venue": "J. ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Strong consistency of reduced k-means clustering", "author": ["Yoshikazu Terada"], "venue": "Scand. J. Stat.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "Techniques like reduced k-means [17], PCA (Principal Component Analysis), Kernel PCA, LLE (Locally Linear Embedding), LEM (Laplacian Eigenmaps), MDS (Metric Multidimensional Scaling), Isomap, SDE (Semidefinite Embedding), just to mention a few.", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "The starting point here is the Johnson-Lindenstrauss Lemma [13].", "startOffset": 59, "endOffset": 63}, {"referenceID": 3, "context": "by k-means algorithm [4].", "startOffset": 21, "endOffset": 24}, {"referenceID": 10, "context": "[11, 1, 3, 12, 14, 10].", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "[11, 1, 3, 12, 14, 10].", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "[11, 1, 3, 12, 14, 10].", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": "[11, 1, 3, 12, 14, 10].", "startOffset": 0, "endOffset": 22}, {"referenceID": 13, "context": "[11, 1, 3, 12, 14, 10].", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "[11, 1, 3, 12, 14, 10].", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "It is claimed afterwards that this mapping is distance-preserving not only for a single vector, but also for large sets of points with some, usually very small probability, as Dasgupta and Gupta [11] maintain.", "startOffset": 195, "endOffset": 199}, {"referenceID": 10, "context": "We postpone the proof of the theorems 2-5 till section 3, as we need first to derive the basic theorem 6 in section 2 which is essentially based on the results reported by Dasgupta and Gupta [11].", "startOffset": 191, "endOffset": 195}, {"referenceID": 10, "context": "Dasgupta and Gupta [11] in their Lemma 2.", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "Two brands may be distinguished: additive [2] and multiplicative ones [9] (the limit of perturbation is upper-bounded either by an absolute value or by a coefficient).", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "Two brands may be distinguished: additive [2] and multiplicative ones [9] (the limit of perturbation is upper-bounded either by an absolute value or by a coefficient).", "startOffset": 70, "endOffset": 73}, {"referenceID": 15, "context": "\u2022 \u03c3-Separatedness [16] meaning that the cost J(Q,Ck) of optimal clustering Ck of the data set Q into k clusters is less than \u03c3 (0 < \u03c3 < 1) times the cost J(Q,Ck\u22121) of optimal clustering Ck\u22121 into k\u2212 1 clusters J(Q,Ck\u22121) < \u03c3 J(Q,Ck\u22121) \u2022 (c, \u03c3)-Approximation-Stability [7] meaning that if the cost function values of two partitions Ca,Cb differ by at most the factor c > 1 (that is c \u00b7J(Q,Ca) \u2265 J(Q,Cb) and c \u00b7J(Q,Cb) \u2265 J(Q,Ca)), then the distance (in some space) between the partitions is at most \u03c3 (d(Ca,Cb) < \u03c3 for some distance function d between partiitions).", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "\u2022 \u03c3-Separatedness [16] meaning that the cost J(Q,Ck) of optimal clustering Ck of the data set Q into k clusters is less than \u03c3 (0 < \u03c3 < 1) times the cost J(Q,Ck\u22121) of optimal clustering Ck\u22121 into k\u2212 1 clusters J(Q,Ck\u22121) < \u03c3 J(Q,Ck\u22121) \u2022 (c, \u03c3)-Approximation-Stability [7] meaning that if the cost function values of two partitions Ca,Cb differ by at most the factor c > 1 (that is c \u00b7J(Q,Ca) \u2265 J(Q,Cb) and c \u00b7J(Q,Cb) \u2265 J(Q,Ca)), then the distance (in some space) between the partitions is at most \u03c3 (d(Ca,Cb) < \u03c3 for some distance function d between partiitions).", "startOffset": 267, "endOffset": 270}, {"referenceID": 7, "context": "As Ben-David [8] recalls, this implies the uniqueness of optimal solution.", "startOffset": 13, "endOffset": 16}, {"referenceID": 5, "context": "\u2022 \u03b2-Centre Stability [6] meaning, for any centric clustering, that the distance of an element to its cluster centre is \u03b2 > 1 times smaller than the distance to any other cluster centre under optimal clustering.", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "\u2022 (1 + \u03b2) Weak Deletion Stability [5] (\u03b2 > 0) meaning that given an optimal cost function value OPT for k centric clusters, the cost function of a clustering obtained by deleting one of the cluster centres and assigning elements of that cluster to one of the remaining clusters should be bigger than (1 + \u03b2) \u00b7OPT .", "startOffset": 34, "endOffset": 37}, {"referenceID": 10, "context": "1) from the paper [11]:", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "Table 5: Comparison of effort needed for k-means under our dimensionality reduction approach and that of Dasgupta and Gupta [11], depending on sample size m .", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": "Table 6: Comparison of effort needed for k-means under our dimensionality reduction approach and that of Dasgupta and Gupta [11], depending on failure prob.", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": "Table 7: Comparison of effort needed for k-means under our dimensionality reduction approach and that of Dasgupta and Gupta [11], depending on error range \u03b4 .", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": "Table 8: Comparison of effort needed for k-means under our dimensionality reduction approach and that of Dasgupta and Gupta [11], depending on original dimensionality n .", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "Let us first recall that the original proof of Johnson and Lindenstrauss [13] is probabilistic, showing that projecting the m -point subset onto a random subspace of O(lnm/ ) dimensions only changes the (squared) distances between points by at most 1 \u2212 \u03b4 with positive probability.", "startOffset": 73, "endOffset": 77}, {"referenceID": 0, "context": "Note that the choice of n\u2032 has been estimated by [1] n\u2032 \u2265 (4 + 2\u03b3) lnm \u03b42 \u2212 \u03b43 where \u03b3 is some positive number.", "startOffset": 49, "endOffset": 52}, {"referenceID": 13, "context": "Larsen and Nelson [14] concentrate on finding the highest value of n\u2032 for which Johnson-Lindenstrauss Lemma does not hold demonstrating that the value they found is the tightest even for non-linear mappings f .", "startOffset": 18, "endOffset": 22}], "year": 2017, "abstractText": "In this paper we make a novel use of the Johnson-Lindenstrauss Lemma. The Lemma has an existential form saying that there exists a JL transformation f of the data points into lower dimensional space such that all of them fall into predefined error range \u03b4. We formulate in this paper a theorem stating that we can choose the target dimensionality in a random projection type JL linear transformation in such a way that with probability 1\u2212 all of them fall into predefined error range \u03b4 for any user-predefined failure probability . This result is important for applications such a data clustering where we want to have a priori dimensionality reducing transformation instead of trying out a (large) number of them, as with traditional Johnson-Lindenstrauss Lemma. In particular, we take a closer look at the k-means algorithm and prove that a good solution in the projected space is also a good solution in the original space. Furthermore, under proper assumptions local optima in the original space are also ones in the projected space. We define also conditions for which clusterability property of the original space is transmitted to the projected space, so that special case algorithms for the original space are also applicable in the projected space.", "creator": "LaTeX with hyperref package"}}}