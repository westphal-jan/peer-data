{"id": "1703.01678", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2017", "title": "Data-Dependent Stability of Stochastic Gradient Descent", "abstract": "We establish a data-dependent notion of algorithmic stability for Stochastic Gradient Descent (SGD) and employ it to develop novel generalization bounds. This is in contrast to previous distribution-free algorithmic stability results for SGD which depend on the worst-case constants. By virtue of the data-dependent argument, our bounds provide new insights into learning with SGD on convex and non-convex problems. In the convex case, we show that the bound on the generalization error is multiplicative in the risk at the initialization point. In the non-convex case, we prove that the expected curvature of the objective function around the initialization point has crucial influence on the generalization error. In both cases, our results suggest a simple data-driven strategy to stabilize SGD by pre-screening its initialization.", "histories": [["v1", "Sun, 5 Mar 2017 22:22:34 GMT  (29kb,D)", "https://arxiv.org/abs/1703.01678v1", null], ["v2", "Wed, 22 Mar 2017 17:16:17 GMT  (33kb,D)", "http://arxiv.org/abs/1703.01678v2", null], ["v3", "Fri, 26 May 2017 17:33:50 GMT  (119kb,D)", "http://arxiv.org/abs/1703.01678v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ilja kuzborskij", "christoph h lampert"], "accepted": false, "id": "1703.01678"}, "pdf": {"name": "1703.01678.pdf", "metadata": {"source": "CRF", "title": "Data-Dependent Stability of Stochastic Gradient Descent", "authors": ["Ilja Kuzborskij", "Christoph H. Lampert"], "emails": ["ilja.kuzborskij@idiap.ch", "chl@ist.ac.at"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are living, in which they are able to live, in which they are living, in which they are living, in which they"}, {"heading": "2 Related Work", "text": "In fact, it is so that most people who are able to survive themselves are also able to survive themselves. In fact, it is so that they are able to survive themselves by surviving themselves. In fact, it is so that they are able to survive themselves. In fact, it is so that they are able to survive themselves. In the ability to survive themselves, it is so that they are able to survive themselves. In the ability to survive themselves, it is not that they do it as if they want what they want. In the ability to survive themselves, it is not that they do it as if they want what they want."}, {"heading": "3 Stability of Stochastic Gradient Descent", "text": "First, we introduce definitions that will be used in the rest of the essay."}, {"heading": "3.1 Definitions", "text": "We will use small and large bold letters to denote column vectors and matrices, e.g. a \"ra1, a2,.., adsT P Rd and A P Rd1\u0445 d2,} a} is understood as a Euclidean norm and} A} 2 as a spectral norm. We will refer to the enumeration by rns\" t1,..., nu for n P N.We will give an example space by Z and its member by z P Z. In particular, we will refer to the training set as S \"tziumi\" 1 \"DM. For a parameter space H, we will define a learning algorithm as a map A: Zm: pichting H and for the abbreviation, we will use the notation AS\" Sempiumi \"1\" DM. For a parameter space H: we will define a learning algorithm as a map."}, {"heading": "3.2 Uniform Stability and Generalization", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "4 Data-dependent Stability Bounds for SGD", "text": "In this section, we describe an idea of data-dependent algorithmic stability, which allows us to specify q q q generalization limits, which, however, depend not only on the properties of the learning algorithm, but also on the additional parameters of the algorithm. We specify such additional parameters by \u03b8, and therefore we call stability a function p\u03b8q. In the following, we will be particularly interested in scenarios in which \u03b8 describes the data-generating distribution and the initialization point of SGD. Definition 2 (On-Average stability). A randomized algorithm A is p\u03b8q-on-average stable, if it is true."}, {"heading": "5 Main Results", "text": "Before we present our main results in this section, we will discuss algorithmic details and assumptions. We will study the following variant of SGD: \"SGD.\" \"SGD.\" \"SGD.\" \"SGD.\" \"SGGD.\" \"SGGD.\" \"SGD.\" \"SGD.\" \"SGD.\" \"\" SGD. \"\" \"SGpwt.\" \"\" SGpwt. \"\" \"SGpwt.\" \"\" SGpwt. \"\" \"SGpwt.\" \"\" \"SGpppwt.\" \"\" \"In practice, this corresponds to a permutation of the training set before it goes through, as is usual in practice.\" Next, we will provide statements about the loss functions f. \"\" \"\" SGppwp. \"\" pppppppppppppppp. \"pppppppppppppp.\" pppppppppppp. \"pppppppppp.\" pppppppppp. \"pppppppppp."}, {"heading": "5.1 Convex Losses", "text": "First, we do not provide a new and data-dependent stability result for the SGD in expectation of a complete update. Suppose we cannot consider this result as equivalent, and the SGD risk is so great that it is a multiplicative risk at the initialization point, that is aRpw1q, instead of a Lipschitz constant. Thus, our bound intuition shows that if we start at a good point in the objective function, the algorithm is more stable and thus better."}, {"heading": "5.2 Non-convex Losses", "text": "\"We have a new stability for the non-convex losses of the SGD.\" (3) In particular, we characterize how the new stability affects the stability of the SGD. (3) \"We have a new stability in the SGD.\" (4) \"We have a new stability in the SGD.\" (3) \"We have a new stability in the SGD.\" (4) \"We have a new stability in the SGD.\" (4) \"We have a new stability in the SGD.\" (4) \"We have a new stability in the SGD.\" (4) \"We have a new stability in the SGD.\" (4) \"We have a new stability in the SGD.\" (4) \"We have a new stability in the SGD.\" (4) \"We have a new stability in the SGD.\""}, {"heading": "5.2.1 Tightness of Non-convex Bounds", "text": "In the following experiment, we train a neural network with three revolutionary layers connected by maximization, followed by the fully connected layer of 16 units on the MNIST dataset. This adds up in a model with 18K parameters. Figure 1 compares our data-dependent layer (2) with the distribution-free layer of [16, theorem 3.8]. For reference, we also include an empirical estimate of the generalization error as the absolute difference between the validation and training mean losses. Since our limit also depends on the initialization point, we draw (2) for multiple \"warm starts,\" i.e. SGD initialized from a pre-trained position. We consider 7 such warm starts as the absolute difference between the validation and training mean losses, and report data-dependent parameters used to calculate (2) nitrates."}, {"heading": "5.3 Application to Transfer Learning", "text": "An example of the application of data-dependent boundaries presented prior to Transfer Learning (TL) is the hypothesis that we are interested in generalizing a target task more quickly by evaluating lateral information coming from different but related source tasks. TL literature has explored many ways to do this, and here we will focus on the one that is most compatible with our boundaries. Formally, we assume that the target task at hand is characterized by a common source probability distribution, and as before, we have a S iid \"Dm training environment. Some TL approaches also assume that access to the data selected from the distributions related to the source tasks is viewed as a conservative approach - instead of the source data, we get a set of hypotheseswsrck (K\" 1H, trained on the source tasks. The goal of a learner is to arrive at a target hypothesis that we base in the optimistic scenario by generalizing source data."}, {"heading": "6 Conclusions and Future Work", "text": "In this work, we demonstrated data-dependent stability limits for SGD and verified their generalizability. We presented new limits for convex and non-convex smooth loss functions, partly controlled by data-dependent variables, while previous stability limits for SGD were derived from the worst-case analysis. In particular, for non-convex learning, we theoretically demonstrated that the generalization of SGD is strongly influenced by the expected curvature around the initialization point. We demonstrated empirically that our limit is actually narrower than the uniform one. Furthermore, our data-dependent analysis allowed us to be optimistic about the generalization error of the SGD, which exhibits fast rates subject to the vanishing empirical risk of the algorithm. In future work, we intend to continue experimentally investigating our theoretical results and evaluate the feasibility of transfer learning based on second-order information. Another direction is to adjust our limits."}, {"heading": "Acknowledgments", "text": "This work was partly funded by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (Funding Agreement No. 637076) and partly funded by the European Research Council under the Seventh Framework Programme of the European Union (FP7 / 2007-2013) / ERC Funding Agreement No. 308036."}, {"heading": "A Proofs", "text": "In this area we are able to lean on the \"E.\" \"We,\" so SrpASqs, \"we.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\". \"\" We. \"\". \"\" \"We.\" \".\" \"\". \"\" \".\" \"\". \"\" \"\". \"\" \"\". \"\" \"\". \"\" \"\".. \"\" \"\" \"..\" \"\" \"\".. \"\" \"\". \"\". \"\". \"\" \".\" \".\" \"\". \"\" \".\" \"\". \"\" \"\".. \"\" \"\". \"\" \"\" \"\".. \"\" \".\" \"\". \"\" \".\" \".\" \".\" \"\". \".\" \".\" \".\" \".\" \".\" \".\" \".\" \".\" \".\" \"\". \"\". \"\". \"\". \"\". \"\" \".\" \".\" \".\" \".\". \"\" \".\" \".\". \"\". \"\". \".\" \".\" \".\". \"\" \".\". \"\" \"\". \".\". \"\". \"\" \".\". \"\". \"\". \".\" \"\" \".\" \".\". \".\" \"\". \".\" \"\". \".\" \"\". \".\" \"\". \".\" \".\" \"\". \".\" \"\". \".\" \"\" \".\". \"\". \".\" \"\". \".\" \"\". \".\". \"\" \".\" \".\" \".\" \"\". \".\". \"\". \"\" \"\". \"\". \"\". \"\". \"\" \".\" \".\" \"\". \"\". \"\" \".\" \".\" \"\". \"\" \".\" \".\" \".\" \".\" \".\". \"\" \"\". \"\" \"\". \".\" \"\". \"\" \".\" \"\". \"\". \".\". \"\" \"\". \".\" \".\" \".\". \"\". \""}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We establish a data-dependent notion of algorithmic stability for Stochastic Gradient Descent (SGD), and employ it to develop novel generalization bounds. This is in contrast to previous distribution-free algorithmic stability results for SGD which depend on the worst-case constants. By virtue of the data-dependent argument, our bounds provide new insights into learning with SGD on convex and non-convex problems. In the convex case, we show that the bound on the generalization error is multiplicative in the risk at the initialization point. In the non-convex case, we prove that the expected curvature of the objective function around the initialization point has crucial influence on the generalization error. In both cases, our results suggest a simple data-driven strategy to stabilize SGD by pre-screening its initialization. As a corollary, our results allow us to show optimistic generalization bounds that exhibit fast convergence rates for SGD subject to a vanishing empirical risk.", "creator": "LaTeX with hyperref package"}}}