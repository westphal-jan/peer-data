{"id": "1703.01694", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Word forms - not just their lengths- are optimized for efficient communication", "abstract": "The inverse relationship between the length of a word and the frequency of its use, first identified by G.K. Zipf in 1935, is a classic empirical law that holds across a wide range of human languages. We demonstrate that length is one aspect of a much more general property of words: how distinctive they are with respect to other words in a language. Distinctiveness plays a critical role in recognizing words in fluent speech, in that it reflects the strength of potential competitors when selecting the best candidate for an ambiguous signal. Phonological information content, a measure of a word's string probability under a statistical model of a language's sound or character sequences, concisely captures distinctiveness. Examining large-scale corpora from 13 languages, we find that distinctiveness significantly outperforms word length as a predictor of frequency. This finding provides evidence that listeners' processing constraints shape fine-grained aspects of word forms across languages.", "histories": [["v1", "Mon, 6 Mar 2017 00:38:51 GMT  (3231kb,D)", "https://arxiv.org/abs/1703.01694v1", "14 pages, 7 figures"], ["v2", "Wed, 31 May 2017 18:40:08 GMT  (3447kb,D)", "http://arxiv.org/abs/1703.01694v2", "16 pages, 8 figures"]], "COMMENTS": "14 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["stephan c meylan", "thomas l griffiths"], "accepted": false, "id": "1703.01694"}, "pdf": {"name": "1703.01694.pdf", "metadata": {"source": "CRF", "title": "Word forms\u2014not just their lengths\u2014are optimized for efficient communication", "authors": ["Stephan C. Meylan", "Thomas L. Griffiths"], "emails": [], "sections": [{"heading": null, "text": "Despite their apparent diversity, natural languages exhibit striking structural regularities [1-3]. How such regularities relate to human cognition remains an open question with implications for linguistics, psychology, and neuroscience [2, 4-6]. Notable among these regularities is the well-known relationship between word length and frequency: frequently used words tend to be short [7]. While the underlying cause lies in the debate, this relationship between the speakers, who minimize the overall articulation of words that are most commonly used, following the principle of least effect [8]. While the underlying cause lies in the debate, this relationship between word length and frequency remains one of the most robust statistical laws describing human language."}, {"heading": "Model", "text": "We define a probabilistic linguistic model to characterize the distinctive power of word forms in terms of their constituent sound-to-sound transitions. This model can be adapted to a particular language using a large written sample. The starting point of the model is the formalization of the listener's task as a rational statistical conclusion."}, {"heading": "Bayesian inference and distinctiveness", "text": "This can be formulated as a problem of Bayesian inference; the listener should calculate a posterior distribution P (s | s) over the words based on the sounds. Applying Bayes \"rule, this is of P (w | s) = P (s | w) the preceding probability of the word P (w) is the preceding probability of the word intended by the speaker, and P (s) is the probability of hearing. Suppose the sounds are faithfully produced so that P (sw | s) is close to 1 for a certain string sw for each W and close to 0, we get the approximation P (sw | sw). P (sw) P (sw) P (2), which expresses the probability that the word is correctly identified as a distinction."}, {"heading": "Length and distinctiveness", "text": "Intuitively, short words have more similar competitors and are therefore easier to confuse with other words, while long words have fewer neighbors (Fig. 1). Zipped formulation can make a signal that is too short ambiguous, and a listener is less likely to derive the intended meaning of a speaker [8]. We argue that it is not the length per se that drives this effect, but the contribution of length to distinguishability. Word length is an important determinant of string probability: under a probable treatment is a longer sequence of events, and therefore the same or lower probability. In fact, the length is strictly proportional to the probability (or protocol probability) of a string of words with a \"monkey-on-typewriter model\" is a longer sequence of events."}, {"heading": "Benefits of distinctiveness", "text": "The conception of words in terms of their distinctiveness has several advantages over length. First, distinctiveness (measured by PIC) is a much finer-grained measure of the complexity of word forms, and in many cases generates predictions that run counter to word length. A shorter word may contain a relatively low probability of phonemes (e.g. depth, 4 phonemes / 5 letters, PIC = 17.86 bits under a type-weighted model from the English corpus of Google Books 2012), while a longer word may contain a higher probability, less informative sequence (e.g. bottom, 5 phonemes / 6 letters, PIC = 12.85 bits under the same model). Second, PIC is closely related to metrics of lexical neighborhood density used in psycholinguistic models of spoken word recognition. Neighborhood density reflects how many words have a similar shape to a given word; while suggestions, such as the best word density, reflect the most intuitive word density."}, {"heading": "A relationship between distinctiveness and frequency", "text": "We offer two listener-centric explanations for why there might be an inverse relationship between distinctness and frequency. First, distinctness provides a way to measure the effort that listeners expend on understanding speakers; the overall understanding effort is minimized when the most common words are least distinguishable. Zipf's original explanation for the inverse relationship is between the frequency of a word and its length based on the idea that languages are shaped by the speaker's desire to expend the least effort on generating words. If longer strings are more elaborate to produce, it makes sense that they be associated with less frequent words. However, we can imagine a similar argument being used on the listener's side: that languages are shaped by the listener's desire to minimize the effort they spend to understand speakers."}, {"heading": "Results", "text": "We study the correlation between distinctiveness and frequency in large corpus samples (43m to 266b words) in 13 languages across three large datasets. In each case, we calculate the frequency and information content in context (mean trigram surprise probability or negative mean log probability as part of a trigram model) for each word and measure its PIC using an N-phoneme and an N-sign model estimated from different words in the language (see Methods for more details). For OPUS datasets, we also calculate the PIC among analog token-weighted models. If the length represents a reduced resolution of distinctiveness, we expect an even stronger relationship between distinctiveness - measured by the PIC - and frequency between languages."}, {"heading": "Cross-Linguistic Results", "text": "Following the methodology used in [19], we examine the correlation between word-level predictors (frequency and information contained in context) and the metric of structural form (word length or PIC) for the 25,000 most common types in each language. Contrary to [19], we limit our analysis to indicative types, excluding person names, place names, and acronyms from the analysis. We obtain a systematically stronger negative correlation between frequency and distinguishability, measured by the type-weighted model PIC, as frequency and word length (Fig. 3). Even if we keep word length constant, the distinctness explains significant additional differences in word frequency (Fig. 2). The structure of the model of phonemic transcriptions maintains this pattern in 11 of 11 languages in the Google 1T datasets, 6 of 7 languages in Google Books 2012, and all 13 languages in the OPUS corpus."}, {"heading": "Relationship to Preceding Context", "text": "While Piantadosi et al. [19] found that, taking into account contextual predictability / information content (in the form of an average surprisal trigram in a dataset), word length is predicted better than by simple frequency measurement (operationalized as a surprisal unigram or negative log probability), we find a qualitatively different result pattern for PIC. In the above analysis, we find that the correlation between frequency and word length is higher in all but one case than the correlation between mean surprisal trigram and PIC (fig. 5, A). The correlation between frequency (negative logigram probability) and PIC is also greater than the correlation between mean surprisal and word length in all but one case (fig. 5, B). We find in Piantadosi et al. [19] a significantly weakened support for the main claim in Piantadosi et al. [19] We find that word length is unsurprising in all but one case (fig. 5, B)."}, {"heading": "Discussion", "text": "In fact, it is such that most of them are able to move into another world, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to stay in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live, in which they live in which they live in which they live, in which they live, in which they are able to"}, {"heading": "Conclusion", "text": "The canonical inverse relationship between word length and frequency is a special case in an even broader relationship between word characteristics and frequency. Distinction plays a crucial role in word recognition, as it captures the strength of competing audiences for a speech signal. Rational analysis shows limitations to word forms: speakers can simplify and shorten words, but they are constrained by the listener's need for distinctive word forms for successful recognition."}, {"heading": "Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Datasets for Frequency and Surprisal Estimates", "text": "The Google Web 1T datasets have been downloaded from the Linguistic Data Consortium [35, 36]; the Google Books 2012 datasets have been downloaded from storage.googleapis.com / books / ngrams / books / datasetsv2.html [37] and OPUS (2013) from opensubtitles.org [38]. All pure-word n-grams have been discarded, and the punctuation that appears with other text, with the exception of apostrophes, has been removed. To simplify matters, we assume that the tokenized orthographic forms correspond to psychologically significant words in the speaker's lexicon; while this assumption does not apply to all forms (e.g. English compound nouns), it applies to the vast majority of the word forms in the analysis (see also our analysis of morphologically simple forms in the discussion, which also addresses this question)."}, {"heading": "Estimating Sentential Information Content", "text": "Following Piantadosi et al. (2013), we analyze a word list constructed from the 25,000 most common words in each dataset. Tokenfrequencies were calculated from the release of the OPUS subtitle corpus in 2013. For each language, the list of unique types was filtered based on the words detected by the UNIX Aspell utility for the relevant locale. After filtering, we calculated the negative logigram probability (proportional to the log normalization frequency) for each word w together with the negative meaning trigram probability across contexts according to [19], \u2212 1N \u2211 Ni = 1 logP (W = w | C = ci), where ci is the context for the ith occurrence of w and N is the frequency of w in the dataset."}, {"heading": "Estimating Phonological Information Content", "text": "For the type-weighted models (OPUS only), a five-character transition model with all in-dictionary characters in the corresponding OPUS subtitle corpus was estimated. In both cases, we also produced a five-character transition model for all languages except Hebrew with IPA transcriptions from an automatic speech synthesizer, eSpeak. IPA representations for words explain language-specific variations in orthographic conventions. For example, written Spanish only contains accents when the placement of prosodic words cannot be derived from more general rules in the language."}, {"heading": "Code availability", "text": "Our library for quick cleaning, manipulating, summarizing and retrieving n-gram data is available at github.com / smeylan / ngrawk. Jupyter notebooks for the analyses presented here are available at github.com / smeylan / pic-analysis."}, {"heading": "Acknowledgements", "text": "This material is based on work supported by the US National Science Foundation Graduate Research Fellowship under grant number DGE-1106400 and NSF grant number SMA-1228541. Special thanks go to Steven Piantadosi for exchanging materials and lexical content estimates, helpful comments on early drafts by Terry Regier, and members of the Computational Cognitive Science Lab at UC Berkeley for valuable discussion."}], "references": [{"title": "Some universals of grammar with particular reference to the order of meaningful elements", "author": ["JH Greenberg"], "venue": "JH Greenberg, editor, Universals of Human Language, pages 73\u2013113. MIT Press, Cambridge, MA", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1963}, {"title": "The myth of language universals: language diversity and its importance for cognitive science", "author": ["N Evans", "SC Levinson"], "venue": "Behav Brain Sci, 32(5):429\u2013448", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Large-scale evidence of dependency length minimization in 37 languages", "author": ["R Futrell", "K Mahowald", "E Gibson"], "venue": "Proc. Natl. Acad. Sci. U.S.A., 112(33):10336\u201310341", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "The faculty of language: what is it", "author": ["MD Hauser", "N Chomsky", "WT Fitch"], "venue": "who has it, and how did it evolve? Science, 298(5598):1569\u20131579", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Kinship categories across languages reflect general communicative principles", "author": ["C Kemp", "T Regier"], "venue": "Science, 336(6084): 1049\u20131054", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Language learners restructure their input to facilitate efficient communication", "author": ["M Fedzechkina", "TF Jaeger", "EL Newport"], "venue": "Proc. Natl. Acad. Sci. U.S.A., 109(44):17897\u201317902", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "The Psychobiology of Language", "author": ["GK Zipf"], "venue": "Houghton-Mifflin", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1935}, {"title": "Human Behaviour and the Principle of Least-Effort", "author": ["G Zipf"], "venue": "Addison-Wesley, Cambridge, MA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1949}, {"title": "The Statistical Study of Literary Vocabulary", "author": ["G.U. Yule"], "venue": "Cambridge University Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1944}, {"title": "Some effects of intermittent silence", "author": ["GA Miller"], "venue": "American Journal of Psychology, 70:311\u2013314", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1957}, {"title": "Least effort and the origins of scaling in human language", "author": ["Ramon Ferrer i Cancho", "Ricard V. Sol"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Power laws for monkeys typing randomly: the case of unequal probabilities", "author": ["B. Conrad", "M. Mitzenmacher"], "venue": "IEEE Transactions on Information Theory, 50(7):1403\u20131414", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Zipf\u2019s word frequency law in natural language: A critical review and future directions", "author": ["S.T. Piantadosi"], "venue": "Psychonomic Bulletin & Review, 21(5):1112\u20131130", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Shortlist B: a Bayesian model of continuous speech recognition", "author": ["D Norris", "JM McQueen"], "venue": "Psychol Rev,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Probability and surprisal in auditory comprehension of morphologically complex", "author": ["LW Balling", "RH Baayen"], "venue": "words. Cognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "A Performance Theory of Order and Constituency", "author": ["JA Hawkins"], "venue": "Cambridge University Press, Cambrdige, UK", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "Using information content to predict phone deletion", "author": ["U Cohen Priva"], "venue": "Proceedings of the 27th West Coast Conference on Formal Linguistics, pages 90\u201398, Somerville, MA", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Expectation-based syntactic comprehension", "author": ["R Levy"], "venue": "Cognition, 106(3):1126\u20131177", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Word lengths are optimized for efficient communication", "author": ["ST Piantadosi", "H Tily", "E Gibson"], "venue": "Proc. Natl. Acad. Sci. U.S.A., 108(9):3526\u20139", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "The effect of word predictability on reading time is logarithmic", "author": ["NJ Smith", "R Levy"], "venue": "Cognition, 128(3):302\u2013319", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["CD Manning", "H Sch\u00fctze"], "venue": "MIT Press, Cambridge, MA, USA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "CLEARPOND: cross-linguistic easy-access resource for phonological and orthographic neighborhood densities", "author": ["V. Marian", "J. Bartolotti", "S. Chabal", "A. Shook"], "venue": "PLoS ONE, 7(8):e43230", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Simple games of strategy occurring in communication through natural languages", "author": ["B Mandelbrot"], "venue": "Transaction of the IRE Professional Group on Information Theory PGIT, 3(3):124\u2013137", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1954}, {"title": "Probabilistic phonotactics and neighborhood activation in spoken word recognition", "author": ["MS Vitevitch", "PA Luce"], "venue": "J Mem Lang, 40(3):374\u2013408", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "Phonotactics", "author": ["PA Luce", "NR Large"], "venue": "density, and entropy in spoken word recognition. Lang Cognitive Proc, 16(5-6): 565\u2013581", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Processing interactions and lexical access during word recognition in continuous speech", "author": ["WD Marslen-Wilson", "A Welsh"], "venue": "Cognitive Psychology, 10(1):29 \u2013 63", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1978}, {"title": "Functional parallelism in spoken word-recognition", "author": ["WD Marslen-Wilson"], "venue": "Cognition, 25(1-2):71\u2013102", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1987}, {"title": "The locus of the effects of sentential-semantic context in spoken-word processing", "author": ["P Zwitserlood"], "venue": "Cognition, 32(1): 25\u201364", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1989}, {"title": "Eye movements as a window into real-time spoken language comprehension in natural contexts", "author": ["KM Eberhard", "MJ Spivey-Knowlton", "JC Sedivy", "MK Tanenhaus"], "venue": "J Psycholinguist Res, 24(6):409\u2013436", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1995}, {"title": "Recognizing spoken words: The neighborhood activation model", "author": ["PA Luce", "DB Pisoni"], "venue": "Ear Hear, 19(1):1\u201336", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}, {"title": "Access to the internal lexicon", "author": ["M Coltheart", "E Davelaar", "JT Jonasson", "D Besner"], "venue": "S Dornic, editor, Attention and Performance VI, pages 535\u2013555. Lawrence Erlbaum Associates", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1977}, {"title": "Entropy rate constancy in text", "author": ["D Genzel", "E Charniak"], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 199\u2013206", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "The smooth signal redundancy hypothesis: a functional explanation for relationships between redundancy", "author": ["M Aylett", "A Turk"], "venue": "prosodic prominence, and duration in spontaneous speech. Lang Speech, 47(1):31\u201356", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "Speakers optimize information density through syntactic reduction", "author": ["R Levy", "TF Jaeger"], "venue": "B Sch\u00f6lkopf, J Platt, and T Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 849\u2013856, Cambridge, MA", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Web 1T 5-gram", "author": ["T Brants", "A Franz"], "venue": "10 European Languages Version 1 LDC2009T25", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Web 1T 5-gram Version 1 LDC2006T13", "author": ["T Brants", "A Franz"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2006}, {"title": "Quantitative analysis of culture using millions of digitized", "author": ["J Michel"], "venue": "books. Science,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}, {"title": "Parallel data", "author": ["J Tiedemann"], "venue": "tools and interfaces in OPUS. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC\u201912)", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Language redundancy predicts syllabic duration and the spectral characteristics of vocalic syllable nuclei", "author": ["M Aylett", "A Turk"], "venue": "The Journal of the Acoustical Society of America, 119(5):3048\u20133058", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Predictability effects on durations of content and function words in conversational english", "author": ["A Bell", "JM Brenier", "M Gregory", "C Girand", "D Jurafsky"], "venue": "Journal of Memory and Language, 60(1):92\u2013111", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Why reduce? phonological neighborhood density and phonetic reduction in spontaneous speech", "author": ["S Gahl", "Y Yao", "K Johnson"], "venue": "Journal of Memory and Language, 66(4):789\u2013806", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "Info/information theory: speakers choose shorter words in predictive contexts", "author": ["K. Mahowald", "E. Fedorenko", "S.T. Piantadosi", "E. Gibson"], "venue": "Cognition,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Quantifying the functional load of phonemic oppositions", "author": ["D Surendran", "P Niyogi"], "venue": "distinctive features, and suprasegmentals. Amsterdam Studies in the Theory and History of Linguistic Science Series 4, 279:43", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "High functional load inhibits phonological contrast loss: A corpus study", "author": ["A Wedel", "A Kaplan", "S Jackson"], "venue": "Cognition, 128(2):179\u2013186", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "The CELEX lexical database", "author": ["HR Baayen", "R Piepenbrock", "L Gulikers"], "venue": "Release 2 (CD-ROM)", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1995}, {"title": "Making long-distance relationships work: Quantifying lexical competition with hidden markov models", "author": ["J Strand", "D Liben-Nowell"], "venue": "Journal of Memory and Language, 90:88 \u2013 102", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Syntactic annotations for the google books ngram corpus", "author": ["Yuri Lin", "Jean-Baptiste Michel", "Erez Lieberman Aiden", "Jon Orwant", "Will Brockman", "Slav Petrov"], "venue": "In Proceedings of the ACL 2012 system demonstrations,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2012}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["SF Chen", "J Goodman"], "venue": "Comput Speech & Lang, 13(4):359\u2013393", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1999}, {"title": "Proceedings of icslp", "author": ["A Stolcke"], "venue": "volume 2, pages 901\u2013904", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Despite their apparent diversity, natural languages display striking structural regularities [1\u20133].", "startOffset": 93, "endOffset": 98}, {"referenceID": 1, "context": "Despite their apparent diversity, natural languages display striking structural regularities [1\u20133].", "startOffset": 93, "endOffset": 98}, {"referenceID": 2, "context": "Despite their apparent diversity, natural languages display striking structural regularities [1\u20133].", "startOffset": 93, "endOffset": 98}, {"referenceID": 1, "context": "How such regularities relate to human cognition remains an open question with implications for linguistics, psychology, and neuroscience [2, 4\u20136].", "startOffset": 137, "endOffset": 145}, {"referenceID": 3, "context": "How such regularities relate to human cognition remains an open question with implications for linguistics, psychology, and neuroscience [2, 4\u20136].", "startOffset": 137, "endOffset": 145}, {"referenceID": 4, "context": "How such regularities relate to human cognition remains an open question with implications for linguistics, psychology, and neuroscience [2, 4\u20136].", "startOffset": 137, "endOffset": 145}, {"referenceID": 5, "context": "How such regularities relate to human cognition remains an open question with implications for linguistics, psychology, and neuroscience [2, 4\u20136].", "startOffset": 137, "endOffset": 145}, {"referenceID": 6, "context": "Prominent among these regularities is the wellknown relationship between word length and frequency: across languages, frequently-used words tend to be short [7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 6, "context": "In a classic work, Zipf [7] posited that this pattern emerges from speakers minimizing total articulatory effort by using the shortest form for words that are used most often, following what he later called the Principle of Least Effort [8].", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "In a classic work, Zipf [7] posited that this pattern emerges from speakers minimizing total articulatory effort by using the shortest form for words that are used most often, following what he later called the Principle of Least Effort [8].", "startOffset": 237, "endOffset": 240}, {"referenceID": 8, "context": "While the underlying cause has been the subject of debate [9\u201313], this relationship between word length and frequency remains one of the most robust statistical laws that describe human languages.", "startOffset": 58, "endOffset": 64}, {"referenceID": 9, "context": "While the underlying cause has been the subject of debate [9\u201313], this relationship between word length and frequency remains one of the most robust statistical laws that describe human languages.", "startOffset": 58, "endOffset": 64}, {"referenceID": 10, "context": "While the underlying cause has been the subject of debate [9\u201313], this relationship between word length and frequency remains one of the most robust statistical laws that describe human languages.", "startOffset": 58, "endOffset": 64}, {"referenceID": 11, "context": "While the underlying cause has been the subject of debate [9\u201313], this relationship between word length and frequency remains one of the most robust statistical laws that describe human languages.", "startOffset": 58, "endOffset": 64}, {"referenceID": 12, "context": "While the underlying cause has been the subject of debate [9\u201313], this relationship between word length and frequency remains one of the most robust statistical laws that describe human languages.", "startOffset": 58, "endOffset": 64}, {"referenceID": 13, "context": "If word recognition is modeled as Bayesian inference [14, 15], the probability of successful recognition depends on both the prior probability of the intended word and on the number and strength of alternative words (\u201ccompetitors\u201d).", "startOffset": 53, "endOffset": 61}, {"referenceID": 14, "context": "If word recognition is modeled as Bayesian inference [14, 15], the probability of successful recognition depends on both the prior probability of the intended word and on the number and strength of alternative words (\u201ccompetitors\u201d).", "startOffset": 53, "endOffset": 61}, {"referenceID": 2, "context": "This relationship between frequency and distinctiveness adds to a growing body of evidence that cognitive constraints influence the structural properties of natural languages [3, 6, 16].", "startOffset": 175, "endOffset": 185}, {"referenceID": 5, "context": "This relationship between frequency and distinctiveness adds to a growing body of evidence that cognitive constraints influence the structural properties of natural languages [3, 6, 16].", "startOffset": 175, "endOffset": 185}, {"referenceID": 15, "context": "This relationship between frequency and distinctiveness adds to a growing body of evidence that cognitive constraints influence the structural properties of natural languages [3, 6, 16].", "startOffset": 175, "endOffset": 185}, {"referenceID": 16, "context": "Following this logic, we use the phonological information content (PIC) of a word to measure the distinctiveness of its phoneme-to-phoneme transitions [17], or its approximation in character-tocharacter transitions.", "startOffset": 151, "endOffset": 155}, {"referenceID": 17, "context": "Analogous to the metric of lexical surprisal (negative log conditional probability) used to measure the predictability of a word given preceding words [18\u201320], PIC is the negative log probability of the sequence of phonemes (distinct meaningful sounds) or letters in the string comprising a word.", "startOffset": 151, "endOffset": 158}, {"referenceID": 18, "context": "Analogous to the metric of lexical surprisal (negative log conditional probability) used to measure the predictability of a word given preceding words [18\u201320], PIC is the negative log probability of the sequence of phonemes (distinct meaningful sounds) or letters in the string comprising a word.", "startOffset": 151, "endOffset": 158}, {"referenceID": 19, "context": "Analogous to the metric of lexical surprisal (negative log conditional probability) used to measure the predictability of a word given preceding words [18\u201320], PIC is the negative log probability of the sequence of phonemes (distinct meaningful sounds) or letters in the string comprising a word.", "startOffset": 151, "endOffset": 158}, {"referenceID": 20, "context": "To obtain a compact representation of the probabilities of various phone or character sequences in a language we estimate an n-phone or n-character model (analogous to an n-gram model over words [21] but computed over individual phonemes or characters) from a corpus sample.", "startOffset": 195, "endOffset": 199}, {"referenceID": 21, "context": "Data from 27,751 words in the English Clearpond dataset [22].", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "Per Zipf\u2019s formulation, a signal which is too short may be ambiguous, and a listener is less likely to infer a speaker\u2019s intended meaning [8].", "startOffset": 138, "endOffset": 141}, {"referenceID": 22, "context": "In fact, length is strictly proportional to the probability (or log probability) of a string under a \u201cmonkeys-on-typewriters\u201d model of letter-to-letter transitions such as that proposed by Mandelbrot [23] and further explored by Miller [10].", "startOffset": 200, "endOffset": 204}, {"referenceID": 9, "context": "In fact, length is strictly proportional to the probability (or log probability) of a string under a \u201cmonkeys-on-typewriters\u201d model of letter-to-letter transitions such as that proposed by Mandelbrot [23] and further explored by Miller [10].", "startOffset": 236, "endOffset": 240}, {"referenceID": 23, "context": "People have rich knowledge of the relative prominence of these sequences in their respective languages\u2014just as they have rich knowledge of inter-word statistical dependencies\u2014 and can call upon this knowledge in spoken word recognition [24, 25].", "startOffset": 236, "endOffset": 244}, {"referenceID": 24, "context": "People have rich knowledge of the relative prominence of these sequences in their respective languages\u2014just as they have rich knowledge of inter-word statistical dependencies\u2014 and can call upon this knowledge in spoken word recognition [24, 25].", "startOffset": 236, "endOffset": 244}, {"referenceID": 25, "context": "Ample psycholinguistic evidence also suggests that people are capable of using sub-word information, for example using sounds from the beginning of a word to predict possible continuations [26\u201329].", "startOffset": 189, "endOffset": 196}, {"referenceID": 26, "context": "Ample psycholinguistic evidence also suggests that people are capable of using sub-word information, for example using sounds from the beginning of a word to predict possible continuations [26\u201329].", "startOffset": 189, "endOffset": 196}, {"referenceID": 27, "context": "Ample psycholinguistic evidence also suggests that people are capable of using sub-word information, for example using sounds from the beginning of a word to predict possible continuations [26\u201329].", "startOffset": 189, "endOffset": 196}, {"referenceID": 28, "context": "Ample psycholinguistic evidence also suggests that people are capable of using sub-word information, for example using sounds from the beginning of a word to predict possible continuations [26\u201329].", "startOffset": 189, "endOffset": 196}, {"referenceID": 29, "context": "PIC is formally very similar to frequency-weighted neighborhood density [30], however it measures the number of competitors at each successive phone\u2014a feature consistent with empirical results suggesting incremental phoneme-by-phoneme processing in some cases [29].", "startOffset": 72, "endOffset": 76}, {"referenceID": 28, "context": "PIC is formally very similar to frequency-weighted neighborhood density [30], however it measures the number of competitors at each successive phone\u2014a feature consistent with empirical results suggesting incremental phoneme-by-phoneme processing in some cases [29].", "startOffset": 260, "endOffset": 264}, {"referenceID": 30, "context": "PIC thus constitutes a more detailed measure of neighborhood density than the canonical measure of Coltheart\u2019s N [31], the number of words within a edit distance of one of the target word.", "startOffset": 113, "endOffset": 117}, {"referenceID": 18, "context": "Recent work in computational psycholinguistics has discovered that a variety of syntactic phenomena can be predicted from the Uniform Information Density hypothesis: that languages are structured such that each word in an utterance provides approximately the same amount of information [19, 32\u201334].", "startOffset": 286, "endOffset": 297}, {"referenceID": 31, "context": "Recent work in computational psycholinguistics has discovered that a variety of syntactic phenomena can be predicted from the Uniform Information Density hypothesis: that languages are structured such that each word in an utterance provides approximately the same amount of information [19, 32\u201334].", "startOffset": 286, "endOffset": 297}, {"referenceID": 32, "context": "Recent work in computational psycholinguistics has discovered that a variety of syntactic phenomena can be predicted from the Uniform Information Density hypothesis: that languages are structured such that each word in an utterance provides approximately the same amount of information [19, 32\u201334].", "startOffset": 286, "endOffset": 297}, {"referenceID": 33, "context": "Recent work in computational psycholinguistics has discovered that a variety of syntactic phenomena can be predicted from the Uniform Information Density hypothesis: that languages are structured such that each word in an utterance provides approximately the same amount of information [19, 32\u201334].", "startOffset": 286, "endOffset": 297}, {"referenceID": 34, "context": "To evaluate this hypothesis we evaluate the correlation between frequency and PIC across large corpus samples of 13 languages from three large-scale datasets: web scrapes from the Google 1T corpora [35, 36], scanned books from the Google Books 2012 corpus [37], and subtitles from the 2013 OPUS corpus [38].", "startOffset": 198, "endOffset": 206}, {"referenceID": 35, "context": "To evaluate this hypothesis we evaluate the correlation between frequency and PIC across large corpus samples of 13 languages from three large-scale datasets: web scrapes from the Google 1T corpora [35, 36], scanned books from the Google Books 2012 corpus [37], and subtitles from the 2013 OPUS corpus [38].", "startOffset": 198, "endOffset": 206}, {"referenceID": 36, "context": "To evaluate this hypothesis we evaluate the correlation between frequency and PIC across large corpus samples of 13 languages from three large-scale datasets: web scrapes from the Google 1T corpora [35, 36], scanned books from the Google Books 2012 corpus [37], and subtitles from the 2013 OPUS corpus [38].", "startOffset": 256, "endOffset": 260}, {"referenceID": 37, "context": "To evaluate this hypothesis we evaluate the correlation between frequency and PIC across large corpus samples of 13 languages from three large-scale datasets: web scrapes from the Google 1T corpora [35, 36], scanned books from the Google Books 2012 corpus [37], and subtitles from the 2013 OPUS corpus [38].", "startOffset": 302, "endOffset": 306}, {"referenceID": 18, "context": "[19], we compute correlations over the 25,000 most frequent types in each language; following that same study we additionally compute the correlation between PIC and average incontext information content (trigram surprisal) over the same set of words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Cross-Linguistic Results Following the methodology adopted in [19], we examine the correlation between word-level predictors (frequency and in-context information content) and metric of structural form (word length or PIC) for the 25,000 most frequent types in each language.", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "Unlike [19], we limit our analysis to indictionary types, thereby excluding person names, place names, and acronyms from the analysis.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "The obtained correlations are even stronger than those recently obtained between word length and average in-context information content [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 18, "context": "[19] found that taking into account contextual predictability / information content (in the form of mean trigram surprisal in a dataset) better predicts word length than using a", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19], in that we find unigram frequencies better predict word length than does mean trigram surprisal (Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "Previous research has demonstrated that speakers may \u201creduce\u201d\u2014underarticulate, shorten, weaken, or omit altogether\u2014frequent or highly predictable words in specific contexts, for example using \u201cprobly\u201d in place of \u201cprobably\u201d in fluent speech [39\u201341].", "startOffset": 241, "endOffset": 248}, {"referenceID": 39, "context": "Previous research has demonstrated that speakers may \u201creduce\u201d\u2014underarticulate, shorten, weaken, or omit altogether\u2014frequent or highly predictable words in specific contexts, for example using \u201cprobly\u201d in place of \u201cprobably\u201d in fluent speech [39\u201341].", "startOffset": 241, "endOffset": 248}, {"referenceID": 40, "context": "Previous research has demonstrated that speakers may \u201creduce\u201d\u2014underarticulate, shorten, weaken, or omit altogether\u2014frequent or highly predictable words in specific contexts, for example using \u201cprobly\u201d in place of \u201cprobably\u201d in fluent speech [39\u201341].", "startOffset": 241, "endOffset": 248}, {"referenceID": 41, "context": "[43] for additional examples).", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "phonemic mergers)\u2014can also be characterized in terms of changes in PIC, similar to entropy-based estimates of functional load [44, 45].", "startOffset": 126, "endOffset": 134}, {"referenceID": 43, "context": "phonemic mergers)\u2014can also be characterized in terms of changes in PIC, similar to entropy-based estimates of functional load [44, 45].", "startOffset": 126, "endOffset": 134}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] and word form distinctiveness both implicate listener-oriented pressures in the relationship between frequency and word form.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19], we classify each word into one of the three categories: those found in the relevant dictionary (by testing for membership in the corresponding GNU Aspell dictionary for each language), those found in English (by testing for membership in the English Aspell dictionary for words in non-English languages), and label the remainder as out-of-dictionary.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] (Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Because [19] represented word forms as closest ASCII equivalents (e.", "startOffset": 8, "endOffset": 12}, {"referenceID": 18, "context": "[19] found higher global correlations between word length and in-context predictability (information content) as measured by mean trigram surprisal (blue bars in panel 1) than between word length and frequency (unigram surprisal), red bars in panel 1).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19]: English and French in the Google 1T and English and German in Google Books 2012 show a stronger correlation of mean trigram surprisal and word length (Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "For now, we note that we obtain the same pattern of results for PIC and frequency when we restrict our analysis to morphologically simple word forms for those languages with morphological annotation in the CELEX database [46] (English, German, and Dutch).", "startOffset": 221, "endOffset": 225}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "With respect to the second simplification, listeners do not have access to the true identity of preceding phonemes, and instead likely marginalize over a distribution of preceding as well as following sounds within a word in the process of word recognition [48].", "startOffset": 257, "endOffset": 261}, {"referenceID": 46, "context": "[49]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Methods Datasets for Frequency and Surprisal Estimates The Google Web 1T datasets were downloaded from the Linguistic Data Consortium [35, 36]; the Google Books 2012 datasets were downloaded from storage.", "startOffset": 134, "endOffset": 142}, {"referenceID": 35, "context": "Methods Datasets for Frequency and Surprisal Estimates The Google Web 1T datasets were downloaded from the Linguistic Data Consortium [35, 36]; the Google Books 2012 datasets were downloaded from storage.", "startOffset": 134, "endOffset": 142}, {"referenceID": 36, "context": "html [37], and OPUS (2013) from opensubtitles.", "startOffset": 5, "endOffset": 9}, {"referenceID": 37, "context": "org [38].", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "log trigram probability across contexts, following [19], \u2212 1 N \u2211N i=1 logP (W = w|C = ci), where ci is the context for the ith occurrence of w and N is the frequency of w in the dataset.", "startOffset": 51, "endOffset": 55}, {"referenceID": 47, "context": "To avoid overfitting among higher order sequences, phone and character transition probabilities for the type-weighted models were computed with modified Kneser-Ney smoothing [51] with interpolation on orders 3, 4, and 5 using the SRILM toolkit [52].", "startOffset": 174, "endOffset": 178}, {"referenceID": 48, "context": "To avoid overfitting among higher order sequences, phone and character transition probabilities for the type-weighted models were computed with modified Kneser-Ney smoothing [51] with interpolation on orders 3, 4, and 5 using the SRILM toolkit [52].", "startOffset": 244, "endOffset": 248}], "year": 2017, "abstractText": "The inverse relationship between the length of a word and the frequency of its use, first identified by G.K. Zipf in 1935, is a classic empirical law that holds across a wide range of human languages. We demonstrate that length is one aspect of a much more general property of words: how distinctive they are with respect to other words in a language. Distinctiveness plays a critical role in recognizing words in fluent speech, in that it reflects the strength of potential competitors when selecting the best candidate for an ambiguous signal. Phonological information content, a measure of a word\u2019s string probability under a statistical model of a language\u2019s sound or character sequences, concisely captures distinctiveness. Examining largescale corpora from 13 languages, we find that distinctiveness significantly outperforms word length as a predictor of frequency. This finding provides evidence that listeners\u2019 processing constraints shape fine-grained aspects of word forms across languages. Despite their apparent diversity, natural languages display striking structural regularities [1\u20133]. How such regularities relate to human cognition remains an open question with implications for linguistics, psychology, and neuroscience [2, 4\u20136]. Prominent among these regularities is the wellknown relationship between word length and frequency: across languages, frequently-used words tend to be short [7]. In a classic work, Zipf [7] posited that this pattern emerges from speakers minimizing total articulatory effort by using the shortest form for words that are used most often, following what he later called the Principle of Least Effort [8]. While the underlying cause has been the subject of debate [9\u201313], this relationship between word length and frequency remains one of the most robust statistical laws that describe human languages. Here, we propose a generalization of Zipf\u2019s analysis and present two possible listener-focused explanations. We show that a word\u2019s frequency is inversely related to its distinctiveness\u2014how easily it can be identified as the intended message for a given speech signal. While speakers prefer easier-to-produce less distinctive forms, they are constrained by listeners\u2019 need for sufficiently distinctive forms to differentiate each word from others, especially if a speaker\u2019s intended word has higher-frequency competitors. If word recognition is modeled as Bayesian inference [14, 15], the probability of successful recognition depends on both the prior probability of the intended word and on the number and strength of alternative words (\u201ccompetitors\u201d). We define a statistical measure of distinctiveness that succinctly captures the diagnosticity of a word form by assessing the aggregate strength of competitors in the language. We then show that distinctiveness should be inversely related to frequency, if languages are constructed to equalize error rates for low and high frequency words. Importantly, distinctiveness subsumes Zipf\u2019s observation regarding the relationship of length and frequency as a special case. Length is a n\u00e4\u0131ve approximation of the distinctiveness of a word form insofar as longer strings are simply less probable. We demonstrate that a more comprehensive measure of distinctiveness that takes into account the sound-to-sound (phoneme-to-phoneme or letter-to-letter) sequences in a language accounts for significantly more frequency-related variance 1 ar X iv :1 70 3. 01 69 4v 2 [ cs .C L ] 3 1 M ay 2 01 7 than does length across a broad sample of natural languages. This relationship between frequency and distinctiveness adds to a growing body of evidence that cognitive constraints influence the structural properties of natural languages [3, 6, 16]. Model We define a probabilistic language model to characterize the distinctiveness of word forms in terms of their constituent sound-to-sound transitions. This model can be fit using a large written sample of a particular language. The starting point for the model is formalizing the task of the listener as a rational statistical inference. Bayesian inference and distinctiveness Upon hearing a string of sounds s, a listener has to infer what word w was intended by the speaker. This can be formulated as a problem of Bayesian inference. The listener should calculate a posterior distribution P (w|s) over words based on the sounds. Applying Bayes\u2019 rule, this is given by P (w|s) = P (s|w)P (w) P (s) (1) where P (s|w) is the probability of hearing s if w is the intended word, P (w) is the prior probability of the word w intended by the speaker, and P (s) is the probability of hearing s. Assuming that sounds are produced faithfully, such that P (sw|w) is close to 1 for a particular string sw for each w and close to 0 otherwise, we obtain the approximation P (w|sw) \u2248 P (w) P (sw) (2) which expresses the probability that the word w is correctly identified as a function of its normalized frequency, P (w), and the probability of the string sw in the language, P (sw). We define the distinctiveness of a word to be inversely related to P (sw): intuitively, a word that shares the same sound sequences with many other words is necessarily less distinctive. Following this logic, we use the phonological information content (PIC) of a word to measure the distinctiveness of its phoneme-to-phoneme transitions [17], or its approximation in character-tocharacter transitions. Analogous to the metric of lexical surprisal (negative log conditional probability) used to measure the predictability of a word given preceding words [18\u201320], PIC is the negative log probability of the sequence of phonemes (distinct meaningful sounds) or letters in the string comprising a word. To obtain a compact representation of the probabilities of various phone or character sequences in a language we estimate an n-phone or n-character model (analogous to an n-gram model over words [21] but computed over individual phonemes or characters) from a corpus sample. To support a stronger test of the relationship between distinctiveness and frequency, we avoid the circularity that more probable words necessarily contain more probable sequences by estimating the transition probabilities using the type inventory (unique words) in the language. For the smaller datasets (those from the OPUS corpus) we also present the results for a model with token-weighted phonological transitions, though we note that interpreting the correlations obtained from the weighted model is challenging given this circularity. Under this model, the phonological information content PIC(w) of a word is defined as: PIC(w) = \u2212 logP (sw) (3) = \u2212 logP (l1, . . . , l|sw|) for l \u2208 sw (4)", "creator": "LaTeX with hyperref package"}}}