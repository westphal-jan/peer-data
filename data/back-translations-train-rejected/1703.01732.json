{"id": "1703.01732", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning", "abstract": "Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as $\\epsilon$-greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent's surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the $k$-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.", "histories": [["v1", "Mon, 6 Mar 2017 05:51:42 GMT  (673kb,D)", "http://arxiv.org/abs/1703.01732v1", "Appeared in Deep RL Workshop at NIPS 2016"]], "COMMENTS": "Appeared in Deep RL Workshop at NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["joshua achiam", "shankar sastry"], "accepted": false, "id": "1703.01732"}, "pdf": {"name": "1703.01732.pdf", "metadata": {"source": "CRF", "title": "SURPRISE-BASED INTRINSIC MOTIVATION FOR DEEP REINFORCEMENT LEARNING", "authors": ["Joshua Achiam"], "emails": ["jachiam@berkeley.edu,", "sastry@coe.berkeley.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2 PRELIMINARIES", "text": "A Markov Decision Process (MDP) is a tuple (S, A, R, P, \u00b5), where S is the set of states, A is the set of actions, R: S \u00b7 A \u00b7 S \u2192 R is the reward function, P: S \u00b7 A \u00b7 S \u2192 [0, 1] is the transition probability function (where P (s \u2032 | s, a) is the probability of transition to the state s, since the previous state was s and the actor acted a in s), and \u00b5: S \u2192 [0, 1] is the initial state distribution. A policy \u03c0: S \u00b7 A \u2192 [0, 1] is a distribution over actions per state, where \u03c0 (a | s) is the probability of selecting a state. We aim to choose a policy that maximizes a performance measure, L (\u03c0), which normally takes the form of the expected finite horizon total return (sum of rewards over a fixed period), or a policy that dissipates the entire horizon."}, {"heading": "3 SURPRISE INCENTIVES", "text": "To train an agent with surprise-based exploration, we switch between creating an update step to a dynamic model (an approximation of the transition probability function of the MDP) and creating a policy update step that maximizes a trade-off between political performance and a surprise measurement; the dynamic model step makes progress in terms of reward; (the dynamic step makes progress in terms of the optimization problem (reward) in the environment; P\u03c6 is the model we learn; f is a regulatory function; and \u03b1 > 0 is a regulatory complication coefficient; the policy update step makes progress in terms of approximating the optimization problem. (1) The dynamics process is a dataset of transition factors from the environment; P\u03c6 is the model we are considering; f is a regulatory function; and \u03b1 > is an actualization step that makes progress in a trade-off coefficient."}, {"heading": "3.1 DISCUSSION", "text": "Ideally, we would like the intrinsic rewards to disappear in the boundary as P\u03c6 \u2192 P, in which case the agent should have sufficiently explored the state space and primarily learn from extrinsic rewards. However, this is not the case for the proposed intrinsic reward in (5), and it could lead to poor performance in that boundary. Thinking goes that if P\u03c6 = P is stimulated, the agent will be able to locate states with the most intrinsic transitions in P. However, we argue that this cannot be a problem because the intrinsic motivation usually appears useful long before the dynamic model is fully learned. As long as the agent is able to locate the extrinsic rewards before the intrinsic reward in P, the pathological behavior of noise-seeking in Pnamiht is not the case. On the other hand, the intrinsic reward in (8) should not suffer from this pathology, as the boundary is where the dynamic model should be based on surprise."}, {"heading": "3.2 IMPLEMENTATION DETAILS", "text": "Our implementation of usesL2 regularization in the dynamic model fit, and we impose an additional constraint to keep model iterations close to the KL divergence. Naming the average divergence asD-KL (P\u03c6) = 1 | D-KL (s, a) \"DDKL (P\u03c6) [s, a], (10) Update our dynamic model is\u03c6i + 1 = arg min \u03c6 \u2212 1 | D-D (s, a) + scale (s, a)\" mineralize \"mineral \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 scale \u2212 mineralize \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 mineral \u2212 min"}, {"heading": "4 EXPERIMENTS", "text": "We evaluate our proposed surprise incentives against a wide range of benchmarks that challenge na\u00efve exploration methods, including continuous control and discrete control tasks. Our continuous control tasks include the slate of sparse reward tasks performed by Houthooft et al. [7]: sparse MountainCar, sparse CartPoleSwingup and sparse HalfCheetah, as well as a new sparse reward task we present here: sparse Swimmer et al. (We refer to these environments with the prefix \"sparse\" to distinguish them from other versions that appear in the literature where agents do not receive sparse reward signals.) In addition, we evaluate performance on a highly challenging hierarchical reward task introduced by Duan et al. [5], SwimmerGather. The discrete action tasks are multiple games from the Atari RAM domain of the OpenAGyI Gyrement [4] and we will repeat them with Pong and Environment.Vong [4]."}, {"heading": "4.1 CONTINUOUS CONTROL RESULTS", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "4.2 ATARI RAM DOMAIN RESULTS", "text": "In Pong, of course, naive exploration is successful, so we are not surprised to see that intrinsic motivation does not improve performance because we shorten the game in 5000 time periods.) In BankHeist, we find that intrinsic motivation significantly accelerates learning; the agents with the surprising incentives achieved a high level of performance (scores > 1000) faster than naive exploration, while the agents with learning progress reached a high level of almost 20%. In Freeway, the median performance for TRPO without intrinsic motivation is sufficient, but the lower quarterly margin was pretty poor."}, {"heading": "4.3 COMPARING INCENTIVES", "text": "Interestingly, learning progress with k = 10 performed much worse on the continuous control tasks than with k = 1. However, we observed virtually no difference in their performance in the Atari games; it is unclear why this should be the case. Surprisingly, learning progress with k = 10 was significantly worse on the more difficult continuous control tasks, as we learned to solve them faster and without forgetting. Since we used fully factorial Gaussians for all our dynamic models, the surprise was the form \u2212 logP\u03c6 (s \u2032 | s, a) = n \u0432i = 1 (s \u2032 i \u2212 \u00b5\u0445, i (s, a) 22\u04452\u0445, i (s, a) + log (s, a) + k2 log 2\u0442, which essentially indicated the L2 relative square error as additional information about the difference in activity between terms."}, {"heading": "5 RELATED WORK", "text": "Significant theoretical work has been done to optimize exploration in finite MDPs, leading to algorithms such as E3 [10], R-max [3], and UCRL [9] that scale polynomially to MDP size. However, this work does not allow obvious generalizations of MDPs with continuous states and action spaces. C-PACE [18] provides a theoretical basis for PAC-optimal exploration in continuous state-space MDPs, but it does require a metric on state-spaces. Lopes et al. [11] examined exploration driven by learning progress and proved to be a theoretical guarantee for their approach in the finite MDP case, but they did not address the question of scaling their approach to continuous or high-dimensional MDPs. Also, although they shaped learning progress in the same way as (8), they formed intrinsic rewards differently."}, {"heading": "6 CONCLUSIONS", "text": "In this paper, we formulated surprise for intrinsic motivation as a KL divergence of the true transition probabilities of learned model probabilities and derived two approaches - surprising and k-step learning progress - that are scalable, computationally inexpensive, and suitable for use on high-dimensional and continuous control tasks. We demonstrated that motivation through surprising and single-step learning progress led empirically to efficient exploration of several hard, deeply reinforced learning benchmarks. In particular, we found that surprise was a robust and effective intrinsic motivator that outperformed other heuristics on a wide range of tasks and competed with the current state of the art in terms of intrinsic motivation in continuous control."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank Rein Houthoooften for interesting discussions and sharing data from the original VIME experiments. We also thank Rocky Duan, Carlos Florensa, Vicenc Rubies-Royo, Dexter Scobee and Eric Mazumdar for insightful discussions and reviews of the preliminary manuscript. This work is supported by TRUST (Team for Research in Ubiquitous Secure Technology), which is supported by NSF (price number CCF-0424422)."}, {"heading": "A SINGLE STEP SECOND-ORDER OPTIMIZATION", "text": "In our experiments, we roughly solve several optimization problems by using a single second step with a line search. (This section describes the exact methodology originally used by Schulman et al. [20].We consider the optimization problem to be a single second step with a line search. (This section assumes that the exact methodology originally used by Schulman et al. [20].We consider the optimization problem to be a single second step with a line search. (This section includes the exact calculation used by Schulman et al. [20].We consider the optimization problem to be much larger than the curvature of the target. As a result, we feel entitled to approach the goal of linear order and contraction.) We assume that the optimal point will be near the old point. We also assume that the curvature of the curvature of the constraint is much larger than the target curvature."}, {"heading": "B EXPERIMENT DETAILS", "text": "In fact, it is so that it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way and a way, in which it is about a way, in which it is about a way and a way in which it is about a way, in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and"}, {"heading": "C ANALYSIS OF SPEEDUP COMPARED TO VIME", "text": "In this section, we provide an analysis of the time cost of using VIME or our bonuses, and derive the potential magnitude of the acceleration achieved by our bonuses over VIME. Each iteration involves bonuses based on learned dynamic models resulting in two primary costs: \u2022 the time cost of adjusting the dynamic model, \u2022 and the time cost of calculating the rewards. We refer to the dynamics adjustment costs for VIME and our methods as T-fitvime and T-fit. Although the Bayesian dynamic model for VIME is more complex than our model, the adjustment times may be similar depending on the choice of the adjustment algorithm. In our speed test, the adjustment times were almost equivalent, but used different algorithms. For the time cost of calculating rewards, we first introduce the following quantities: \u2022 n: the number of available CPU threads \u2022 for running forward through a time model, \u2022 f: forward through a forward through a time model."}], "references": [{"title": "Novelty or Surprise", "author": ["Andrew Barto", "Marco Mirolli", "Gianluca Baldassarre"], "venue": "Frontiers in Psychology,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Unifying Count-Based Exploration and Intrinsic Motivation", "author": ["Marc G Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "Google Deepmind", "R\u00e9mi Munos"], "venue": "arXiv, (Im),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "R-max \u2013 A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning", "author": ["Ronen I Brafman", "Moshe Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Benchmarking Deep Reinforcement Learning for Continuous Control", "author": ["Yan Duan", "Xi Chen", "John Schulman", "Pieter Abbeel"], "venue": "The 33rd International Conference on Machine Learning", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "VIME Open-Source Code", "author": ["Rein Houthooft"], "venue": "https://github.com/openai/vime,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Variational Information Maximizing Exploration", "author": ["Rein Houthooft", "Xi Chen", "Yan Duan", "John Schulman", "Filip De Turck", "Pieter Abbeel"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Bayesian surprise attracts human attention", "author": ["Laurent Itti", "Pierre Baldi"], "venue": "Vision Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Near-optimal Regret Bounds for Reinforcement Learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Near Optimal Reinforcement Learning in Polynomial Time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Proceedings of the 15th International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Exploration in model-based reinforcement learning by empirically estimating learning progress", "author": ["Manuel Lopes", "Tobias Lang", "Marc Toussaint", "Py Oudeyer"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Asynchronous Methods for Deep Reinforcement Learning", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei a Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning", "author": ["Shakir Mohamed", "Danilo J Rezende"], "venue": "In Proceedings of the 29th Conference on Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Massively Parallel Methods for Deep Reinforcement Learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen", "Shane Legg", "Volodymyr Mnih", "David Silver"], "venue": "ICML Deep Learning Workshop 2015,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Action- Conditional Video Prediction using Deep Networks in Atari Games", "author": ["Junhyuk Oh", "Guo Xiaoxiao", "Lee Honglak", "Lewis Richard", "Singh Satinder"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "How can we define intrinsic motivation", "author": ["Pierre-Yves Oudeyer", "Frederic Kaplan"], "venue": "In 8th International Conference on Epigenetic Robotics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "PAC Optimal Exploration in Continuous Space Markov Decision Processes", "author": ["Jason Pazis", "Ronald Parr"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Curious Model-Building Control Systems", "author": ["J\u00fcrgen Schmidhuber"], "venue": "International Joint Conference on Neural Networks,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1991}, {"title": "Trust Region Policy Optimization", "author": ["John Schulman", "Philipp Moritz", "Michael Jordan", "Pieter Abbeel"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "High- Dimensional Continuous Control Using Generalized Advantage Estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "In ICLR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Incentivizing Exploration In Reinforcement Learning", "author": ["Bradly C. Stadie", "Sergey Levine", "Pieter Abbeel"], "venue": "With Deep Predictive Models. arXiv,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Reinforcement driven information acquisition in non-deterministic environments", "author": ["Jan Storck", "Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the International . . . ,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1995}, {"title": "Planning to be surprised: Optimal Bayesian exploration in dynamic environments", "author": ["Yi Sun", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In International Conference on Artificial General Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Deep Reinforcement Learning with Double Q-learning", "author": ["Hado van Hasselt", "Arthur Guez", "David Silver"], "venue": "In AAAI 2016,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": "[13] used -greedy exploration in training deep neural networks to play Atari games directly from raw pixels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "On many games, the algorithm resulted in superhuman play; however, on games like Montezuma\u2019s Revenge, where rewards are extremely sparse, DQN (and its variants [25], [26], [15], [12]) with -greedy exploration failed to achieve scores even at the level of a novice human.", "startOffset": 160, "endOffset": 164}, {"referenceID": 13, "context": "On many games, the algorithm resulted in superhuman play; however, on games like Montezuma\u2019s Revenge, where rewards are extremely sparse, DQN (and its variants [25], [26], [15], [12]) with -greedy exploration failed to achieve scores even at the level of a novice human.", "startOffset": 172, "endOffset": 176}, {"referenceID": 10, "context": "On many games, the algorithm resulted in superhuman play; however, on games like Montezuma\u2019s Revenge, where rewards are extremely sparse, DQN (and its variants [25], [26], [15], [12]) with -greedy exploration failed to achieve scores even at the level of a novice human.", "startOffset": 178, "endOffset": 182}, {"referenceID": 3, "context": "[5] found that policy optimization algorithms that explored by acting according to the current stochastic policy, including REINFORCE and Trust Region Policy Optimization (TRPO), could succeed across a diverse slate of simulated robotics control tasks with well-defined, non-sparse reward signals (like rewards proportional to the forward velocity of the robot).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Examples of intrinsic motivation include empowerment, where the agent enjoys the level of control it has about its future; surprise, where the agent is excited to see outcomes that run contrary to its understanding of the world; and novelty, where the agent is excited to see new states (which is tightly connected to surprise, as shown in [2]).", "startOffset": 340, "endOffset": 343}, {"referenceID": 0, "context": "For in-depth reviews of the different types of intrinsic motivation, we direct the reader to [1] and [17].", "startOffset": 93, "endOffset": 96}, {"referenceID": 15, "context": "For in-depth reviews of the different types of intrinsic motivation, we direct the reader to [1] and [17].", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "Recently, several applications of intrinsic motivation to the deep reinforcement learning setting (such as [2], [7], [22]) have found promising success.", "startOffset": 107, "endOffset": 110}, {"referenceID": 5, "context": "Recently, several applications of intrinsic motivation to the deep reinforcement learning setting (such as [2], [7], [22]) have found promising success.", "startOffset": 112, "endOffset": 115}, {"referenceID": 20, "context": "Recently, several applications of intrinsic motivation to the deep reinforcement learning setting (such as [2], [7], [22]) have found promising success.", "startOffset": 117, "endOffset": 121}, {"referenceID": 5, "context": "[7] to benchmark exploration incentives, and introduce a new task to complement the slate, 3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2], we learn a transition model as opposed to a state-action occupancy density; unlike Stadie et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[22], our formulation naturally encompasses environments with stochastic dynamics; unlike Houthooft et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[7], we avoid the overhead of maintaining a distribution over possible dynamics models, and learn a single deep dynamics model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In particular, we compare to Variational Information Maximizing Exploration (VIME) [7], a method which approximately maximizes Bayesian surprise and currently achieves state-of-the-art performance on continuous control with sparse rewards.", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "A Markov decision process (MDP) is a tuple, (S,A,R, P, \u03bc), where S is the set of states, A is the set of actions, R : S \u00d7 A \u00d7 S \u2192 R is the reward function, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the transition probability function (where P (s\u2032|s, a) is the probability of transitioning to state s\u2032 given that the previous state was s and the agent took action a in s), and \u03bc : S \u2192 [0, 1] is the starting state distribution.", "startOffset": 172, "endOffset": 178}, {"referenceID": 0, "context": "A Markov decision process (MDP) is a tuple, (S,A,R, P, \u03bc), where S is the set of states, A is the set of actions, R : S \u00d7 A \u00d7 S \u2192 R is the reward function, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the transition probability function (where P (s\u2032|s, a) is the probability of transitioning to state s\u2032 given that the previous state was s and the agent took action a in s), and \u03bc : S \u2192 [0, 1] is the starting state distribution.", "startOffset": 368, "endOffset": 374}, {"referenceID": 0, "context": "A policy \u03c0 : S \u00d7 A \u2192 [0, 1] is a distribution over actions per state, with \u03c0(a|s) the probability of selecting a in state s.", "startOffset": 21, "endOffset": 27}, {"referenceID": 0, "context": "The Bayesian surprise associated with a transition is the reduction in uncertainty over possibly dynamics models from observing it ([1],[8]): DKL (P (\u03c6|ht, at, st+1)||P (\u03c6|ht)) .", "startOffset": 132, "endOffset": 135}, {"referenceID": 6, "context": "The Bayesian surprise associated with a transition is the reduction in uncertainty over possibly dynamics models from observing it ([1],[8]): DKL (P (\u03c6|ht, at, st+1)||P (\u03c6|ht)) .", "startOffset": 136, "endOffset": 139}, {"referenceID": 18, "context": "We solve this optimization problem approximately using a single second-order step with a line search, as described by [20]; full details are given in supplementary material.", "startOffset": 118, "endOffset": 122}, {"referenceID": 5, "context": "Also, similarly to [7], we adjust the bonus coefficient \u03b7 at each iteration, to keep the average bonus magnitude upper-bounded (and usually fixed).", "startOffset": 19, "endOffset": 22}, {"referenceID": 5, "context": "[7]: sparse MountainCar, sparse CartPoleSwingup, and sparse HalfCheetah, as well as a new sparse reward task that we introduce here: sparse Swimmer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": ") Additionally, we evaluate performance on a highly-challenging hierarchical sparse reward task introduced by Duan et al [5], SwimmerGather.", "startOffset": 121, "endOffset": 124}, {"referenceID": 18, "context": ") We use Trust Region Policy Optimization (TRPO) [20], a state-of-the-art policy gradient method, as our base reinforcement learning algorithm throughout our experiments, and we use the rllab implementations of TRPO and the continuous control tasks [5].", "startOffset": 49, "endOffset": 53}, {"referenceID": 3, "context": ") We use Trust Region Policy Optimization (TRPO) [20], a state-of-the-art policy gradient method, as our base reinforcement learning algorithm throughout our experiments, and we use the rllab implementations of TRPO and the continuous control tasks [5].", "startOffset": 249, "endOffset": 252}, {"referenceID": 20, "context": "The model prediction error was investigated as intrinsic motivation for deep reinforcement learning by Stadie et al [22], although they used a different method for learning the model \u03bc\u03c6.", "startOffset": 116, "endOffset": 120}, {"referenceID": 5, "context": "[7] for Variational Information Maximizing Exploration (VIME), a method where the intrinsic reward associated with a transition approximates its Bayesian surprise using variational methods.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7], reproduced here with permission.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "To illustrate this point, in Figure 3 we show the results of a speed comparison making use of the open-source VIME code [6], with the settings described in the VIME paper.", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "Substantial theoretical work has been done on optimal exploration in finite MDPs, resulting in algorithms such as E [10], R-max [3], and UCRL [9], which scale polynomially with MDP size.", "startOffset": 116, "endOffset": 120}, {"referenceID": 2, "context": "Substantial theoretical work has been done on optimal exploration in finite MDPs, resulting in algorithms such as E [10], R-max [3], and UCRL [9], which scale polynomially with MDP size.", "startOffset": 128, "endOffset": 131}, {"referenceID": 7, "context": "Substantial theoretical work has been done on optimal exploration in finite MDPs, resulting in algorithms such as E [10], R-max [3], and UCRL [9], which scale polynomially with MDP size.", "startOffset": 142, "endOffset": 145}, {"referenceID": 16, "context": "C-PACE [18] provides a theoretical foundation for PAC-optimal exploration in MDPs with continuous state spaces, but it requires a metric on state spaces.", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": "[11] investigated exploration driven by learning progress and proved theoretical guarantees for their approach in the finite MDP case, but they did not address the question of scaling their approach to continuous or high-dimensional MDPs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.", "startOffset": 93, "endOffset": 108}, {"referenceID": 17, "context": "Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.", "startOffset": 93, "endOffset": 108}, {"referenceID": 21, "context": "Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.", "startOffset": 93, "endOffset": 108}, {"referenceID": 22, "context": "Conceptually and mathematically, our work is closest to prior work on curiosity and surprise [8, 19, 23, 24], although these works focus mainly on small finite MDPs.", "startOffset": 93, "endOffset": 108}, {"referenceID": 20, "context": "[22] learn deterministic dynamics models by minimizing Euclidean loss\u2014whereas in our work, we learn stochastic dynamics with cross entropy loss\u2014and use L2 prediction errors for intrinsic motivation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[7] train Bayesian neural networks to approximate posterior distributions over dynamics models given observed data, by maximizing a variational lower bound; they then use second-order approximations of the Bayesian surprise as intrinsic motivation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] derived pseudo-counts from CTS density models over states and used those to form intrinsic rewards, notably resulting in dramatic performance improvement on Montezuma\u2019s Revenge, one of the hardest games in the Atari domain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Mohamed and Rezende [14] developed a scalable method of approximating empowerment, the mutual information between an agent\u2019s actions and the future state of the environment, using variational methods.", "startOffset": 20, "endOffset": 24}, {"referenceID": 14, "context": "[16] estimated state visit frequency using Gaussian kernels to compare against a replay memory, and used these estimates for directed exploration.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as -greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent\u2019s surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the k-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.", "creator": "LaTeX with hyperref package"}}}