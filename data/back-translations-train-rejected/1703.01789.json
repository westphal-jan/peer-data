{"id": "1703.01789", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Sample-level Deep Convolutional Neural Networks for Music Auto-tagging Using Raw Waveforms", "abstract": "Recently, the end-to-end approach that learns hierarchical representations from raw data using deep convolutional neural networks has been successfully explored in the image, text and speech domains. This approach was applied to musical signals as well but has been not fully explored yet. To this end, we propose sample-level deep convolutional neural networks which learn representations from very small grains of waveforms (e.g. 2 or 3 samples) beyond typical frame-level input representations. This allows the networks to hierarchically learn filters that are sensitive to log-scaled frequency, such as mel-frequency spectrogram that is widely used in music classification systems. It also helps learning high-level abstraction of music by increasing the depth of layers. We show how deep architectures with sample-level filters improve the accuracy in music auto-tagging and they provide results that are com- parable to previous state-of-the-art performances for the Magnatagatune dataset and Million song dataset. In addition, we visualize filters learned in a sample-level DCNN in each layer to identify hierarchically learned features.", "histories": [["v1", "Mon, 6 Mar 2017 09:49:48 GMT  (3001kb,D)", "http://arxiv.org/abs/1703.01789v1", "7 pages, Sound and Music Computing Conference, 2017 submitted"], ["v2", "Mon, 22 May 2017 04:46:36 GMT  (6859kb,D)", "http://arxiv.org/abs/1703.01789v2", "7 pages, Sound and Music Computing Conference (SMC), 2017"]], "COMMENTS": "7 pages, Sound and Music Computing Conference, 2017 submitted", "reviews": [], "SUBJECTS": "cs.SD cs.LG cs.MM cs.NE", "authors": ["jongpil lee", "jiyoung park", "keunhyoung luke kim", "juhan nam"], "accepted": false, "id": "1703.01789"}, "pdf": {"name": "1703.01789.pdf", "metadata": {"source": "META", "title": "SAMPLE-LEVEL DEEP CONVOLUTIONAL NEURAL NETWORKS FOR MUSIC AUTO-TAGGING USING RAW WAVEFORMS", "authors": ["Jongpil Lee", "Jiyoung Park Keunhyoung", "Luke Kim", "Juhan Nam"], "emails": ["juhannam]@kaist.ac.kr"], "sections": [{"heading": "1. INTRODUCTION", "text": "In fact, most people who move in this realm are able to recognize and understand themselves. Most people who move in this realm have a different idea of themselves. Most people who move in this realm have a different idea of themselves. Most people who move in this realm have a different idea of themselves. Most people who move in this realm have a different idea of themselves. Most people who move in this realm have a different idea of themselves. Most people who move in this realm have a different idea of themselves. Most people who move in this realm have a different idea of themselves. Most people who move in this realm have a different idea of themselves."}, {"heading": "2. RELATED WORK", "text": "Since the waveform is one-dimensional data, earlier work used a CNN, which consists of one-dimensional folding and pooling steps. While the folding process and the filter length in the upper layers usually resemble those in the image area, the lower layer that directly accommodates the waveform performed a special operation called step folding, which requires a large filter length and progresses as far as the filter length (or half).This image-level approach is similar to sliding windows with 100% or 50% hop size in short-term Fourier transformation.In many previous work, the stride and filter length of the first folding layer was set to 10-20 ms (160-320 samples at 16 kHz audio) [8, 10-12].In this essay, we reduce the filter length and the step of the first folding layer at sample level, which can be as small as 2 samples, so we increase the layer depth by a few (16, some of the audios are sufficient).In this essay, we reduce the filter length and the step of the first folding layer at sample level, which can be as small as 2 samples."}, {"heading": "3. LEARNING MODELS", "text": "Figure 1 illustrates three CNN-based models of the music auto tagging task that we compare in our experiments. In this section we describe the three models in detail."}, {"heading": "3.1 Frame-level mel-spectrogram based model", "text": "Since time-frequency representation is two-dimensional data, previous work considered it either as two-dimensional images or as a one-dimensional sequence of vectors [11,13-15]. We used only one-dimensional (1D) CNN models for experimental comparisons, since the power gap between 1D and 2D models is not significant and 1D models can be directly compared with models that use raw waveforms."}, {"heading": "3.2 Frame-level raw waveform based model", "text": "In the raw waveform-based frame-level model, a striped waveform layer is added under the lower layer of the raw waveform spectrogram at frame level. The striped waveform layer learns a filter bank representation that corresponds to the filter cores in a time-frequency representation. Once the raw waveform layer passes through the first striped waveform layer, the output feature map has the same dimensions as the mel spectrogram. This is because step width, filter length, and number of filters of the first waveform layer correspond to the hop size, window size, and number of mel bands in the mel spectrogram. This configuration was used in [11, 12] for the autosagging task of music, and so we used it as the base model."}, {"heading": "3.3 Sample-level raw waveform based model", "text": "As described in Section 1, the approach that uses the raw waveforms should be able to address log-scale amplitude compression and phase invariance. Simply adding a striped folding layer is not enough to solve the problems. To improve this, we add several layers below the frame layer so that the first folding layer can handle much smaller sample lengths in filter length. For example, if we reduce the step of the first folding layer from 729 to 243, we add a three-dimensional folding layer and a maxpooling layer to keep the output dimensions equal in the subsequent folding layers. If we descend deep into the sample layer along this frame, six folding layers (five pairs of three-dimensional folding and maxpooling layer after a three-dimensional stripe folding layer) will occur. This is because the temporal dimensionality reduction is obtained only by applying the max folding and pooling method."}, {"heading": "3.4 Model Design", "text": "Since the length of an audio clip is variable, the following issues should be taken into account when configuring the temporary architecture: \u2022 Convolution Filter length and sub-sampling length. \u2022 The remaining time dimension after the last subsampling layer. \u2022 The length of the audio clip corresponds to the input size of the network. First, we have a very small filter length in conventional layers by pointing to the VGG network."}, {"heading": "39 model, 19683 frames", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "59049 samples (2678 ms) as input", "text": "Example of mn-DCNN models is shown in Table 1, where m 3 is and n 9 is. According to the definition, the filter length and pooling length of the folding layer 3 are different from the first step length of the folding layer. If the hop size (step length) of the first step length is 3, the temporal output dimension of the folding layer will be 19683 if the input of the network is 59049 samples. We call this \"39 model with 19683 frames and 59049 samples as input.\""}, {"heading": "4. EXPERIMENTAL SETUP", "text": "In this section we present the data sets used in our experiments and describe detailed experimental settings."}, {"heading": "4.1 Datasets", "text": "We evaluated the proposed model using two datasets, the Magnatagatune dataset (MTT) [16] and the Million Song dataset (MSD) with Last.FM tags [17]. We primarily examined the proposed model on MTT and then confirmed the effectiveness of our model on MSD, which is much larger than MTT (MTT contains 170 hours of audio, and MSD contains a total of 1955 hours of audio). We filtered out the tags and used the most commonly designated 50 tags in both datasets, following the previous work [11], [14,15] 1. In addition, all the songs in the two datasets were shortened to 29.1 seconds. We used AUC (Area Under Receiver Operating Characteristic) as the primary rating variable for automatic music tagging.1 https: / / github.com / keunwoochoi / MSD _ split _ for going _ tagging"}, {"heading": "4.2 Optimization", "text": "We trained the networks with the following settings: Binary cross entropy loss with sigmoid activation on the prediction layer is set to targets; Batch normalization [18] and ReLU activation for each fold layer is used; we should note that in our experiments batch normalization plays a crucial role in raw waveform-based deep learning; dropout of 0.5 was applied to the output of the last fold layer; we trained the models with stochastic gradient descent at 0.9 Nesterov momentum; the learning rate was initially set to 0.01 and decreased to a factor of 5 when the validation loss did not decrease more than 3 epochs; a total decrease of 4 times the learning rate of the last training was 0.000016; we also used batch size of 23 for MTT and 50 for MSD; in the mel spectrogram-based model, we performed the input normalization simply by subtracting the total mean data."}, {"heading": "5. RESULTS", "text": "In this section, we will examine the proposed methods and finally compare them with previous state-of-the-art results. 5.1 mn DCNN ModelsTable 2 shows the evaluation results for the mn-DCNN models on MTT for different input sizes, number of layers, filter length, and step width of the first folding layer. As described in Section 3.4, m refers to the filter length and pooling length of the middle folding layer module, and n refers to the number of modules. In Table 2, we can first determine that the accuracy in most m is proportional to n. Increasing n in our model with the same mvalue and input size means that the filter length and step of the first folding layer decrease to sample level (e.g. 2 or 3 size). When the filter length and pooling length of the first layer reach the sample level, the sample level architectures are simply considered models presented with the same filter length and sub-sampling length of all 3 layers."}, {"heading": "5.2 Mel-spectrogram and raw waveforms", "text": "Based on the fact that the output size of the first folding layer of the model using the raw waveform corresponds to the size of the mel spectrogram, divides the filter length (window), step (jump), and the number of filters (the number of mel bands), we continue to review the effectiveness of the proposed sample-level architecture by conducting experiments as described in Table 3. The models used in the experiments follow our model configuration strategy described in Section 3.4. We have added a folding layer and a pooling layer module at the top module of the 3n \u2212 1 model to compress all time dimensions to 1 in the 3n model. In the mel spectrogram experiments, 128 mel bands are used to match the number of filters of the first convolutional layer of the raw waveform-based model. The FFT size has been set to 729 in all comparisons, and the magnitude + curve is applied to a non-linear compression of the model | 4, with the FFT size of the 729 and the magnitude + curve being achieved."}, {"heading": "5.3 MSD result and the number of filters", "text": "We are investigating the capacity of our sample-level architecture even further by evaluating performance on MSD, which is ten times greater than MTT. The result is in Table 4. While we are training the network on MSD, it has been shown that the number of filters in the folding layers affects performance. According to our preliminary test results, increasing the number of filters from 16 to 512 along the layers was sufficient for MTT. However, the test on MSD shows that increasing the number of filters in the first folding layer improves performance, so we have increased the number of filters in the first folding layer from 16 to 128."}, {"heading": "5.4 Comparison to state-of-the-arts", "text": "In Table 4 we show the performance of the proposed architecture compared to the previous state of the art in terms of MTT and MSD. They show that our proposed sample level architecture is highly effective compared to these."}, {"heading": "5.5 Visualization of learned filters", "text": "The technique of visualizing the filters learned at each level enables a better understanding of the performance learning in the hierarchical network. Previous work in the music field, however, was limited to visualizing the learned filters only on the first fold layer [11, 12]. In particular, the method of gradient ascent was proposed for the visualization of filters [20], and this technology has created a deeper understanding of what Convolutionary Neural Networks learn from images [21,22]. Therefore, we apply the method of gradient ascent to see how each layer of the proposed network hears the raw waveforms. The method of gradient ascent is as follows: First, we generate random noise and propagate it back into the trained network. The loss is placed on the target filter. Then, we add the lower gradient to the input with gradient normalization. By repeating this process, we can obtain the waveform ascent filters at the target filter, obtaining the input frequency."}, {"heading": "6. CONCLUSION AND FUTURE WORK", "text": "In this work, we proposed sample-level DCNN models that use raw waveforms as input. Through our experiments, we demonstrated that the deep architectures can improve the performance of automatic tagging of music, and they provide results comparable to previous state-of-the-art presentations for the two sets of data, using raw waveforms as input. We also effectively visualized hierarchically learned filters. Future studies will analyze the filters learned more thoroughly by applying multiple visualization techniques. In addition, we can explore the transfer of music styles to the music or instrument level."}, {"heading": "7. REFERENCES", "text": "[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet networks with deep convolutional neural networks,\" in Advances in neural information processing systems, 2012, pp. 1097-1105. [2] K. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" arXiv preprint arXiv: 1409.1556, 2014. [3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, \"Distributed representations of words and phrases and their compositionality,\" in Advances in neural information processing systems, 2013, pp. 3111-3119. [4] X. Zhang, J. Zhao, and Y. LeCun, \"Character-level convolutional networks for text classification,\" in Advances in neural information processing systems, 2015, pp."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, 2013, pp. 3111\u20133119.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "Advances in neural information processing systems, 2015, pp. 649\u2013657.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "arXiv preprint arXiv:1508.06615, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks-based continuous speech recognition using raw speech signal", "author": ["D. Palaz", "M.M. Doss", "R. Collobert"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4295\u20134299.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of cnn-based speech recognition system using raw speech as input", "author": ["D. Palaz", "R. Collobert"], "venue": "Idiap, Tech. Rep., 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Wav2letter: an end-to-end convnet-based speech recognition system", "author": ["R. Collobert", "C. Puhrsch", "G. Synnaeve"], "venue": "arXiv preprint arXiv:1609.03193, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Wavenet: A generative model for raw audio", "author": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "CoRR abs/1609.03499, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end learning for music audio", "author": ["S. Dieleman", "B. Schrauwen"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6964\u20136968.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Experimenting with musically motivated convolutional neural networks", "author": ["J. Pons", "T. Lidy", "X. Serra"], "venue": "Content-Based Multimedia Indexing (CBMI), 2016 14th International Workshop on. IEEE, 2016, pp. 1\u2013 6.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic tagging using deep convolutional neural networks", "author": ["K. Choi", "G. Fazekas", "M. Sandler"], "venue": "Proceedings of the 17th International Conference on Music Information Retrieval (ISMIR), 2016, pp. 805\u2013 811.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional recurrent neural networks for music classification", "author": ["K. Choi", "G. Fazekas", "M. Sandler", "K. Cho"], "venue": "arXiv preprint arXiv:1609.04243, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Evaluation of algorithms using games: The case of music tagging", "author": ["E. Law", "K. West", "M.I. Mandel", "M. Bay", "J.S. Downie"], "venue": "ISMIR, 2009, pp. 387\u2013392.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "The million song dataset", "author": ["T. Bertin-Mahieux", "D.P. Ellis", "B. Whitman", "P. Lamere"], "venue": "Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR), vol. 2, no. 9, 2011, pp. 591\u2013 596.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Applying topological persistence in convolutional neural network for music audio signals", "author": ["J.-Y. Liu", "S.-K. Jeng", "Y.-H. Yang"], "venue": "arXiv preprint arXiv:1608.07373, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing higher-layer features of a deep network", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Vincent"], "venue": "University of Montreal, vol. 1341, p. 3, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European conference on computer vision. Springer, 2014, pp. 818\u2013 833.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 427\u2013436.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "This end-to-end hierarchical learning was attempted early in the image domain, particularly since the DCNN achieves break-through results in image classification [1].", "startOffset": 162, "endOffset": 165}, {"referenceID": 1, "context": "been found to be effective in learning more complex hierarchical filters while conserving receptive fields [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "While word embedding plays a very important role in language processing [3], it has limitations in that it is learned independently from the system.", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "Recent work using CNNs that take character-level text as input showed that the end-toend learning approach can yield comparable results to the word-level learning system [4, 5].", "startOffset": 170, "endOffset": 176}, {"referenceID": 4, "context": "Recent work using CNNs that take character-level text as input showed that the end-toend learning approach can yield comparable results to the word-level learning system [4, 5].", "startOffset": 170, "endOffset": 176}, {"referenceID": 5, "context": "In the audio domain, learning from raw audio has been explored mainly in the automatic speech recognition task [6\u201310].", "startOffset": 111, "endOffset": 117}, {"referenceID": 6, "context": "In the audio domain, learning from raw audio has been explored mainly in the automatic speech recognition task [6\u201310].", "startOffset": 111, "endOffset": 117}, {"referenceID": 7, "context": "In the audio domain, learning from raw audio has been explored mainly in the automatic speech recognition task [6\u201310].", "startOffset": 111, "endOffset": 117}, {"referenceID": 8, "context": "In the audio domain, learning from raw audio has been explored mainly in the automatic speech recognition task [6\u201310].", "startOffset": 111, "endOffset": 117}, {"referenceID": 9, "context": "This end-to-end learning approach has been applied to music classification tasks as well [11, 12].", "startOffset": 89, "endOffset": 97}, {"referenceID": 9, "context": "In particular, Dieleman and Schrauwen used raw waveforms as input of CNN models for music auto-tagging task and attempted to achieve comparable results to those using mel-spectrograms as input [11].", "startOffset": 193, "endOffset": 197}, {"referenceID": 7, "context": "In many of previous work, the stride and filter length of the first convolution layer was set to 10-20 ms (160-320 samples at 16 kHz audio) [8, 10\u201312].", "startOffset": 140, "endOffset": 150}, {"referenceID": 9, "context": "In many of previous work, the stride and filter length of the first convolution layer was set to 10-20 ms (160-320 samples at 16 kHz audio) [8, 10\u201312].", "startOffset": 140, "endOffset": 150}, {"referenceID": 5, "context": "6 ms (10 samples at 16 kHz audio) as a stride length [6, 7], but they used a CNN model only with three convolution layers, which is not sufficient to learn the complex structure of musical signals.", "startOffset": 53, "endOffset": 59}, {"referenceID": 6, "context": "6 ms (10 samples at 16 kHz audio) as a stride length [6, 7], but they used a CNN model only with three convolution layers, which is not sufficient to learn the complex structure of musical signals.", "startOffset": 53, "endOffset": 59}, {"referenceID": 9, "context": "Since the time-frequency representation is two dimensional data, previous work regarded it as either twodimensional images or one-dimensional sequence of vectors [11,13\u201315].", "startOffset": 162, "endOffset": 172}, {"referenceID": 10, "context": "Since the time-frequency representation is two dimensional data, previous work regarded it as either twodimensional images or one-dimensional sequence of vectors [11,13\u201315].", "startOffset": 162, "endOffset": 172}, {"referenceID": 11, "context": "Since the time-frequency representation is two dimensional data, previous work regarded it as either twodimensional images or one-dimensional sequence of vectors [11,13\u201315].", "startOffset": 162, "endOffset": 172}, {"referenceID": 12, "context": "Since the time-frequency representation is two dimensional data, previous work regarded it as either twodimensional images or one-dimensional sequence of vectors [11,13\u201315].", "startOffset": 162, "endOffset": 172}, {"referenceID": 9, "context": "This configuration was used for music autotagging task in [11, 12] and so we used it as a baseline model.", "startOffset": 58, "endOffset": 66}, {"referenceID": 1, "context": "First, we attempted a very small (sample-level) filter length in convolutional layers by referring to the VGG net [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "Although sub-sampling using strided convolution has recently been proposed in generative model [9], our preliminary test showed that max-pooling was superior to the stride-style sub-sampling method.", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "In the mel-spectrogram based model, one song is generally segmented into 1-4 seconds [11], and then the predictions of all the segments in one song are averaged to make a song-level prediction.", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "We evaluate the proposed model on two datasets, Magnatagatune dataset (MTT) [16] and Million song dataset (MSD) annotated with the Last.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "FM tags [17].", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "We filtered out the tags and used most frequently labeled 50 tags in both datasets, following the previous work [11], [14,15] 1 .", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "We filtered out the tags and used most frequently labeled 50 tags in both datasets, following the previous work [11], [14,15] 1 .", "startOffset": 118, "endOffset": 125}, {"referenceID": 12, "context": "We filtered out the tags and used most frequently labeled 50 tags in both datasets, following the previous work [11], [14,15] 1 .", "startOffset": 118, "endOffset": 125}, {"referenceID": 15, "context": "Batch normalization [18] and ReLU activation for every convolution layer is used.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "Frame-level (mel-spectrogram) Persistent CNN [19] 0.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "9013 2D CNN [14] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "851 CRNN [15] - 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 9, "context": "Frame-level (raw waveforms) 1D CNN [11] 0.", "startOffset": 35, "endOffset": 39}, {"referenceID": 1, "context": "Interestingly, the length of 3 corresponds to the 3-size spatial filters in the VGG net [2].", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "However, previous works in music domain are limited to visualizing learned filters only on the first convolution layer [11, 12].", "startOffset": 119, "endOffset": 127}, {"referenceID": 17, "context": "Especially the gradient ascent method has been proposed [20] for filter visualization and this technology has provided deeper understanding of what convolutional neural networks learn from images [21,22].", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "Especially the gradient ascent method has been proposed [20] for filter visualization and this technology has provided deeper understanding of what convolutional neural networks learn from images [21,22].", "startOffset": 196, "endOffset": 203}, {"referenceID": 19, "context": "Especially the gradient ascent method has been proposed [20] for filter visualization and this technology has provided deeper understanding of what convolutional neural networks learn from images [21,22].", "startOffset": 196, "endOffset": 203}, {"referenceID": 9, "context": "This nonlinearity was found in learned filters with a frame-level end-to-end learning [11] and also in perceptual pitch scales such as mel or bark.", "startOffset": 86, "endOffset": 90}], "year": 2017, "abstractText": "Recently, the end-to-end approach that learns hierarchical representations from raw data using deep convolutional neural networks has been successfully explored in the image, text and speech domains. This approach was applied to musical signals as well but has been not fully explored yet. To this end, we propose sample-level deep convolutional neural networks which learn representations from very small grains of waveforms (e.g. 2 or 3 samples) beyond typical frame-level input representations. This allows the networks to hierarchically learn filters that are sensitive to log-scaled frequency, such as mel-frequency spectrogram that is widely used in music classification systems. It also helps learning high-level abstraction of music by increasing the depth of layers. We show how deep architectures with sample-level filters improve the accuracy in music auto-tagging and they provide results that are comparable to previous state-of-the-art performances for the Magnatagatune dataset and Million song dataset. In addition, we visualize filters learned in a sample-level DCNN in each layer to identify hierarchically learned features.", "creator": "LaTeX with hyperref package"}}}