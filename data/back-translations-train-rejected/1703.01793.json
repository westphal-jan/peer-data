{"id": "1703.01793", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Multi-Level and Multi-Scale Feature Aggregation Using Pre-trained Convolutional Neural Networks for Music Auto-tagging", "abstract": "Music auto-tagging is often handled in a similar manner to image classification by regarding the 2D audio spectrogram as image data. However, music auto-tagging is distinguished from image classification in that the tags are highly diverse and have different levels of abstractions. Considering this issue, we propose a convolutional neural networks (CNN)-based architecture that embraces multi-level and multi-scaled features. The architecture is trained in three steps. First, we conduct supervised feature learning to capture local audio features using a set of CNNs with different input sizes. Second, we extract audio features from each layer of the pre-trained convolutional networks separately and aggregate them altogether given a long audio clip. Finally, we put them into fully-connected networks and make final predictions of the tags. Our experiments show that using the combination of multi-level and multi-scale features is highly effective in music auto-tagging and the proposed method outperforms previous state-of-the-arts on the Magnatagatune dataset and the million song dataset. We further show that the proposed architecture is useful in transfer learning.", "histories": [["v1", "Mon, 6 Mar 2017 09:57:25 GMT  (2332kb,D)", "http://arxiv.org/abs/1703.01793v1", "5 pages, 5 figures, 2 tables"], ["v2", "Wed, 7 Jun 2017 17:21:04 GMT  (2434kb,D)", "http://arxiv.org/abs/1703.01793v2", "5 pages, 5 figures, 2 tables"]], "COMMENTS": "5 pages, 5 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.NE cs.LG cs.MM cs.SD", "authors": ["jongpil lee", "juhan nam"], "accepted": false, "id": "1703.01793"}, "pdf": {"name": "1703.01793.pdf", "metadata": {"source": "CRF", "title": "Multi-Level and Multi-Scale Feature Aggregation Using Pre-trained Convolutional Neural Networks for Music Auto-tagging", "authors": ["Jongpil Lee"], "emails": [], "sections": [{"heading": null, "text": "The fact is that most of them will be able to go to another world, in which they will go to another world, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, in which they will not find themselves, but in which they will find themselves."}, {"heading": "II. PROPOSED METHOD", "text": "The overall architecture we propose is illustrated in Figure 1. Below, the three steps to train them are described."}, {"heading": "A. Local feature learning", "text": "In the first step, we perform supervised feature learning with a series of CNNs. We selected the segment sizes so that the hidden layers capture multi-level audio features within one to several music beats for different beats per minute (BPM). For example, 18, 27 and 54 frames are equivalent to 420, 630 and 1260 msec. They take care of at least one beat long in songs at 48 to 143 BPM, which is the tempo range that covers the majority of popular music. CNNs are configured to perform 1-D convolution in all layers, assuming that low-frequency and high-frequency content does not split weights (as opposed to images), and so the entire frequency range is below the receptive field of the filters. We determined the filter width \"3\" in the revolutionary filter layers by referring to the VGA network (2 layers)."}, {"heading": "B. Multi-level and multi-scale feature aggregation", "text": "The pre-trained CNNs can be considered feature extractors. Since a single CNN model can extract different feature layers based on the input size, and we train them with different input sizes, we can extract multi-level and multi-level features from them. To handle the long audio clips (typically about 30 seconds), we aggregate them into a single large feature vector and use them as song-level representation. For example, the output form of each convolution layer of a segment in a 27-frame model is (27,128) - (9,128) - (3,256). After extracting features for all segments in 30 seconds of audio, the song-level feature dimension (46.27.128) - (46.3.256). To extract the most representative feature, we apply max pooling over each segment individually."}, {"heading": "C. Global classification", "text": "In this step, we make final predictions of keywords from the aggregated multi-level and multi-level characteristics. We train another classifier, a neural network with two fully connected hidden layers, with 512 or 1024 units depending on the data set. Since the feature aggregation and global classification steps are separate from the local feature sets, we can perform transfer learning that has also been effectively explored for music audio data [12], [13] by using pre-trained CNNs with a large data set. In our experiment, we evaluate the learning settings for the transfer of several different data sets."}, {"heading": "III. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Datasets", "text": "To evaluate the proposed architecture, we primarily used the Magnatagatune dataset (MTT) [14] and the Million song dataset (MSD) with the Last.fm tags [15]. We filtered out the tags and used the most commonly designated 50 tags, followed by the previous work [2], [4] 1. In addition, all songs in the two datasets were cut to 29.1 seconds in length. We used AUC (Area Under Receiver Operating Characteristic) as the primary evaluation parameter for music auto tagging. In addition, we used two genre classification datasets, GTZAN [16] and daydream genre annotations to MSD [17], in a transfer learning environment in which the pre-trained CNNs with MSD are used as a feature extractor. We used error-filtered split lists for GTZAN [18] and layered version of the CD2C training space with 80% split data."}, {"heading": "B. Training details", "text": "The input representation is a Mel frequency spectrogram with 128 containers. The parameters are set to 22.05 kHz sampling rate (if necessary by re-sampling), 512 samples of hop size, 1024 samples of the Hanning window and size compression with nonlinear curve, log (1 + C | A |) where A is the size and C is 10. As a result, each clip has 1250 frames and is divided into 69, 46, 23, 11 and 5 segments for the corresponding models 18, 27, 54, 108 and 216, respectively. Detailed parameters for forming the networks are as follows: sigmoid activation for output layer with binary cross-drop loss, batch normalization [19] and ReLU activation for each interlayer, 0.5 of drops1 https: / / github.com / keunchoi / MSD split for tagging layers with invisible cross-drop loss, normalization for each interlayer, 0.5 of droplet activation for each interlayer https: / / github.com / keunchoi / MSD split for tagging layers with invisible cross-drop loss, 0.5 of MhttD and 0.5 of activation for each interlayer, 0.5 of MhttD and 0.5 of each activation are combined with Mchochodynamics / L1."}, {"heading": "C. Compared models", "text": "A typical approach for automatic tagging of music is to take audio segments about 3 seconds long as inputs and cut the outputs to make final predictions for an audio clip (e.g. [2]). At this point, we call them \"local\" models, as already described in Section II-A. On the other hand, we call our proposed architectural models \"global\" models because they aggregate features from local models and make final predictions directly from the audio clip. In our experiment, we evaluate the two models with different combinations of input sizes and feature levels."}, {"heading": "IV. RESULTS AND DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Comparison of local and global models", "text": "Figure 2 shows the evaluation results for the local and global models of MTT and MSD for different input sizes. The local models show that the AUC reaches the highest value when the input size is 108 frames (about 2.5 seconds), suggesting that it is indeed a reasonable choice to take 3 seconds as the input size. However, the proposed global models consistently outperform the local models and the performance increase is more pronounced with MSD."}, {"heading": "B. Effect of multi-level features", "text": "Figure 3 dissects the effect of multi-level traits in the global model. If a single-level trait is used, higher-level traits (L3) seem to function better than lower-level traits (L1 and L2). If multi-level traits are combined, the AUC traits on both data sets continuously increase. An interesting result is that each level has different meanings. For example, the absence of L1 traits on the MTT reduces AUC the most, while on the MSD the absence of L3 traits decreases the most. This could be due to differences in keywords between the two data sets. For example, MTT contains more instrument-related tags than gene or mood labels compared to MSD."}, {"heading": "C. Effect of multi-scaled features", "text": "Figure 4 shows the results for different combinations of different input sizes in the global models. Compared to multi-level models, the performance increase is not strong, but the use of multi-level functions is definitely helpful. MSD as well as MTT get the best result when combining 18, 27 and 54 frame models, as shown in Figure 2."}, {"heading": "D. Performances visualization of individual tags", "text": "We study the global model even further by comparing the performance sensitivity of individual tags with different trait levels and time scales, as shown in Figure 5. In the multi-level comparison (above), as the supervised training is performed with the tags in the on-site learning phase, a gradual increase is expected as the shift increases, but this trend does not work uniformly for every single day. Thus, some tags such as blues, chill, guitar and 80s prefer more L2 features than L3. Even in the multi-level comparison (below), the non-gradual increase can be observed. Some tags such as heavy metal, experimental, progressive rock, alternative, chill, guitar and 90s prefer 54 frames, while others such as hard rock, easy listening, singer and 70s work better on 18 frames. Overall, we can ensure that the best AUCs are achieved in almost all tags when the multi-level and multi-level features are combined."}, {"heading": "E. Transfer learning and comparison to state-of-the-arts", "text": "In Table I, we show the performance of the proposed architecture in the transfer learning environments where MSD is used to pre-train the CNNs as a feature extractor and other data sets are for final classification. Interestingly, the autotagging performance on MTT is even higher than that with local models trained from the MTT data set itself. It also shows the classification results of the music genres on daydream (15 genres) and GTZAN (10 genres). To our knowledge, this is the first time we report on the performance on the daydream data set. Table II shows that the accuracy on the error-filtered GTZAN is higher than previously reported. Table II also compares the best results in the local and global models with those of the obsolete state of the art on MTT and MSD. They show that our proposed architecture is highly effective."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "In this article, we presented a CNN-based architecture designed with different levels of abstraction of music tags in mind. We demonstrated the effectiveness of the architecture by evaluating different combinations of multi-level and multi-level features and applying them to the transfer of learning settings. Finally, we showed that our proposed architecture achieves the best results on the three commonly used data sets. As a future work, we plan to train the architecture in a multitascale learning method by simultaneously optimizing the local CNNs and global aggregated networks."}], "references": [{"title": "Temporal pooling and multiscale learning for automatic annotation and ranking of music audio", "author": ["P. Hamel", "S. Lemieux", "Y. Bengio", "D. Eck"], "venue": "Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR), 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "End-to-end learning for music audio", "author": ["S. Dieleman", "B. Schrauwen"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 6964\u20136968.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Experimenting with musically motivated convolutional neural networks", "author": ["J. Pons", "T. Lidy", "X. Serra"], "venue": "2016 14th International Workshop on Content-Based Multimedia Indexing (CBMI). IEEE, 2016, pp. 1\u20136.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic tagging using deep convolutional neural networks", "author": ["K. Choi", "G. Fazekas", "M. Sandler"], "venue": "Proceedings of the 17th International Conference on Music Information Retrieval (ISMIR), 2016, pp. 805\u2013811.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["H. Lee", "Y. Largman", "P. Pham", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems 22, 2009, pp. 1096\u20131104.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning features from music audio with deep belief networks", "author": ["P. Hamel", "D. Eck"], "venue": "Proceedings of the 11th International Conference on Music Information Retrieval (ISMIR). Utrecht, The Netherlands, 2010, pp. 339\u2013344.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Building musically-relevant audio features through multiple timescale representations", "author": ["P. Hamel", "Y. Bengio", "D. Eck"], "venue": "Proceedings of the 13th International Conference on Music Information Retrieval (ISMIR), 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiscale approaches to music audio feature learning", "author": ["S. Dieleman", "B. Schrauwen"], "venue": "14th International Society for Music Information Retrieval Conference (ISMIR-2013). Pontif\u0131\u0301cia Universidade Cat\u00f3lica do Paran\u00e1, 2013, pp. 116\u2013121.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning sparse feature representations for music annotation and retrieval", "author": ["J. Nam", "J. Herrera", "M. Slaney", "J.O. Smith"], "venue": "Proceedings of the 13th International Conference on Music Information Retrieval (ISMIR), 2012, pp. 565\u2013570.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "A deep bag-of-features model for music auto-tagging", "author": ["J. Nam", "J. Herrera", "K. Lee"], "venue": "arXiv preprint arXiv:1508.04999, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Transfer learning in mir: Sharing learned latent representations for music audio classification and similarity", "author": ["P. Hamel", "M.E.P. Davies", "K. Yoshii", "M. Goto"], "venue": "14th International Conference on Music Information Retrieval (ISMIR \u201913), 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Transfer learning by supervised pre-training for audio-based music classification", "author": ["A. Van Den Oord", "S. Dieleman", "B. Schrauwen"], "venue": "Conference of the International Society for Music Information Retrieval (ISMIR 2014), 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluation of algorithms using games: The case of music tagging", "author": ["E. Law", "K. West", "M.I. Mandel", "M. Bay", "J.S. Downie"], "venue": "ISMIR, 2009, pp. 387\u2013392.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "The million song dataset", "author": ["T. Bertin-Mahieux", "D.P. Ellis", "B. Whitman", "P. Lamere"], "venue": "Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR), vol. 2, no. 9, 2011, pp. 591\u2013596.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Musical genre classification of audio signals", "author": ["G. Tzanetakis", "P. Cook"], "venue": "IEEE Transactions on speech and audio processing, vol. 10, no. 5, pp. 293\u2013302, 2002.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Improving genre annotations for the million song dataset", "author": ["H. Schreiber"], "venue": "Proceedings of the 16th International Conference on Music Information Retrieval (ISMIR), 2015, pp. 241\u2013247.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning and music adversaries", "author": ["C. Kereliuk", "B.L. Sturm", "J. Larsen"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 11, pp. 2059\u20132071, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Applying topological persistence in convolutional neural network for music audio signals", "author": ["J.-Y. Liu", "S.-K. Jeng", "Y.-H. Yang"], "venue": "arXiv preprint arXiv:1608.07373, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional recurrent neural networks for music classification", "author": ["K. Choi", "G. Fazekas", "M. Sandler", "K. Cho"], "venue": "arXiv preprint arXiv:1609.04243, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning temporal features using a deep neural network and its application to music genre classification", "author": ["I.-Y. Jeong", "K. Lee"], "venue": "Proceedings of the 17th International Conference on Music Information Retrieval (ISMIR), 2016, pp. 434\u2013440.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Recently, as convolutional neural networks (CNN) has become the de-facto standard in image classification, the deep learning approach has been actively explored in music auto-tagging as well by using the spectrogram and its variants as input data and so recasting it as a multi-label classification task on the 2-D timefrequency images [1]\u2013[4].", "startOffset": 336, "endOffset": 339}, {"referenceID": 3, "context": "Recently, as convolutional neural networks (CNN) has become the de-facto standard in image classification, the deep learning approach has been actively explored in music auto-tagging as well by using the spectrogram and its variants as input data and so recasting it as a multi-label classification task on the 2-D timefrequency images [1]\u2013[4].", "startOffset": 340, "endOffset": 343}, {"referenceID": 4, "context": "used convolutional restricted Boltzmann machine to learn hierarchical features in an unsupervised manner [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 5, "context": "Hamel and Eck applied deep neural networks pretrained with deep belief networks to learn hierarchical features and compared each layer of features [6].", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "investigated combining different resolutions of spectrograms [7], and Dieleman and Schrauwen improved the approach further using Gaussian and Laplacian pyramids [8].", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "investigated combining different resolutions of spectrograms [7], and Dieleman and Schrauwen improved the approach further using Gaussian and Laplacian pyramids [8].", "startOffset": 161, "endOffset": 164}, {"referenceID": 8, "context": "We determined the filter width \u201d3\u201d in the convolutional layers by referring to the VGG net [9].", "startOffset": 91, "endOffset": 94}, {"referenceID": 9, "context": "This scheme, that is, max-pooling followed by average pooling, was used as an effective means to summarize local features [10], [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 10, "context": "This scheme, that is, max-pooling followed by average pooling, was used as an effective means to summarize local features [10], [11].", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "Since the feature aggregation and global classification steps are separated from the local feature leanring, we can conduct transfer learning, which has been explored effectively as well for music audio data [12], [13], using pretrained CNNs with a large dataset.", "startOffset": 208, "endOffset": 212}, {"referenceID": 12, "context": "Since the feature aggregation and global classification steps are separated from the local feature leanring, we can conduct transfer learning, which has been explored effectively as well for music audio data [12], [13], using pretrained CNNs with a large dataset.", "startOffset": 214, "endOffset": 218}, {"referenceID": 13, "context": "To evaluate the proposed architecture, we primarily used the Magnatagatune (MTT) dataset [14] and Million song dataset (MSD) annotated with the Last.", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "fm tags [15].", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "the previous work [2], [4]1.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "the previous work [2], [4]1.", "startOffset": 23, "endOffset": 26}, {"referenceID": 15, "context": "In addition, we used two genre classification datasets, GTZAN [16] and Tagtraum genre annotations on MSD [17], in a transfer learning setting where the pre-trained CNNs with MSD is used as a feature extractor.", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "In addition, we used two genre classification datasets, GTZAN [16] and Tagtraum genre annotations on MSD [17], in a transfer learning setting where the pre-trained CNNs with MSD is used as a feature extractor.", "startOffset": 105, "endOffset": 109}, {"referenceID": 17, "context": "We used fault-filtered split list for GTZAN [18] and stratified split with 80% training data of CD2C version for Tagtraum.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "The detailed parameters to train the networks are as follows: sigmoid activation for output layer with binary cross entropy loss, batch normalization [19] and ReLU activation for every intermediate layer, 0.", "startOffset": 150, "endOffset": 154}, {"referenceID": 1, "context": "[2]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Bag of multi-scaled features [8] 0.", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "898 1D CNN [2] 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 12, "context": "8815 Transfer learning [13] 0.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "88 Persistent CNN [20] 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "9013 2D CNN [4] 0.", "startOffset": 12, "endOffset": 15}, {"referenceID": 20, "context": "851 CRNN [21] - 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "862 2D CNN [18] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 21, "context": "632 Temporal features [22] 0.", "startOffset": 22, "endOffset": 26}], "year": 2017, "abstractText": "Music auto-tagging is often handled in a similar manner to image classification by regarding the 2D audio spectrogram as image data. However, music auto-tagging is distinguished from image classification in that the tags are highly diverse and have different levels of abstractions. Considering this issue, we propose a convolutional neural networks (CNN)-based architecture that embraces multi-level and multi-scaled features. The architecture is trained in three steps. First, we conduct supervised feature learning to capture local audio features using a set of CNNs with different input sizes. Second, we extract audio features from each layer of the pre-trained convolutional networks separately and aggregate them altogether given a long audio clip. Finally, we put them into fully-connected networks and make final predictions of the tags. Our experiments show that using the combination of multi-level and multi-scale features is highly effective in music auto-tagging and the proposed method outperforms previous state-of-the-arts on the Magnatagatune dataset and the million song dataset. We further show that the proposed architecture is useful in transfer learning.", "creator": "LaTeX with hyperref package"}}}