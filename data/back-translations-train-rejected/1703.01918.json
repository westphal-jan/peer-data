{"id": "1703.01918", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "High-Resolution Multispectral Dataset for Semantic Segmentation", "abstract": "Unmanned aircraft have decreased the cost required to collect remote sensing imagery, which has enabled researchers to collect high-spatial resolution data from multiple sensor modalities more frequently and easily. The increase in data will push the need for semantic segmentation frameworks that are able to classify non-RGB imagery, but this type of algorithmic development requires an increase in publicly available benchmark datasets with class labels. In this paper, we introduce a high-resolution multispectral dataset with image labels. This new benchmark dataset has been pre-split into training/testing folds in order to standardize evaluation and continue to push state-of-the-art classification frameworks for non-RGB imagery.", "histories": [["v1", "Mon, 6 Mar 2017 15:16:56 GMT  (1748kb,D)", "http://arxiv.org/abs/1703.01918v1", "9 pages, 8 Figures"]], "COMMENTS": "9 pages, 8 Figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["ronald kemker", "carl salvaggio", "christopher kanan"], "accepted": false, "id": "1703.01918"}, "pdf": {"name": "1703.01918.pdf", "metadata": {"source": "CRF", "title": "High-Resolution Multispectral Dataset for Semantic Segmentation", "authors": ["Ronald Kemker", "Carl Salvaggio", "Christopher Kanan", "Chester F. Carlson"], "emails": ["kanan}@rit.edu"], "sections": [{"heading": "1. Introduction", "text": "Semantic segmentation of remote sensing images provides the end user with a pixel-by-pixel classification map for a given scene. Countless machine learning and depth-learning algorithms have been developed to accomplish this task, but access to large amounts of labeled data for non-RGB sensors makes it difficult to deploy these frameworks. In computer vision, semantic segmentation has made significant progress due to deep-rooted neural networks (DCNNs) [23, 10] trained with large amounts of labeled images [21]. The quantity of labeled data for multispectra (MSI) and hyperspectival images is minuscule in comparison, making DCNNs less successful for remote sensing. Common benchmark datasets have been acquired from aircraft and satellite platforms, so the dataset distance (GSD) is usually in the order of 1-20 meters."}, {"heading": "2. Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Non-RGB Labeled Datasets", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2.2. Separate Training/Testing Folds", "text": "The majority of published work, which includes the classification of non-RGB remote sensing images, involves the use of small, single image sets such as the HSI datasets listed in Table 1. The training / test sets are usually generated by randomly sampling a percentage of the image. Many of these works use different training / test sets rather than established benchmarks, making it difficult to 1) identify the current state of the art, and 2) provide a fair comparison with other published algorithms. The construction of training / test folds from a single image may be useful for prototyping algorithms, but it is not representative of a reliable framework. A pre-trained machine learning model will not have access to new labels in a deployed environment, so that the model must be able to adapt to a wide range of circumstances in order to make generalized predictions based on data it has already seen. To show that test data must remain a separate model, the training machine can do this - one for all."}, {"heading": "3. Data Collection", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Collection Site", "text": "The imagery for this dataset was collected at Hamlin Beach State Park on the shores of Lake Ontario in Hamlin, N.Y. Training and validation data were collected at one location, and test data at another location in the park. These two locations are unique, but they share many of the same class types. Table 2 lists some other survey parameters that may be of interest."}, {"heading": "3.2. Collection Equipment", "text": "The Tetracam MicroMCA6 MSI sensor features six independent optical systems with bandpass filters in the middle of the visual and near infrared (VNIR) spectrum. Figure 2 shows an image of the MicroMCA6 on board the DJI-S1000 octocopter."}, {"heading": "4. Data Processing", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Pre-Processing", "text": "For each acquisition campaign, we filtered out data that was not collected along our desired trajectory (i.e. take-off and landing steps); the six spectral images were taken from independent imaging systems, so they must be registered with each other; the manufacturer provided an affinity transformation matrix that was not designed to work at the altitude at which this data was collected and caused a discernible registration error; we used one of the parking lot images to develop a global perspective transformation for the other images in our dataset; Figure 3 (a) illustrates the registration error caused by the affinity transformation provided by the manufacturer; and Figure 3 (b) shows that this error coincided with our perspective transformation.The global transformation worked well for some of the images, but there were misregistration errors in other parts of the scene indicating that the transformation must be done on a per-image basis; this was done by piecing together SIFT tape to form homographic features from each one."}, {"heading": "4.2. Orthomosaics", "text": "Agisoft PhotoScan [3] was used to create the orthomosaics from the individual images in Section 4.1. PhotoScan workflow includes: 1. Find key points in the images and customize them as binding dots. 2. Create a dense dot cloud from the image data. 3. Create a 3D mesh and a corresponding UV texture map from the dense dot cloud. 4. Generate an orthomosaic on the WGS-84 coordinate system. 5. Manually correct problematic areas by removing photos caused by motion blur or moving objects. PhotoScan can can produce high-quality orthomosaics, but manual steps have been taken to ensure the best quality. Firstly, not all images were in focus. And although PhotoScan has an image quality algorithm, we opted to build photos caused by motion blur or moving objects to be projectively removed from the 3D, and the large parts of the image will be pre-focussed during the second scan."}, {"heading": "5. Proposed Dataset", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Training/Testing Split", "text": "The Hamlin Beach State Park dataset (Figure 1) is divided into a training, validation and test fold. Each fold contains an orthomosaic image and a corresponding classification map. Each orthomosaic contains the six-band image described in Section 4.2 together with a mask in which the image data is valid. Spatial dimensions for each fold are 9,393 x 5,642 (pull), 8,833 x 6,918 (validation) and 12,446 x 7,654 (test)."}, {"heading": "5.2. Classification Labels", "text": "Table 4 lists the 18 class names for this dataset. Each orthomosaic was commented manually using the Regional-of-Interest (ROI) tool in ENVI. Several people participated in the labeling process. Class-designated instances differ by orders of magnitude, as shown in Figure 4, and these under-represented classes should make this dataset more difficult."}, {"heading": "5.2.1 Water/Beach Area", "text": "The two classes of water are lake and pond. The sea category is for Lake Ontario, which is located north of the beach. The pond water class is for the small inland pond, which is present in all three folds and is surrounded by swamp and trees. Along Lake Ontario there is a sand beach class. This class also includes every place where sand flew into the air along the asphalt trails. Along the beach there are some white painted wooden lifechairs. The buoy class is for the buoys available in the water and on the beach. They are very small, primarily red and / or white, and take different shapes. The rock class is for the large breakwaters on the beach."}, {"heading": "5.2.2 Vegetation", "text": "There are three vegetation classes, including grass, trees and low vegetation; the tree class includes a variety of trees present in the scene; the grass includes all the pixels on the lawn; there are some mixtures in the grass (such as sand, dirt or various weeds), so the classification algorithm must take into account adjacent pixel information; the grass spots on the beach and the asphalt are automatically marked with a Normalized Difference Vegetation Index (NDVI); the low vegetation class includes any other vegetation, including well-kept plants, around the building or the swamp next to the pond."}, {"heading": "5.2.3 Roadway", "text": "The asphalt class covers all parking lots, roads and pavements made of asphalt, but no cement or stone paths around the buildings. The road marking class is for all painted asphalt surfaces, including parking / roadways. This class has been automatically marked with manual re-cleaning. The road markings in the validation image are sharper than those in the training image, as the park has repainted the lines between the collection points."}, {"heading": "5.2.4 Underrepresented Classes", "text": "Underrepresented classes, which may be small and / or rare, will be difficult to identify. As some of the land coverage classes are massive by comparison, the middle class accuracy measurement will be the most important in the classification experiments in Section 6.1. Small object classes, such as person and picnic table, represent only a tiny fraction of the image and should remain very difficult to classify. These small objects will be surrounded by larger classes and may even hide in the shade. There are also a few classes that occur only a few times in the scene, such as the white-black wood panes, the orange UAS landing flaps, lifeguard chairs and buildings. The building class consists mainly of roofs / shingles of some buildings that can be found throughout the scene. The similarity between the white wooden reflection calibration target and the lifeguard chair should make semantic information in the scene crucial for classification accuracy."}, {"heading": "6. Benchmark Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Semantic Segmentation", "text": "The main objective of this dataset is to advance the state-of-the-art semantic segmentation of non-RGB images. This section will provide some benchmark results for future development; the training and validation folds are used to cross-validate hyperparameters, and then the model is combined with the two folds using these hyperparameters; the test data and labels are never used to cross-validate hyperparameters or adapt to the model."}, {"heading": "6.1.1 Spectral-Only Features", "text": "This paper examines three purely spectral classification methods, including k-next neighbor (kNN), linear SVM, and MLP. Pure spectral classification does not take into account adjacent pixel information, so the high spatial variability in small GSD images adversely affects classification performance. In addition, semantic information for objects present in small GSD images requires frameworks that capture pixels from a broader receptive field. We chose the LIBLINEAR [8] implementation of SVM for its speed and stability with large datasets. This implementation uses L2 loss and a one-on-rest multi-class approach. We also tried to use the radial base function (RBF) for SVM, but the large number of samples prevented the classifier from properly matching. The only pre-processing step was normalization (channel mean subtraction and standard deviation)."}, {"heading": "6.1.2 Spatial-Spectral Features", "text": "The resolution of this date encourages the use of neighboring pixels to improve the pure systematics. This paper explores several spatial and spectral extraction techniques, including pooling (MP), and the pooled reaction is directed in the same direction."}, {"heading": "6.1.3 Experimental Results", "text": "The semantic segmentation results for this dataset are listed in Table 5. Each algorithm is evaluated for accuracy per class, total accuracy (OA), middle-class accuracy (AA) and Kappa statistics. Mean-class accuracy is the most important measurement for evaluating our classification models due to the disparity in numbers for each class. KNN classification uses Euclidean distance metrics and validation for the range 1-15. Linear SVM validation for the range 2 \u2212 9 \u2212 216 and is weighted by inverse class frequency."}, {"heading": "6.2. Target Detection", "text": "This challenge consists of two sets of white and black wood panels, shown in Figure 8, with one set placed in the shade and the other in direct sunlight, and the signature of both panels can be extracted using the training labels. This challenge is evaluated by the area below the curve metric.Table 7 has some benchmark results, using commonly used algorithms to detect significantly matched targets, including the spectral angle mapper (SAM) [15], spectral-tuned filter (SMF) [6], constrainee energy minimization (CEM) [9] and adaptive cosine esti-mator (ACE) [14]. These benchmarks used global background assessments created by using every pixel in the image apart from the targets. These algorithms were performed on both the training and test data, but no single algorithm worked well on each target."}, {"heading": "7. Discussion/Conclusion", "text": "We have separated the training, validation and test data sets to allow researchers to compare the classification performance with the current state of the art. We will work to make this data available on the IEEE GRSS evaluation server to standardize the evaluation of new semantic segmentation frameworks.Our experimental results demonstrate the challenges associated with this dataset. In addition, the large number of samples (pixels) present in this dataset takes a great deal of computing effort. The MLP classifier and the SCAE feature extraction frame were trained using the graphical processing unit NVIDIA Titan X (GPU), which has 12 GB of memory. None of the orthomosaics contained in this dataset could be loaded into the GPU memory with high probability. Future work on this dataset should include an end-to-end classification framework, capable of reclassifying this system more quickly and maximizing the problem."}, {"heading": "Acknowledgements", "text": "We would like to thank Nina Raqueno, Paul Sponagle, Timothy Bausch, Michael McClelland II and other members of the RIT Signature Interdisciplinary Research Area, the UAS research laboratory, who assisted us in collecting the data."}], "references": [{"title": "Thermal and Narrowband Multispectral Remote Sensing for Vegetation Monitoring from an Unmanned Aerial Vehicle", "author": ["J.A. Berni"], "venue": "IEEE Trans. Geosci. Remote Sens.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Automated Hyperspectral Cueing for Civilian Search and Rescue", "author": ["M.T. Eismann"], "venue": "IEEE Proceedings,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "The PASCAL Visual Object Classes Challenge: A Retrospective", "author": ["M. Everingham"], "venue": "Int. J. Comp. Vis.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "LIBLINEAR: A Library for Large Linear Classification", "author": ["R.-E. Fan"], "venue": "J. Mach. Learn. Rsrch.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Detection and Classification of Subpixel Spectral Signatures in Hyperspectral Image Sequences", "author": ["J.C. Harsanyi"], "venue": "PhD thesis, University of Maryland,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He"], "venue": "In Proc. of IEEE CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Multispectral Imaging Systems for Airborne Remote Sensing to Support Agricultural Production Management", "author": ["Y. Huang"], "venue": "Int. J. Agricultural & Bio. Eng.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Self-Taught Feature Learning for Hyperspectral Image Classification", "author": ["R. Kemker", "C. Kanan"], "venue": "IEEE Trans. Geosci. Remote Sens., 2017", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "The CFAR Adaptive Subspace Detector is a Scale-invariant GLRT", "author": ["S. Kraut", "L.L. Scharf"], "venue": "IEEE Trans. Sig. Process.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "The Spectral Image Processing System (SIPS)Interactive Visualization and Analysis of Imaging Spectrometer Data", "author": ["F. Kruse"], "venue": "Remote Sens. of Env.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1993}, {"title": "Multispectral Remote Sensing from Unmanned Aircraft: Image Processing Workflows and Applications for Rangeland Environments", "author": ["A.S. Laliberte"], "venue": "Remote Sensing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Microsoft COCO: Common Objects in Context", "author": ["T. Lin"], "venue": "CoRR, abs/1405.0312,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Urban Object Detection and 3D Building Reconstruction", "author": ["J. Meidow"], "venue": "ISPRS J. Photo. & Remote Sens.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Improving Hyperspectral Image Classification by Combining Spectral, Texture, and Shape Features", "author": ["F. Mirzapour", "H. Ghassemian"], "venue": "Int. J. Remote Sens.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Learning to Refine Object Segments", "author": ["P.O. Pinheiro"], "venue": "In Euro. Conf. on Comp. Vis.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky"], "venue": "Int. J. Comp. Vis.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Three-dimensional Gabor Wavelets for Pixel-based Hyperspectral Imagery Classification", "author": ["L. Shen", "S. Jia"], "venue": "IEEE Trans. Geosci. Remote Sens.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Imaging Chlorophyll Fluorescence with an Airborne Narrow-band Multispectral Camera for Vegetation Stress Detection", "author": ["P. Zarco-Tejada"], "venue": "Remote Sens. of Env.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "On combining multiscale deep learning features for the classification of hyperspectral remote sensing imagery", "author": ["W. Zhao", "Z. Guo", "J. Yue", "X. Zhang", "L. Luo"], "venue": "Int. J. Remote Sens.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "In computer vision literature, semantic segmentation has made significant progress due to deep-convolutional neural networks (DCNNs) [23, 10] trained with large quantities of labeled imagery [21].", "startOffset": 133, "endOffset": 141}, {"referenceID": 5, "context": "In computer vision literature, semantic segmentation has made significant progress due to deep-convolutional neural networks (DCNNs) [23, 10] trained with large quantities of labeled imagery [21].", "startOffset": 133, "endOffset": 141}, {"referenceID": 16, "context": "In computer vision literature, semantic segmentation has made significant progress due to deep-convolutional neural networks (DCNNs) [23, 10] trained with large quantities of labeled imagery [21].", "startOffset": 191, "endOffset": 195}, {"referenceID": 17, "context": "If the folds are randomly sampled, the training data could contaminate the test data, which would artificially inflate classification performance [22, 19, 25].", "startOffset": 146, "endOffset": 158}, {"referenceID": 14, "context": "If the folds are randomly sampled, the training data could contaminate the test data, which would artificially inflate classification performance [22, 19, 25].", "startOffset": 146, "endOffset": 158}, {"referenceID": 20, "context": "If the folds are randomly sampled, the training data could contaminate the test data, which would artificially inflate classification performance [22, 19, 25].", "startOffset": 146, "endOffset": 158}, {"referenceID": 18, "context": "Image classification frameworks, such as VGG-16 [23] and ResNet [10], are trained on the massive ImageNet dataset[21].", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "Image classification frameworks, such as VGG-16 [23] and ResNet [10], are trained on the massive ImageNet dataset[21].", "startOffset": 64, "endOffset": 68}, {"referenceID": 16, "context": "Image classification frameworks, such as VGG-16 [23] and ResNet [10], are trained on the massive ImageNet dataset[21].", "startOffset": 113, "endOffset": 117}, {"referenceID": 2, "context": "State-of-the-art semantic segmentation frameworks transfer the weights from DCNNs trained on ImageNet and fine-tune them on a much smaller semantic segmentation dataset like PASCAL VOC [7] or MS COCO[17].", "startOffset": 185, "endOffset": 188}, {"referenceID": 12, "context": "State-of-the-art semantic segmentation frameworks transfer the weights from DCNNs trained on ImageNet and fine-tune them on a much smaller semantic segmentation dataset like PASCAL VOC [7] or MS COCO[17].", "startOffset": 199, "endOffset": 203}, {"referenceID": 13, "context": "There are also several MSI datasets publicly available, including (but not limited to) semantic segmentation challenges hosted by the International Society for Photogrammetry and Remote Sensing (ISPRS) [18], the 2016 IEEE Geoscience and Remote Sensing Society (GRSS) data fusion contest[1], and the Satellite Imagery Feature Detection challenge on Kaggle[2].", "startOffset": 202, "endOffset": 206}, {"referenceID": 6, "context": "The authors in [11] characterized three MSI payloads on numerous applications including crop health sensing, variable-rate application prescription, irrigation engineering, and crop-field variability.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "The same sensor used to build the dataset presented in this paper has also been used on-board UASs to assess crop stress by measuring the variability in chlorophyll fluorescence [24] and through the acquisition of other biophysical parameters[5].", "startOffset": 178, "endOffset": 182}, {"referenceID": 0, "context": "The same sensor used to build the dataset presented in this paper has also been used on-board UASs to assess crop stress by measuring the variability in chlorophyll fluorescence [24] and through the acquisition of other biophysical parameters[5].", "startOffset": 242, "endOffset": 245}, {"referenceID": 11, "context": "The sensor used here has also been used to perform vegetation classification on orthomosaic imagery[16].", "startOffset": 99, "endOffset": 103}, {"referenceID": 3, "context": "We chose the LIBLINEAR [8] implementation of SVM for its speed and stability with large datasets.", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "MICA is an unsupervised low-level spatial-spectral feature extractor that learns a set of Gabor-type bar/edge and color-opponency detectors from natural images [12].", "startOffset": 160, "endOffset": 164}, {"referenceID": 7, "context": "Full details are available in [12].", "startOffset": 30, "endOffset": 34}, {"referenceID": 7, "context": "SCAE has a deeper neural network architecture than MICA and is capable of extracting higher-level features [12].", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "The reconstruction error is reduced by using skip connections from the feedforward network, inspired by [20].", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": "The MLP was trained using the NAdam optimizer [13] with a batch size of 256 and the class weighted update in Equation 1 where \u03bc = 0.", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "Table 7 has some benchmark results using commonly-used signature matched target detection algorithms, including the spectral angle mapper (SAM) [15], spectral-matched filter (SMF) [6], constrainedenergy minimization (CEM) [9], and adaptive-cosine esti-", "startOffset": 144, "endOffset": 148}, {"referenceID": 1, "context": "Table 7 has some benchmark results using commonly-used signature matched target detection algorithms, including the spectral angle mapper (SAM) [15], spectral-matched filter (SMF) [6], constrainedenergy minimization (CEM) [9], and adaptive-cosine esti-", "startOffset": 180, "endOffset": 183}, {"referenceID": 4, "context": "Table 7 has some benchmark results using commonly-used signature matched target detection algorithms, including the spectral angle mapper (SAM) [15], spectral-matched filter (SMF) [6], constrainedenergy minimization (CEM) [9], and adaptive-cosine esti-", "startOffset": 222, "endOffset": 225}, {"referenceID": 9, "context": "mator (ACE) [14].", "startOffset": 12, "endOffset": 16}], "year": 2017, "abstractText": "Unmanned aircraft have decreased the cost required to collect remote sensing imagery, which has enabled researchers to collect high-spatial resolution data from multiple sensor modalities more frequently and easily. The increase in data will push the need for semantic segmentation frameworks that are able to classify non-RGB imagery, but this type of algorithmic development requires an increase in publicly available benchmark datasets with class labels. In this paper, we introduce a high-resolution multispectral dataset with image labels. This new benchmark dataset has been pre-split into training/testing folds in order to standardize evaluation and continue to push stateof-the-art classification frameworks for non-RGB imagery.", "creator": "LaTeX with hyperref package"}}}