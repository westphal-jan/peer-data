{"id": "1703.02019", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Performing Stance Detection on Twitter Data using Computational Linguistics Techniques", "abstract": "As humans, we can often detect from a persons utterances if he or she is in favor of or against a given target entity (topic, product, another person, etc). But from the perspective of a computer, we need means to automatically deduce the stance of the tweeter, given just the tweet text. In this paper, we present our results of performing stance detection on twitter data using a supervised approach. We begin by extracting bag-of-words to perform classification using TIMBL, then try and optimize the features to improve stance detection accuracy, followed by extending the dataset with two sets of lexicons - arguing, and MPQA subjectivity; next we explore the MALT parser and construct features using its dependency triples, finally we perform analysis using Scikit-learn Random Forest implementation.", "histories": [["v1", "Mon, 6 Mar 2017 18:44:49 GMT  (543kb)", "http://arxiv.org/abs/1703.02019v1", "8 pages, 9 figures, 5 tables"]], "COMMENTS": "8 pages, 9 figures, 5 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["gourav g shenoy", "erika h dsouza", "sandra k\\\"ubler"], "accepted": false, "id": "1703.02019"}, "pdf": {"name": "1703.02019.pdf", "metadata": {"source": "CRF", "title": "Performing Stance Detection on Twitter Data using Computational Linguistics Techniques", "authors": ["Gourav G. Shenoy", "Erika H. Dsouza", "Sandra Kuebler"], "emails": ["skuebler}@indiana.edu"], "sections": [{"heading": "1 Introduction", "text": "In this project, we have used various computational linguistic mechanisms to identify the position. Consider the target-tweet pair: Goal: Atheism Tweet: Faith means believing in something without evidence like a fool. As human beings, we can easily interpret the tweet as being AGAINST the target. We are aiming for a model that would automatically detect the steadfastness of the tweet. To detect the attitude, the workflow of the model should be able to identify the relevant parts and often not be present in the entire tweet. For example, if a person continuously quotes from the Bible in his / her tweets, the model should recognize that the person is against atheism and rather against belief in God. Due to this fact, the model is explicitly given a corpus for the goal that would significantly contribute to the use of two different sentient information to recognize feelings."}, {"heading": "2 Data", "text": "The training data consists of 2913 tweets in total and the test data from 1956 tweets in total. The training data belong to six objectives: \"atheism,\" \"climate change is a real concern,\" \"feminist movement,\" \"Hillary Clinton\" and \"legalization of abortion.\" The test data has an additional goal: \"Donald Trump.\" No training data is explicitly provided for this goal, but a large number of tweets seem to be associated with it. Each of the tweets has a possible position label: \u25cf FAVOR: This label indicates that the twitter is for the target. \u25cf AGAINST: This label indicates that the twitter is not the favorite of the target. \u25cf NONE: This label is given when none of the above representations can be made."}, {"heading": "3 Our Approach", "text": "We learned a new hypothesis for each of the 6 objectives (hillary, donald, feminism, legalization of abortion, climate and atheism) by training a different model for each of them, using the standard k-NN algorithm used by TIMBL. These methods, along with the results, are explained in detail below from Section 3.1 to 3.8. Interestingly, for these models we constructed the feature vectors using the following 3 strategies: \u25cf NP Extracting words from nouns, verbs and adjectives for individual objectives. \u25cf Extracting words with all words for individual objectives. \u25cf Extracting words with nouns, verbs and adjectives, along with the feeling associated with the tweet, for individual objectives - this is an optimization technique. We also tried to analyze the parameters of TIMBL (k-NN algorithm)."}, {"heading": "3.2 Classification using TIMBL default settings", "text": "TiMBL is an open source software package that implements several memory-based learning algorithms, including IB1-IG, an implementation of the k-next neighbor classification with feature weighting for symbolic attribute spaces. What all the implemented algorithms have in common is that they explicitly store some representation of the training set in memory. During the test, new cases are classified by extrapolation from the most similar stored cases. BOW was used as input into the TiMBL software using its default settings, and the command used was: $timbl -f train.csv -t test.csv > output.txtFigure 1: TiMBL accuracy for all targets with default settings Here we observed that Timbl gave quite good accuracy for the default setting (KNN with k = 1), which ranged from 45% (for feminist movement) to 74% (Hillary Clinton)."}, {"heading": "3.3 Tuning TIMBL parameters to improve accuracy", "text": "TIMBL accepts a parameter k as an argument on the command line that represents the number of closest neighbors in the k-NN algorithm used. TIMBL uses a default value of k = 1, which will definitely match the training data, since there are a large number of small clusters that are formed with a next neighbor, and the model cannot generalize to new test data points. Therefore, we have tried to optimize this k value to observe the changes in accuracy. We noticed that by changing the k value, the accuracy for higher values increases from k to a point after which it decreases and then remains constant after a point. Example: for category \"atheism,\" the accuracy with the defaults of TIMBL is about 69%. But if we increase the k value beyond 11, the accuracy of k = 13 increases, while the accuracy of the measurement of TIMB1 decreases. Figure 2 shows the results for multiple k values, for atheism."}, {"heading": "3.5 Constructing Bag-of-Words using all words", "text": "The earlier analysis in Section 3.2 showed Timbl results for characteristics that were nouns, verbs, and adjectives. In this section, we compare the Timbl results when we use all the words. We observe that if we use all the words in our analysis, the accuracy decreases by a certain percentage for each target except Donald Trump."}, {"heading": "3.6 Extending the dataset with MPQA subjectivity lexicons", "text": "\"We feel that we are able to capture the word when we use it,\" he said. \"We feel that it is a way in which we use it.\" \"We have noted the polarity of that word in the Lexicon file.\" \"If the polarity in the Lexicon file is\" positive, \"we associate a\" + 1 \"with the function.\" \"If the polarity is negative.\" \"We provide weights to our features vectors based on the polarity of the words and try to learn a model.\" \"Through the use of strategies we are able to capture word-by-word meta information, which we greatly contribute to the polarity of the words and try to learn a model.\""}, {"heading": "4 Scikit-learn Python Analysis", "text": "Scikit-learn provides a set of monitored and unattended learning algorithms through a unified user interface in Python. The library is based on the SciPy (Scientific Python) that needs to be installed before you can use Scikit learning. In this section, we analyze the performance of the implementation of Scikit-learn Random Forest because all the experiments we conducted in Sections 3.1 through 3.8 are Random Forests or Random Decision Forests an ensemble learning method for classification, regression and other tasks that work by constructing a variety of decision trees at training time and output the class that is the mode for the classes (classification) or the average prediction (regression) of the individual trees. Random Decision Forests correct the habit of decision trees to revise their training set."}, {"heading": "4.1 Scikit-learn with Random Forest", "text": "We executed Scikit-learn's Random Forest implementation with the same feature vectors used in Sections 3.1 through 3.8, and recorded the accuracy for each target. We used this approach because the only difference between Sections 3 and 4 is the machine learning model used to learn the hypothesis. In Section 3, we used TIMBL (and associated k-NN) to perform the learning, while in this section, we use Random Forest (RF) provided by the Scikit Learn Python package."}, {"heading": "4.1.1 Scikit RF with bag-of-words as features", "text": "We observed similar behavior in the accuracy of position recognition of the Scikit learn RF algorithm; features with 3 POS tags (nouns, adjectives, verbs) gave the vectors a better meaning, resulting in a higher overall accuracy in position recognition for each target. Table 4 confirms this observation. Note: We used standard parameters for Scikit-learn Random Forest."}, {"heading": "4.1.2 Scikit RF with additional lexicons", "text": "After adding MPQA to the dataset and arguing lexicographs, we used the same feature vectors for training as in Section 3.6, 3.7. Results showed similar behavior as expected; features with MPQA lexicographs performed better than those with arguments. Table 5 lists the accuracy as a reference."}, {"heading": "4.1.3 Scikit RF with MALT output as features", "text": "We had a similar observation: the features generated from the MALT parser output did little to improve the accuracy of position recognition."}, {"heading": "5 Conclusion", "text": "We discussed the various computational linguistic mechanisms we used to position the Twitter data for 6 different goals (Hillary, Donald, feminism, legalization of abortion, climate, and atheism).The basic idea we used was machine learning to learn a new hypothesis for each of the 6 goals by training a different model for each, using the default k-NN algorithm of TIMBL. We started by extracting words from nouns, adjectives, verbs (tagging with TnT) for individual goals and constructing feature vectors to train our model. We performed the classification using the default TIMBL and observed that we achieved good accuracy with these attributes. We tried to use all words as attributes, but found that accuracy declined drastically. Tuning the TIMBL parameters, which is the value of K, but we did contribute to improving accuracy."}, {"heading": "Acknowledgments", "text": "We would like to thank Prof. Sandra Kuebler and Associate Lecturer Ms. Atreyee Mukherjee for all their help, support and guidance during the project, which would not have been possible without their help. We would also like to thank the Faculty of Computer Science and Computer Science and the Department of Linguistics at Indiana University for providing us with the infrastructure to work on this project. Finally, we would like to thank all class members for their ongoing feedback and help in discussions when needed."}], "references": [], "referenceMentions": [], "year": 0, "abstractText": "As humans, we can often detect from a person\u2019s utterances if he/she is in favor of or against a given target entity (topic, product, another person, etc). But from the perspective of a computer, we need means to automatically deduce the stance of the tweeter, given just the tweet text. In this paper, we present our results of performing stance detection on twitter data using a supervised approach. We begin by extracting bag-of-words to perform classification using TIMBL, then try and optimize the features to improve stance detection accuracy, followed by extending the dataset with two sets of lexicons arguing, and MPQA subjectivity; next we explore the MALT parser and construct features using its dependency triples, finally we perform analysis using Scikit-learn Random Forest implementation.", "creator": null}}}