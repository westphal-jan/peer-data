{"id": "1703.02136", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "English Conversational Telephone Speech Recognition by Humans and Machines", "abstract": "One of the most difficult speech recognition tasks is accurate recognition of human to human communication. Advances in deep learning over the last few years have produced major speech recognition improvements on the representative Switchboard conversational corpus. Word error rates that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues - what IS human performance, and how far down can we still drive speech recognition error rates? A recent paper by Microsoft suggests that we have already achieved human performance. In trying to verify this statement, we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own English conversational telephone LVCSR system to the level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000 evaluation, which - at least at the writing of this paper - is a new performance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multi-task learning and a third residual net (ResNet) with 25 convolutional layers and time-dilated convolutions. On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models.", "histories": [["v1", "Mon, 6 Mar 2017 22:37:43 GMT  (106kb,D)", "http://arxiv.org/abs/1703.02136v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["george saon", "gakuto kurata", "tom sercu", "kartik audhkhasi", "samuel thomas", "dimitrios dimitriadis", "xiaodong cui", "bhuvana ramabhadran", "michael picheny", "lynn-li lim", "bergul roomi", "phil hall"], "accepted": false, "id": "1703.02136"}, "pdf": {"name": "1703.02136.pdf", "metadata": {"source": "CRF", "title": "English Conversational Telephone Speech Recognition by Humans and Machines", "authors": ["George Saon", "Gakuto Kurata", "Tom Sercu", "Kartik Audhkhasi", "Samuel Thomas", "Dimitrios Dimitriadis", "Xiaodong Cui", "Bhuvana Ramabhadran", "Michael Picheny", "Lynn-Li Lim", "Bergul Roomi", "Phil Hall"], "emails": [], "sections": [{"heading": null, "text": "Word error rates, which were 14% just a few years ago, have fallen to 8.0% in recent years, then to 6.6%, and finally to 5.8%, and are now thought to be within striking range of human performance, raising two questions - what is human performance, and how far can we go to reduce speech recognition error rates? A recent paper by Microsoft suggests that we have already reached human performance. In trying to verify this statement, we conducted a series of measurements of human performance in two conversation tasks and found that human performance can be significantly better than previously reported, giving the community a much more difficult goal. We also report on our own efforts in this area, and present a set of acoustic and speech modeling techniques that reduce the word error rate in our own voice conversations to the level of 5.5% / 10.3%. STMs report our own efforts in this area by presenting a set of acoustic and language modeling techniques that reduce the word error rate in our own language conversations to 10.5% / 10.3%."}, {"heading": "1. Introduction", "text": "With the performance of ASR systems moving ever closer to that of humans, it is important to measure human performance more accurately. In [1], the authors claim a human word error rate (WER) of 5.9% / 11.3% on the Switchboard / CallHome subset (SWB / CH) of the NIST Hub5 2000 evaluation test. Compared to [2], which quotes a WHO of 4%, the estimate of 5.9% appeared quite high (though measured on other data).This intriguing discrepancy prompted us to launch our own human transcription effort to confirm (or refute) the estimates of [1].The results from this effort were doubly surprising. First, we expected the SWB measurement to be closer to the Lippmann estimate of 4%, but we could only come down to 5.1% for the best transcription systems. Second, the same transcriber achieved a surprisingly low WER figure of 6.8% (we expected a much higher WER for a Home system of 11.8%)."}, {"heading": "2. Human transcription experiments", "text": "These experiments were conducted by IBM's preferred language transcription provider Appen. The transcription protocol agreed upon consisted of three independent transcribers, vetted by a fourth senior transcriptionist. All four transcriptionists are native U.S. English Xiv: 170 3.02 136v 1 [cs.C L] 6M ar2 017 loudspeakers and were selected based on the quality of their work in past transcription projects. Transcription participants were familiarized with the LDC transcription guidelines covering syllables, abbreviations, subwords, speech sounds, etc. Transcription time was estimated to be 12-14 times in real time (xRT) for the first pass for transcribers 1-3 and pass an additional 1.7-2xRT for the second quality check (of transcribers, subscribers, non-speakers, etc.)."}, {"heading": "3. System improvements", "text": "In this section, we discuss the training data and test kits used, as well as improvements in acoustic and speech modeling. The training kit for our acoustic models consists of 262 hours of Switchboard 1 audio with transcripts provided by Mississippi State University, 1698 hours of Fisher data collection, and 15 hours of CallHome audio. To allay fears that we may be overfilling Hub5-2000 test kits with extensive testing, we have decided to report the results of a variety of test kits. As the RT '02, RT' 03, RT '04, and DEV' 04f test kits have not been used for more than a decade, we are fairly certain that performance improvements in these test kits indicate real progress. Statistics on all the test kits used in the experiments are listed in Table 4.In [7] that both revolutionary and non-revolutionary AMs exhibit comparable performance and good complementarity. Therefore, the strategy for our previous Net5 systems [8 year] we have replaced the STM and STM architecture with a combination of STM and STM architecture in the last year."}, {"heading": "3.1. LSTM acoustic models", "text": "All the models presented here are built in Torch [11] with DNN 5.0. Their architecture consists of 4-6 bidirectional layers with 1024 cells per layer (512 per direction), a linear bot-tleneck layer with 256 units, and an output level with 32K units corresponding to as many context-dependent HMM states (shown on the left side of Figure 1). Training is done on non-overlapping size 128 partial sequences for processing speed and reliable gradient estimates. Training consists of 14 passes of SGD sequence training using the advanced MMI criteria by adding the scaled gradients of cross-entropy losses."}, {"heading": "3.2. ResNet acoustic models", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "3.3. Model combination", "text": "In Table 7, we report on the performance of the best individual models described in the previous paragraphs, as well as the results obtained by frame-level score fusion in all test kits. All decodings are done with an 85K vocabulary and a 4-gram language model with 36M n-grams. We note that LSTMs and ResNets have a strong complementarity that improves the WHO for all test kits."}, {"heading": "3.4. Language modeling improvements", "text": "In addition to n-gram and Char-LSTM, we used in our previous system [5], we introduced LSTM-based as well as convolutionbased LMs in this paper. We experimented with four LSTM LMs, namely WordLSTM, Char-LSTM-MTL, and Char-LSTM-MTL, each containing four LSTM LMs, two LSTM layers, one fully connected layer, and a softmax layer as shown in Figure 3. The upper LSTM layer and the fully connected layer were wound by residual connections [6]. Dropout was applied only to the vertical dimension and not to the time dimension [25]. We added an additional LSTM layer to the Char-LSTM layer to estimate word embedding from character sequences as shown in Figure 4. Both we used the conventional Word-ChM-STM and LSTM-STM function."}, {"heading": "4. Conclusion", "text": "On the acoustic side, two things played a crucial role in achieving this level of performance: First, a steady improvement in bidirectional LSTM modeling, most notably a simple experiment on functional fusion; second, the replacement of VGG networks with residual networks, which represent a more effective architecture in the ImageNet classification task; and, in combination, these recurring and convolutionary networks show good complementarity and improved accuracy in a variety of test sets. On the linguistic modeling side, we used the same complementarity between recurring and evolutionary architectures by adding word and character-based LSTM LMs and a Convolutionary WaveNet LM. Second, and perhaps more important, in this paper, we did not believe that human equality was achieved in this task, contrary to what was stated in [1]."}, {"heading": "5. References", "text": "[1] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig S. 2016, \"Achieving human parity in conversations speech recognition,\" arXiv preprint arXiv: 1610.05256, 2016. [2] R. P. Lippmann, \"Speech recognition by machines and humans,\" Speech communication, vol. 22, pp. 1-15, 1997. [3] J. Fiscus, W. M. Fisher, A. F. Przybocki, and D. S. Pallett, \"2000 nist evaluation of conversations speech recognition over the telephone: English and Mandarin performance results, in Proc. Speech Transcription Workshop. Citeseer, pp. 1-5. [4] K. Simonyan and A. Zisserman\" Very deep convolu-tional networks for large-scale image recognition,. \""}], "references": [{"title": "Achieving human parity in conversational speech recognition", "author": ["W. Xiong", "J. Droppo", "X. Huang", "F. Seide", "M. Seltzer", "A. Stolcke", "D. Yu", "G. Zweig"], "venue": "arXiv preprint arXiv:1610.05256, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech recognition by machines and humans", "author": ["R.P. Lippmann"], "venue": "Speech communication, vol. 22, no. 1, pp. 1\u201315, 1997.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "2000 nist evaluation of conversational speech recognition over the telephone: English and mandarin performance results", "author": ["J. Fiscus", "W.M. Fisher", "A.F. Martin", "M.A. Przybocki", "D.S. Pallett"], "venue": "Proc. Speech Transcription Workshop. Citeseer, 2000, pp. 1\u20135.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Very deep convolu-  tional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR arXiv:1409.1556, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "The IBM 2016 English conversational speech recognition system", "author": ["G. Saon", "T. Sercu", "S. Rennie", "H.-K. Kuo"], "venue": "Seventeenth Annual Conference of the International Speech Communication Association, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. CVPR, 2016, pp. 770\u2013 778.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint training of convolutional and non-convolutional neural networks", "author": ["H. Soltau", "G. Saon", "T.N. Sainath"], "venue": "to Proc. ICASSP, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "The IBM 2015 English conversational speech recognition system", "author": ["G. Saon", "H.-K. Kuo", "S. Rennie", "M. Picheny"], "venue": "Sixteenth Annual Conference of the International Speech Communication Association, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Boosted MMI for model and feature-space discriminative training", "author": ["D. Povey", "D. Kanevsky", "B. Kingsbury", "B. Ramabhadran", "G. Saon", "K. Visweswariah"], "venue": "Proc. of ICASSP, 2008, pp. 4057\u20134060.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Error back propagation for sequence training of context-dependent deep networks for conversational speech transcription", "author": ["H. Su", "G. Li", "D. Yu", "F. Seide"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop, no. EPFL-CONF-192376, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1923}, {"title": "Domain-adversarial training of neural networks", "author": ["Y. Ganin", "E. Ustinova", "H. Ajakan", "P. Germain", "H. Larochelle", "F. Laviolette", "M. Marchand", "V. Lempitsky"], "venue": "Journal of Machine Learning Research, vol. 17, no. 59, pp. 1\u201335, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial multi-task learning of deep neural networks for robust speech recognition", "author": ["Y. Shinohara"], "venue": "Interspeech 2016, pp. 2369\u20132372, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Y. Miao", "M. Gowayyed", "F. Metze"], "venue": "arXiv preprint arXiv:1507.08240, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning acoustic frame labeling for speech recognition with recurrent neural networks", "author": ["H. Sak", "A. Senior", "K. Rao", "O. Irsoy", "A. Graves", "F. Beaufays", "J. Schalkwyk"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4280\u20134284.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition", "author": ["H. Soltau", "H. Liao", "H. Sak"], "venue": "arXiv preprint arXiv:1610.09975, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Gram-ctc: Automatic unit selection and target decomposition for sequence labelling", "author": ["L. Hairong", "Z. Zhu", "X. Li", "S. Satheesh"], "venue": "CoRR arXiv:1703.00096, 2017.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR arXiv:1512.03385, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for end-to-end speech recognition", "author": ["Y. Zhang", "W. Chan", "N. Jaitly"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Wide residual networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "arXiv preprint arXiv:1605.07146, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1603.05027, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep multilingual convolutional neural networks for lvcsr", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "Proc. ICASSP, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Advances in very deep convolutional neural networks for lvcsr", "author": ["T. Sercu", "V. Goel"], "venue": "arXiv, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Dense prediction on sequences with time-dilated convolutions for speech recognition", "author": ["\u2014\u2014"], "venue": "NIPS End-to-end Learning for Speech and Audio Processing Workshop, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["W. Ling", "T. Lu\u0131\u0301s", "L. Marujo", "R.F. Astudillo", "S. Amir", "C. Dyer", "A.W. Black", "I. Trancoso"], "venue": "arXiv preprint arXiv:1508.02096, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. Desouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational Linguistics, vol. 18, no. 4, pp. 467\u2013479, 1992.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1992}, {"title": "WaveNet: A generative model for raw audio", "author": ["A. v. d. Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1609.03499, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "ADAM: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Self-stabilized deep neural network", "author": ["P. Ghahremani", "J. Droppo"], "venue": "Proc. ICASSP, 2016, pp. 6645\u20136649.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Empirical exploration of LSTM and CNN language models for speech recognition", "author": ["G. Kurata", "A. Sethy", "B. Ramabhadran", "G. Saon"], "venue": "Submitted to Interspeech 2017, 2017.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "In [1], the authors claim a human word error rate (WER) of 5.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "When compared with [2] which quotes a WER of 4%, the 5.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "This intriguing discrepancy prompted us to launch our own human transcription effort in order to confirm (or disconfirm) the estimates from [1].", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "What makes the Switchboard and CallHome testsets so different one might ask? The biggest problem with the SWB testset is that 36 out of 40 test speakers appear in the training data, some in as many as 8 different conversations [3], and our acoustic models are very good at memorizing speech patterns seen during training.", "startOffset": 227, "endOffset": 230}, {"referenceID": 3, "context": "Additionally, replacing the VGG convolutional nets [4] that we had in our last year\u2019s system [5] with ResNets [6] turned out to be beneficial for performance.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "Additionally, replacing the VGG convolutional nets [4] that we had in our last year\u2019s system [5] with ResNets [6] turned out to be beneficial for performance.", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "Additionally, replacing the VGG convolutional nets [4] that we had in our last year\u2019s system [5] with ResNets [6] turned out to be beneficial for performance.", "startOffset": 110, "endOffset": 113}, {"referenceID": 0, "context": "In Table 1 we show the error rates of the three transcribers before and after quality checking by the fourth transcriber as well as the human WER reported in [1].", "startOffset": 158, "endOffset": 161}, {"referenceID": 0, "context": "Human WER from [1] 5.", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "Table 1: Word error rates on SWB and CH for human transcribers before and after quality checking contrasted with the human WER reported in [1].", "startOffset": 139, "endOffset": 142}, {"referenceID": 0, "context": "While many of the errors look similar to those reported in [1], there is a glaring discrepancy in the frequency of top deletions for CallHome between our human transcript and theirs.", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "In [7], we have shown that convolutional and nonconvolutional AMs have comparable performance and good complementarity.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "Hence, the strategy for our previous systems [8, 5] was to use a combination of recurrent and convolutional nets.", "startOffset": 45, "endOffset": 51}, {"referenceID": 4, "context": "Hence, the strategy for our previous systems [8, 5] was to use a combination of recurrent and convolutional nets.", "startOffset": 45, "endOffset": 51}, {"referenceID": 8, "context": "The training consists of 14 passes of cross-entropy followed by 1 pass of SGD sequence training using the boosted MMI criterion [9] smoothed by adding the scaled gradient of the cross-entropy loss [10].", "startOffset": 128, "endOffset": 131}, {"referenceID": 9, "context": "The training consists of 14 passes of cross-entropy followed by 1 pass of SGD sequence training using the boosted MMI criterion [9] smoothed by adding the scaled gradient of the cross-entropy loss [10].", "startOffset": 197, "endOffset": 201}, {"referenceID": 10, "context": "Implementation of the LSTM was done in Torch [11] with cuDNN v5.", "startOffset": 45, "endOffset": 49}, {"referenceID": 4, "context": "The first two improvements are fairly banal and consist in increasing the number of layers from 4 (like in our previous model [5]) to 6 and in realigning the training data with a 6-layer LSTM and retraining another LSTM.", "startOffset": 126, "endOffset": 129}, {"referenceID": 11, "context": "In [12], the authors introduce domain-adversarial neural networks which are models that are trained to not distiguish between in-domain, labeled data and out-of-domain, unlabeled data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "This idea has been successfully applied in speech by [13] in the context of noise robustness where the author proposes noise-adversarial MTL to suppress the effects of noise.", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "One possibility was to train an end-to-end LSTM using CTC as in [14, 15, 16, 17] and append the features from the last LSTM layer before the softmax to our existing features.", "startOffset": 64, "endOffset": 80}, {"referenceID": 14, "context": "One possibility was to train an end-to-end LSTM using CTC as in [14, 15, 16, 17] and append the features from the last LSTM layer before the softmax to our existing features.", "startOffset": 64, "endOffset": 80}, {"referenceID": 15, "context": "One possibility was to train an end-to-end LSTM using CTC as in [14, 15, 16, 17] and append the features from the last LSTM layer before the softmax to our existing features.", "startOffset": 64, "endOffset": 80}, {"referenceID": 16, "context": "One possibility was to train an end-to-end LSTM using CTC as in [14, 15, 16, 17] and append the features from the last LSTM layer before the softmax to our existing features.", "startOffset": 64, "endOffset": 80}, {"referenceID": 16, "context": "Finally, we note that the feature fusion LSTM compares favorably with other single acoustic models from the literature as mentioned in [17] (Table 4).", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": "Residual Networks were introduced for image recognition in [18] and used in speech recognition in [1, 19].", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "Residual Networks were introduced for image recognition in [18] and used in speech recognition in [1, 19].", "startOffset": 98, "endOffset": 105}, {"referenceID": 18, "context": "Residual Networks were introduced for image recognition in [18] and used in speech recognition in [1, 19].", "startOffset": 98, "endOffset": 105}, {"referenceID": 19, "context": "We achieved best results with basic residual blocks without bottleneck, similar to the observations from [20] on CIFAR and SVHN experiments.", "startOffset": 105, "endOffset": 109}, {"referenceID": 21, "context": "We perform data-balancing according to [22] with exponent \u03b3 = 0.", "startOffset": 39, "endOffset": 43}, {"referenceID": 20, "context": "We use full pre-activation identity shortcut connections which keep a clean information path [21] without nonlinearity after addition.", "startOffset": 93, "endOffset": 97}, {"referenceID": 23, "context": "For batch normalization the statistics are accumulated per feature map and per frequency bin following [24].", "startOffset": 103, "endOffset": 107}, {"referenceID": 22, "context": "Padding along the time direction would modify the values on the edges based on the input sliding window location, thus making efficient convolution over a full utterance impossible (see [23]).", "startOffset": 186, "endOffset": 190}, {"referenceID": 23, "context": "Secondly, along the time direction, strided convolutions and strided pooling is optional, but was found to improve performance [24].", "startOffset": 127, "endOffset": 131}, {"referenceID": 23, "context": "When transitioning from cross-entropy (XE) to sequence training (ST), we want to modify our network to do dense prediction efficiently [24].", "startOffset": 135, "endOffset": 139}, {"referenceID": 23, "context": "We can achieve this by using time-dilated convolutions according to the same recipe as in [24]: for each layer which originally strides in time with factor 2, set time-stride to 1 and dilate with factor 2 all consecutive convolutions, maxpooling and fully connected layers.", "startOffset": 90, "endOffset": 94}, {"referenceID": 8, "context": "We sequence trained this model for 200M frames with the boosted MMI criterion [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "In addition to n-gram and model-M used in our previous system [5], we introduced LSTM-based as well as convolutionbased LMs in this paper.", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "The upper LSTM layer and the fullyconnected layer were wrapped by residual connections [6].", "startOffset": 87, "endOffset": 90}, {"referenceID": 24, "context": "Dropout was only applied to the vertical dimension and not applied to the time dimension [25].", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "The Char-LSTM added an additional LSTM layer to estimate word-embeddings from character sequences as illustrated in Figure 4 [26].", "startOffset": 125, "endOffset": 129}, {"referenceID": 26, "context": "We first clustered the vocabulary using Brown clustering [27].", "startOffset": 57, "endOffset": 61}, {"referenceID": 27, "context": "Inspired by the complementarity of convolutional and non-convolutional acoustic models, we experimented with a convolution-based LM in the form of dilated causal convolution as used in WAVENET [28].", "startOffset": 193, "endOffset": 197}, {"referenceID": 4, "context": "\u2022 We used the same vocabulary of 85K words from [5].", "startOffset": 48, "endOffset": 51}, {"referenceID": 28, "context": "\u2022 We controlled the leaning rate by ADAM [29] and introduced a self-stabilization term to coordinate the layerwise learning rates [30].", "startOffset": 41, "endOffset": 45}, {"referenceID": 29, "context": "\u2022 We controlled the leaning rate by ADAM [29] and introduced a self-stabilization term to coordinate the layerwise learning rates [30].", "startOffset": 130, "endOffset": 134}, {"referenceID": 30, "context": "More details about the language modeling are given in a companion paper [31].", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "The second and perhaps more important point made in this paper is that, unlike what was claimed in [1], we do not believe that human parity has been reached on this task.", "startOffset": 99, "endOffset": 102}], "year": 2017, "abstractText": "One of the most difficult speech recognition tasks is accurate recognition of human to human communication. Advances in deep learning over the last few years have produced major speech recognition improvements on the representative Switchboard conversational corpus. Word error rates that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues what IS human performance, and how far down can we still drive speech recognition error rates? A recent paper by Microsoft suggests that we have already achieved human performance. In trying to verify this statement, we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own English conversational telephone LVCSR system to the level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000 evaluation, which at least at the writing of this paper is a new performance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multitask learning and a third residual net (ResNet) with 25 convolutional layers and time-dilated convolutions. On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models.", "creator": "LaTeX with hyperref package"}}}