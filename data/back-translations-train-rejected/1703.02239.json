{"id": "1703.02239", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Functions that Emerge through End-to-End Reinforcement Learning - The Direction for Artificial General Intelligence -", "abstract": "Recently, triggered by the impressive results in TV-games or game of Go by Google DeepMind, end-to-end reinforcement learning (RL) is collecting attentions. Although little is known, the author's group has propounded this framework for around 20 years and already has shown a variety of functions that emerge in a neural network (NN) through RL. In this paper, they are introduced again at this timing.", "histories": [["v1", "Tue, 7 Mar 2017 06:51:19 GMT  (1398kb)", "http://arxiv.org/abs/1703.02239v1", "5 pages, 4 figures"], ["v2", "Tue, 16 May 2017 07:22:07 GMT  (1385kb)", "http://arxiv.org/abs/1703.02239v2", "The Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM) 2017, 5 pages, 4 figures"]], "COMMENTS": "5 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["katsunari shibata"], "accepted": false, "id": "1703.02239"}, "pdf": {"name": "1703.02239.pdf", "metadata": {"source": "CRF", "title": "Functions that Emerge through End-to-endReinforcement Learning", "authors": ["Katsunari Shibata"], "emails": ["shibata@oita-u.ac.jp"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.02 239v 1 [cs.A I] 7 Mar 201 7 \"Function Modularization\" approach is subconsciously deeply penetrated. The inputs and outputs of a learning system can be raw sensor signals and motor commands. \"State Space\" or \"Action Space,\" commonly used in RL, indicate the existence of functional modules. This has limited amplification of learning only for the action planning module. In order to extend the amplification learning to learning of the entire function to an enormous degree of freedom of a massively parallel learning system and to explain or develop human intelligence, the author believed that an end-to-end RL of sensors on motors with a recursive NN (RNN) will be an essential key. Especially in the higher functions, since their inputs or outputs are difficult to decide, this approach is very effective by being free from the need to decide them. The functions that arise we cover through RL using a wide NL prediette of a learning palette."}, {"heading": "Acknowledgements", "text": "This research was supported by JSPS KAKENHI grant numbers JP07780305, JP08233204, JP13780295, JP15300064, JP19300070 and many of our group members at http: / / shws.cc.oita-u.ac.jp / \u02dc shibata / home.html."}, {"heading": "1 Introduction", "text": "Recently, triggered by the impressive results in TV games [1, 2] or game of Go [3] by Google DeepMind, the ability of amplifying learning (RL) with a neural network (NN) and the importance of end-to-end RL collects attention. A noteworthy point especially in the results in TV games is the gap that although the inputs of a deep NN are raw image pixels without pre-processing and the NN is learned straight through RL, the ability gained through learning extends to the excellent strategies for multiple games. Learning does not require specific knowledge of learned tasks and necessary functions arise through learning, and so it is expected that it will pave the way to AGI (Artificial General Intelligence) or Strong AI.It has been widely accepted that a NN is only regarded as a non-linear functional approach for RL, and a recurring neural network (RNN) is used to explain POMDP (Partially Observable Intelligence Group at the end of the human problem) under such circumstances."}, {"heading": "2 The Direction for Human-like Intelligence", "text": "There is no reason to worry that this will happen."}, {"heading": "3 Functions that Emerge through Reinforcement Learning (RL)", "text": "Here, functions that have been observed to be created in an NN by RL are introduced in our facts.NN are trained with the training signals automatically produced by RL; Q-Learning, Actor-Critic or Actor-Q (for learning both continuous motion and discrete action).The details can be seen in any reference point.3.1 Static functions that arise in a layered network are used in a layered neural network and trained according to the error propagation."}, {"heading": "3.2 Dynamic Functions that Emerge in a Recurrent Neural Network (RNN)", "text": "In this case, an RNN is used to deal with dynamics, and is trained by BPTT (Back Propagation Through Time) with the training signals automatically generated on RL. Both for memory acquisition and error propagation, the feedback weights are set so that the transition matrix is the identity matrix or is close to it when it is approached linearly. \u00a7 Memory There are some works in which necessary information is extracted, stored and reflected in post-learning behaviors almost solely from the reward or punishment at each target. In [16] a very interesting behavior where unexpected results have occurred, an agent went back to check the condition in the previous phase without any direction. In [17] and [18], real camera image was used as input, and both camera motion and word motion [as well as pattern identification [17] and [18] were removed."}, {"heading": "3.3 Frame-free Function Emergence for AGI (Artificial General Intelligence)", "text": "As already mentioned, when learning [20] an agent with a visual sensor and an RNN, as shown in Fig. 3, learns both the motion and recording time behavior of a moving object, which sometimes happens to be invisible. In a general approach, the object motion is estimated using a gift model, the future object position is predicted, the recording point and time are determined by an optimization method, a reference path is derived from the recording point, and the movements are controlled to follow the trajectory. Fig. 4 shows four RL examples based on the reward for the object acquisition. The agent did not know in advance how to predict the motion, or even the fact that prediction is necessary to catch it. Nevertheless, he moved at the very front of his range of motion, waited for the object, and as the object approached, the agent moved backwards with it and reciprocated the object, even though the agent had made the object unexpectedly visible in the middle of the ball (where everything was expected to fall)."}], "references": [{"title": "Playing Atari with Deep Reinforcement Learning,NIPS", "author": ["V. Minh", "K Kavukcuoglu"], "venue": "Deep Learning Workshop", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Minh", "K. Kavukcuoglu", "D Silver"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Mastering the game of Go with deep neural networks and tree search,Nature", "author": ["D. Silver", "A Huang"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Practical Issues in Temporal Difference", "author": ["G. Tesauro"], "venue": "Learning,Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Reinforcement Learning When Visual Sensory Signals are Directly Given as Inputs", "author": ["K. Shibata", "Y. Okabe"], "venue": "Proc. of ICNN(Int\u2019l Conf. on Neural Networks)97,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Direct-Vision-Based Reinforcement Learning in \u201dGoing to an Target\u201d Task with an Obstacle and with a Variety of Target Sizes", "author": ["K. Shibata", "Y. Okabe", "K. Ito"], "venue": "Proc. of NEURAP(Neural Networks and their Applications)\u201998,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Emergence of Intelligence through Reinforcement Learning with a Neural Network, Advances in Reinforcement Learning, Intech, 99\u2013120", "author": ["K. Shibata"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Acquisition of Box Pushing by Direct-Vision-Based Reinforcement Learning", "author": ["K. Shibata", "M. Iida"], "venue": "Proc. of SICE Annual Conf", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Learning of Action Generation from Raw Camera Images in a Real-World-like Environment by Simple Coupling of Reinforcement Learning and a Neural Network", "author": ["K. Shibata", "T. Kawano"], "venue": "Adv. in Neuro-Information Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Acquisition of Flexible Image Recognition by Coupling of Reinforcement Learning and a Neural Network", "author": ["K. Shibata", "T. Kawano"], "venue": "SICE J. of Control, Measurement, and System Integration,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Emergence of Color Constancy Illusion through Reinforcement Learning with a Neural Network", "author": ["K. Shibata", "S. Kurizaki"], "venue": "Proc. of ICDL-EpiRob(Int\u2019l Conf. on Developmental Learning & Epigenetic Robotics)2012,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Effect of Force Load in Hand Reaching Movement Acquired by Reinforcement Learning", "author": ["K. Shibata", "K. Ito"], "venue": "Proc. of ICONIP(Int\u2019l Conf. on Neural Information", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Hidden Representation after Reinforcement Learning of Hand Reaching Movement with Variable Link Length", "author": ["K. Shibata", "K. Ito"], "venue": "Proc. of IJCNN (Int\u2019l Joint Conf. on Neural Networks)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Spatial Abstraction and Knowledge Transfer in Reinforcement Learning Using a Multi-Layer Neural Network", "author": ["K. Shibata"], "venue": "Proc. of ICDL (5th Int\u2019l Conf. on Development and Learning)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Contextual Behavior and Internal Representations Acquired by Reinforcement Learning with a Recurrent Neural Network", "author": ["H. Utsunomiya", "K. Shibata"], "venue": "Adv. in Neuro-Information Processing, Lecture Notes in Comp. Sci., Proc. of ICONIP", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Discovery of Pattern Meaning from Delayed Rewards by Reinforcement Learning with a Recurrent Neural Network", "author": ["K. Shibata", "H. Utsunomiya"], "venue": "Proc. of IJCNN (Int\u2019l Joint Conf. on Neural Networks)", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Acquisition of Context-Based Active Word Recognition by Q-Learning Using a Recurrent Neural Network, Robot", "author": ["A.A.M. Faudzi", "K. Shibata"], "venue": "Intelligent Technology and Applications,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "2004)Dynamics of a RecurrentNeural NetworkAcquired throughLearning of a Context-basedAttention Task", "author": ["K. Shibata", "M. Sugisaka"], "venue": "Artificial Life and Robotics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Emergence of Flexible Prediction-Based Discrete Decision Making and Continuous Motion Generation through Actor-Q-Learning", "author": ["K. Shibata", "K. Goto"], "venue": "Proc. of ICDL-Epirob (Int\u2019l Conf. on Developmental Learning & Epigenetic Robotics)", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "A model to explain the emergence of reward expectancy neurons using reinforcement learning and neural network,Neurocomputing", "author": ["Ishii S", "M. Shidara", "K. Shibata"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Learning of Deterministic Exploration and Temporal Abstraction in Reinforcement Learning", "author": ["K. Shibata"], "venue": "Proc. of SICE-ICCAS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Acquisition of Deterministic Exploration and Purposive Memory through Reinforcement Learning with a Recurrent Neural Network", "author": ["K. Goto", "K. Shibata"], "venue": "Proc. of SICE Annual Conf. 2010,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Communications that Emerge through Reinforcement Learning Using a Neural Network, RLDM2017", "author": ["K. Shibata"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Recently, triggered by the impressive results in TV-games[1, 2] or game of Go[3] by Google DeepMind, the ability of reinforcement learning (RL) using a neural network (NN) and the importance of end-to-end RL is collecting attentions.", "startOffset": 57, "endOffset": 63}, {"referenceID": 1, "context": "Recently, triggered by the impressive results in TV-games[1, 2] or game of Go[3] by Google DeepMind, the ability of reinforcement learning (RL) using a neural network (NN) and the importance of end-to-end RL is collecting attentions.", "startOffset": 57, "endOffset": 63}, {"referenceID": 2, "context": "Recently, triggered by the impressive results in TV-games[1, 2] or game of Go[3] by Google DeepMind, the ability of reinforcement learning (RL) using a neural network (NN) and the importance of end-to-end RL is collecting attentions.", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "Under such circumstances, the origin of the end-to-end RL can be found in the Tesauro\u2019s work called TD-gammon[4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "The author\u2019s group is the only one who has propounded this framework for around 20 years using the symbolic name of \u201cDirect-Vision-based Reinforcement Learning\u201d[5, 6] and has shown already a variety of functions that emerge in a NN or RNN through RL[7] although little is known about them unfortunately.", "startOffset": 160, "endOffset": 166}, {"referenceID": 5, "context": "The author\u2019s group is the only one who has propounded this framework for around 20 years using the symbolic name of \u201cDirect-Vision-based Reinforcement Learning\u201d[5, 6] and has shown already a variety of functions that emerge in a NN or RNN through RL[7] although little is known about them unfortunately.", "startOffset": 160, "endOffset": 166}, {"referenceID": 6, "context": "The author\u2019s group is the only one who has propounded this framework for around 20 years using the symbolic name of \u201cDirect-Vision-based Reinforcement Learning\u201d[5, 6] and has shown already a variety of functions that emerge in a NN or RNN through RL[7] although little is known about them unfortunately.", "startOffset": 249, "endOffset": 252}, {"referenceID": 18, "context": "The convolutional structure is not used except for [20].", "startOffset": 51, "endOffset": 55}, {"referenceID": 7, "context": "(2003)[8] A layered neural network is used and trained according to error backpropagation (BP), and static functions emerge as follows.", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "\u00a7 Image Recognition Same as [1, 2], images (pixels) were put into a neural network directly as inputs, and appropriate behaviors to get a reward were acquired[5, 6].", "startOffset": 28, "endOffset": 34}, {"referenceID": 1, "context": "\u00a7 Image Recognition Same as [1, 2], images (pixels) were put into a neural network directly as inputs, and appropriate behaviors to get a reward were acquired[5, 6].", "startOffset": 28, "endOffset": 34}, {"referenceID": 4, "context": "\u00a7 Image Recognition Same as [1, 2], images (pixels) were put into a neural network directly as inputs, and appropriate behaviors to get a reward were acquired[5, 6].", "startOffset": 158, "endOffset": 164}, {"referenceID": 5, "context": "\u00a7 Image Recognition Same as [1, 2], images (pixels) were put into a neural network directly as inputs, and appropriate behaviors to get a reward were acquired[5, 6].", "startOffset": 158, "endOffset": 164}, {"referenceID": 7, "context": "That was confirmed also in real robot tasks; Box Pushing (continuous motion)[8] and Kissing AIBO (real-world-like environment)[9, 10] as shown in Fig.", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "That was confirmed also in real robot tasks; Box Pushing (continuous motion)[8] and Kissing AIBO (real-world-like environment)[9, 10] as shown in Fig.", "startOffset": 126, "endOffset": 133}, {"referenceID": 9, "context": "That was confirmed also in real robot tasks; Box Pushing (continuous motion)[8] and Kissing AIBO (real-world-like environment)[9, 10] as shown in Fig.", "startOffset": 126, "endOffset": 133}, {"referenceID": 10, "context": "From the internal representation, we tried to explain the optical illusion of color constancy[11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "Furthermore, adaptation of force field and its after effect were observed[13].", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "\u00a7 Explanation of the Brain Activations during Tool Use From the internal representation after learning of a reaching task with variable link length, we tried to explain the emergence of the activities observed in the monkey brain when the monkey used a tool to get a food[14].", "startOffset": 271, "endOffset": 275}, {"referenceID": 13, "context": "After the agent learned the task using 3 combinations, learning for the remainder sensor-and-motor combination was drastically accelerated[15].", "startOffset": 138, "endOffset": 142}, {"referenceID": 3, "context": "\u00a7 Game Strategy (Not our works, but wonderful results can be seen in [4, 1, 2, 3])", "startOffset": 69, "endOffset": 81}, {"referenceID": 0, "context": "\u00a7 Game Strategy (Not our works, but wonderful results can be seen in [4, 1, 2, 3])", "startOffset": 69, "endOffset": 81}, {"referenceID": 1, "context": "\u00a7 Game Strategy (Not our works, but wonderful results can be seen in [4, 1, 2, 3])", "startOffset": 69, "endOffset": 81}, {"referenceID": 2, "context": "\u00a7 Game Strategy (Not our works, but wonderful results can be seen in [4, 1, 2, 3])", "startOffset": 69, "endOffset": 81}, {"referenceID": 8, "context": "(2008)[9]", "startOffset": 6, "endOffset": 9}, {"referenceID": 14, "context": "In [16], a very interesting behavior in which if unexpected results occurred, an agent went back to check the state in the previous stage without any direction could be observed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "In [17] and [18], real camera image was used as input, and both camera motion and pattern meaning[17] or both camera motion and word recognition[18] were learned.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "In [17] and [18], real camera image was used as input, and both camera motion and pattern meaning[17] or both camera motion and word recognition[18] were learned.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "In [17] and [18], real camera image was used as input, and both camera motion and pattern meaning[17] or both camera motion and word recognition[18] were learned.", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "In [17] and [18], real camera image was used as input, and both camera motion and pattern meaning[17] or both camera motion and word recognition[18] were learned.", "startOffset": 144, "endOffset": 148}, {"referenceID": 15, "context": "Purposive associative memory could be also observed[17].", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "\u00a7 Selective Attention It was learned that based on the previously-presented image, the attended area of the next presented pattern is changed without any special structure for attention and the area of the image was correctly classified[19].", "startOffset": 236, "endOffset": 240}, {"referenceID": 18, "context": "\u00a7 Prediction (explained in the next subsection[20]) \u00a7 Explanation of the Emergence of Reward Expectancy Neurons From a simulation result of RL using an RNN, we tried to explain the emergence of reward expectancy neurons, which responded only in the non-reward trials in a multi-trial task observed in the monkey brain[21].", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "\u00a7 Prediction (explained in the next subsection[20]) \u00a7 Explanation of the Emergence of Reward Expectancy Neurons From a simulation result of RL using an RNN, we tried to explain the emergence of reward expectancy neurons, which responded only in the non-reward trials in a multi-trial task observed in the monkey brain[21].", "startOffset": 317, "endOffset": 321}, {"referenceID": 20, "context": "\u00a7 Exploration Effective and deterministic exploration behavior for ambiguous or invisible goal considering past experiencewas learned and temporal abstraction was discussed[22, 23].", "startOffset": 172, "endOffset": 180}, {"referenceID": 21, "context": "\u00a7 Exploration Effective and deterministic exploration behavior for ambiguous or invisible goal considering past experiencewas learned and temporal abstraction was discussed[22, 23].", "startOffset": 172, "endOffset": 180}, {"referenceID": 22, "context": "\u00a7 Communication (introduced in another paper[24]) Although each function has been examined in a very simple task, it is known that a variety of functions emerge based on extraction and memory of necessary information using an RNN.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "For example, in the learning of [20], an agent who has a visual sensor and an RNN as shown in Fig.", "startOffset": 32, "endOffset": 36}, {"referenceID": 18, "context": "(2013)[20]", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "(2013)[20] In a general approach, the object motion is estimated from some frames of image using some givenmodel, the future object location is predicted, the capture point and time are decided by some optimizationmethod, a reference trajectory is derived from the capture point, and the motions are controlled to follow the trajectory.", "startOffset": 6, "endOffset": 10}], "year": 2017, "abstractText": "Recently, triggered by the impressive results in TV-games or game of Go by Google DeepMind, end-to-end reinforcement learning (RL) is collecting attentions. Although little is known, the author\u2019s group has propounded this framework for around 20 years and already has shown a variety of functions that emerge in a neural network (NN) through RL. In this paper, they are introduced again at this timing. \u201cFunction Modularization\u201d approach is deeply penetrated subconsciously. The inputs and outputs for a learning system can be raw sensor signals and motor commands. \u201cState space\u201d or \u201caction space\u201d generally used in RL show the existence of functional modules. That has limited reinforcement learning to learning only for the action-planning module. In order to extend reinforcement learning to learning of the entire function on a huge degree of freedom of a massively parallel learning system and to explain or develop human-like intelligence, the author has believed that end-to-end RL from sensors to motors using a recurrent NN (RNN) becomes an essential key. Especially in the higher functions, since their inputs or outputs are difficult to decide, this approach is very effective by being free from the need to decide them. The functions that emerge, we have confirmed, through RL using a NN cover a broad range from real robot learning with raw camera pixel inputs to acquisition of dynamic functions in a RNN. Those are (1)image recognition, (2)color constancy (optical illusion), (3)sensor motion (active recognition), (4)hand-eye coordination and hand reaching movement, (5)explanation of brain activities, (6)communication, (7)knowledge transfer, (8)memory, (9)selective attention, (10)prediction, (11)exploration. The end-to-end RL enables the emergence of very flexible comprehensive functions that consider many things in parallel although it is difficult to give the boundary of each function clearly.", "creator": "LaTeX with hyperref package"}}}