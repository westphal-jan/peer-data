{"id": "1703.02437", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "PathTrack: Fast Trajectory Annotation with Path Supervision", "abstract": "Progress in Multiple Object Tracking (MOT) has been limited by the size of the available datasets. We present an efficient framework to annotate trajectories and use it to produce a MOT dataset of unprecedented size. A novel path supervision paradigm lets the annotator loosely track the object with a cursor while watching the video. This results in a path annotation for each object in the sequence. These path annotations, together with object detections, are fed into a two-step optimization to produce full bounding-box trajectories. Our experiments on existing datasets prove that our framework produces more accurate annotations than the state of the art and this in a fraction of the time. We further validate our approach by generating the PathTrack dataset, with more than 15,000 person trajectories in 720 sequences. We believe tracking approaches can benefit from a larger dataset like this one, just as was the case in object recognition. We show its potential by using it to retrain an off-the-shelf person matching network, originally trained on the MOT15 dataset, almost halving the misclassification rate. Additionally, training on our data consistently improves tracking results, both on our dataset and on MOT15. In the latter, where we improve the top-performing tracker (NOMT) dropping the number of ID Switches by 18% and fragments by 5%.", "histories": [["v1", "Tue, 7 Mar 2017 15:36:39 GMT  (5042kb,D)", "https://arxiv.org/abs/1703.02437v1", "10 pages, 7 figures, ICCV submission"], ["v2", "Wed, 22 Mar 2017 07:08:34 GMT  (4472kb,D)", "http://arxiv.org/abs/1703.02437v2", "10 pages, ICCV submission"]], "COMMENTS": "10 pages, 7 figures, ICCV submission", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MM", "authors": ["santiago manen", "michael gygli", "dengxin dai", "luc van gool"], "accepted": false, "id": "1703.02437"}, "pdf": {"name": "1703.02437.pdf", "metadata": {"source": "CRF", "title": "PathTrack: Fast Trajectory Annotation with Path Supervision", "authors": ["Santiago Manen", "Michael Gygli", "Dengxin Dai", "Luc Van Gool"], "emails": ["vangool}@vision.ee.ethz.ch"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to pave the way for the future."}, {"heading": "2. Related work", "text": "In fact, most of them are able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "3. Trajectory annotation with path supervision", "text": "In this section, we describe our annotation framework: We formalize path monitoring in paragraph 3.1 and then explain how we use it to derive precise path sequences in paragraph 3.2. In paragraph 3.3, we show how we integrate box monitoring."}, {"heading": "3.1. Path supervision", "text": "A path annotation of an object i consists of a (x, y) coordinate pi (t) within the bounding frame of frame id t. Path annotations are intuitive and efficient to obtain by looking at each object independently while following its position with the mouse cursor, cf. fig. 1. Our results show that the annotation of paths is only 33% slower than watching the video in real time. We say that a video has a path monitoring when a human annotator has provided a path annotation for the interesting objects. The following section explains how we use this annotation to obtain precise boxes."}, {"heading": "3.2. From path supervision to full box trajectories", "text": "While path monitoring is intuitive and efficient, it comes with its own set of challenges: a) It does not provide information about the spatial extent of the object. b) The relative position of directions within the object is unknown. To some extent, we solve these two problems by relying on the success of object detection, since our end goal is to generate large MOT records, and we know what kind of objects we want to comment on. Object detection gains maturity for objects of primary interest, so it is natural to use it as an established technique. Each detection is presented with a box and a confidence value to a specific frame.Our goal is to close the trajectories T of the objects in sequence, since the set of input path annotations P andobject detection D. This problem is similar to the trackingby detection data problem, but with additional information from the path monitoring. The number of objects, their time span and their location are given."}, {"heading": "3.2.1 Detection pre-labeling", "text": "The aim of this step is to find a way of approaching the respective object."}, {"heading": "3.2.2 Detection linkage", "text": "In this second step, the goal is to derive the definitive object trajectories. Pending identification of the most likely detection paths in a set of detections has been well studied in the MOT literature [31]. We assume that the detection step before labeling has adequately labeled the set of detection paths. So, any detection may be either part of its assigned path or a false positive, but it may not belong to another path. Thus, we will independently locate each detection cluster Fig. 2c and find the most likely detection path in the cluster Fig. 2d.Let Ti be the final path that corresponds to the detection cluster i. It will be composed of a set of time-sorted detections x1 to xK. We will find the most likely path that we have by minimizing the sum of detection-trust costs and between the detection transition costs."}, {"heading": "3.3. Incorporating box supervision", "text": "We propose a simple but effective way to expand our method with box comments to achieve a basic truth quality. Let's look at the detection path we use to interpolate a trajectory. To insert a box comment, we simply add it to the path and then remove near-temporal detections that are less than half a second away. Interpolation of the updated detection path results in the final trajectory. These rapid updates gradually improve trajectory comments as more box comments are added. Our method is more accurate than the state of the art for any number of updates, as we show in the experiments."}, {"heading": "4. The PathTrack dataset", "text": "This PathTrack dataset is an important part of our contribution. We will first give an overview of the dataset in paragraph 4.1. Then we will describe how we crowd-sourced the notes in paragraph 4.2. We will make the final trajectory notes using our approach, which links R-CNN detections [41] with path monitoring. Importantly, we will focus on training data to promote research in fully data-driven trackers."}, {"heading": "4.1. Dataset overview", "text": "The PathTrack dataset consists of 720 sequences with a total of 16,287 human trajectories. Focusing on tracking people allows us to collect more data for this specific class, which is of great interest both in the MOT community and in practical applications. The sequences are divided into a training set of 640 sequences with 15,380 trajectories and a test set of 80 sequences with 907 trajectories. Importantly, we note a certain amount of noise in the training scenes. This noise stems from inaccuracies in path monitoring and has allowed us to comment on more sequences for a given time budget. Our experiments show that we can learn strong appearance models from large amounts of data, even if the attributes are not perfectly clean (Sec. 5). In fact, the quantity favors over-quality in the collection of training data."}, {"heading": "4.2. Crowdsourcing path annotations", "text": "rf\u00fc ide rf\u00fc rf\u00fc eeisrteeGsrtee\u00fcn, \"tlrrsrteeeirsrrVnlrteeoiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiteteteteteteteteteteteteteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerrrrrrrrrrrrrrrrrrlrrrrrrrrrlrrrrrrlrrrrrlrrrrrrlrrrrrrrrlrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrr"}, {"heading": "5. Experiments", "text": "We present our experiments in three parts: first, we evaluate our annotation framework in paragraph 5.1 and then, in paragraph 5.2, demonstrate its impact on the collection of training data for the detection of matches, which is a central problem of the MOT [9] shared by most trackers. Finally, we evaluate the impact of our data on the multi-object tracking task."}, {"heading": "5.1. Trajectory annotation efficiency", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "5.2. Person matching", "text": "This year is the highest in the history of the country."}, {"heading": "5.3. Multi Object Tracking", "text": "In the previous section, we demonstrated how to train strong person matching models with PathTrack. We are now evaluating the impact of this improvement on T\u00dcV performance. In Tab. 2a, we compare the performance of this tracker with two different person matching models: one MOT15 trainedon and the other based on our data. Training on PathTrack significantly improves all metrics, which are also the first tracking results on our data set. We further demonstrate the potential of PathTrack by improving the most powerful tracker in MOT15 [9] with our person matching model c.f. Tab. 2b. More specifically, we are using our discriminatory person matching tool to further link their trajectory results by chance and improve the number of ID switches by 18% with 5% fewer fragments."}, {"heading": "6. Conclusion", "text": "In this paper, we propose a new framework for trajectory annotation in videos using trajectory monitoring, with the aim of generating massive MOT datasets. In the trajectory monitoring paradigm, the user comments on the position of the interesting objects with the cursor while watching the video. Our user study shows that this process is efficient; our optimization uses path annotation and object recognition, and delivers precise box trajectories; we show in our experiments that we can quickly generate large datasets with our trajectory monitoring; we use our approach to annotate PathTrack, a crowd-sourced MOT dataset that is 33 times larger than currently available; our experiments show that we can improve current person-related depth models using our data and that this has an impact on MOT accuracy; value oasis PathTrack promotes research into more comprehensive and complete tracking models."}], "references": [{"title": "Maria Florina Balcan and J", "author": ["X.R. Alireza Fathi"], "venue": "M. Rehg. Combining Self Training and Active Learning for Video Segmentation. In BMVC", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Label Propagation in Video Sequences", "author": ["V. Badrinarayanan", "F. Galasso", "R. Cipolla"], "venue": "CVPR", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Active Learning of an Action Detector from Untrimmed Videos", "author": ["S. Bandla", "K. Grauman"], "venue": "ICCV", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "What\u2019s the Point: Semantic Segmentation with Point Supervision", "author": ["A. Bearman", "O. Russakovsky", "V. Ferrari", "L. Fei-Fei"], "venue": "ECCV", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "The theory of dynamic programming", "author": ["R. Bellman"], "venue": "BAMS", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1954}, {"title": "Evaluating multiple object tracking performance: The clear mot metrics", "author": ["K. Bernardin", "R. Stiefelhagen"], "venue": "EURASIP Journal on Image and Video Processing", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "An Interactive Tool for Manual", "author": ["S. Bianco", "G. Ciocca", "P. Napoletano", "R. Schettini"], "venue": "Semi-automatic and Automatic Video Annotation. CVIU", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Near-Online Multi-Target Tracking With Aggregated Local Flow Descriptor", "author": ["W. Choi"], "venue": "In ICCV, December", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Minimizing human effort in interactive tracking by incremental learning of model parameters", "author": ["A. Ciptadi", "J.M. Rehg"], "venue": "ICCV", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Knowledge-driven Multimedia Information Extraction and Ontology Evolution", "author": ["S. Dasiopoulou", "E. Giannakidou", "G. Litos", "P. Malasioti", "Y. Kompatsiaris"], "venue": "chapter A Survey of Semantic Image and Video Annotation Tools, pages 196\u2013239. Springer-Verlag, Berlin, Heidelberg", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "A Crowdsourcing Approach to Support Video Annotation", "author": ["R. Di Salvo", "D. Giordano", "I. Kavasidis"], "venue": "VIGTA, VIGTA \u201913, pages 8:1\u20138:6, New York, NY, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "PETS2010: Dataset and Challenge", "author": ["A. Ellis", "J. Ferryman"], "venue": "AVSS, 00(undefined):143\u2013150", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding", "author": ["B.G. Fabian Caba Heilbron", "Victor Escorcia", "J.C. Niebles"], "venue": "In CVPR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Two- Granularity Tracking: Mediating Trajectory and Detection Graphs for Tracking under Occlusions", "author": ["K. Fragkiadaki", "W. Zhang", "G. Zhang", "J. Shi"], "venue": "pages 552\u2013565. Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Virtual worlds as proxy for multi-object tracking analysis", "author": ["A. Gaidon", "Q. Wang", "Y. Cabon", "E. Vig"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "CVPR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Geometric bounding box interpolation: an alternative for efficient video annotation", "author": ["P. Gil-Jim\u00e9nez", "H. G\u00f3mez-Moreno", "R.J. L\u00f3pez-Sastre", "S. Maldonado-Basc\u00f3n"], "venue": "EURASIP J. Image and Video Processing, 2016:8", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Video2GIF: Automatic Generation of Animated GIFs from Video", "author": ["M. Gygli", "Y. Song", "L. Cao"], "venue": "CVPR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Arm-Pointer: 3D Pointing Interface for Real- World Interaction", "author": ["E. Hosoya", "H. Sato", "M. Kitabata", "I. Harada", "H. Nojima", "A. Onozawa"], "venue": "pages 72\u201382. Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Click Carving: Segmenting Objects in Video with Point Clicks", "author": ["S.D. Jain", "K. Grauman"], "venue": "CoRR, abs/1607.01115", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "A Semi-automatic Tool for Detection and Tracking Ground Truth Generation in Videos", "author": ["I. Kavasidis", "S. Palazzo", "R. Di Salvo", "D. Giordano", "C. Spampinato"], "venue": "VIGTA, VIGTA \u201912, pages 6:1\u20136:5, New York, NY, USA", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "What Energy Functions Can Be Minimized via Graph Cuts? In ECCV", "author": ["V. Kolmogorov", "R. Zabih"], "venue": "London, UK, UK", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}, {"title": "The Visual Object Tracking VOT2015 Challenge Results", "author": ["M. Kristan", "J. Matas", "A. Leonardis", "M. Felsberg", "L. Cehovin", "G. Fernandez", "T. Vojir", "G. Hager", "G. Nebehay", "R. Pflugfelder"], "venue": "In ICCV Workshops,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised Object Discovery and Tracking in Video Collections", "author": ["S. Kwak", "M. Cho", "I. Laptev", "J. Ponce", "C. Schmid"], "venue": "ICCV", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning by tracking: Siamese cnn for robust target association", "author": ["L. Leal-Taix\u00e9", "C. Canton-Ferrer", "K. Schindler"], "venue": "CVPR Workshop", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards a Benchmark for Multi- Target Tracking", "author": ["L. Leal-Taix\u00e9", "A. Milan", "I. Reid", "S. Roth", "K. Schindler"], "venue": "MOTChallenge", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Key-segments for video object segmentation", "author": ["Y.J. Lee", "J. Kim", "K. Grauman"], "venue": "ICCV, pages 1995\u20132002", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple Object Tracking: A Literature Review", "author": ["W. Luo", "J. Xing", "X. Zhang", "X. Zhao", "T.-K. Kim"], "venue": "arXiv preprint arXiv:1409.7618", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Spot On: Action Localization from Pointly-Supervised Proposals", "author": ["P. Mettes", "J.C. van Gemert", "C.G.M. Snoek"], "venue": "In ECCV", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "The Design and Implementation of ViPER", "author": ["D. Mihalcik", "D. Doermann"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2003}, {"title": "MOT16: A Benchmark for Multi-Object Tracking", "author": ["A. Milan", "L. Leal-Taix\u00e9", "I. Reid", "S. Roth", "K. Schindler"], "venue": "[cs],", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "A", "author": ["J. Ni\u00f1o-Casta\u00f1eda"], "venue": "Fr\u0131\u0301as-Vel\u00e1zquez, N. B. Bo, M. Slembrouck, J. Guan, G. Debard, B. Vanrumste, T. Tuytelaars, and W. Philips. Scalable Semi-Automatic Annotation for Multi- Camera Person Tracking. IEEE Transactions on Image Processing", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient semantic video annotation by object and shot re-detection", "author": ["O.S.P. Schallauer", "H. Neuschmied"], "venue": "SAMT", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation", "author": ["F. Perazzi", "J. Pont-Tuset", "B. McWilliams", "L.V. Gool", "M. Gross", "A. Sorkine-Hornung"], "venue": "CVPR", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning object class detectors from weakly annotated video", "author": ["A. Prest", "C. Leistner", "J. Civera", "C. Schmid", "V. Ferrari"], "venue": "CVPR", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Does active learning work? A review of the research", "author": ["M. Prince"], "venue": "JEE, pages 223\u2013231", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "YouTube-BoundingBoxes: A Large High-Precision Human- Annotated Data Set for Object Detection in Video", "author": ["E. Real", "J. Shlens", "S. Mazzocchi", "X. Pan", "V. Vanhoucke"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2017}, {"title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "NIPS", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "IJCV", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "AVSS2011 demo session: Real-time human detection using fast contour template matching for visual surveillance", "author": ["D. Shao", "M. Rauter", "C. Beleznai"], "venue": "AVSS, 00:514", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Active Frame Selection for Label Propagation in Videos", "author": ["S. Vijayanarasimhan", "K. Grauman"], "venue": "pages 496\u2013509. Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficiently Scaling Up Crowdsourced Video Annotation", "author": ["C. Vondrick", "D. Patterson", "D. Ramanan"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}, {"title": "Video Annotation and Tracking with Active Learning", "author": ["C. Vondrick", "D. Ramanan"], "venue": "NIPS", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficiently Scaling Up Video Annotation with Crowdsourced Marketplaces", "author": ["C. Vondrick", "D. Ramanan", "D. Patterson"], "venue": "ECCV, pages 610\u2013623, Berlin, Heidelberg", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Ensemble-Based Tracking: Aggregating Crowdsourced Structured Time Series Data", "author": ["N. Wang", "D. yan Yeung"], "venue": "JMLR Workshop and Conference Proceedings,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "Interactive Offline Tracking for Color Objects", "author": ["Y. Wei", "J. Sun", "X. Tang", "H.-Y. Shum"], "venue": "ICCV", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning to track for spatio-temporal action localization", "author": ["P. Weinzaepfel", "Z. Harchaoui", "C. Schmid"], "venue": "In ICCV 2015 - IEEE International Conference on Computer Vision,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking", "author": ["L. Wen", "D. Du", "Z. Cai", "Z. Lei", "M. Chang", "H. Qi", "J. Lim", "M. Yang", "S. Lyu"], "venue": "arXiv CoRR, abs/1511.04136", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning From Massive Noisy Labeled Data for Image Classification", "author": ["T. Xiao", "T. Xia", "Y. Yang", "C. Huang", "X. Wang"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2015}, {"title": "LabelMe video: Building a video database with human annotations", "author": ["J. Yuen", "B.C. Russell", "C. Liu", "A. Torralba"], "venue": "ICCV, pages 1451\u20131458. IEEE Computer Society", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2009}, {"title": "Global data association for multi-object tracking using network flows", "author": ["L. Zhang", "Y. Li", "R. Nevatia"], "venue": "In CVPR,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2008}], "referenceMentions": [{"referenceID": 24, "context": "An example is the surge of Deep Learning thanks to ImageNet [26, 42].", "startOffset": 60, "endOffset": 68}, {"referenceID": 40, "context": "An example is the surge of Deep Learning thanks to ImageNet [26, 42].", "startOffset": 60, "endOffset": 68}, {"referenceID": 16, "context": "As a consequence, MOT datasets consist of only a couple dozens of sequences [18, 29, 34] or are restricted to the surveillance scenario [51].", "startOffset": 76, "endOffset": 88}, {"referenceID": 27, "context": "As a consequence, MOT datasets consist of only a couple dozens of sequences [18, 29, 34] or are restricted to the surveillance scenario [51].", "startOffset": 76, "endOffset": 88}, {"referenceID": 32, "context": "As a consequence, MOT datasets consist of only a couple dozens of sequences [18, 29, 34] or are restricted to the surveillance scenario [51].", "startOffset": 76, "endOffset": 88}, {"referenceID": 49, "context": "As a consequence, MOT datasets consist of only a couple dozens of sequences [18, 29, 34] or are restricted to the surveillance scenario [51].", "startOffset": 136, "endOffset": 140}, {"referenceID": 36, "context": "The cheapest way is to provide video-level object labels [38] or action labels [4].", "startOffset": 57, "endOffset": 61}, {"referenceID": 2, "context": "The cheapest way is to provide video-level object labels [38] or action labels [4].", "startOffset": 79, "endOffset": 82}, {"referenceID": 35, "context": "On the other end of the spectrum, sophisticated methods [37, 30, 3, 44, 2] produce pixel-accurate segmentations of objects.", "startOffset": 56, "endOffset": 74}, {"referenceID": 28, "context": "On the other end of the spectrum, sophisticated methods [37, 30, 3, 44, 2] produce pixel-accurate segmentations of objects.", "startOffset": 56, "endOffset": 74}, {"referenceID": 1, "context": "On the other end of the spectrum, sophisticated methods [37, 30, 3, 44, 2] produce pixel-accurate segmentations of objects.", "startOffset": 56, "endOffset": 74}, {"referenceID": 42, "context": "On the other end of the spectrum, sophisticated methods [37, 30, 3, 44, 2] produce pixel-accurate segmentations of objects.", "startOffset": 56, "endOffset": 74}, {"referenceID": 0, "context": "On the other end of the spectrum, sophisticated methods [37, 30, 3, 44, 2] produce pixel-accurate segmentations of objects.", "startOffset": 56, "endOffset": 74}, {"referenceID": 51, "context": "The common approach to it is to annotate a sparse set of boxes and interpolate between them linearly [53] or with shortest-paths [45].", "startOffset": 101, "endOffset": 105}, {"referenceID": 43, "context": "The common approach to it is to annotate a sparse set of boxes and interpolate between them linearly [53] or with shortest-paths [45].", "startOffset": 129, "endOffset": 133}, {"referenceID": 45, "context": "it cost tens of thousands of dollars to annotate the VIRAT dataset [47].", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "Virtual KITTI [17] - - - - - - 5* 4* 261* C* car-mounted KITTI [18] 21 13 29 18 - 50 30 - C + P car-mounted MOT15 [29] 11 6 500 11 10 721 22 16 1221 P S+M MOT16 [34] 7 4 512 7 4 830 14 8 1342 C+P** S+M PathTrack (ours) 640 161 15,380 80 11 907 720 172 16,287 P S+M 3 3", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "Virtual KITTI [17] - - - - - - 5* 4* 261* C* car-mounted KITTI [18] 21 13 29 18 - 50 30 - C + P car-mounted MOT15 [29] 11 6 500 11 10 721 22 16 1221 P S+M MOT16 [34] 7 4 512 7 4 830 14 8 1342 C+P** S+M PathTrack (ours) 640 161 15,380 80 11 907 720 172 16,287 P S+M 3 3", "startOffset": 63, "endOffset": 67}, {"referenceID": 27, "context": "Virtual KITTI [17] - - - - - - 5* 4* 261* C* car-mounted KITTI [18] 21 13 29 18 - 50 30 - C + P car-mounted MOT15 [29] 11 6 500 11 10 721 22 16 1221 P S+M MOT16 [34] 7 4 512 7 4 830 14 8 1342 C+P** S+M PathTrack (ours) 640 161 15,380 80 11 907 720 172 16,287 P S+M 3 3", "startOffset": 114, "endOffset": 118}, {"referenceID": 32, "context": "Virtual KITTI [17] - - - - - - 5* 4* 261* C* car-mounted KITTI [18] 21 13 29 18 - 50 30 - C + P car-mounted MOT15 [29] 11 6 500 11 10 721 22 16 1221 P S+M MOT16 [34] 7 4 512 7 4 830 14 8 1342 C+P** S+M PathTrack (ours) 640 161 15,380 80 11 907 720 172 16,287 P S+M 3 3", "startOffset": 161, "endOffset": 165}, {"referenceID": 15, "context": "* [17] provides 10 different conditions (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 32, "context": "** [34] provides a rich set of labels, such as whether an object is an occluder or a target is riding a vehicle.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "This PathTrack dataset is our second major contribution: a large MOT dataset of more than 15,000 person trajectories in 720 sequences, 30 times more than currently available ones [29].", "startOffset": 179, "endOffset": 183}, {"referenceID": 7, "context": "NOMT [9].", "startOffset": 5, "endOffset": 8}, {"referenceID": 27, "context": "We show its potential by improving the top tracker on MOT15 [29].", "startOffset": 60, "endOffset": 64}, {"referenceID": 9, "context": "There is quite some work on multimedia annotation [11].", "startOffset": 50, "endOffset": 54}, {"referenceID": 46, "context": "Of less relevance to us are those that work on videos with only a few people, such as [48, 35].", "startOffset": 86, "endOffset": 94}, {"referenceID": 33, "context": "Of less relevance to us are those that work on videos with only a few people, such as [48, 35].", "startOffset": 86, "endOffset": 94}, {"referenceID": 31, "context": "Hence, VIPER-GT [33] and LabelMe video [53] propose to linearly interpolate boxes between annotated keyframes.", "startOffset": 16, "endOffset": 20}, {"referenceID": 51, "context": "Hence, VIPER-GT [33] and LabelMe video [53] propose to linearly interpolate boxes between annotated keyframes.", "startOffset": 39, "endOffset": 43}, {"referenceID": 45, "context": "VATIC [47] uses this appearance model to define a graph on which it performs a shortestpath interpolation between manual annotations with Dynamic Programming [6].", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "VATIC [47] uses this appearance model to define a graph on which it performs a shortestpath interpolation between manual annotations with Dynamic Programming [6].", "startOffset": 158, "endOffset": 161}, {"referenceID": 47, "context": "The shortest-path interpolation allows for larger time gaps without manual annotations, assuming that the object is clearly visible, and it can be efficient [49].", "startOffset": 157, "endOffset": 161}, {"referenceID": 44, "context": "A VATIC improvement [46] incorporated active learning to decide which frames to annotate, to maximize the gain coming with such frames [39].", "startOffset": 20, "endOffset": 24}, {"referenceID": 37, "context": "A VATIC improvement [46] incorporated active learning to decide which frames to annotate, to maximize the gain coming with such frames [39].", "startOffset": 135, "endOffset": 139}, {"referenceID": 8, "context": "[10] built on top of shortest-path interpolation by updating the optimization weights with each extra annotation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Recently, [19] reconstructed annotated boxes and interpolated the final trajectories in 3D space.", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "Based on the aforementioned approaches, multiple annotation tools have been developed [23, 8, 36].", "startOffset": 86, "endOffset": 97}, {"referenceID": 6, "context": "Based on the aforementioned approaches, multiple annotation tools have been developed [23, 8, 36].", "startOffset": 86, "endOffset": 97}, {"referenceID": 34, "context": "Based on the aforementioned approaches, multiple annotation tools have been developed [23, 8, 36].", "startOffset": 86, "endOffset": 97}, {"referenceID": 10, "context": "Some gamify the annotation process [12].", "startOffset": 35, "endOffset": 39}, {"referenceID": 25, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Path supervision Pointing at objects comes very natural and has often been used in human-computer interaction [21, 22], yet it only recently gained popularity in Computer Vision.", "startOffset": 110, "endOffset": 118}, {"referenceID": 20, "context": "Path supervision Pointing at objects comes very natural and has often been used in human-computer interaction [21, 22], yet it only recently gained popularity in Computer Vision.", "startOffset": 110, "endOffset": 118}, {"referenceID": 30, "context": "In parallel with our work, [32] found path annotations promising for action localization in videos.", "startOffset": 27, "endOffset": 31}, {"referenceID": 30, "context": "Compared to [32, 50], we annotate dozens of people in highly-crowded sequences, ideal for MOT purposes.", "startOffset": 12, "endOffset": 20}, {"referenceID": 48, "context": "Compared to [32, 50], we annotate dozens of people in highly-crowded sequences, ideal for MOT purposes.", "startOffset": 12, "endOffset": 20}, {"referenceID": 3, "context": "Also recently, [5] and [22] used point supervision to segment objects in images and videos, resp.", "startOffset": 15, "endOffset": 18}, {"referenceID": 20, "context": "Also recently, [5] and [22] used point supervision to segment objects in images and videos, resp.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "[22] uses multiple points to segment, by iteratively re-ranking a collection of thousands of object proposals, called Click Carving.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Tracking datasets There is a corpus of video datasets that provide frame-level [15, 20] or pixel-level annotations [37].", "startOffset": 79, "endOffset": 87}, {"referenceID": 18, "context": "Tracking datasets There is a corpus of video datasets that provide frame-level [15, 20] or pixel-level annotations [37].", "startOffset": 79, "endOffset": 87}, {"referenceID": 35, "context": "Tracking datasets There is a corpus of video datasets that provide frame-level [15, 20] or pixel-level annotations [37].", "startOffset": 115, "endOffset": 119}, {"referenceID": 23, "context": "[25] and [40] are the largest datasets for single object tracking.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[25] and [40] are the largest datasets for single object tracking.", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "Most large-scale MOT datasets are restricted to surveillance videos [13, 43, 51], since they depict smooth and quasi-linear trajectories that are easy to annotate.", "startOffset": 68, "endOffset": 80}, {"referenceID": 41, "context": "Most large-scale MOT datasets are restricted to surveillance videos [13, 43, 51], since they depict smooth and quasi-linear trajectories that are easy to annotate.", "startOffset": 68, "endOffset": 80}, {"referenceID": 49, "context": "Most large-scale MOT datasets are restricted to surveillance videos [13, 43, 51], since they depict smooth and quasi-linear trajectories that are easy to annotate.", "startOffset": 68, "endOffset": 80}, {"referenceID": 16, "context": "More related to ours, KITTI [18] is collected from a car-mounted camera and focuses on pedestrians and vehicles.", "startOffset": 28, "endOffset": 32}, {"referenceID": 15, "context": "Parts of this dataset have been reproduced and rendered virtually, to show the potential of virtual datasets [17].", "startOffset": 109, "endOffset": 113}, {"referenceID": 27, "context": "[29, 34] have become the standard benchmarks for MOT, containing complex pedestrian scenes with static or moving cameras.", "startOffset": 0, "endOffset": 8}, {"referenceID": 32, "context": "[29, 34] have become the standard benchmarks for MOT, containing complex pedestrian scenes with static or moving cameras.", "startOffset": 0, "endOffset": 8}, {"referenceID": 22, "context": "(1) can be solved with Graph Cuts [24] efficiently.", "startOffset": 34, "endOffset": 38}, {"referenceID": 14, "context": "These are obtained by linking pixels through time using frame-to-frame optical flow and forward-backward consistency checks [16].", "startOffset": 124, "endOffset": 128}, {"referenceID": 7, "context": "Thus, we define the affinity between two detections as the intersection-overunion of their OF-trajectories, in the spirit of [9].", "startOffset": 125, "endOffset": 128}, {"referenceID": 29, "context": "Finding the most probable detection-paths in a set of detections has been well studied in the MOT literature [31].", "startOffset": 109, "endOffset": 113}, {"referenceID": 52, "context": "We find the most likely trajectory by minimizing the sum of detection-confidence costs and between-detections transition costs [54].", "startOffset": 127, "endOffset": 131}, {"referenceID": 52, "context": "We refer the reader to [54] for details.", "startOffset": 23, "endOffset": 27}, {"referenceID": 51, "context": "Thus we opt to linearly interpolate between detections to obtain the final trajectory, as per standard practice [53].", "startOffset": 112, "endOffset": 116}, {"referenceID": 39, "context": "We generate the final trajectory annotations with our approach, which associates R-CNN detections [41] with the help of path supervision.", "startOffset": 98, "endOffset": 102}, {"referenceID": 50, "context": "Indeed, favoring quantity over quality when collecting training data has also been found to be beneficial for other tasks [52, 20].", "startOffset": 122, "endOffset": 130}, {"referenceID": 18, "context": "Indeed, favoring quantity over quality when collecting training data has also been found to be beneficial for other tasks [52, 20].", "startOffset": 122, "endOffset": 130}, {"referenceID": 27, "context": "Compared to MOT15 [29], our dataset contains 33 times more sequences and 26 times more trajectory annotations available.", "startOffset": 18, "endOffset": 22}, {"referenceID": 49, "context": "Dataset diversity MOT datasets typically focus on surveillance [51], street-scenes [29, 34] or car-mounted cameras [18, 17].", "startOffset": 63, "endOffset": 67}, {"referenceID": 27, "context": "Dataset diversity MOT datasets typically focus on surveillance [51], street-scenes [29, 34] or car-mounted cameras [18, 17].", "startOffset": 83, "endOffset": 91}, {"referenceID": 32, "context": "Dataset diversity MOT datasets typically focus on surveillance [51], street-scenes [29, 34] or car-mounted cameras [18, 17].", "startOffset": 83, "endOffset": 91}, {"referenceID": 16, "context": "Dataset diversity MOT datasets typically focus on surveillance [51], street-scenes [29, 34] or car-mounted cameras [18, 17].", "startOffset": 115, "endOffset": 123}, {"referenceID": 15, "context": "Dataset diversity MOT datasets typically focus on surveillance [51], street-scenes [29, 34] or car-mounted cameras [18, 17].", "startOffset": 115, "endOffset": 123}, {"referenceID": 45, "context": "Reviewing process If the users are not trained properly or the interface is cumbersome to use, crowdsourced annotations can be erroneous [47].", "startOffset": 137, "endOffset": 141}, {"referenceID": 43, "context": "Interestingly, only 3 out of our 81 workers were revoked, while previous work had difficulties collecting annotations of sufficient quality [45].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "2 its impact on training data collection for matching detections, which is a key problem of MOT [9] that is shared by most trackers.", "startOffset": 96, "endOffset": 99}, {"referenceID": 27, "context": "Dataset description We evaluate our method on the MOT15 dataset [29] since it is most similar to our final goal, the generation of a massive MOT dataset.", "startOffset": 64, "endOffset": 68}, {"referenceID": 51, "context": "LabelMe [53] is an effective framework based on linear interpolation between box annotations.", "startOffset": 8, "endOffset": 12}, {"referenceID": 45, "context": "The more sophisticated VATIC [47] learns an appearance model from the box annotation, which it uses for a shortest-paths interpolation.", "startOffset": 29, "endOffset": 33}, {"referenceID": 44, "context": "An additional extension of VATIC uses active learning to propose to the user which frame to annotate [46].", "startOffset": 101, "endOffset": 105}, {"referenceID": 43, "context": "Effectiveness of path supervision We first follow the standard evaluation of trajectory annotation frameworks [45].", "startOffset": 110, "endOffset": 114}, {"referenceID": 44, "context": "Except for the active learning version of VATIC [46], box annotations are distributed uniformly in time, e.", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "for different Intersection-over-Union (IOU) [14] thresholds.", "startOffset": 44, "endOffset": 48}, {"referenceID": 45, "context": "Our method is efficient, as VATIC [47] and LabelMe [53] respectively require almost twice and three times more time to obtain our accuracy with only path supervision.", "startOffset": 34, "endOffset": 38}, {"referenceID": 51, "context": "Our method is efficient, as VATIC [47] and LabelMe [53] respectively require almost twice and three times more time to obtain our accuracy with only path supervision.", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "These models require extensive training data [26, 52], which we can provide with PathTrack.", "startOffset": 45, "endOffset": 53}, {"referenceID": 50, "context": "These models require extensive training data [26, 52], which we can provide with PathTrack.", "startOffset": 45, "endOffset": 53}, {"referenceID": 26, "context": "Experimental protocol We base our conclusions on a person matching network similar to SiameseCNN [28].", "startOffset": 97, "endOffset": 101}, {"referenceID": 26, "context": "The network has a simple AlexNet style architecture of 3 convolutional and 2 fully connected layers [28].", "startOffset": 100, "endOffset": 104}, {"referenceID": 26, "context": "relative distance, size) [28] in the network, we also see an improvement when using our data, from 84% to 90%.", "startOffset": 25, "endOffset": 29}, {"referenceID": 51, "context": "We estimate that it would take 22h to perfectly annotate the 11 videos in the training set of the MOT Challenge with LabelMe [53].", "startOffset": 125, "endOffset": 129}, {"referenceID": 50, "context": "Other works [52, 20] have also found a quantity strategy to be advantageous to train deep models.", "startOffset": 12, "endOffset": 20}, {"referenceID": 18, "context": "Other works [52, 20] have also found a quantity strategy to be advantageous to train deep models.", "startOffset": 12, "endOffset": 20}, {"referenceID": 52, "context": "We first use a standard tracker based on Linear Programming (LP) [54] and evaluate it on the test set of PathTrack with the standard CLEAR MOT metrics [7].", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "We first use a standard tracker based on Linear Programming (LP) [54] and evaluate it on the test set of PathTrack with the standard CLEAR MOT metrics [7].", "startOffset": 151, "endOffset": 154}, {"referenceID": 27, "context": "MOT15 [29] 24.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "NOMTwSDP [9] 55.", "startOffset": 9, "endOffset": 12}, {"referenceID": 7, "context": "We further show the potential of PathTrack by improving the topperforming tracker in MOT15 [9] with our person-matching model c.", "startOffset": 91, "endOffset": 94}], "year": 2017, "abstractText": "Progress in Multiple Object Tracking (MOT) has been historically limited by the size of the available datasets. We present an efficient framework to annotate trajectories and use it to produce a MOT dataset of unprecedented size. In our novel path supervision the annotator loosely follows the object with the cursor while watching the video, providing a path annotation for each object in the sequence. Our approach is able to turn such weak annotations into dense box trajectories. Our experiments on existing datasets prove that our framework produces more accurate annotations than the state of the art, in a fraction of the time. We further validate our approach by crowdsourcing the PathTrack dataset, with more than 15,000 person trajectories in 720 sequences1. Tracking approaches can benefit training on such large-scale datasets, as did object recognition. We prove this by re-training an off-the-shelf person matching network, originally trained on the MOT15 dataset, almost halving the misclassification rate. Additionally, training on our data consistently improves tracking results, both on our dataset and on MOT15. On the latter, we improve the top-performing tracker (NOMT) dropping the number of ID Switches by 18% and fragments by 5%.", "creator": "LaTeX with hyperref package"}}}