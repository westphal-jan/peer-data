{"id": "1703.02507", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features", "abstract": "The recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e. semantic representations) of word sequences as well. We present a simple but efficient unsupervised objective to train distributed representations of sentences. Our method outperforms the state-of-the-art unsupervised models on most benchmark tasks, and on many tasks even beats supervised models, highlighting the robustness of the produced sentence embeddings.", "histories": [["v1", "Tue, 7 Mar 2017 18:19:11 GMT  (91kb,D)", "http://arxiv.org/abs/1703.02507v1", null], ["v2", "Mon, 10 Jul 2017 18:05:48 GMT  (108kb,D)", "http://arxiv.org/abs/1703.02507v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR", "authors": ["matteo pagliardini", "prakhar gupta", "martin jaggi"], "accepted": false, "id": "1703.02507"}, "pdf": {"name": "1703.02507.pdf", "metadata": {"source": "META", "title": "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features", "authors": ["Matteo Pagliardini", "Prakhar Gupta", "Martin Jaggi"], "emails": ["<martin.jaggi@epfl.ch>."], "sections": [{"heading": "1. Introduction", "text": "It is as if it is a reactionary party that is able to transform itself into a reactionary party."}, {"heading": "2. Model", "text": "Our model is inspired by simple matrix factorization models recently used very successfully in unattended learning of word embedding (Mikolov et al., 2013b; ar Xiv: 170 3.02 507v 1 [cs.C L] 7M ar2 017Pennington et al., 2014; Bojanowski et al., 2016) and supervised and unsupervised learning of sentence representations (Joulin et al., 2016). These models will all be described by the following matrix factoring problems in U, V, S, C, fS (UV, 2016). In all models studied, the columns of matrix V will collect the learned word descriptions. For a given sentence S, which can be of any length, the indicator vector S, {R}, will be a great property."}, {"heading": "2.1. Proposed Unsupervised Model", "text": "We propose a new, unattended model, Sent2Vec, for learning sentence embedding (R > S) (S > P), which can be interpreted as a natural extension of the word contexts of CBOW (Mikolov et al., 2013b; a) to a larger sentence context in which the sentence words are specifically optimized for additive combination across the sentence. (S) Sentence embedding as in (2) is defined as the mean of the word embedding of its constituent words. (S) We further expand this model by learning source embedding for not only unigrams, but also n-grams present in each sentence, and averaging the n-gram embedding along with the words, i.e. the sentence embedding vS for S is modeled asvS: = 1 (S)."}, {"heading": "2.2. Computational Efficiency", "text": "In contrast to more complex models of neural networks, one of the main advantages of the proposed technique is the low computing costs for both inference and training. In view of a set S and a trained model U, V, calculating the resulting sentence representation vS requires only floating-point operations (or | R (S) | \u00b7 h to be precise for the n-gram case, see (2) where h is the dimension of embedding. The same applies to a training step with SGD on target (3), for each pair of sentence context and target word. Parallel training of the model takes place directly by means of parallel or distributed SGD.1 In order to efficiently sample negatives, a pre-processing table is constructed containing the words corresponding to the square root of their body frequency. Subsequently, the negatives Nwt are uniformly derived from the negative table with the exception of the target itself according to the random principle Boampowski 2016, Jouelt, Joual."}, {"heading": "2.3. Comparison to CBOW", "text": "CBOW tries to predict a selected target word based on its specified context window, where the context is defined by the average of the vectors associated with the words at a distance smaller than the window size hyperparameter ws. If our system, when limited to unigram attributes, can be considered an extension of CBOW, where the context window contains the entire sentence, there are few important differences in practice, since CBOW uses important tricks to facilitate learning word embeddings. CBOW initially uses frequent word subsampling on the sentences and chooses to discard each token w with the probability of qp (w) or equal to it (small variations exist between implementations). Subsampling prevents the generation of n-gram attributes and deprives the sentence of an important part of its syntactic attributes. It also shortens the distance between examined words, implicitly increasing the span of the context window."}, {"heading": "2.4. Model Training", "text": "The Wikipedia record consists of 69 million sentences and 1.7 billion words, while the Twitter record contains 1.2 billion tweets and 19.7 billion words. Wikipedia records were tokenized using the Stanford NLP library (Manning et al., 2014), while tweets were tokenized using the NLTK tokenizer (Bird et al., 2009). To prevent us from selecting one sentence from the record for each sentence and then selecting all possible target unigrams using subsampling, we update the weights using the SGD, which is associated with a linear decrease in learning rates. To prevent us from using a dropout on the list of n-grams R (S) for each sentence, we will match the results with the available 1-disk library, with U (S) being the totality of all the unigrams contained in sentence S."}, {"heading": "3. Related Work", "text": "We discuss existing models that have been proposed to construct sentence embeddings. While some methods require ordered raw text, i.e. a coherent corpus in which the next sentence is a logical continuation of the previous sentence, others rely only on raw text, i.e. a disordered collection of sentences. Finally, we discuss alternative models built from structured data sources."}, {"heading": "3.1. Unsupervised Models Independent of Sentence Ordering", "text": "The ParagraphVector DBOW model from (Le & Mikolov \u03b2, 2014) is a loglinear model that is trained to learn sentences and word embedding and then use a softmax distribution to predict words contained in the sentence. They also propose another model ParagraphVector DM, in which they use n-grams of consecutive words along with sentence vector representation to predict the next word. (Hill et al., 2016a) suggest a sequential (denoising) autoencoder, S (D) AE. This model first introduces the noise into the input data: After each word has been deleted with the probability of p0, words are then swapped for each non-overlapping Bigram with the probability of px. The model then uses an LSTM-based architecture to retrieve the original sentence from the corrupt version, and the model can then be used to encode new sentences in vector representations."}, {"heading": "3.2. Unsupervised Models Depending on Sentence Ordering", "text": "The SkipThought model of (Kiros et al., 2015) combines sentence-level models with recurrent neural networks. Faced with a sentence Si from an ordered corpus, the model is trained to predict Si \u2212 1 and Si + 1. FastSent by (Hill et al., 2016a) is a loglinear sack-of-words model at sentence level. Like SkipThought, it uses adjacent sentences as a predictive target and is trained in an unsupervised manner. Using word sequences allows the model to improve on the previous work of Paragraph2vec (Le & Mikolov, 2014). (Hill et al., 2016a) FastSent further supplements FastSent by predicting the constituent words of the sentence. This model is referred to in our comparisons as FastSent + AE. Note that FastText by (Bojanowski et al., 2016) uses character sequences instead of the same word sequences to obtain better conceptual model."}, {"heading": "3.3. Supervised and Semi-Supervised Models", "text": "DictRep by (Hill et al., 2016b) is trained to map dictionary definitions of words onto the pre-trained word embeddings of these words. They use two different architectures, namely BOW and RNN (LSTM), with the choice of learning the input word embeddings or pre-trained to use them. A similar architecture is used by the CaptionRep variant, but here the task is to map predefined captions to a pre-trained vector representation of these images. The model is trained in such a way that sentences occurring in a pair exhibit a high cosmic similarity. In this context, the paraphrase model is trained to this goal, on a representation that is the average word embeddings."}, {"heading": "4. Evaluation", "text": "We use both superordinate and unsupervised benchmark tasks to evaluate our trained models. (For the superordinate assessment we use the same method as for the superordinate assessment as for the human note. (Hill et al., 2016a) to facilitate comparison with other models. (Wieting et al., 2016), those of the Paraphrase Database (Ganitkevitch et al., 2013) and the more recent work of Arora et al. (Wieting et al., 2016), those of the Paraphrase Database (Ganitkevitch et al.)."}, {"heading": "5. Results", "text": "In Tables 2 and 3, we compare our results with those of (Hill et al., 2016a) on different models. In addition to the models discussed in Section 3, this also includes the embedding of baselines, which are achieved by simply averaging word embeddings over the sentence, both in the CBOW and Skip-gram variants. TF-IDF BOW is a representation consisting of the counting of the 200,000 most common feature words weighted by their TF-IDF frequency. Finally, the comparison also includes sentence embeddings learned through neural machine translation models that translate the architecture from (Cho et al., 2014) to English to German (En-De) and English to French (En-Fr) data from the Workshop on Statistical MT (WMT) 3. For the methodology of training these models, we refer the reader to (Hill et al., 2016a).We also compare our results with the modules that were made (semi-evaluated) in 2017."}, {"heading": "5.1. Supervised Evaluation Results", "text": "If we perform supervised evaluations and look at the results in Table 2, we find that our Sent2Vec Unigrams + Bigrams model is able to outperform all previous state-of-the-art methods in 3 of the datasets. It also performs better than all previous models except SkipThought for the SUBJ and TREC tasks, although we find that our models do not perform well in the MSRP task. Twitter-trained models perform similarly to wikipedia-trained models.3http: / / www.statmt.org / wmt15 / translation-task.html"}, {"heading": "5.2. Unsupervised Evaluation Results", "text": "In Table 3, in addition to the results compiled by Hill et al., 2016a, we also compare our comparability assessment results with those obtained by (Wieting et al., 2016) for the same tasks. We see that our Twitter models are state-of-the-art for most tasks. Also, for most tasks, our Sent2Vec Twitter models are able to outperform or match the monitored models of (Wieting et al., 2016). However, the DictRep models and CPHRASE perform significantly better on the WordNet dataset and CaptionRep remains the state-of-the-art model for the image dataset."}, {"heading": "5.3. Comparison with (Arora et al., 2017)", "text": "In Table 4, we report on an experimental comparison with the model of (Arora et al., 2017), which is especially tailored to sentence similarity tasks. In the table, the suffix W indicates that their downweighting scheme was used, while the suffix R indicates that the first main component has been subtracted. They gave values between 10 \u2212 4 and 10 \u2212 3 as the best results and used a = 10 \u2212 3 for all their experiments. Their rational and downweighted scheme indicates that we are also trying to reduce the importance of syntactical features. To do this, we use a simple black list of the 25 most common tokens in the Twitter corpus. These tokens are discarded before averaging. The results are also noted in Table 4.We find that our results compete with the embedding of (Arora et al., 2017), for purely uncontrolled methods."}, {"heading": "6. Discussion", "text": "We see that both the models and the unattended assessments, our Sent2Vec models, do not work when asked about the way in which they have occurred in the majority of tasks / datasets compared to other unattended methods. In fact, they are even better than or on par with the paraphrase basedembeddings, which are learned in a superordinate manner, on which most datasets are based in terms of the similarity of the assessments. In fact, our unattended models are weaker than the tasks of the MSRP task (which consists of identifying the described paraphrases) compared to the statasets on the way they are evaluated. However, we observe that the superordinate models, which work extremely well on this task, work very poorly on the other tasks, which indicate a lack of generalizability. For the TREC (question of answering the task), our models are in second place, our models are significant in terms of performance, although we are second to a significant one."}, {"heading": "7. Conclusion", "text": "In this paper, we introduced a novel, unsupervised and computationally efficient method of training and inferring sentence embeddings. Our method performs better than any other unsupervised competitor - including those trained on deep-learning architectures - for most of the tasks, and even outperforms some newer supervised methods on multiple sets of data. Future work could focus on expanding the model to exploit data with ordered sentences. In addition, the context information could be better used to learn the importance of words present in a sentence. Last but not least, we would like to further investigate the ability of the models to provide pre-trained embeddings to facilitate downstream transfer learning assignments.Recognition. We are committed to helpful discussions with Piotr Bojanowski and Armand Joulin."}], "references": [{"title": "Semeval-2014 task 10: Multilingual semantic textual similarity", "author": ["Agirre", "Eneko", "Banea", "Carmen", "Cardie", "Claire", "Cer", "Daniel", "Diab", "Mona", "Gonzalez-Agirre", "Aitor", "Guo", "Weiwei", "Mihalcea", "Rada", "Rigau", "German", "Wiebe", "Janyce"], "venue": "In Proceedings of the 8th international workshop on semantic evaluation (SemEval", "citeRegEx": "Agirre et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "A Latent Variable Model Approach to PMIbased Word Embeddings", "author": ["Arora", "Sanjeev", "Li", "Yuanzhi", "Liang", "Yingyu", "Ma", "Tengyu", "Risteski", "Andrej"], "venue": "In Transactions of the Association for Computational Linguistics,", "citeRegEx": "Arora et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "A simple but tough-to-beat baseline for sentence embeddings", "author": ["Arora", "Sanjeev", "Liang", "Yingyu", "Ma", "Tengyu"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Arora et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2017}, {"title": "Natural language processing with Python: analyzing text with the natural language toolkit", "author": ["Bird", "Steven", "Klein", "Ewan", "Loper", "Edward"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Enriching Word Vectors with Subword Information", "author": ["Bojanowski", "Piotr", "Grave", "Edouard", "Joulin", "Armand", "Mikolov", "Tomas"], "venue": null, "citeRegEx": "Bojanowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["Dolan", "Bill", "Quirk", "Chris", "Brockett"], "venue": "In Proceedings of the 20th International Conference on Computational Linguistics,", "citeRegEx": "Dolan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Ppdb: The paraphrase database", "author": ["Ganitkevitch", "Juri", "Van Durme", "Benjamin", "Callison-Burch", "Chris"], "venue": "In HLT-NAACL,", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "word2vec Explained: deriving Mikolov et al.\u2019s negative-sampling word-embedding method", "author": ["Goldberg", "Yoav", "Levy", "Omer"], "venue": null, "citeRegEx": "Goldberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2014}, {"title": "Learning Distributed Representations of Sentences from Unlabelled Data", "author": ["Hill", "Felix", "Cho", "Kyunghyun", "Korhonen", "Anna"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Learning to understand phrases by embedding the dictionary", "author": ["Hill", "Felix", "Cho", "KyungHyun", "Korhonen", "Anna", "Bengio", "Yoshua"], "venue": "TACL, 4:17\u201330,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Mining and summarizing customer reviews", "author": ["Hu", "Minqing", "Liu", "Bing"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Hu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2004}, {"title": "Bag of tricks for efficient text classification", "author": ["Joulin", "Armand", "Grave", "Edouard", "Bojanowski", "Piotr", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1607.01759,", "citeRegEx": "Joulin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2016}, {"title": "Skip-Thought Vectors", "author": ["Kiros", "Ryan", "Zhu", "Yukun", "Salakhutdinov", "Ruslan R", "Zemel", "Richard", "Urtasun", "Raquel", "Torralba", "Antonio", "Fidler", "Sanja"], "venue": "In NIPS 2015 - Advances in Neural Information Processing Systems", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Le", "Quoc V", "Mikolov", "Tomas"], "venue": "In ICML 2014 - Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Levy", "Omer", "Goldberg", "Yoav", "Dagan", "Ido"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "The automatic creation of literature abstracts", "author": ["Luhn", "Hans Peter"], "venue": "IBM Journal of research and development,", "citeRegEx": "Luhn and Peter.,? \\Q1958\\E", "shortCiteRegEx": "Luhn and Peter.", "year": 1958}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Manning", "Christopher D", "Surdeanu", "Mihai", "Bauer", "John", "Finkel", "Jenny Rose", "Bethard", "Steven", "McClosky", "David"], "venue": "In ACL (System Demonstrations),", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Marelli", "Marco", "Menini", "Stefano", "Baroni", "Bentivogli", "Luisa", "Bernardi", "Raffaella", "Zamparelli", "Roberto"], "venue": "In LREC,", "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In NIPS - Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Bo", "Lee", "Lillian"], "venue": "In Proceedings of the 43rd annual meeting on association for computational linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model", "author": ["NT Pham", "G Kruszewski", "A Lazaridou", "M. Baroni"], "venue": "ACL/IJCNLP,", "citeRegEx": "Pham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2015}, {"title": "The proof and measurement of association between two things", "author": ["Spearman", "Charles"], "venue": "The American journal of psychology,", "citeRegEx": "Spearman and Charles.,? \\Q1904\\E", "shortCiteRegEx": "Spearman and Charles.", "year": 1904}, {"title": "Overview of the trec 2001 question answering track", "author": ["Voorhees", "Ellen M"], "venue": "In NIST special publication,", "citeRegEx": "Voorhees and M.,? \\Q2002\\E", "shortCiteRegEx": "Voorhees and M.", "year": 2002}, {"title": "Annotating expressions of opinions and emotions in language", "author": ["Wiebe", "Janyce", "Wilson", "Theresa", "Cardie", "Claire"], "venue": "Language resources and evaluation,", "citeRegEx": "Wiebe et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["Wieting", "John", "Bansal", "Mohit", "Gimpel", "Kevin", "Livescu", "Karen", "Roth", "Dan"], "venue": null, "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["Wieting", "John", "Bansal", "Mohit", "Gimpel", "Kevin", "Livescu", "Karen"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Wieting et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 23, "context": "A very notable exception comes from the text and natural language processing domain, in the form of semantic word embeddings trained unsupervised (Mikolov et al., 2013b;a; Pennington et al., 2014).", "startOffset": 146, "endOffset": 196}, {"referenceID": 12, "context": ", 2016) as well as supervised and unsupervised learning of sentence representations (Joulin et al., 2016).", "startOffset": 84, "endOffset": 105}, {"referenceID": 23, "context": ", 2013b;a) and GloVe (Pennington et al., 2014).", "startOffset": 21, "endOffset": 46}, {"referenceID": 12, "context": "In contrast, for sentence embeddings as in the focus of our paper here, S will be entire sentences or documents (therefore variable length), just like in the supervised FastText classifier (Joulin et al., 2016).", "startOffset": 189, "endOffset": 210}, {"referenceID": 12, "context": "To select the possible target unigrams (positives), we use subsampling as in (Joulin et al., 2016; Bojanowski et al., 2016), each word w being discarded with probability 1 \u2212 qp(w) where qp(w) := min { 1, \u221a t/fw + t/fw } .", "startOffset": 77, "endOffset": 123}, {"referenceID": 4, "context": "To select the possible target unigrams (positives), we use subsampling as in (Joulin et al., 2016; Bojanowski et al., 2016), each word w being discarded with probability 1 \u2212 qp(w) where qp(w) := min { 1, \u221a t/fw + t/fw } .", "startOffset": 77, "endOffset": 123}, {"referenceID": 12, "context": "Then, the negatives Nwt are sampled uniformly at random from the negatives table except the target wt itself, following (Joulin et al., 2016; Bojanowski et al., 2016).", "startOffset": 120, "endOffset": 166}, {"referenceID": 4, "context": "Then, the negatives Nwt are sampled uniformly at random from the negatives table except the target wt itself, following (Joulin et al., 2016; Bojanowski et al., 2016).", "startOffset": 120, "endOffset": 166}, {"referenceID": 15, "context": "Using dynamic context windows is equivalent to weighing by the distance from the focus word w divided by the window size (Levy et al., 2015).", "startOffset": 121, "endOffset": 140}, {"referenceID": 3, "context": ", 2014), while the tweets have been tokenized using the NLTK tweets tokenizer (Bird et al., 2009).", "startOffset": 78, "endOffset": 97}, {"referenceID": 12, "context": "Our C++ implementation builds upon the FastText library (Joulin et al., 2016; Bojanowski et al., 2016).", "startOffset": 56, "endOffset": 102}, {"referenceID": 4, "context": "Our C++ implementation builds upon the FastText library (Joulin et al., 2016; Bojanowski et al., 2016).", "startOffset": 56, "endOffset": 102}, {"referenceID": 1, "context": "Using the generative model of (Arora et al., 2016), words are generated conditioned on a sentence \u201cdiscourse\u201d vector cs:", "startOffset": 30, "endOffset": 50}, {"referenceID": 2, "context": "(Arora et al., 2017) demonstrated that for this model, the MLE of c\u0303s can be approximated by \u2211", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": "They generate sentence embeddings from diverse pre-trained word embeddings among which are unsupervised word embeddings such as GloVe (Pennington et al., 2014) as well as supervised word embeddings such as paragram-SL999 (PSL)", "startOffset": 134, "endOffset": 159}, {"referenceID": 28, "context": "(Wieting et al., 2015) trained on the Paraphrase Database (Ganitkevitch et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": ", 2015) trained on the Paraphrase Database (Ganitkevitch et al., 2013).", "startOffset": 43, "endOffset": 70}, {"referenceID": 13, "context": "The SkipThought model of (Kiros et al., 2015) combines sentence level models with recurrent neural networks.", "startOffset": 25, "endOffset": 45}, {"referenceID": 4, "context": "Note that on the character sequence level instead of word sequences, FastText by (Bojanowski et al., 2016) uses the same conceptual model to obtain better word embeddings.", "startOffset": 81, "endOffset": 106}, {"referenceID": 29, "context": "Paraphrastic Sentence Embeddings by (Wieting et al., 2016) are trained on a paraphrase database in a supervised manner.", "startOffset": 36, "endOffset": 58}, {"referenceID": 29, "context": "Experiments in (Wieting et al., 2016) show that these two simple models outperforms more complex RNN as well as LSTM architectures on similarity evaluation tasks.", "startOffset": 15, "endOffset": 37}, {"referenceID": 29, "context": "We also compare our unsupervised method to the supervised sentence embeddings of (Wieting et al., 2016) which are learnt from the Paraphrase Database (Ganitkevitch et al.", "startOffset": 81, "endOffset": 103}, {"referenceID": 7, "context": ", 2016) which are learnt from the Paraphrase Database (Ganitkevitch et al., 2013) as well as the more recent work of (Arora et al.", "startOffset": 54, "endOffset": 81}, {"referenceID": 2, "context": ", 2013) as well as the more recent work of (Arora et al., 2017).", "startOffset": 43, "endOffset": 63}, {"referenceID": 6, "context": "We evaluate on paraphrase identification (MSRP) (Dolan et al., 2004), movie review sentiment (MR) (Pang & Lee, 2005), product reviews (CR) (Hu & Liu, 2004), subjectivity classification (SUBJ)(Pang & Lee, 2004), opinion polarity (MPQA) (Wiebe et al.", "startOffset": 48, "endOffset": 68}, {"referenceID": 27, "context": ", 2004), movie review sentiment (MR) (Pang & Lee, 2005), product reviews (CR) (Hu & Liu, 2004), subjectivity classification (SUBJ)(Pang & Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Voorhees, 2002).", "startOffset": 174, "endOffset": 194}, {"referenceID": 13, "context": "To classify, we use the code provided by (Kiros et al., 2015) in the same manner as in (Hill et al.", "startOffset": 41, "endOffset": 61}, {"referenceID": 0, "context": "We perform unsupervised evaluation of the the learnt sentence embeddings using the sentence cosine similarity, on the STS 2014 (Agirre et al., 2014) and SICK 2014 (Marelli et al.", "startOffset": 127, "endOffset": 148}, {"referenceID": 18, "context": ", 2014) and SICK 2014 (Marelli et al., 2014) datasets.", "startOffset": 22, "endOffset": 44}, {"referenceID": 0, "context": "See (Agirre et al., 2014) for more precise information on how the pairs have", "startOffset": 4, "endOffset": 25}, {"referenceID": 5, "context": "Finally, the comparison also includes sentence embeddings learnt by neural machine translation models which use the architecture by (Cho et al., 2014) on English to German (En-De) and English to French (En-Fr) data from the 2015 Workshop on Statistical MT (WMT)3.", "startOffset": 132, "endOffset": 150}, {"referenceID": 29, "context": "We also compare our results with the the similarity evaluations done by (Wieting et al., 2016) on their supervised paragram-phrase models as well as on the unsupervised", "startOffset": 72, "endOffset": 94}, {"referenceID": 2, "context": "and semi-supervised embeddings obtained by (Arora et al., 2017).", "startOffset": 43, "endOffset": 63}, {"referenceID": 2, "context": "Comparison of the performance of the unsupervised and semi-supervised sentence embeddings by (Arora et al., 2017) with our", "startOffset": 93, "endOffset": 113}, {"referenceID": 29, "context": ", 2016a), we also compare our similarity evaluation results with those obtained by (Wieting et al., 2016) on the same set of tasks.", "startOffset": 83, "endOffset": 105}, {"referenceID": 29, "context": "Also, on a majority of tasks, our Sent2Vec Twitter models are able to outperform or are on par with the supervised models of (Wieting et al., 2016).", "startOffset": 125, "endOffset": 147}, {"referenceID": 2, "context": "Comparison with (Arora et al., 2017)", "startOffset": 16, "endOffset": 36}, {"referenceID": 2, "context": "In Table 4, we report an experimental comparison to the model of (Arora et al., 2017), which is particularly tailored to sentence similarity tasks.", "startOffset": 65, "endOffset": 85}, {"referenceID": 2, "context": "We observe that our results are competitive with the embeddings of (Arora et al., 2017) for purely unsupervised methods.", "startOffset": 67, "endOffset": 87}, {"referenceID": 2, "context": "Bottom figure: down-weighting scheme proposed by (Arora et al., 2017): weight(w) = a a+fw .", "startOffset": 49, "endOffset": 69}, {"referenceID": 2, "context": "In Figure 1 we show the profile of the norm as a function of log(fw) for each w \u2208 V , and compare it to the static down-weighting mechanism of (Arora et al., 2017).", "startOffset": 143, "endOffset": 163}], "year": 2017, "abstractText": "The recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e. semantic representations) of word sequences as well. We present a simple but efficient unsupervised objective to train distributed representations of sentences. Our method outperforms the state-of-the-art unsupervised models on most benchmark tasks, and on many tasks even beats supervised models, highlighting the robustness of the produced sentence embeddings.", "creator": "LaTeX with hyperref package"}}}