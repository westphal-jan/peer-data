{"id": "1703.02618", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Bootstrapped Graph Diffusions: Exposing the Power of Nonlinearity", "abstract": "Graph-based semi-supervised learning (SSL) algorithms predict labels for all nodes based on provided labels of a small set of seed nodes. Classic methods capture the graph structure through some underlying diffusion process that propagates through the graph edges. Spectral diffusion, which includes personalized page rank and label propagation, propagates through random walks. Social diffusion propagates through shortest paths. A common ground to these diffusions is their {\\em linearity}, which does not distinguish between contributions of few \"strong\" relations and many \"weak\" relations.", "histories": [["v1", "Tue, 7 Mar 2017 22:10:34 GMT  (1393kb,D)", "https://arxiv.org/abs/1703.02618v1", "11 pages, 5 figures, 6 tables"], ["v2", "Wed, 15 Mar 2017 11:54:41 GMT  (1393kb,D)", "http://arxiv.org/abs/1703.02618v2", "11 pages, 5 figures, 6 tables"]], "COMMENTS": "11 pages, 5 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["eliav buchnik", "edith cohen"], "accepted": false, "id": "1703.02618"}, "pdf": {"name": "1703.02618.pdf", "metadata": {"source": "CRF", "title": "Bootstrapped Graph Di\u0080usions: Exposing the Power of Nonlinearity", "authors": ["Eliav Buchnik", "Edith Cohen"], "emails": ["eliavbuh@gmail.com", "edith@cohenwang.com"], "sections": [{"heading": null, "text": "Recently, non-linear methods such as node embedding and Convolutionary Graph Networks (GCN) have shown great quality gains for SSL tasks, which introduce multiple components and vary greatly in the way graph structure, seed identification information and other traits are used. Our goal is to investigate the contribution of nonlinearity as an isolated component to performance enhancement by placing classic linear graph representations within a self-training framework. Surprisingly, we find that SSL is not only significantly better than the respective non-bootstrapped baselines, but also surpasses state-of-the-art non-linear SSL methods. Moreover, because the self-training wrapper maintains the scalability of the basic method, we get both higher quality and better scalability."}, {"heading": "1 INTRODUCTION", "text": "In fact, it is as if most of them are able to surpass themselves by putting themselves at the center. (...) It is as if they were able to surpass themselves. (...) \"It is as if they surpass themselves.\" (...) \"It is as if they surpass themselves.\" (...) \"It is as if they surpass themselves.\" (...) \"It is as if they surpass themselves.\" (...) \"It is so.\" (...) \"(...)\" (...) \"(...)\" (...) \"(...).\" (...) \"(...\" (). \"()\" () \"().\" () \"().\" () \"().\" (). \"()\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\"). \"().\" (). \"().\" (). \"().\" (). \").\" (). \"().\" (). \"().\"). \"().\" (). \"().\" (). \"().\" (). \").\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \").\" (). \"().\" (). \"().\" (). \"().\" (). \"().\"). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" ()"}, {"heading": "2 LINEAR AND BOOTSTRAPPED DIFFUSIONS", "text": "In this section, we will discuss SSL methods based on linear (spectral and social) graph propagation methods and emphasize the variants we have used in our experiments. In particular, we will discuss two textbook propagation methods [42, 43] and one distance propagation method [11]. Afterwards, we will discuss the application of self-training to these basic methods."}, {"heading": "2.1 Spectral di usion methods", "text": "We note that a higher quality of the knowledge learned is achieved through training that corresponds to the number of classes we use but are used in such a way. [L] each node i \u2264 n \"is set to yi j = 1 and other entries i.\" [L] each node i \u2264 n \"is provided as input or scholar. In our experiments we use the adjacency matrix of provided undirected graphs with uniform edge weights and without self-loops. at isWi j = 1, if the edge (i, j) is present and Wi j = 0 otherwise.e algorithms we use to thus label yi > n\" the dimension that is equal to the number of classes L. e have learned, we follow the maximum entry arg maxj yi j."}, {"heading": "2.2 Social di usion methods", "text": "We consider recently proposed SSL methods based on removal di usion [8]. e input is a directed graph G = (V, E) where nodes [n'] are the seed nodes and a distributionW that generated a set of length w > 0 for the edges e. e so label yi is computered from the shorest-path distances d (t) i j from i to each seed length j \u2264 n \"and the each labels label [j] as labels y (t) i for i > n.\" e so label yi is compuths from the shortpath distances d (t) i j from i to each seed length j \u2264 n \"and the each labels label [j] as edels [L].e nal so label we seek for each i > n\" is the expectation E [t) i], and we approximate it by the average over the iterations. \"e number of iterations is not expuths [j] as expusions [j]."}, {"heading": "2.3 Bootstrapping Wrapper", "text": "There are many variants [1, 41]. e bootstrapping wrapper takes as input a base algorithmA and a set S of labeled seed nodes. In each step we have used a very basic formulation that is shown as algorithm 4. We x the number of new seed nodes from each class that are selected in each step to be proportional to the respective frequency of class i in the data by using a proportional parameter r. Specifically, in each step t we consider Eliav Buchnik and Edith Cohen Algorithm 3: Nearest Seed [11] Input: G = (V, E), distributionW of edge lengths, provided by Classeslabel [i]."}, {"heading": "3 DATASETS AND EXPERIMENTAL SETUP", "text": "In fact, we are able to maneuver ourselves into a situation where we are able to, in which we are able to change the world, in which we are able to change the world, \"he said in an interview with the New York Times."}, {"heading": "4 RESULTS ON BENCHMARK DATASETS", "text": "In this set of experiments, we follow the notification effectiveness and appropriation we mentioned in the table."}, {"heading": "5 RANDOM SPLITS EXPERIMENTS", "text": "rf\u00fc ide eeirteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerrrreeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerrrreeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerrrrrrrrrrrrrrrrrrrVnleeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "6 BOOTSTRAPPING PARAMETER STUDY", "text": "In this section, we take a closer look at how the quality gain in bootstrapping depends on the properties of the data and parameters."}, {"heading": "6.1 Labeling rate", "text": "We examine the accuracy of both the base and bootstrapped algorithms depending on the labeling rate. We used 10 random splits of the data sets. For the citation networks and planted partition diagrams, we varied the labeling rate while maintaining the same number of seeds from each class. Representative results showing the precision depending on the labeling rate of selected data sets are visualized in Figure 1. Figure 2 shows the precision gain by bootstrapping across the base method as a function of the labeling rate. E-diagrams show that the precision of all methods is expected to increase with the labeling rate and that bootstrapping continuously improves performance. We can find that across methods, the precision gain from bootstrapping at the extremes is lower if the 1h ps: / / giubth.com / bootstrapped particles / nonlinear gradient is very low or very low."}, {"heading": "6.2 Seed set augmentations", "text": "We examine the precision gain by balancing the bootstrapping parameter r, which determines the proportion of nodes that are seed-seeded in each bootstrapping step (see Algorithm 4). Figure 3 shows the precision gain relative to r (in percent) for selected datasets. We can see that the gain decreases as expected with r, but that we can achieve significant gains even with relatively large values of r."}, {"heading": "6.3 Number of bootstrapping steps", "text": "We study precision depending on the number of bootstrapping steps performed. In our experiments, we used the power of a validation set to select the step that provides the internal learning labels. In general, we expect precision to improve initially as the more predictable nodes are added to the seed and eventually stabilize or decrease. Figure 4 shows the precision for each step on representative data sets. In this set of experiments, we mixed all the other parameters of each algorithm, as specified in the legends of the Gure, and used a fixed value r = 0.03 for the proportion of nodes that become new seeds in each step. The results are averaged over 10 random splits of the data."}, {"heading": "6.4 Scalability", "text": "erD rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the"}, {"heading": "7 FEATURE DIFFUSION", "text": "In the previous sections, we focused on methods that only use the graph structure: e provided labels of seed nodes are \"di used\" along the edges of graphs, thus obtaining labels for all nodes that are then used for prediction. If we have more information, in the form of node characteristics f i for all nodes i \u2264 n \"+ nu, we can use them for educational purposes. e di used vectors are a smoothed version of the raw vectors, which also includes the properties and labels of seed nodes f i \u2264 n\" i \u2264 n. \"We can instead use the graph structure to obtain di used characteristics f. e di used vectors are a smoothed version of the raw vectors, which is also the characteristics of related nodes re ect, where the relationship is according to the basis di sion process."}, {"heading": "7.1 Experiments settings", "text": "We used the three sets of citation networks (Citeseer, Cora and Pubmed) listed in Table 1. We use the methodology of Section 3 to select training, testing and validation sets. We use the defined benchmark seed sets [24, 40] used in previous work to facilitate comparison, and random splits for robustness. E citation networks contain a bag representation for each document, which we treat as a feature vector following [24, 40]. e vectors are encoded with 0 / 1, indicating the absence / presence of the corresponding term from the dictionary. e dictionaries of Citeseer, Cora and Pubmed contain 3703, 1433 and 500 unique words, respectively. For the classification of feature vectors (original and di used), we have used vectors that we use one against all regressive Coreseer, Cora and Pubmed dictionary, respectively, 3703 and 3733."}, {"heading": "7.2 Results on benchmark datasets", "text": "The results of the benchmark datasets are listed in Table 5. e Table. For comparison, we also list the results obtained from GCN [24] and Planetoid [40] (best variant with node characteristics). We observe the following: First, the quality of learning with the feature vectors used is significantly better than with the raw feature vectors, with an average improvement of about 12%. Therefore, the use of the graph structure and the particular way it was used were important in these datasets. Second, the normalized laplactic FP was more effective than the basic FP, consistent with our observations with the label propagation experiments. Bird, bootstrapping consistently improved performance on two of the datasets (Citeseer and Cora)."}, {"heading": "7.3 Results on random splits", "text": "In this set of experiments, we generated several random splits of the nodes for each set of data to start, test, and validation sets, and calculated the average of the results. Our results are listed in Table 6. For reference, we also list the results we reported in Section 5 and the results 2h p: / / scikit-learn.org / stable / about.html using label propagation (without using node attributes). Eliav Buchnik and Edith Cohen reported similar random splits using GCNs [24]. The results lend robustness to our observations from the benchmark experiments: The use of di-used attributes significantly improves quality, the normalized laplac FP consistently achieves the best results at Citeseer and Cora, and is (within error margins) very close to the results reported by [24] at Pubmed."}, {"heading": "8 CONCLUSION", "text": "We investigated the use of self-training, which is perhaps the most basic form of introducing non-linearity to SSL methods based on linear graph representation. We observed that the resulting bootstrapped di usions ubiquitously improved label quality across base methods on a variety of real and synthetic data sets. In addition, we obtained state-of-the-art quality previously achieved by more complex methods while maintaining the high scalability of base methods. Our results are proof of a concept that uses the simplest base algorithms and boot strapping wrappers. Some natural enhancements include fine-tuning the wrap along with base algorithms that provide more precise results and the use of a richer set of base algorithms."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank Aditya Grover, author of node2vec [20], and Zhilin Yang, author of Planetoid [40], for providing quick and helpful answers to our questions about their work and implementation, as well as Fernando Pereira for his advice, which is partially supported by the Israel Science Foundation (grant number 1841 / 14)."}], "references": [{"title": "Understanding the Yarowsky algorithm", "author": ["S. Abney"], "venue": "Comput. Linguist.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Di\u0082usion-convolutional neural networks", "author": ["J. Atwood", "D. Towsley"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "A mathematical model for small group structures", "author": ["A. Bavelas"], "venue": "Human Organization,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1948}, {"title": "\u008ce formation of networks with transfers among players", "author": ["F. Bloch", "M.O. Jackson"], "venue": "Journal of Economic \u008aeory,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Learning from labeled and unlabeled data using graph mincuts", "author": ["A. Blum", "S. Chawla"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Semi-supervised learning using randomized mincuts", "author": ["A. Blum", "J. La\u0082erty", "M.R. Rwebangira", "R. Reddy"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Toward an architecture for never-ending language learning", "author": ["J. Carlson", "A. Be\u008aeridge", "B. Kisiel", "B. Se\u008ales", "E.R. Hruschka", "Jr.", "T.M. Mitchell"], "venue": "In AAAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Semi-supervised learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Spectral Graph \u008aeory", "author": ["F.R.K. Chung"], "venue": "American Mathematical Society,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Size-estimation framework with applications to transitive closure and reachability", "author": ["E. Cohen"], "venue": "J. Comput. System Sci.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Semi-supervised learning on graphs through reach and distance di\u0082usion", "author": ["E. Cohen"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Scalable similarity estimation in social networks: Closeness, node labels, and random edge lengths", "author": ["E. Cohen", "D. Delling", "F. Fuchs", "A. Goldberg", "M. Goldszmidt", "R. Werneck"], "venue": "In COSN. ACM,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Distance-based in\u0083uence in networks: Computation and maximization", "author": ["E. Cohen", "D. Delling", "T. Pajor", "R.F. Werneck"], "venue": "Technical Report cs.SI/1410.6976,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Spatially-decaying aggregation over a network: Model and algorithms", "author": ["E. Cohen", "H. Kaplan"], "venue": "J. Comput. System Sci.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Algorithms for graph partitioning on the planted partition model", "author": ["A. Condon", "R.M. Karp"], "venue": "Random Struct. Algorithms,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Convolutional neural networks on graphs with fast localized spectral \u0080ltering", "author": ["M. De\u0082errard", "X. Bresson", "P. Vandergheynst"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Scalable in\u0083uence estimation in continuous-time di\u0082usion networks", "author": ["N. Du", "L. Song", "M. Gomez-Rodriguez", "H. Zha"], "venue": "In NIPS", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Centrality in social networks: Conceptual clari\u0080cation", "author": ["L.C. Freeman"], "venue": "Social Networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1979}, {"title": "Inferring networks of di\u0082usion and in\u0083uence", "author": ["M. Gomez-Rodriguez", "J. Leskovec", "A. Krause"], "venue": "In KDD,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "node2vec: Scalable feature learning for networks", "author": ["A. Grover", "J. Leskovec"], "venue": "In KDD. ACM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Deep convolutional networks on graphstructured data", "author": ["M. Hena", "J. Bruna", "Y. LeCun"], "venue": "CoRR, abs/1506.05163,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Transductive inference for text classi\u0080cation using support vector machines", "author": ["T. Joachims"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Maximizing the spread of in\u0083uence through a social network", "author": ["D. Kempe", "J.M. Kleinberg", "\u00c9. Tardos"], "venue": "In KDD. ACM,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Semi-supervised classi\u0080cation with graph convolutional networks", "author": ["T.N. Kipf", "M. Welling"], "venue": "In ICLR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2017}, {"title": "\u008ce multirank bootstrap algorithm: Self-supervised political blog classi\u0080cation and ranking using semi-supervised link classi\u0080cation", "author": ["F. Lin", "W.W. Cohen"], "venue": "In ICWSM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Pregel: a system for large-scale graph processing", "author": ["G.M.H. Malewicz", "Austern", "A.J.C Bik", "J.C. Dehnert", "I. Horn", "N. Leiser", "G. Czajkowski"], "venue": "In SIGMOD. ACM,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Bha\u008aacharjee. Measurement and Analysis of Online Social Networks", "author": ["A. Mislove", "M. Marcon", "K.P. Gummadi", "P. Druschel"], "venue": "In IMC,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Structural neighborhood based classi\u0080cation of nodes in a network", "author": ["S. Nandanwar", "N.N. Murty"], "venue": "In KDD", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Node centrality in weighted networks: Generalizing degree and shortest paths", "author": ["T. Opsahl", "F. Agneessens", "J. Skvoretz"], "venue": "Social Networks,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "\u008ce pagerank citation ranking: Bringing order to the web", "author": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": "Technical report, Stanford InfoLab,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1999}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "In KDD. ACM,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Large-scale semi-supervised learning using streaming approximation", "author": ["S. Ravi", "Q. Diao"], "venue": "In AISTATS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "\u008ce centrality index of a graph", "author": ["G. Sabidussi"], "venue": "Psychometrika, 31(4):581\u2013603,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1966}, {"title": "Probability of error of some adaptive pa\u008aern-recognition machines", "author": ["H.J. Scudder"], "venue": "IEEE Transactions on Information \u008aeory,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1965}, {"title": "Collective classi\u0080cation in network data", "author": ["P. Sen", "G. Namata", "M. Bilgic", "L. Getoor", "B. Gallagher", "T. Eliassi-Rad"], "venue": "AI Magazine,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "A local clustering algorithm for massive graphs and its application to nearly linear time graph partitioning", "author": ["D.A. Spielman", "S-H Teng"], "venue": "SIAM J. Comput.,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Graph-based semi-supervised learning", "author": ["A. Subramanya", "P.P. Talukdar"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Bootstrapping via graph propagation", "author": ["M. Whitney", "A. Sarkar"], "venue": "In ACL,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "Revisiting semi-supervised learning with graph embeddings", "author": ["Z. Yang", "W.W. Cohen", "R. Salakhutdinov"], "venue": "In ICML. JMLR.org,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["D. Yarowsky"], "venue": "In ACL,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1995}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T. Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "In NIPS,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2004}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["X. Zhu", "Z. Ghahramani"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2002}, {"title": "Semi-supervised learning using Gaussian \u0080elds and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. La\u0082ery"], "venue": "In ICML,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2003}], "referenceMentions": [{"referenceID": 7, "context": "the surveys [8, 38].", "startOffset": 12, "endOffset": 19}, {"referenceID": 37, "context": "the surveys [8, 38].", "startOffset": 12, "endOffset": 19}, {"referenceID": 8, "context": "Most popular SSL methods can be interpreted through underlying spectral di\u0082usions [9], utilizing the graph Laplacian, graph cuts [5, 6], and random walks.", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "Most popular SSL methods can be interpreted through underlying spectral di\u0082usions [9], utilizing the graph Laplacian, graph cuts [5, 6], and random walks.", "startOffset": 129, "endOffset": 135}, {"referenceID": 5, "context": "Most popular SSL methods can be interpreted through underlying spectral di\u0082usions [9], utilizing the graph Laplacian, graph cuts [5, 6], and random walks.", "startOffset": 129, "endOffset": 135}, {"referenceID": 43, "context": "\u008cey include label propagation [44], label propagation using the normalized graph Laplacian [25, 42], and many variations.", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "\u008cey include label propagation [44], label propagation using the normalized graph Laplacian [25, 42], and many variations.", "startOffset": 91, "endOffset": 99}, {"referenceID": 41, "context": "\u008cey include label propagation [44], label propagation using the normalized graph Laplacian [25, 42], and many variations.", "startOffset": 91, "endOffset": 99}, {"referenceID": 32, "context": "\u008ce algorithms are applied successfully to massive graphs with billions of edges [33] using highly distributed platforms [26].", "startOffset": 80, "endOffset": 84}, {"referenceID": 25, "context": "\u008ce algorithms are applied successfully to massive graphs with billions of edges [33] using highly distributed platforms [26].", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 127, "endOffset": 149}, {"referenceID": 3, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 127, "endOffset": 149}, {"referenceID": 13, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 127, "endOffset": 149}, {"referenceID": 17, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 127, "endOffset": 149}, {"referenceID": 29, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 127, "endOffset": 149}, {"referenceID": 33, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 127, "endOffset": 149}, {"referenceID": 12, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 160, "endOffset": 176}, {"referenceID": 16, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 160, "endOffset": 176}, {"referenceID": 18, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 160, "endOffset": 176}, {"referenceID": 22, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 160, "endOffset": 176}, {"referenceID": 11, "context": "Another class of graph di\u0082usions, which we refer to here as social, underline classic social and economic models of centrality [3, 4, 14, 18, 30, 34], in\u0083uence [13, 17, 19, 23], and similarity [12] of nodes.", "startOffset": 193, "endOffset": 197}, {"referenceID": 11, "context": "A powerful extension de\u0080nes a generative model from a graph by randomizing the presence (with reach di\u0082usion) or the length (with distance di\u0082usion) of edges [12, 13, 17, 19, 23] and then works with respective expectations.", "startOffset": 158, "endOffset": 178}, {"referenceID": 12, "context": "A powerful extension de\u0080nes a generative model from a graph by randomizing the presence (with reach di\u0082usion) or the length (with distance di\u0082usion) of edges [12, 13, 17, 19, 23] and then works with respective expectations.", "startOffset": 158, "endOffset": 178}, {"referenceID": 16, "context": "A powerful extension de\u0080nes a generative model from a graph by randomizing the presence (with reach di\u0082usion) or the length (with distance di\u0082usion) of edges [12, 13, 17, 19, 23] and then works with respective expectations.", "startOffset": 158, "endOffset": 178}, {"referenceID": 18, "context": "A powerful extension de\u0080nes a generative model from a graph by randomizing the presence (with reach di\u0082usion) or the length (with distance di\u0082usion) of edges [12, 13, 17, 19, 23] and then works with respective expectations.", "startOffset": 158, "endOffset": 178}, {"referenceID": 22, "context": "A powerful extension de\u0080nes a generative model from a graph by randomizing the presence (with reach di\u0082usion) or the length (with distance di\u0082usion) of edges [12, 13, 17, 19, 23] and then works with respective expectations.", "startOffset": 158, "endOffset": 178}, {"referenceID": 22, "context": "Social di\u0082usion, inspired by the independent cascade model of [23] and the continuous time (distance-based) model of [12, 13, 17, 19] was recently adapted to SSL [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "Social di\u0082usion, inspired by the independent cascade model of [23] and the continuous time (distance-based) model of [12, 13, 17, 19] was recently adapted to SSL [11].", "startOffset": 117, "endOffset": 133}, {"referenceID": 12, "context": "Social di\u0082usion, inspired by the independent cascade model of [23] and the continuous time (distance-based) model of [12, 13, 17, 19] was recently adapted to SSL [11].", "startOffset": 117, "endOffset": 133}, {"referenceID": 16, "context": "Social di\u0082usion, inspired by the independent cascade model of [23] and the continuous time (distance-based) model of [12, 13, 17, 19] was recently adapted to SSL [11].", "startOffset": 117, "endOffset": 133}, {"referenceID": 18, "context": "Social di\u0082usion, inspired by the independent cascade model of [23] and the continuous time (distance-based) model of [12, 13, 17, 19] was recently adapted to SSL [11].", "startOffset": 117, "endOffset": 133}, {"referenceID": 10, "context": "Social di\u0082usion, inspired by the independent cascade model of [23] and the continuous time (distance-based) model of [12, 13, 17, 19] was recently adapted to SSL [11].", "startOffset": 162, "endOffset": 166}, {"referenceID": 9, "context": "\u008ce use of distance or reachability sketching based on [10] allows for highly scalable label learning also over the sketched a\u0081nity matrix.", "startOffset": 54, "endOffset": 58}, {"referenceID": 32, "context": "Both spectral and social di\u0082usion based SSL models scale well even with a large number of labels, using heavy hi\u008aer sketches with label propagation [33] or naturally with social di\u0082usion using sketches.", "startOffset": 148, "endOffset": 152}, {"referenceID": 31, "context": "In particular, DeepWalk [32] applied the hugely successful word embedding framework of [27] to embed the graph nodes in a way that preserves the a\u0081nity relation de\u0080ned by co-occurrence frequencies of pairs in short random walks: A so\u0089max applied to inner products of embeddings approximates the frequency of the pair.", "startOffset": 24, "endOffset": 28}, {"referenceID": 26, "context": "In particular, DeepWalk [32] applied the hugely successful word embedding framework of [27] to embed the graph nodes in a way that preserves the a\u0081nity relation de\u0080ned by co-occurrence frequencies of pairs in short random walks: A so\u0089max applied to inner products of embeddings approximates the frequency of the pair.", "startOffset": 87, "endOffset": 91}, {"referenceID": 19, "context": "Node2vec [20] re\u0080ned the approach using hyperparameters that tune the depth and breadth of the random", "startOffset": 9, "endOffset": 13}, {"referenceID": 39, "context": "Another method, Planetoid, used a multi-layer neural network instead of a single so\u0089max layer [40].", "startOffset": 94, "endOffset": 98}, {"referenceID": 1, "context": "Another successful proposal are Graph convolutional networks (GCN) [2, 16, 21, 24], which are neural networks with layers that follow the graph adjacency structure.", "startOffset": 67, "endOffset": 82}, {"referenceID": 15, "context": "Another successful proposal are Graph convolutional networks (GCN) [2, 16, 21, 24], which are neural networks with layers that follow the graph adjacency structure.", "startOffset": 67, "endOffset": 82}, {"referenceID": 20, "context": "Another successful proposal are Graph convolutional networks (GCN) [2, 16, 21, 24], which are neural networks with layers that follow the graph adjacency structure.", "startOffset": 67, "endOffset": 82}, {"referenceID": 23, "context": "Another successful proposal are Graph convolutional networks (GCN) [2, 16, 21, 24], which are neural networks with layers that follow the graph adjacency structure.", "startOffset": 67, "endOffset": 82}, {"referenceID": 28, "context": "\u008cis abundance of recent work introduced many new components and o\u0089en at the same time: Low-rank embeddings, non-linear propagations, learning of weights of hidden layers (GCNs), learning of node weights [29].", "startOffset": 203, "endOffset": 207}, {"referenceID": 34, "context": "Self-training is arguably the earliest approach to SSL, dating back \u0080ve decades to Scudder [35], and extensively studied by the NLP community [1, 39, 41].", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "Self-training is arguably the earliest approach to SSL, dating back \u0080ve decades to Scudder [35], and extensively studied by the NLP community [1, 39, 41].", "startOffset": 142, "endOffset": 153}, {"referenceID": 38, "context": "Self-training is arguably the earliest approach to SSL, dating back \u0080ve decades to Scudder [35], and extensively studied by the NLP community [1, 39, 41].", "startOffset": 142, "endOffset": 153}, {"referenceID": 40, "context": "Self-training is arguably the earliest approach to SSL, dating back \u0080ve decades to Scudder [35], and extensively studied by the NLP community [1, 39, 41].", "startOffset": 142, "endOffset": 153}, {"referenceID": 43, "context": "We use classic Label propagation [44], Label propagation using the normalized Laplacian [42], and nearest-seed, which is the simplest distance di\u0082usion model [11].", "startOffset": 33, "endOffset": 37}, {"referenceID": 41, "context": "We use classic Label propagation [44], Label propagation using the normalized Laplacian [42], and nearest-seed, which is the simplest distance di\u0082usion model [11].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "We use classic Label propagation [44], Label propagation using the normalized Laplacian [42], and nearest-seed, which is the simplest distance di\u0082usion model [11].", "startOffset": 158, "endOffset": 162}, {"referenceID": 23, "context": "We apply the di\u0082erent methods to benchmark data and seed sets used and made available by previous work [24, 40].", "startOffset": 103, "endOffset": 111}, {"referenceID": 39, "context": "We apply the di\u0082erent methods to benchmark data and seed sets used and made available by previous work [24, 40].", "startOffset": 103, "endOffset": 111}, {"referenceID": 31, "context": "We compare the quality of the learned labels to state of the art baselines, including DeepWalk [32], node2vec [20], Planetoid [40], and GCNs [24].", "startOffset": 95, "endOffset": 99}, {"referenceID": 19, "context": "We compare the quality of the learned labels to state of the art baselines, including DeepWalk [32], node2vec [20], Planetoid [40], and GCNs [24].", "startOffset": 110, "endOffset": 114}, {"referenceID": 39, "context": "We compare the quality of the learned labels to state of the art baselines, including DeepWalk [32], node2vec [20], Planetoid [40], and GCNs [24].", "startOffset": 126, "endOffset": 130}, {"referenceID": 23, "context": "We compare the quality of the learned labels to state of the art baselines, including DeepWalk [32], node2vec [20], Planetoid [40], and GCNs [24].", "startOffset": 141, "endOffset": 145}, {"referenceID": 14, "context": "We also perform more elaborate experiments on additional data and seed sets and on the well-studied planted partition (stochastic block) model [15] which is o\u0089en used to understand the performance of clustering and community detection algorithms.", "startOffset": 143, "endOffset": 147}, {"referenceID": 39, "context": "Furthermore, our results dominated those reported (with the use of node features) by the state of the art baselines Planetoid [40] and GCNs [24].", "startOffset": 126, "endOffset": 130}, {"referenceID": 23, "context": "Furthermore, our results dominated those reported (with the use of node features) by the state of the art baselines Planetoid [40] and GCNs [24].", "startOffset": 140, "endOffset": 144}, {"referenceID": 41, "context": "In particular, we discuss two textbook label propagations methods [42, 43] and a distance-di\u0082usion method [11].", "startOffset": 66, "endOffset": 74}, {"referenceID": 42, "context": "In particular, we discuss two textbook label propagations methods [42, 43] and a distance-di\u0082usion method [11].", "startOffset": 66, "endOffset": 74}, {"referenceID": 10, "context": "In particular, we discuss two textbook label propagations methods [42, 43] and a distance-di\u0082usion method [11].", "startOffset": 106, "endOffset": 110}, {"referenceID": 10, "context": "We note that o\u0089en higher quality learned labels are obtaining by training a learning algorithm on the so\u0089 label [11] but in our experiments here we used these common simple class predictions.", "startOffset": 112, "endOffset": 116}, {"referenceID": 42, "context": "Label propagation (LP) Zhu and Ghahramani [43].", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "Pseudocode is provided as Algorithm 1 (As presented in [8]).", "startOffset": 55, "endOffset": 58}, {"referenceID": 42, "context": "Algorithm 1: Label propagation (LP) [43]", "startOffset": 36, "endOffset": 40}, {"referenceID": 41, "context": "Normalized Laplacian label propagation Zhou et al [42].", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "\u008cis algorithm is related to Personalized Page Rank (PPR) and uses the normalized graph Laplacian [9].", "startOffset": 97, "endOffset": 100}, {"referenceID": 41, "context": "Algorithm 2: Normalized Laplacian LP [42]", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": "We consider recently proposed SSL methods based on distance di\u0082usion [8].", "startOffset": 69, "endOffset": 72}, {"referenceID": 7, "context": "Nearest-seed [8].", "startOffset": 13, "endOffset": 16}, {"referenceID": 10, "context": "Guided by [11] \u008ce lengths of edges are drawn independently from an exponential distribution with parameter that is equal to the inverse of the degree of the source node with possibly a \u0080xed o\u0082set:", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "We comment that our experiments did not exploit the full power of the rich class of distance-based model proposed in [11].", "startOffset": 117, "endOffset": 121}, {"referenceID": 0, "context": "\u008ce self-training framework has many variants [1, 41].", "startOffset": 45, "endOffset": 52}, {"referenceID": 40, "context": "\u008ce self-training framework has many variants [1, 41].", "startOffset": 45, "endOffset": 52}, {"referenceID": 10, "context": "Algorithm 3: Nearest Seed [11]", "startOffset": 26, "endOffset": 30}, {"referenceID": 39, "context": "To facilitate comparison, we use the benchmark dataset and seed set combinations used in prior work [40].", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "We note that the base and bootstrapped algorithms we use can naturally be extended to a multi-label se\u008aing but there are di\u0082erent mechanisms to do so [11, 20] that may or may not assume that the number of classes of each node is provided.", "startOffset": 150, "endOffset": 158}, {"referenceID": 19, "context": "We note that the base and bootstrapped algorithms we use can naturally be extended to a multi-label se\u008aing but there are di\u0082erent mechanisms to do so [11, 20] that may or may not assume that the number of classes of each node is provided.", "startOffset": 150, "endOffset": 158}, {"referenceID": 23, "context": "Moreover, some of the algorithms we compare with [24] do not have multi-label variants.", "startOffset": 49, "endOffset": 53}, {"referenceID": 35, "context": "\u008ce data sets include three citation networks: Cora, Pubmed, and Citeseer from [36], one Knowledge graph (entity classi\u0080cation) dataset (NELL) preprocessed by [40] from [7], and the YouTube group membership data set from [28].", "startOffset": 78, "endOffset": 82}, {"referenceID": 39, "context": "\u008ce data sets include three citation networks: Cora, Pubmed, and Citeseer from [36], one Knowledge graph (entity classi\u0080cation) dataset (NELL) preprocessed by [40] from [7], and the YouTube group membership data set from [28].", "startOffset": 158, "endOffset": 162}, {"referenceID": 6, "context": "\u008ce data sets include three citation networks: Cora, Pubmed, and Citeseer from [36], one Knowledge graph (entity classi\u0080cation) dataset (NELL) preprocessed by [40] from [7], and the YouTube group membership data set from [28].", "startOffset": 168, "endOffset": 171}, {"referenceID": 27, "context": "\u008ce data sets include three citation networks: Cora, Pubmed, and Citeseer from [36], one Knowledge graph (entity classi\u0080cation) dataset (NELL) preprocessed by [40] from [7], and the YouTube group membership data set from [28].", "startOffset": 220, "endOffset": 224}, {"referenceID": 39, "context": "Our seed set selection followed [40].", "startOffset": 32, "endOffset": 36}, {"referenceID": 14, "context": "We also use synthetic data generated using the planted partition (stochastic block) random graph model [15].", "startOffset": 103, "endOffset": 107}, {"referenceID": 0, "context": "\u008cese random graphs are speci\u0080ed by a number L of equal-size classes, the total number of nodes, and two parameters q < p \u2208 [0, 1].", "startOffset": 123, "endOffset": 129}, {"referenceID": 42, "context": "We used our own Python implementation of the three base linear di\u0082usions discussed in Section 3: Label propagation (LP) [43] (Algorithm 1) Normalized Laplacian LP [42] (Algorithm 2)", "startOffset": 120, "endOffset": 124}, {"referenceID": 41, "context": "We used our own Python implementation of the three base linear di\u0082usions discussed in Section 3: Label propagation (LP) [43] (Algorithm 1) Normalized Laplacian LP [42] (Algorithm 2)", "startOffset": 163, "endOffset": 167}, {"referenceID": 10, "context": "and Nearest Seed [11] (Algorithm 3) and also the bootstrapping wrapper (Algorithm 4).", "startOffset": 17, "endOffset": 21}, {"referenceID": 23, "context": "In this set of experiments we follow the benchmark seed set and test set selection of [24, 40] with the properties as listed in Table 1.", "startOffset": 86, "endOffset": 94}, {"referenceID": 39, "context": "In this set of experiments we follow the benchmark seed set and test set selection of [24, 40] with the properties as listed in Table 1.", "startOffset": 86, "endOffset": 94}, {"referenceID": 19, "context": "\u2022 node2vec [20]: We used the published Python code of [20] (which builds on [27, 32]) to compute an embedding.", "startOffset": 11, "endOffset": 15}, {"referenceID": 19, "context": "\u2022 node2vec [20]: We used the published Python code of [20] (which builds on [27, 32]) to compute an embedding.", "startOffset": 54, "endOffset": 58}, {"referenceID": 26, "context": "\u2022 node2vec [20]: We used the published Python code of [20] (which builds on [27, 32]) to compute an embedding.", "startOffset": 76, "endOffset": 84}, {"referenceID": 31, "context": "\u2022 node2vec [20]: We used the published Python code of [20] (which builds on [27, 32]) to compute an embedding.", "startOffset": 76, "endOffset": 84}, {"referenceID": 19, "context": "0} forp,q and set other hyperparameters (10 epochs, 10 walks, walk length 80, embedding dimension 128) as in [20] .", "startOffset": 109, "endOffset": 113}, {"referenceID": 31, "context": "\u2022 DeepWalk [32]: \u008ce parameter se\u008aing p = q = 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 39, "context": "For reference, we also list the results as reported in [40] using a di\u0082erent implementation.", "startOffset": 55, "endOffset": 59}, {"referenceID": 39, "context": "\u2022 Planetoid-G [40].", "startOffset": 14, "endOffset": 18}, {"referenceID": 39, "context": "We report the results as listed in [40].", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "\u2022 Transductive support vector machines (TSVM) [22], for which we list the results reported in [40] when available \u2013 due to scalability issues.", "startOffset": 46, "endOffset": 50}, {"referenceID": 39, "context": "\u2022 Transductive support vector machines (TSVM) [22], for which we list the results reported in [40] when available \u2013 due to scalability issues.", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "\u2022 Graph convolutional networks (GCN): Kipf and Welling [24] used the benchmark data and seed sets of [40] but only reported results with the use of node features.", "startOffset": 55, "endOffset": 59}, {"referenceID": 39, "context": "\u2022 Graph convolutional networks (GCN): Kipf and Welling [24] used the benchmark data and seed sets of [40] but only reported results with the use of node features.", "startOffset": 101, "endOffset": 105}, {"referenceID": 39, "context": "We only list their results on the NELL dataset, where node features did not enhance performance according to [40].", "startOffset": 109, "endOffset": 113}, {"referenceID": 39, "context": "We note that the results for LP reported in [40] used a di\u0082erent implementation (Junto with a \u0080xed number of 100 iterations).", "startOffset": 44, "endOffset": 48}, {"referenceID": 39, "context": "For the NELL graph, produced by [40], we only used the provided 2000 labeled nodes and selected a seed set with 0.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "TSVM [22] *[40] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 39, "context": "TSVM [22] *[40] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 31, "context": "DeepWalk [32] *[40] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 39, "context": "DeepWalk [32] *[40] 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 31, "context": "DeepWalk [32] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "node2vec [20] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 39, "context": "Planetoid-G [40] *[40] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 39, "context": "Planetoid-G [40] *[40] 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": "Graph Conv Nets [24] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "Nearest-seed [11] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 42, "context": "Label Propagation[43] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 41, "context": "Norm Lap LP [42] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 31, "context": "DeepWalk [32] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "node2vec [20] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "Nearest-seed [11] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 42, "context": "Label Propagation[43] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 41, "context": "Norm Lap LP [42] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "the in-memory Python implementation [20].", "startOffset": 36, "endOffset": 40}, {"referenceID": 42, "context": "Finally, we note that we tested the bootstrapping wrapper with two other implementations of Label Propagation including: \u008ce sklearn Python library and Junto [43]1 and observed similar performance gains over the base methods.", "startOffset": 157, "endOffset": 161}, {"referenceID": 25, "context": "\u008ce computation of each iteration involves a linear number of edge traversals and is highly suitable for Pregel-like [26] distributed graph processing platforms.", "startOffset": 116, "endOffset": 120}, {"referenceID": 25, "context": "\u008cis computation can also be performed e\u0081ciently on Pregel [26] by essentially performing all iterations (di\u0082erent sets of hash-speci\u0080ed edge lengths) together.", "startOffset": 58, "endOffset": 62}, {"referenceID": 42, "context": "Feature Propagation (FP), in Algorithm 5, that uses the di\u0082usion rule of the label propagation method of [43] (Algorithm 1), and Normalized Laplacian FP, in Algorithm 6, that uses the di\u0082usion rule of [42] (Algorithm 2).", "startOffset": 105, "endOffset": 109}, {"referenceID": 41, "context": "Feature Propagation (FP), in Algorithm 5, that uses the di\u0082usion rule of the label propagation method of [43] (Algorithm 1), and Normalized Laplacian FP, in Algorithm 6, that uses the di\u0082usion rule of [42] (Algorithm 2).", "startOffset": 201, "endOffset": 205}, {"referenceID": 1, "context": "We note that our linear feature di\u0082usions can be viewed as a toneddown GCN [2, 24], without the backprop training, and non-linear aggregations.", "startOffset": 75, "endOffset": 82}, {"referenceID": 23, "context": "We note that our linear feature di\u0082usions can be viewed as a toneddown GCN [2, 24], without the backprop training, and non-linear aggregations.", "startOffset": 75, "endOffset": 82}, {"referenceID": 23, "context": "We use the benchmark \u0080xed seed sets used in prior work [24, 40], to facilitate comparison, and random splits for robustness.", "startOffset": 55, "endOffset": 63}, {"referenceID": 39, "context": "We use the benchmark \u0080xed seed sets used in prior work [24, 40], to facilitate comparison, and random splits for robustness.", "startOffset": 55, "endOffset": 63}, {"referenceID": 23, "context": "\u008ce citation networks contain a bag-of-words representation for each document which, following [24, 40], we treat as a feature vector.", "startOffset": 94, "endOffset": 102}, {"referenceID": 39, "context": "\u008ce citation networks contain a bag-of-words representation for each document which, following [24, 40], we treat as a feature vector.", "startOffset": 94, "endOffset": 102}, {"referenceID": 23, "context": "For comparison, we also list the quality reported by GCN [24] and Planetoid [40] (best variant with node features).", "startOffset": 57, "endOffset": 61}, {"referenceID": 39, "context": "For comparison, we also list the quality reported by GCN [24] and Planetoid [40] (best variant with node features).", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "Fourth, the bootstrapped version of normalized Laplacian FP improves over the state of the art results of GCN [24] on Citeseer and Cora.", "startOffset": 110, "endOffset": 114}, {"referenceID": 23, "context": "reported on similar random splits using GCNs [24].", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "\u008ce results add robustness to our observations from the benchmark experiments: \u008ce use of di\u0082used features signi\u0080cantly improves quality, the normalized Laplacian FP consistently achieves the best results on Citeseer and Cora and is very close (within error margins) to the results reported by [24] on Pubmed.", "startOffset": 292, "endOffset": 296}, {"referenceID": 41, "context": "Norm Lap LP[42] +Bootstrapped 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 23, "context": "Graph Conv Nets [24] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 39, "context": "Planetoid [40] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 42, "context": "Label Propagation[43] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 41, "context": "Norm Lap LP [42] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "Graph Conv Nets [24] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 8, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 11, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 12, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 13, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 16, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 18, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 22, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 29, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 30, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 213, "endOffset": 246}, {"referenceID": 36, "context": "On a \u0080nal note, we recall that spectral and social graph di\u0082usions are an important tool in graph mining: \u008cey are the basis of centrality, in\u0083uence, and similarity measures of a node or sets of nodes in a network [4, 9, 12\u201314, 17, 19, 23, 30, 31] and also underline community detection and local clustering algorithms [37].", "startOffset": 318, "endOffset": 322}, {"referenceID": 19, "context": "\u008ce authors would like to thank Aditya Grover, the author of node2vec [20] and Zhilin Yang, the author of Planetoid [40] for prompt and helpful answers to our questions on their work and implementations, and to Fernando Pereira for his advice.", "startOffset": 69, "endOffset": 73}, {"referenceID": 39, "context": "\u008ce authors would like to thank Aditya Grover, the author of node2vec [20] and Zhilin Yang, the author of Planetoid [40] for prompt and helpful answers to our questions on their work and implementations, and to Fernando Pereira for his advice.", "startOffset": 115, "endOffset": 119}], "year": 2017, "abstractText": "Graph-based semi-supervised learning (SSL) algorithms predict labels for all nodes based on provided labels of a small set of seed nodes. Classic methods capture the graph structure through some underlying di\u0082usion process that propagates through the graph edges. Spectral di\u0082usion, which includes personalized page rank and label propagation, propagates through random walks. Social di\u0082usion propagates through shortest paths. A common ground to these di\u0082usions is their linearity, which does not distinguish between contributions of few \u201cstrong\u201d relations and many \u201cweak\u201d relations. Recently, non-linear methods such as node embeddings and graph convolutional networks (GCN) demonstrated a large gain in quality for SSL tasks. \u008cese methods introduce multiple components and greatly vary on how the graph structure, seed label information, and other features are used. We aim here to study the contribution of non-linearity, as an isolated ingredient, to the performance gain. To do so, we place classic linear graph di\u0082usions in a self-training framework. Surprisingly, we observe that SSL using the resulting bootstrapped di\u0082usions not only signi\u0080cantly improves over the respective non-bootstrapped baselines but also outperform state-of-the-art non-linear SSL methods. Moreover, since the self-training wrapper retains the scalability of the base method, we obtain both higher quality and be\u008aer scalability.", "creator": "LaTeX with hyperref package"}}}