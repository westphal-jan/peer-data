{"id": "1703.02620", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Linguistic Knowledge as Memory for Recurrent Neural Networks", "abstract": "Training recurrent neural networks to model long term dependencies is difficult. Hence, we propose to use external linguistic knowledge as an explicit signal to inform the model which memories it should utilize. Specifically, external knowledge is used to augment a sequence with typed edges between arbitrarily distant elements, and the resulting graph is decomposed into directed acyclic subgraphs. We introduce a model that encodes such graphs as explicit memory in recurrent neural networks, and use it to model coreference relations in text. We apply our model to several text comprehension tasks and achieve new state-of-the-art results on all considered benchmarks, including CNN, bAbi, and LAMBADA. On the bAbi QA tasks, our model solves 15 out of the 20 tasks with only 1000 training examples per task. Analysis of the learned representations further demonstrates the ability of our model to encode fine-grained entity information across a document.", "histories": [["v1", "Tue, 7 Mar 2017 22:13:17 GMT  (1152kb,D)", "http://arxiv.org/abs/1703.02620v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bhuwan dhingra", "zhilin yang", "william w cohen", "ruslan salakhutdinov"], "accepted": false, "id": "1703.02620"}, "pdf": {"name": "1703.02620.pdf", "metadata": {"source": "META", "title": "Linguistic Knowledge as Memory for Recurrent Neural Networks", "authors": ["Bhuwan Dhingra", "Zhilin Yang", "William W. Cohen", "Ruslan Salakhutdinov"], "emails": ["gra@cs.cmu.edu>."], "sections": [{"heading": "1. Introduction", "text": "The predictions that include such data require accurate modelling of dependencies between elements of the sequence, which may be very distant from each other. \"However, the dependencies of interest in the sequence can be viewed in a purely data-driven manner, in which recursive neural networks (RNs) are the architecture of choice.\" \"However, the dependencies of interest in the sequence are notoriously difficult to detect in order to detect prolonged dependencies (Koutnik et al., 2014; Bengio et al., 1994).Hochreiter & Schmidhuber (1997) introduced Long Short Term Memory Networks, which use a special unit called the Constant Error Carousel (CEC) to the this1School of Computer Science, Carnegie Mellon University, USA."}, {"heading": "2. Related Work", "text": "This year it is more than ever before."}, {"heading": "3. Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. From Sequences to DAGs", "text": "Suppose that together with the input sequence x1,., xT, where xi-Rdin, we also get information about which pairs of elements are connected to each other. Suppose that these additional \"edges\" are typed - i.e., they belong to one of several different categories. Such additional information is common in natural language processing (NLP). For example, one edge type could combine several mentions of the same entity (core reference), while another edge type would combine generic terms with their specific instances (hyponymy and hypernymia). Figure 1 shows a simple example. Each part of text can be expanded in this way by using standard preprocessing tools such as coreference taggers and entity linkers.Let G = (X, E) denote the resulting directed graph containing the sequential edges between successive elements, as well as the extra typed edges."}, {"heading": "3.2. MAGE-GRUs", "text": "We ask ourselves whether we are able to parameterise these two functions by decomposing and maintaining the hidden states in the hidden states in the hidden states. Intuition behind this is that, for example, the representation by the black edges in Figure 1 need not be the same as it is in the red or green edges.As t variations from 1 to T. The hidden states in the hidden states in Ef. The intuition behind this is that the representation by the black edges in Figure 1 need not be the same."}, {"heading": "3.3. Multiple Sequences", "text": "Following on from our motivational example, Figure 3 shows an example where the first sequence is a context passage and the second sequence is a question posed over the passage, and the sequences are further magnified by co-references and hypernymic relationships, resulting in an undirected cyclic graph. We want to split this graph into a collection of DAGs and use the MAGE-GRU shown above to learn representations of the elements in the sequences, and we also want to maintain the order of the original sequences in the dissected DAGs. Suppose we have S sequences X1,... XS. One way to do this is as follows: For each permutation of the sequences (Xk1, Xk2,...) we want to maintain the original sequence in the dissected DAGs. Let's treat it as a single long sequence and dissect it into forward and backward subgraphs, as described in section 3.1."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Text Comprehension with Coreference", "text": "In the extractive setting, the response is a range of text in the document context. Often, a number of Candidate C is available, in which case the task becomes a classification task. Several large-scale benchmarks (Rajpurkar et al., 2016; Onishi et al., 2016) and deep learning models (Munkhdalai & Yu, 2016) have been suggested for this task. Capturing a document requires complex thought processes and prior knowledge of the part of the reader. One of these processes is correlation resolution, where the reader must identify expressions in the text that relate to the same entity (see red edges in Figures 1, 3). Chu et al. (2016) show that existing state-of-the-art models perform poorly in cases where some form of explicit resolution is required."}, {"heading": "4.2. Performance Comparison", "text": "This year it is more than ever before."}, {"heading": "5. Conclusions", "text": "We have provided a framework for incorporating symbolic knowledge such as linguistic relationships between tokens in text into recursive neural networks. We interpret these relationships as an explicit memory signal and extend the chain of structured RNN with edges that connect the arguments of the relationships. Our model, MAGE-RNN, parameterizes each edge separately and also maintains a separate hidden state representation for unique edges at each node. It can be interpreted as a memory-enhanced RNN in which memory access is dictated by the graph structure. We apply the MAGE-RNN framework to model the coreference for text comprehension tasks by extracting coreference relationships and replacing recurring units in communication models with MAGE-RNN. We observe consistent improvements in three widely studied benchmarks, both for simple and sophisticated architectures."}, {"heading": "Acknowledgments", "text": "This work was funded by NSF as part of CCF14030 and Google Research."}], "references": [{"title": "A neural knowledge language model", "author": ["References Ahn", "Sungjin", "Choi", "Heeyoul", "P\u00e4rnamaa", "Tanel", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1608.00318,", "citeRegEx": "Ahn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Bollacker", "Kurt", "Evans", "Colin", "Paritosh", "Praveen", "Sturge", "Tim", "Taylor", "Jamie"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "A thorough examination of the cnn/daily mail reading comprehension", "author": ["Chen", "Danqi", "Bolton", "Jason", "Manning", "Christopher D"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Gulcehre", "Caglar", "Bahdanau", "Dzmitry", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Broad context language modeling as reading comprehension", "author": ["Chu", "Zewei", "Wang", "Hai", "Gimpel", "Kevin", "McAllester", "David"], "venue": "arXiv preprint arXiv:1610.08431,", "citeRegEx": "Chu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2016}, {"title": "Frustratingly short attention spans in neural language modeling", "author": ["Daniluk", "Micha", "Rocktschel", "Tim", "Welbl", "Johannes", "Riedel", "Sebastian"], "venue": "arXiv preprint arXiv:1702.04521,", "citeRegEx": "Daniluk et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Daniluk et al\\.", "year": 2017}, {"title": "Gated-attention readers for text comprehension", "author": ["Dhingra", "Bhuwan", "Liu", "Hanxiao", "Cohen", "William W", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1606.01549,", "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Tracking the world state with recurrent entity networks", "author": ["Henaff", "Mikael", "Weston", "Jason", "Szlam", "Arthur", "Bordes", "Antoine", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1612.03969,", "citeRegEx": "Henaff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Hermann", "Karl Moritz", "Kocisky", "Tomas", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Text understanding with the attention sum reader network", "author": ["Kadlec", "Rudolf", "Schmid", "Martin", "Bajgar", "Ondrej", "Kleindienst", "Jan"], "venue": "arXiv preprint arXiv:1603.01547,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Skip-thought vectors. In Advances in neural information processing", "author": ["Kiros", "Ryan", "Zhu", "Yukun", "Salakhutdinov", "Ruslan R", "Zemel", "Richard", "Urtasun", "Raquel", "Torralba", "Antonio", "Fidler", "Sanja"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "A clockwork rnn", "author": ["Koutnik", "Jan", "Greff", "Klaus", "Gomez", "Faustino", "Schmidhuber", "Juergen"], "venue": "arXiv preprint arXiv:1402.3511,", "citeRegEx": "Koutnik et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koutnik et al\\.", "year": 2014}, {"title": "Gated graph sequence neural networks", "author": ["Li", "Yujia", "Tarlow", "Daniel", "Brockschmidt", "Marc", "Zemel", "Richard"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Key-value memory networks for directly reading documents", "author": ["Miller", "Alexander", "Fisch", "Adam", "Dodge", "Jesse", "Karimi", "Amir-Hossein", "Bordes", "Antoine", "Weston", "Jason"], "venue": "arXiv preprint arXiv:1606.03126,", "citeRegEx": "Miller et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Introduction to wordnet: An on-line lexical database", "author": ["Miller", "George A", "Beckwith", "Richard", "Fellbaum", "Christiane", "Gross", "Derek", "Katherine J"], "venue": "International journal of lexicography,", "citeRegEx": "Miller et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1990}, {"title": "Reasoning with memory augmented neural networks for language comprehension", "author": ["Munkhdalai", "Tsendsuren", "Yu", "Hong"], "venue": "arXiv preprint arXiv:1610.06454,", "citeRegEx": "Munkhdalai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai et al\\.", "year": 2016}, {"title": "Who did what: A largescale person-centered cloze dataset", "author": ["Onishi", "Takeshi", "Wang", "Hai", "Bansal", "Mohit", "Gimpel", "Kevin", "McAllester", "David"], "venue": "arXiv preprint arXiv:1608.05457,", "citeRegEx": "Onishi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Onishi et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["Oord", "Aaron van den", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Rajpurkar", "Pranav", "Zhang", "Jian", "Lopyrev", "Konstantin", "Liang", "Percy"], "venue": "arXiv preprint arXiv:1606.05250,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "The graph neural network model", "author": ["Scarselli", "Franco", "Gori", "Marco", "Tsoi", "Ah Chung", "Hagenbuchner", "Markus", "Monfardini", "Gabriele"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Scarselli et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Scarselli et al\\.", "year": 2009}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Seo", "Minjoon", "Kembhavi", "Aniruddha", "Farhadi", "Ali", "Hajishirzi", "Hannaneh"], "venue": null, "citeRegEx": "Seo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "Query-reduction networks for question answering", "author": ["Seo", "Minjoon", "Min", "Sewon", "Farhadi", "Ali", "Hajishirzi", "Hannaneh"], "venue": null, "citeRegEx": "Seo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "Dag-recurrent neural networks for scene labeling", "author": ["Shuai", "Bing", "Zuo", "Zhen", "Wang", "Gang"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Shuai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shuai et al\\.", "year": 2016}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Weston", "Jason", "Fergus", "Rob"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from treestructured long short-term memory networks", "author": ["Tai", "Kai Sheng", "Socher", "Richard", "Manning", "Christopher D"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Emergent logical structure in vector representations of neural readers", "author": ["Wang", "Hai", "Onishi", "Takeshi", "Gimpel", "Kevin", "McAllester", "David"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Weston", "Jason", "Bordes", "Antoine", "Chopra", "Sumit", "Rush", "Alexander M", "van Merri\u00ebnboer", "Bart", "Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Reference-aware language models", "author": ["Yang", "Zichao", "Blunsom", "Phil", "Dyer", "Chris", "Ling", "Wang"], "venue": "arXiv preprint arXiv:1611.01628,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "RNNs show excellent performance when the dependencies of interest range short spans of the sequence, however they can be notoriously hard to train to discover longer range dependencies (Koutnik et al., 2014; Bengio et al., 1994).", "startOffset": 185, "endOffset": 228}, {"referenceID": 2, "context": "RNNs show excellent performance when the dependencies of interest range short spans of the sequence, however they can be notoriously hard to train to discover longer range dependencies (Koutnik et al., 2014; Bengio et al., 1994).", "startOffset": 185, "endOffset": 228}, {"referenceID": 29, "context": "Both LSTMs and GRUs have been hugely popular for modeling sequence data (Sutskever et al., 2014; Kiros et al., 2015; Oord et al., 2016).", "startOffset": 72, "endOffset": 135}, {"referenceID": 15, "context": "Both LSTMs and GRUs have been hugely popular for modeling sequence data (Sutskever et al., 2014; Kiros et al., 2015; Oord et al., 2016).", "startOffset": 72, "endOffset": 135}, {"referenceID": 22, "context": "Both LSTMs and GRUs have been hugely popular for modeling sequence data (Sutskever et al., 2014; Kiros et al., 2015; Oord et al., 2016).", "startOffset": 72, "endOffset": 135}, {"referenceID": 5, "context": "Cho et al. (2014b) introduced a simplified version of LSTMs called Gated Recurrent Units (GRU) with has one less gate, and consequently fewer parameters.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "This led to the introduction of the attention mechanism (Bahdanau et al., 2014) which adapts the sequence model with a more explicit form of long term memory.", "startOffset": 56, "endOffset": 79}, {"referenceID": 12, "context": "Augmenting sequence models with attention has lead to significant improvements in various language modeling domains (Hermann et al., 2015).", "startOffset": 116, "endOffset": 138}, {"referenceID": 10, "context": ", 2014), further build on this idea by introducing a memory module for the soft-lookup operation, and a number of models allow the RNN to hold differentiable \u201cmemories\u201d of past elements to discover long range correlations (Graves et al., 2014).", "startOffset": 222, "endOffset": 243}, {"referenceID": 1, "context": "This led to the introduction of the attention mechanism (Bahdanau et al., 2014) which adapts the sequence model with a more explicit form of long term memory. At each time step t, the model can perform a \u201csoft\u201dlookup over all previous outputs through a weighted average \u2211t\u22121 i=1 \u03b1ihi. The weights \u03b1i are the outputs of another network whose parameters are learned from data. Augmenting sequence models with attention has lead to significant improvements in various language modeling domains (Hermann et al., 2015). Other architectures, such as Memory Networks (Weston et al., 2014), further build on this idea by introducing a memory module for the soft-lookup operation, and a number of models allow the RNN to hold differentiable \u201cmemories\u201d of past elements to discover long range correlations (Graves et al., 2014). However, Daniluk et al. (2017) showed that even memory-augmented neural models do not look beyond the immediately preceding time steps.", "startOffset": 57, "endOffset": 850}, {"referenceID": 24, "context": "Graph based neural networks (Scarselli et al., 2009) can be used to handle such data, but they are computationally expensive when the number of nodes in the graph is large.", "startOffset": 28, "endOffset": 52}, {"referenceID": 24, "context": "Models such as the Graph Neural Networks (Scarselli et al., 2009) and Gated Graph Sequence Neural Networks (Li et al.", "startOffset": 41, "endOffset": 65}, {"referenceID": 17, "context": ", 2009) and Gated Graph Sequence Neural Networks (Li et al., 2016) can be used to handle such data.", "startOffset": 49, "endOffset": 66}, {"referenceID": 30, "context": "Trees are another commonly encountered graph structure, and Tai et al. (2015) proposed Tree-Structured LSTMs for handling such data.", "startOffset": 60, "endOffset": 78}, {"referenceID": 11, "context": "Our model is also related to the Recurrent Entity Networks architecture (Henaff et al., 2016).", "startOffset": 72, "endOffset": 93}, {"referenceID": 30, "context": "The importance of reference resolution for reading comprehension was previously studied in Wang et al. (2017). They showed that models which utilize explicit coreference information, for example via the attention sum mechanism (see Section 4.", "startOffset": 91, "endOffset": 110}, {"referenceID": 33, "context": "Recently, there has been interest in incorporating symbolic knowledge, such as that from a Knowledge Base or coreference information, within RNN-based language models (Yang et al., 2016; Ahn et al., 2016).", "startOffset": 167, "endOffset": 204}, {"referenceID": 0, "context": "Recently, there has been interest in incorporating symbolic knowledge, such as that from a Knowledge Base or coreference information, within RNN-based language models (Yang et al., 2016; Ahn et al., 2016).", "startOffset": 167, "endOffset": 204}, {"referenceID": 8, "context": "(2016) and Daniluk et al. (2017) argue that overloaded use of state representations as both memory content and address makes training of the network difficult, and decompose these two functions by parameterizing them separately.", "startOffset": 11, "endOffset": 33}, {"referenceID": 23, "context": "Several large-scale benchmarks (Rajpurkar et al., 2016; Onishi et al., 2016) and deep learning models (Munkhdalai & Yu, 2016) have been proposed for this task recently.", "startOffset": 31, "endOffset": 76}, {"referenceID": 21, "context": "Several large-scale benchmarks (Rajpurkar et al., 2016; Onishi et al., 2016) and deep learning models (Munkhdalai & Yu, 2016) have been proposed for this task recently.", "startOffset": 31, "endOffset": 76}, {"referenceID": 7, "context": "Chu et al. (2016) show that existing state-of-the art models have poor", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "Following previous work (Hermann et al., 2015; Kadlec et al., 2016) our basic architecture consists of bidirectional GRUs to encode the document and query into a matrix H = [h1, .", "startOffset": 24, "endOffset": 67}, {"referenceID": 14, "context": "Following previous work (Hermann et al., 2015; Kadlec et al., 2016) our basic architecture consists of bidirectional GRUs to encode the document and query into a matrix H = [h1, .", "startOffset": 24, "endOffset": 67}, {"referenceID": 14, "context": "For extractive tasks, we use this attention distribution directly to predict the answer, using the attention-sum mechanism suggested by Kadlec et al. (2016). Hence, the probability of selecting token w as the answer is given by \u2211 i\u2208I(w,d) \u03b1i, where I(w, d) is the set of positions w occurs in d.", "startOffset": 136, "endOffset": 157}, {"referenceID": 31, "context": "Such additional features were shown to be useful by Wang et al. (2017). Henceforth, we refer to this baseline as \u201cone-hot\u201d.", "startOffset": 52, "endOffset": 71}, {"referenceID": 30, "context": "Story Based QA: Our first benchmark is the bAbi dataset from Weston et al. (2015), a set of 20 toy tasks aimed at measuring the ability of agents to reason about natural language.", "startOffset": 61, "endOffset": 82}, {"referenceID": 25, "context": "Following Seo et al. (2017b), we ran 10 random initializations of the model and report the test set performance for the model with the best validation set performance.", "startOffset": 10, "endOffset": 29}, {"referenceID": 27, "context": "The DAG-RNN baseline from Shuai et al. (2016) and the shared version of MAGE (where edge representations are tied) also perform worse, showing that our proposed architecture is superior.", "startOffset": 26, "endOffset": 46}, {"referenceID": 28, "context": "N2N refers to End-to-end Memory Networks (Sukhbaatar et al., 2015).", "startOffset": 41, "endOffset": 66}, {"referenceID": 27, "context": "proposed by Shuai et al. (2016) incorporated with GA.", "startOffset": 12, "endOffset": 32}, {"referenceID": 7, "context": "Results marked with \u2020 are cf (Chu et al., 2016).", "startOffset": 29, "endOffset": 47}, {"referenceID": 7, "context": "3% accuracy on this dataset, but Chu et al. (2016) improved this to 49% by reformulating the task as a reading comprehension one, and training on a large corpus of automatically extracted passages.", "startOffset": 33, "endOffset": 51}, {"referenceID": 7, "context": "Our implementation of GA gave higher performance than that reported by (Chu et al., 2016), without the use of linguistic features.", "startOffset": 71, "endOffset": 89}, {"referenceID": 9, "context": "We believe the difference is because we use a newer release of the code by the authors of GA (Dhingra et al., 2016).", "startOffset": 93, "endOffset": 115}, {"referenceID": 7, "context": "Table 4 shows a comparison of the baseline GA architecture with the coreference augmented GA+MAGE model on the 100 manually labeled validation instances available from Chu et al. (2016). The small sample size for each category makes it hard to draw strong conclusions from these results.", "startOffset": 168, "endOffset": 186}, {"referenceID": 12, "context": "Cloze-style QA: Lastly, we test our models on the CNN dataset from Hermann et al. (2015), which consists of pairs of news articles and a cloze-style question over the contents", "startOffset": 67, "endOffset": 89}, {"referenceID": 9, "context": ", 2017a) and with \u2021 from (Dhingra et al., 2016).", "startOffset": 25, "endOffset": 47}, {"referenceID": 4, "context": "This is an impressive improvement, given that previous works have reported that we are already close to the upper bound possible on this dataset (Chen et al., 2016).", "startOffset": 145, "endOffset": 164}, {"referenceID": 19, "context": "Our encouraging results motivate us to explore other potentially useful sources of knowledge, which may include \u2013 dependency parses, semantic role labels, semantic frames, ontologies such as Wordnet (Miller et al., 1990), and databases such as Freebase (Bollacker et al.", "startOffset": 199, "endOffset": 220}, {"referenceID": 3, "context": ", 1990), and databases such as Freebase (Bollacker et al., 2008).", "startOffset": 40, "endOffset": 64}], "year": 2017, "abstractText": "Training recurrent neural networks to model long term dependencies is difficult. Hence, we propose to use external linguistic knowledge as an explicit signal to inform the model which memories it should utilize. Specifically, external knowledge is used to augment a sequence with typed edges between arbitrarily distant elements, and the resulting graph is decomposed into directed acyclic subgraphs. We introduce a model that encodes such graphs as explicit memory in recurrent neural networks, and use it to model coreference relations in text. We apply our model to several text comprehension tasks and achieve new state-of-the-art results on all considered benchmarks, including CNN, bAbi, and LAMBADA. On the bAbi QA tasks, our model solves 15 out of the 20 tasks with only 1000 training examples per task. Analysis of the learned representations further demonstrates the ability of our model to encode fine-grained entity information across a document.", "creator": "LaTeX with hyperref package"}}}