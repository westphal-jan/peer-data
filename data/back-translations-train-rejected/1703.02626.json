{"id": "1703.02626", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Horde of Bandits using Gaussian Markov Random Fields", "abstract": "The gang of bandits (GOB) model \\cite{cesa2013gang} is a recent contextual bandits framework that shares information between a set of bandit problems, related by a known (possibly noisy) graph. This model is useful in problems like recommender systems where the large number of users makes it vital to transfer information between users. Despite its effectiveness, the existing GOB model can only be applied to small problems due to its quadratic time-dependence on the number of nodes. Existing solutions to combat the scalability issue require an often-unrealistic clustering assumption. By exploiting a connection to Gaussian Markov random fields (GMRFs), we show that the GOB model can be made to scale to much larger graphs without additional assumptions. In addition, we propose a Thompson sampling algorithm which uses the recent GMRF sampling-by-perturbation technique, allowing it to scale to even larger problems (leading to a \"horde\" of bandits). We give regret bounds and experimental results for GOB with Thompson sampling and epoch-greedy algorithms, indicating that these methods are as good as or significantly better than ignoring the graph or adopting a clustering-based approach. Finally, when an existing graph is not available, we propose a heuristic for learning it on the fly and show promising results.", "histories": [["v1", "Tue, 7 Mar 2017 22:21:50 GMT  (651kb,D)", "http://arxiv.org/abs/1703.02626v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sharan vaswani", "mark schmidt", "laks v s lakshmanan"], "accepted": false, "id": "1703.02626"}, "pdf": {"name": "1703.02626.pdf", "metadata": {"source": "CRF", "title": "Horde of Bandits using Gaussian Markov Random Fields", "authors": ["Sharan Vaswani", "Mark Schmidt", "Laks V.S. Lakshmanan"], "emails": [], "sections": [{"heading": null, "text": "The Bandit Bandit Model [7] is an up-to-date context-dependent bandit framework that exchanges information between a number of bandit problems linked to each other by a known (possibly noisy) graph. This model is useful for problems such as referral systems where the large number of users makes it vital to transfer information between users. Despite its effectiveness, the existing GOB model can only be applied to small problems due to its square time dependence on the number of nodes. Existing solutions to the scaling problem require an often unrealistic cluster assumption. By using a link to Gaussian Markov Random Fields (GMRFs), we show that the GOB model can be scaled to much larger graphics without additional assumptions. In addition, we propose a Thompson sampling algorithm that takes advantage of the recent GMRF sampling-by-persistent technique (which allows for even larger problems)."}, {"heading": "1 Introduction", "text": "Consider the fact that the number of people who are able to survive themselves has risen sharply in recent years. (...) In fact, the number of people who are able to survive themselves has risen sharply in recent years. (...) The number of people who are able to survive themselves is increasing. (...) The number of people who are able to survive themselves is increasing. (...) The number of people who are able to survive themselves is increasing. (...) The number of people who are able to survive themselves is increasing. (...) The number of people who are able to survive themselves is increasing. (...) The number of people who are able to survive themselves is increasing. (...) The number of people who are able to survive themselves is increasing. (...) The number of people who are able to survive themselves is increasing. (...) The number of people who are able to survive themselves is increasing. (...) The number of people who are able to survive themselves is increasing."}, {"heading": "2 Related Work", "text": "They used matrix factorization to adjust existing valuation data, but limited a user's latent vector to be similar to his friends on the social network. Other methods based on collaborative filtering followed [38, 13], but this work assumes that we already have valuation data available. Thus, if characteristics (context) are available to the \"poor\" and change, it is called the contextual bandit problem that we are looking at: the multiarmed bandit problem is a classic approach to trading in exploration and exploitation data while users get a grip on it."}, {"heading": "3 Scaling up Gang of Bandits", "text": "In this section we first describe the general GOB framework, then discuss the relationship to GMRFs, and finally show how this leads to a more scalable method. In this paper, Tr (A) denotes the track of matrix A, A B the Kronecker product of matrices A and B, Id is used for the d-dimensional identity matrix, and vec (A) is stacking the columns of a matrix A into a vector."}, {"heading": "3.1 Gang of Bandits Framework", "text": "We assume that each element can be described by a single target user per round. It is straightforward to extend our results to multiple target users, and our task is to recommend an available element that allows users to access them. Users then provide feedback on the recommended element in the form of a rating rit, jt. Based on this feedback, the estimated preference vector for the user is updated."}, {"heading": "3.2 Connection to GMRFs", "text": "Unfortunately, the approach of Cesa-Bianchi [7] for solving (2) has a computational complexity of O (d2n2). In order to solve (2) more efficiently, we now show that it can be interpreted as an MAP estimate in a GMRF. This allows us to apply the GOB model to much larger datasets and lead to an even more scalable algorithm based on Thompson sampling (Section 4). Consider the following generative model for the ri, j and user preference vectors wi, ri, j-N (wTi xj, \u03c32), w-N (0, (\u03bbL Id) \u2212 1). This GMRF model assumes that the ri, j, and xj ratings are given independently, which is the standard1To ensure immutability, we use L = LG + In the case that LG is the normalized graphics laplacian.regression assumption."}, {"heading": "3.3 Scalability", "text": "Instead, we propose to use its structure to extend the GOB framework to problems where n is very large. In particular, the solution (2) corresponds to determining the mean vector of the GMRF, which corresponds to the solution of the linear system successtw = bt. Since this linear system is positively defined, it can be solved with the help of the conjugate gradient [20]. Remarkably, the conjugate gradient does not require \u03a3 \u2212 1t, but instead uses matrix vector products \u0445tv = (\u03a6Tt) v + \u03bb (L Id) v for vectors v Rdn. Note that \u03a6Tt is block diagonal and has only O (nd2) non-zeros, so this Tt vector can be calculated in O (nd2) time."}, {"heading": "4 Alternative Bandit Algorithms", "text": "The above structure can be used to accelerate the mean estimation for each algorithm in the GOB Framework. However, the LINUCB-like algorithm in [7] must estimate the confidence intervals for each available element j-Ct. Estimating this requires O (| Ct | \u0432 (nd2 + d \u00b7 nnz (L)))) time, since we must solve the linear system with | Ct | right sides, one for each available element. However, this becomes impractical if the number of available items in each round is greater. We propose two approaches to mitigate this: First, in this section we adapt the epochal [27] algorithm to the GOB Framework. Epochily does not require confidence intervals and is therefore highly scalable, but unfortunately it does not achieve the optimal regret of O-T. In order to achieve the optimal regret, we also propose problems in this larger scale between the GOB-Sampler and the GMB-Thompson."}, {"heading": "4.1 Epoch-Greedy", "text": "An \"exploration\" round consists of selecting the available item that maximizes the expected valuation, and the time complexity for Epoch-Greedy is dominated by the exploitation rounds that require the calculation of the mean and the estimate of the expected valuation."}, {"heading": "4.2 Thompson sampling", "text": "However, it is also possible that we achieve the optimal remorse: the conventional approach to the sampling method from a multivariate gauserior involves the formation of the cholesky factorization of the covariance. In the GOB model of the posterior covariance matrix is a n-dimensional matrix in which the fillings from the cholesky factorization can lead to a computational complexity of the O (d2n2)."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setup", "text": "Data: We first test the scalability of different algorithms using synthetic data and then assess their regret performance on two real datasets. For synthetic data, we generate random d-dimensional context vectors and soil-truth user preferences and generate the ratings according to the linear model. We generate a random Kronecker graph with the thrift 0.005 (roughly equivalent to the thrift of our real datasets). It is known that such graphics capture many characteristics of the real world of social networks and the real data we use the Last.fm and Delicious datasets available as part of the HetRec workshop 2011. Last.fm is a music streaming website where each article corresponds to a music artist and the dataset consists of the set of artists to which each user belongs. The associated social network consists of 1.8K users (nodes) and 12.7K friendship relationships (edges)."}, {"heading": "5.2 Results", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to achieve our objectives."}, {"heading": "6 Discussion", "text": "This paper establishes a link between the GOB framework and GMRFs and uses it to extend the existing GOB model to much larger diagrams. We have also proposed and analyzed Thompson sampling and epochal variants. Our experiments with recommendation systems indicate that the Thompson sampling approach in particular is much more scalable than existing GOB methods, receives theoretical optimum regret, and works similarly or better than other scalable approaches. In many practical scenarios, we do not have an explicit graph structure available. In the supplementary material, we consider a variant of the GOB model, in which we use the L1 regulation to learn graphics on the fly. Our experiments there show that this approach works similarly or much better than approaches that use the fixed graph structure. It would be interesting to explore the theoretical properties of this approach."}, {"heading": "A Learning the Graph", "text": "In the main work, we assume that in practice the graph will use such a user chart to encourage additional graphs, but in practice this user chart may not be available. (In such a case, we are not able to learn the graph between users in an efficient way.) The graph for learning the graph is related to the methods proposed for learning multitas- and multitask- learning processes. (The graph for learning tasks / labels, while the graph and preferences are used in thousands of users. Let's leave the inverse covariance matrix according to the graph between users in the round.) Since Nuli corresponds to the corresponding nodes in the inverse covariance matrix, we use L1 regulation matrix for exploration."}, {"heading": "B Regret bound for Epoch-Greedy", "text": "= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Yasin Abbasi-Yadkori", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "arXiv preprint arXiv:1209.3352,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Nonstochastic multi-armed bandits with graph-structured feedback", "author": ["Noga Alon", "Nicolo Cesa-Bianchi", "Claudio Gentile", "Shie Mannor", "Yishay Mansour", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1409.8428,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartlett", "Shahar Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Leveraging side observations in stochastic bandits", "author": ["St\u00e9phane Caron", "Branislav Kveton", "Marc Lelarge", "Smriti Bhagat"], "venue": "In Proceedings of the Twenty- Eighth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "A gang of bandits", "author": ["Nicolo Cesa-Bianchi", "Claudio Gentile", "Giovanni Zappella"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "An empirical evaluation of thompson sampling", "author": ["Olivier Chapelle", "Lihong Li"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Contextual bandits with linear payoff functions", "author": ["Wei Chu", "Lihong Li", "Lev Reyzin", "Robert E Schapire"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade"], "venue": "In 21st Annual Conference on Learning Theory - COLT", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Algorithm 849: A concise sparse cholesky factorization package", "author": ["Timothy A Davis"], "venue": "ACM Transactions on Mathematical Software (TOMS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Socially enabled preference learning from implicit feedback data", "author": ["Julien Delporte", "Alexandros Karatzoglou", "Tomasz Matuszczyk", "St\u00e9phane Canu"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Regularized multi\u2013task learning", "author": ["Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Algebraic connectivity of graphs", "author": ["Miroslav Fiedler"], "venue": "Czechoslovak mathematical journal,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1973}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Online clustering of bandits", "author": ["Claudio Gentile", "Shuai Li", "Giovanni Zappella"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Multi-task sparse structure learning", "author": ["Andre R Goncalves", "Puja Das", "Soumyadeep Chatterjee", "Vidyashankar Sivakumar", "Fernando J Von Zuben", "Arindam Banerjee"], "venue": "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Multi-label structure learning with ising model selection", "author": ["Andr\u00e9 R Gon\u00e7alves", "Fernando J Von Zuben", "Arindam Banerjee"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Methods of conjugate gradients for solving linear systems, volume", "author": ["Magnus Rudolph Hestenes", "Eduard Stiefel"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1952}, {"title": "Sparse inverse covariance matrix estimation using quadratic approximation", "author": ["Cho-Jui Hsieh", "Inderjit S Dhillon", "Pradeep K Ravikumar", "M\u00e1ty\u00e1s A Sustik"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Big & quic: Sparse inverse covariance estimation for a million variables", "author": ["Cho-Jui Hsieh", "M\u00e1ty\u00e1s A Sustik", "Inderjit S Dhillon", "Pradeep K Ravikumar", "Russell Poldrack"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Spectral thompson sampling", "author": ["Tom\u00e1\u0161 Koc\u00e1k", "Michal Valko", "R\u00e9mi Munos", "Shipra Agrawal"], "venue": "Horde of Bandits using Gaussian Markov Random Fields Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Distributed clustering of linear bandits in peer to peer networks", "author": ["Nathan Korda", "Bal\u00e1zs Sz\u00f6r\u00e9nyi", "Shuai Li"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Approximate gaussian elimination for laplacians-fast, sparse, and simple", "author": ["Rasmus Kyng", "Sushant Sachdeva"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1985}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["John Langford", "Tong Zhang"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Kronecker graphs: An approach to modeling networks", "author": ["Jure Leskovec", "Deepayan Chakrabarti", "Jon Kleinberg", "Christos Faloutsos", "Zoubin Ghahramani"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Lihong Li", "Wei Chu", "John Langford", "Robert E Schapire"], "venue": "In Proceedings of the 19th international conference on World wide web,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Collaborative filtering bandits", "author": ["Shuai Li", "Alexandros Karatzoglou", "Claudio Gentile"], "venue": "In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Recommender systems with social regularization", "author": ["Hao Ma", "Dengyong Zhou", "Chao Liu", "Michael R Lyu", "Irwin King"], "venue": "In Proceedings of the fourth ACM international conference on Web search and data mining,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Latent bandits", "author": ["Odalric-Ambrym Maillard", "Shie Mannor"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "From bandits to experts: On the value of side-observations", "author": ["Shie Mannor", "Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "The rademacher complexity of linear transformation classes. In Learning Theory, pages 65\u201378", "author": ["Andreas Maurer"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Birds of a feather: Homophily in social networks", "author": ["Miller McPherson", "Lynn Smith-Lovin", "James M Cook"], "venue": "Annual review of sociology,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Dynamic clustering of contextual multi-armed bandits", "author": ["Trong T Nguyen", "Hady W Lauw"], "venue": "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Gaussian sampling by local perturbations", "author": ["George Papandreou", "Alan L Yuille"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}, {"title": "Collaborative filtering with graph information: Consistency and scalable methods", "author": ["Nikhil Rao", "Hsiang-Fu Yu", "Pradeep K Ravikumar", "Inderjit S Dhillon"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Gaussian Markov random fields: theory and applications", "author": ["Havard Rue", "Leonhard Held"], "venue": "CRC Press,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Online learning of multiple tasks and their relationships", "author": ["Avishek Saha", "Piyush Rai", "Suresh Venkatasubramanian", "Hal Daume"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "A survey of collaborative filtering techniques", "author": ["Xiaoyuan Su", "Taghi M Khoshgoftaar"], "venue": "Advances in artificial intelligence,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Spectral bandits for smooth graph functions", "author": ["Michal Valko", "R\u00e9mi Munos", "Branislav Kveton", "Tom\u00e1\u0161 Koc\u00e1k"], "venue": "In 31th International Conference on Machine Learning,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "The gang of bandits (GOB) model [7] is a recent contextual bandits framework that shares information between a set of bandit problems, related by a known (possibly noisy) graph.", "startOffset": 32, "endOffset": 35}, {"referenceID": 39, "context": "The unavailability of rating data implies that we can not use traditional collaborative filtering based methods [41].", "startOffset": 112, "endOffset": 116}, {"referenceID": 27, "context": "Assuming each item can be described by its content (like tags describing a news article or video), the contextual bandits framework [29] offers a popular approach for addressing this exploration-exploitation trade-off.", "startOffset": 132, "endOffset": 136}, {"referenceID": 6, "context": "One way to use a social network of users to improve recommendations is with the recent gang of bandits (GOB) model [7].", "startOffset": 115, "endOffset": 118}, {"referenceID": 33, "context": "In particular, the GOB model exploits the homophily effect [35] that suggests users with similar preferences are more likely to form links in a social network.", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "Several recent works have tried to improve the scaling of the GOB model by clustering the users into groups [17, 36], but this limits the flexibility of the model and loses the ability to model individual users\u2019 preferences.", "startOffset": 108, "endOffset": 116}, {"referenceID": 34, "context": "Several recent works have tried to improve the scaling of the GOB model by clustering the users into groups [17, 36], but this limits the flexibility of the model and loses the ability to model individual users\u2019 preferences.", "startOffset": 108, "endOffset": 116}, {"referenceID": 35, "context": "In addition, we propose a Thompson sampling GOB variant that exploits the recent sampling-by-perturbation idea from the GMRF literature [37] to scale to even larger problems.", "startOffset": 136, "endOffset": 140}, {"referenceID": 29, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "Other methods based on collaborative filtering followed [38, 13], but these works assume that we already have rating data available.", "startOffset": 56, "endOffset": 64}, {"referenceID": 11, "context": "Other methods based on collaborative filtering followed [38, 13], but these works assume that we already have rating data available.", "startOffset": 56, "endOffset": 64}, {"referenceID": 24, "context": "Bandits: The multi-armed bandit problem is a classic approach for trading off exploration and exploitation as we collect data [26].", "startOffset": 126, "endOffset": 130}, {"referenceID": 3, "context": "When features (context) for the \u201carms\u201d are available and changing, it is referred to as the contextual bandit problem [4, 29, 9].", "startOffset": 118, "endOffset": 128}, {"referenceID": 27, "context": "When features (context) for the \u201carms\u201d are available and changing, it is referred to as the contextual bandit problem [4, 29, 9].", "startOffset": 118, "endOffset": 128}, {"referenceID": 8, "context": "When features (context) for the \u201carms\u201d are available and changing, it is referred to as the contextual bandit problem [4, 29, 9].", "startOffset": 118, "endOffset": 128}, {"referenceID": 25, "context": "Algorithms for the contextual bandits problem include epoch-greedy methods [27], those based on upper confidence bounds (UCB) [9, 1], and Thompson sampling methods [2].", "startOffset": 75, "endOffset": 79}, {"referenceID": 8, "context": "Algorithms for the contextual bandits problem include epoch-greedy methods [27], those based on upper confidence bounds (UCB) [9, 1], and Thompson sampling methods [2].", "startOffset": 126, "endOffset": 132}, {"referenceID": 0, "context": "Algorithms for the contextual bandits problem include epoch-greedy methods [27], those based on upper confidence bounds (UCB) [9, 1], and Thompson sampling methods [2].", "startOffset": 126, "endOffset": 132}, {"referenceID": 1, "context": "Algorithms for the contextual bandits problem include epoch-greedy methods [27], those based on upper confidence bounds (UCB) [9, 1], and Thompson sampling methods [2].", "startOffset": 164, "endOffset": 167}, {"referenceID": 5, "context": "Several graph-based methods to model dependencies between the users have been explored in the (noncontextual) multi-armed bandit framework [6, 33, 3, 32], but the GOB model of Cesa-Bianchi et al.", "startOffset": 139, "endOffset": 153}, {"referenceID": 31, "context": "Several graph-based methods to model dependencies between the users have been explored in the (noncontextual) multi-armed bandit framework [6, 33, 3, 32], but the GOB model of Cesa-Bianchi et al.", "startOffset": 139, "endOffset": 153}, {"referenceID": 2, "context": "Several graph-based methods to model dependencies between the users have been explored in the (noncontextual) multi-armed bandit framework [6, 33, 3, 32], but the GOB model of Cesa-Bianchi et al.", "startOffset": 139, "endOffset": 153}, {"referenceID": 30, "context": "Several graph-based methods to model dependencies between the users have been explored in the (noncontextual) multi-armed bandit framework [6, 33, 3, 32], but the GOB model of Cesa-Bianchi et al.", "startOffset": 139, "endOffset": 153}, {"referenceID": 6, "context": "[7] is the first to exploit the network between users in the contextual bandit framework.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "To scale up the GOB model, several recent works propose to cluster the users and assume that users in the same cluster have the same preferences [17, 36].", "startOffset": 145, "endOffset": 153}, {"referenceID": 34, "context": "To scale up the GOB model, several recent works propose to cluster the users and assume that users in the same cluster have the same preferences [17, 36].", "startOffset": 145, "endOffset": 153}, {"referenceID": 28, "context": "Another interesting approach to relax the clustering assumption is to cluster both items and users [30], but this only applies if we have a fixed set of items.", "startOffset": 99, "endOffset": 103}, {"referenceID": 40, "context": "Some works consider item-item similarities to improve recommendations [42, 23], but this again requires a fixed set of items while we are interested in RS where the set of items may constantly be changing.", "startOffset": 70, "endOffset": 78}, {"referenceID": 21, "context": "Some works consider item-item similarities to improve recommendations [42, 23], but this again requires a fixed set of items while we are interested in RS where the set of items may constantly be changing.", "startOffset": 70, "endOffset": 78}, {"referenceID": 22, "context": "There has also been work on solving a single bandit problem in a distributed fashion [24], but this differs from our approach where we are solving an individual bandit problem on each of the n nodes.", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "Also without loss of generality, we assume that the ratings are in the range [0, 1].", "startOffset": 77, "endOffset": 83}, {"referenceID": 27, "context": "The true ratings can be given by a linear model [29], meaning that ri,j = (wi )xj + \u03b7i,j,t for some noise term \u03b7i,j,t.", "startOffset": 48, "endOffset": 52}, {"referenceID": 1, "context": "The noise \u03b7i,j,t is conditionally subGaussian [2][7] with zero mean and bounded variance, meaning that E[\u03b7i,j,t | Ct\u22121,Ht\u22121] = 0 and that there exists a \u03c3 > 0 such that for all \u03b3 \u2208 R, we have E[exp(\u03b3\u03b7i,j,t) | Ht\u22121,Ct\u22121] \u2264 exp( 2\u03c32 2 ).", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "The noise \u03b7i,j,t is conditionally subGaussian [2][7] with zero mean and bounded variance, meaning that E[\u03b7i,j,t | Ct\u22121,Ht\u22121] = 0 and that there exists a \u03c3 > 0 such that for all \u03b3 \u2208 R, we have E[exp(\u03b3\u03b7i,j,t) | Ht\u22121,Ct\u22121] \u2264 exp( 2\u03c32 2 ).", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "The GOB model [7] solves a contextual bandit problem for each user, where the mean vectors in the different problems are related according to the Laplacian L1 of the graph G.", "startOffset": 14, "endOffset": 17}, {"referenceID": 12, "context": "Note that the same objective function has also been explored for graph-regularized multi-task learning [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 6, "context": "2 Connection to GMRFs Unfortunately, the approach of Cesa-Bianchi [7] for solving (2) has a computational complexity of O(d2n2).", "startOffset": 66, "endOffset": 69}, {"referenceID": 6, "context": "We can view the approach in [7] as explicitly constructing the dense dn \u00d7 dn matrix \u03a3\u22121 t , leading to an O(d2n2) memory requirement.", "startOffset": 28, "endOffset": 31}, {"referenceID": 18, "context": "Since \u03a3t is positivedefinite, the linear system can be solved using conjugate gradient [20].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "However, the LINUCB-like algorithm in [7] needs to estimate the confidence intervals \u221a \u03c6i,j\u03a3 t \u03c6i,j for each available item j \u2208 Ct.", "startOffset": 38, "endOffset": 41}, {"referenceID": 25, "context": "We propose two approaches for mitigating this: first, in this section we adapt the epoch-greedy [27] algorithm to the GOB framework.", "startOffset": 96, "endOffset": 100}, {"referenceID": 27, "context": "To achieve the optimal regret, we also propose a GOB variant of Thompson sampling [29].", "startOffset": 82, "endOffset": 86}, {"referenceID": 35, "context": "In this section we further exploit the connection to GMRFs to scale Thompson sampling to even larger problems by using the recent sampling-by-perturbation trick [37].", "startOffset": 161, "endOffset": 165}, {"referenceID": 25, "context": "1 Epoch-Greedy Epoch-greedy [27] is a variant of the popular -greedy algorithm that explicitly differentiates between exploration and exploitation rounds.", "startOffset": 28, "endOffset": 32}, {"referenceID": 25, "context": "The attainable regret is thus proportional to the generalization error for the class of hypothesis functions mapping the context vector to an expected rating [27].", "startOffset": 158, "endOffset": 162}, {"referenceID": 32, "context": "We characterize the generalization error in the GOB framework in terms of its Rademacher complexity [34], and use this to bound the expected regret leading to the result below.", "startOffset": 100, "endOffset": 104}, {"referenceID": 25, "context": "1 from [27] to our context:", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "We use [34] to bound the generalization error of our class of hypotheses in terms of its empirical Rademacher complexity R\u0302q (H).", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "Using Theorem 2 in [34] and Theorem 12 from [5], we obtain", "startOffset": 19, "endOffset": 23}, {"referenceID": 4, "context": "Using Theorem 2 in [34] and Theorem 12 from [5], we obtain", "startOffset": 44, "endOffset": 47}, {"referenceID": 32, "context": "For a connected graph, we have the following upper-bound Tr(L \u22121) n \u2264 (1\u22121/n) \u03bd2 + 1 n [34].", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "The value \u03bd2 represents the algebraic connectivity of the graph [15].", "startOffset": 64, "endOffset": 68}, {"referenceID": 35, "context": "In order to implement Thompson sampling for large values of n, we adapt the recent sampling-by-perturbation approach [37] to our setting, and this allows us to sample from a Gaussian prior and then solve a linear system to sample from the posterior.", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "Since S tends to be sparse (using for example [12, 25]), this equation can be solved efficiently using conjugate gradient.", "startOffset": 46, "endOffset": 54}, {"referenceID": 23, "context": "Since S tends to be sparse (using for example [12, 25]), this equation can be solved efficiently using conjugate gradient.", "startOffset": 46, "endOffset": 54}, {"referenceID": 1, "context": "We obtain the result below by following a similar argument to Theorem 1 in [2].", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "Given that the event E (t) holds with high probability, we follow an argument similar to Lemma 4 of [2] and obtain the following bound:", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "To bound the variance of the selected items, \u2211T t=1 st(jt), we extend the analysis in [11, 43] to include the prior covariance term.", "startOffset": 86, "endOffset": 94}, {"referenceID": 0, "context": "If L = Idn, we match the \u00d5(dn \u221a T ) regret bound for a dn-dimensional contextual bandit problem [1].", "startOffset": 96, "endOffset": 99}, {"referenceID": 6, "context": "Note that we have a dependence on d and n similar to the original GOB paper [7] and that this method performs similarly in practice in terms of regret.", "startOffset": 76, "endOffset": 79}, {"referenceID": 26, "context": "It is well known that such graphs capture many properties of real-world social networks [28].", "startOffset": 88, "endOffset": 92}, {"referenceID": 6, "context": "Similar to [7], we use the set of associated tags to construct the TF-IDF vector for each item and reduce the dimension of these vectors to d = 25.", "startOffset": 11, "endOffset": 14}, {"referenceID": 15, "context": "Similar to [17], all hyper-parameters are set using an initial validation set of 5 thousand rounds.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "To control the amount of exploration for Thompson sampling, we the use posterior reshaping trick [8] which reduces the variance of the posterior by a factor of 0.", "startOffset": 97, "endOffset": 100}, {"referenceID": 6, "context": "Baselines: We consider two variants of graph-based UCB-style algorithms: GOBLIN is the method proposed in the original GOB paper [7] while we use GOBLIN++ to refer to a variant that exploits the fast mean estimation strategy we develop in Section 3.", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "Similar to [7], for both variants we discount the confidence bound term by a factor of \u03b1 = 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 27, "context": "We consider 3 variants of this baseline: the LINUCBIND proposed in [29], an epoch-greedy variant of this approach (EG-IND), and a Thompson sampling variant (TS-IND).", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "Finally, we compared against the state-of-the-art online clusteringbased approach proposed in [17], denoted CLUB.", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "Regret Minimization: We follow [17] in evaluating recommendation performance by plotting the ratio of cumulative regret incurred by the algorithm divided by the regret incurred by a random selection policy.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "[1] Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Shipra Agrawal and Navin Goyal.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Noga Alon, Nicolo Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, and Ohad Shamir.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Peter L Bartlett and Shahar Mendelson.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] St\u00e9phane Caron, Branislav Kveton, Marc Lelarge, and Smriti Bhagat.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Nicolo Cesa-Bianchi, Claudio Gentile, and Giovanni Zappella.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Olivier Chapelle and Lihong Li.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Wei Chu, Lihong Li, Lev Reyzin, and Robert E Schapire.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[11] Varsha Dani, Thomas P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] Timothy A Davis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Julien Delporte, Alexandros Karatzoglou, Tomasz Matuszczyk, and St\u00e9phane Canu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Theodoros Evgeniou and Massimiliano Pontil.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] Miroslav Fiedler.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] Jerome Friedman, Trevor Hastie, and Robert Tibshirani.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] Claudio Gentile, Shuai Li, and Giovanni Zappella.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] Andre R Goncalves, Puja Das, Soumyadeep Chatterjee, Vidyashankar Sivakumar, Fernando J Von Zuben, and Arindam Banerjee.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] Andr\u00e9 R Gon\u00e7alves, Fernando J Von Zuben, and Arindam Banerjee.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] Magnus Rudolph Hestenes and Eduard Stiefel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] Cho-Jui Hsieh, Inderjit S Dhillon, Pradeep K Ravikumar, and M\u00e1ty\u00e1s A Sustik.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] Cho-Jui Hsieh, M\u00e1ty\u00e1s A Sustik, Inderjit S Dhillon, Pradeep K Ravikumar, and Russell Poldrack.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] Tom\u00e1\u0161 Koc\u00e1k, Michal Valko, R\u00e9mi Munos, and Shipra Agrawal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] Nathan Korda, Bal\u00e1zs Sz\u00f6r\u00e9nyi, and Shuai Li.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] Rasmus Kyng and Sushant Sachdeva.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] Tze Leung Lai and Herbert Robbins.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] John Langford and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] Jure Leskovec, Deepayan Chakrabarti, Jon Kleinberg, Christos Faloutsos, and Zoubin Ghahramani.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] Lihong Li, Wei Chu, John Langford, and Robert E Schapire.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] Shuai Li, Alexandros Karatzoglou, and Claudio Gentile.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] Hao Ma, Dengyong Zhou, Chao Liu, Michael R Lyu, and Irwin King.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] Odalric-Ambrym Maillard and Shie Mannor.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] Shie Mannor and Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] Andreas Maurer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35] Miller McPherson, Lynn Smith-Lovin, and James M Cook.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[36] Trong T Nguyen and Hady W Lauw.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[37] George Papandreou and Alan L Yuille.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[38] Nikhil Rao, Hsiang-Fu Yu, Pradeep K Ravikumar, and Inderjit S Dhillon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[39] Havard Rue and Leonhard Held.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[40] Avishek Saha, Piyush Rai, Suresh Venkatasubramanian, and Hal Daume.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[41] Xiaoyuan Su and Taghi M Khoshgoftaar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[42] Michal Valko, R\u00e9mi Munos, Branislav Kveton, and Tom\u00e1\u0161 Koc\u00e1k.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Our approach for learning the graph is related to methods proposed for multitask and multilabel learning in the batch setting [19, 18] and multitask learning in the online setting [40].", "startOffset": 126, "endOffset": 134}, {"referenceID": 16, "context": "Our approach for learning the graph is related to methods proposed for multitask and multilabel learning in the batch setting [19, 18] and multitask learning in the online setting [40].", "startOffset": 126, "endOffset": 134}, {"referenceID": 38, "context": "Our approach for learning the graph is related to methods proposed for multitask and multilabel learning in the batch setting [19, 18] and multitask learning in the online setting [40].", "startOffset": 180, "endOffset": 184}, {"referenceID": 37, "context": "Since zeroes in the inverse covariance matrix correspond to conditional independences between the corresponding nodes (users) [39], we use L1 regularization on Vt for encouraging sparsity in the inferred graph.", "startOffset": 126, "endOffset": 130}, {"referenceID": 38, "context": "Following [40], we choose \u2206 to be the log-determinant Bregman divergence given by \u2206(X||Y ) = Tr(XY \u22121)\u2212 log |XY \u22121| \u2212 dn.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "This problem can be written as a graphical lasso problem [16], minX Tr(SX) + \u03bb2||X||1 \u2212 log |X|, where the empirical covariance matrix S is equal to \u03bbWTt W t + V \u22121 t\u22121.", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "We use the highly-scalable second order methods described in [21, 22] to solve (11).", "startOffset": 61, "endOffset": 69}, {"referenceID": 20, "context": "We use the highly-scalable second order methods described in [21, 22] to solve (11).", "startOffset": 61, "endOffset": 69}, {"referenceID": 38, "context": "Similar to [40], we start off with an empty graph and start learning the graph only after the preference vectors have become stable, which happens in this case after each user has received 10 recommendations.", "startOffset": 11, "endOffset": 15}, {"referenceID": 25, "context": "1 from [27]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "The generalization error for H can be bounded as follows: Lemma 3 (Theorem 1 from [34]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 32, "context": "The Rademacher complexity for a class of linear predictors with graph regularization for a 0/1 loss function `0,1 can be bounded using Theorem 2 of [34].", "startOffset": 148, "endOffset": 152}, {"referenceID": 4, "context": "Since we perform regression using a least squares loss function instead of classification, the Rademacher complexity in our case can be bounded using Theorem 12 from [5].", "startOffset": 166, "endOffset": 169}, {"referenceID": 1, "context": "Our argument closely follows the proof structure in [2], but is modified to include the prior covariance.", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "To show that the event E(t) holds with high probability, we use the following lemma from [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "Lemma 5 (Lemma 2 of [2]).", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "Lemma 6 (Lemma 4 in [2]).", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "To bound ||St\u22121||\u03a3\u22121 t\u22121 , we use Theorem 1 from [1] which we restate in our context.", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "Note that using this theorem with the prior covariance equal to Idn gives Lemma 8 of [2].", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "Theorem 2 (Theorem 1 of [1]).", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "If L = In, Tr(L) = Tr(L\u22121) = n, we recover the bound in [2] i.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "Following the proof in [11, 43],", "startOffset": 23, "endOffset": 31}], "year": 2017, "abstractText": "The gang of bandits (GOB) model [7] is a recent contextual bandits framework that shares information between a set of bandit problems, related by a known (possibly noisy) graph. This model is useful in problems like recommender systems where the large number of users makes it vital to transfer information between users. Despite its effectiveness, the existing GOB model can only be applied to small problems due to its quadratic timedependence on the number of nodes. Existing solutions to combat the scalability issue require an often-unrealistic clustering assumption. By exploiting a connection to Gaussian Markov random fields (GMRFs), we show that the GOB model can be made to scale to much larger graphs without additional assumptions. In addition, we propose a Thompson sampling algorithm which uses the recent GMRF sampling-by-perturbation technique, allowing it to scale to even larger problems (leading to a \u201chorde\u201d of bandits). We give regret bounds and experimental results for GOB with Thompson sampling and epoch-greedy algorithms, indicating that these methods are as good as or significantly better than ignoring the graph or adopting a clustering-based approach. Finally, when an existing graph is not available, we propose a heuristic for learning it on the fly and show promising results.", "creator": "LaTeX with hyperref package"}}}