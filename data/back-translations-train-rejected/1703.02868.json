{"id": "1703.02868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Discriminative models for multi-instance problems with tree-structure", "abstract": "Modeling network traffic is gaining importance in order to counter modern threats of ever increasing sophistication. It is though surprisingly difficult and costly to construct reliable classifiers on top of telemetry data due to the variety and complexity of signals that no human can manage to interpret in full. Obtaining training data with sufficiently large and variable body of labels can thus be seen as prohibitive problem. The goal of this work is to detect infected computers by observing their HTTP(S) traffic collected from network sensors, which are typically proxy servers or network firewalls, while relying on only minimal human input in model training phase. We propose a discriminative model that makes decisions based on all computer's traffic observed during predefined time window (5 minutes in our case). The model is trained on collected traffic samples over equally sized time window per large number of computers, where the only labels needed are human verdicts about the computer as a whole (presumed infected vs. presumed clean). As part of training the model itself recognizes discriminative patterns in traffic targeted to individual servers and constructs the final high-level classifier on top of them. We show the classifier to perform with very high precision, while the learned traffic patterns can be interpreted as Indicators of Compromise. In the following we implement the discriminative model as a neural network with special structure reflecting two stacked multi-instance problems. The main advantages of the proposed configuration include not only improved accuracy and ability to learn from gross labels, but also automatic learning of server types (together with their detectors) which are typically visited by infected computers.", "histories": [["v1", "Tue, 7 Mar 2017 06:53:34 GMT  (198kb,D)", "http://arxiv.org/abs/1703.02868v1", null]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["tomas pevny", "petr somol"], "accepted": false, "id": "1703.02868"}, "pdf": {"name": "1703.02868.pdf", "metadata": {"source": "CRF", "title": "Discriminative models for multi-instance problems with tree-structure", "authors": ["Tom\u00e1\u0161 Pevn\u00fd", "Petr Somol"], "emails": ["pevnak@gmail.com", "psomol@cisco.com"], "sections": [{"heading": "1. MOTIVATION", "text": "In network security, it is increasingly difficult to respond to the influx of new malicious programs such as Trojans, viruses and others."}, {"heading": "2. RELATED WORK", "text": "In the following, we look at the development of paradigms leading to the solution proposed in the next chapter."}, {"heading": "2.1 Multi instance learning problem", "text": "The pioneering work [6] coined multiple instance or multi-instance learning as a problem in which each sample b (hereinafter referred to as a bag) consists of a series of instances that are used either in the way it is also used in the training set. Sample b was considered positive if at least one of its instances had a positive label, i.e. the label of a sample b is a label of a label b = maxx b yx. In this scenario, the predominant approach is the so-called instance space paradigm, i.e. a classification at the level of individual instances f: X 7 \u2192 1} and then the label of the bag b as maxx."}, {"heading": "2.2 Simultaneous Optimization of Embedding and Classifier", "text": "The important innovation introduced in [16] is that the embedding functions {\u03c6i} mi = 1 are optimized simultaneously with the classifier using them, in contrast to the state of the art, where the two optimization problems are handled independently of each other. Simultaneous optimization is achieved by using the formalism of the neural mesh, with one (or more) lower layers followed by a pooling layer implementing the embedding function \u03c6, and subsequent layers implementing the classifier, which thus builds on the bag representation in the form of a feature vector of fixed length. The model is outlined in Figure 2, whereby a single output neuron implements a linear classifier as soon as embedding into a feature representation of fixed length is realized."}, {"heading": "3. THE PROPOSED SOLUTION", "text": "In the light of the previous paragraph, the problem of identifying infected computers can be regarded as two MIL problems, one stacked on top of the other, in which the traffic of a computer b is generated by a two-step generative model."}, {"heading": "3.1 Generative Model", "text": "In fact, the fact is that most of us are able to feel as they want, and that they see themselves as being able to see themselves as being able to move, and that they are able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to move, to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to move."}, {"heading": "3.2 Discriminative model", "text": "In fact, most people are able to move to another world in which they are in the position in which they find themselves."}, {"heading": "3.3 Extracting indicators of compromise", "text": "The presented model is based on the assumption that there are server types that are contacted with varying probability by infected and clean computers, although generally not much is known about them. If these types do not exist, then the probability distributions of infected and clean computers would be the same and it would be impossible to create a reliable detector for them. However, if the neural network has learned to detect them, the vector representation of servers (output of the first part of the network (from input to first pooling in Figure 3) must have different probability distributions for clean and infected computers. As the above line of reasoning can be extended to output of the layer just before the first pooling function, the output of each neuron of this layer can be considered indicators of compromise, since it must contribute to the identification of infected computers. By an accurate inspection of the flows on which these neurons provide the highest output, the output of this type of network can be shown together with a specific IOD (which can be identified by a section 4.it)."}, {"heading": "3.4 Explaining the decision", "text": "Neural networks have the reputation of being a black box in the sense that they do not provide details of the decision. In the detection of intrusions, this behavior is undesirable, since the investigation of security must begin from scratch. Therefore, the investigator is given an explanation as to why the classifier considers the computer to be infectious of great help. The explanation method is based on the assumption that the currents caused by the infection are additive, i.e. the malware does not block the user's streams, but adds their own. This means that if the computer is considered to be infected, the network should reverse its decision by removing the correct streams (instances). Although locating the smallest number of such streams is probably a complete NP problem, a greedy approach inspired by [17] works surprisingly well. The greedy approach takes place in each iteration set of streams going to the same server (subbag), which causes the largest reduction in the output of the positive classification (if infected)."}, {"heading": "3.5 Computational complexity", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, a country, a city and a country, a country and a country."}, {"heading": "4.1 Network architecture", "text": "All evaluated neural networks use simple feature vectors (instances) with 36 cheap to calculate statistics, such as the length of the url, query and path parts, frequency of vowels and consonants, HTTP status, port of the client and server, etc., but not a single feature was extracted from the host name. Rated neural networks followed the architecture in Figure 3 with layer of 40 ReLu neurons before the first pooling, but then differentiated into: using either the medium or maximum pooling functions, using either a layer of 40 ReLu neurons or two layers of 20 ReLu neurons each between the first and second pooling, but finally using additional layers of 20 ReLu neurons each after the second pooling and final output neurons. Precision recall curves of all six evaluated neural networks or two layers with 20 ReLu neurons each between the first and second pools."}, {"heading": "4.2 Indicators of compromise", "text": "Since one of the main features of the proposed architecture is the ability to learn indicators of compromise IOCs, the types of traffic neurons in the plane just before the first pooling are shown below. Sensitivity was estimated by infected computers in the test kit for the simplest architectures (top line in Figure 4) with medium and maximum pooling functionality. We did not observe much difference between IOCs learned by network with medium and maximum pooling functionality. Learned IOCs include: \u2022 Tunneling by url (example shown in the appendix for its length); \u2022 sinkholed domains such as hxxxp: / / malware.ywaauuqmsksk.org / malware.9f6qmf0hs.ru / a.htm? u = 3969 23, hxxxp: / ww1ww1wwi-ww1wwi / malware.ywaauuqmsk.org / domax.ru with more repeatable characteristics such as.xx.m?"}, {"heading": "4.3 Example of explanation", "text": "Table 1 shows an explanation of the simplest evaluated neural network with maximum pooling functionality, which consists of a list of domain names with examples of requests to them, as described by the greedy algorithm in Section 3.4.The column \"NN Output\" shows how the output of the neural network decreases when flows to individual domains are iteratively removed. At the time of writing, the last three domains were all involved in communicating with some malware examples according to VirusTotal [1].Upon further searching a web page, we found this Article4, which states that www.inkstuds.org is hacked and used for malware.3We refer to hxxxxps: / www.herdprotect.com / domain-d. 7-zip.org.aspx for confirmation that it is indeed malware."}, {"heading": "5. CONCLUSION", "text": "This expansion of the MIL paradigm has been proven to bring many benefits, especially for our targeted application of intruder detection. The hierarchical model is easy to implement and requires only a minor modification of a standard neural network architecture, allowing extensive neural network knowledge to be used, including deep learning paradigms. The proposed architecture has key benefits that are particularly important for network security. First, it requires labels (clean / infected) only at the high level of computers rather than on individual data streams, which drastically saves human analysts time to construct the basic truth and also makes them more precise (sometimes it is almost impossible to determine whether the flow of data is related to an infection or not). Second, the learned assignment of traffic patterns to neurons can be used by compromise to detect intelligible indicators. Third, it is possible to identify data streams that have classified the computer as infected."}, {"heading": "6. REFERENCES", "text": "[1] Virus total. https: / / www.virustotal.com, 2016. [2] Tanzu Alpcan and Tamer Bas, 2001. [3] Jaume Amores. Multiple Instance networks: Review, taxonomy and comparative study. Artificial Intelligence, 201: 81-105, 2013. [4] Y-Lan Boureau, Jean Ponce, and Yann LeCun. A theoretical analysis of feature pooling instance in visual recognition. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 111-118, 2010. [5] Cisco Systems Inc. Cognitive Threat Analytics. https: / / cognitive.cisco.com. [6] Thomas G Dietterich, Richard H Lathrop, and Toma."}, {"heading": "A. TYPES OF LEARNED IOCS", "text": "In fact, the number of deaths has fallen by more than half in the last ten years, and the number of deaths has doubled; the number of deaths has doubled in the last ten years, and the number of deaths has doubled, and the number of deaths has doubled."}], "references": [{"title": "Network security: A decision and game-theoretic approach", "author": ["Tansu Alpcan", "Tamer Ba\u015far"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Multiple instance classification: Review, taxonomy and comparative study", "author": ["Jaume Amores"], "venue": "Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "A theoretical analysis of feature pooling in visual recognition", "author": ["Y-Lan Boureau", "Jean Ponce", "Yann LeCun"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["Thomas G Dietterich", "Richard H Lathrop", "Tom\u00e1s Lozano-P\u00e9rez"], "venue": "Artificial intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "A review of multi-instance learning assumptions", "author": ["James Foulds", "Eibe Frank"], "venue": "The Knowledge Engineering Review,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Learned-norm pooling for deep feedforward and recurrent neural networks", "author": ["Caglar Gulcehre", "Kyunghyun Cho", "Razvan Pascanu", "Yoshua Bengio"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint  arXiv:1412.6980,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Learning nonstationary models of normal network traffic for detecting novel attacks", "author": ["Matthew V. Mahoney", "Philip K. Chan"], "venue": "In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Learning from distributions via support measure machines", "author": ["Krikamol Muandet", "Kenji Fukumizu", "Francesco Dinuzzo", "Bernhard Sch\u00f6lkopf"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "A survey of techniques for internet traffic classification using machine learning", "author": ["T.T.T. Nguyen", "G. Armitage"], "venue": "IEEE Communications Surveys Tutorials,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Machine literature searching x. machine language; factors underlying its design and development", "author": ["James W. Perry", "Allen Kent", "Madeline M. Berry"], "venue": "American Documentation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1955}, {"title": "Optimizing pooling function for pooled steganalysis", "author": ["T. Pevn\u00fd", "I. Nikolaev"], "venue": "In Information Forensics and Security (WIFS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Using neural network formalism to solve multiple-instance problems", "author": ["Tom\u00e1\u0161 Pevn\u00fd", "Petr Somol"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Urca: Pulling out anomalies by their root causes", "author": ["F. Silveira", "C. Diot"], "venue": "In INFOCOM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Machine learning methods have been recently in focus due to their promise to improve false-positive-rate of behavioral malware detection[2].", "startOffset": 136, "endOffset": 139}, {"referenceID": 8, "context": ")[11, 13].", "startOffset": 1, "endOffset": 9}, {"referenceID": 10, "context": ")[11, 13].", "startOffset": 1, "endOffset": 9}, {"referenceID": 1, "context": "Our problem thus belongs to the family of multi-instance learning (MIL) problems [3, 7] where one sample is commonly called a bag (in our case representing a computer) and consists of a variable number of instances (in our case one instance is one flow), each described by a fixed dimensional vector.", "startOffset": 81, "endOffset": 87}, {"referenceID": 4, "context": "Our problem thus belongs to the family of multi-instance learning (MIL) problems [3, 7] where one sample is commonly called a bag (in our case representing a computer) and consists of a variable number of instances (in our case one instance is one flow), each described by a fixed dimensional vector.", "startOffset": 81, "endOffset": 87}, {"referenceID": 14, "context": "Finally, using approach similar to URCA [17], it is possible to identify particular connections which made the neural network decide that the computer is infected; hence effectively providing an explanation of the learned IoC.", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "The pioneering work [6] coined multiple-instance or multiinstance learning as a problem, where each sample b (to be denoted bag in the following) consists of a set of instances x, i.", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "Later works (see reviews [3, 7]) have introduced different assumptions on relationships between the labels on the instance level and labels of bags or even dropped the notion of instance-level labels and considered only labels on the level of bags, i.", "startOffset": 25, "endOffset": 31}, {"referenceID": 4, "context": "Later works (see reviews [3, 7]) have introduced different assumptions on relationships between the labels on the instance level and labels of bags or even dropped the notion of instance-level labels and considered only labels on the level of bags, i.", "startOffset": 25, "endOffset": 31}, {"referenceID": 13, "context": "Since the solution presented in Section 3 belongs to the embedded-space paradigm, we describe this class of methods in necessary detail and adopt the formalism of [16], which is for our solution essential.", "startOffset": 163, "endOffset": 167}, {"referenceID": 13, "context": "The formalism of [16] is intended for a general formulation of MIL problems, where labels are assumed only on the level of bags without any labels on the level of instances.", "startOffset": 17, "endOffset": 21}, {"referenceID": 3, "context": "Note that this definition also includes that used in [6].", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "Methods from embedded space-paradigm [3, 7] first represent each bag b as a fixed-dimensional vector and then use any machine learning algorithm with samples of fixed dimension.", "startOffset": 37, "endOffset": 43}, {"referenceID": 4, "context": "Methods from embedded space-paradigm [3, 7] first represent each bag b as a fixed-dimensional vector and then use any machine learning algorithm with samples of fixed dimension.", "startOffset": 37, "endOffset": 43}, {"referenceID": 3, "context": "[6] assumed labels on instances and a bag was classified as positive if it contained at least one positive instance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "The important novelty introduced in [16] is that embedding functions {\u03c6i}i=1 are optimized simultaneously with the classifier that uses them, as opposed to the prior art where the two optimization problems are treated indepedently.", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "in [9], where the pooling function has form", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "\u2022 Mean function should be theoretically better [12], since it is more general.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "The advantage of mean pooling function has been experimental demonstrated in [16].", "startOffset": 77, "endOffset": 81}, {"referenceID": 2, "context": "This problem has been recently studied in [4] in context of natural images.", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "Although finding the smallest number of such flows is likely an NP complete problem, a greedy approximation inspired by [17] performs surprisingly well.", "startOffset": 120, "endOffset": 124}, {"referenceID": 5, "context": "Therefore evaluated architectures used simple building blocks: rectified linear units [8, 12], mean and maximum pooling functions, and ADAM optimization algorithm [10].", "startOffset": 86, "endOffset": 93}, {"referenceID": 9, "context": "Therefore evaluated architectures used simple building blocks: rectified linear units [8, 12], mean and maximum pooling functions, and ADAM optimization algorithm [10].", "startOffset": 86, "endOffset": 93}, {"referenceID": 7, "context": "Therefore evaluated architectures used simple building blocks: rectified linear units [8, 12], mean and maximum pooling functions, and ADAM optimization algorithm [10].", "startOffset": 163, "endOffset": 167}, {"referenceID": 11, "context": "The performance was measured using precision-recall curve (PR curve) [14] popular in document classification and information retrieval due to its better properties for highly imbalanced problems, into which intrusion detection belongs (in the testing data there is approximately one infected computer per one thousand clean ones).", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "This conclusion is supported by the fact that max pooling function can be approximated from the mean if layers preceding the aggregation are sufficiently complicated [15].", "startOffset": 166, "endOffset": 170}], "year": 2017, "abstractText": "Modeling network traffic is gaining importance in order to counter modern threats of ever increasing sophistication. It is though surprisingly difficult and costly to construct reliable classifiers on top of telemetry data due to the variety and complexity of signals that no human can manage to interpret in full. Obtaining training data with sufficiently large and variable body of labels can thus be seen as prohibitive problem. The goal of this work is to detect infected computers by observing their HTTP(S) traffic collected from network sensors, which are typically proxy servers or network firewalls, while relying on only minimal human input in model training phase. We propose a discriminative model that makes decisions based on all computer\u2019s traffic observed during predefined time window (5 minutes in our case). The model is trained on collected traffic samples over equally sized time window per large number of computers, where the only labels needed are human verdicts about the computer as a whole (presumed infected vs. presumed clean). As part of training the model itself recognizes discriminative patterns in traffic targeted to individual servers and constructs the final high-level classifier on top of them. We show the classifier to perform with very high precision, while the learned traffic patterns can be interpreted as Indicators of Compromise. In the following we implement the discriminative model as a neural network with special structure reflecting two stacked multi-instance problems. The main advantages of the proposed configuration include not only improved accuracy and ability to learn from gross labels, but also automatic learning of server types (together with their detectors) which are typically visited by infected computers.", "creator": "LaTeX with hyperref package"}}}