{"id": "1703.02905", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Learning a Unified Control Policy for Safe Falling", "abstract": "Being able to fall safely is a necessary motor skill for humanoids performing highly dynamic tasks, such as running and jumping. We propose a new method to learn a policy that minimizes the maximal impulse during the fall. The optimization solves for both a discrete contact planning problem and a continuous optimal control problem. Once trained, the policy can compute the optimal next contacting body part (e.g. left foot, right foot, or hands), contact location and timing, and the required joint actuation. We represent the policy as a mixture of actor-critic neural network, which consists of n control policies and the corresponding value functions. Each pair of actor-critic is associated with one of the n possible contacting body parts. During execution, the policy corresponding to the highest value function will be executed while the associated body part will be the next contact with the ground. With this mixture of actor-critic architecture, the discrete contact sequence planning is solved through the selection of the best critics while the continuous control problem is solved by the optimization of actors. We show that our policy can achieve comparable, sometimes even higher, rewards than a recursive search of the action space using dynamic programming, while enjoying 50 to 400 times of speed gain during online execution.", "histories": [["v1", "Wed, 8 Mar 2017 16:38:21 GMT  (2688kb,D)", "https://arxiv.org/abs/1703.02905v1", null], ["v2", "Thu, 20 Apr 2017 15:17:35 GMT  (2688kb,D)", "http://arxiv.org/abs/1703.02905v2", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LG", "authors": ["visak cv kumar", "sehoon ha", "c karen liu"], "accepted": false, "id": "1703.02905"}, "pdf": {"name": "1703.02905.pdf", "metadata": {"source": "CRF", "title": "Learning a Unified Control Policy for Safe Falling", "authors": ["Visak C.V. Kumar", "Sehoon Ha", "C. Karen Liu"], "emails": ["visak3@gatech.edu,", "karenliu@cc.gatech.edu", "sehoon.ha@disneyresearch.com"], "sections": [{"heading": null, "text": "In fact, most people who are in a position to put themselves in the world, to put themselves in the world, are forced into the world, are forced into the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live."}, {"heading": "II. RELATED WORK", "text": "One possible approach is to find a few pitfalls in an environment where the types of falls are to be expected."}, {"heading": "III. PRELIMINARIES", "text": "The object of the problem is to find a sequence of actions so as to minimize the maximum momentum during the fall. The value function of such an optimization can be expressed as follows: V (s) = min a max (g (s, a), V (s, a)), (1) where s (s) and a) are the next state s (s, a) the contact space and the action. Function g (s, a): S \u00b7 A \u2192 R evaluates the momentum induced by taking the action a in the state s. The transition function, s \u2032 f (s, a), calculates the next state s \u00b2 by integrating the dynamic equations of the system. To solve the case problem efficiently, Ha and Liu [1] proposed an abstract model for approximating the motion of the humanoid (Figure 1 (a)."}, {"heading": "IV. METHOD", "text": "Our goal is to learn a control policy that is able to calculate the optimal contact sequence, the contact point and points of contact, and the joint action in real time. General policy consists of two components: abstract policy and common policy. Abstract policy determines the optimal action on the abstract model, while common policy maps the abstract action on the entire body of the robot and generates joint action. Figure 2 illustrates the workflow of general contact policy. We first define a function p: X 7 \u2192 S that maps the state of a robot (x-X) to the state of an abstract model (s-S). Mapping is easy because the complete set of common position and speed contains all the information necessary to calculate s. Since the robot first receives an external force, the state of the robot is first projected onto S and fed into abstract policy."}, {"heading": "A. Abstract-level Policy", "text": "8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,"}, {"heading": "B. Joint-level Policy", "text": "The goal of the common-level policy is to execute the action calculated by the abstract-level policy in the common space of the robot. Remember that the abstract-level policy gives an action vector a = {\u03b82, \u0445, r-d1} and the next contacting body part c. Along with the current state s = {c1, r1, \u03b81, r-1, \u03b8-1} we can define a triangle formed by the pivot point of the pendulum (v1), the mass point (v2) and the tip of the stopper (v3) (Figure 1 (b)).The shape of the triangle at the next moment of impact can be determined by aligning the triangle at the next moment of impact with the current point of contact of the robot and r1 + 0r-d1. The latter two terms are the predicted level 1 and the predicted level 1 at the next moment of impact. We then transform the triangle into the next contact frame of the current coordinate with the three-dimensional coordinate of the robot, aligning the next one with the three-dimensional coordinate of the robot."}, {"heading": "V. EXPERIMENTS", "text": "The test platform is a small humanoid, BioloidGP [17] with a height of 34.6 cm, a weight of 1.6 kg and 16 degrees of freedom activated. We also compare the results of our policy with those calculated by the Dynamic Programming (DP) method used by HaAlgorithm 1 Learning abstract-level policy 1: Randomly initialize \u03b8 2: Initialize training buffers with tuples from DP 3: while not done 4: EXPLORATION: 5: for k = 1 \u00b7 K do 6: s; Policy N0 7: while s.\u03b8 1 \u2265 0 do 8: c; Select actor stochastically using Equation 49: a \u043fai (s) + Nt 10: Apply and simulation until next impact moment11: s \"Current state of the abstract model 12: r \u00b2 r (s, a)."}, {"heading": "A. Learning of Abstract-Level Policy", "text": "In our experiment, we construct a network with 8 pairs of actors and critics to represent 8 possible contacting body parts: right toe, right heel, left toe, left heel, knee, elbow, hands and head. During training, we initially generate 5000 tuples from DP to initialize the training buffer. The learning process takes 1000 iterations, about 4 hours on 6 cores with 3.3GHz Intel i7 processor. Figure 4 shows the average reward of 10 randomly selected test cases over iterations. Once the policy is trained, a single query of the political network takes about 0.8 milliseconds, followed by 25 milliseconds of the inverse kinematic routine. The total of 25.8 milliseconds of computing time is a drastic improvement over DP, which requires 1 to 10 seconds of computing time. We compare the rewards of the trained policy with those of the DP by performing 1000 test cases starting from randomly sampled initial rewards, the left heel, the left heel, the 93 plans for a histogram only, the policy 25o84, the policy 25ogram only, the policy of the heel, the left heel, the policy is actually shown."}, {"heading": "B. Different falling strategies", "text": "Table I shows three different case strategies from the test cases. The table shows the starting position, the external pressure, the resulting contact sequence, the impulse through each contact (the number in the bracket) as well as the maximum impulse for each test case. If the robot starts with a two-foot posture, it uses knees and hands to stop a fall. If the initial state leans forward and is out of balance, the robot directly uses its hands to catch itself. If the robot starts with a one-foot posture, it is easier to use the swing foot followed by the hands to stop a fall. The motion sequences of the robot can be visualized in Figure 6 and the supplementary video. In any case, we compare our policy with DP and a naive controller who simply follows the starting position (referred to as Unplanned). Both our policy and our DP reduce the maximum impulse compared to unplanned. In cases where our policy exceeds the DP, the foot improvement can be achieved (by different time points)."}, {"heading": "C. Hardware Results", "text": "Finally, we compare the case strategy generated by our policy with the unplanned movement on the hardware of BioloidGP. Due to the lack of sensor capacity on board, BioloidGP cannot take advantage of the feedback aspect of our policy. Nevertheless, we can still use this platform to demonstrate the case strategy generated by our policy and compare it to an unplanned movement. We first align the starting position of the simulated BioloidGP with the real one and move the simulated BioloidGP from behind by 3 N and 5 N, provided that the pushes we applied with our hand to the robot are roughly the same. Then, we apply our policy to the simulated BioloidGP to obtain a sequence of target positions. In the hardware experiment, we program BioloidGP to follow these poses as soon as a fall is detected. During the falls, we measure the acceleration of the head with the help of an external IMU. Figure 8 shows the results of two different falls being pushed by the first case of a 3N and the third by a robot."}, {"heading": "VI. CONCLUSION", "text": "Unlike most optimal control problems, the scope of action of our problem consists of both discrete and continuous variables. To address this problem, our algorithm trains control measures (actors) and corresponding value functions (critics) in a network of actors and critics. As a result of this mix of actors and critics, discrete contact planning is included in the problem of expert selection, while the policy that corresponds to the highest value function is executed while the corresponding body part is the next contact. As a result of this mix of actors and critics, we throw discrete contact planning into the problem of expert selection as we optimize policy in continuous space. We show that our algorithm reliably reduces the maximum impulse of a variety of falls. Compared to the previous work [1], which employs a costly dynamic programming method during online execution, it is possible to achieve a better reward and only 0.25% to 2% of the forced time."}], "references": [{"title": "Multiple Contact Planning for Minimizing Damage of Humanoid Falls", "author": ["S. Ha", "C.K. Liu"], "venue": "IEEE IEEE/RSJ International Conference on Intelligent Robots and Systems, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement Learning in Continuous Action Spaces", "author": ["H. Van Hasselt", "M.A. Wiering"], "venue": "no. Adprl, pp. 272\u2013279, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Reinforcement Learning in Continuous State and Action Spaces", "author": ["H. Van Hasselt"], "venue": "Reinforcement Learning, pp. 207\u2014-251, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Terrain-Adaptive Locomotion Skills using Deep Reinforcement Learning", "author": ["X.B. Peng", "G. Berseth", "M. van de Panne"], "venue": "ACM Transactions on Graphics, vol. 35, no. 4, pp. 1\u201310, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "UKEMI: falling motion control to minimize damage to biped humanoid robot", "author": ["K. Fujiwara", "F. Kanehiro", "S. Kajita", "K. Kaneko", "K. Yokoi", "H. Hirukawa"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems, vol. 3, no. October, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Fall Detection and Management in Biped Humanoid Robots", "author": ["J. Ruiz-del solar", "S. Member", "J. Moya", "I. Parra-tsunekawa"], "venue": "Management, vol. 12, no. April, pp. 3323\u20133328, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards an optimal falling motion for a humanoid robot", "author": ["K. Fujiwara", "S. Kajita", "K. Harada", "K. Kaneko", "M. Morisawa", "F. Kanehiro", "S. Nakaoka", "H. Hirukawa"], "venue": "Proceedings of the 2006 6th IEEE- RAS International Conference on Humanoid Robots, HUMANOIDS, pp. 524\u2013529, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "An Optimal planning of falling motions of a humanoid robot", "author": ["\u2014\u2014"], "venue": "IEEE International Conference on Intelligent Robots and Systems, no. Table I, pp. 456\u2013462, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Whole-body trajectory optimization for humanoid falling", "author": ["J. Wang", "E. Whitman", "M. Stilman"], "venue": ". . . Control Conference (ACC), . . . , pp. 4837\u20134842, 2012. [Online]. Available: http://ieeexplore.ieee. org/xpls/abs{ }all.jsp?arnumber=6315177", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Tripod fall: Concept and experiments of a novel approach to humanoid robot fall damage reduction", "author": ["S.K. Yun", "A. Goswami"], "venue": "Proceedings - IEEE International Conference on Robotics and Automation, pp. 2799\u20132805, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Direction-changing fall control of humanoid robots: theory and experiments", "author": ["A. Goswami", "S.-k. Yun", "U. Nagarajan", "S.-H. Lee", "K. Yin", "S. Kalyanakrishnan"], "venue": "Autonomous Robots, vol. 36, no. 3, pp. 199\u2013223, jul 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. a. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv preprint arXiv:1509.02971, pp. 1\u201314, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deterministic Policy Gradient Algorithms", "author": ["D. Silver", "G. Lever", "N. Heess", "T. Degris", "D. Wierstra", "M. Riedmiller"], "venue": "Proceedings of the 31st International Conference on Machine Learning, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Reinforcement Learning in Parameterized Action Space", "author": ["M. Hausknecht", "P. Stone"], "venue": "arXiv, pp. 1\u201312, 2016. [Online]. Available: http://arxiv.org/abs/1511.04143", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel"], "venue": "CoRR, abs/1502.05477, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["J. Schulman", "P. Moritz", "S. Levine", "M. Jordan", "P. Abbeel"], "venue": "arXiv preprint arXiv:1506.02438, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "As shown in Ha and Liu [1], the falling problem can be formulated as a Markov Decision Process (MDP) that solves for a contact sequence with the ground such that the damage incurred during the fall is minimized.", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "[1], is computationally demanding and infeasible to deploy to the real world.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Indeed, the MDP formulation of the falling problem [1] makes it innately suitable for algorithms that take advantage of the recursive Bellman equation.", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "Our algorithm is based on CACLA [2], [3] which solves an actor-critic controller for continuous state and action spaces.", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "Our algorithm is based on CACLA [2], [3] which solves an actor-critic controller for continuous state and action spaces.", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "We also show that the algorithm can run in real time (as opposed to 1-10 seconds reported by [1]) once the policy is trained.", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "Comparing to the actions computed by Ha and Liu [1], our policy produces better rewards on average with only 0.", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "When a fall is detected, the sequence designed for the most similar falling scenario is executed [5], [6].", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "When a fall is detected, the sequence designed for the most similar falling scenario is executed [5], [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "To accelerate the computation, various simplified models have been proposed to approximate falling motions, such as an inverted pendulum [7], [8], a planar robot in the sagittal plane [9], a tripod [10], and a sequence of inverted pendulums [1].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "To accelerate the computation, various simplified models have been proposed to approximate falling motions, such as an inverted pendulum [7], [8], a planar robot in the sagittal plane [9], a tripod [10], and a sequence of inverted pendulums [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 8, "context": "To accelerate the computation, various simplified models have been proposed to approximate falling motions, such as an inverted pendulum [7], [8], a planar robot in the sagittal plane [9], a tripod [10], and a sequence of inverted pendulums [1].", "startOffset": 184, "endOffset": 187}, {"referenceID": 9, "context": "To accelerate the computation, various simplified models have been proposed to approximate falling motions, such as an inverted pendulum [7], [8], a planar robot in the sagittal plane [9], a tripod [10], and a sequence of inverted pendulums [1].", "startOffset": 198, "endOffset": 202}, {"referenceID": 0, "context": "To accelerate the computation, various simplified models have been proposed to approximate falling motions, such as an inverted pendulum [7], [8], a planar robot in the sagittal plane [9], a tripod [10], and a sequence of inverted pendulums [1].", "startOffset": 241, "endOffset": 244}, {"referenceID": 10, "context": "[11], who proposed to compute the optimal stepping location to change the falling direction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] are crucial to the efficiency and stability of our learning process, despite that the original work (DQN) is designed for learning Atari video games from pixels with the assumption that the action space is discrete.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] later combined the ideas of DQN and the deterministic policy gradient (DPG) [14] to learn actor-critic networks for continuous action space and demonstrated that end-to-end (vision perception to actuation) policies for dynamic tasks can be efficiently trained.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[13] later combined the ideas of DQN and the deterministic policy gradient (DPG) [14] to learn actor-critic networks for continuous action space and demonstrated that end-to-end (vision perception to actuation) policies for dynamic tasks can be efficiently trained.", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": "van Hasselt and Wiering introduced CACLA [2], [3] that represents an actor-critic approach using neural networks.", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "van Hasselt and Wiering introduced CACLA [2], [3] that represents an actor-critic approach using neural networks.", "startOffset": 46, "endOffset": 49}, {"referenceID": 12, "context": "Comparing to the recent work using actor-critic networks [13], [16], the main difference of CACLA (and our work as well) lies in the update scheme for the actor networks.", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "Comparing to the recent work using actor-critic networks [13], [16], the main difference of CACLA (and our work as well) lies in the update scheme for the actor networks.", "startOffset": 63, "endOffset": 67}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "For completeness we briefly review the MDP formulation of the falling problem proposed by Ha and Liu [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "To solve the falling problem efficiently, Ha and Liu [1] proposed an abstract model to approximate the motion of the humanoid (Figure 1(a)).", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "Ha and Liu [1] discretized the continuous action parameters and exploited the monotonic nature of the falling problem to accelerate the dynamic programming.", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "To overcome the challenge of optimizing over both discrete and continuous action variables, we introduce a new policy representation based on a neural network architecture inspired by MACE [4].", "startOffset": 189, "endOffset": 192}, {"referenceID": 0, "context": "The state space is defined as s = {c1, r1, \u03b81, \u1e591, \u03b8\u03071}, where the total elapsed time t is removed from the state space previously defined by Ha and Liu [1].", "startOffset": 153, "endOffset": 156}, {"referenceID": 0, "context": "where M is the mass and I is the inertia of the abstract model (see details in [1]).", "startOffset": 79, "endOffset": 82}, {"referenceID": 0, "context": "To ensure that these tuples have high reward, we use the dynamic-programming-based algorithm described in [1] (referred as DP thereafter) to simulate a large amount of rollouts from various initial states and collect tuples.", "startOffset": 106, "endOffset": 109}, {"referenceID": 3, "context": "We follow the same Boltzmann exploration scheme as in [4] to select the actor \u03a0i(s) based on the probability defined by the predicted output of the critics:", "startOffset": 54, "endOffset": 57}, {"referenceID": 11, "context": "We use the temporal difference to update the chosen critic similar to [12]:", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "Note that we also adopt the idea of target network from [12] to compute the target, yi, for the critic update.", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "and Liu [1].", "startOffset": 8, "endOffset": 11}, {"referenceID": 15, "context": "We implement and train the proposed network architecture using PyCaffe [18] on Ubuntu Linux.", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "Comparing to the previous work [1] that employs an expensive dynamic programming method during online execution, our policy can reach better reward and only takes 0.", "startOffset": 31, "endOffset": 34}, {"referenceID": 9, "context": "This assumption can be easily challenged when considering real-world falling scenarios, such as those described in [10], [11].", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "This assumption can be easily challenged when considering real-world falling scenarios, such as those described in [10], [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 10, "context": "One possible solution to handling more general falls is to employ a more complex model similar to the inertialoaded inverted pendulum proposed by [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 16, "context": "Given the increasingly more powerful policy learning algorithms for deep reinforcement learning [21], [22], motor skill learning with a large number of variables, as is the case with falling, becomes a feasible option.", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "Given the increasingly more powerful policy learning algorithms for deep reinforcement learning [21], [22], motor skill learning with a large number of variables, as is the case with falling, becomes a feasible option.", "startOffset": 102, "endOffset": 106}], "year": 2017, "abstractText": "Being able to fall safely is a necessary motor skill for humanoids performing highly dynamic tasks, such as running and jumping. We propose a new method to learn a policy that minimizes the maximal impulse during the fall. The optimization solves for both a discrete contact planning problem and a continuous optimal control problem. Once trained, the policy can compute the optimal next contacting body part (e.g. left foot, right foot, or hands), contact location and timing, and the required joint actuation. We represent the policy as a mixture of actor-critic neural network, which consists of n control policies and the corresponding value functions. Each pair of actor-critic is associated with one of the n possible contacting body parts. During execution, the policy corresponding to the highest value function will be executed while the associated body part will be the next contact with the ground. With this mixture of actor-critic architecture, the discrete contact sequence planning is solved through the selection of the best critics while the continuous control problem is solved by the optimization of actors. We show that our policy can achieve comparable, sometimes even higher, rewards than a recursive search of the action space using dynamic programming, while enjoying 50 to 400 times of speed gain during online execution.", "creator": "LaTeX with hyperref package"}}}