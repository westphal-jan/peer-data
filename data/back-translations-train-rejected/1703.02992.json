{"id": "1703.02992", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "A Manifold Approach to Learning Mutually Orthogonal Subspaces", "abstract": "Although many machine learning algorithms involve learning subspaces with particular characteristics, optimizing a parameter matrix that is constrained to represent a subspace can be challenging. One solution is to use Riemannian optimization methods that enforce such constraints implicitly, leveraging the fact that the feasible parameter values form a manifold. While Riemannian methods exist for some specific problems, such as learning a single subspace, there are more general subspace constraints that offer additional flexibility when setting up an optimization problem, but have not been formulated as a manifold.", "histories": [["v1", "Wed, 8 Mar 2017 19:08:28 GMT  (564kb,D)", "http://arxiv.org/abs/1703.02992v1", "9 pages, 3 Figures"]], "COMMENTS": "9 pages, 3 Figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["stephen giguere", "francisco garcia", "sridhar mahadevan"], "accepted": false, "id": "1703.02992"}, "pdf": {"name": "1703.02992.pdf", "metadata": {"source": "CRF", "title": "A Manifold Approach to Learning Mutually Orthogonal Subspaces", "authors": ["Stephen Giguere", "Francisco Garcia", "Sridhar Mahadevan"], "emails": ["SGIGUERE@CS.UMASS.EDU", "FMGARCIA@CS.UMASS.EDU", "SRIDHAR.MAHADEVAN@SRI.COM"], "sections": [{"heading": null, "text": "We propose Partitioned Subspace (PS) to optimize matrices that are forced to represent one or more subspaces. Each point on the manifold defines a partition of the input space into mutually orthogonal subspaces, in which the number of partitions and their size are defined by the user. As a result, different groups of characteristics can be learned by defining different objective functions for each partition. We demonstrate the properties of the manifold through experiments to analyze multiple data sets and domain adjustments."}, {"heading": "1. Introduction", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "2. Background", "text": "Before we define the partitioned subspace manifold, we briefly introduce some necessary background concepts from differential geometry."}, {"heading": "2.1. Riemannian Manifolds", "text": "In fact, most of us are able to go in search of a suitable place to settle down."}, {"heading": "2.2. Riemannian Optimization", "text": "While many standard optimization methods have been derived from manifolds (Edelman, Arias and Smith, 1998), due to their simplicity, we focus on the Riemman gradient parentage Q \u00b7 Q \u00b7. As with the standard gradient parentage, the goal is to use the gradient of a differentiable convex function f with respect to the matrix Q, which is called GQ, to construct a sequence of estimates Q \u2032 = Q \u2212 GQ that approaches the minimum of f, with the parameter \u03b1 controlling the size of the update at each step. However, in the Riemman gradient parentage, estimates are limited to a variety. Riemman gradient parentage proceeds by iterating three steps, as in Figure 2. The first step is to calculate the gradient at the current location given by GQ = Qf."}, {"heading": "3. The Partitioned Subspace Manifold", "text": "After defining the necessary basic concepts, we will now describe the Multiple Partitioned Subspace (PS), emphasizing the intuition behind its derivative. We will also derive the components necessary to apply the belt optimization to the Multiple. Note that in this section, we will use [Q] to designate [Q] PS for notation convenience."}, {"heading": "3.1. Formulation", "text": "The PS manifold equivalence relationship by equating two matrices if they are related by rotation of the first k1 columns, or the next k2 columns, etc.. The PS manifold equivalence relationship, if they are arerelated by both a rotation of their last n \u2212 k columns and a rotation of their first k columns and a rotation of their first k columns. The PS manifold equivalence relationship by equating two matrices if they are related by rotation of the first k1 columns, etc."}, {"heading": "3.2. Optimization on the PS Manifold", "text": "To apply the Rieman optimization to the Qi columns of the Qi column, we must derive the Qi column to the Qi gap (Q = Q = Q span) and an appropriate retraction. The projection of an arbitrary updating direction Z onto the tangent space at [Q] can be derived as follows. First, we project Z onto the matrices of formQA, where A is skew-symmetrical. \u2212 The result is given by Q skew (QTZ), where skew (X) = 12 (X \u2212 XT) (Edelman, Arias and Smith, 1998). Next, we replace the diagonal blocks of this matrix with zeros to obtain the projection of Z on matrices of the form given in the equation."}, {"heading": "4. Applications", "text": "In order to demonstrate how the PS distributor can be used in practice, we applied it to the problems of analyzing multiple data sets and domain matching. Specifically, in all these applications, we were not interested in establishing new state-of-the-art methods, but in providing concept evidence and demonstrating the properties of the PS distributor."}, {"heading": "4.1. Multiple Dataset Analysis", "text": "First, we use PS multiplicity to extract features that describe a collection of data sets, grouped by whether they are unique for a particular dataset (the attributes per dataset) or divided between several (the common attributes). Given a collection of D datasets X = {Xi} Di = 1, we define D + 1 partitions where the first D partitions capture the attributes per dataset and the last partition capture the common attributes. We note that generally, additional partitions could be defined to model more complex relationships, for example, to learn attributes that are unique to different pairs or triplets of datasets. For simplicity's sake, in this section we will consider only perdata sets and common attributes, leaving the problem of learning more general partitioning schemes as a promising direction for future work."}, {"heading": "4.1.1. PROBLEM FORMULATION", "text": "Our approach is motivated by the Principal Component Analysis (PCA), which calculates the k-dimensional subspace that best reconstructs a single input dataset (Dunteman, 1989).The subspace returned by PCA can be considered a solution to the following optimization problem: YPCA = arg min Y-Gr (n, k) | | X \u2212 XY Y Y Y Y Y Y T | | 2F, where Gr (n, k) Xi \u2212 niF denotes the dimensional subrange of IRn, and | \u00b7 F the Frobenius norm. The expression | X \u2212 XY Y Y Y T | | 2F calculates the reconstruction error between the original dataset X \u2212 Xi and its projection on Y given by XY Y Y T. Based on this optimization problem, we present Multiple Dataset PCA to solve the following related problem: Definition 2 Let Xi leave one of the D datasets in a collection, X, Qi, Qi, and Qi (Qi)."}, {"heading": "4.1.2. EXPERIMENTS", "text": "To evaluate MD-PCA, we used the Office + Caltech10 set, a standard object recognition benchmark that includes four sets of processed image data (Gong et al., 2012), each of which comes from a different source, which is either Amazon (A), DSLR (D), Caltech (C) or Webcam (W.) The original images are encoded as 800-bin histograms of SURF features, which were normalized and z-rated to have zero mean and unit standard deviation in each dimension, as described in (Gong et al., 2012). Each image is associated with one of ten class labels, but these were not used in our experiments because MD-PCA, like PCA, is an unattended learning approach. To evaluate the sensitivity of the MD-PCA results with respect to the selected partition sizes, we performed an exhaustive sweep value for this pk1 and pd mapping by dividing the result \u2212 3."}, {"heading": "4.1.3. REMARKS", "text": "This year, the number of job-related redundancies has multiplied in recent years, so that the number of job-related redundancies will double in the next few years."}, {"heading": "4.2. Domain Adaptation", "text": "In order to further illustrate the partitioned subspace in a variety of ways, we applied it to the problem of domain matching. Here, the goal is to use a marked dataset Xs originating from a source distribution to improve accuracy in predicting an unmarked target dataset Xt originating from a different target distribution. In this work, we look at the problem of classification in the target domain. Typical strategies for domain matching include the project Xs and Xt to a common space on which a classifier is then trained (Fernando et al., 2013; Gong et al., 2012; Gopalan, Li and Chellappa, 2011) or finding a transformation of the datasets so that a classifier generalizes well to both domains (Kulis, Saenko and Darrell, 2011; Chen et al., 2012). The unique properties of the partitioned subspace make it well suited to learning the source, the source and the backdrop."}, {"heading": "4.2.1. PROBLEM FORMULATION", "text": "To achieve this, we assume that there are class-specific sub-spaces where the source and target distributions are of equal value, so that a classifier trained in the projected data will generalize them across the domains as well. Secondly, we limit the sub-spaces to each other orthogonally to promote a better differentiation between classes according to projection. Partitioned sub-space makes it easy to specify and implement these objective terms. We point out that while the strategy of identifying domain or label-specific features is not new, the use of explicitly partitioned sub-spaces to capture these features is, to the best of our knowledge, straightforward to specify and implement them."}, {"heading": "4.2.2. EXPERIMENTS", "text": "We evaluated our approach using the Office + Caltech10 dataset described in Subsection 4.1.2. Unlike the multi-dataset analysis application, here we use the provided labels to perform supervised training on the source domain and to evaluate test accuracy on the target domain. Experiments were conducted for any pair of source and target domains, resulting in 12 configurations.To measure the effectiveness of using the CST subspace for domain matching, we compared two commonly used approaches related to ours. The first method, called the geodesic flow kernel (GFK), calculates one subspace to represent the source data and another to represent the target. A similarity between the samples is thencomputed by projecting its inner product to all the spaces between source and target. The similarities are then used as input to a 1-NN classification (Gong et)."}, {"heading": "4.2.3. REMARKS", "text": "Comparing the accuracy with SVM as a classifier, the partitioned subspace manifold can achieve a performance comparable to that of GRP and SA. This suggests that the different partitions are able to learn discriminatory subspaces that generalize to the target domain. Our approach also seems particularly suitable for the use of Naive Bayes for classification, in which case the predictions are made not by the distance between the data, but by the probability distributions of the individual characteristics."}, {"heading": "5. Conclusion", "text": "In this paper, we presented a formulation for the Partitioned Subspace Manifold that captures the geometry of reciprocal orthogonal subspaces and provides the components necessary for optimizing multiplicity by means of belt optimization techniques. To illustrate this, we proposed and analyzed the Multiple Dataset PCA, an extension of the principle of component analysis that is capable of grouping features from multiple datasets depending on whether they are unique to a particular dataset or shared between them. Furthermore, we demonstrated how the PS Mannifold can be used to learn class-specific characteristics for domain adaptations, and proposed a method that achieves comparable accuracy of multiple standard approaches. We are actively interested in extending this formulation to make partitions online so that they can adapt to optimum partitions where they can."}], "references": [{"title": "Optimization algorithms on matrix manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": "Princeton University Press.", "citeRegEx": "Absil et al\\.,? 2009", "shortCiteRegEx": "Absil et al\\.", "year": 2009}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K. Weinberger", "F. Sha"], "venue": "Langford, J., and Pineau, J., eds., Proceedings of the 29th International Conference on Machine Learning (ICML-12), ICML \u201912. New York, NY, USA: ACM.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Principal components analysis", "author": ["G.H. Dunteman"], "venue": "Number 69. Sage.", "citeRegEx": "Dunteman,? 1989", "shortCiteRegEx": "Dunteman", "year": 1989}, {"title": "The geometry of algorithms with orthogonality constraints", "author": ["A. Edelman", "T.A. Arias", "S.T. Smith"], "venue": "SIAM journal on Matrix Analysis and Applications 20(2):303\u2013353.", "citeRegEx": "Edelman et al\\.,? 1998", "shortCiteRegEx": "Edelman et al\\.", "year": 1998}, {"title": "Unsupervised visual domain adaptation using subspace alignment", "author": ["B. Fernando", "A. Habrard", "M. Sebban", "T. Tuytelaars"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2960\u20132967.", "citeRegEx": "Fernando et al\\.,? 2013", "shortCiteRegEx": "Fernando et al\\.", "year": 2013}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["B. Gong", "Y. Shi", "F. Sha", "K. Grauman"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, 2066\u20132073. IEEE.", "citeRegEx": "Gong et al\\.,? 2012", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "2011 international conference on computer vision, 999\u20131006. IEEE.", "citeRegEx": "Gopalan et al\\.,? 2011", "shortCiteRegEx": "Gopalan et al\\.", "year": 2011}, {"title": "Online robust subspace tracking from partial information", "author": ["J. He", "L. Balzano", "J. Lui"], "venue": "arXiv preprint arXiv:1109.3827.", "citeRegEx": "He et al\\.,? 2011", "shortCiteRegEx": "He et al\\.", "year": 2011}, {"title": "Incremental gradient on the grassmannian for online foreground and background separation in subsampled video", "author": ["J. He", "L. Balzano", "A. Szlam"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, 1568\u20131575. IEEE.", "citeRegEx": "He et al\\.,? 2012", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "On-line learning of mutually orthogonal subspaces for face recognition by image sets", "author": ["T.-K. Kim", "J. Kittler", "R. Cipolla"], "venue": "IEEE Transactions on Image Processing 19(4):1067\u20131074.", "citeRegEx": "Kim et al\\.,? 2010", "shortCiteRegEx": "Kim et al\\.", "year": 2010}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis", "K. Saenko", "T. Darrell"], "venue": "Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR \u201911, 1785\u20131792. Washington, DC, USA:", "citeRegEx": "Kulis et al\\.,? 2011", "shortCiteRegEx": "Kulis et al\\.", "year": 2011}, {"title": "Locally time-invariant models of human activities using trajectories on the grassmannian", "author": ["P. Turaga", "R. Chellappa"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, 2435\u2013 2441. IEEE.", "citeRegEx": "Turaga and Chellappa,? 2009", "shortCiteRegEx": "Turaga and Chellappa", "year": 2009}, {"title": "Plsregression: a basic tool of chemometrics", "author": ["S. Wold", "M. Sj\u00f6str\u00f6m", "L. Eriksson"], "venue": "Chemometrics and intelligent laboratory systems 58(2):109\u2013130.", "citeRegEx": "Wold et al\\.,? 2001", "shortCiteRegEx": "Wold et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 11, "context": "The feasible set for this constraint corresponds to the Grassmannian manifold, which has proven useful for a large number of applications including background separation in video, human activity analysis, subspace tracking, and others (He, Balzano, and Lui, 2011; Turaga and Chellappa, 2009; He, Balzano, and Szlam, 2012).", "startOffset": 235, "endOffset": 321}, {"referenceID": 2, "context": "Our approach is motivated by the principal components analysis (PCA), which computes the k-dimensional subspace that best reconstructs a single input dataset (Dunteman, 1989).", "startOffset": 158, "endOffset": 174}, {"referenceID": 5, "context": "To evaluate MD-PCA, we used The Office+Caltech10 set, which is a standard object recognition benchmark containing four datasets of processed image data (Gong et al., 2012).", "startOffset": 152, "endOffset": 171}, {"referenceID": 5, "context": "The original images are encoded as 800-bin histograms of SURF features that have been normalized and z-scored to have zero mean and unit standard deviation in each dimension, as described in (Gong et al., 2012).", "startOffset": 191, "endOffset": 210}, {"referenceID": 4, "context": "Typical strategies for domain adaptation include projecting Xs and Xt onto a common space on which a classifier is then trained (Fernando et al., 2013; Gong et al., 2012; Gopalan, Li, and Chellappa, 2011), or finding a transformation of the datasets such that a classifier generalizes well to both domains (Kulis, Saenko, and Darrell, 2011; Chen et al.", "startOffset": 128, "endOffset": 204}, {"referenceID": 5, "context": "Typical strategies for domain adaptation include projecting Xs and Xt onto a common space on which a classifier is then trained (Fernando et al., 2013; Gong et al., 2012; Gopalan, Li, and Chellappa, 2011), or finding a transformation of the datasets such that a classifier generalizes well to both domains (Kulis, Saenko, and Darrell, 2011; Chen et al.", "startOffset": 128, "endOffset": 204}, {"referenceID": 1, "context": ", 2012; Gopalan, Li, and Chellappa, 2011), or finding a transformation of the datasets such that a classifier generalizes well to both domains (Kulis, Saenko, and Darrell, 2011; Chen et al., 2012).", "startOffset": 143, "endOffset": 196}, {"referenceID": 5, "context": "The similarities are then used as input to a 1-NN classifier (Gong et al., 2012).", "startOffset": 61, "endOffset": 80}, {"referenceID": 4, "context": "After the transformation is applied, a support vector machine classifier is trained (Fernando et al., 2013).", "startOffset": 84, "endOffset": 107}, {"referenceID": 5, "context": "To ensure a fair comparison, we used the same evaluation protocols used for GFK and SA for our experiments (Gong et al., 2012; Fernando et al., 2013).", "startOffset": 107, "endOffset": 149}, {"referenceID": 4, "context": "To ensure a fair comparison, we used the same evaluation protocols used for GFK and SA for our experiments (Gong et al., 2012; Fernando et al., 2013).", "startOffset": 107, "endOffset": 149}], "year": 2017, "abstractText": "Although many machine learning algorithms involve learning subspaces with particular characteristics, optimizing a parameter matrix that is constrained to represent a subspace can be challenging. One solution is to use Riemannian optimization methods that enforce such constraints implicitly, leveraging the fact that the feasible parameter values form a manifold. While Riemannian methods exist for some specific problems, such as learning a single subspace, there are more general subspace constraints that offer additional flexibility when setting up an optimization problem but have not been formulated as a manifold. We propose the partitioned subspace (PS) manifold for optimizing matrices that are constrained to represent one or more subspaces. Each point on the manifold defines a partitioning of the input space into mutually orthogonal subspaces, where the number of partitions and their sizes are defined by the user. As a result, distinct groups of features can be learned by defining different objective functions for each partition. We illustrate the properties of the manifold through experiments on multiple dataset analysis and domain adaptation.", "creator": "LaTeX with hyperref package"}}}