{"id": "1703.03054", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection", "abstract": "Despite progress in visual perception tasks such as image classification and detection, computers still struggle to understand the interdependency of objects in the scene as a whole, e.g., relations between objects or their attributes. Existing methods often ignore global context cues capturing the interactions among different object instances, and can only recognize a handful of types by exhaustively training individual detectors for all possible relationships. To capture such global interdependency, we propose a deep Variation-structured Reinforcement Learning (VRL) framework to sequentially discover object relationships and attributes in the whole image. First, a directed semantic action graph is built using language priors to provide a rich and compact representation of semantic correlations between object categories, predicates, and attributes. Next, we use a variation-structured traversal over the action graph to construct a small, adaptive action set for each step based on the current state and historical actions. In particular, an ambiguity-aware object mining scheme is used to resolve semantic ambiguity among object categories that the object detector fails to distinguish. We then make sequential predictions using a deep RL framework, incorporating global context cues and semantic embeddings of previously extracted phrases in the state vector. Our experiments on the Visual Relationship Detection (VRD) dataset and the large-scale Visual Genome dataset validate the superiority of VRL, which can achieve significantly better detection results on datasets involving thousands of relationship and attribute types. We also demonstrate that VRL is able to predict unseen types embedded in our action graph by learning correlations on shared graph nodes.", "histories": [["v1", "Wed, 8 Mar 2017 22:09:10 GMT  (4055kb,D)", "http://arxiv.org/abs/1703.03054v1", "This manuscript is accepted by CVPR 2017 as a spotlight paper"]], "COMMENTS": "This manuscript is accepted by CVPR 2017 as a spotlight paper", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["xiaodan liang", "lisa lee", "eric p xing"], "accepted": false, "id": "1703.03054"}, "pdf": {"name": "1703.03054.pdf", "metadata": {"source": "CRF", "title": "Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection", "authors": ["Xiaodan Liang", "Lisa Lee", "Eric P. Xing"], "emails": ["xiaodan1@cs.cmu.edu", "lslee@cs.cmu.edu", "epxing@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "Although much progress has been made in image classification [7], recognition [20] and segmentation [15] are still far from achieving the goal of holistic scene understanding - that is, a model that is capable of recognizing the interactions and relationships between objects and describing their attributes. While objects are the core building blocks of an image, it is often the relationships and attributes that determine the holistic interpretation of the scene. For example, the left image can be understood as \"a man standing on a yellow and green skateboard,\" and the right image as \"a woman wearing a blue suit and kneeling on a surfboard.\" The ability to extract and exploit such visual information would be useful for many real-world applications such as image search, answering questions [1, 9] and fine-grained recognition [27, 4]. Visual relationships are a pair of localized objects connected via a predicate."}, {"heading": "2. Related Works", "text": "Most existing approaches [22] [13], however, can detect only a handful of predefined, common types by training individual detectors for each relationship. Furthermore, some methods [10, 23, 14] have organized predictions into a scene graph that can provide a structured representation for describing the objects, their attributes, and relationships in each image. In particular, Johnson et al. [10] have introduced a conditional random field model to think about possible foundations of scene graphics, while Schuster et al. [23] have proposed a rules-based and classifying scene graph parser."}, {"heading": "3. Deep Variation-structured Reinforcement", "text": "LearningWe propose a novel VFL framework that formulates the problem of recognizing visual relationships and attributes as a sequential decision-making process. An overview is provided by Figure 2. Key components of the VFL, including the directed semantic action graph, the variation-structured transversal scheme, the state space, and the reward function, are described in detail in the following sections."}, {"heading": "3.1. Directed Semantic Action Graph", "text": "We build a directed semantic graph G = (V, E) to structure all possible object categories, attributes, and relationships into a compact and semantically meaningful representation (see Figure 2). Nodes V consist of the set of all object categories C, attributes A, and predicates P. Object categories in C are nouns and subject categories over a predicate. Predicates in P can be spatial (e.g. \"within\"), compositional (e.g. \"part of\"), or acting (e.g. \"vibrating\"). The directed edges E consist of attribute concepts and subject notions over a predicate. Predicates in P can be spatial (e.g. \"within\"), compositional (e.g. \"part of\"), or acting (e.g. \"vibrating\")."}, {"heading": "3.2. Variation-structured RL", "text": "Instead of exploring the entire action space, as in the traditional deep RL [18, 28], we propose a new, multi-layered approach to semantic action graphics that dynamically constructs small action sets for each step. First, VRL uses an object detector to obtain a set of object parameters, and then sequentially maps relationships and attributes to each instance. Since subject instances in an image often have multiple relationships and attributes, we do a broad initial search: We say all relationships and attributes in relation to the current subject instance of interest, and then we move on to the next instance."}, {"heading": "3.3. Deep Variation-structured RL", "text": "We optimize three strategies to select three measures for each state by maximizing the sum of discounted Q-rewards that can be formulated as a decision-making process in the Deep RL framework. Due to the high-dimensional continuous image data and a model-free environment, we resort to the Deep Q-Network (DQN) Framework (max) proposed by [17, 18], which generalizes well to invisible input factors. Fig. 3. Specifically, we use DQN to estimate three Q-value sets (DQN), parameterized by network weights (\u03b1), p-values, p-values corresponding to action sets A, P, C-numbers. In each training phase, we use a greedy strategy to select actions ga, gp, gc values in the variationally structured action space."}, {"heading": "4. Experiments", "text": "We conduct our experiments on the basis of the Visual Relationship Detection (VRD) for a shared network of 60 people. [16] and the Visual Genome Dataset [13]. VRD [16] contains 5000 images (4000 for training, 1000 for testing) with 100 object categories and 70 predicates. In total, the dataset contains 37,993 relationship cases with 6,672 relationship types, of which 1,877 relationships occur only in the test set and not in the training set. For the Visual Genome Dataset [13], we experiment with 87,398 images (5000 of which are held for validation and 5000 for testing) containing 703,839 relationship instances with 13,823 relationship types and 1,464,446 attribute types with 8,561 attribute types. There are 2,015 relationship types that occur in the test set but not in the training set, which allows us to evaluate VRL on zero-shooting implementation.LearnImplementation."}, {"heading": "4.1. Comparison with State-of-the-art Models", "text": "The comparison of results with baseline methods on VRD and Visual Genome is reported in Tables 1, 2 and 3.Shared Detectors vs. Individual Detectors. The compared models can be divided into two classes: (1) Models that train individual detectors for each predicate or attribute type, i.e., Visual Phrases [22], Joint CNN + RCNN [25], Joint CNN + RPN [25], Faster R-CNN [20], Joint CNN + Trained RPN [20]. Models that train common detectors for predicate or attribute types, and then combine their results with object detectors to generate the final prediction, i.e., Lu et al. V only [16], Faster R-CNN V only attribute al. [20], Lu et al. [16] and our VRune network is often large."}, {"heading": "4.2. Discussion", "text": "In fact, most people who are able to move are able to move to another world, in which they are able to move to another world, in which they are able to integrate, \"he said in an interview with the New York Times:\" I don't think we will be able to change the world we live in. \""}, {"heading": "4.3. Zero-shot Learning", "text": "A promising model should be able to predict invisible relationships, as the training data does not cover all possible relationship types. Lu et al. [16] uses Word embedding to project similar relationships onto invisible ones, while our VFL uses a large semantic plot graph to learn similar relationships on common graph nodes. Our VFL achieves performance improvements of > 5% over Lu et al. [16] in both sets of data."}, {"heading": "5. Conclusion and Future Work", "text": "We proposed a novel, variation-structured reinforcement learning system for recognizing visual relationships and attributes. VRL finds relationships and attribute instances sequentially according to a variation-structured transversal scheme on a directed semantic action graph. It takes into account global interdependencies to facilitate predictions in local regions. Our experiments on VRD and the Visual Genome Dataset demonstrate the power and efficiency of our model over baselines. As a future work, a larger directed action graph can be created using natural language sets. In addition, VRL can be generalized to an unattended learning framework to learn from an enormous number of unlabeled images."}], "references": [{"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Active object localization with deep reinforcement learning", "author": ["J.C. Caicedo", "S. Lazebnik"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pages 2488\u20132496,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "International journal of computer vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Deep reinforcement learning for robotic manipulation", "author": ["S. Gu", "E. Holly", "T. Lillicrap", "S. Levine"], "venue": "arXiv preprint arXiv:1610.00633,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Image retrieval using scene graphs", "author": ["J. Johnson", "R. Krishna", "M. Stark", "L.-J. Li", "D.A. Shamma", "M.S. Bernstein", "L. Fei-Fei"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1996}, {"title": "Skip-thought vectors. In Advances in neural information processing", "author": ["R. Kiros", "Y. Zhu", "R.R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "On support relations and semantic scene graphs", "author": ["W. Liao", "M.Y. Yang", "H. Ackermann", "B. Rosenhahn"], "venue": "arXiv preprint arXiv:1609.05834,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Visual relationship detection with language priors", "author": ["C. Lu", "R. Krishna", "M. Bernstein", "L. Fei-Fei"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "In Deep Learning, Neural Information Processing Systems Workshop,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Query adaptive similarity for large scale object retrieval", "author": ["D. Qin", "C. Wengert", "L. Van Gool"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Viske: Visual knowledge extraction and question answering by visual verification of relation phrases", "author": ["F. Sadeghi", "S.K. Divvala", "A. Farhadi"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Recognition using visual phrases", "author": ["M.A. Sadeghi", "A. Farhadi"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Generating semantically precise scene graphs from textual descriptions for improved image retrieval", "author": ["S. Schuster", "R. Krishna", "A. Chang", "L. Fei-Fei", "C.D. Manning"], "venue": "In Proceedings of the Fourth Workshop on Vision and Language,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In ICLR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Reasoning about object affordances in a knowledge base representation", "author": ["Y. Zhu", "A. Fathi", "L. Fei-Fei"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Y. Zhu", "R. Mottaghi", "E. Kolve", "J.J. Lim", "A. Gupta", "L. Fei- Fei", "A. Farhadi"], "venue": "arXiv preprint arXiv:1609.05143,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Although much progress has been made in image classification [7], detection [20] and segmentation [15], we are still far from reaching the goal of holistic scene man", "startOffset": 61, "endOffset": 64}, {"referenceID": 19, "context": "Although much progress has been made in image classification [7], detection [20] and segmentation [15], we are still far from reaching the goal of holistic scene man", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "Although much progress has been made in image classification [7], detection [20] and segmentation [15], we are still far from reaching the goal of holistic scene man", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "Being able to extract and exploit such visual information would benefit many realworld applications such as image search [19], question answering [1, 9], and fine-grained recognition [27, 4].", "startOffset": 121, "endOffset": 125}, {"referenceID": 0, "context": "Being able to extract and exploit such visual information would benefit many realworld applications such as image search [19], question answering [1, 9], and fine-grained recognition [27, 4].", "startOffset": 146, "endOffset": 152}, {"referenceID": 8, "context": "Being able to extract and exploit such visual information would benefit many realworld applications such as image search [19], question answering [1, 9], and fine-grained recognition [27, 4].", "startOffset": 146, "endOffset": 152}, {"referenceID": 26, "context": "Being able to extract and exploit such visual information would benefit many realworld applications such as image search [19], question answering [1, 9], and fine-grained recognition [27, 4].", "startOffset": 183, "endOffset": 190}, {"referenceID": 3, "context": "Being able to extract and exploit such visual information would benefit many realworld applications such as image search [19], question answering [1, 9], and fine-grained recognition [27, 4].", "startOffset": 183, "endOffset": 190}, {"referenceID": 19, "context": "Detecting relationships and attributes is more challenging than traditional object detection [20] due to the following reasons: (1) There are a massive number of possible relationship and attribute types (e.", "startOffset": 93, "endOffset": 97}, {"referenceID": 12, "context": ", 13,894 relationship types in Visual Genome [13]), resulting in a greater skew of rare and infrequent types.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "Existing approaches [22, 10, 16] only predict a limited set of relationship types (e.", "startOffset": 20, "endOffset": 32}, {"referenceID": 9, "context": "Existing approaches [22, 10, 16] only predict a limited set of relationship types (e.", "startOffset": 20, "endOffset": 32}, {"referenceID": 15, "context": "Existing approaches [22, 10, 16] only predict a limited set of relationship types (e.", "startOffset": 20, "endOffset": 32}, {"referenceID": 21, "context": ", 13 in Visual Phrase [22]) and ignore semantic interdependencies between relationships and attributes by evaluating each region within a scene separately [16].", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": ", 13 in Visual Phrase [22]) and ignore semantic interdependencies between relationships and attributes by evaluating each region within a scene separately [16].", "startOffset": 155, "endOffset": 159}, {"referenceID": 23, "context": "Second, existing deep reinforcement learning (RL) models [24] often require several costly episodes of trial and error to converge, even with a small action space, and our large action space would exacerbate this problem.", "startOffset": 57, "endOffset": 61}, {"referenceID": 27, "context": "It makes a better tradeoff between increasing the input dimension and utilizing more historical context, compared to appending history frames [28] or binary action vectors [2] as in previous RL methods.", "startOffset": 142, "endOffset": 146}, {"referenceID": 1, "context": "It makes a better tradeoff between increasing the input dimension and utilizing more historical context, compared to appending history frames [28] or binary action vectors [2] as in previous RL methods.", "startOffset": 172, "endOffset": 175}, {"referenceID": 15, "context": "Extensive experiments on the Visual Relationship Detection (VRD) dataset [16] and Visual Genome dataset [13] demonstrate that the proposed VRL outperforms state-ofthe-art methods for both relationship and attribute detection, and also has good generalization capabilities for predicting unseen types.", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "Extensive experiments on the Visual Relationship Detection (VRD) dataset [16] and Visual Genome dataset [13] demonstrate that the proposed VRL outperforms state-ofthe-art methods for both relationship and attribute detection, and also has good generalization capabilities for predicting unseen types.", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "relationship detection [22, 21, 13].", "startOffset": 23, "endOffset": 35}, {"referenceID": 20, "context": "relationship detection [22, 21, 13].", "startOffset": 23, "endOffset": 35}, {"referenceID": 12, "context": "relationship detection [22, 21, 13].", "startOffset": 23, "endOffset": 35}, {"referenceID": 21, "context": "However, most existing approaches [22] [13] can detect only a handful of pre-defined, frequent types by training individual detectors for each relationship.", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "However, most existing approaches [22] [13] can detect only a handful of pre-defined, frequent types by training individual detectors for each relationship.", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "[16] leveraged word embeddings to handle large-scale relationships.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Furthermore, some methods [10, 23, 14] organized predictions into a scene graph which can provide a structured representation for describing the objects, their attributes and relationships in each image.", "startOffset": 26, "endOffset": 38}, {"referenceID": 22, "context": "Furthermore, some methods [10, 23, 14] organized predictions into a scene graph which can provide a structured representation for describing the objects, their attributes and relationships in each image.", "startOffset": 26, "endOffset": 38}, {"referenceID": 13, "context": "Furthermore, some methods [10, 23, 14] organized predictions into a scene graph which can provide a structured representation for describing the objects, their attributes and relationships in each image.", "startOffset": 26, "endOffset": 38}, {"referenceID": 9, "context": "[10] introduced a conditional random field model for reasoning about possible groundings of scene graphs while Schuster et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] proposed a rule-based and classifier-based scene graph parser.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Integrating deep learning methods with reinforcement learning (RL) [11] has recently shown very promising results on decision-making problems.", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "[18] proposed using deep Q-networks to play ATARI games.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] proposed a new search algorithm based on the integration of Monte-Carlo tree search with deep RL, which beat the world champion in the game of Go.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": ", robotic manipulation [6], indoor navigation [28], and object proposal generation [2].", "startOffset": 23, "endOffset": 26}, {"referenceID": 27, "context": ", robotic manipulation [6], indoor navigation [28], and object proposal generation [2].", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": ", robotic manipulation [6], indoor navigation [28], and object proposal generation [2].", "startOffset": 83, "endOffset": 86}, {"referenceID": 12, "context": "The recently released Visual Genome dataset [13] provides a large-scale annotation of images containing 18,136 unique object categories, 13,041 unique attributes, and 13,894 unique relationships.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "Instead of learning in the entire action space as in traditional deep RL [18, 28], we propose a novel variationstructured traversal scheme over the semantic action graph that dynamically constructs small action sets for each step.", "startOffset": 73, "endOffset": 81}, {"referenceID": 27, "context": "Instead of learning in the entire action space as in traditional deep RL [18, 28], we propose a novel variationstructured traversal scheme over the semantic action graph that dynamically constructs small action sets for each step.", "startOffset": 73, "endOffset": 81}, {"referenceID": 19, "context": "For our experiments, we used state-of-the-art Faster R-CNN [20] as the object detector, where the network parameters were initialized using the pre-trained VGG-16 ImageNet model [25].", "startOffset": 59, "endOffset": 63}, {"referenceID": 24, "context": "For our experiments, we used state-of-the-art Faster R-CNN [20] as the object detector, where the network parameters were initialized using the pre-trained VGG-16 ImageNet model [25].", "startOffset": 178, "endOffset": 182}, {"referenceID": 24, "context": "The state vector f is a concatenation of (1) a 4096-dim feature of the whole image, taken from the fc6 layer of the pre-trained VGG-16 ImageNet model [25]; (2) two 4096-dim features of the subject s and object s\u2032 instances, taken from the conv5 3 layer of the trained Faster R-CNN object detector; and (3) a 9600-dim history phrase embedding, which is created by concatenating four 2400-dim semantic embeddings from a Skip-thought language model [12] of the last two relationship phrases (relating s and s\u2032) and the last two attribute phrases (describing s) that were predicted by VRL.", "startOffset": 150, "endOffset": 154}, {"referenceID": 11, "context": "The state vector f is a concatenation of (1) a 4096-dim feature of the whole image, taken from the fc6 layer of the pre-trained VGG-16 ImageNet model [25]; (2) two 4096-dim features of the subject s and object s\u2032 instances, taken from the conv5 3 layer of the trained Faster R-CNN object detector; and (3) a 9600-dim history phrase embedding, which is created by concatenating four 2400-dim semantic embeddings from a Skip-thought language model [12] of the last two relationship phrases (relating s and s\u2032) and the last two attribute phrases (describing s) that were predicted by VRL.", "startOffset": 446, "endOffset": 450}, {"referenceID": 11, "context": ", \u201cperson riding bicycle\u201d) is embedded into a 2400-dim vector using a pre-trained Skip-thought language model [12], thus resulting in a 9600-dim history phrase embedding.", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "Due to the high-dimensional continuous image data and a model-free environment, we resort to the deep Q-Network (DQN) framework proposed by [17, 18], which generalizes well to unseen inputs.", "startOffset": 140, "endOffset": 148}, {"referenceID": 17, "context": "Due to the high-dimensional continuous image data and a model-free environment, we resort to the deep Q-Network (DQN) framework proposed by [17, 18], which generalizes well to unseen inputs.", "startOffset": 140, "endOffset": 148}, {"referenceID": 16, "context": "The replay memory helps stabilize the training by smoothing the training distribution over past experiences and reducing correlation between training samples [17, 18].", "startOffset": 158, "endOffset": 166}, {"referenceID": 17, "context": "The replay memory helps stabilize the training by smoothing the training distribution over past experiences and reducing correlation between training samples [17, 18].", "startOffset": 158, "endOffset": 166}, {"referenceID": 15, "context": "We conduct our experiments on the Visual Relationship Detection (VRD) dataset [16] and the Visual Genome dataset [13].", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": "We conduct our experiments on the Visual Relationship Detection (VRD) dataset [16] and the Visual Genome dataset [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 15, "context": "VRD [16] contains 5000 images (4000 for training, 1000 for testing) with 100 object categories and 70 predicates.", "startOffset": 4, "endOffset": 8}, {"referenceID": 12, "context": "For the Visual Genome Dataset [13], we experiment on 87,398 images (out of which 5000 are held out for validation, and 5000 for testing), containing 703,839 relationship instances with 13,823 relationship types and 1,464,446 attribute instances with", "startOffset": 30, "endOffset": 34}, {"referenceID": 21, "context": "Visual Phrases [22] 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": "Joint CNN+R-CNN [25] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "Joint CNN+RPN [25] 2.", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "V only [16] 2.", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "Faster R-CNN [20] 3.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "Joint CNN+Trained RPN [20] 3.", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "Faster R-CNN V only [20] 6.", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "[16] 17.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] (zero-shot) 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "We train a deep Q-network for 60 epochs with a shared RMSProp optimizer [26].", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "On VRD [16], VRL takes about 8 hours to train an object detector with 100 object categories, and two days to converge.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "On the Visual Genome dataset [13], VRL takes between 4 to 5 days to train an object detector with 1750 object categories, and one week to converge.", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "Following [16], we use recall@100 and recall@50 as our evaluation metrics.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "As discussed in [16], we do not use the mean average precision (mAP), which is a pessimistic evaluation metric because the dataset cannot exhaustively annotate all Table 2.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "Joint CNN+R-CNN [25] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "Joint CNN+RPN [25] 1.", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "V only [16] 1.", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "Faster R-CNN [20] 2.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "Joint CNN+Trained RPN [20] 2.", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "Faster R-CNN V only [20] 5.", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "[16] 10.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] (zero-shot) 1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Joint CNN+R-CNN [25] 2.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "Joint CNN+RPN [25] 3.", "startOffset": 14, "endOffset": 18}, {"referenceID": 19, "context": "Faster R-CNN [20] 7.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "Joint CNN+Trained RPN [20] 9.", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "Following [16], we evaluate on three tasks: (1) In relationship phrase detection [22], the goal is to predict a \u201csubject-predicate-object\u201d phrase, where the localization of the entire relationship has at least 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "Following [16], we evaluate on three tasks: (1) In relationship phrase detection [22], the goal is to predict a \u201csubject-predicate-object\u201d phrase, where the localization of the entire relationship has at least 0.", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "First, we compare our model with state-of-the-art approaches, Visual Phrases [22], Joint CNN+R-CNN [25] and Lu et al.", "startOffset": 77, "endOffset": 81}, {"referenceID": 24, "context": "First, we compare our model with state-of-the-art approaches, Visual Phrases [22], Joint CNN+R-CNN [25] and Lu et al.", "startOffset": 99, "endOffset": 103}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Note that the latter two methods use R-CNN [5] to extract object proposals.", "startOffset": 43, "endOffset": 46}, {"referenceID": 15, "context": "Their results on VRD are reported in [16], and we also experiment their methods on the Visual Genome dataset.", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "V only [16] trains individual detectors for object and predicate categories separately, and then combines their confidences to generate a relationship prediction.", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "Furthermore, we train and compare with the following models: \u201cFaster R-CNN [20]\u201d directly detects each unique relationship or attribute type, following Visual Phrases [22].", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "Furthermore, we train and compare with the following models: \u201cFaster R-CNN [20]\u201d directly detects each unique relationship or attribute type, following Visual Phrases [22].", "startOffset": 167, "endOffset": 171}, {"referenceID": 19, "context": "\u201cFaster R-CNN V only [20]\u201d model is similar to Lu et al.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "V only [16], with the only difference being that Faster RCNN is used for object detection.", "startOffset": 7, "endOffset": 11}, {"referenceID": 24, "context": "\u201cJoint CNN+RPN [25]\u201d extracts proposals using the pre-trained RPN [20] model on VOC 2012 [3] and then performs the classification.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "\u201cJoint CNN+RPN [25]\u201d extracts proposals using the pre-trained RPN [20] model on VOC 2012 [3] and then performs the classification.", "startOffset": 66, "endOffset": 70}, {"referenceID": 2, "context": "\u201cJoint CNN+RPN [25]\u201d extracts proposals using the pre-trained RPN [20] model on VOC 2012 [3] and then performs the classification.", "startOffset": 89, "endOffset": 92}, {"referenceID": 19, "context": "\u201cJoint CNN+Trained RPN [20]\u201d trains a separate RPN model on our dataset to generate proposals.", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "Qualitative comparison between VRD [16] and our VRL.", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": ", Visual Phrases [22], Joint CNN+RCNN [25], Joint CNN+RPN [25], Faster R-CNN [20], Joint CNN+Trained RPN [20].", "startOffset": 17, "endOffset": 21}, {"referenceID": 24, "context": ", Visual Phrases [22], Joint CNN+RCNN [25], Joint CNN+RPN [25], Faster R-CNN [20], Joint CNN+Trained RPN [20].", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": ", Visual Phrases [22], Joint CNN+RCNN [25], Joint CNN+RPN [25], Faster R-CNN [20], Joint CNN+Trained RPN [20].", "startOffset": 58, "endOffset": 62}, {"referenceID": 19, "context": ", Visual Phrases [22], Joint CNN+RCNN [25], Joint CNN+RPN [25], Faster R-CNN [20], Joint CNN+Trained RPN [20].", "startOffset": 77, "endOffset": 81}, {"referenceID": 19, "context": ", Visual Phrases [22], Joint CNN+RCNN [25], Joint CNN+RPN [25], Faster R-CNN [20], Joint CNN+Trained RPN [20].", "startOffset": 105, "endOffset": 109}, {"referenceID": 15, "context": "V only [16], Faster R-CNN V only [20], Lu et al.", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "V only [16], Faster R-CNN V only [20], Lu et al.", "startOffset": 33, "endOffset": 37}, {"referenceID": 15, "context": "[16] and our VRL.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "In all cases, we obtain performance improvements using RPN network [20] over R-CNN [5] for proposal generation.", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "In all cases, we obtain performance improvements using RPN network [20] over R-CNN [5] for proposal generation.", "startOffset": 83, "endOffset": 86}, {"referenceID": 15, "context": "[16] leverage language priors to facilitate prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] uses semantic word embeddings to finetune the likelihood of a predicted relationship, while VRL follows a variationalstructured traversal scheme over a directed semantic action graph built from language priors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] achieve significantly better performance than other baselines, which demonstrates the necessity of language priors for relationship and attribute detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] in Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "\u201cRandom Walk\u201d only achieves slightly better results than \u201cJoint+Trained RPN [20]\u201d and performs much worse than the remaining variants, again demonstrating the benefit of sequential mining in RL.", "startOffset": 76, "endOffset": 80}, {"referenceID": 19, "context": "Joint CNN+Trained RPN [20] 2.", "startOffset": 22, "endOffset": 26}, {"referenceID": 1, "context": "(2) \u201cVRL w/ historical actions\u201d directly stores a historical action vector in the state [2].", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "Similar to [2], we experiment using spatial actions in the deep RL setting to sequentially extract object instances.", "startOffset": 11, "endOffset": 14}, {"referenceID": 7, "context": "3 are replaced with LSTM [8] layers, which have shown promising results in capturing long-term dependencies.", "startOffset": 25, "endOffset": 28}, {"referenceID": 15, "context": "[16] in the zeroshot learning setting (see Tables 1 and 2).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] uses word embeddings to project similar relationships onto unseen ones, while our VRL uses a large semantic action graph to learn similar relationships on shared graph nodes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] on both datasets.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Despite progress in visual perception tasks such as image classification and detection, computers still struggle to understand the interdependency of objects in the scene as a whole, e.g., relations between objects or their attributes. Existing methods often ignore global context cues capturing the interactions among different object instances, and can only recognize a handful of types by exhaustively training individual detectors for all possible relationships. To capture such global interdependency, we propose a deep Variation-structured Reinforcement Learning (VRL) framework to sequentially discover object relationships and attributes in the whole image. First, a directed semantic action graph is built using language priors to provide a rich and compact representation of semantic correlations between object categories, predicates, and attributes. Next, we use a variation-structured traversal over the action graph to construct a small, adaptive action set for each step based on the current state and historical actions. In particular, an ambiguity-aware object mining scheme is used to resolve semantic ambiguity among object categories that the object detector fails to distinguish. We then make sequential predictions using a deep RL framework, incorporating global context cues and semantic embeddings of previously extracted phrases in the state vector. Our experiments on the Visual Relationship Detection (VRD) dataset and the large-scale Visual Genome dataset validate the superiority of VRL, which can achieve significantly better detection results on datasets involving thousands of relationship and attribute types. We also demonstrate that VRL is able to predict unseen types embedded in our action graph by learning correlations on shared graph nodes.", "creator": "LaTeX with hyperref package"}}}