{"id": "1703.03073", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations", "abstract": "Deep convolutional neural network (CNN) inference requires significant amount of memory and computation, which limits its deployment on embedded devices. To alleviate these problems to some extent, prior research utilize low precision fixed-point numbers to represent the CNN weights and activations. However, the minimum required data precision of fixed-point weights varies across different networks and also across different layers of the same network. In this work, we propose using floating-point numbers for representing the weights and fixed-point numbers for representing the activations. We show that using floating-point representation for weights is more efficient than fixed-point representation for the same bit-width and demonstrate it on popular large-scale CNNs such as AlexNet, SqueezeNet, GoogLeNet and VGG-16. We also show that such a representation scheme enables compact hardware multiply-and-accumulate (MAC) unit design. Experimental results show that the proposed scheme reduces the weight storage by up to 36% and power consumption of the hardware multiplier by up to 50%.", "histories": [["v1", "Wed, 8 Mar 2017 23:49:20 GMT  (1608kb,D)", "http://arxiv.org/abs/1703.03073v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["liangzhen lai", "naveen suda", "vikas chandra"], "accepted": false, "id": "1703.03073"}, "pdf": {"name": "1703.03073.pdf", "metadata": {"source": "CRF", "title": "Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations", "authors": ["Liangzhen Lai", "Naveen Suda", "Vikas Chandra"], "emails": ["LIANGZHEN.LAI@ARM.COM", "NAVEEN.SUDA@ARM.COM", "VIKAS.CHANDRA@ARM.COM"], "sections": [{"heading": "1. Introduction", "text": "The main challenges of CNCun et al., are the computer-based complexity and model size that affect their use on end-user client devices, limiting them to cloud-based high-performance servers. For example, AlexNet (Krizhevsky et al., 2012), a popular CNN model, requires 1.45 billion operations per input image with 240MB weights. Many researchers have researched hardware accelerators for the use of CNC devices."}, {"heading": "2. Related Work", "text": "In fact, it is the case that it is a matter of a way in which people are able to determine for themselves what they want and what they want. (...) It is the case that people are able to decide whether they want to or not. (...) It is as if they want that they want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we want that we that we want that we want that we want that we want that we that we want that we want that we want that we want that we that we want that we that we want that we that we want that we want that we that we that we want that we that we want that we that we that we want that we that we want that we that we that we that we want that we that we that we that we that we that we want that we that we that we want that we that we that we that we that we want that we that we that we that we that we that we that we that we that we"}, {"heading": "3. Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Fixed-Point Number Representation", "text": "The fixed point representation is very similar to the representation of integers. The difference is that integers have a scaling factor of 1 and fixed points have a predefined scaling factor as strength of 2. Some examples of fixed point numbers are shown in Fig. 1. It is usually assumed that all fixed point representations have the same scaling factor throughout the calculation. In some scenarios, the calculation can be divided into different groups, e.g. for different CNN layers with fixed point numbers of different scaling factors. This is also called dynamic fixed point representation (Courbariaux et al., 2014)."}, {"heading": "3.2. Floating-point Number Representation", "text": "An example of a floating-point number representation is shown in Fig. 2. For floating-point representation, there are typically three parts: Sign, Mantissa, and Exponent. The sign bit determines whether the number is a positive or negative number, the Mantissa determines the significance portion and the exponent the scale of the value. Normally, there are some special encodings to represent some special numbers (e.g. 0, NaN, and + / - infinity), but for binary floating-point numbers, the Mantissa can assume an implicit bit, which is also adopted by the IEEE floating-point standard. This ensures that the value of the mantissa is always between 1 and 2, so that the leading bit 1 can be omitted to save memory space. However, such an implicit bit of the smallest representable number sets a limit for the significance and part. \"The exponent is typically presented as an unsigned integer with an inclination."}, {"heading": "3.3. Hardware Implementation of CNNs", "text": "CNNs typically consist of multiple folding layers interspersed with pooling, ReLU, and normalization layers, followed by fully connected layers. Folding and fully connected layers are the most computationally intensive and data-intensive layers respectively (Qiu et al., 2016). The calculation in these layers consists of multiplication and accumulation operations (MAC).The data path is illustrated in Fig. 3, where the input functions are multiplied by weights to obtain the intermediate data (i.e. subtotals).These subtotals are cumulated to generate the output functions. As the fixed-point tarithmetic is typically more efficient for hardware implementation, most hardware accelerators implement the MAC operations using fixed-point representation. The power / area distribution of the CNN hardware accelerator AC typically depends largely on the data flow architecture (e.g., in an area of E52% or E52% of the storage architecture)."}, {"heading": "4. Proposed Number Representation Scheme", "text": "Unlike most existing CNN implementations, we suggest using a combination of floating-point and fixed-point representation; the network weights are represented as floating-point numbers, while the input / output functions are represented as fixed-point numbers; the multiplier is implemented to include a floating-point number and a fixed-point number and to generate output, i.e. floating-point data, in fixed-point format; the floating-point data is in fixed-point format and can have a larger bit width than the input / output functions; the accumulation is the same as the fixed-point representation, which can have a higher bit width; and from a hardware perspective, it is more efficient to multiply using floating-point numbers and adding using fixed-point numbers; the multiplication operations have a fixed-point input, a floating-point implementation, and a fixed-point output."}, {"heading": "5. Algorithmic Perspective", "text": "In this section, we examine and explain why the proposed numerical representation scheme is better from an algorithmic point of view. Section 5.1 shows that different CNNs may have inconsistent fixed point bit width requirements for the representation of weights. Section 5.2 examines this inconsistency by analyzing CNN weight distribution and properties. Section 5.3 shows that the representation range is the main factor determining inference accuracy. Section 5.4 shows that floating point representation is more efficient and consistent for CNN weights."}, {"heading": "5.1. CNN Accuracy with Fixed-Point Weights", "text": "To make a fair comparison, we assume that there is always a character bit to represent negative values for both fixed-point and floating-point numbers. We will represent the bit width of the floating-point representation as m + e, with m representing the number of mantissabits and e representing the number of exponent bites. We apply the weight quantification to four popular CNN networks: AlexNet (Krizhevsky et al., 2012), SqueezeNet (Iandola et al., 2016), GoogLeNet (Szegedy et al., 2015) and VGG-16 (Simonyan & Zisserman, 2014). We evaluate network accuracy by quantifying for all constant and fully connected bit deviations. Activation is quantified on 16-bit fixed-point."}, {"heading": "5.2. Weight Distribution", "text": "There may be several reasons that cause the inconsistency in Fig. 4. Network depth is one of the possible reasons. Similar to the idea in (Lin et al., 2015), fixed point quantization can be modeled as quantization noise for each layer. Network accuracy can continue to decrease, i.e., accumulate more noise as network depth increases. The other possible reason is the number of MAC operations in each layer. Small quantization errors can accumulate over a large amount of MAC operations. The total number of MAC operations used to calculate an output for revolutionary and fully connected layers in AlexNet are (363, 1200, 2304, 1728, 1728, 9216, 4096).However, none of the above reasons can explain the first observation earlier, rather than the difference in gradual, accuracy change. To investigate this further, we plot the weight distribution of four different layers in fixed weights in AlexNet 5."}, {"heading": "5.3. Range vs. Precision", "text": "The results in Fig. 4 and the weight distributions in Fig. 6 show that the network can achieve almost full accuracy, e.g. with a 7-bit fixed point for AlexNet, even if most weights are hard to represent, i.e., only with 1 or 2 significant bits. This means that the display area, i.e. the ability to represent larger / smaller values, is more important than the display accuracy, i.e., the differentiation between close values. Since the display area and the display accuracy in the fixed point representation is difficult to decompose, we investigate this using the sliding point representation. For the sliding point representation, the Mantissa bit width controls the precision and the exponent bit representation. Figure 8 highlights some of the results of the network accuracy by means of sliding point representation with varying exponent-exponent-exponent-exponent-exponent-exponent-exponent-exponent-exponent-exponent-exponent."}, {"heading": "5.4. CNN Accuracy with Floating-Point Weights", "text": "The results in Fig. 8 show that a 4-bit exponent is appropriate and consistent in different networks. The next question for a floating-point representation is how much precision, i.e. how many mantissabits, is needed. Fig. 11 highlights some of the results of network accuracy using a floating-point representation with varying mantissabit width. The floating-point number has a 4-bit exponent and is with the implicit bit. Most networks have very high accuracy even with a 1-bit mantissa and achieve full accuracy with a 3-bit mantissabit. This is also consistent in different networks, further demonstrating that the inconsistency with the fixed-point representation as seen in Fig. 4 mainly results from the inconsistent requirements for representation and not from representation accuracy."}, {"heading": "6. Implementation Perspective", "text": "This section motivates the proposed numerical representation scheme from a hardware implementation perspective. Implementation considerations are discussed in Section 6.1. Hardware trade-offs are described in Section 6.2."}, {"heading": "6.1. Hardware Implementation Considerations", "text": "As described in Section 3.3, calculations in CNNs are typically implemented as MAC operations, and for the same 32-bit-wide operations, the hardware implementation of fixed-point arithmetic can be more efficient than floating-point arithmetic. This is one of the reasons why most previous work has focused on optimizing fixed-point representation.The comparison becomes less obvious when the bit width is smaller, especially when the number of exponent bits in floating-point representation is small. For example, as shown in Figure 14, a floating-point multiplier can be implemented with a multiplier (the bit width of the mantis) and a barrel shifter (the bit width of the exponent), which can be more efficient than multiplying two fixed-point numbers as the multiplier gets smaller and the shifter is easier."}, {"heading": "6.2. Hardware Trade-off Results", "text": "To confirm the assertion in Section 6.1, we implement the multiplier with different operand configurations using a commercial 16nm process technology and libraries, the results of which are highlighted in Fig. 15. Floating point configuration is presented as m + e, i.e., mantissabit width + exponent bit width. Here, we assume that the baseline is a multiplier with two 8-bit fixed point operands, i.e. 8 x 8. The area and performance indicators are all normalized with respect to the baseline. For the same bit width, the proposed scheme of combining fixed and floating point operands can reduce both area and performance, the reduction increases with less mantissabit and more exponent bits, as a shifter is more efficient than a multiplier. As an example, floating component operation with 4-bit and 4-bit multiplier can decrease by more than point, 8x4 and 8x4 respectively."}, {"heading": "7. Experimental Results", "text": "As discussed in Section 5.3, we perform the weight quantification based on Caffe (Jia et al., 2014). We refer to the representation as (m, e), where m is the bit width of the mantissa and e is the bit width of the exponent. e = 0 means fixed point representation. We evaluate network accuracy with different bit width settings. Table 1 highlights some results for AlexNet. Networking does not work (i.e. with close to 0 accuracy) when e is small, i.e. with a limited representation range. To achieve full accuracy, AlexNet needs 8 bits to represent the fixed point, i.e. (7, 0) with 1 character bit or 7 bits for floating point representation with (3, 3) configuration."}, {"heading": "8. Conclusion", "text": "In this thesis, we propose CNN inference implementation with floating-point weights and fixed-point activations. We provide the motivation for the proposed numerical representation scheme from both an algorithmic and a hardware implementation perspective. The proposed scheme can reduce weight storage by up to 36% and multiplier power by up to 50%."}], "references": [{"title": "Origami: A convolutional network accelerator", "author": ["Cavigelli", "Lukas", "Gschwend", "David", "Mayer", "Christoph", "Willi", "Samuel", "Muheim", "Beat", "Benini", "Luca"], "venue": "In Proceedings of the 25th edition on Great Lakes Symposium on VLSI,", "citeRegEx": "Cavigelli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cavigelli et al\\.", "year": 2015}, {"title": "Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks", "author": ["Chen", "Yu-Hsin", "Krishna", "Tushar", "Emer", "Joel S", "Sze", "Vivienne"], "venue": "IEEE Journal of Solid-State Circuits,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "Training deep neural networks with low precision multiplications", "author": ["Courbariaux", "Matthieu", "David", "Jean-Pierre", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Reduced-precision memory value approximation for deep learning", "author": ["Deng", "Zhaoxia", "Xu", "Cong", "Cai", "Qiong", "Faraboschi", "Paolo"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2015}, {"title": "Deep learning with limited numerical precision", "author": ["Gupta", "Suyog", "Agrawal", "Ankur", "Gopalakrishnan", "Kailash", "Narayanan", "Pritish"], "venue": "CoRR, abs/1502.02551,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Ristretto: Hardware-oriented approximation of convolutional neural networks", "author": ["Gysel", "Philipp"], "venue": "arXiv preprint arXiv:1605.06402,", "citeRegEx": "Gysel and Philipp.,? \\Q2016\\E", "shortCiteRegEx": "Gysel and Philipp.", "year": 2016}, {"title": "Hardware-oriented approximation of convolutional neural networks", "author": ["Gysel", "Philipp", "Motamedi", "Mohammad", "Ghiasi", "Soheil"], "venue": "arXiv preprint arXiv:1604.03168,", "citeRegEx": "Gysel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gysel et al\\.", "year": 2016}, {"title": "A vlsi architecture for highperformance, low-cost, on-chip learning", "author": ["Hammerstrom", "Dan"], "venue": "In Neural Networks,", "citeRegEx": "Hammerstrom and Dan.,? \\Q1990\\E", "shortCiteRegEx": "Hammerstrom and Dan.", "year": 1990}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and\u00a1 0.5 mb model size", "author": ["Iandola", "Forrest N", "Han", "Song", "Moskewicz", "Matthew W", "Ashraf", "Khalid", "Dally", "William J", "Keutzer", "Kurt"], "venue": "arXiv preprint arXiv:1602.07360,", "citeRegEx": "Iandola et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Iandola et al\\.", "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Reduced-precision strategies for bounded memory in deep neural nets", "author": ["Judd", "Patrick", "Albericio", "Jorge", "Hetherington", "Tayler", "Aamodt", "Tor", "Jerger", "Natalie Enright", "Urtasun", "Raquel", "Moshovos", "Andreas"], "venue": "arXiv preprint arXiv:1511.05236,", "citeRegEx": "Judd et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Judd et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fixed point quantization of deep convolutional networks", "author": ["Lin", "Darryl D", "Talathi", "Sachin S", "Annapureddy", "V Sreekanth"], "venue": "arXiv preprint arXiv:1511.06393,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "An energy-efficient precision-scalable convnet processor in a 40-nm cmos", "author": ["Moons", "Bert", "Verhelst", "Marian"], "venue": "IEEE Journal of Solid-State Circuits,", "citeRegEx": "Moons et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moons et al\\.", "year": 2016}, {"title": "Going deeper with embedded fpga platform for convolutional neural network", "author": ["Qiu", "Jiantao", "Wang", "Jie", "Yao", "Song", "Guo", "Kaiyuan", "Li", "Boxun", "Zhou", "Erjin", "Yu", "Jincheng", "Tang", "Tianqi", "Xu", "Ningyi", "Sen"], "venue": "In Proceedings of the 2016 ACM/SIGDA International Sympo-", "citeRegEx": "Qiu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2016}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Rastegari", "Mohammad", "Ordonez", "Vicente", "Redmon", "Joseph", "Farhadi", "Ali"], "venue": null, "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Dnpu: An 8.1tops/w reconfigurable cnn-rnn processor for general-purpose deep neural networks", "author": ["Shin", "Dongjoo", "Lee", "Jinmook", "Jinsu", "Yoo", "HoiJun"], "venue": "In SolidState Circuits Conference (ISSCC),", "citeRegEx": "Shin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Shin et al\\.", "year": 2017}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Throughput-optimized opencl-based fpga accelerator for large-scale convolutional neural networks", "author": ["Suda", "Naveen", "Chandra", "Vikas", "Dasika", "Ganesh", "Mohanty", "Abinash", "Ma", "Yufei", "Vrudhula", "Sarma", "Seo", "Jae-sun", "Cao", "Yu"], "venue": "In Proceedings of the 2016 ACM/SIGDA International", "citeRegEx": "Suda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Suda et al\\.", "year": 2016}, {"title": "Accelerating deep convolutional networks using lowprecision and sparsity", "author": ["Venkatesh", "Ganesh", "Nurvitadhi", "Eriko", "Marr", "Debbie"], "venue": null, "citeRegEx": "Venkatesh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Venkatesh et al\\.", "year": 2016}, {"title": "The microsoft 2016 conversational speech recognition system", "author": ["W Xiong", "J Droppo", "X Huang", "F Seide", "M Seltzer", "A Stolcke", "D Yu", "G. Zweig"], "venue": "arXiv preprint arXiv:1609.03528,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["Zhou", "Shuchang", "Ni", "Zekun", "Xinyu", "Wen", "He", "Wu", "Yuxin", "Zou", "Yuheng"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 22, "context": "Deep learning algorithms have achieved or surpassed human-levels of perception in some applications (He et al., 2016; Xiong et al., 2016), enabling them to be deployed in the real-world applications.", "startOffset": 100, "endOffset": 137}, {"referenceID": 13, "context": "For example, AlexNet (Krizhevsky et al., 2012), a popular CNN model, requires 1.", "startOffset": 21, "endOffset": 46}, {"referenceID": 20, "context": "45 billion operations per input image with 240MB weights (Suda et al., 2016).", "startOffset": 57, "endOffset": 76}, {"referenceID": 16, "context": "Many researchers have explored hardware accelerators for CNNs to enable deployment of CNNs in embedded devices and demonstrated good performance at low power consumption (Qiu et al., 2016; Shin et al., 2017).", "startOffset": 170, "endOffset": 207}, {"referenceID": 18, "context": "Many researchers have explored hardware accelerators for CNNs to enable deployment of CNNs in embedded devices and demonstrated good performance at low power consumption (Qiu et al., 2016; Shin et al., 2017).", "startOffset": 170, "endOffset": 207}, {"referenceID": 12, "context": "Fixed-point representation with shorter bit-width for CNN weights and activations has been widely explored (Judd et al., 2015; Gupta et al., 2015; Gysel et al., 2016; Lin et al., 2015), which significantly reduces the storage requirements, memory bandwidth and power consumption without sacrificing accuracy.", "startOffset": 107, "endOffset": 184}, {"referenceID": 5, "context": "Fixed-point representation with shorter bit-width for CNN weights and activations has been widely explored (Judd et al., 2015; Gupta et al., 2015; Gysel et al., 2016; Lin et al., 2015), which significantly reduces the storage requirements, memory bandwidth and power consumption without sacrificing accuracy.", "startOffset": 107, "endOffset": 184}, {"referenceID": 7, "context": "Fixed-point representation with shorter bit-width for CNN weights and activations has been widely explored (Judd et al., 2015; Gupta et al., 2015; Gysel et al., 2016; Lin et al., 2015), which significantly reduces the storage requirements, memory bandwidth and power consumption without sacrificing accuracy.", "startOffset": 107, "endOffset": 184}, {"referenceID": 14, "context": "Fixed-point representation with shorter bit-width for CNN weights and activations has been widely explored (Judd et al., 2015; Gupta et al., 2015; Gysel et al., 2016; Lin et al., 2015), which significantly reduces the storage requirements, memory bandwidth and power consumption without sacrificing accuracy.", "startOffset": 107, "endOffset": 184}, {"referenceID": 13, "context": "From the algorithmic perspective, using popular largescale CNNs such as AlexNet (Krizhevsky et al., 2012), SqueezeNet (Iandola et al.", "startOffset": 80, "endOffset": 105}, {"referenceID": 10, "context": ", 2012), SqueezeNet (Iandola et al., 2016), GoogLeNet (Szegedy et al.", "startOffset": 20, "endOffset": 42}, {"referenceID": 5, "context": "(Gupta et al., 2015) present the impacts of different fixed-point rounding schemes on the accuracy.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "(Judd et al., 2015) demonstrate that the minimum required data precision not only varies across different networks, but also across different layers of the same network.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "(Lin et al., 2015) present a fixed-point quantization methodology to identify the optimal data precision for all layers of a network.", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "(Gysel et al., 2016) present a framework Ristretto for fixed-point quantization and re-training of CNNs based on Caffe (Jia et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": ", 2016) present a framework Ristretto for fixed-point quantization and re-training of CNNs based on Caffe (Jia et al., 2014).", "startOffset": 106, "endOffset": 124}, {"referenceID": 3, "context": "More recently, in (Courbariaux et al., 2014), the authors train neural networks with floating-point, fixed-point and dynamic fixedpoint formats and demonstrate that fixed-point weights are sufficient for training.", "startOffset": 18, "endOffset": 44}, {"referenceID": 5, "context": "(Gupta et al., 2015) demonstrate network training with 16-bit fixed-point weights using stochastic rounding scheme.", "startOffset": 0, "endOffset": 20}, {"referenceID": 9, "context": "(Han et al., 2015) propose a combination of network pruning, weight quantization during training and compression based on Huffman coding to reduce the VGG-16 network size by 49X.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "In (Deng et al., 2015), the authors propose to store both 8-bit quantized floating-point weights and 32-bit full precision weights.", "startOffset": 3, "endOffset": 22}, {"referenceID": 21, "context": "The continuous research effort to reduce the data precision has led to many interesting demonstrations with 2-bit weights (Venkatesh et al., 2016) and even binary weights/activations (Courbariaux & Bengio, 2016; Rastegari et al.", "startOffset": 122, "endOffset": 146}, {"referenceID": 17, "context": ", 2016) and even binary weights/activations (Courbariaux & Bengio, 2016; Rastegari et al., 2016).", "startOffset": 44, "endOffset": 96}, {"referenceID": 23, "context": "(Zhou et al., 2016) demonstrate AlexNet training with 1-bit weights, 2-bit activations and 6-bit gradients.", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "It further shows that floating-point representation of weights achieves better range/accuracy trade-off compared for the fixed-point representation of same number of bits and we empirically demonstrate it on state of the art CNNs such as AlexNet (Krizhevsky et al., 2012), VGG-16 (Simonyan & Zisserman, 2014), GoogLeNet (Szegedy et al.", "startOffset": 246, "endOffset": 271}, {"referenceID": 10, "context": ", 2015) and SqueezeNet (Iandola et al., 2016).", "startOffset": 23, "endOffset": 45}, {"referenceID": 3, "context": "This is also referred as dynamic fixed-point representation (Courbariaux et al., 2014).", "startOffset": 60, "endOffset": 86}, {"referenceID": 16, "context": "Convolution and fullyconnected layers are the most compute and data intensive layers respectively (Qiu et al., 2016).", "startOffset": 98, "endOffset": 116}, {"referenceID": 1, "context": "For example, in Eyeriss (Chen et al., 2016), in each processing element (PE), MAC and memory account for about 9% and 52% area respectively.", "startOffset": 24, "endOffset": 43}, {"referenceID": 0, "context": "For Origami (Cavigelli et al., 2015), MAC and memory account for about 32% and 34% area respectively.", "startOffset": 12, "endOffset": 36}, {"referenceID": 11, "context": "To evaluate different number representation schemes, we implement weight quantization based on Caffe (Jia et al., 2014) framework.", "startOffset": 101, "endOffset": 119}, {"referenceID": 13, "context": "We apply the weight quantization on four popular CNN networks: AlexNet (Krizhevsky et al., 2012), SqueezeNet (Iandola et al.", "startOffset": 71, "endOffset": 96}, {"referenceID": 10, "context": ", 2012), SqueezeNet (Iandola et al., 2016), GoogLeNet (Szegedy et al.", "startOffset": 20, "endOffset": 42}, {"referenceID": 7, "context": "This is similar to the dynamic fixedpoint quantization (Moons & Verhelst, 2016; Gysel et al., 2016; Courbariaux et al., 2014).", "startOffset": 55, "endOffset": 125}, {"referenceID": 3, "context": "This is similar to the dynamic fixedpoint quantization (Moons & Verhelst, 2016; Gysel et al., 2016; Courbariaux et al., 2014).", "startOffset": 55, "endOffset": 125}, {"referenceID": 14, "context": "Similar to the idea in (Lin et al., 2015), the fixed-point quantization can be modeled as quantization noise for each layer.", "startOffset": 23, "endOffset": 41}, {"referenceID": 9, "context": "The distribution is concentrated at the center, which is also the motivation for Huffman encoding of the weights proposed in (Han et al., 2015).", "startOffset": 125, "endOffset": 143}, {"referenceID": 5, "context": "This also explains why stochastic rounding works better than round-to-nearest reported in (Gupta et al., 2015).", "startOffset": 90, "endOffset": 110}, {"referenceID": 11, "context": "3, we perform the weight quantization based on Caffe (Jia et al., 2014).", "startOffset": 53, "endOffset": 71}], "year": 2017, "abstractText": "Deep convolutional neural network (CNN) inference requires significant amount of memory and computation, which limits its deployment on embedded devices. To alleviate these problems to some extent, prior research utilize low precision fixed-point numbers to represent the CNN weights and activations. However, the minimum required data precision of fixed-point weights varies across different networks and also across different layers of the same network. In this work, we propose using floating-point numbers for representing the weights and fixed-point numbers for representing the activations. We show that using floating-point representation for weights is more efficient than fixed-point representation for the same bit-width and demonstrate it on popular large-scale CNNs such as AlexNet, SqueezeNet, GoogLeNet and VGG16. We also show that such a representation scheme enables compact hardware multiply-andaccumulate (MAC) unit design. Experimental results show that the proposed scheme reduces the weight storage by up to 36% and power consumption of the hardware multiplier by up to 50%.", "creator": "LaTeX with hyperref package"}}}