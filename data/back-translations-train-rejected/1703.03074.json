{"id": "1703.03074", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Learning the Probabilistic Structure of Cumulative Phenomena with Suppes-Bayes Causal Networks", "abstract": "One of the critical issues when adopting Bayesian networks (BNs) to model dependencies among random variables is to \"learn\" their structure, given the huge search space of possible solutions, i.e., all the possible direct acyclic graphs. This is a well-known NP-hard problem, which is also complicated by known pitfalls such as the issue of I-equivalence among different structures. In this work we restrict the investigations on BN structure learning to a specific class of networks, i.e., those representing the dynamics of phenomena characterized by the monotonic accumulation of events. Such phenomena allow to set specific structural constraints based on Suppes' theory of probabilistic causation and, accordingly, to define constrained BNs, named Suppes-Bayes Causal Networks (SBCNs). We here investigate the structure learning of SBCNs via extensive simulations with various state-of-the-art search strategies, such as canonical local search techniques and Genetic Algorithms. Among the main results we show that Suppes' constraints deeply simplify the learning task, by reducing the solution search space and providing a temporal ordering on the variables.", "histories": [["v1", "Wed, 8 Mar 2017 23:50:19 GMT  (640kb,D)", "http://arxiv.org/abs/1703.03074v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["daniele ramazzotti", "marco s nobile", "marco antoniotti", "alex graudenzi"], "accepted": false, "id": "1703.03074"}, "pdf": {"name": "1703.03074.pdf", "metadata": {"source": "CRF", "title": "Learning the Probabilistic Structure of Cumulative Phenomena with Suppes-Bayes Causal Networks", "authors": ["Daniele Ramazzotti", "Marco S. Nobile"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is the case that most people who are able to determine themselves are not able to determine themselves, and that they are not able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are not able to determine themselves. (...) Most of them are not able to determine themselves. (...) Most of them are not able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...) Most of them are able to determine themselves. (...)"}, {"heading": "II. BACKGROUND", "text": "In this section, we provide an introduction to Bayesian networks along with a review of some state-of-the-art methods to address the problem of learning their structures through a series of observations D about the variables described in the network."}, {"heading": "A. Bayesian graphical models", "text": "A Bayesian network is a statistical graphical model that concisely represents a common distribution over n random variables and represents them in a direct acyclic graph G = (V, E) over n nodes V in relation to the variables and their relations E (arcs in the DAG). Given the structure of a BN, the complete common distribution of n variables can be written to each variable as a product of conditional distributions. In fact, an edge between two nodes, e.g. A and B, denotes statistical dependence, i.e. P (A) 6 = P (A) 6 = P (B) P (B) B), regardless of which other variables we condition, that is, for each other variable it states that P (A) 6 = P (B | C) [1].In such a DAG, the variables in relation to any node X are determined."}, {"heading": "B. Approaches to learn the structure of a BN", "text": "In fact, it is such that most of them are in a position to move into another world, in which they are able to move, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they"}, {"heading": "III. METHOD", "text": "In this section, we will present the basics of our framework and, in particular, define the main features of the Suppes-Bayes causal networks and some heuristic strategies for adapting probabilities. Without losing generality, we will henceforth consider a simplified formulation of the problem of learning the structure of BNs, where all the variables shown in the diagram are Bernoulli random variables, i.e. their support is (0, 1). All the conclusions derived from these constellations can also be applied directly to the general case in which the nodes in the BN describe genetic random variables [1]. Specifically, we consider as input for our learning task a data set D of n Bernoulli variables and m cross-sectional samples. We assume that the value 1 indicates that a particular variable has been observed in the sample and 0 that the variable has not been observed."}, {"heading": "A. Suppes-Bayes causal networks", "text": "[9], Suppes introduced the concept of prima facie causality in the context of general causality [10], however, this relationship between a cause u and its effect v appears to be when the following two conditions are applicable: \u2022 (i) temporal priority (TP): any cause that occurs before its effect; \u2022 (ii) probability increase (PR): the presence of the cause increases the probability of observing its effectiveness. (u), P (v) < 1, the event u is designated as prima facie cause of v when it occurs and the probability of u and tv, under the mild assumptions that 0 < P (u) < P (v) < 1, the event is designated as prima facie cause of v and increases the probability of u, i.e. (TP) tu < tv (PR) P (v | u) > P (1) The concept of prima facie causality has known limitations in the general context of causality."}, {"heading": "B. Optimization and Evolutionary Computation", "text": "In fact, most of them will be able to play by the rules they have had in the past, and they will be able to play by the rules they have had in the past."}, {"heading": "IV. RESULTS", "text": "We will now discuss the results of a large number of experiments with single roots: 3) precursors of most nodes, all confluences are governed by logical confluences, a unique root; 4) conjunctive DAGs with multiple roots; 3) single root disjunctive DAGs: 3 precursors of most nodes, all confluences are governed by logical confluences, a unique root; 4) multi-root conjunctive DAGs: as many possible roots; 5) single root disjunctive DAGs: 3 precursors at each node, all confluences are governed by logical confluences, a unique root; 4) multi-root conjunctive DAGs:"}, {"heading": "V. CONCLUSION", "text": "In this paper, we examined the structural learning of Bayesian networks, which aim to model phenomena caused by the monotonous accumulation of events over time. To this end, we used a subclass of restricted Bayesian networks called Suppes-Bayes Causal Networks, which include structural constraints based on Suppes \"theory of probable causality. Although the problem of learning the structure of a Bayesian network is known to be insoluble, such constraints allow the search space of possible solutions to be narrowed, resulting in a huge reduction in the number of valid networks that must be taken into account, thereby remarkably limiting the complexity of the problem. Here, we discussed the theoretical implications of the follow-up process in the various steps, including by comparing various state-of-the-art algorithmic approaches and regulatory methods. Finally, we provided an in-depth study of realistically simulated data on the effects of each search with sound algorithmic guidelines that can help to improve the design of the algorithmic algorithms and some of the algorithms that provide."}], "references": [{"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Learning bayesian networks is npcomplete", "author": ["D.M. Chickering"], "venue": "Learning from data. Springer, 1996, pp. 121\u2013130.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Largesample learning of bayesian networks is np-hard", "author": ["D.M. Chickering", "D. Heckerman", "C. Meek"], "venue": "Journal of Machine Learning Research, vol. 5, no. Oct, pp. 1287\u2013 1330, 2004.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Structure learning of bayesian networks by genetic algorithms", "author": ["P. Larranaga", "M. Poza", "Y. Yurramendi", "R.H. Murga", "C.M.H. Kuijpers"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 18, no. 9, pp. 912\u2013926, 1996.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Ordering-based search: A simple and effective algorithm for learning bayesian networks", "author": ["M. Teyssier", "D. Koller"], "venue": "arXiv preprint arXiv:1207.1429, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Catastrophic cascade of failures in interdependent networks", "author": ["S.V. Buldyrev", "R. Parshani", "G. Paul", "H.E. Stanley", "S. Havlin"], "venue": "Nature, vol. 464, no. 7291, pp. 1025\u20131028, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "A probabilistic theory of causality", "author": ["P. Suppes"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1970}, {"title": "Probabilistic causation", "author": ["C. Hitchcock"], "venue": "Stanford encyclopedia of philosophy, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Exposing the probabilistic causal structure of discrimination", "author": ["F. Bonchi", "S. Hajian", "B. Mishra", "D. Ramazzotti"], "venue": "arXiv preprint arXiv:1510.00552, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Inferring tree causal models of cancer progression with probability raising", "author": ["L.O. Loohuis", "G. Caravagna", "A. Graudenzi", "D. Ramazzotti", "G. Mauri", "M. Antoniotti", "B. Mishra"], "venue": "PloS one, vol. 9, no. 10, p. e108358, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Capri: efficient inference of cancer progression models from cross-sectional data", "author": ["D. Ramazzotti", "G. Caravagna", "L.O. Loohuis", "A. Graudenzi", "I. Korsunsky", "G. Mauri", "M. Antoniotti", "B. Mishra"], "venue": "Bioinformatics, vol. 31, no. 18, pp. 3016\u20133026, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Algorithmic methods to infer the evolutionary trajectories in cancer progression", "author": ["G. Caravagna", "A. Graudenzi", "D. Ramazzotti", "R. Sanz- Pamplona", "L. De Sano", "G. Mauri", "V. Moreno", "M. Antoniotti", "B. Mishra"], "venue": "PNAS, in press, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Theory refinement on bayesian networks", "author": ["W. Buntine"], "venue": "Proceedings of the Seventh conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers Inc., 1991, pp. 52\u201360.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1991}, {"title": "A bayesian method for the induction of probabilistic networks from data", "author": ["G.F. Cooper", "E. Herskovits"], "venue": "Machine  learning, vol. 9, no. 4, pp. 309\u2013347, 1992.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Parallel implementation of efficient search schemes for the inference of cancer progression models", "author": ["D. Ramazzotti", "M.S. Nobile", "P. Cazzaniga", "G. Mauri", "M. Antoniotti"], "venue": "IEEE International Conference on Computational Intelligence in Bioinformatics and Computational Biology. IEEE, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Equivalence and synthesis of causal models", "author": ["T.V.J. Judea Pearl"], "venue": "Proceedings of Sixth Conference on Uncertainty in Artificial Intelligence, 1991, pp. 220\u2013227.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1991}, {"title": "Inference of cancer progression models with biological noise", "author": ["I. Korsunsky", "D. Ramazzotti", "G. Caravagna", "B. Mishra"], "venue": "arXiv preprint arXiv:1408.6032, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "A model of selective advantage for the efficient inference of cancer clonal evolution", "author": ["D. Ramazzotti"], "venue": "arXiv preprint arXiv:1602.07614, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "The algorithm design manual: Text", "author": ["S.S. Skiena"], "venue": "Springer Science & Business Media,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Tabu search-part i", "author": ["F. Glover"], "venue": "ORSA Journal on computing, vol. 1, no. 3, pp. 190\u2013206, 1989.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1989}, {"title": "Optimization by simulated annealing: Quantitative studies", "author": ["S. Kirkpatrick"], "venue": "Journal of statistical physics, vol. 34, no. 5-6, pp. 975\u2013986, 1984.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1984}, {"title": "Genetic algorithms in search, optimization, and machine learning", "author": ["D.E. Golberg"], "venue": "Addion wesley, vol. 1989, p. 102, 1989.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1989}, {"title": "Holland, adaptation in natural and artificial systems", "author": ["H. John"], "venue": "1992.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1992}, {"title": "Stat-  14 nikov, \u201cAlgorithms for large scale markov blanket discovery.", "author": ["I. Tsamardinos", "C.F. Aliferis", "A.R. Statnikov"], "venue": "in FLAIRS conference,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "Scoring functions for learning bayesian networks", "author": ["A.M. Carvalho"], "venue": "Inesc-id Tec. Rep, 2009.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Conjunctive bayesian networks", "author": ["N. Beerenwinkel", "N. Eriksson", "B. Sturmfels"], "venue": "Bernoulli, pp. 893\u2013909, 2007.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Quantifying cancer progression with conjunctive bayesian networks", "author": ["M. Gerstung", "M. Baudis", "H. Moch", "N. Beerenwinkel"], "venue": "Bioinformatics, vol. 25, no. 21, pp. 2809\u20132815, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning oncogenetic networks by reducing to mixed integer linear programming", "author": ["H.S. Farahani", "J. Lagergren"], "venue": "PloS one, vol. 8, no. 6, p. e65773, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Ant colony optimization for learning bayesian networks", "author": ["L.M. De Campos", "J.M. Fernandez-Luna", "J.A. G\u00e1mez", "J.M. Puerta"], "venue": "International Journal of Approximate Reasoning, vol. 31, no. 3, pp. 291\u2013311, 2002.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning bayesian networks by ant colony optimisation: searching in two different spaces", "author": ["L.M. d. Campos", "J.A. G\u00e1mez Mart\u0131\u0301n", "J.M. Puerta Castell\u00f3n"], "venue": "Mathware & soft computing. 2002 Vol. 9 N\u00fam. 2 [-3], 2002.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence", "author": ["J.H. Holland"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1975}, {"title": "Selective pressure in evolutionary algorithms: A characterization of selection mechanisms", "author": ["T. Back"], "venue": "Evolutionary Computation, 1994. IEEE World Congress on Computational Intelligence., Proceedings of the First IEEE Conference on. IEEE, 1994, pp. 57\u201362.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1994}, {"title": "Learning bayesian networks with the bnlearn r package", "author": ["M. Scutari"], "venue": "arXiv preprint arXiv:0908.3817, 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "inspyred: Bio-inspired algorithms in python", "author": ["A. Garrett"], "venue": "URL https://pypi. python. org/pypi/inspyred, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploring network structure, dynamics, and function using networkx", "author": ["D.A. Schult", "P. Swart"], "venue": "Proceedings of the 7th Python in Science Conferences (SciPy 2008), vol. 2008, 2008, pp. 11\u201316.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "A guide to numpy, vol. 1", "author": ["T. Oliphant"], "venue": "Spanish Fork: Trelgol Publishing, 2006.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Information theory and an extension of the maximum likelihood principle", "author": ["H. Akaike"], "venue": "Selected Papers of Hirotugu Akaike. Springer, 1998, pp. 199\u2013213.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1998}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The annals of statistics, vol. 6, no. 2, pp. 461\u2013464, 1978.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1978}, {"title": "Learning bayesian networks: The combination of knowledge and statistical data", "author": ["D. Heckerman", "D. Geiger", "D.M. Chickering"], "venue": "Machine learning, vol. 20, no. 3, pp. 197\u2013 243, 1995.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1995}, {"title": "A bayesian method for constructing bayesian belief networks from databases", "author": ["G.F. Cooper", "E. Herskovits"], "venue": "Proceedings of the Seventh conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers Inc., 1991, pp. 86\u201394.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1991}, {"title": "Mathematical contributions to the theory of evolution.\u2013on a form of spurious correlation which may arise when indices are used in the measurement of organs", "author": ["K. Pearson"], "venue": "Proceedings of the royal society of london, vol. 60, no. 359-367, pp. 489\u2013498, 1896.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1896}], "referenceMentions": [{"referenceID": 0, "context": "BAYESIAN networks (BNs) are probabilistic graphical models representing the relations of conditional dependence among random variables, encoded in directed acyclic graphs (DAGs) [1].", "startOffset": 178, "endOffset": 181}, {"referenceID": 0, "context": "the last decades, BNs have been effectively applied in several different fields and disciplines, such as (but not limited to) diagnostics and predictive analytics [1].", "startOffset": 163, "endOffset": 166}, {"referenceID": 0, "context": ", all the possible DAGs) is evaluated via a score based on a likelihood function [1].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "Regardless of the approach, the main difficulty in this learning problem is the huge number of valid solutions in the search space, namely, all the possible DAGs, which makes this task a known NP hard problem, even when constraining each node to have at most two parents [3], [4].", "startOffset": 271, "endOffset": 274}, {"referenceID": 2, "context": "Regardless of the approach, the main difficulty in this learning problem is the huge number of valid solutions in the search space, namely, all the possible DAGs, which makes this task a known NP hard problem, even when constraining each node to have at most two parents [3], [4].", "startOffset": 276, "endOffset": 279}, {"referenceID": 0, "context": "state-of-the-art techniques solve this task by means of meta-heuristics [1], [5], [6].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "state-of-the-art techniques solve this task by means of meta-heuristics [1], [5], [6].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "state-of-the-art techniques solve this task by means of meta-heuristics [1], [5], [6].", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "different structures can encode the same set of conditional independence properties [1].", "startOffset": 84, "endOffset": 87}, {"referenceID": 0, "context": "emerging from their induced distributions rather than the structure itself [1].", "startOffset": 75, "endOffset": 78}, {"referenceID": 5, "context": "In these scenarios, different configurations may lead to failure, but some of them are more likely than others and, hence, can be modeled probabilistically [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 6, "context": "The two particular conditions mentioned above represent the basis of the notion of probabilistic causation by Patrick Suppes [9], [10], and allow us to define a set of structural constraints to the BNs to be inferred, which, accordingly, have been dubbed as Suppes-Bayes Causal Networks (SBCNs) in previous works [11], [7].", "startOffset": 125, "endOffset": 128}, {"referenceID": 7, "context": "The two particular conditions mentioned above represent the basis of the notion of probabilistic causation by Patrick Suppes [9], [10], and allow us to define a set of structural constraints to the BNs to be inferred, which, accordingly, have been dubbed as Suppes-Bayes Causal Networks (SBCNs) in previous works [11], [7].", "startOffset": 130, "endOffset": 134}, {"referenceID": 8, "context": "The two particular conditions mentioned above represent the basis of the notion of probabilistic causation by Patrick Suppes [9], [10], and allow us to define a set of structural constraints to the BNs to be inferred, which, accordingly, have been dubbed as Suppes-Bayes Causal Networks (SBCNs) in previous works [11], [7].", "startOffset": 313, "endOffset": 317}, {"referenceID": 9, "context": "SBCNs have been already applied in a number of different fields, ranging from cancer progression inference [12], [13], [14] to social discrimination discovery [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "SBCNs have been already applied in a number of different fields, ranging from cancer progression inference [12], [13], [14] to social discrimination discovery [11].", "startOffset": 113, "endOffset": 117}, {"referenceID": 11, "context": "SBCNs have been already applied in a number of different fields, ranging from cancer progression inference [12], [13], [14] to social discrimination discovery [11].", "startOffset": 119, "endOffset": 123}, {"referenceID": 8, "context": "SBCNs have been already applied in a number of different fields, ranging from cancer progression inference [12], [13], [14] to social discrimination discovery [11].", "startOffset": 159, "endOffset": 163}, {"referenceID": 12, "context": "events, poset in the terminology of Bayesian networks) of a BN, finding the optimal solution that is consistent with the ordering can be accomplished in time O(n), where n is the number of variables and k the bounded in-degree of a node [15], [16].", "startOffset": 237, "endOffset": 241}, {"referenceID": 13, "context": "events, poset in the terminology of Bayesian networks) of a BN, finding the optimal solution that is consistent with the ordering can be accomplished in time O(n), where n is the number of variables and k the bounded in-degree of a node [15], [16].", "startOffset": 243, "endOffset": 247}, {"referenceID": 4, "context": "Thus, the search in the space of orderings can be performed way more efficiently than the search in the space of structures, as the search space is much smaller, the branching factor is lower and acyclicity checks are not necessary [6], [17].", "startOffset": 232, "endOffset": 235}, {"referenceID": 14, "context": "Thus, the search in the space of orderings can be performed way more efficiently than the search in the space of structures, as the search space is much smaller, the branching factor is lower and acyclicity checks are not necessary [6], [17].", "startOffset": 237, "endOffset": 241}, {"referenceID": 0, "context": ", P(A \u2227B) 6= P(A)P(B), regardless of which any other variables we condition on, that is, for any other set of variables C it holds that P(A \u2227B | C) 6= P(A | C)P(B | C) [1].", "startOffset": 168, "endOffset": 171}, {"referenceID": 0, "context": "It can be proven that, for any BN, the Markov blanket consists of a node\u2019s parents, its children and the parents of the children [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 15, "context": ", A\u2192 B \u2190 C) [18].", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "BNs have an interesting relation to canonical boolean logical operators \u2227, \u2228 and \u2295 and formulas over variables [19], [7].", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "that we will hear music even without power or headphone (perhaps we are next to a concert and overhear that music) [19], [20].", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "that we will hear music even without power or headphone (perhaps we are next to a concert and overhear that music) [19], [20].", "startOffset": 121, "endOffset": 125}, {"referenceID": 1, "context": "Since both approaches lead to intractability (NP -hardness) [3], [4], computing and verifying an exact solution is impractical.", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "Since both approaches lead to intractability (NP -hardness) [3], [4], computing and verifying an exact solution is impractical.", "startOffset": 65, "endOffset": 68}, {"referenceID": 18, "context": "For this reason, heuristic methods like Hill Climbing [21], Tabu Search [22], Simulated Annealing [23] and Genetic Algorithms [24], [25] are generally employed.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "For this reason, heuristic methods like Hill Climbing [21], Tabu Search [22], Simulated Annealing [23] and Genetic Algorithms [24], [25] are generally employed.", "startOffset": 72, "endOffset": 76}, {"referenceID": 20, "context": "For this reason, heuristic methods like Hill Climbing [21], Tabu Search [22], Simulated Annealing [23] and Genetic Algorithms [24], [25] are generally employed.", "startOffset": 98, "endOffset": 102}, {"referenceID": 21, "context": "For this reason, heuristic methods like Hill Climbing [21], Tabu Search [22], Simulated Annealing [23] and Genetic Algorithms [24], [25] are generally employed.", "startOffset": 126, "endOffset": 130}, {"referenceID": 22, "context": "For this reason, heuristic methods like Hill Climbing [21], Tabu Search [22], Simulated Annealing [23] and Genetic Algorithms [24], [25] are generally employed.", "startOffset": 132, "endOffset": 136}, {"referenceID": 0, "context": "In the rest of this section we describe in detail some of these approaches, leaving to specific readings more detailed discussions [1], [19], [20].", "startOffset": 131, "endOffset": 134}, {"referenceID": 16, "context": "In the rest of this section we describe in detail some of these approaches, leaving to specific readings more detailed discussions [1], [19], [20].", "startOffset": 136, "endOffset": 140}, {"referenceID": 17, "context": "In the rest of this section we describe in detail some of these approaches, leaving to specific readings more detailed discussions [1], [19], [20].", "startOffset": 142, "endOffset": 146}, {"referenceID": 23, "context": "For more detailed explanations and analyses of complexity, correctness and stability, we refer the reader to the related references [26], [27].", "startOffset": 138, "endOffset": 142}, {"referenceID": 0, "context": "Bayesian network is a product of simple terms [1].", "startOffset": 46, "endOffset": 49}, {"referenceID": 24, "context": "statistics (see [28] and references therein), which all serve to promote sparsity in the learned graph structure, though different regularization terms are", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "better suited for particular applications [1], [20].", "startOffset": 42, "endOffset": 45}, {"referenceID": 17, "context": "better suited for particular applications [1], [20].", "startOffset": 47, "endOffset": 51}, {"referenceID": 25, "context": "A widespread approach for the learning of monotonic progression networks with a directed acyclic graph (DAG) structure and conjunctive events are Conjunctive Bayesian Networks (see CBNs, [29]).", "startOffset": 187, "endOffset": 191}, {"referenceID": 10, "context": "This approach was originally adopted to model cancer progression in terms of accumulation of drivers genes [13], [14], in a way closely related to the model we discuss in this work.", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "This approach was originally adopted to model cancer progression in terms of accumulation of drivers genes [13], [14], in a way closely related to the model we discuss in this work.", "startOffset": 113, "endOffset": 117}, {"referenceID": 26, "context": "In response to this, hidden CBNs [30] were developed by augmenting the set of variables: a correspondence to a new variable Y that represents the observed state is assigned to each CBN variable X , which captures the \u201ctrue\u201d state.", "startOffset": 33, "endOffset": 37}, {"referenceID": 0, "context": "All the conclusions derived in these settings can be also directily applied to the general case where the nodes in the BN describe geneal random variables [1].", "startOffset": 155, "endOffset": 158}, {"referenceID": 6, "context": "In [9], Suppes introduced the notion of prima facie causation.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "Definition 1 (Probabilistic causation, [9]).", "startOffset": 39, "endOffset": 42}, {"referenceID": 7, "context": "The notion of prima facie causality has known limitations in the context of the general theories of causality [10], however, this characterizations seems to appropriate to model the dynamics of phenomena driven by the monotonic accumulation of events", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "To this extent, we can define a class of BNs over Bernoulli random variables named Monotonic Progression Networks (MPNs) [7], [19], [31].", "startOffset": 126, "endOffset": 130}, {"referenceID": 27, "context": "To this extent, we can define a class of BNs over Bernoulli random variables named Monotonic Progression Networks (MPNs) [7], [19], [31].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "Moreover, as discussed in [7], [19], [31], such MPNs can model accumulative phenomena in a probabilistic fashion, i.", "startOffset": 31, "endOffset": 35}, {"referenceID": 27, "context": "Moreover, as discussed in [7], [19], [31], such MPNs can model accumulative phenomena in a probabilistic fashion, i.", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "Given these premises, in [13] the authors describe an efficient algorithm (named CAPRI, see Algorithm 1) to learn the structure of constrained Bayesian networks which account for Suppes\u2019 cri-", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "teria and which later on are dubbed Suppes-Bayes Causal Networks (SBCNs) in [11].", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "on the observation (see [6]) that a way for circumventing the intrinsic computational complexity of the task of learning the structure of a Bayesian Network is to postulate a pre-determined ordering among the nodes.", "startOffset": 24, "endOffset": 27}, {"referenceID": 8, "context": "We refer the reader to [7] for further details and, following [11], we now formally define a SBCN.", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "Once again, we refer the interested reader to the discussions provided in [13], [7] and, without losing in generality, for the purpose of this work, we consider the efficient implementation of Algorithm 1.", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "We conclude this Section by reporting the pseudocode (see Algorithm 1) of the efficient learning algorithm to infer SBCNs presented in [13], which we adopt for the assessment of the performance in the simulations of the next Section.", "startOffset": 135, "endOffset": 139}, {"referenceID": 2, "context": "Moreover, as stated above, the problem of learning the structure of a BN is NP -hard [4].", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "Because of that, state-of-theart techniques largely rely on heuristics [1], often", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "For instance, methods based on Genetic Algorithms (GAs) [5], [17] and Ant Colony Optimization [32], [33] have been proposed in the literature.", "startOffset": 56, "endOffset": 59}, {"referenceID": 14, "context": "For instance, methods based on Genetic Algorithms (GAs) [5], [17] and Ant Colony Optimization [32], [33] have been proposed in the literature.", "startOffset": 61, "endOffset": 65}, {"referenceID": 28, "context": "For instance, methods based on Genetic Algorithms (GAs) [5], [17] and Ant Colony Optimization [32], [33] have been proposed in the literature.", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "For instance, methods based on Genetic Algorithms (GAs) [5], [17] and Ant Colony Optimization [32], [33] have been proposed in the literature.", "startOffset": 100, "endOffset": 104}, {"referenceID": 30, "context": "Genetic Algorithms: GAs were introduced by Holland in 1975 [34] as a global search methodology inspired by the mechanisms of natural selection.", "startOffset": 59, "endOffset": 63}, {"referenceID": 3, "context": "GAs were shown to be effective for BN learning, both in the case of available and not available a priori knowledge about nodes\u2019 ordering [5], [17].", "startOffset": 137, "endOffset": 140}, {"referenceID": 14, "context": "GAs were shown to be effective for BN learning, both in the case of available and not available a priori knowledge about nodes\u2019 ordering [5], [17].", "startOffset": 142, "endOffset": 146}, {"referenceID": 31, "context": "cedure [35].", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "For further information about our implementation of GAs for the inference of BNs, including the correction phase, we refer the interested reader to [17].", "startOffset": 148, "endOffset": 152}, {"referenceID": 0, "context": "All these measures are values in [0, 1] with results close to 1 indicators of good", "startOffset": 33, "endOffset": 39}, {"referenceID": 32, "context": ", the fitness evaluations) are implemented using the bnlearn package [36] written in the R language, while GA [34] algorithm is implemented using the Python language exploiting the inspyred [37], networkx [38] and numpy [39] packages.", "startOffset": 69, "endOffset": 73}, {"referenceID": 30, "context": ", the fitness evaluations) are implemented using the bnlearn package [36] written in the R language, while GA [34] algorithm is implemented using the Python language exploiting the inspyred [37], networkx [38] and numpy [39] packages.", "startOffset": 110, "endOffset": 114}, {"referenceID": 33, "context": ", the fitness evaluations) are implemented using the bnlearn package [36] written in the R language, while GA [34] algorithm is implemented using the Python language exploiting the inspyred [37], networkx [38] and numpy [39] packages.", "startOffset": 190, "endOffset": 194}, {"referenceID": 34, "context": ", the fitness evaluations) are implemented using the bnlearn package [36] written in the R language, while GA [34] algorithm is implemented using the Python language exploiting the inspyred [37], networkx [38] and numpy [39] packages.", "startOffset": 205, "endOffset": 209}, {"referenceID": 35, "context": ", the fitness evaluations) are implemented using the bnlearn package [36] written in the R language, while GA [34] algorithm is implemented using the Python language exploiting the inspyred [37], networkx [38] and numpy [39] packages.", "startOffset": 220, "endOffset": 224}, {"referenceID": 0, "context": "The experiments were repeated either (i) including or (ii) not including the Suppes\u2019 constraints described in Algorithm 1, and independently using 5 distinct optimization scores and regularizators, namely standard (i) log-likelihood [1], (ii) AIC [40], (iii) BIC [41], (iv) BDE [42] and (v) K2 [43], leading to a final number of 86, 400, 000 different configurations.", "startOffset": 233, "endOffset": 236}, {"referenceID": 36, "context": "The experiments were repeated either (i) including or (ii) not including the Suppes\u2019 constraints described in Algorithm 1, and independently using 5 distinct optimization scores and regularizators, namely standard (i) log-likelihood [1], (ii) AIC [40], (iii) BIC [41], (iv) BDE [42] and (v) K2 [43], leading to a final number of 86, 400, 000 different configurations.", "startOffset": 247, "endOffset": 251}, {"referenceID": 37, "context": "The experiments were repeated either (i) including or (ii) not including the Suppes\u2019 constraints described in Algorithm 1, and independently using 5 distinct optimization scores and regularizators, namely standard (i) log-likelihood [1], (ii) AIC [40], (iii) BIC [41], (iv) BDE [42] and (v) K2 [43], leading to a final number of 86, 400, 000 different configurations.", "startOffset": 263, "endOffset": 267}, {"referenceID": 38, "context": "The experiments were repeated either (i) including or (ii) not including the Suppes\u2019 constraints described in Algorithm 1, and independently using 5 distinct optimization scores and regularizators, namely standard (i) log-likelihood [1], (ii) AIC [40], (iii) BIC [41], (iv) BDE [42] and (v) K2 [43], leading to a final number of 86, 400, 000 different configurations.", "startOffset": 278, "endOffset": 282}, {"referenceID": 39, "context": "The experiments were repeated either (i) including or (ii) not including the Suppes\u2019 constraints described in Algorithm 1, and independently using 5 distinct optimization scores and regularizators, namely standard (i) log-likelihood [1], (ii) AIC [40], (iii) BIC [41], (iv) BDE [42] and (v) K2 [43], leading to a final number of 86, 400, 000 different configurations.", "startOffset": 294, "endOffset": 298}, {"referenceID": 40, "context": "Although such a trend is intuitively expected, given the larger number of parameters to be learned for more complex models, we here underline the role of statistical complications, such as the presence of spurious correlations [44] and the occurrence of Simpson\u2019s paradox [45].", "startOffset": 227, "endOffset": 231}, {"referenceID": 0, "context": ", the standard log-likelihood [1], AIC [40], BIC [41], BDE [42] and K2 [43].", "startOffset": 30, "endOffset": 33}, {"referenceID": 36, "context": ", the standard log-likelihood [1], AIC [40], BIC [41], BDE [42] and K2 [43].", "startOffset": 39, "endOffset": 43}, {"referenceID": 37, "context": ", the standard log-likelihood [1], AIC [40], BIC [41], BDE [42] and K2 [43].", "startOffset": 49, "endOffset": 53}, {"referenceID": 38, "context": ", the standard log-likelihood [1], AIC [40], BIC [41], BDE [42] and K2 [43].", "startOffset": 59, "endOffset": 63}, {"referenceID": 39, "context": ", the standard log-likelihood [1], AIC [40], BIC [41], BDE [42] and K2 [43].", "startOffset": 71, "endOffset": 75}], "year": 2017, "abstractText": "One of the critical issues when adopting Bayesian networks (BNs) to model dependencies among random variables is to \u201clearn\u201d their structure, given the huge search space of possible solutions, i.e., all the possible direct acyclic graphs. This is a wellknown NP -hard problem, which is also complicated by known pitfalls such as the issue of I-equivalence among different structures. In this work we restrict the investigations on BN structure learning to a specific class of networks, i.e., those representing the dynamics of phenomena characterized by the monotonic accumulation of events. Such phenomena allow to set specific structural constraints based on Suppes\u2019 theory of probabilistic causation and, accordingly, to define constrained BNs, named Suppes-Bayes Causal Networks (SBCNs). We here investigate the structure learning of SBCNs via extensive simulations with various state-of-the-art search strategies, such as canonical local search techniques and Genetic Algorithms. Among the main results we show that Suppes\u2019 constraints deeply simplify the learning task, by reducing the solution search space and providing a temporal ordering on the variables.", "creator": "LaTeX with hyperref package"}}}