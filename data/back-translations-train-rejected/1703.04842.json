{"id": "1703.04842", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Budgeted Batch Bayesian Optimization With Unknown Batch Sizes", "abstract": "Parameter settings profoundly impact the performance of machine learning algorithms and laboratory experiments. The classical grid search or trial-error methods are exponentially expensive in large parameter spaces, and Bayesian optimization (BO) offers an elegant alternative for global optimization of black box functions. In situations where the black box function can be evaluated at multiple points simultaneously, batch Bayesian optimization is used. Current batch BO approaches are restrictive in that they fix the number of evaluations per batch, and this can be wasteful when the number of specified evaluations is larger than the number of real maxima in the underlying acquisition function. We present the Budgeted Batch Bayesian Optimization (B3O) for hyper-parameter tuning and experimental design - we identify the appropriate batch size for each iteration in an elegant way. To set the batch size flexible, we use the infinite Gaussian mixture model (IGMM) for automatically identifying the number of peaks in the underlying acquisition functions. We solve the intractability of estimating the IGMM directly from the acquisition function by formulating the batch generalized slice sampling to efficiently draw samples from the acquisition function. We perform extensive experiments for both synthetic functions and two real world applications - machine learning hyper-parameter tuning and experimental design for alloy hardening. We show empirically that the proposed B3O outperforms the existing fixed batch BO approaches in finding the optimum whilst requiring a fewer number of evaluations, thus saving cost and time.", "histories": [["v1", "Wed, 15 Mar 2017 00:05:41 GMT  (1064kb,D)", "https://arxiv.org/abs/1703.04842v1", "22 pages"], ["v2", "Sat, 15 Apr 2017 04:54:47 GMT  (2454kb)", "http://arxiv.org/abs/1703.04842v2", "24 pages"]], "COMMENTS": "22 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vu nguyen", "santu rana", "sunil gupta", "cheng li", "svetha venkatesh"], "accepted": false, "id": "1703.04842"}, "pdf": {"name": "1703.04842.pdf", "metadata": {"source": "CRF", "title": "Budgeted Batch Bayesian Optimization With Unknown Batch Sizes", "authors": ["Vu Nguyen", "Santu Rana", "Sunil Gupta", "Cheng Li"], "emails": ["V.NGUYEN@DEAKIN.EDU.AU", "SANTU.RANA@DEAKIN.EDU.AU", "SUNIL.GUPTA@DEAKIN.EDU.AU", "CHENG.L@DEAKIN.EDU.AU", "SVETHA.VENKATESH@DEAKIN.EDU.AU"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.04 842v 2 [cs.L G] 15 AParameter settings have a profound impact on the performance of machine learning algorithms and laboratory experiments. Classical search or experimental error methods are exponentially expensive in large parameter spaces, and Bayesian Batch Optimization (BO) offers an elegant alternative to global optimization of black box functions. In situations where the black box function can be evaluated at multiple points simultaneously, Bayesian Batch Optimization is used. Current Batch BO approaches are restrictive in that they specify the number of evaluations per batch, and this can be wasteful if the number of specified evaluations exceeds the number of real maxims in the underlying acquisition function. We present the budgeted Batch Bayesian Optimization (B3O) for hyperparameter setting and the experimental design of the Iteration function, we identify the appropriate size of the GMO for each GMO Charge function."}, {"heading": "1. Introduction", "text": "Global optimization is fundamental to various real-world problems where parameter settings and design decisions are critical to the functioning of the BO - for example, in algorithm performance (deep learning networks (Bengio, 2009)) or the quality of products (chemical processes or technical design (Wang & Shan, 2007) - essentially requiring us to find the global maximum of non-concave objective function through sequential, and often noise-efficient, observations; the objective functions are unknown and expensive to evaluate, so the challenge is to find the maximum of such costly objective functions in a few sequential queries, minimizing time and cost. Bayesian optimization (BO) is an approach to find the global optimum of such expensive, black box objective functions by using limited ratings. (Snoek, Larochelle, & Adams, 2012; Shahriari, Sweras, Wang Adams, 2016, Feitghi, 2016 & Freitghi)."}, {"heading": "2. Preliminary", "text": "We first review the Gaussian Process (GP), the Bayesian Optimization (BO) and the acquisition functions. We then summarize the Bayesian Optimization approach and the existing approaches."}, {"heading": "2.1 Gaussian process", "text": "Gaussian Processes (GP) (Rasmussen, 2006) extend a multivariate Gaussian distribution to infinite dimensionality. Formally, the Gaussian process generates data located in some domains, so that each finite subset of the domain follows a multivariate Gaussian distribution. Given the N observations Y = {y1, y2,... yN}, which can always be regarded as a single point out of some multivariate Gaussian distributions, it is assumed that the mean value of GP is zero everywhere. What one observation refers to another is merely the covariance function, k (x, x \u2032). From the assumption of GP, we assume that the y-N \u2212 K distribution matrix (0, K), where the covariance matrix-k is defined as follows: K = k (x1) k (x2, x2), xxxxx."}, {"heading": "2.2 Bayesian optimization", "text": "We assume that f is a black box function, that is, its shape is unknown and beyond that it is expensive to evaluate it. Perturbed evaluations of type yi = f (xi) + \u03b5i, where \u03b5i \u0445 N (0, \u03c3 2), are available. Example of the Branin function is in Fig. 1. Bayesian optimization makes a series of evaluations x1,..., xT of f that the maximum of f is found in the Fewest iterations (Snoek et al., 2012; Shahriari et al., 2016; Dai Nguyen, Gupta, Rana, Nguyen, Venkatesh, Deane, & Sanders, 2016; Rana, Gupta, Li, & Venkatesh, 2016c). Formally, let f: X \u2192 R be a well behaved function defined on a compact subset X RD."}, {"heading": "2.3 Batch Bayesian optimization", "text": "In practice, it can be advantageous to perform multiple function evaluations in parallel. Therefore, we will consider the evaluation of f based on a number of points. An example of Bayesian batch optimization versus Bayesian sequence optimization is illustrated in Fig. 2. As discussed earlier, such scenarios occur, for example, in the optimization of computer models where multiple machines (or cores) are available to perform experiments in parallel, or in wet laboratory experiments where the time of testing an experimental design is identical to testing a batch. Formally, we maximize the capture function by finding a collection of points such as X t = [xt1, xt2,..., xtnt] = argmax x = X instead of \u2212 1 (x) (2), where nt is the batch size for iteration. Existing batches often set the batch size to a constant value for all iterations."}, {"heading": "2.4 Existing batch Bayesian optimization methods", "text": "The first BO parallelization was used in the context of the learning experiments to select a batch of size q containing points \"close to the best point.\" Development of the expected improvement of the acquisition function (EI), the expected multipoint improvement (Q-EI), the expected improvement (Q-EI) (Ginsbourger, Le Riche, & Carraro, 2007), which include the acquisition function as conditional expectation of the improvement achieved by q points. Constant Liar (CL) is a heuristic q-EI algorithm (Ginsbourger et al, 2008) that constructs a greedy approach to iterative."}, {"heading": "3. Budgeted Batch Bayesian Optimization", "text": "We propose the novel batch-bayesian optimization method, which determines the appropriate batch size for each iteration. We call our approach budgeted batch-bayesian optimization (B3O). The proposed method is economical in terms of the number of optimal evaluations while maintaining performance. We first describe our approach to approximating the acquisition function using the infinite Gaussian mixing model (IGMM). We then present the batch-generalized slice sampler to efficiently collect samples under the capture function. Next, we briefly describe the existing variable inference technology for IGMM. Finally, we summarize our algorithm."}, {"heading": "3.1 Batch Bayesian optimization", "text": "We look at Bayesian batch optimization. As a Bayesian optimization task, our ultimate goal is to solve the global optimization problem by performing a series of batch evaluations X t, X2... XT, where X t = [xt1, xt2,..., xtnt] so that the maximum of f is found (Snoek et al., 2012; Shahriari et al., 2016), where nt is the batch size."}, {"heading": "3.2 Approximating the acquisition function with infinite Gaussian mixture models", "text": "However, it is intuitive to assume that the detection function can be approximated by a mixture of Gaussian non-parametries (see Figure 3), with the peaks in the detection function corresponding to the middle positions of the underlying Gaussian components. As the number of peaks is unknown, we borrow the elegance of Bajian non-parametries (Hjort, Holmes, Mu M\u00fcller, & Walker, 2010) to identify the unknown number of Gaussian components. We use the Infinite Gaussian Mixture Model (IGMM) (Rasmussen, 1999), which induces the dirichlet process before a possibly infinite number of Gaussian components. In IGMM, each Gaussian component representing a peak k is parameterized by the middle Gaussian component (IGMM) and the covariance of Ik. Interestingly, the estimated mean \u00b5ance is the position of the detection function of the estimated GMM during the coordination."}, {"heading": "3.3 Batch generalized slice sampler (BGSS)", "text": "This implies that most of the samples come from the top areas."}, {"heading": "3.4 Variational inference for infinite Gaussian mixture model", "text": "IGMM is the non-parametric mixing model where the prior distribution via the mixing ratio q q = q q q (Ferguson, 1973) is a process. There are few existing approaches to learning an IGMM, such as collapsed Gibbs samplers (Kamper, 2013) and variative inference (Lead, Jordan, et al., 2006). In this paper, we follow (Lead et al., 2006) to derive the variable inference for IGMM (Rasmussen, 1999) because the variable approach is generally faster than the Gibbs sampler. After adjusting the IGMM using variable inference, we obtain the mean places \u00b51.... K as selected points for the ratings in the Batch BO setting. We note that K is automatically identified in this Bayesian non-parametric sampler. Using the collection of samples (si) N i = 1 from our batch sampler, we consider the following IGMM as a sampler."}, {"heading": "3.5 Algorithm", "text": "Although the technique of inferring variations for IGMM and scanning sections has existed for years, the idea of combining these things in the right place for Batch BO is novel. We summarize the steps for B3O in Algorithm 2. With an iteration t, we find a batch of nt points in which the number of points nt varies with each iteration. We note that steps 2,3,7 and 8 are standard in Bayesian optimization techniques. Our proposed procedure is emphasized in steps 4,5 and 6 to find a batch of points X t from the capture function. In particular, Fig. 3 illustrates and summarizes our steps 4,5 and 6 in 1D and 2D, respectively. We capture the calculation time of steps 4, 5 and 6 in Sec 4.4 and analyze step 4 by simulation in Sec 4.5, which requires more calculation than other steps."}, {"heading": "4. Experiments", "text": "We evaluate the B3O algorithm using both the synthetic functions and the experimental settings we perform; for synthetic function evaluation we use 8 functions over dimensions (1-10); we also perform hyper-parameter tuning for three machine learning algorithms - vector regression support (Smola & Vapnik, 1997), Bayesian nonparametric multi-label classification (Nguyen, Gupta, Rana, Li, & Venkatesh, 2016a) and multi-layer perceptron (Ruck, Rogers, Oxley, Suter, 1990); we also consider the real-world experimental design for aluminum scandium hardening. This heat treatment design involves searching for the best combination of temperatures and times in two stages to achieve the required properties - in our case, hardness.We compare our results with the best-found values."}, {"heading": "4.1 Comparison - best found value", "text": "We examine the performance of B3O in determining the optimum of the chosen benchmark functions. We show that our method can find better optimum values (minimum) in 5 of 8 functions. In particular, B3O exceeds all baselines in Forrester 1D, Alpine2 5D, gSobol 5D, Alpine2 10D, gSobol 10D. In the other three cases, B3O still delivers relatively good values. The fixed batch batches (e.g. LP) may suffer from under-specification (the number of specified batch sizes is smaller than the number of real peak values) and thus negatively affect the final performance. As expected, the batch batches are always better than the sequential batches using EI and UCB because the number of specified points for batch methods is much higher than the number of real peak values)."}, {"heading": "4.2 Comparison - number of evaluations", "text": "The next criterion for comparison is the total number of scores, N = \u2211 Tt = 0 nt. The requirement of Bayean optimization is to keep the number of scores as low as possible, as these scores can be costly. For example, it may be very expensive to conduct a single experiment in materials science (perform an alloy test), or it may take a few days to train a deep network on a large dataset. It is not natural to set the number of scores per batch nt, as the number of peaks is unknown, and it is important that these peaks are changed after new scores have been performed. Therefore, we may waste time and resources if we overestimate the number of points when we need them. In particular, after all actual peaks are detected, when we set the batch size nt large, we obtain the enoisy points that may be close to the points already determined, due to the effect of penalizing the peaks (2016 Gonza, D\u00f6ral)."}, {"heading": "4.3 Analysis of B3O - number of points per iteration", "text": "We show that B3O is flexible when it comes to determining the required number of evaluations per batch. We examine the number of estimated points in B3O at each iteration. In particular, we record the number of points nt at each iteration, see Figure 7 for two functions - Dropwave 2D and gSobol 5D. The estimated points per batch in our approach are flexible and are automatically determined in this Bayean non-parametric setting. For some iterations, B3O recommends only one point per batch (see gSobol 5D in Figure 7). This fact is crucial for the homogeneous capture function, which includes a single peak, while fixed batch approaches can be wasted on evaluating at some unnecessary points."}, {"heading": "4.4 Analysis of computational time", "text": "Next, we compare the computation time for each iteration for the different batch approaches. Specifically, we are interested in the CPU time needed to find the batch between different approaches, and the sequential methods and the random batch approaches are the cheapest to calculate than the other batch approaches. Therefore, we do not compare sequential and random batch approaches. Constant Liar (Ginsbourger et al., 2008) and BUCB (Desautels et al., 2014) take time to re-evaluate the GP when the fake observations are added, and significantly slower when the number of data points N is large. The Local Penalization approach (Gonza'lez et al., 2016) takes a considerable amount of time to estimate the Lipschitz constant. Furthermore, LP calculates and maintains the penalty costs around the visited points, and reduces the cost of punishing and optimizing the lutch dimensions overall (BO-3 and BO-3 dimensions), while the number of the observed dimensions are large or low."}, {"heading": "4.5 Analysis of batch generalized slice sampling (BGSS)", "text": "We study the efficiency of the BGSS for taking samples under the capturing function shown in Section 3.3. We vary the observation dimension D - 5,10,20,40.50 - and build up the capturing function \u03b1UCB. We then design two BGSS settings of size 100 and 200 and compare them with the generalized sample shown in Fig. 4. We include two important factors, including the computing time and the number of accepted samples under the curve. In Fig. 8, we present the simulation results that BGSS significantly exceeds the standard sample in terms of calculation (faster process) and efficiency (higher number of accepted points). For example, BGSS of size 200 takes 32 seconds to obtain 2485 accepted data points at D = 10. These accepted points are below the curve and surround the summit in general. Therefore, these samples are advantageous for the adaptation of the IGMM as we are particularly interested in finding the peaks."}, {"heading": "4.6 Tuning hyper-parameters for machine learning algorithms", "text": "In practice, however, it is such that most people are able to go in search of a better solution than they are able to go in search of a better solution. (\"Snoek et al.,\" \"Snoek et al.,\" \"Snoek et al.,\" \"Snoek et al.,\" \"Snoek et al.,\" \"Snoek et al.,\" \"\" Snoek et al., \"\" Snoek., \"\" Snoek., \"\" \"Snoek.,\"., \"\", \"\", \",\" \",\", \"\", \",\", \",\", \",\", \"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\", \",\", \",\" \",\", \"\", \",\", \"\" \"\", \",\" \",\" \"\" \",\", \",\" \"\" \"\" \",\", \",\" \"\", \",\" \",\", \"\" \"\" \",\" \",\" \",\", \",\", \"\" \",\" \"\", \",\", \",\", \"\" \",\", \",\", \"\", \"\" \"\", \"\" \",\" \",\" \",\", \"\" \"\", \",\", \"\", \",\", \",\", \",\", \",\", \",\", \",\" \"\" \",\", \",\", \",\", \"\", \",\", \",\", \"\", \",\", \",\", \"\" \"\", \",\", \"\", \",\" \",\", \",\" \",\", \"\", \",\", \"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\""}, {"heading": "4.7 Bayesian optimization for experimental design", "text": "We look at the alloy hardening process of aluminum scandium (Wagner, Kampmann, & Voorhees, 1991), which consists of two phases: nucleation and precipitation coarsening. Our goal is to maximize the hardness of aluminum scandium alloys by designing the appropriate times and temperatures for the two phases of the process; the traditional approach is to apply iterative trial and error, selecting a material design (temperature and time) based on intuition, past experience or theoretical knowledge, and testing the material in physical experiments; and then using what we learn from these experiments in selecting the material design next; this iterative process repeats until some combination of success or exhaustion is achieved! As the parameter space grows exponentially, the trial and error approach is prohibitive in terms of both time and cost."}, {"heading": "5. Conclusion", "text": "The proposed approach of B3O can identify the appropriate batch size for each iteration that most existing batch BO approaches cannot provide. We have introduced batch generalized slice sampling for drawing samples under the acquisition function. We are conducting extensive experiments to find the optimal solution for 8 synthetic functions and continue to evaluate performance based on 4 real-world tasks. Experimental results underscore B3O's ability to find the optimum while requiring fewer evaluations than the baselines."}, {"heading": "6. Acknowledgment", "text": "We thank Dr. Paul G Sanders and Kyle J Deane for their hands-on case study of the aluminum scandium hardening process."}], "references": [{"title": "Batch bayesian optimization via simulation matching", "author": ["J. Azimi", "A. Fern", "X.Z. Fern"], "venue": null, "citeRegEx": "Azimi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Azimi et al\\.", "year": 2010}, {"title": "Hybrid batch bayesian optimization", "author": ["J. Azimi", "A. Jalali", "X. Zhang-fern"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Azimi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Azimi et al\\.", "year": 2012}, {"title": "Algorithms for hyper-parameter optimization", "author": ["J.S. Bergstra", "R. Bardenet", "Y. Bengio", "B. K\u00e9gl"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "Variational inference for dirichlet process mixtures", "author": ["D.M. Blei", "Jordan", "M. I"], "venue": "Bayesian Analysis,", "citeRegEx": "Blei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2006}, {"title": "Generalized accept-reject sampling schemes", "author": ["G. Casella", "C.P. Robert", "M.T. Wells"], "venue": "Lecture Notes-Monograph", "citeRegEx": "Casella et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Casella et al\\.", "year": 2004}, {"title": "Parallel gaussian process optimization with upper confidence bound and pure exploration", "author": ["E. Contal", "D. Buffoni", "A. Robicquet", "N. Vayatis"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Contal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Contal et al\\.", "year": 2013}, {"title": "Cascade bayesian optimization", "author": ["T. Dai Nguyen", "S. Gupta", "S. Rana", "V. Nguyen", "S. Venkatesh", "K.J. Deane", "P.G. Sanders"], "venue": "In Australasian Joint Conference on Artificial Intelligence,", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Parallelizing exploration-exploitation tradeoffs in gaussian process bandit optimization", "author": ["T. Desautels", "A. Krause", "J.W. Burdick"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Desautels et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Desautels et al\\.", "year": 2014}, {"title": "Support vector regression machines", "author": ["H. Drucker", "C.J. Burges", "L. Kaufman", "A. Smola", "V Vapnik"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Drucker et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Drucker et al\\.", "year": 1997}, {"title": "Gaussian processes for regression: A quick introduction", "author": ["M. Ebden"], "venue": "The Website of Robotics Research Group in Department on Engineering Science, University of Oxford.", "citeRegEx": "Ebden,? 2008", "shortCiteRegEx": "Ebden", "year": 2008}, {"title": "A Bayesian analysis of some nonparametric problems", "author": ["T.S. Ferguson"], "venue": "The Annals of Statistics, 1(2), 209\u2013230.", "citeRegEx": "Ferguson,? 1973", "shortCiteRegEx": "Ferguson", "year": 1973}, {"title": "Parallel global optimization using an improved multi-points expected improvement criterion", "author": ["P.I. Frazier", "S.C. Clark"], "venue": "In INFORMS Optimization Society Conference, Miami FL,", "citeRegEx": "Frazier and Clark,? \\Q2012\\E", "shortCiteRegEx": "Frazier and Clark", "year": 2012}, {"title": "Exponential regret bounds for gaussian process bandits with deterministic observations", "author": ["N.D. Freitas", "M. Zoghi", "A.J. Smola"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Freitas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Freitas et al\\.", "year": 2012}, {"title": "A multi-points criterion for deterministic parallel global optimization based on kriging", "author": ["D. Ginsbourger", "R. Le Riche", "L. Carraro"], "venue": null, "citeRegEx": "Ginsbourger et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ginsbourger et al\\.", "year": 2007}, {"title": "A multi-points criterion for deterministic parallel global optimization based on gaussian processes", "author": ["D. Ginsbourger", "R. Le Riche", "L. Carraro"], "venue": null, "citeRegEx": "Ginsbourger et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ginsbourger et al\\.", "year": 2008}, {"title": "Kriging is well-suited to parallelize optimization", "author": ["D. Ginsbourger", "R. Le Riche", "L. Carraro"], "venue": "In Computational Intelligence in Expensive Optimization Problems,", "citeRegEx": "Ginsbourger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ginsbourger et al\\.", "year": 2010}, {"title": "Batch bayesian optimization via local penalization", "author": ["J. Gonz\u00e1lez", "Z. Dai", "P. Hennig", "N.D. Lawrence"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Gonz\u00e1lez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gonz\u00e1lez et al\\.", "year": 2016}, {"title": "Entropy search for information-efficient global optimization", "author": ["P. Hennig", "C.J. Schuler"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hennig and Schuler,? \\Q2012\\E", "shortCiteRegEx": "Hennig and Schuler", "year": 2012}, {"title": "Predictive entropy search for efficient global optimization of black-box functions", "author": ["J.M. Hern\u00e1ndez-Lobato", "M.W. Hoffman", "Z. Ghahramani"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2014}, {"title": "Bayesian nonparametrics", "author": ["N. Hjort", "C. Holmes", "P. M\u00fcller", "S. Walker"], "venue": null, "citeRegEx": "Hjort et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hjort et al\\.", "year": 2010}, {"title": "An evaluation of sequential model-based optimization for expensive blackbox functions", "author": ["F. Hutter", "H. Hoos", "K. Leyton-Brown"], "venue": "In Proceedings of the 15th annual conference companion on Genetic and evolutionary computation,", "citeRegEx": "Hutter et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2013}, {"title": "A literature survey of benchmark functions for global optimisation problems", "author": ["M. Jamil", "Yang", "X.-S"], "venue": "International Journal of Mathematical Modelling and Numerical Optimisation,", "citeRegEx": "Jamil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jamil et al\\.", "year": 2013}, {"title": "A taxonomy of global optimization methods based on response surfaces", "author": ["D.R. Jones"], "venue": "Journal of global optimization, 21(4), 345\u2013383.", "citeRegEx": "Jones,? 2001", "shortCiteRegEx": "Jones", "year": 2001}, {"title": "Lipschitzian optimization without the lipschitz constant", "author": ["D.R. Jones", "C.D. Perttunen", "B.E. Stuckman"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Jones et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Jones et al\\.", "year": 1993}, {"title": "Efficient global optimization of expensive black-box functions", "author": ["D.R. Jones", "M. Schonlau", "W.J. Welch"], "venue": "Journal of Global optimization,", "citeRegEx": "Jones et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Jones et al\\.", "year": 1998}, {"title": "Gibbs sampling for fitting finite and infinite gaussian mixture models", "author": ["H. Kamper"], "venue": null, "citeRegEx": "Kamper,? \\Q2013\\E", "shortCiteRegEx": "Kamper", "year": 2013}, {"title": "Kinetics of precipitation in metastable binary alloys-theory and applications to cu-1.9 at% ti and ni-14 at% al. In Decomposition of Alloys: The Early Stages, Proceedings of the 2 nd Acta-Scripta", "author": ["R. Kampmann", "R. Wagner"], "venue": "Metallurgica Conference,", "citeRegEx": "Kampmann and Wagner,? \\Q1983\\E", "shortCiteRegEx": "Kampmann and Wagner", "year": 1983}, {"title": "Designing engaging games using bayesian optimization", "author": ["M.M. Khajah", "B.D. Roads", "R.V. Lindsey", "Liu", "Y.-E", "M.C. Mozer"], "venue": "In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems,", "citeRegEx": "Khajah et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Khajah et al\\.", "year": 2016}, {"title": "A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise", "author": ["H.J. Kushner"], "venue": "Journal of Basic Engineering, 86(1), 97\u2013106.", "citeRegEx": "Kushner,? 1964", "shortCiteRegEx": "Kushner", "year": 1964}, {"title": "High dimensional bayesian optimization with elastic gaussian process", "author": ["C. Li", "S. Gupta", "S. Rana", "V. Nguyen", "S. Venkatesh"], "venue": "In Workshop on Bayesian Optimization at Neural Information Processing Systems (NIPS)", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Equation of state calculations by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller"], "venue": "The journal of chemical physics,", "citeRegEx": "Metropolis et al\\.,? \\Q1953\\E", "shortCiteRegEx": "Metropolis et al\\.", "year": 1953}, {"title": "The application of bayesian methods for seeking the extremum", "author": ["J. Mockus", "V. Tiesis", "A. Zilinskas"], "venue": "Towards global optimization,", "citeRegEx": "Mockus et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Mockus et al\\.", "year": 1978}, {"title": "Slice sampling", "author": ["R. Neal"], "venue": "Annals of statistics, 705\u2013741.", "citeRegEx": "Neal,? 2003", "shortCiteRegEx": "Neal", "year": 2003}, {"title": "A bayesian nonparametric approach for multi-label classification", "author": ["V. Nguyen", "S. Gupta", "S. Rana", "C. Li", "S. Venkatesh"], "venue": "In Proceedings of The 8th Asian Conference on Machine Learning,", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Think globally, act locally: a local strategy for bayesian optimization", "author": ["V. Nguyen", "S. Gupta", "S. Rana", "C. Li", "S. Venkatesh"], "venue": "In Workshop on Bayesian Optimization at Neural Information Processing Systems (NIPS)", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Budgeted batch bayesian optimization", "author": ["V. Nguyen", "S. Rana", "S.K. Gupta", "C. Li", "S. Venkatesh"], "venue": "In Data Mining (ICDM),", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "The parallel bayesian optimization algorithm", "author": ["J. O\u010den\u00e1\u0161ek", "J. Schwarz"], "venue": "In The State of the Art in Computational Intelligence,", "citeRegEx": "O\u010den\u00e1\u0161ek and Schwarz,? \\Q2000\\E", "shortCiteRegEx": "O\u010den\u00e1\u0161ek and Schwarz", "year": 2000}, {"title": "Parallel slice sampling", "author": ["T. Pietrabissa", "S. Rusconi"], "venue": "In The Contribution of Young Researchers to Bayesian Statistics,", "citeRegEx": "Pietrabissa and Rusconi,? \\Q2014\\E", "shortCiteRegEx": "Pietrabissa and Rusconi", "year": 2014}, {"title": "The infinite gaussian mixture model", "author": ["C.E. Rasmussen"], "venue": "In NIPS,", "citeRegEx": "Rasmussen,? \\Q1999\\E", "shortCiteRegEx": "Rasmussen", "year": 1999}, {"title": "Gaussian processes for machine learning", "author": ["C.E. Rasmussen"], "venue": null, "citeRegEx": "Rasmussen,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen", "year": 2006}, {"title": "The multilayer perceptron as an approximation to a bayes optimal discriminant function", "author": ["D.W. Ruck", "S.K. Rogers", "M. Kabrisky", "M.E. Oxley", "B.W. Suter"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Ruck et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Ruck et al\\.", "year": 1990}, {"title": "Parallel predictive entropy search for batch global optimization of expensive objective functions", "author": ["A. Shah", "Z. Ghahramani"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Shah and Ghahramani,? \\Q2015\\E", "shortCiteRegEx": "Shah and Ghahramani", "year": 2015}, {"title": "Taking the human out of the loop: A review of bayesian optimization", "author": ["B. Shahriari", "K. Swersky", "Z. Wang", "R.P. Adams", "N. de Freitas"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Shahriari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shahriari et al\\.", "year": 2016}, {"title": "Support vector regression machines", "author": ["A. Smola", "V. Vapnik"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Smola and Vapnik,? \\Q1997\\E", "shortCiteRegEx": "Smola and Vapnik", "year": 1997}, {"title": "Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": null, "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["N. Srinivas", "A. Krause", "S. Kakade", "M. Seeger"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Srinivas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2010}, {"title": "Auto-weka: Combined selection and hyperparameter optimization of classification algorithms", "author": ["C. Thornton", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Thornton et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Thornton et al\\.", "year": 2013}, {"title": "Parallel multivariate slice sampling", "author": ["M.M. Tibbits", "M. Haran", "J.C. Liechty"], "venue": "Statistics and Computing,", "citeRegEx": "Tibbits et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tibbits et al\\.", "year": 2011}, {"title": "Homogeneous second-phase precipitation", "author": ["R. Wagner", "R. Kampmann", "P.W. Voorhees"], "venue": "Materials science and technology", "citeRegEx": "Wagner et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Wagner et al\\.", "year": 1991}, {"title": "Review of metamodeling techniques in support of engineering design optimization", "author": ["G.G. Wang", "S. Shan"], "venue": "Journal of Mechanical design,", "citeRegEx": "Wang and Shan,? \\Q2007\\E", "shortCiteRegEx": "Wang and Shan", "year": 2007}, {"title": "Parallel bayesian global optimization of expensive functions", "author": ["J. Wang", "S.C. Clark", "E. Liu", "P.I. Frazier"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Bayesian optimization in a billion dimensions via random embeddings", "author": ["Z. Wang", "F. Hutter", "M. Zoghi", "D. Matheson", "N. de Feitas"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 38, "context": "However, since the number of underlying peaks is unknown, we use the infinite Gaussian mixture model (IGMM) (Rasmussen, 1999) so that the number of peaks in the acquisition function can be estimated automatically.", "startOffset": 108, "endOffset": 125}, {"referenceID": 39, "context": "1 Gaussian process Gaussian processes (GP) (Rasmussen, 2006) extends a multivariate Gaussian distribution to infinite dimensionality.", "startOffset": 43, "endOffset": 60}, {"referenceID": 39, "context": "(Rasmussen, 2006; Ebden, 2008) where its mean and variance are given by", "startOffset": 0, "endOffset": 30}, {"referenceID": 9, "context": "(Rasmussen, 2006; Ebden, 2008) where its mean and variance are given by", "startOffset": 0, "endOffset": 30}, {"referenceID": 44, "context": ",xT of f such that the maximum of f is found in the fewest iterations (Snoek et al., 2012; Shahriari et al., 2016; Dai Nguyen, Gupta, Rana, Nguyen, Venkatesh, Deane, & Sanders, 2016; Nguyen, Rana, Gupta, Li, & Venkatesh, 2016c).", "startOffset": 70, "endOffset": 227}, {"referenceID": 42, "context": ",xT of f such that the maximum of f is found in the fewest iterations (Snoek et al., 2012; Shahriari et al., 2016; Dai Nguyen, Gupta, Rana, Nguyen, Venkatesh, Deane, & Sanders, 2016; Nguyen, Rana, Gupta, Li, & Venkatesh, 2016c).", "startOffset": 70, "endOffset": 227}, {"referenceID": 39, "context": "Bayesian optimization reasons about f by building a Gaussian process through evaluations (Rasmussen, 2006).", "startOffset": 89, "endOffset": 106}, {"referenceID": 18, "context": "Among many existing acquisition functions in literature (Hennig & Schuler, 2012; Hern\u00e1ndez-Lobato et al., 2014; Srinivas, Krause, Kakade, & Seeger, 2010; Mockus, Tiesis, & Zilinskas, 1978; Jones, 2001; Freitas, Zoghi, & Smola, 2012; Nguyen, Gupta, Rana, Li, & Venkatesh, 2016b), we briefly describe three common acquisition functions including probability of improvement, expected improvement and upper confidence bound.", "startOffset": 56, "endOffset": 277}, {"referenceID": 22, "context": "Among many existing acquisition functions in literature (Hennig & Schuler, 2012; Hern\u00e1ndez-Lobato et al., 2014; Srinivas, Krause, Kakade, & Seeger, 2010; Mockus, Tiesis, & Zilinskas, 1978; Jones, 2001; Freitas, Zoghi, & Smola, 2012; Nguyen, Gupta, Rana, Li, & Venkatesh, 2016b), we briefly describe three common acquisition functions including probability of improvement, expected improvement and upper confidence bound.", "startOffset": 56, "endOffset": 277}, {"referenceID": 28, "context": "The early work of (Kushner, 1964) suggested maximizing the probability of improvement (PI) over the incumbent \u03b1(x) = \u03a6 ( \u03bc(x)\u2212y+ \u03c3(x) )", "startOffset": 18, "endOffset": 33}, {"referenceID": 31, "context": "Thus, one could instead measure the expected improvement (EI) (Mockus et al., 1978; Jones, Schonlau, & Welch, 1998).", "startOffset": 62, "endOffset": 115}, {"referenceID": 45, "context": "\u03b2 to achieve sublinear regret (Srinivas et al., 2010).", "startOffset": 30, "endOffset": 53}, {"referenceID": 14, "context": "Constant Liar (CL) is a heuristic q-EI algorithm (Ginsbourger et al., 2008), which uses a greedy approach to iteratively construct a batch of q points.", "startOffset": 49, "endOffset": 75}, {"referenceID": 5, "context": "Another direction (Contal et al., 2013; Desautels et al., 2014) in batch Bayesian optimization exploits an interesting fact about GPs: the predictive variance of GPs depends only on the feature x, but not the outcome values y.", "startOffset": 18, "endOffset": 63}, {"referenceID": 7, "context": "Another direction (Contal et al., 2013; Desautels et al., 2014) in batch Bayesian optimization exploits an interesting fact about GPs: the predictive variance of GPs depends only on the feature x, but not the outcome values y.", "startOffset": 18, "endOffset": 63}, {"referenceID": 7, "context": "The GP-BUCB algorithm (Desautels et al., 2014) and GP-UCB-PE (Contal et al.", "startOffset": 22, "endOffset": 46}, {"referenceID": 5, "context": ", 2014) and GP-UCB-PE (Contal et al., 2013) extend the sequential UCB to a batch setting by first selecting the next point, updating the predictive variance which in turn alters the acquisition function, and then selecting the next point.", "startOffset": 22, "endOffset": 43}, {"referenceID": 5, "context": "In particular, the GP-UCB-PE (Contal et al., 2013) chooses the first point of the batch via the UCB score and then defines a \u201crelevance region\u201d and selects the remaining points from this region greedily to maximize the information gain, to focus on pure exploration (PE).", "startOffset": 29, "endOffset": 50}, {"referenceID": 18, "context": "Batch BO can also be developed using information-based policies (Hern\u00e1ndez-Lobato et al., 2014).", "startOffset": 64, "endOffset": 95}, {"referenceID": 18, "context": "Parallel Predictive Entropy Search (PPES) (Shah & Ghahramani, 2015) extends the Predictive Entropy Search (PES) algorithm of (Hern\u00e1ndez-Lobato et al., 2014) to a batch setting.", "startOffset": 125, "endOffset": 156}, {"referenceID": 16, "context": "More recently, Local Penalization (LP) (Gonz\u00e1lez et al., 2016) presents a heuristic approach for batch BO by iteratively penalizing the current peak in the acquisition function to find the next", "startOffset": 39, "endOffset": 62}, {"referenceID": 16, "context": "For ease of computation, (Gonz\u00e1lez et al., 2016) estimates the Lipschitz constant of the GP predictive mean instead of the actual acquisition function.", "startOffset": 25, "endOffset": 48}, {"referenceID": 16, "context": "Gonzalez et al (Gonz\u00e1lez et al., 2016) has demonstrated that LP outperforms a wide range of baselines in batch Bayesian optimization.", "startOffset": 15, "endOffset": 38}, {"referenceID": 14, "context": "First, most of the proposed batch BO (Ginsbourger et al., 2008; Azimi et al., 2010; Azimi, Jalali, & Zhang-fern, 2012; Contal et al., 2013; Desautels et al., 2014; Gonz\u00e1lez et al., 2016) involve a greedy algorithm, which chooses individual points until the batch is filled.", "startOffset": 37, "endOffset": 186}, {"referenceID": 0, "context": "First, most of the proposed batch BO (Ginsbourger et al., 2008; Azimi et al., 2010; Azimi, Jalali, & Zhang-fern, 2012; Contal et al., 2013; Desautels et al., 2014; Gonz\u00e1lez et al., 2016) involve a greedy algorithm, which chooses individual points until the batch is filled.", "startOffset": 37, "endOffset": 186}, {"referenceID": 5, "context": "First, most of the proposed batch BO (Ginsbourger et al., 2008; Azimi et al., 2010; Azimi, Jalali, & Zhang-fern, 2012; Contal et al., 2013; Desautels et al., 2014; Gonz\u00e1lez et al., 2016) involve a greedy algorithm, which chooses individual points until the batch is filled.", "startOffset": 37, "endOffset": 186}, {"referenceID": 7, "context": "First, most of the proposed batch BO (Ginsbourger et al., 2008; Azimi et al., 2010; Azimi, Jalali, & Zhang-fern, 2012; Contal et al., 2013; Desautels et al., 2014; Gonz\u00e1lez et al., 2016) involve a greedy algorithm, which chooses individual points until the batch is filled.", "startOffset": 37, "endOffset": 186}, {"referenceID": 16, "context": "First, most of the proposed batch BO (Ginsbourger et al., 2008; Azimi et al., 2010; Azimi, Jalali, & Zhang-fern, 2012; Contal et al., 2013; Desautels et al., 2014; Gonz\u00e1lez et al., 2016) involve a greedy algorithm, which chooses individual points until the batch is filled.", "startOffset": 37, "endOffset": 186}, {"referenceID": 44, "context": ",xtnt ] such that the maximum of f is found (Snoek et al., 2012; Shahriari et al., 2016) where nt is the batch size.", "startOffset": 44, "endOffset": 88}, {"referenceID": 42, "context": ",xtnt ] such that the maximum of f is found (Snoek et al., 2012; Shahriari et al., 2016) where nt is the batch size.", "startOffset": 44, "endOffset": 88}, {"referenceID": 38, "context": "We use the infinite Gaussian mixture model (IGMM) (Rasmussen, 1999) which induces the Dirichlet Process prior over possibly infinite number of Gaussian components.", "startOffset": 50, "endOffset": 67}, {"referenceID": 16, "context": "In contrast, previous work (Gonz\u00e1lez et al., 2016) relies on an unique Lipschitz constant to represent the shape of the peak that is not reasonable for the heteroscedastic setting when the peaks have different shapes.", "startOffset": 27, "endOffset": 50}, {"referenceID": 32, "context": "In particular, we select to use the slice sampling (Neal, 2003) because it is easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn.", "startOffset": 51, "endOffset": 63}, {"referenceID": 32, "context": "Slice sampling approach (Neal, 2003) also brings a challenge for high-dimensional data that it is hard to find accepted area R such that g(R) > u.", "startOffset": 24, "endOffset": 36}, {"referenceID": 10, "context": "4 Variational inference for infinite Gaussian mixture model IGMM is the nonparametric mixture model where the prior distribution over the mixing proportion is a Dirichlet process (Ferguson, 1973).", "startOffset": 179, "endOffset": 195}, {"referenceID": 25, "context": "There are few existing approaches to learn a IGMM, such as collapsed Gibbs sampler (Kamper, 2013) and variational inference (Blei, Jordan, et al.", "startOffset": 83, "endOffset": 97}, {"referenceID": 3, "context": "In this paper, we follow (Blei et al., 2006) to derive the variational inference for IGMM (Rasmussen, 1999) since the variational approach is generally faster than the Gibbs sampler.", "startOffset": 25, "endOffset": 44}, {"referenceID": 38, "context": ", 2006) to derive the variational inference for IGMM (Rasmussen, 1999) since the variational approach is generally faster than the Gibbs sampler.", "startOffset": 53, "endOffset": 70}, {"referenceID": 3, "context": "For this approach be tractable, we truncate the variational distribution at some value K by setting q(vK = 1) = 1 and we can ignore \u03bck,\u03a3k for k > K (Blei et al., 2006).", "startOffset": 148, "endOffset": 167}, {"referenceID": 38, "context": "Due to the space restriction, we shall refer the detailed inference of the infinite Gaussian mixture model to (Rasmussen, 1999; Blei et al., 2006).", "startOffset": 110, "endOffset": 146}, {"referenceID": 3, "context": "Due to the space restriction, we shall refer the detailed inference of the infinite Gaussian mixture model to (Rasmussen, 1999; Blei et al., 2006).", "startOffset": 110, "endOffset": 146}, {"referenceID": 31, "context": "BASELINES \u2022 Expected Improvement (EI)(Mockus et al., 1978; Jones, 2001): this is a sequential approach using EI for the acquisition function.", "startOffset": 37, "endOffset": 71}, {"referenceID": 22, "context": "BASELINES \u2022 Expected Improvement (EI)(Mockus et al., 1978; Jones, 2001): this is a sequential approach using EI for the acquisition function.", "startOffset": 37, "endOffset": 71}, {"referenceID": 45, "context": "\u2022 GP-Upper Confident Bound (UCB)(Srinivas et al., 2010): this is a sequential approach using UCB with \u221a", "startOffset": 32, "endOffset": 55}, {"referenceID": 7, "context": "\u2022 Gaussian process - Batch Upper Confidence Bound (GP-BUCB) (Desautels et al., 2014): this is a batch BO utilizing the variance of GP for finding the next point until the batch is filled.", "startOffset": 60, "endOffset": 84}, {"referenceID": 14, "context": "\u2022 Constant Liar (EI and UCB) (Ginsbourger et al., 2008; Ginsbourger, Le Riche, & Carraro, 2010): CL uses the predictive mean (from GP) to obtain new batch elements, implemented in GPyOpt toolbox.", "startOffset": 29, "endOffset": 95}, {"referenceID": 16, "context": "\u2022 Local Penalization (LP) (Gonz\u00e1lez et al., 2016): This is currently the state-of-the-art method for batch BO which has been demonstrated to outperform most of other baselines (Gonz\u00e1lez et al.", "startOffset": 26, "endOffset": 49}, {"referenceID": 16, "context": ", 2016): This is currently the state-of-the-art method for batch BO which has been demonstrated to outperform most of other baselines (Gonz\u00e1lez et al., 2016).", "startOffset": 134, "endOffset": 157}, {"referenceID": 16, "context": "\u03b2 is fixed to 2 (following the setting used in (Gonz\u00e1lez et al., 2016)), which allows us to compare the different batch designs using the same acquisition function.", "startOffset": 47, "endOffset": 70}, {"referenceID": 16, "context": "noisy points which are possibly close to the already detected ones due to the effect of penalizing the peaks (Gonz\u00e1lez et al., 2016).", "startOffset": 109, "endOffset": 132}, {"referenceID": 14, "context": "Constant Liar (Ginsbourger et al., 2008) and BUCB (Desautels et al.", "startOffset": 14, "endOffset": 40}, {"referenceID": 7, "context": ", 2008) and BUCB (Desautels et al., 2014) take time for reestimation of the GP when the fake observations are added and noticeably slower when the number of data points N is large.", "startOffset": 17, "endOffset": 41}, {"referenceID": 16, "context": "The Local Penalization approach (Gonz\u00e1lez et al., 2016) consumes a considerable amount of time to estimate the Lipschitz constant.", "startOffset": 32, "endOffset": 55}, {"referenceID": 16, "context": "B3O is generally competitive to LP (Gonz\u00e1lez et al., 2016) in terms of computation.", "startOffset": 35, "endOffset": 58}, {"referenceID": 44, "context": "In practice, however, Bayesian optimization has been shown (Bergstra, Bardenet, Bengio, & K\u00e9gl, 2011; Snoek et al., 2012; Thornton et al., 2013) to obtain better results in fewer experiments than required by a full grid search.", "startOffset": 59, "endOffset": 144}, {"referenceID": 46, "context": "In practice, however, Bayesian optimization has been shown (Bergstra, Bardenet, Bengio, & K\u00e9gl, 2011; Snoek et al., 2012; Thornton et al., 2013) to obtain better results in fewer experiments than required by a full grid search.", "startOffset": 59, "endOffset": 144}], "year": 2017, "abstractText": "Parameter settings profoundly impact the performance of machine learning algorithms and laboratory experiments. The classical grid search or trial-error methods are exponentially expensive in large parameter spaces, and Bayesian optimization (BO) offers an elegant alternative for global optimization of black box functions. In situations where the black box function can be evaluated at multiple points simultaneously, batch Bayesian optimization is used. Current batch BO approaches are restrictive in that they fix the number of evaluations per batch, and this can be wasteful when the number of specified evaluations is larger than the number of real maxima in the underlying acquisition function. We present the Budgeted Batch Bayesian Optimization (B3O) for hyper-parameter tuning and experimental design we identify the appropriate batch size for each iteration in an elegant way. To set the batch size flexible, we use the infinite Gaussian mixture model (IGMM) for automatically identifying the number of peaks in the underlying acquisition functions. We solve the intractability of estimating the IGMM directly from the acquisition function by formulating the batch generalized slice sampling to efficiently draw samples from the acquisition function. We perform extensive experiments for both synthetic functions and two real world applications machine learning hyper-parameter tuning and experimental design for alloy hardening. We show empirically that the proposed B3O outperforms the existing fixed batch BO approaches in finding the optimum whilst requiring a fewer number of evaluations, thus saving cost and time.", "creator": "LaTeX with hyperref package"}}}