{"id": "1703.04854", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Distributed-Representation Based Hybrid Recommender System with Short Item Descriptions", "abstract": "Collaborative filtering (CF) aims to build a model from users' past behaviors and/or similar decisions made by other users, and use the model to recommend items for users. Despite of the success of previous collaborative filtering approaches, they are all based on the assumption that there are sufficient rating scores available for building high-quality recommendation models. In real world applications, however, it is often difficult to collect sufficient rating scores, especially when new items are introduced into the system, which makes the recommendation task challenging. We find that there are often \"short\" texts describing features of items, based on which we can approximate the similarity of items and make recommendation together with rating scores. In this paper we \"borrow\" the idea of vector representation of words to capture the information of short texts and embed it into a matrix factorization framework. We empirically show that our approach is effective by comparing it with state-of-the-art approaches.", "histories": [["v1", "Wed, 15 Mar 2017 00:47:28 GMT  (666kb)", "http://arxiv.org/abs/1703.04854v1", "10 pages, 5 figures"]], "COMMENTS": "10 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["junhua he", "hankz hankui zhuo", "jarvan law"], "accepted": false, "id": "1703.04854"}, "pdf": {"name": "1703.04854.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["hejunh@mail2.sysu.edu.cn,"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.04 854v 1 [cs.I R] 15 Mar 201 7cisions made by other users, and use the model to recommend items for users. Despite the success of previous collaborative filtering approaches, they are all based on the assumption that sufficient ratings are available to create high-quality recommendation models. However, in the real world it is often difficult to collect sufficient ratings, especially when new items are introduced into the system, which makes the task of recommendation difficult. We find that there are often \"short\" texts describing characteristics of items on the basis of which we can approximate the similarity of items and make recommendations along with ratings. In this essay, we \"borrow\" the idea of vector representation of words to capture the information of short texts and embed it in a matrix factoring framework. We demonstrate empirically that our approach is effective by comparing it with modern approaches."}, {"heading": "1 Introduction", "text": "In fact, there are three ways to design recommender systems (Adomavicius and Tuzhilin, 2005), i.e. collaborative filtration (Breese et al., 1998), content-based filtration (Gopalan et al., 2014), and hybrid filtration of commender systems (Adomavicius and Tuzhilin, 2005), collaborative filtration (Breese et al., 1998), content-based filtration (Gopalan et al., 2014)."}, {"heading": "2 Related Work", "text": "Our work relates to distributed representations of words. In previous work, many models have been proposed to learn a distributed representation of words. Collobert and Weston (Collobert and Weston, 2008) propose a single revolutionary neural network called SENNA to output a variety of speech processing predictions. Mnih and Hinton (Mnih and Hinton, 2008) propose a fast hierarchical language model called HLBL, based on log bilinear in (Mnih and Hinton, 2007) along with a simple feature-based algorithm that outperforms non-hierarchical neural models in its assessments. Mikolov (Mikolov, 2012) suggests a new statistical language model, RNLM, based on RNN in (Mikolov et al., 2010). Huang et al. (Huang et al., 201p) propose a new model that enhances the global contextual prediction of the sec network of words, and Mikoloet."}, {"heading": "3 Problem Formulation", "text": "A rating matrix is defined by R like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR R like BR like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR like BR R like BR like BR R like BR like BR R like BR like BR R like BR like BR R like BR like BR like BR R R like BR like BR R R R R R like BR R R R R R R like BR R R R R R R R like BR R R R like BR like BR R R R R like BR like BR like BR R R R R like BR like BR like BR like BR like BR like BR like BR like BR like BR BR like BR like BR like BR BR like BR like BR like BR like BR like BR BR like BR like BR like BR like BR like BR like BR like BR like BR BR like BR like BR like BR like BR like BR like BR like BR BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR like BR R like BR R like BR R like BR R like R like R like R like BR R like R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR R like BR like BR like BR like BR like BR R like BR R like BR R like BR R like BR R like BR R like BR like BR like BR R like BR R like BR R like BR R like BR R like BR"}, {"heading": "4.1 Distributed representations of descriptions", "text": "Since the first step of the algorithm is 1, we strive to build the distributed representations of item descriptions with Q as input. We first learn the vector representations for words using the Skip-gram model with hierarchical softmax, which is an efficient method for learning high-quality vector representations of words from unstructured words (Mikolov et al., 2013c). The goal of the Skip-gram model is to learn vector representations for predicting surrounding words in a sentence or document. Faced with a corpus C consisting of a sequence of training words < w1, w2, w2,., wT >, where the Skip-gram model maximizes the average log word in a sentence or document. Faced with a corpus C consisting of a sequence of training words < w1, w2, wT >, the Skip-gram model maximizes the average word probability in a sentence or document."}, {"heading": "4.2 The hybrid model with item descriptions", "text": "The ratio of the hybrid model is based on the following four assumptions: Each user u and each element v are characterized by an unknown vector Vu controlled by a parameter Vu. In other words, the rating rate Ruv T should be close to UuBRV T v, i.e. the rating Ruv controlled by parameter Alpha is characterized as a result of the bridging of Uu and VV with unknown matrix BR. In other words, the rating rate Ruv T, i.e. the similar idea is used by (Pan and Yang, 2013)."}, {"heading": "4.3 The EM algorithm", "text": "In step 3 of algorithm 1, we want to learn the parameters BR, BL, WC, U and V using the EM approach. As the beginning of the EM approach, we initialize U and V using the SVD result of labeling L, since the labeling data L describe the \"high\" or general interest of users in items. Afterwards, we initialize BR, BL and WC using equations (5) and (6) using U and V, which are presented in section 4.3.2."}, {"heading": "4.3.1 Learning V and U", "text": "B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B B"}, {"heading": "4.3.3 Tradeoff between \u03bbL and \u03bbC", "text": "The reason for this is that the identification data contains accurate information, while the item description matrix C is estimated on the basis of distributed representations of descriptions. If the identification data is sparse, the noise problem in item descriptions may worsen. Thus, the positive influence of C only plays at the macroscopic level, but not at a microcosmic level. In the later stage of convergence, further use of C can reduce accuracy. In other words, the influence of C should be gradually reduced as the algorithm is executed. We therefore propose three options to adjust the value of \u03bbC as shown below. 1. Linear decline is the simplest model to specify the decrease by compensating the decrease of C as follows:"}, {"heading": "5 Experiments", "text": "In this section, we evaluate our RECF algorithm using two MovieLens and Douban2 datasets by comparing it with other four algorithms, SVD (Pan and Yang, 2013), CSVD (Pan and Yang, 2013), CSVD + Binary (Saveski and Mantrach, 2014), and CSVD + TFIDF (Saveski and Mantrach, 2014). \u2212 SVD is an approach that only uses ratings for building description information for building description recommendations. \u2212 CSVD is an approach that evaluates both ratings and labeling information for building description recommendations for building description recommendations. \u2212 You convert the object descriptions into binary matrix and tf-idf representations for building description recommendations for building description recommendations."}, {"heading": "5.1 Performance w.r.t. sparsity", "text": "In our RECF algorithm, we set a value of 0.2 and a value of 2.5 in the equation (2).The results are shown in Figures 2 and 3, where we varied the spareness from 1.4% to 0.21% in the Douban datasets, and in Figures 0.16% to 0.2% in the MovieLens datasets, and 2.5 in the Equation (2).The results are shown in Figures 2 and 3, where we vary the sparity from 1.4% to 0.21% in the Douban datasets, and in the descriptions from 0.16% to 0.2% in the MovieLens datasets, respectively, we can see that both MAE and RMSE become larger when the percentage of ratings in both datasets decreases."}, {"heading": "5.2 Tradeoff between \u03bbC and \u03bbL", "text": "Next, we would like to see the effects of \u03bbC in Equation (2). We have adjusted the compromise between \u03bbL and \u03bbC by varying the value of \u03bbC in relation to the number of iterations in RECF, as shown in Equation (9). As we can see from Equation (9), the value of \u03bbC is fixed to be m before the first convergence and 0 as soon as our RECF algorithm converges, where m is the default initial value of \u03bbC. We specify that m 2.5 and \u03bbL should be 0.2, as in the last subsection. We present the results in Figures 4 and 5. We find that the changes in power (i.e., curves) can be divided into two stages indicating two phases of convergence. The first phase is for the target parameter of the description matrix C, namely for Equation C. At the beginning of convergence, C weights more than L and dominates the convergence."}, {"heading": "6 Conclusion", "text": "In this article, we propose a novel RECF algorithm for researching article descriptions to improve recommendation accuracy through distributed representations of article descriptions. Using this vector representation, we transform article descriptions into vector representations and combine them with rating and labeling data to form a hybrid recommendation system. We show that our RECF approach is effective by comparing it with the state-of-the-art approaches that exploit article descriptions. In the future, we would like to examine more information in our algorithm framework, such as user profiles or reviews, to further improve recommendation accuracy."}], "references": [{"title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions", "author": ["Adomavicius", "Alexander Tuzhilin"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "Adomavicius et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Adomavicius et al\\.", "year": 2005}, {"title": "Empirical analysis of predictive algorithms for collaborative filtering", "author": ["David Heckerman", "Carl Myers Kadie"], "venue": "InUAI \u201998: Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Breese et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Breese et al\\.", "year": 1998}, {"title": "Hybrid recommender systems: Survey and experiments", "author": ["Robin D. Burke"], "venue": "User Model. UserAdapt. Interact.,", "citeRegEx": "Burke.,? \\Q2002\\E", "shortCiteRegEx": "Burke.", "year": 2002}, {"title": "Context-aware collaborative topic regression with social matrix factorization for recommender systems", "author": ["Chen et al.2014] Chaochao Chen", "Xiaolin Zheng", "Yan Wang", "Fuxing Hong", "Zhen Lin"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Machine Learning, Proceedings of the TwentyFifth International Conference (ICML", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "The VITA financial services sales support environment", "author": ["Klaus Isak", "Kalman Szabo", "Peter Zachar"], "venue": "In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence, July 22-26,", "citeRegEx": "Felfernig et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Felfernig et al\\.", "year": 2007}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Frome et al.2013] Andrea Frome", "Gregory S. Corrado", "Jonathon Shlens", "Samy Bengio", "Jeffrey Dean", "Marc\u2019Aurelio Ranzato", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Content-based recommendations with poisson factorization", "author": ["Gopalan et al.2014] Prem Gopalan", "Laurent Charlin", "David M. Blei"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Gopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2014}, {"title": "Wtf: The who to follow service at twitter", "author": ["Pankaj Gupta", "Ashish Goel", "Jimmy Lin", "Aneesh Sharma", "Dong Wang", "Reza Zadeh"], "venue": "In Proceedings of the 22Nd International Conference on World Wide Web,", "citeRegEx": "Gupta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2013}, {"title": "Improving word representations via global context and multiple word prototypes. In The 50th Annual Meeting of the Association for Computational Linguistics", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "Proceedings of the Conference,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc V. Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Hidden factors and hidden topics: understanding rating dimensions with review text", "author": ["McAuley", "Leskovec2013] Julian J. McAuley", "Jure Leskovec"], "venue": "In Proceedings of RecSys,", "citeRegEx": "McAuley et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McAuley et al\\.", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["Mikolov et al.2010] TomasMikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "TomasMikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "TomasMikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168", "author": ["Quoc V. Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Statistical language models based on neural networks. Presentation at Google, Mountain View, 2nd April", "author": ["Tom\u00e1\u0161 Mikolov"], "venue": null, "citeRegEx": "Mikolov.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Three new graphical models for statistical language modelling", "author": ["Mnih", "Hinton2007] Andriy Mnih", "Geoffrey E. Hinton"], "venue": "InMachine Learning, Proceedings of the Twenty-Fourth International Conference (ICML", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2008] Andriy Mnih", "Geoffrey E. Hinton"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2008}, {"title": "Transfer learning in heterogeneous collaborative filtering domains", "author": ["Pan", "Yang2013] Weike Pan", "Qiang Yang"], "venue": "Artif. Intell.,", "citeRegEx": "Pan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2013}, {"title": "Syntactic dependencies and distributed word representations for analogy detection and mining", "author": ["Qiu et al.2015] Likun Qiu", "Yue Zhang", "Yanan Lu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Qiu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2015}, {"title": "Item cold-start recommendations: learning local collective embeddings", "author": ["Saveski", "Mantrach2014] Martin Saveski", "Amin Mantrach"], "venue": "In Proceedings of RecSys,", "citeRegEx": "Saveski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saveski et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": ", movies, music, news, books, research articles, search queries, social tags, financial services (Felfernig et al., 2007), and Twitter followers (Gupta et al.", "startOffset": 97, "endOffset": 121}, {"referenceID": 8, "context": ", 2007), and Twitter followers (Gupta et al., 2013).", "startOffset": 31, "endOffset": 51}, {"referenceID": 1, "context": ", collaborative filtering (Breese et al., 1998), content-based filtering (Gopalan et al.", "startOffset": 26, "endOffset": 47}, {"referenceID": 7, "context": ", 1998), content-based filtering (Gopalan et al., 2014), and the hybrid filtering (Burke, 2002).", "startOffset": 33, "endOffset": 55}, {"referenceID": 2, "context": ", 2014), and the hybrid filtering (Burke, 2002).", "startOffset": 34, "endOffset": 47}, {"referenceID": 3, "context": "present a topic-model based approach to utilize the context and item information (Chen et al., 2014) to help with recommendation.", "startOffset": 81, "endOffset": 100}, {"referenceID": 16, "context": "Mikolov (Mikolov, 2012) proposes a new statistical language model, RNNLM, based on RNN in (Mikolov et al.", "startOffset": 8, "endOffset": 23}, {"referenceID": 9, "context": "(Huang et al., 2012) propose a new model which increases the global context-aware to enrich the semantic information of words.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "used it to make the language model pre-training of a new deep visual-semantic embedding model, as it has been shown to efficiently learn semantically-meaningful floating point representations of terms from unannotated text (Frome et al., 2013).", "startOffset": 223, "endOffset": 243}, {"referenceID": 20, "context": "(Qiu et al., 2015) explored distributed representations of words to detect analogies.", "startOffset": 0, "endOffset": 18}], "year": 2017, "abstractText": "Collaborative filtering (CF) aims to build a model from users\u2019 past behaviors and/or similar decisions made by other users, and use the model to recommend items for users. Despite of the success of previous collaborative filtering approaches, they are all based on the assumption that there are sufficient rating scores available for building high-quality recommendation models. In real world applications, however, it is often difficult to collect sufficient rating scores, especially when new items are introduced into the system, which makes the recommendation task challenging. We find that there are often \u201cshort\u201d texts describing features of items, based on which we can approximate the similarity of items and make recommendation together with rating scores. In this paper we \u201cborrow\u201d the idea of vector representation of words to capture the information of short texts and embed it into a matrix factorization framework. We empirically show that our approach is effective by comparing it with state-of-the-art approaches.", "creator": "LaTeX with hyperref package"}}}