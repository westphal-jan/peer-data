{"id": "1703.04887", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets", "abstract": "This paper proposes a new route for applying the generative adversarial nets (GANs) to NLP tasks (taking the neural machine translation as an instance) and the widespread perspective that GANs can't work well in the NLP area turns out to be unreasonable. In this work, we build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generative model (generator) which translates the source sentence into the target sentence as the traditional NMT models do and a discriminative model (discriminator) which discriminates the machine-translated target sentence from the human-translated sentence. From the perspective of Turing test, the proposed model is to generate the translation which is indistinguishable from the human-translated one. Experiments show that the proposed model achieves significant improvements than the traditional NMT model. In Chinese-English translation tasks, we obtain up to +2.0 BLEU points improvement. To the best of our knowledge, this is the first time that the quantitative results about the application of GANs in the traditional NLP task is reported. Meanwhile, we present detailed strategies for GAN training. In addition, We find that the discriminator of the proposed model shows great capability in data cleaning.", "histories": [["v1", "Wed, 15 Mar 2017 02:26:25 GMT  (1083kb,D)", "http://arxiv.org/abs/1703.04887v1", "9 pages , 3 figures, 3 tables"], ["v2", "Mon, 17 Apr 2017 02:24:05 GMT  (527kb,D)", "http://arxiv.org/abs/1703.04887v2", "More strong baselines, update the equation and pictures"]], "COMMENTS": "9 pages , 3 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zhen yang", "wei chen", "feng wang", "bo xu"], "accepted": false, "id": "1703.04887"}, "pdf": {"name": "1703.04887.pdf", "metadata": {"source": "CRF", "title": "Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets", "authors": ["Zhen Yang", "Wei Chen", "Feng Wang", "Bo Xu"], "emails": ["xubo}@ia.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "Machine translation is one of the important and traditional NLP tasks aimed at translating a source code based on a dynamically generated context. * Wei Chen is the corresponding author of this papertarget-language sentence automatically. Recently, with the rapid development of a deep neural network, neural machine translation (Kalchburner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2016; Ranzato et al., 2015), which directly uses a single neural network to transform the source sentence into the target sentence, has achieved state-of-the-art performance for multiple language pairs (Wu al., 2016; Bradbury and Socher, 2016). This end-to-end NMT typically consists of two recurrent neural networks."}, {"heading": "2 Related work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Neural machine translation", "text": "This subsection briefly describes the attention-based NMT model, which performs dynamic alignment and generation of the target sentence at the same time. NMT model generates the translation sentence by generating a target word at each time step. If an input sequence is x = (x1,.., xTx) and previous translated words (y1,.., yi \u2212 1), the probability of the next word is yi: p (yi | y1,.) (2) Here, f and g are nonlinear transformation functions that can be implemented as long-term storage network (LSTM) or gated recurrent unit (GRU), where si = f (si \u2212 1, yi \u2212 1, ci) (2) can be implemented as nonlinear transformation functions."}, {"heading": "2.2 Generative adversarial net", "text": "Generative adversarial network, in which a generative model is trained to generate results in order to deceive the discriminator, has great success in computer vision and has been widely applied to image generation.The conditional adversarial networks apply an extension of the generative adversarial network to a conditional setting that allows the networks to condition any external data.However, to our knowledge, this idea has not been applied in traditional NLP tasks with comparable success and few quantitative experimental outcomes.Some recent work has begun to apply generative adversarial training to the NLP domain: (Chen et al., 2016) apply the idea of generative adversarial training to sentimental analyses and (Zhang et al, 2017) use the idea of domaining the adaptation problems.For sequence generation problems, (Yu et al., 2016) we use the sequence adversarial work for the adversarial problem."}, {"heading": "3 The CSGAN-NMT", "text": "In this section we describe in detail the CSGANNMT, which consists of a generator G, which generates the target language sentence based on the source language sentence, and a discriminator D, which distinguishes the machine-generated sentence from the man-made sentence. The process of sentence generation is considered a sequence of actions carried out according to a policy regulated by the generator, and we take the gradient training strategies that are the same as (Yu et al., 2016)."}, {"heading": "3.1 Generator", "text": "Similar to the traditional NMT model, Generator G generates the target language sentence based on the source language input sentence. It defines the policies that generate the target sentence y with respect to the source sentence x. The generator adopts exactly the same architecture as the traditional NMT model. Note that we do not adopt the specific architecture of the generator. In this case, we adopt the highly attention-based NMT model implemented as an open source dl4mt * system as the generator."}, {"heading": "3.2 Discriminator", "text": "To test the effectiveness of the discriminator, we propose two different architectures for the discriminator: the CNN-based and RNN-based one.CNN-based operation Since the generated sentence has a variable length, the CNN padding is used to transform the sentence into a fixed length T sequence, which is the maximum length for the input sequence set by the user before. Given the source code sequence x1,.. xT and the target language sequence y1,., yT, we build the source language X1: T or the target matrix Y1: T respectively as: X1; x2;. xT (7) and Y1: Y."}, {"heading": "3.3 Policy gradient training", "text": "In the following (Yu et al., 2016), the objective of generator G is defined in such a way that we keep the reward line constant (using the initial state to maximize the expected final reward. Formally, the objective function is calculated as follows: J (\u03b8) = \u2211 yt reward (yt | y1: t \u2212 1, x) \u00b7 RG\u03b8D (((y1: t \u2212 1, x), yt) (13), where the action value function of a target language set gives the source set, i.e. the expected accumulative reward from the state (y1: t \u2212 1, x), taking action yt, and following the policy G\u03b8 function, we consider the estimated probability of being after discriminator D as reward: RGD (y1: T \u2212 1, x), yT: D (x, y1: T) \u2212 b (x, y1: T)."}, {"heading": "4 Training strategies", "text": "To make this work easier to reproduce, this paper gives detailed strategies for training the CSGAN-NMT model. First, we use the maximum discrimination probability estimate to pre-train the generator on the parallel training until the best translation performance (measured by the BLEU metric) is achieved. Then, we generate the machine-generated sentences by using the generator to decrypt the training data. We simply use the greedy sampling method instead of the beam search method for the decryption. Hence, it is very fast to decrypt the entire training. Next, pre-train the discriminator on the combination of the true parallel data and the machine-generated data until the classification accuracy does at f.Finally we joint train the generator and discriminator is trained with the policy gradient."}, {"heading": "5 Experiments and Results", "text": "In this section we explain our experimental results on the CSGAN-NMT model for Chinese-English translation tasks. The base model is the open source NMT system dl4mt. Note that the generator of the CSGAN-NMT is identical to the base model."}, {"heading": "5.1 Setup", "text": "For the Sino-English translation task, the training data consists of 1.0M pairs of sentences, which are randomly extracted from the LDC corpora. We select the NIST02 as the development set. To test, we use the NIST03, NIST04, and NIST05 records. In our experiments, we apply word-level translation, and the Chinese sentences are segmented in advance. To speed up the training process, the sentences of more than 50 words are removed. We limit the vocabulary in both Chinese and English to the most 30K words, and the words outside the vocabulary are replaced by UNK. The embedding dimension of the word is set to 512 and the size of the hidden layer is 1024. The other hyperparameters are specified in Section 4. The training time for the proposed CSCSNT model is approximately 3 days."}, {"heading": "5.2 CNN or RNN for discriminator", "text": "First, we test different architectures for the discriminator, CNN-based or RNN-based (LSTM for example).Figure 3 shows the BLEU values for the development set tested at different stages of time. We can find that the performance of the RNN-based (LSTM and BiLSTM) discriminators deteriorates rapidly over time.What is even more remarkable is that the performance of the discriminator based on the BiLSTM plummets after several times.Training for the RNN-based discriminator is not stable. On the contrary, the CNN-based discriminator performs very well. Empirically, the classification accuracy of RNN-based discriminators can easily be achieved with a few updates up to 0.9, which is too strong for the generator.The sets generated by the generator can be easily detected by the strong discriminator and the generator is constantly discouraged."}, {"heading": "5.3 Results on Chinese-English translation", "text": "Compared to the base model, the proposed CSGAN-NMT model leads to an improvement to an average of + 1.6 BLEU points (see (1) and (3) in Table 1. Of course, it is doubtful that this improvement is due much more to the low learning rate of the optimization method than to the CSGAN-NMT itself. To remove this doubt and verify the effectiveness of the proposed model, we use a stochastic downward optimization method (with the learning rate of 0.0001 used in CSGAN-NMT) to refine the base model, and we only get an improvement of + 0.7 BLEU points (see (2) in Table 1. There is an average gap of 1.0 BLEU points (0.71 vs 1.62) between the fine-grained base model and the CSGAN-NMT model (see (2) and (3) in Table 1). Additionally, we can achieve a further improvement of the BLEU-NMT base table (0.4 points) if this improvement is not completed (see NMT table)."}, {"heading": "5.4 Initial accuracy of the discriminator", "text": "The initial accuracy f of the discriminator, which can be considered a hyperparameter, can be carefully controlled during the training. A natural question is when to finish the training. Do we need to pretrain the discriminator until its accuracy is as high as possible? To test the effects of the initial accuracy of the discriminator, we train five discriminators that have accuracy as 0.6, 0.7, 0.8, 0.9, and 0.95, respectively. With the five discriminators, we train five different CSGAN-NMT models and test their translation performance on the development set at regular intervals. Figure 4 reports the result and we can find that the initial accuracy of the discriminator has a major impact on the translation performance of the proposed model. Starting from Figure 4, the initial accuracy of the discriminator must be carefully adjusted and whether it is set too high (0.9 and 0.95) or too low (0.6 and 0.7), the CSGAN-NMT reaches its accuracy from 82 to the irical discriminator."}, {"heading": "N NIST02 NIST03 NIST04 NIST05", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.5 Sample times for Monte Carlo search", "text": "We are also curious about the sample time N for the Monte Carlo search. If N is set as a small number, the intermediate reward calculated as Equation 16 may be wrong, and otherwise the calculation will be very time-consuming. There is a trade-off between accuracy and computational complexity. Table 2 shows the translation performance of the CSGAN-NMT on the test sets when the N is set from 5 to 30. From Table 2, the proposed CSGAN-NMT model does not achieve an improvement than the baseline when N is set to less than 15. With N set as 30, we get little improvement than the model where N is set to 20. However, the training time has exceeded our tolerance."}, {"heading": "5.6 Discriminator for data cleaning", "text": "Since the discriminator of the CSGAN-NMT directly indicates the probability that the source language sentence is a human-generated sentence, we check the feasibility of applying the discriminator to data cleansing. Therefore, when we have finished training the CSGAN-NMT model, the accuracy of the discriminator is about 0.6, which is a little weak in the handling of the data cleansing task. Therefore, we train the discriminator for four epochs (with the accuracy of 0.95). By feeding the parallel training data into the discriminator, we then get a probability of being humanely translated for each sentence pair. We select a series of examples (s1) from the training data based on the probability in descending order. Additionally, we randomly select the other sentence s2, which has the same number of examples with s1. Two traditional NMT models are trained on the data sets s1 or s2. The results are presented in Table 3."}, {"heading": "6 Conclusions and Future work", "text": "We test different architectures (RNN-based and CNN-based) on the discriminator of the CSGAN-NMT. Experimental results show that our proposed model can significantly outperform the strong attention-based NMT baseline and that the CNN-based discriminator performs better than the RNN-based one. Furthermore, we offer detailed training strategies for the CSGAN-NMT model. Furthermore, we find that the discriminator in the CSGAN-NMT has great skills in data cleansing. In the future, we would like to try a multi-adversarial framework consisting of multi-discriminators and generators for generative hostile training. Furthermore, we plan to test our method in other NLP tasks, such as dialogue system and question answering. Since higher BLEU points do not guarantee a better set, another interesting direction is to apply the discriminator to the more consistent performance of the translation."}, {"heading": "Acknowledgments", "text": "We would like to thank Xu Shuang for producing the data used in this work. We would also like to thank Chen Zhineng, Geng Wang, Wang Wenfu, Zhang Xiaowei and Wang Chunqi for their valuable discussions about this work."}], "references": [{"title": "Re-evaluating the role", "author": ["Miles Osborne"], "venue": null, "citeRegEx": "Osborne.,? \\Q2006\\E", "shortCiteRegEx": "Osborne.", "year": 2006}, {"title": "Deep generative image", "author": ["Rob Fergus"], "venue": null, "citeRegEx": "Fergus,? \\Q2015\\E", "shortCiteRegEx": "Fergus", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167", "author": ["Ioffe", "Szegedy2015] Sergey Ioffe", "Christian Szegedy"], "venue": null, "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": null, "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Professor forcing: A new algorithm for training recurrent networks", "author": ["Lamb et al.2016] Alex Lamb", "Anirudh Goyal", "Ying Zhang", "Saizheng Zhang", "Aaron Courville", "Yoshua Bengio"], "venue": "Advances In Neural Information Processing Systems,", "citeRegEx": "Lamb et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lamb et al\\.", "year": 2016}, {"title": "Adversarial learning for neural dialogue generation", "author": ["Li et al.2017] Jiwei Li", "Will Monroe", "Tianlin Shi", "Alan Ritter", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1701.06547", "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732", "author": ["Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Minimum risk training for neural machine translation", "author": ["Shen et al.2015] Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "arXiv preprint arXiv:1512.02433", "citeRegEx": "Shen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Seqgan: Sequence generative adversarial nets with policy gradient. The Association for the Advancement of Artificial Intelligence 2017", "author": ["Yu et al.2016] Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Generating text via adversarial training", "author": ["Zhang et al.2016] Yizhe Zhang", "Zhe Gan", "Lawrence Carin"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Aspect-augmented adversarial networks for domain adaptation", "author": ["Zhang et al.2017] Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola"], "venue": "arXiv preprint arXiv:1701.00188", "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}, {"title": "Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126", "author": ["Zhao et al.2016] Junbo Zhao", "Michael Mathieu", "Yann LeCun"], "venue": null, "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Recently, with the rapid development of deep neural network, the neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Ranzato et al., 2015) which leverages a single neural network directly to transform the source sentence into the target sentence, has obtained state-of-the-art performance for several language pairs (Wu et al.", "startOffset": 92, "endOffset": 211}, {"referenceID": 7, "context": "Recently, with the rapid development of deep neural network, the neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Ranzato et al., 2015) which leverages a single neural network directly to transform the source sentence into the target sentence, has obtained state-of-the-art performance for several language pairs (Wu et al.", "startOffset": 92, "endOffset": 211}, {"referenceID": 7, "context": "(Ranzato et al., 2015) indicates that loss function of the maximum likelihood estimation is only defined at the word level instead of sentence level.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "(Shen et al., 2015) gives a solution by introducing the minimum risk training from the statistical machine translation (SMT).", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "the BLEU point is computed as the geometric mean of the modified n-gram precisions (Papineni et al., 2002), we conclude that almost all of the prior NMT models are trained to cover more n-grams with the ground target sentence (MLE can be viewed as training the NMT to cover more 1-gram with the target sentence).", "startOffset": 83, "endOffset": 106}, {"referenceID": 13, "context": "This kind of adversarial training achieves a winwin situation when the generator and discriminator reaches a Nash Equilibrium (Zhao et al., 2016).", "startOffset": 126, "endOffset": 145}, {"referenceID": 12, "context": ", 2016) apply the idea of generative adversarial training to sentiment analysis and (Zhang et al., 2017) use the idea to domain adaptation tasks.", "startOffset": 84, "endOffset": 104}, {"referenceID": 10, "context": "For sequence generation problem, (Yu et al., 2016) leverage policy gradient reinforcement learning to back-propagate the reward from the discriminator, showing presentable results for poem generation, speech language generation and music generation.", "startOffset": 33, "endOffset": 50}, {"referenceID": 11, "context": "Similarly, (Zhang et al., 2016) generate the text from random noise via adversarial training.", "startOffset": 11, "endOffset": 31}, {"referenceID": 5, "context": "In parallel to our work, (Li et al., 2017) propose a similar conditional sequence generative adversarial training for dialogue generation.", "startOffset": 25, "endOffset": 42}, {"referenceID": 10, "context": "The sentence generation process is viewed as a sequence of actions that are taken according to a policy regulated by the generator and we take the policy gradient training strategies which is the same as (Yu et al., 2016).", "startOffset": 204, "endOffset": 221}, {"referenceID": 10, "context": "Following (Yu et al., 2016), the objective of the generator G is defined as to generate a sequence from the start state to maximize its expected end reward.", "startOffset": 10, "endOffset": 27}, {"referenceID": 10, "context": "Following (Yu et al., 2016), the gradient of the objective function J(\u03b8) w.", "startOffset": 10, "endOffset": 27}, {"referenceID": 4, "context": "To alleviate this issue, we adopt the professor forcing approach which is similar to (Lamb et al., 2016).", "startOffset": 85, "endOffset": 104}], "year": 2017, "abstractText": "This paper proposes a new route for applying the generative adversarial nets (GANs) to NLP tasks (taking the neural machine translation as an instance) and the widespread perspective that GANs can\u2019t work well in the NLP area turns out to be unreasonable. In this work, we build a conditional sequence generative adversarial net which comprises of two adversarial sub models, a generative model (generator) which translates the source sentence into the target sentence as the traditional NMT models do and a discriminative model (discriminator) which discriminates the machinetranslated target sentence from the humantranslated sentence. From the perspective of Turing test, the proposed model is to generate the translation which is indistinguishable from the human-translated one. Experiments show that the proposed model achieves significant improvements than the traditional NMT model. In Chinese-English translation tasks, we obtain up to +2.0 BLEU points improvement. To the best of our knowledge, this is the first time that the quantitative results about the application of GANs in the traditional NLP task is reported. Meanwhile, we present detailed strategies for GAN training. In addition, We find that the discriminator of the proposed model shows great capability in data cleaning.", "creator": "LaTeX with hyperref package"}}}