{"id": "1703.04929", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "SyntaxNet Models for the CoNLL 2017 Shared Task", "abstract": "We describe a baseline dependency parsing system for the CoNLL2017 Shared Task. This system, which we call \"ParseySaurus,\" uses the DRAGNN framework [Kong et al, 2017] to combine transition-based recurrent parsing and tagging with character-based word representations. On the v1.3 Universal Dependencies Treebanks, the new system outpeforms the publicly available, state-of-the-art \"Parsey's Cousins\" models by 3.47% absolute Labeled Accuracy Score (LAS) across 52 treebanks.", "histories": [["v1", "Wed, 15 Mar 2017 04:57:17 GMT  (141kb)", "http://arxiv.org/abs/1703.04929v1", "Tech report"]], "COMMENTS": "Tech report", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chris alberti", "daniel", "or", "ivan bogatyy", "michael collins", "dan gillick", "lingpeng kong", "terry koo", "ji ma", "mark omernick", "slav petrov", "chayut thanapirom", "zora tung", "david weiss"], "accepted": false, "id": "1703.04929"}, "pdf": {"name": "1703.04929.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Chris Alberti", "Daniel Andor", "Ivan Bogatyy", "Michael Collins", "Dan Gillick", "Lingpeng Kong", "Terry Koo", "Ji Ma", "Mark Omernick", "Slav Petrov", "Chayut Thanapirom", "Zora Tung", "David Weiss"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 170 3.04 929v 1 [cs.C L] 15 MWe describe a base-based dependency analysis system for the CoNLL2017 Shared Task. This system, which we call \"ParseySaurus,\" uses the DRAGNN framework [Kong et al., 2017] to combine transition-based recurring parsing and tagging with character-based word representation. In v1.3 Universal Dependencies Treebanks, the new system outperforms publicly available, state-of-the-art \"Parsey's Cousins\" models by 3.47% absolute Labeled Accuracy Score (LAS) over 52 tree trunks."}, {"heading": "1 Introduction", "text": "Universal dependences1 are enjoying growing popularity due to the linguistic consistency and wide language coverage of the data provided. The initiative has connected researchers around the world and now includes 64 tree banks in 45 languages. Therefore, it is not surprising that the Computational Natural Language Learning (CoNLL) Conference 2017 will offer a joint task on \"Multilingual Parsing from Raw Text to Universal Dependencies.\" In order to facilitate further research on multilingual parsing and also allow small teams to participate in the joint task, we are publishing basic implementations that correspond to our best models. This short essay describes (1) the model structure used in these models, (2) how the models were formed, and (3) an empirical evaluation comparing these models with those in Andor et al. [2016]. Our model uses the DRAGNN framework [Kong et al., 2017] to improve Andor et al. [2016] with dynamically based net-based recursive models based on this recursive one."}, {"heading": "2 Character-based representation", "text": "Recent work has shown that learned subword representations can improve both static word embedding and manually extracted function sets to describe word morphology. Jozefowicz et al. [2016] use a revolutionary model of the letters in each word for language modeling. Similarly, Ling et al. [2015a, b] use a bidirectional LSTM over letters in each word for analysis and machine translation."}, {"heading": "3 Model", "text": "\"We are the first we put into the world,\" he says. \"We are the first we put into the world.\" \"We are the first we put into the world.\" \"We are the first we put into the world.\" \"We are the first we put into the world.\" \"We are the first we put into the world.\" \"We are the first we put into the world.\" \"\" We are the first we put into the world. \"\" The second, third, third, fourth, fourth, fourth, fourth, fourth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, seventh, fifth, seventh, fifth, seventh, fifth, fifth, fifth, seventh, seventh, seventh, seventh, fifth, seventh, seventh, seventh, fifth, seventh, seventh, seventh, seventh, fourth, fourth, fourth, fourth, fourth, fourth, fourth, fourth, fourth, fourth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, fifth, seventh, seventh, seventh, seventh, fifth, seventh, fifth, seventh, seventh, fifth, seventh, seventh, seventh, seventh, seventh, seventh, fourth, fourth, fourth, fourth, fourth, fourth, fourth, fourth, fourth, fourth, fourth, fourth, fourth, seventh, fourth, fourth, fourth, fourth, fourth, fourth, seventh, fourth, fourth, seventh, fourth, seventh, seventh, fourth, seventh, fourth, fourth, seventh, fourth, seventh, seventh, fourth, seventh, seventh, fourth, seventh, seventh, fourth, fourth, seventh, seventh, fourth, fourth, seventh, fourth, seventh, seventh, fourth, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, fourth, seventh, seventh, seventh, seventh, fourth, seventh, seventh, seventh, seventh, seventh, fourth, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, fourth, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, seventh, fourth, seventh, seventh, seventh, seventh, seventh,"}, {"heading": "3.2 Hyperparameters", "text": "Like the above ratio, many hyperparameters, including design decisions, were tuned to find reasonable values before we trained all 64 base models. While the full recipe can be decoded from the code, here are some key points for practitioners: \u2022 We use LSTM standardization [Ba et al., 2016] in all of our networks, both LSTM and the recurrent parser's relay network cell. \u2022 We always project the hidden LSTM representations down from 256 \u2192 64 as we move from one component to another. \u2022 We use moving averages of the parameters at inference time. \u2022 We use the following ADAM recipe: \u03b21 = \u03b22 = 0.9 and specify that they must be one of 10 \u2212 3, 10 \u2212 4, 10 \u2212 5 (typically 10 \u2212 4). \u2022 We normalize all gradients to have unit standards before applying the ADAM updates. \u2022 We always use a synchronous input (both Drop7 and 0.4)."}, {"heading": "4 Comparison to Parsey\u2019s Cousins", "text": "Since the test kit is not available for the competition, we use v1.3 of the Universal Dependencies Treebanks to compare it with the state of the art in 52 languages. Our results are in Table 1. We note that the new model in some cases drastically exceeds the original SyntaxNet baselines (e.g. in Latvia by almost 12% absolute LAS.) We note that this is not an exhaustive experiment and that further studies are warranted in the future. Nonetheless, these results show that the new baselines can be compared very cheaply with at least one publicly available state-of-the-art baseline. We provide pre-trained models for all 64 treebanks in the CoNLLL2017 Shared Task on the SyntaxNet website. All source codes and data are publicly available. Updates can be found on the task website."}, {"heading": "Acknowledgements", "text": "We would like to thank our employees at Universal Dependencies and Conll2017 Shared, as well as Milan Straka from UD-Pipe. We would also like to thank all members of the Google Parsing Team (current and former) who made this publication possible."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "When and why are log-linear models self-normalizing", "author": ["Jacob Andreas", "Dan Klein"], "venue": "In HLT-NAACL,", "citeRegEx": "Andreas and Klein.,? \\Q2015\\E", "shortCiteRegEx": "Andreas and Klein.", "year": 2015}, {"title": "Improved transition-based parsing by modeling characters instead of words with LSTMs", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.01704,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Multilingual language processing from bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": "arXiv preprint arXiv:1512.00103,", "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Dragnn: A transition-based framework for dynamically connected neural networks. ArXiV, 2017", "author": ["Lingpeng Kong", "Chris Alberti", "Daniel Andor", "Ivan Bogatyy", "David Weiss"], "venue": null, "citeRegEx": "Kong et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2017}, {"title": "A character-word compositional neural language model for finnish", "author": ["Matti Lankinen", "Hannes Heikinheimo", "Pyry Takala", "Tapani Raiko", "Juha Karhunen"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Lankinen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lankinen et al\\.", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Decoding with large-scale neural", "author": ["2016. Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang"], "venue": null, "citeRegEx": "Vaswani et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2016}, {"title": "Stack-propagation: Improved representation learning for syntax", "author": ["Yuan Zhang", "David Weiss"], "venue": null, "citeRegEx": "Zhang and Weiss.,? \\Q2013\\E", "shortCiteRegEx": "Zhang and Weiss.", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "This system, which we call \u201cParseySaurus,\u201d uses the DRAGNN framework [Kong et al., 2017] to combine transitionbased recurrent parsing and tagging with character-based word representations.", "startOffset": 69, "endOffset": 88}, {"referenceID": 8, "context": "Our model uses on the DRAGNN framework [Kong et al., 2017] to improve upon Andor et al.", "startOffset": 39, "endOffset": 58}, {"referenceID": 0, "context": "This short paper describes (1) the model structure employed in these models, (2) how the models were trained and (3) an empirical evaluation comparing these models to those in Andor et al. [2016]. Our model uses on the DRAGNN framework [Kong et al.", "startOffset": 176, "endOffset": 196}, {"referenceID": 0, "context": "This short paper describes (1) the model structure employed in these models, (2) how the models were trained and (3) an empirical evaluation comparing these models to those in Andor et al. [2016]. Our model uses on the DRAGNN framework [Kong et al., 2017] to improve upon Andor et al. [2016] with dynamically constructed, recurrent transition-based models.", "startOffset": 176, "endOffset": 292}, {"referenceID": 6, "context": "Jozefowicz et al. [2016] use a convolutional model over the characters in each word for language modeling.", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": "In principle, this structure permits fully dynamic word representations based on left context (unlike previous work) and simplifies recurrent computation at the word level (unlike previous work with standard stacked LSTMs [Gillick et al., 2015]).", "startOffset": 222, "endOffset": 244}, {"referenceID": 2, "context": "Like Ballesteros et al. [2015], we use character-based LSTMs to improve the Stack-LSTM Dyer et al.", "startOffset": 5, "endOffset": 31}, {"referenceID": 2, "context": "Like Ballesteros et al. [2015], we use character-based LSTMs to improve the Stack-LSTM Dyer et al. [2015] model for dependency parsing, but we share a single LSTM run over the entire sentence.", "startOffset": 5, "endOffset": 106}, {"referenceID": 8, "context": "\u2022 The recurrent compositional parsing model [Kong et al., 2017] predicts parse tree left-to-right using the arc-standard transition system.", "startOffset": 44, "endOffset": 63}, {"referenceID": 8, "context": "Our model combines the recurrent multi-task parsing model of Kong et al. [2017] with character-based representations learned by a LSTM.", "startOffset": 61, "endOffset": 80}, {"referenceID": 8, "context": "Our model combines the recurrent multi-task parsing model of Kong et al. [2017] with character-based representations learned by a LSTM. Given a tokenized text input, the model processes as follows: \u2022 A single LSTM processes the entire character string (including whitespace) left-to-right. The last hidden state in a given token (as given by the word boundaries) is used to represent that word in subsequent parts of the model. \u2022 A single LSTM processes the word representations (from the first step) in right-to-left order. We call this the \u201clookahead\u201d model. \u2022 A single LSTM processes the lookahead representations right-to-left. This LSTM has a softmax layer which is trained to predict POS tags, and we refer to it as the \u201ctagger\u201d model. \u2022 The recurrent compositional parsing model [Kong et al., 2017] predicts parse tree left-to-right using the arc-standard transition system. Given a stack s and a input pointer to the buffer i, the parser dynamically links and concatenates the following input representations: \u2013 Recurrently, the two steps that last modified the so and s1 (either SHIFT or REDUCE operations). \u2013 From the tagger layer, the hidden representations for s0, s1, and i. \u2013 From the lookahead layer, the hidden representation for i. \u2013 All are projected to 64 dimensions before concatenating. \u2013 The parser also extracts 12 discrete features for previously predicted parse labels, the same as in Kong et al. [2017]. At inference time, we use beam decoding in the parser with a beam size of 8.", "startOffset": 61, "endOffset": 1429}, {"referenceID": 8, "context": "1 Training We train using the multi-task, maximum-likelihood \u201cstack-propagation\u201d method described in Kong et al. [2017] and Zhang and Weiss [2016].", "startOffset": 101, "endOffset": 120}, {"referenceID": 8, "context": "1 Training We train using the multi-task, maximum-likelihood \u201cstack-propagation\u201d method described in Kong et al. [2017] and Zhang and Weiss [2016]. Specifically, we use the gold labels to alternate between two updates:", "startOffset": 101, "endOffset": 147}, {"referenceID": 9, "context": "\u201dVaswani et al. [2013], Andreas and Klein [2015].", "startOffset": 1, "endOffset": 23}, {"referenceID": 0, "context": "[2013], Andreas and Klein [2015]. With this modification to the softmax, the log scores of the model are encouraged (but not constrained) to sum to one.", "startOffset": 8, "endOffset": 33}, {"referenceID": 0, "context": "We find that this helps mitigate some of the bias induced by local normalization Andor et al. [2016], while being fast and efficient to train.", "startOffset": 81, "endOffset": 101}], "year": 2017, "abstractText": "We describe a baseline dependency parsing system for the CoNLL2017 Shared Task. This system, which we call \u201cParseySaurus,\u201d uses the DRAGNN framework [Kong et al., 2017] to combine transitionbased recurrent parsing and tagging with character-based word representations. On the v1.3 Universal Dependencies Treebanks, the new system outpeforms the publicly available, state-of-the-art \u201cParsey\u2019s Cousins\u201d models by 3.47% absolute Labeled Accuracy Score (LAS) across 52 treebanks.", "creator": "LaTeX with hyperref package"}}}