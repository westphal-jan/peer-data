{"id": "1703.05123", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Character-based Neural Embeddings for Tweet Clustering", "abstract": "In this paper we show how the performance of tweet clustering can be improved by leveraging character-based neural networks. The proposed approach overcomes the limitations related to the vocabulary explosion in the word-based models and allows for the seamless processing of the multilingual content. Our evaluation results and code are available on-line at", "histories": [["v1", "Wed, 15 Mar 2017 12:37:22 GMT  (295kb,D)", "https://arxiv.org/abs/1703.05123v1", "Accepted at the SocialNLP 2017 workshop held in conjunction with EACL 2017, April 3, 2017, Valencia, Spain"], ["v2", "Thu, 16 Mar 2017 08:57:29 GMT  (295kb,D)", "http://arxiv.org/abs/1703.05123v2", "Accepted at the SocialNLP 2017 workshop held in conjunction with EACL 2017, April 3, 2017, Valencia, Spain"]], "COMMENTS": "Accepted at the SocialNLP 2017 workshop held in conjunction with EACL 2017, April 3, 2017, Valencia, Spain", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["svitlana vakulenko", "lyndon nixon", "mihai lupu"], "accepted": false, "id": "1703.05123"}, "pdf": {"name": "1703.05123.pdf", "metadata": {"source": "CRF", "title": "Character-based Neural Embeddings for Tweet Clustering", "authors": ["Svitlana Vakulenko", "Lyndon Nixon", "Mihai Lupu"], "emails": ["svitlana.vakulenko@wu.ac.at", "nixon@modultech.eu", "mihai.lupu@tuwien.ac.at"], "sections": [{"heading": null, "text": "In this paper we show how the performance of tweet clusters can be improved by using character-based neural networks. The proposed approach overcomes the constraints associated with the vocabulary explosion in word-based models and enables seamless processing of multilingual content. Our evaluation results and code are available online."}, {"heading": "1 Introduction", "text": "Our application scenario, as part of the InVID project2, is based on the needs of professional journalists responsible for timely coverage of breaking news. News often appears exclusively or just before it appears in traditional news media. Social media is also responsible for the rapid spread of inaccurate or incomplete tweets (rumors). Therefore, it is important to provide efficient tools that allow journalists to quickly detect breaking news on social media (Petrovic et al., 2013).The SNOW 2014 Data Challenge set the task of extracting newsworthy topics from Twitter. Results of the challenge confirmed that the task is ambitious: The best result was 0.4 F measurement. Breaking news detection comprises 3 subtasks: selection, clustering and ranking of tweets. In this paper, we address the task of tweet clustering as one of the key subtasks needed to effectively detect breaking news from Twitter. Traditional approaches to clustering textual documents include the creation of a documentation matrix."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Breaking news detection", "text": "In recent years, continuous efforts have been made to develop effective and efficient algorithms capable of recognizing newsworthy topics in the Twitter stream (Hayashi et al., 2015; Ifrim et al., 2014; Vosecky et al., 2013; Wurzer et al., 2015). These current, state-of-the-art approaches are based on the sack-of-words document model, which results in high-dimensional, sparse representations that do not scale well and are unaware of semantic similarities, such as paraphrases. The problem is evident in tweets that contain short texts with a long tail of rare slang and misspelled words. Performance of such approaches via Twitter datasets is very low, with an F measurement of up to 0.2 compared to commented Wikipidea articles as reference topics (Wurzer et al., 2015) and 0.4 over the curated topic pool (Papadopoual, 2014)."}, {"heading": "2.2 Neural embeddings", "text": "Artificial neural networks (ANNs) enable the generation of dense vector representations (embeddings). Word2vec (Mikolov et al., 2013) is by far the most popular approach, collecting the random statistics of words that efficiently summarize their semantics. Brigadir et al. (2014) showed encouraging results by using the word2vec Skip-gram model to generate event timelines from tweets. Moran et al. (2016) achieved an improvement over the state-of-the-art recognition of first story (FSD) by adding their semantically related terms to tweets, allowing word2vec.Neural embeddings to be efficiently generated at character level as well. They repeatedly exceeded the word-level baselines in the tasks of language modeling (Kim et al., 2016) by pre-classifying parts of the language tag (Santos and the textual model, Zadrozny, 2014 and Dasrozny)."}, {"heading": "3 Experimental Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Dataset", "text": "Description and pre-processing. We use SNOW 2014 test data sets (Papadopoulos et al., 2014) in our review. It contains the IDs of about 1 million tweets that were produced within 24 hours. We have retrieved 845,626 tweets from Justin Justin Justin Justin API because other tweets have already been deleted from the platform. The pre-processing process: Remove RT prefixes, URLs and user mentions, bring all characters to lowercase letters and separate punctuation with spaces (the later tweet is only required for the word level baseline).The data set is further divided into 5 subsets that correspond to the 1-hour time intervals (18: 00, 22: 00, 23: 15, 01: 30) that are commented on with the list of current news topics. In total, we have 48,399 tweets for clustering rating; the majority of them (42,758 tweets) are in English."}, {"heading": "3.2 Tweet representation approaches", "text": "This approach consists of two steps: (1) forming a neural network to predict hashtags using the subset of tweets that in our case contain hashtags (88,148 tweets); (2) encoding: use the trained model to create tweet embeddings for all tweets, whether they contain hashtags or not. We use Tweet2Vec, a bi-directional recurrent neural network that consumes textual input as a sequence of characters; and the network architecture includes two Gated Recurrent Units (GRUs) (Cho al, 2014)."}, {"heading": "3.3 Clustering", "text": "To cluster tweet vectors (character-based tweet embedding generated by the neural network for Tweet2Vec evaluation or the document-term matrix for TweetTerm), we use the hierarchical implementation of the clustering algorithm from the Fastcluster library (M\u00fclner, 2013). Hierarchical clustering involves calculating pairwise distances between the tweet vectors, followed by linking them to a single dendrogram. There are several distance metrics (Euclidean, Manhattan, Kosinus, etc.) and linking methods for comparing distances (single, average, complete, weighted, etc.) We evaluated the performance of different methods using the copper correlation coefficient (CPCC) (Sokal and Rohlf, 1962) and found the most powerful combination: Euclidean distance and average linking methods per cluster cluster, which can produce different data sets for a single cluster hierarchical cluster."}, {"heading": "3.4 Distance threshold selection", "text": "The grid search helps us to determine the optimal distance threshold for dendrogram shutdown. We compiled a list of values in the range of 0.1 to 1.5 with 0.1 steps and examined their performance in terms of mapping ground-truth clusters. We create flat clusters for each value of the distance threshold from the grid and compare them in terms of quality metrics. Since we also want to be able to select the optimal distance threshold in the absence of real labels, we examine the values provided by the mean Silhouette coefficient (Rousseeuw, 1987). Silhouette is an unattended intrinsic valuation metric (Cluster Validity Index) that measures the quality of the clusters produced and can be used for unattended intrinsic evaluations (i.e. without the basic truth marking). It was reported that it exceeds alternative methods in a comparative study of 30 validity indices (Araet) in 2013."}, {"heading": "3.5 Clustering Evaluation Metrics", "text": "We evaluate the cluster results using standard metrics for the evaluation of extrinsic clusters: homogeneity, completeness, V-measurement (Rosenberg and Hirschberg, 2007), Adjusted Rand Index (ARI) (Hubert and Arabie, 1985) and Adjusted Mutual Information (AMI) (Nguyen et al., 2010). All measurements provide a value in the range [0; 1] for the pair of groups that contain truth and cluster identifiers as input. The higher the value, the more similar the two clusters are. The homogeneity value represents the measure of the purity of the clusters produced. It penalizes clustering, where members of different classes come together. Thus, the best homogeneity values are always at the lower end of the dendrogram, i.e. at the level of the sheets where each document belongs to its own cluster."}, {"heading": "3.6 Manual Cluster Evaluation", "text": "Our partial labeling covers a small subset of the data and constructively provides the clusters with the high degree of overlap between the selected tweets and the commented topics. Therefore, we extend the cluster evaluation to the rest of the data set to assess whether the tweets models can detect less linear semantic similarities in tweets. We select the results for a manual evaluation motivated by the cluster designation (headline).The next step in the pipeline for detecting breaking news after the cluster task is headline selection (tweet labeling task).The most common approach to labeling a tweet is to select a single tweet as a representative member for the entire cluster selection (Papadopoulos et al., 2014).We have decided to test this assumption and manually verify how many clusters lose their semantics when they are represented by a single tweet.The selection of individual tweets motivates the phase of clusters producing coherence as the clusters are discarded in that cluster."}, {"heading": "4 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Results of Clustering Evaluation", "text": "The values marked with bold font show the best result among the two competing approaches for the same subset of tweets according to the respective time interval. Tweet2Vec performs better than the baseline, according to the majority of evaluation metrics at all intervals. In all cases, Tweet2Vec wins in terms of homogeneity and TweetTerm gains in completeness. This result shows that Tweet2Vec is better at separating tweets that are not similar enough to the base model. Tweet2Vec fails only once when the Ground Truth Clusters are perfectly separated (18: 00 interval). This result shows that Tweet2Vec is able to replicate the results of the blurred string matching algorithm used to generate the Ground Truth Labeling."}, {"heading": "4.2 Results of Distance Threshold Selection", "text": "The increase in the V-Measure correlates with the decrease in the silhouette coefficient and the steep decrease in the number of clusters produced (see Figure 1). We observed that the optimal distance threshold for Tweet2Vec clustering according to the V-Measure is in the interval [0.8; 1] (see Table 1: Distance Threshold), which is also in line with the results reported in Ifrim et. al (2014)."}, {"heading": "4.3 Results of Manual Cluster Evaluation", "text": "The results of manual cluster evaluation by four independent evaluators are summarized in Table 2. Bold font indicates the maximum values achieved in the competing representation approaches. Tables 3 and 4 show sample clusters produced by both models in addition to their average values. TweetTerm assigns a 0 vector representation to tweets that do not contain any of the common terms. Therefore, all tweets end in a single \"garbage\" cluster. Therefore, we discount the number of expected \"garbage\" clusters (1 cluster per interval = 5 clusters) from the score for TweetTerm (Table 2). Tweet2Vec model produces the largest number of perfectly homogeneous clusters for which all 5 selected tweets are identical (see Table 2 column Cluster = 1). The percentage of correct results among manually evaluated clusters is higher than with the TweetTerm model, but the number of errors (incorrectly) is higher Tweet2 cluster results, etc."}, {"heading": "5 Discussion", "text": "Our experimental assessment showed that the character-based embedding generated by a neural network outperforms the document / term baseline of the tweet clustering task. However, the baseline approach (TweetTerm) performs very well compared to the simplicity of its implementation, but it falls short of recognizing patterns that go beyond the simple n-gram linkage. We attribute this result to the inherent limitation of the document / term model, which only retains the common terms and disregards the long tail of rare patterns. This limitation appears crucial in the task of emerging message recognition, where the topics must be recognized long before they become popular. Neural embedding, on the other hand, can maintain a sufficient level of detail in its representations and are able to reflect the fuzzy string matching performance beyond simple n-gram linkages."}, {"heading": "6 Conclusion", "text": "We demonstrated that character-based neural embedding enables precise tweet clustering with minimal monitoring, providing fine-grained representations that can help reveal blurred similarities in strings that go beyond simple n-gram matching, and demonstrating the limitations of the current approach, which is unable to distinguish semantic from syntactic patterns in strings, providing a clear direction for future work."}, {"heading": "7 Acknowledgments", "text": "Mihai Lupu was supported by SelfOptimizer (FFG 852624) as part of the EUROSTARS programme, which is funded by EUREKA, the BMWFW and the European Union, and ADMIRE (P25905N23) from the FWF. We thank Bhuwan Dhingra for his support in using Tweet2Vec and Linda Andersson for the review and helpful comments."}], "references": [{"title": "An extensive comparative study of cluster validity indices", "author": ["Ibai Gurrutxaga", "Javier Muguerza", "Jes\u00fas M. P\u00e9rez", "I\u00f1igo Perona"], "venue": "Pattern Recognition,", "citeRegEx": "Arbelaitz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arbelaitz et al\\.", "year": 2013}, {"title": "Adaptive Representations for Tracking Breaking News on Twitter", "author": ["Derek Greene", "Padraig Cunningham"], "venue": "In NewsKDD - Workshop on Data Science for News Publishing at The 20th ACM SIGKDD International", "citeRegEx": "Brigadir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Brigadir et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Tweet2vec: Character-based distributed representations for social media", "author": ["Zhong Zhou", "Dylan Fitzpatrick", "Michael Muehl", "William W. Cohen"], "venue": "In Proceedings of the 54th Annual Meeting of the Associ-", "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["dos Santos", "Bianca Zadrozny"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Event Detection in Twitter using Aggressive Filtering and Hierarchical Tweet Clustering", "author": ["Bichen Shi", "Igor Brigadir"], "venue": null, "citeRegEx": "Ifrim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ifrim et al\\.", "year": 2014}, {"title": "Characteraware neural language models", "author": ["Kim et al.2016] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Binary codes capable of correcting deletions, insertions and reversals", "author": ["Vladimir I. Levenshtein"], "venue": "In Soviet physics doklady,", "citeRegEx": "Levenshtein.,? \\Q1966\\E", "shortCiteRegEx": "Levenshtein.", "year": 1966}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Enhancing First Story Detection using Word Embeddings", "author": ["Moran et al.2016] Sean Moran", "Richard McCreadie", "Craig Macdonald", "Iadh Ounis"], "venue": "In Proceedings of the 39th International ACM SIGIR conference on Research and Development", "citeRegEx": "Moran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moran et al\\.", "year": 2016}, {"title": "fastcluster: Fast hierarchical, agglomerative clustering routines for r and python", "author": ["Daniel M\u00fcllner"], "venue": "Journal of Statistical Software,", "citeRegEx": "M\u00fcllner.,? \\Q2013\\E", "shortCiteRegEx": "M\u00fcllner.", "year": 2013}, {"title": "Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance", "author": ["Julien Epps", "James Bailey"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Nguyen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2010}, {"title": "SNOW 2014 Data Challenge: Assessing the Performance of News Topic Detection Methods in Social Media", "author": ["David Corney", "Luca Maria Aiello"], "venue": null, "citeRegEx": "Papadopoulos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Papadopoulos et al\\.", "year": 2014}, {"title": "Can twitter replace newswire for breaking news", "author": ["Miles Osborne", "Richard McCreadie", "Craig Macdonald", "Iadh Ounis", "Luke Shrimpton"], "venue": null, "citeRegEx": "Petrovic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Petrovic et al\\.", "year": 2013}, {"title": "V-measure: A conditional entropy-based external cluster evaluation measure", "author": ["Rosenberg", "Hirschberg2007] Andrew Rosenberg", "Julia Hirschberg"], "venue": "EMNLP-CoNLL", "citeRegEx": "Rosenberg et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rosenberg et al\\.", "year": 2007}, {"title": "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis", "author": ["Peter J. Rousseeuw"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "Rousseeuw.,? \\Q1987\\E", "shortCiteRegEx": "Rousseeuw.", "year": 1987}, {"title": "The comparison of dendrograms by objective methods", "author": ["Sokal", "Rohlf1962] Robert R. Sokal", "F. James Rohlf"], "venue": "Taxon,", "citeRegEx": "Sokal et al\\.,? \\Q1962\\E", "shortCiteRegEx": "Sokal et al\\.", "year": 1962}, {"title": "Dynamic multi-faceted topic discovery in twitter", "author": ["Vosecky et al.2013] Jan Vosecky", "Di Jiang", "Kenneth Wai-Ting Leung", "Wilfred Ng"], "venue": "In 22nd ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Vosecky et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vosecky et al\\.", "year": 2013}, {"title": "Tweet2vec: Learning tweet embeddings using character-level CNN-LSTM encoder-decoder", "author": ["Prashanth Vijayaraghavan", "Deb Roy"], "venue": "In Proceedings of the 39th International ACM SIGIR conference", "citeRegEx": "Vosoughi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vosoughi et al\\.", "year": 2016}, {"title": "Tracking unbounded Topic Streams", "author": ["Victor Lavrenko", "Miles Osborne"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Wurzer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wurzer et al\\.", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "Therefore, it is important to provide efficient tools to enable journalists rapidly detect breaking news in social media streams (Petrovic et al., 2013).", "startOffset": 129, "endOffset": 152}, {"referenceID": 21, "context": "Artificial neural networks (ANNs) allow to generate dense vector representation (embeddings), which can be efficiently generated on the word- as well as character levels (dos Santos and Zadrozny, 2014; Zhang et al., 2015; Dhingra et al., 2016).", "startOffset": 170, "endOffset": 243}, {"referenceID": 3, "context": "Artificial neural networks (ANNs) allow to generate dense vector representation (embeddings), which can be efficiently generated on the word- as well as character levels (dos Santos and Zadrozny, 2014; Zhang et al., 2015; Dhingra et al., 2016).", "startOffset": 170, "endOffset": 243}, {"referenceID": 6, "context": "There has been a continuous effort over the recent years to design effective and efficient algorithms capable of detecting newsworthy topics in the Twitter stream (Hayashi et al., 2015; Ifrim et al., 2014; Vosecky et al., 2013; Wurzer et al., 2015).", "startOffset": 163, "endOffset": 248}, {"referenceID": 18, "context": "There has been a continuous effort over the recent years to design effective and efficient algorithms capable of detecting newsworthy topics in the Twitter stream (Hayashi et al., 2015; Ifrim et al., 2014; Vosecky et al., 2013; Wurzer et al., 2015).", "startOffset": 163, "endOffset": 248}, {"referenceID": 20, "context": "There has been a continuous effort over the recent years to design effective and efficient algorithms capable of detecting newsworthy topics in the Twitter stream (Hayashi et al., 2015; Ifrim et al., 2014; Vosecky et al., 2013; Wurzer et al., 2015).", "startOffset": 163, "endOffset": 248}, {"referenceID": 20, "context": "ence topics (Wurzer et al., 2015) and 0.", "startOffset": 12, "endOffset": 33}, {"referenceID": 13, "context": "4 against the curated topic pool (Papadopoulos et al., 2014).", "startOffset": 33, "endOffset": 60}, {"referenceID": 9, "context": "Word2vec (Mikolov et al., 2013) is by far the most popular approach.", "startOffset": 9, "endOffset": 31}, {"referenceID": 7, "context": "They repeatedly outperformed the word-level baselines on the tasks of language modeling (Kim et al., 2016), part-ofspeech tagging (dos Santos and Zadrozny, 2014), and text classification (Zhang et al.", "startOffset": 88, "endOffset": 106}, {"referenceID": 21, "context": ", 2016), part-ofspeech tagging (dos Santos and Zadrozny, 2014), and text classification (Zhang et al., 2015).", "startOffset": 88, "endOffset": 108}, {"referenceID": 3, "context": "Our work extends the evaluation of the Tweet2Vec model (Dhingra et al., 2016) to the tweet clustering task, versus the traditional document-term matrix representation.", "startOffset": 55, "endOffset": 77}, {"referenceID": 13, "context": "We use the SNOW 2014 test dataset (Papadopoulos et al., 2014) in our evaluation.", "startOffset": 34, "endOffset": 61}, {"referenceID": 8, "context": "Fuzzy string matching uses the Levenstein (edit) distance (Levenshtein, 1966) between the two input strings as the measure of similarity.", "startOffset": 58, "endOffset": 77}, {"referenceID": 6, "context": "Our baseline is the tweet representation approach that was used in the winnersystem of SNOW 2014 Data Challenge4 (Ifrim et al., 2014).", "startOffset": 113, "endOffset": 133}, {"referenceID": 2, "context": "The network architecture includes two Gated Recurrent Units (GRUs) (Cho et al., 2014): forward and backward GRUs.", "startOffset": 67, "endOffset": 85}, {"referenceID": 11, "context": "To cluster tweet vectors (character-based tweet embeddings produced by the neural network for Tweet2Vec evaluation or the document-term matrix for TweetTerm) we employ the hierarchical clustering algorithm implementation from fastcluster library (M\u00fcllner, 2013).", "startOffset": 246, "endOffset": 261}, {"referenceID": 16, "context": "Silhouette coefficient (Rousseeuw, 1987).", "startOffset": 23, "endOffset": 40}, {"referenceID": 0, "context": "native methods in a comparative study of 30 validity indices (Arbelaitz et al., 2013).", "startOffset": 61, "endOffset": 85}, {"referenceID": 12, "context": "We evaluate the clustering results using the standard metrics for extrinsic clustering evaluation: homogeneity, completeness, V-Measure (Rosenberg and Hirschberg, 2007), Adjusted Rand Index (ARI) (Hubert and Arabie, 1985) and Adjusted Mutual Information (AMI) (Nguyen et al., 2010).", "startOffset": 260, "endOffset": 281}, {"referenceID": 13, "context": "The most common approach to label a cluster of tweets is to select a single tweet as a representative member for the whole cluster (Papadopoulos et al., 2014).", "startOffset": 131, "endOffset": 158}], "year": 2017, "abstractText": "In this paper we show how the performance of tweet clustering can be improved by leveraging character-based neural networks. The proposed approach overcomes the limitations related to the vocabulary explosion in the word-based models and allows for the seamless processing of the multilingual content. Our evaluation results and code are available on-line1.", "creator": "LaTeX with hyperref package"}}}