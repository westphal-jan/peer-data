{"id": "1703.05260", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "InScript: Narrative texts annotated with script information", "abstract": "This paper presents the InScript corpus (Narrative Texts Instantiating Script structure). InScript is a corpus of 1,000 stories centered around 10 different scenarios. Verbs and noun phrases are annotated with event and participant types, respectively. Additionally, the text is annotated with coreference information. The corpus shows rich lexical variation and will serve as a unique resource for the study of the role of script knowledge in natural language processing.", "histories": [["v1", "Wed, 15 Mar 2017 17:01:20 GMT  (442kb,D)", "http://arxiv.org/abs/1703.05260v1", "Paper accepted at LREC 2016, 9 pages, The corpus can be downloaded at:this http URL"]], "COMMENTS": "Paper accepted at LREC 2016, 9 pages, The corpus can be downloaded at:this http URL", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["ashutosh modi", "tatjana anikina", "simon ostermann", "manfred pinkal"], "accepted": false, "id": "1703.05260"}, "pdf": {"name": "1703.05260.pdf", "metadata": {"source": "CRF", "title": "InScript: Narrative texts annotated with script information", "authors": ["Ashutosh Modi", "Tatjana Anikina", "Simon Ostermann", "Manfred Pinkal"], "emails": ["pinkal}@coli.uni-saarland.de"], "sections": [{"heading": null, "text": "Keywords: scripts, narrative texts, script knowledge, common sense"}, {"heading": "1. Motivation", "text": "In fact, it is the case that most of them will be able to move into another world, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2. Data Collection", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Collection via Amazon M-Turk", "text": "We selected 10 scenarios from various available scenarios (e.g. Regneri et al. (2010), Raisig et al. (2009) and the OMICS corpus (Singh et al., 2002), including scripts of varying complexity (TAKING A BATH vs. FLYING IN AN AIRPLANE) and specificity (RIDING A PUBLIC BUS vs. REPAIRING A FLAT BICYCLE TIRE). To measure the effect of different M-Turk instructions on our task, we first conducted pilot experiments with different variants of instructions explaining the task. We completed the instructions for the complete data collection and asked the Turks to describe a scenario in the form of a story as if it were explained to a child, using a minimum of 150 words. We performed the selected instructions based on different instructions in the US, selecting 38 different instructions per scenario."}, {"heading": "2.2. Data Statistics", "text": "On average, each story has a length of 12 sentences and 217 words with an average of 98 word types. Stories are coherent and focus mainly on the corresponding scenario. Neglecting tools, modals and copulas, on average, each story has 32 verbs, of which 58% denote events related to the respective scenario. As you can see in Table 2, there are some variations in stories across scenarios: The scenario FLYING IN AN AIRPLANE, for example, is the most complex in terms of the number of sentences, chips and word types used. This is probably due to the inherent complexity of the scenario: an escape, for example, is more complicated and leads to more steps than taking a bath. The average number of sentences, chips and types is also very high for the BAKE scenario. Stories from the scenario often resemble cake recipes that usually contain very detailed steps, so people tend to give more detailed descriptions in both situations."}, {"heading": "3. Annotation", "text": "This section deals with the annotation of the data. First we describe the final annotation scheme. Then we describe the iterative process of corpus annotation and the refinement of the scheme. This refinement was necessary due to the complexity of the annotation."}, {"heading": "3.1. Annotation Schema", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "3.2. Development of the Schema", "text": "The templates were carefully designed in an iterated process. For each scenario, one of the authors of this paper provided a preliminary version of the template based on the review of some of the stories. For a subset of the scenarios, preliminary templates developed at our department for a psycholinguistic experiment with scripting knowledge were used as a starting point. Subsequently, the authors commented on 5 randomly selected texts for each of the scenarios based on the preliminary template. Necessary extensions and changes to the templates were discussed and agreed upon. Most of the cases of disagreement related to the granularity of the event and the types of the participants. We agreed on the script-specific functional equivalence as the guiding principle. For example, reading a book, listening to music and having a conversation are subsumed under the same event label in the FLIGHT scenario, as they have the usual function of flight entertainment in the scenario."}, {"heading": "3.3. First Annotation Phase", "text": "We used the annotation tool WebAnno (Yimam et al., 2013) for our project. Stories from each scenario were distributed among four different annotators. In a calibration phase, the annotators were given some sample texts for test annotations; the results were discussed with the authors. Throughout the entire annotation phase, the annotators were able to discuss any problems that might arise with the authors. All annotations were performed by students of computer linguistics; the annotation was quite time-consuming due to the complexity of the task, so we opted for the single annotation mode. To assess the quality of the annotation, a small sample of texts was commented on by all four annotators and measured their interannotator agreement (see section 4.1.). It turned out to be sufficiently high. Annotation of the corpus together with some pre-processing and post-processing of the data required about 500 hours of work. All stories were commented with event types and participant types in total (12,446 cases on average)."}, {"heading": "3.4. Modification of the Schema", "text": "As already mentioned, we used MISSSCREV and MISSSCRPART labels to mark verbs and nouns that include events and participants for whom appropriate labels were not available in the templates. Starting with the instances of these labels (a total of 941 and 1717 instances, respectively), we expanded the guidelines to cover sufficiently frequent cases. To include new labels for event and participant types, we tried to estimate the number of instances that would fall under a given label. We added new labels according to the following conditions: \u2022 For participant comments, we added new labels for types that we expected to appear at least 10 times in total in five different stories (i.e. about 5% of the event labels)."}, {"heading": "3.5. Special Cases", "text": "In fact, most of them will be able to play by the rules."}, {"heading": "4. Data Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Inter-Annotator Agreement", "text": "In order to calculate the agreement between the commentators, a total of 30 stories from 6 scenarios were randomly selected and commented on in parallel by all 4 commentators after the first comment phase. (The results are shown in Figure 4a, indicating moderate to significant agreement (Landis and Koch, 1977). Interestingly, if we calculated the agreement on these data based only on the subset of cases provided with script-specific events and participant names by all commentators, the results were better than the results of the evaluation of all marked events (including unrelated and unscripted events), indicating one of the challenges of the annotation task: in many cases, it is difficult to decide whether a particular event should be considered a central scripting event, or an event loosely related to the script or unrelated to the script."}, {"heading": "4.2. Annotated Corpus Statistics", "text": "Figure 5 gives an overview of the number of events and participant types provided in the templates. In contrast, tarpaulins contain the fewest labels for a tree and a train. There are 19 event and participant types on average. Figure 6 presents overview statistics on the use of labels and corrective campaigns. As you can see, there are many more labels of participants as such. For evidence chains, there are some chains that are really long (which also leads to a large standard deviation)."}, {"heading": "4.3. Comparison to the DeScript Corpus", "text": "As already mentioned, the InScript corpus is part of a larger research project in which a different kind of corpus, the DeScript corpus, was also created. DeScript comprises 40 scenarios and also contains the 10 scenarios of InScript Threshold. This corpus contains texts that describe scripts on an abstract and general level, while InScript contains various instantiations of scripts in narrative texts. Script events in DeScript are described in a very simple, telegram-like language (see Figure 2). As one of the long-term goals of the project is to align InScript texts with the script structure in narrative texts, it is interesting to compare the two resources. InScript corpus shows much more lexical variation than DeScript. Many approaches use the art-token ratio to measure this variance."}, {"heading": "5. Conclusion", "text": "In this paper, we described the InScript corpus of 1,000 narrative texts commented on with script structure and correlation information. We described the annotation process, various difficulties encountered during annotation, and various remedies taken to overcome them. One of the future research objectives of our project is also to search for automatic methods for text-to-script mapping, i.e. aligning text segments to script states. We consider InScript and DeScript together as a resource to investigate this orientation. The corpus has a rich lexical variation and will serve as a unique resource for studying the role of scripting knowledge in natural language processing."}, {"heading": "Acknowledgements", "text": "This research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) as part of CRC 1102 \"Information Density and Language Coding.\""}, {"heading": "6. References", "text": "The Handbook of Artificial Intelligence. Addison-Wesley. Chambers, N. and Jurafsky, D. (2008). Unsupervisedlearning of narrative event chains. Proceedings of the ACL08.Chambers, N. and Jurafsky, D. (2009). Unsupervised learning of narrative schemas and their participants. Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP.Cullingford, R. E. (1978). Script application: Computer understanding of newspaper stories. Technical report, DTIC Document.Fleiss, J. L. (1971). Measuring nominal scale agreement between many raters. Psychological bulletin, 76 (5): 378.Landis, J. R. and Koch, G. G. G. G. (1977)."}], "references": [{"title": "An Assessment of the Range and", "author": ["P.M. ods. McCarthy"], "venue": null, "citeRegEx": "McCarthy,? \\Q2005\\E", "shortCiteRegEx": "McCarthy", "year": 2005}, {"title": "Inducing neural models", "author": ["A. Modi", "I. Titov"], "venue": null, "citeRegEx": "Modi and Titov,? \\Q2014\\E", "shortCiteRegEx": "Modi and Titov", "year": 2014}, {"title": "Understanding script-based stories", "author": ["E.T. Mueller"], "venue": "In CoNLL,", "citeRegEx": "Mueller,? \\Q2004\\E", "shortCiteRegEx": "Mueller", "year": 2004}, {"title": "Learning to predict script events from domain-specific text. Lexical and Computational Semantics", "author": ["R. Rudinger", "V. Demberg", "A. Modi", "B. Van Durme", "M. Pinkal"], "venue": null, "citeRegEx": "Rudinger et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rudinger et al\\.", "year": 2015}, {"title": "A Mathematical Theory of Communication", "author": ["C.E. Shannon"], "venue": "The Bell System Technical Journal,", "citeRegEx": "Shannon,? \\Q1948\\E", "shortCiteRegEx": "Shannon", "year": 1948}, {"title": "Open mind common sense: Knowledge acquisition from the general public", "author": ["P. Singh", "T. Lin", "E.T. Mueller", "G. Lim", "T. Perkins", "W.L. Zhu"], "venue": null, "citeRegEx": "Singh et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2002}, {"title": "How Variable May a Constant Be? Measures of Lexical Richness in Perspective", "author": ["F.J. Tweedie", "R.H. Baayen"], "venue": "Computers and the Humanities,", "citeRegEx": "Tweedie and Baayen,? \\Q1998\\E", "shortCiteRegEx": "Tweedie and Baayen", "year": 1998}, {"title": "A crowdsourced database of event sequence descriptions for the acquisition of high-quality script knowledge", "author": ["L.D.A. Wanzare", "A. Zarcone", "S. Thater", "M. Pinkal"], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evalua-", "citeRegEx": "Wanzare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wanzare et al\\.", "year": 2016}, {"title": "WebAnno: A Flexible, Web-based and Visually Supported System for Distributed Annotations", "author": ["S.M. Yimam", "I. Gurevych", "R.E. de Castilho", "C. Biemann"], "venue": "In ACL (Conference System Demonstrations),", "citeRegEx": "Yimam et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yimam et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "Script knowledge has been shown to play an important role in text understanding (Cullingford (1978), Miikkulainen (1995), Mueller (2004), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Modi and Titov (2014), Rudinger et al.", "startOffset": 122, "endOffset": 137}, {"referenceID": 1, "context": "Script knowledge has been shown to play an important role in text understanding (Cullingford (1978), Miikkulainen (1995), Mueller (2004), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Modi and Titov (2014), Rudinger et al.", "startOffset": 122, "endOffset": 167}, {"referenceID": 1, "context": "Script knowledge has been shown to play an important role in text understanding (Cullingford (1978), Miikkulainen (1995), Mueller (2004), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Modi and Titov (2014), Rudinger et al.", "startOffset": 122, "endOffset": 197}, {"referenceID": 1, "context": "Script knowledge has been shown to play an important role in text understanding (Cullingford (1978), Miikkulainen (1995), Mueller (2004), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Modi and Titov (2014), Rudinger et al.", "startOffset": 198, "endOffset": 220}, {"referenceID": 1, "context": "Script knowledge has been shown to play an important role in text understanding (Cullingford (1978), Miikkulainen (1995), Mueller (2004), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Modi and Titov (2014), Rudinger et al. (2015)).", "startOffset": 198, "endOffset": 244}, {"referenceID": 5, "context": "Efforts have been made to collect scenariospecific script knowledge via crowdsourcing, for example the OMICS and SMILE corpora (Singh et al. (2002), Regneri et al.", "startOffset": 128, "endOffset": 148}, {"referenceID": 5, "context": "Efforts have been made to collect scenariospecific script knowledge via crowdsourcing, for example the OMICS and SMILE corpora (Singh et al. (2002), Regneri et al. (2010), Regneri (2013)), but these corpora describe script events in a pointwise telegram style rather than in full texts.", "startOffset": 128, "endOffset": 171}, {"referenceID": 5, "context": "Efforts have been made to collect scenariospecific script knowledge via crowdsourcing, for example the OMICS and SMILE corpora (Singh et al. (2002), Regneri et al. (2010), Regneri (2013)), but these corpora describe script events in a pointwise telegram style rather than in full texts.", "startOffset": 128, "endOffset": 187}, {"referenceID": 6, "context": "Besides InScript, this project also released a corpus of generic descriptions of script activities called DeScript (for Describing Script Structure, Wanzare et al. (2016)).", "startOffset": 149, "endOffset": 171}, {"referenceID": 5, "context": "DeScript contains a range of short and textually simple phrases that describe script events in the style of OMICS or SMILE (Singh et al. (2002), Regneri et al.", "startOffset": 124, "endOffset": 144}, {"referenceID": 5, "context": "DeScript contains a range of short and textually simple phrases that describe script events in the style of OMICS or SMILE (Singh et al. (2002), Regneri et al. (2010)).", "startOffset": 124, "endOffset": 167}, {"referenceID": 5, "context": "(2009), and the OMICS corpus (Singh et al., 2002)), including scripts of different complexity (TAKING A BATH vs.", "startOffset": 29, "endOffset": 49}, {"referenceID": 8, "context": "We used the WebAnno annotation tool (Yimam et al., 2013) for our project.", "startOffset": 36, "endOffset": 56}, {"referenceID": 7, "context": "Additionally, we looked at the DeScript corpus (Wanzare et al., 2016), which contains manually clustered event paraphrase sets for the 10 scenarios that are also covered by InScript (see Section 4.", "startOffset": 47, "endOffset": 69}, {"referenceID": 5, "context": "Tweedie and Baayen (1998)), which would result in very small values for InScript and relatively large ones for DeScript, given the large average difference of text lengths between the corpora.", "startOffset": 0, "endOffset": 26}, {"referenceID": 0, "context": "Instead, we decided to use the Measure of Textual Lexical Diversity (MTLD) (McCarthy and Jarvis (2010), McCarthy (2005)), which is familiar in corpus linguistics.", "startOffset": 76, "endOffset": 103}, {"referenceID": 0, "context": "Instead, we decided to use the Measure of Textual Lexical Diversity (MTLD) (McCarthy and Jarvis (2010), McCarthy (2005)), which is familiar in corpus linguistics.", "startOffset": 76, "endOffset": 120}, {"referenceID": 4, "context": "We used entropy (Shannon, 1948) over lemmas to measure the variance of lexical realizations for events.", "startOffset": 16, "endOffset": 31}], "year": 2017, "abstractText": "This paper presents the InScript corpus (Narrative Texts Instantiating Script structure). InScript is a corpus of 1,000 stories centered around 10 different scenarios. Verbs and noun phrases are annotated with event and participant types, respectively. Additionally, the text is annotated with coreference information. The corpus shows rich lexical variation and will serve as a unique resource for the study of the role of script knowledge in natural language processing.", "creator": "TeX"}}}