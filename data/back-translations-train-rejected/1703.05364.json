{"id": "1703.05364", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "A Study of Complex Deep Learning Networks on High Performance, Neuromorphic, and Quantum Computers", "abstract": "Current Deep Learning approaches have been very successful using convolutional neural networks (CNN) trained on large graphical processing units (GPU)-based computers. Three limitations of this approach are: 1) they are based on a simple layered network topology, i.e., highly connected layers, without intra-layer connections; 2) the networks are manually configured to achieve optimal results, and 3) the implementation of neuron model is expensive in both cost and power. In this paper, we evaluate deep learning models using three different computing architectures to address these problems: quantum computing to train complex topologies, high performance computing (HPC) to automatically determine network topology, and neuromorphic computing for a low-power hardware implementation. We use the MNIST dataset for our experiment, due to input size limitations of current quantum computers. Our results show the feasibility of using the three architectures in tandem to address the above deep learning limitations. We show a quantum computer can find high quality values of intra-layer connections weights, in a tractable time as the complexity of the network increases; a high performance computer can find optimal layer-based topologies; and a neuromorphic computer can represent the complex topology and weights derived from the other architectures in low power memristive hardware.", "histories": [["v1", "Wed, 15 Mar 2017 19:37:08 GMT  (2378kb,D)", "http://arxiv.org/abs/1703.05364v1", null], ["v2", "Thu, 13 Jul 2017 18:47:59 GMT  (2378kb,D)", "http://arxiv.org/abs/1703.05364v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["thomas e potok", "catherine schuman", "steven r young", "robert m patton", "federico spedalieri", "jeremy liu", "ke-thia yao", "garrett rose", "gangotree chakma"], "accepted": false, "id": "1703.05364"}, "pdf": {"name": "1703.05364.pdf", "metadata": {"source": "CRF", "title": "A Study of Complex Deep Learning Networks on High Performance, Neuromorphic, and Quantum Computers", "authors": ["Thomas E. Potok", "Catherine Schuman", "Steven R. Young", "Robert M. Patton", "Federico Spedalieri", "Jeremy Liu", "Ke-Thia Yao", "Garrett Rose", "Gangotree Chakma"], "emails": [], "sections": [{"heading": null, "text": "Note: This manuscript was written by UT-Battelle, LLC under contract number DE-AC05-00OR22725 with the U.S. Department of Energy. By accepting the article for publication, the United States Government and the publisher acknowledge that the United States Government retains a non-exclusive, paid, irrevocable worldwide license to publish or reproduce the published form of this manuscript for United States government purposes. The Department of Energy makes this research publicly available in accordance with the DOE Public Access Plan (http: / / energy.gov / downloads / doe-public-access-plan). ar Xiv: 170 3.05 364v 1 [cs.N E] 1"}, {"heading": "1 Introduction", "text": "This year, the time has come for only one person to be able to move, to explore the world and to travel around the world."}, {"heading": "2 Related Work", "text": "First, we look at the current state of quantum, high-performance and neuromorphic computer technology in the context of the aforementioned deep learning challenges."}, {"heading": "2.1 Quantum Computing", "text": "Computing with quantum computers was first discussed by Feynman [10], who was motivated by the fact that simulation of a quantum system with a classical computer seems to be insoluble. Interest in quantum computers grew dramatically with the discovery of Shor's polynomial quantum algorithm for factoring numbers [35], since all known classical factoring algorithms require exponential time. Several approaches to quantum computing have since been developed, and they include the well-known quantum circuit model (used by Shor's algorithm), the measurement-based quantum computer model, and the adiabatic quantum computer model [9], all of which have the same computing power. In this paper, we focus on a limited form of adiabatic quantum computing, which performs the ability to perform adiabatic quantum computing."}, {"heading": "2.2 High Performance Computing", "text": "Deep Learning, an early adopter of GPU technology, has benefited greatly from the acceleration offered by these accelerated computing devices, and has received strong support from device manufacturers in the form of deep learning-specific GPU libraries, as they have multiple GPUs per computer node. This leaves open the question of how best to use thousands of GPUs for deep learning, as previous work has only used a maximum of 64 GPUs before encountering scaling problems to exploit the parallelism of the model to spread the weights of the network across multiple GPUs."}, {"heading": "2.3 Neuromorphic Computing", "text": "Neuromorphic computer architectures have historically been designed with one of two objectives in mind: either to develop tailor-made hardware to accurately simulate biological neural systems with the goal of studying biological brains, or to build computationally useful architectures that are inspired by the operation of biological brains and exhibit some of their characteristics. In the development of neuromorphic computer systems for computational purposes, there were two main approaches: the construction of devices based on spikes of neural networks (SNNs), such as IBM's TrueNorth [34], and the construction of devices based on conventional neural networks, such as Google's Tensor Processing Unit [21] or Nervana's Nervana Engine, to serve as deep learning accelerators."}, {"heading": "3 Approach", "text": "The three platforms we study, quantum, supercomputer and neuromorphic calculator, are quite different in the way they process data, and the choice of a deep learning problem that can be used on all three levels is limited by the amount of data anyone can support. Currently, D-Wave supports 1000 qubits, limiting the size of a problem to the inputs and deep learning network. MNIST is a collection of handwritten digits that have been extensively studied in the deep learning community [24]. The images of the qubits are very small (28 x 28 pixels in total 784 pixels) that can be analyzed with 1000 qubits of the quantum computer as well as the other architectures. The next challenge is to select the type of deep learning network that can be supported natively on the three platforms. While many deep learning methods have been proposed over the years, CNN has consistently provided the highest accuracy on standard data sets."}, {"heading": "3.1 Quantum", "text": "In fact, it is the case that one is able to find a solution that is capable of finding a solution that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution."}, {"heading": "3.2 HPC", "text": "That is the quickest way we can go to solve the problem."}, {"heading": "3.3 Neuromorphic", "text": "A neurotic approach to the MNI problem is that it is not a pure problem, but a pure problem."}, {"heading": "4 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Quantum", "text": "We use common parameters to control learning progress, the same ones found in the training of RBMs [12]. We opted for training over 25 epochs (an epoch is a complete run over all educational data) instead of 10 epochs to get a better idea of what performance we can potentially achieve. Another parameter is the learning rate, or how much the LBM learns from each example. We chose a relatively standard learning rate of 0.1 for hidden edges and 0.001 for an error. Setting the parameters is too low."}, {"heading": "4.2 HPC", "text": "We used the titanium computer and the MENNDL system to discover a near-optimal topology of a deep network trained on the handwritten MNIST dataset [24] using the method outlined in [37]. Optimized hyperparameters were the core size, the number of hidden units for each of the revolutionary layers, and the number of hidden units in the fully interconnected layer. The structure of the network is shown in Figure 7. Using 500 nodes of titanium, the evolutionary algorithm was trained for 32 generations with 500 individuals in the population, allowing us to evaluate 16 000 networks. Each hyperparameter is encoded as an integral gene, and the range of this integrator is limited to avoid the evaluation of hyperparameter values that are not of interest. A single node of titanium nets that evaluates the core of the evolutionary algorithm and the fitness function on the rest of the network to evaluate the rest of the network distributed with the network."}, {"heading": "4.3 Neuromorphic", "text": "Dre eeisrrrrrrrreeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerrrrrrrrrrrrrrrrrreeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "5 Discussion", "text": "This is because most people who are in the US are unable to understand, understand, and understand what the world is doing."}, {"heading": "5.1 Future Work", "text": "Our next step is to test the hypothesis that this proposed architecture actually provides more accuracy, flexibility and insight into a dataset than can be derived from a traditional CNN approach. We will apply this architectural proposal to a large scientific dataset and compare the results of a traditional approach with this architectural proposal."}, {"heading": "6 Conclusion", "text": "Current deep learning networks are based on this neural model of human perception and have been highly optimized with CNNs trained on large clusters of GPUs. This technology has been crucial in solving problems that researchers have been challenging for years, such as object and face recognition within photographs. The topology of these CNN networks consists of revolutionary layers with common weights and fully connected layers, without connecting the layers, which are powerful but quite simple. This paper addresses three main limitations of deep learning: 1) training models that contain intra-computing layer connections; 2) automatically determining an optimal configuration for a network computing topology; and 3) implementing a complex topology of computing in native hardware, which we will formulate a simple deep learning problem on three different architectures: quantum, high performance and neuromorphic computing networks, which address the three problems of computing: complex topputologies, complex topputologies, topputologies, topputologies, quantum, and the antum of computing."}, {"heading": "7 Acknowledgments", "text": "This material is based on work supported by the US Department of Energy, the Office of Science, the Office of Advanced Scientific Computing Research, Robinson Pino, Program Manager, under contract number DE-AC05-00OR22725. Resources from the Oak Ridge Leadership Computing Facility, a DoE Office of Science User Facility supported under the DE-AC0500OR22725 contract, were used for this research."}], "references": [{"title": "and X", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu"], "venue": "Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "A learning algorithm for boltzmann machines", "author": ["D.H. Ackley", "G.E. Hinton", "T.J. Sejnowski"], "venue": "Cognitive science, 9(1):147\u2013169", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1985}, {"title": "Towards memristive dynamic adaptive neural network arrays", "author": ["N. Cady", "K. Beckmann", "H. Manem", "M. Dean", "G. Rose", "J.V. Nostrand"], "venue": "In Proceedings of the Government Microcircuit Applications and Critical Technology Conference (GOMACTech),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "et al", "author": ["A.S. Cassidy", "P. Merolla", "J.V. Arthur", "S.K. Esser", "B. Jackson", "R. Alvarez- Icaza", "P. Datta", "J. Sawada", "T.M. Wong", "V. Feldman"], "venue": "Cognitive computing building block: A versatile and efficient digital neuron model for neurosynaptic cores. In Neural Networks (IJCNN), The 2013 International Joint Conference on, pages 1\u201310. IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning with COTS HPC systems", "author": ["A. Coates", "B. Huval", "T. Wang", "D.J. Wu", "B. Catanzaro", "A.Y. Ng"], "venue": "Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pages 1337\u20131345", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "DANNA: A neuromorphic software ecosystem", "author": ["A. Disney", "J. Reynolds", "C.D. Schuman", "A. Klibisz", "A. Young", "J.S. Plank"], "venue": "Biologically-Insipred Cognitive Architectures 2016, page In press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Backpropagation for energy-efficient neuromorphic computing", "author": ["S.K. Esser", "R. Appuswamy", "P. Merolla", "J.V. Arthur", "D.S. Modha"], "venue": "Advances in Neural Information Processing Systems, pages 1117\u20131125", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Quantum computation by adiabatic evolution", "author": ["E. Farhi", "J. Goldstone", "S. Gutmann", "M. Sipser"], "venue": "Report MIT-CTP-2936, Massachusetts Institute of Technology", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Simulating physics with computers", "author": ["R.P. Feynman"], "venue": "International journal of theoretical physics, 21(6):467\u2013488", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1982}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1026\u20131034", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G. Hinton"], "venue": "Momentum, 9(1):926", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Comput.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Labeled faces in the wild: A database forstudying face recognition in unconstrained environments", "author": ["G.B. Huang", "M. Mattar", "T. Berg", "E. Learned-Miller"], "venue": "Workshop on faces in\u2019Real-Life\u2019Images: detection, alignment, and recognition", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "et al", "author": ["K. Jarrett", "K. Kavukcuoglu", "Y. LeCun"], "venue": "What is the best multi-stage architecture for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pages 2146\u20132153. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Nanoscale memristor device as synapse in neuromorphic systems", "author": ["S.H. Jo", "T. Chang", "I. Ebong", "B.B. Bhadviya", "P. Mazumder", "W. Lu"], "venue": "Nano letters, 10(4):1297\u20131301", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Google supercharges machine learning tasks with tpu custom chip, May 2016", "author": ["N. Jouppi"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "A functional hybrid memristor crossbar-array/cmos system for data storage and neuromorphic applications", "author": ["K.-H. Kim", "S. Gaba", "D. Wheeler", "J.M. Cruz-Albrecht", "T. Hussain", "N. Srinivasa", "W. Lu"], "venue": "Nano letters, 12(1):389\u2013395", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u2013 2324", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "and C", "author": ["Y. LeCun", "C. Cortes"], "venue": "J. Burges. The MNIST database of handwritten digits", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "A memristor crossbar based computing engine optimized for high speed and accuracy", "author": ["C. Liu", "Q. Yang", "B. Yan", "J. Yang", "X. Du", "W. Zhu", "H. Jiang", "Q. Wu", "M. Barnell", "H. Li"], "venue": "VLSI (ISVLSI), 2016 IEEE Computer Society Annual Symposium on, pages 110\u2013115. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Harmonica: A framework of heterogeneous computing systems with memristor-based neuromorphic computing accelerators", "author": ["X. Liu", "M. Mao", "B. Liu", "B. Li", "Y. Wang", "H. Jiang", "M. Barnell", "Q. Wu", "J. Yang", "H. Li", "Y. Chen"], "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10), pages 807\u2013814", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Training and operation of an integrated neuromorphic network based on metal-oxide memristors", "author": ["M. Prezioso", "F. Merrikh-Bayat", "B. Hoskins", "G. Adam", "K.K. Likharev", "D.B. Strukov"], "venue": "Nature, 521(7550):61\u201364", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluation of pooling operations in convolutional architectures for object recognition", "author": ["D. Scherer", "A. M\u00fcller", "S. Behnke"], "venue": "International Conference on Artificial Neural Networks, pages 92\u2013101. Springer", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 815\u2013823", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Spatiotemporal classification using neuroscience-inspired dynamic architectures", "author": ["C.D. Schuman", "J.D. Birdwell", "M.E. Dean"], "venue": "Procedia Computer Science, 41:89 \u2013 97", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Darwin: a neuromorphic hardware co-processor based on spiking neural networks", "author": ["J. Shen", "D. Ma", "Z. Gu", "M. Zhang", "X. Zhu", "X. Xu", "Q. Xu", "Y. Shen", "G. Pan"], "venue": "Science China Information Sciences, 59(2):1\u20135", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer", "author": ["P.W. Shor"], "venue": "SIAM J. Comput.,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1997}, {"title": "How we found the missing memristor", "author": ["R.S. Williams"], "venue": "IEEE Spectrum,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "Optimizing deep learning hyper-parameters through an evolutionary algorithm", "author": ["S.R. Young", "D.C. Rose", "T.P. Karnowski", "S.-H. Lim", "R.M. Patton"], "venue": "Proceedings of the Workshop on Machine Learning in High- Performance Computing Environments, MLHPC \u201915, pages 4:1\u20134:5, New York, NY, USA", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "There are multiple designs for deep learning networks, with, CNNs being the most widely used deep learning models [23] and are most commonly used for image classification with remarkably good results.", "startOffset": 114, "endOffset": 118}, {"referenceID": 27, "context": "This pooling operation allows the network to be resilient to shifts of features within the image [31].", "startOffset": 97, "endOffset": 101}, {"referenceID": 13, "context": "Subsequent improvements to CNNS, such as dropout [15], rectified linear units [17, 27], and efficient GPU DL codes [1, 3, 18] have allowed CNNs to achieve impressive results on a variety of benchmarks.", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": "Subsequent improvements to CNNS, such as dropout [15], rectified linear units [17, 27], and efficient GPU DL codes [1, 3, 18] have allowed CNNs to achieve impressive results on a variety of benchmarks.", "startOffset": 78, "endOffset": 86}, {"referenceID": 24, "context": "Subsequent improvements to CNNS, such as dropout [15], rectified linear units [17, 27], and efficient GPU DL codes [1, 3, 18] have allowed CNNs to achieve impressive results on a variety of benchmarks.", "startOffset": 78, "endOffset": 86}, {"referenceID": 0, "context": "Subsequent improvements to CNNS, such as dropout [15], rectified linear units [17, 27], and efficient GPU DL codes [1, 3, 18] have allowed CNNs to achieve impressive results on a variety of benchmarks.", "startOffset": 115, "endOffset": 125}, {"referenceID": 16, "context": "Subsequent improvements to CNNS, such as dropout [15], rectified linear units [17, 27], and efficient GPU DL codes [1, 3, 18] have allowed CNNs to achieve impressive results on a variety of benchmarks.", "startOffset": 115, "endOffset": 125}, {"referenceID": 26, "context": "These include the ImageNet [30] dataset where CNNs have surpassed human level performance at object recognition [11] and the Labeled Faces in the Wild (LFW) [16] dataset where CNNs have surpassed human level performance at face recognition [32].", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "These include the ImageNet [30] dataset where CNNs have surpassed human level performance at object recognition [11] and the Labeled Faces in the Wild (LFW) [16] dataset where CNNs have surpassed human level performance at face recognition [32].", "startOffset": 112, "endOffset": 116}, {"referenceID": 14, "context": "These include the ImageNet [30] dataset where CNNs have surpassed human level performance at object recognition [11] and the Labeled Faces in the Wild (LFW) [16] dataset where CNNs have surpassed human level performance at face recognition [32].", "startOffset": 157, "endOffset": 161}, {"referenceID": 28, "context": "These include the ImageNet [30] dataset where CNNs have surpassed human level performance at object recognition [11] and the Labeled Faces in the Wild (LFW) [16] dataset where CNNs have surpassed human level performance at face recognition [32].", "startOffset": 240, "endOffset": 244}, {"referenceID": 1, "context": "A Boltzmann Machine (BM) is a recurrent neural network consisting of neurons that make binary stochastic decisions based on the states of their symmetrically connected neuron neighbors [2].", "startOffset": 185, "endOffset": 188}, {"referenceID": 1, "context": "A BM training algorithm was proposed in [2].", "startOffset": 40, "endOffset": 43}, {"referenceID": 12, "context": "The RBM network topology is restricted to a bipartite graph [14].", "startOffset": 60, "endOffset": 64}, {"referenceID": 11, "context": "Deep belief networks (DBNs) can be created by composing many RBM layers [13].", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "Computing using quantum computers was first discussed by Feynman [10] who was motivated by the fact that simulating a quantum system using a classical computer seems to be intractable.", "startOffset": 65, "endOffset": 69}, {"referenceID": 31, "context": "Interest in quantum computing increased dramatically with the discovery of the Shor\u2019s polynomial quantum algorithm for factoring numbers [35], because all known classical probabilistic factoring algorithms require exponential time.", "startOffset": 137, "endOffset": 141}, {"referenceID": 7, "context": "Several approaches to quantum computing were since developed, and they include the well-known quantum circuit model (used by Shor\u2019s algorithm), the measurement-based quantum computing model, and the adiabatic quantum computing model [9].", "startOffset": 233, "endOffset": 236}, {"referenceID": 12, "context": "As stated above, the training of a BM is impractical on traditional computer systems, thus the development of a RBM restricts the network topology to that of a bipartite graphs [14].", "startOffset": 177, "endOffset": 181}, {"referenceID": 4, "context": "This leaves the question of how to best utilize thousands of GPUs for deep learning, as previous work has only utilized a maximum of 64 GPUs before encountering scaling problems when trying to exploit model parallelism to spread the weights of the network across multiple GPUs [6].", "startOffset": 277, "endOffset": 280}, {"referenceID": 33, "context": "Previously, it has been shown that HPC can be utilized to optimize the hyper-parameters of a deep learning network [37].", "startOffset": 115, "endOffset": 119}, {"referenceID": 3, "context": "In developing neuromorphic computing devices for computational purposes, there have been two main approaches: building devices based on spiking neural networks (SNNs), such as IBM\u2019s TrueNorth [5] or Darwin [34], and building devices based on convolutional neural networks, such as Google\u2019s Tensor Processing Unit [21] or Nervana\u2019s Nervana Engine [28], to serve as deep learning accelerators.", "startOffset": 192, "endOffset": 195}, {"referenceID": 30, "context": "In developing neuromorphic computing devices for computational purposes, there have been two main approaches: building devices based on spiking neural networks (SNNs), such as IBM\u2019s TrueNorth [5] or Darwin [34], and building devices based on convolutional neural networks, such as Google\u2019s Tensor Processing Unit [21] or Nervana\u2019s Nervana Engine [28], to serve as deep learning accelerators.", "startOffset": 206, "endOffset": 210}, {"referenceID": 18, "context": "In developing neuromorphic computing devices for computational purposes, there have been two main approaches: building devices based on spiking neural networks (SNNs), such as IBM\u2019s TrueNorth [5] or Darwin [34], and building devices based on convolutional neural networks, such as Google\u2019s Tensor Processing Unit [21] or Nervana\u2019s Nervana Engine [28], to serve as deep learning accelerators.", "startOffset": 313, "endOffset": 317}, {"referenceID": 6, "context": "for neuromorphic computers has been to train a CNN offline and then create a mapping process from the CNN to the associated SNN-based neuromorphic hardware [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 32, "context": "Likewise, when no voltage is applied across a memristor, the most recent resistance value is retained [36].", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "Memristors have similar behavior to biological synapses, and as such, have been frequently utilized to implement neuromorphic systems [19, 22, 29].", "startOffset": 134, "endOffset": 146}, {"referenceID": 19, "context": "Memristors have similar behavior to biological synapses, and as such, have been frequently utilized to implement neuromorphic systems [19, 22, 29].", "startOffset": 134, "endOffset": 146}, {"referenceID": 25, "context": "Memristors have similar behavior to biological synapses, and as such, have been frequently utilized to implement neuromorphic systems [19, 22, 29].", "startOffset": 134, "endOffset": 146}, {"referenceID": 21, "context": "MNIST is a collection of hand-written digits that has been very widely studied in the deep learning community [24].", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "First, we will create and train a RBM on D-Wave, applying it to the MNIST handwritten digit classification problem [24] to establish a reference result.", "startOffset": 115, "endOffset": 119}, {"referenceID": 0, "context": "Each 28x28 image is represented by a vector of 784 length, each unit with holding a value representing pixel intensity in the range [0, 1].", "startOffset": 132, "endOffset": 138}, {"referenceID": 20, "context": "The network architecture utilized was LeNet [23].", "startOffset": 44, "endOffset": 48}, {"referenceID": 29, "context": "The neuromorphic system we will use to explore the MNIST problem is a memristive implementation of the neuroscience-inspired dynamic architectures (NIDA) system [33].", "startOffset": 161, "endOffset": 165}, {"referenceID": 5, "context": "A digital hardware implementation based on NIDA, called Dynamic Adaptive Neural Network Array (DANNA), has also been created and is currently implemented on FPGA with a digital VLSI implementation in progress [7].", "startOffset": 209, "endOffset": 212}, {"referenceID": 29, "context": "The EA approach for training networks for the MNIST problem was previously applied to the NIDA SNN [33] and to DANNA [7].", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "The EA approach for training networks for the MNIST problem was previously applied to the NIDA SNN [33] and to DANNA [7].", "startOffset": 117, "endOffset": 120}, {"referenceID": 10, "context": "We utilize common parameters to control the learning progress, the same ones found in training RBMs [12].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "We used the Titan computer and the MENNDL system to discover a near optimal topology of a deep network trained on the MNIST handwritten digit dataset [24] by utilizing the method presented in [37].", "startOffset": 150, "endOffset": 154}, {"referenceID": 33, "context": "We used the Titan computer and the MENNDL system to discover a near optimal topology of a deep network trained on the MNIST handwritten digit dataset [24] by utilizing the method presented in [37].", "startOffset": 192, "endOffset": 196}, {"referenceID": 2, "context": "Such characteristics for LRS, HRS and the associated on-off ratio have been observed for a range of memristive devices, including hafnium-oxide (HfO2)[4], tantalum-oxide (TaO2), and titanium-oxide (TiO2), to name a few.", "startOffset": 150, "endOffset": 153}, {"referenceID": 22, "context": "43mW which is consistent with similar memristor-based neuromorphic systems [25].", "startOffset": 75, "endOffset": 79}, {"referenceID": 23, "context": "Research has also shown that memristive neuromorphic systems are typically 20\u00d7 more energy-efficient than their CMOS counterparts [26], and our results are consistent with this estimation.", "startOffset": 130, "endOffset": 134}], "year": 2017, "abstractText": "Current Deep Learning approaches have been very successful using convolutional neural networks (CNN) trained on large graphical processing units (GPU)-based computers. Three limitations of this approach are: 1) they are based on a simple layered network topology, i.e., highly connected layers, without intra-layer connections; 2) the networks are manually configured to achieve optimal results, and 3) the implementation of neuron model is expensive in both cost and power. In this paper, we evaluate deep learning models using three different computing architectures to address these problems: quantum computing to train complex topologies, high performance computing (HPC) to automatically determine network topology, and neuromorphic computing for a low-power hardware implementation. We use the MNIST dataset for our experiment, due to input size limitations of current quantum computers. Our results show the feasibility of using the three architectures in tandem to address the above deep learning limitations. We show a quantum computer can find high quality values of intra-layer connections weights, in a tractable time as the complexity of the network increases; a high performance computer can find optimal layer-based topologies; and a neuromorphic computer can represent the complex topology and weights derived from the other architectures in low power memristive hardware. Notice: This manuscript has been authored by UT-Battelle, LLC under Contract No. DE-AC05-00OR22725 with the U.S. Department of Energy. The United States Government retains and the publisher, by accepting the article for publication, acknowledges that the United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-public-access-plan). 1 ar X iv :1 70 3. 05 36 4v 1 [ cs .N E ] 1 5 M ar 2 01 7", "creator": "LaTeX with hyperref package"}}}