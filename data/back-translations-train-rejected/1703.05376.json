{"id": "1703.05376", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Two-Timescale Stochastic Approximation Convergence Rates with Applications to Reinforcement Learning", "abstract": "Two-timescale Stochastic Approximation (SA) algorithms are widely used in Reinforcement Learning (RL). In such methods, the iterates consist of two parts that are updated using different stepsizes. We develop the first convergence rate result for these algorithms; in particular, we provide a general methodology for analyzing two-timescale linear SA. We apply our methodology to two-timescale RL algorithms such as GTD(0), GTD2, and TDC.", "histories": [["v1", "Wed, 15 Mar 2017 20:23:45 GMT  (42kb)", "https://arxiv.org/abs/1703.05376v1", null], ["v2", "Wed, 31 May 2017 16:35:17 GMT  (59kb,D)", "http://arxiv.org/abs/1703.05376v2", null], ["v3", "Thu, 7 Sep 2017 07:12:14 GMT  (59kb,D)", "http://arxiv.org/abs/1703.05376v3", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gal dalal", "balazs szorenyi", "gugan thoppe", "shie mannor"], "accepted": false, "id": "1703.05376"}, "pdf": {"name": "1703.05376.pdf", "metadata": {"source": "CRF", "title": "Two-Timescale Stochastic Approximation Convergence Rates with Applications to Reinforcement Learning", "authors": ["Gal Dalal", "Bal\u00e1zs Sz\u00f6r\u00e9nyi", "Gugan Thoppe", "Shie Mannor"], "emails": ["gald@tx.technion.ac.il", "szorenyi.balazs@gmail.com", "gugan.thoppe@gmail.com", "shie@ee.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Stochastic approximation (SA) is the subject of an enormous literature, both theoretical and applied (Kushner & Yin, 1997), which is used to find optimal points, fixed points or zeros of a function for which only a noisy access is available. Consequently, SA is at the core of many machine learning algorithms and especially Reinforcement Learning (RL) algorithms, especially when function approximations are used. The most powerful analysis tool for SA algorithms is the Ordinary Differential Equation (ODE) method (Borkar & Meyn, 2000).The basic idea of the ODE method is that, under the right conditions, the noise effects are average and the SA iterations closely follow the trajectory of the so-called limiting ODE."}, {"heading": "1.1 Related Work", "text": "Two-time scale methods are leading in RL (Peters & Schaal, 2008; Bhatnagar et al., 2009b; Sutton et al., 2009b). However, as already mentioned, there are no concentration limits for this type of algorithm. Below, the concentration limits for individual time scale methods and asymptotic convergence results for two-time RL algorithms are shown. A broad, rigorous study by SA is given in (Borkar, 2008); it contains concentration limits for individual time scale methods. A recent paper (Thoppe & Borkar, 2015) gets tighter concentration limits under weaker assumptions for a variable methodology called Alekseev's Formula. In the context of RL, 2002; Korda & Prashanth, 2015."}, {"heading": "1.2 Our Contributions", "text": "Our main contributions cover three aspects: \u2022 We provide the first concentration limit for two-line SA algorithms; in particular, we analyze the linear SA case. Analysis is a general methodology that can be used in various areas as a \"hammer\" in plug-and-play fashion. \u2022 In particular, we show how our tool can achieve concentration limits for the two-line RL algorithms of the gradient TD family: GTD (0), GTD2 and TDC. We are the first to obtain concentration limits for the above algorithms in their original form. \u2022 We eliminate the usual square sum assumption of step sizes (see note 1). Therefore, our tool is relevant for a broader family of step sizes."}, {"heading": "2 Preliminaries", "text": "(1) Generic Two-Timeframe Analysis (1). (1) Generic Two-Timeframe Analysis (1). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2). (2........). (2.). (2. (2.). (2.). (2. (2.). (2.). (2. (2.). (2. (2.). (2. (2.). (2. (2.). (2. (2.). (2. (2.). (2. (2.). (2. (2.). (2.). (2. (2.). (2. (2.). (2.). (2. (2.). (2. (2.).). (2. (2.). (2.). (2.). (2. (2.). (2.). (2.). (2. (2.). (2.). (2.). (2.). (2.). (2.). (2. (2.). (2.). (2. (2. (2.). (2.).). (2.). (2. (2. (........................). (2. (2.). (2. (2.).). (2.). (2.). (2. (2. (2.).). (2. (2.). (2.). (2. (2.). (2. (2.). (2."}, {"heading": "3 Main Result", "text": "Let us Q1, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q2, Q"}, {"heading": "4 Linear Two Timescale SA Analysis", "text": "As a first step, we define the linear interpolated trajectories of the iterates in each time scale. A continuous version of the discrete SA algorithm enables our analyses. All the evidence for the results in this section is listed in Appendix A."}, {"heading": "4.1 Analysis Preliminaries", "text": "Let t0 = s0 = 0 and for all n \u2265 0, tn + 1 = tn + \u03b1n and sn + 1 = sn + \u03b2n. (9) Let \u03b8 (\u00b7) be the linear interpolation of {\u03b8n} to {tn}; i.e., let \u03b8 (tn) = \u03b8n and, for \u0432 (tn, tn + 1), let\u03b8 [tn) + (\u03c4 \u2212 tn). (10) Let z (\u00b7) be the linear interpolation of {zn}, but in the time steps {sn}. (For ds [tn, tn + 1), let\u044b (\u0432): =. (sn + \u03b2n) \u2212. (10) Let z (\u00b7) be the linear interpolation of {sn}, but in the time steps {sn}. Let 1, tn + 1), let\u044b (\u0432 + 1), let us define the solution (sn \u00b2) of {searn-line} (mapping)."}, {"heading": "Overall Analysis Outline", "text": "The key idea in our analysis is to compare SA trajectories \u043d (t) and z (s) with their respective limiting ODE trajectories \u03b8 (t), z (s). If the step sizes are small enough, it is more difficult for the noise to disrupt SA trajectories. A similar relationship exists between z (s) and z (s).Proving Theorem 3.1 is performed in two steps. First, we use the formula Variation of Constants (VoC) (Lakshmikantham & Deo, 1998) to quantify the distance of the disturbed trajectories."}, {"heading": "4.2 A Smart Decomposition of The Event of Interest", "text": "For an event E, let Ec be its complement. Fix sufficiently large T > > 0. We will say later how large it should be. Select n1 > 0 such event Ec (T), defined in (13), which is easier for analysis. (16) This is possible as {\u03b1n} satisfactory (4). Our goal is to construct a superset for the event Ec (T), defined in (13), which additionally contains the information about what happens until time tn1.Remark 7. Using standard ODE literature, limt \u2192 Solution (t) = Routine G1 is positively defined by A1, d dt."}, {"heading": "4.4 Concentration Bounds for Two Time Scale SA", "text": "In summary, subsection 4.3, about the good event Gn, is bounded from above by \u03bdn + 1, \u03c1n + 1, \u03bd \u0445 n + 1 and \u03c1 \u0445 n + 1 by three types of terms: i) sum of the Martingale differences, ii) exponentially dying term and iii) gradual term. For sufficiently large n, terms of type i) small are highly likely due to A3 and the Azuma Hoeffding concentration inequality; type ii) terms small for sufficiently large n; type iii) terms small for sufficiently small step sizes. On this basis, our main technical result in theorem 4.2 \u2212 theorem 3.1 is then trivial. Let us assume the following additional assumptions about step sizes, with the constants in appendix A.3.4. max."}, {"heading": "5 Applications to Two-timescale RL", "text": "In this section we will show how our novel machinery implies concentration limits on the standard two-timeframe level = > Leave RL methods with linear functional approximation, in a plug-and-play manner. We will consider the problem of policy evaluation and use the standard RL framework and notations detailed in Appendix A.4. We assume that linear functional approximation (sn), v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v (s) v) v (s) v) n (s) v (s) v) v) v (s) v) v (s) v) v) v) v) n (s) v (s) v) v) v) v) v) v) v) n) n) n) n) n (s) n) n) n (s) n) n) n) n) n) n) n) n) n (s) n) n) n) n) n) n) n) n (n) n) n) n) n) n) n (n) n) n) n) n) n) n (n) n) n) n) n) n) n (n) n) n) n) n) n (n) n) n) n) n (n) n) n) n) n) n) n) n (n) n) n) n (n) n) n) n) n (n) n) n) n) n (n) n) n) n) n (n) n) n (n) n) n) n (n) n) n) n (n) n) n) n) n) n (n) n) n) n) n) n (n) n) n) n) n (n) n) n (n) n) n) n (n) n) n) n"}, {"heading": "6 Discussion", "text": "In this work, we have reached the first concentration limit for two-line SA algorithms, which we provide as a general methodology applicable to all linear two-line SA algorithms. A natural extension of our methodology is to consider the case of nonlinear functional approximation in a manner similar to (Thoppe & Borkar, 2015).Such a result may be of great interest due to the recent growing appeal of neural networks in the RL community. An additional direction for future research is to expand to action-critical RL algorithms, in addition to the gradient TD methods studied here."}, {"heading": "A Supplementary Material", "text": "This section contains all the evidence for the lemmas and theorems presented in the paper, and provides additional technical results to support several of these proofs."}, {"heading": "A.1 Proofs from Subsection 4.2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Let R\u2217 :=", "text": "(23) On Gn, for k \u00b2 0,.., n \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c, c \u00b2, c \u00b2, c, c \u00b2, c, c \u00b2, c \u00b2, c, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c, c \u00b2, c \u00b2, c, c \u00b2, c, c \u00b2, c, c \u00b2, c, c \u00b2, c, c, c \u00b2, c, c \u00b2, c, c, c \u00b2, c, c \u00b2, c, c \u00b2, c \u00b2, c, c \u00b2, c \u00b2, c, c \u00b2, c \u00b2, c \u00b2, c, c \u00b2, c \u00b2, c \u00b2, c, c \u00b2, c \u00b2, c, c \u00b2, c \u00b2, c \u00b2, c, c \u00b2, c \u00b2, c, c \u00b2, c \u00b2, c \u00b2, c \u00b2, c, c \u00b2, c, c \u00b2, c \u00b2, c \u00b2, c, c, c \u00b2, c \u00b2, c, c \u00b2, c, c, c \u00b2, c, c, c \u00b2, c, c, c, c, c, c \u00b2, c, c, c, c, c, c, c \u00b2, c, c, c, c, c, c \u00b2, c, c, c, c, c, c \u00b2, c, c, c, c, c, c, c, c, c, c, c, c, c \u00b2, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c"}, {"heading": "A.2 Proofs from Subsection 4.3", "text": "11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,"}, {"heading": "A.3 Proofs from Subsection 4.4", "text": "A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.8 for the A.8 for the A.8 for the A.8 for the A.8 for the A.8 for the A.8 for the A.8 for the A.8 for the A.8 for the A.8 for the A.8 for the A.8 for the A.8 for the A.8 for the A.8 for the A.8 A.8 for the A.8 A.8 for the A.8 for the A.8 for the A.8 A.8 for the A.8 A.8 for the A.8 A.8 for the A.8 A.8 for the A.8 A.8 for the A.8 A.8 for the A.8 for the A.8 A.8 for the A.8 A.8 for the A.8 for the A.8 A.8 for the A.8 for the A.8 A.8 for the A.8 A.8 A.8 for the A.8 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9 for the A.9"}, {"heading": "A.4 Proofs from Section 5", "text": "We begin with a presentation of the RL framework in this section. An MDP is defined by the 5-fold (S, A, P, R, \u03b3) (Sutton, 1988), where S is the set of states, A is the set of actions, P = P (s) | s, a) is the transition core, R (s, a, s) is the reward function, and \u03b3 (0, 1) is the discount factor. At each time step the process is in a state sn, an, s). Let us follow the policy: S \u2192 A will get a stationary mapping of states to actions of the next state s \"n\" S according to a transition core P (sn, an, s \"n) and an immediate reward rn according to R (sn, an, s\" n)."}, {"heading": "A.5 GTD2", "text": "The algorithm of GTD2 (Sutton et al., 2009b) minimizes the objective functionJMSPBE (\u03b8) = 12 (b \u2212 A\u03b8) > C \u2212 1 (b \u2212 A\u03b8). (35) The updating rule of the algorithm takes the form of equations (1) and (2) mith1 (\u03b8, w) = A > w, h2 (\u03b8, w) = b \u2212 A\u03b8 \u2212 Cw, andM (1) n + 1 = (\u03c6n \u2212 formances) n > nwn \u2212 A > wn, M (2) n + 1 = rn\u03c6n + \u03c6n [accelerctun] > \u03b8n \u2212 accelern\u03c6 > nwn \u2212 n \u2212 Cwn]. That is, in the case of GTD2 the relevant matrices in the updating rules are in the form \u04411 = 0, W1 = \u2212 A >, v1 \u2212 formances \u2212 n = C, v2 = addition."}, {"heading": "A.6 TDC", "text": "The TDC algorithm is designed in such a way that it (35) is minimized, just like the GD2.The updating rule of the algorithm takes the form of equations (1) and (2) mith1\u03b8 (\u03b8, w) = b \u2212 A\u03b8 + [A > \u2212 C] w, h2 (\u03b8, w) = b \u2212 A\u03b8 \u2212 Cw, andM (1) n + 1 = rn\u03c6n + \u03c6n [\u03b3n \"n \u2212 \u03c6n] > \u03b8n \u2212 \u03b3n > \u03b8n \u2212 \u2212 A \u2212 C] > \u2212 [b \u2212 A\u03b8n + [A > C] wn], M (2) + 1 = rn\u03c6n [accelern] > \u03b8n \u2212 \u03b3n > \u03b8n \u2212 \u03b3n > \u03b8n \u2212 A \u2212 C] > In the case of the TDC, the relevant matrices in the updating rules are also positive (1 = A, W1 = [C \u2212 A > MM], VISTICTICTICTICTICTICTICTICTICTICTICICTICICTICICTICTICTICTICICICTICTICICINICTICTICTICICTICICTICTICICICICTICTICICTICTICTICTICTICTICTICICTICTICTICTICTICIF and INICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICTICs are), and the algorithms are designed to be minimized (35)."}], "references": [{"title": "Dynamic Programming and Optimal Control", "author": ["D.P. Bertsekas"], "venue": "Vol II. Athena Scientific, fourth edition,", "citeRegEx": "Bertsekas,? \\Q2012\\E", "shortCiteRegEx": "Bertsekas", "year": 2012}, {"title": "Convergent temporal-difference learning with arbitrary smooth function approximation", "author": ["Bhatnagar", "Shalabh", "Precup", "Doina", "Silver", "David", "Sutton", "Richard S", "Maei", "Hamid R", "Szepesv\u00e1ri", "Csaba"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bhatnagar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bhatnagar et al\\.", "year": 2009}, {"title": "Stochastic approximation: a dynamical systems viewpoint", "author": ["Borkar", "Vivek S"], "venue": null, "citeRegEx": "Borkar and S.,? \\Q2008\\E", "shortCiteRegEx": "Borkar and S.", "year": 2008}, {"title": "The ode method for convergence of stochastic approximation and reinforcement learning", "author": ["Borkar", "Vivek S", "Meyn", "Sean P"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Borkar et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Borkar et al\\.", "year": 2000}, {"title": "Actor-Critic Algorithms", "author": ["Konda", "Vijaymohan"], "venue": "PhD thesis, Department of Electrical Engineering and Computer Science, MIT,", "citeRegEx": "Konda and Vijaymohan.,? \\Q2002\\E", "shortCiteRegEx": "Konda and Vijaymohan.", "year": 2002}, {"title": "On td (0) with function approximation: Concentration bounds and a centered variant with exponential convergence", "author": ["Korda", "Nathaniel", "Prashanth", "LA"], "venue": "In ICML, pp", "citeRegEx": "Korda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Korda et al\\.", "year": 2015}, {"title": "Stochastic Approximation Algorithms and Applications", "author": ["Kushner", "Harold J", "Yin", "G. George"], "venue": null, "citeRegEx": "Kushner et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Kushner et al\\.", "year": 1997}, {"title": "Method of variation of parameters for dynamic systems", "author": ["Lakshmikantham", "Vangipuram", "Deo", "Sadashiv G"], "venue": null, "citeRegEx": "Lakshmikantham et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lakshmikantham et al\\.", "year": 1998}, {"title": "Finite-sample analysis of proximal gradient td algorithms", "author": ["Liu", "Bo", "Ji", "Ghavamzadeh", "Mohammad", "Mahadevan", "Sridhar", "Petrik", "Marek"], "venue": "In UAI,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Sutton", "Richard S"], "venue": "Machine learning,", "citeRegEx": "Sutton and S.,? \\Q1988\\E", "shortCiteRegEx": "Sutton and S.", "year": 1988}, {"title": "A convergent o (n) temporal-difference algorithm for off-policy learning with linear function approximation", "author": ["Sutton", "Richard S", "Maei", "Hamid R", "Szepesv\u00e1ri", "Csaba"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["Sutton", "Richard S", "Maei", "Hamid Reza", "Precup", "Doina", "Bhatnagar", "Shalabh", "Silver", "David", "Szepesv\u00e1ri", "Csaba", "Wiewiora", "Eric"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "An emphatic approach to the problem of off-policy temporal-difference learning", "author": ["Sutton", "Richard S", "Mahmood", "A Rupam", "White", "Martha"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Sutton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2015}, {"title": "Ordinary differential equations and dynamical systems", "author": ["Teschl", "Gerald"], "venue": null, "citeRegEx": "Teschl and Gerald.,? \\Q2004\\E", "shortCiteRegEx": "Teschl and Gerald.", "year": 2004}, {"title": "A concentration bound for stochastic approximation via alekseev\u2019s formula", "author": ["Thoppe", "Gugan", "Borkar", "Vivek S"], "venue": null, "citeRegEx": "Thoppe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Thoppe et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "In addition to convergence, there also exists a concentration result for the GTD family, though only for the single-timescale setting (Liu et al., 2015).", "startOffset": 134, "endOffset": 152}, {"referenceID": 0, "context": "It is known that A is positive definite (Bertsekas, 2012).", "startOffset": 40, "endOffset": 57}], "year": 2017, "abstractText": "Two-timescale Stochastic Approximation (SA) algorithms are widely used in Reinforcement Learning (RL). Their iterates have two parts that are updated with distinct stepsizes. In this work we provide a recipe for analyzing two-timescale SA. Using it, we develop the first convergence rate result for them. From this result we extract key insights on stepsize selection. As an application, we obtain convergence rates for two-timescale RL algorithms such as GTD(0), GTD2, and TDC.", "creator": "LaTeX with hyperref package"}}}