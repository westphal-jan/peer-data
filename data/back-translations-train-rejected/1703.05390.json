{"id": "1703.05390", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting", "abstract": "Keyword spotting (KWS) constitutes a major component of human-technology interfaces. Maximizing the detection accuracy at a low false alarm (FA) rate, while minimizing the footprint size, latency and complexity are the goals for KWS. Towards achieving them, we study Convolutional Recurrent Neural Networks (CRNNs). Inspired by large-scale state-of-the-art speech recognition systems, we combine the strengths of convolutional layers and recurrent layers to exploit local structure and long-range context. We analyze the effect of architecture parameters, and propose training strategies to improve performance. With only ~230k parameters, our CRNN model yields acceptably low latency, and achieves 97.71% accuracy at 0.5 FA/hour for 5 dB signal-to-noise ratio.", "histories": [["v1", "Wed, 15 Mar 2017 21:20:44 GMT  (308kb)", "http://arxiv.org/abs/1703.05390v1", "Submitted to Interspeech 2017"], ["v2", "Wed, 24 May 2017 00:37:05 GMT  (316kb)", "http://arxiv.org/abs/1703.05390v2", "Submitted to Interspeech 2017"], ["v3", "Tue, 4 Jul 2017 22:49:18 GMT  (302kb)", "http://arxiv.org/abs/1703.05390v3", "Accepted to Interspeech 2017"]], "COMMENTS": "Submitted to Interspeech 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["sercan o arik", "markus kliegl", "rewon child", "joel hestness", "rew gibiansky", "chris fougner", "ryan prenger", "adam coates"], "accepted": false, "id": "1703.05390"}, "pdf": {"name": "1703.05390.pdf", "metadata": {"source": "CRF", "title": "Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting", "authors": ["Sercan \u00d6. Ar\u0131k", "Markus Kliegl", "Rewon Child", "Joel Hestness", "Andrew Gibiansky", "Chris Fougner", "Ryan Prenger", "Adam Coates"], "emails": ["sercanarik@baidu.com,", "klieglmarkus@baidu.com"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2. Small-footprint keyword spotting", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. End-to-end architecture", "text": "We focus on a canonical CRNN architecture, inspired by the successful large-scale speech recognition systems [12-14]. To adapt these architectures for KWS with a small footprint, the model size must be shrunk by two to three orders of magnitude. We analyze the effects of various parameters on performance while reducing the size of the model. Figure 1 shows the CRNN architecture with the corresponding parameters. The raw time domain inputs are converted into per-channel normalized (PCEN) mel spectrograms [8], for precise representation and efficient training. (Other input representations we have experimented with have shown poorer performance for model architectures of comparable size.) The 2-D PCEN features are given as inputs to the revolutionary layer that use 2-D filters along the time and frequency dimensions."}, {"heading": "2.2. End-to-end training", "text": "In speech recognition, large architectures with recurring layers typically use variants of CTC loss to decipher the most likely output label. Aside from the modeling constraints due to conditional target independence assumptions, CTC loss has prohibitively high computational complexity and typically performs well only when the model size is large enough to learn from a large dataset. As we focus on small-scale architectures, the loss function optimized during training is selected as a CE loss for the estimated and targeted binary labels indicating whether a frame matches a keyword or a note. To train with a CE loss, in contrast to the CTC, the precise alignment of training samples is important. We use Deep Speech 2 [14], a large-area speech recognition model to obtain the estimated probability distributions of keyword characters \"(1), as we get the keyword decoded while the decoded values are clearly delivered."}, {"heading": "3. Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Data and training", "text": "We develop our KWS system for the keyword \"TalkType\" (which can be pronounced as a single word or two words), choosing an image length of T = 1.5 seconds, long enough to capture a reasonable pronunciation of \"TalkType.\" Using a sampling rate of 16 kHz, each image contains 24k raw time domain samples. Appropriate PCEN 25 ms mel spectrograms are obtained for 10 ms increments and 40 channels, giving an input dimensionality of 40 '151. The entire data set consists of ~ 16k different samples collected from more than 5k speakers. The data set is divided into training, development and test kits with a ratio of 6-1-1, with training samples divided by the use of additive noise with power determined by a signal-to-noise ratio (SNR)."}, {"heading": "3.2. Impact of the model architecture", "text": "Table 1 shows the performance of the various CRNN architectures set for development at 5 dB ~ SNR. We note that all models have been trained to convergence, although it requires very different numbers of eras. We observe the general trend that the larger model size typically yields better performance. Increasing the number of folding filters or increasing the number of recurring hidden units are the two effective approaches to improving performance. Increasing the number of recurring layers has a limited impact, and GRU is preferred over LSTM as the better performance that can be achieved for lower complexity. It is desirable to limit the model size given the resource constraints on inference latency, memory and power consumption. Subsequently [7] we select the size limit of 250k (which is more than 6 times smaller than the architecture with CTC loss in [11]). For the rest of the paper, the standard architecture is the unit of measurement for Nexparency and inference."}, {"heading": "3.3. Impact of the amount of training data", "text": "Figure 2 shows the FRR at 0.5 FA / hour (for the 5 dB SNR test set) compared to the number of unique \"talkType\" samples used during training; saturation of performance is faster than in applications with similar data, but using large scale models, e.g. [14]. In addition to increasing the amount of positive samples, we observe an improvement in performance through training with negative samples obtained through hard mining. We collect negative samples by using the preconvergent model on a very large public video data set (which is not used in training, development or test sets), and then continue training with the negative samples collected until convergence."}, {"heading": "3.4. Noise robustness", "text": "For the test set with different SNR values, Fig. 3 shows the FRR vs. FA per hour. For a higher SNR, a lower FRR is achieved, and stable performance starts at a lower FA rate. Note that the SNR values (in dB) of the extended training samples are taken from a distribution with an average of 5 dB, and a deterioration in performance beyond this value is observed. Performance for lower SNR values can be improved by an increase with lower SNR, but this comes at the expense of a decreased performance for a higher SNR due to the limited learning ability of the model. We observe the advantage of recurring layers, especially at lower SNR values. The performance gap of CRNN architectures with CNN architectures (adapted to [7] as explained in Section 3.1) decreases with increasing SNR."}, {"heading": "3.5. Far-field robustness", "text": "Our data set already consists of samples recorded at different distance values, which should be representative for most applications such as smartphone KWS systems. However, some applications, such as smart home KWS systems, require high performance under far-field conditions. Fig.4 shows a performance deterioration with additional distance. Far-field test sets are constructed by adding impulse responses to the original test set that correspond to a variety of configurations at the given distance (taking into account different values for degrees of arrival, etc.) Significant performance deteriorations are observed in particular in connection with higher noise, as explained in [19]. To ensure robustness against this deterioration, we consider training with extended training samples in far-field areas using a variety of impulse responses that differ from those in the test set. This deterioration in performance results in significantly smaller distances, but results in poorer performance for the original data set, which can also be attributed to learning capacity."}, {"heading": "4. Conclusions", "text": "We examined CRNNs for small-footprint KWS systems. We presented the trade-off between model size and performance and demonstrated the optimal selection of parameters given the trade-off. The capacity limitation of the model has several effects. Performance gains are limited by merely increasing the number of positive samples, but hard negative mining improves performance. Training sets should be carefully selected to reflect the application environment, such as noise level or long-range conditions. Overall, our model reaches 97.71%, 98.71% and 99.3% accuracy for the test set with 5 dB, 10 dB and 20 dB SNR values, respectively. Our numerical performance results may appear better than other KWS models in the literature. However, a direct comparison is due to the difference between the data sets and the actual key words, i.e. the consequence, we still do not believe that human performance in the WS task is outstanding."}, {"heading": "5. Acknowledgements", "text": "Discussions with Andrew Ng, Sanjeev Satheesh, Jiaji Huang, Jue Sun and Bing Jiang will be appreciated. We thank Hui Song for the impulse response measurements used for far field magnification."}, {"heading": "6. References", "text": "[1] J.R. Rohlicek, W. Russell, S. Roukos, and H. Gish, \"Continuous hidden Markov modeling for speaker-independent wordspotting,\" in IEEE Proceedings of the International Conference on Acoustics, Speech and Signal Processing, 1990, pp. 627-630. [2] G. Chen, C. Parada, and G. Heigold, \"Small-footprint Keyword Spotting, G. Fu, and S. Vitaladevuni,\" in Proceedings International Conference on Acoustics, Speech, and Signal Processing, 2014, pp. 4087-4091. [3] G. Tucker, M. XiXiXiXiXiu, M. Sun, S. Panchapagesan, G. Fu, and S. Vitaladevuni, \"Model compression, and Signal Processing, Spotting,\" in Proceedings of Interspeech, 2016, pp. 1393-1397 [4] Vikas Sindhwani, Tara N. Sainath and Sanjiar Kumar. \""}], "references": [{"title": "Continuous hidden Markov modeling for speaker-independent wordspotting", "author": ["J.R. Rohlicek", "W. Russell", "S. Roukos", "H. Gish"], "venue": "IEEE Proceedings of the International Conference on Acoustics, Speech and Signal Processing, 1990, pp. 627\u2013630.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Small-footprint keyword spotting using deep neural networks", "author": ["G. Chen", "C. Parada", "G. Heigold"], "venue": "Proceedings International Conference on Acoustics, Speech, and Signal Processing, 2014, pp. 4087-4091.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Model compression applied to small-footprint keyword spotting", "author": ["G. Tucker", "M. Wu", "M. Sun", "S. Panchapagesan", "G. Fu", "S. Vitaladevuni"], "venue": "Proceedings of Interspeech, 2016, pp. 1393-1397", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Structured transforms for small-footprint deep learning", "author": ["Vikas Sindhwani", "Tara N. Sainath", "Sanjiv Kumar"], "venue": "Neural Information Processing Systems, 2015, pp. 3088-3096.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic gain control and multi-style training for robust small-footprint keyword spotting with deep neural networks", "author": ["R. Prabhavalkar", "R. Alvarez", "C. Parada", "P. Nakkiran", "T.N. Sainath"], "venue": "IEEE Proceedings of the International Conference on Acoustics, Speech and Signal Processing, pp. 4704\u20134708.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 0}, {"title": "Multi-task learning and weighted cross-entropy for dnn-based keyword spotting", "author": ["S. Panchapagesan", "M. Sun", "A. Khare", "S. Matsoukas", "A. Mandal", "B. Hoffmeister", "S. Vitaladevuni"], "venue": "Proceedings of Interspeech, 2016, pp. 760-764.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional neural networks for small-footprint keyword spotting", "author": ["T.N. Sainath", "C. Parada"], "venue": "Proceedings of Interspeech, 2015, pp. 1478-1482", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Trainable frontend for robust and far-field keyword spotting", "author": ["Y. Wang", "P. Getreuer", "T. Hughes", "R.F. Lyon", "R.A. Saurous"], "venue": "arXiv preprint, arXiv:1607.05666, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Online keyword spotting with a character-level recurrent neural network", "author": ["K. Hwang", "M. Lee", "W. Sung"], "venue": "arXiv preprint arXiv:1512.08903, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "An application of recurrent neural networks to discriminative keyword spotting", "author": ["S. Fernandez", "A. Graves", "J. Schmidhuber"], "venue": "Artificial Neural Networks. Springer, 2007, pp. 220\u2013229.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "An end-to-end architecture for keyword spotting and voice activity detection", "author": ["C. Lengerich", "A. Hannun"], "venue": "arXiv preprint arXiv:1611.09405, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Ensemble deep learning for speech recognition", "author": ["L. Deng", "J.C. Platt"], "venue": "Proceedings of Interspeech, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior A", "H. Sak"], "venue": "IEEE Proceedings of the International Conference on Acoustics, Speech and Signal Processing, 2015, pp. 4580-4584.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Speech 2: End-to-end speech recognition in English and Mandarin", "author": ["D. Amodei"], "venue": "arXiv preprint arXiv:1512.02595, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735-1780, 1997.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparison between auditory and visual simple reaction times", "author": ["J. Shelton", "G.P. Kumar GP"], "venue": "Neuroscience and medicine, vol. 1 no. 1, pp. 30-32, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Microphone array processing for distant speech recognition: Towards real-world deployment", "author": ["K. Kumatani"], "venue": "IEEE Asia- Pacific Signal & Information Processing Association Annual Summit and Conference, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Traditional approaches for KWS are based on Hidden Markov Models with sequence search algorithms [1].", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "More recently, with the advances in deep learning and increase in the amount of available data, state-ofthe-art KWS has been replaced by deep learning-based approaches due to their superior performance [2].", "startOffset": 202, "endOffset": 205}, {"referenceID": 2, "context": "Deep learning-based KWS systems commonly use Deep Neural Networks (DNNs) combined with compression techniques [3,4] or multi-style training approaches [5,6].", "startOffset": 110, "endOffset": 115}, {"referenceID": 3, "context": "Deep learning-based KWS systems commonly use Deep Neural Networks (DNNs) combined with compression techniques [3,4] or multi-style training approaches [5,6].", "startOffset": 110, "endOffset": 115}, {"referenceID": 4, "context": "Deep learning-based KWS systems commonly use Deep Neural Networks (DNNs) combined with compression techniques [3,4] or multi-style training approaches [5,6].", "startOffset": 151, "endOffset": 156}, {"referenceID": 5, "context": "Deep learning-based KWS systems commonly use Deep Neural Networks (DNNs) combined with compression techniques [3,4] or multi-style training approaches [5,6].", "startOffset": 151, "endOffset": 156}, {"referenceID": 6, "context": "With the goal of exploiting such local connectivity patterns by shared weights, Convolutional Neural Networks (CNNs) were explored for KWS [7,8].", "startOffset": 139, "endOffset": 144}, {"referenceID": 7, "context": "With the goal of exploiting such local connectivity patterns by shared weights, Convolutional Neural Networks (CNNs) were explored for KWS [7,8].", "startOffset": 139, "endOffset": 144}, {"referenceID": 8, "context": "Recurrent neural networks (RNNs) were also studied for KWS with connectionist temporal classification (CTC) loss [9,10], unlike the aforementioned DNN and CNN models [2-6] with cross-entropy (CE) loss.", "startOffset": 113, "endOffset": 119}, {"referenceID": 9, "context": "Recurrent neural networks (RNNs) were also studied for KWS with connectionist temporal classification (CTC) loss [9,10], unlike the aforementioned DNN and CNN models [2-6] with cross-entropy (CE) loss.", "startOffset": 113, "endOffset": 119}, {"referenceID": 1, "context": "Recurrent neural networks (RNNs) were also studied for KWS with connectionist temporal classification (CTC) loss [9,10], unlike the aforementioned DNN and CNN models [2-6] with cross-entropy (CE) loss.", "startOffset": 166, "endOffset": 171}, {"referenceID": 2, "context": "Recurrent neural networks (RNNs) were also studied for KWS with connectionist temporal classification (CTC) loss [9,10], unlike the aforementioned DNN and CNN models [2-6] with cross-entropy (CE) loss.", "startOffset": 166, "endOffset": 171}, {"referenceID": 3, "context": "Recurrent neural networks (RNNs) were also studied for KWS with connectionist temporal classification (CTC) loss [9,10], unlike the aforementioned DNN and CNN models [2-6] with cross-entropy (CE) loss.", "startOffset": 166, "endOffset": 171}, {"referenceID": 4, "context": "Recurrent neural networks (RNNs) were also studied for KWS with connectionist temporal classification (CTC) loss [9,10], unlike the aforementioned DNN and CNN models [2-6] with cross-entropy (CE) loss.", "startOffset": 166, "endOffset": 171}, {"referenceID": 5, "context": "Recurrent neural networks (RNNs) were also studied for KWS with connectionist temporal classification (CTC) loss [9,10], unlike the aforementioned DNN and CNN models [2-6] with cross-entropy (CE) loss.", "startOffset": 166, "endOffset": 171}, {"referenceID": 10, "context": "Recently, [11] proposed a Convolutional Recurrent Neural Network (CRNN) architecture with CTC loss.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "We focus on a canonical CRNN architecture, inspired by the successful large-scale speech recognition systems [12-14].", "startOffset": 109, "endOffset": 116}, {"referenceID": 12, "context": "We focus on a canonical CRNN architecture, inspired by the successful large-scale speech recognition systems [12-14].", "startOffset": 109, "endOffset": 116}, {"referenceID": 13, "context": "We focus on a canonical CRNN architecture, inspired by the successful large-scale speech recognition systems [12-14].", "startOffset": 109, "endOffset": 116}, {"referenceID": 7, "context": "converted to per-channel energy normalized (PCEN) mel spectrograms [8], for succinct representation and efficient", "startOffset": 67, "endOffset": 70}, {"referenceID": 14, "context": "The outputs of the convolutional layer are fed to bidirectional recurrent layers, which might include gated recurrent units (GRUs) [15] or long short-term memory (LSTM) units [16].", "startOffset": 131, "endOffset": 135}, {"referenceID": 15, "context": "The outputs of the convolutional layer are fed to bidirectional recurrent layers, which might include gated recurrent units (GRUs) [15] or long short-term memory (LSTM) units [16].", "startOffset": 175, "endOffset": 179}, {"referenceID": 13, "context": "We use Deep Speech 2 [14], a large-scale speech recognition model, to obtain the estimated probability distributions of keyword characters c\" (1 \u2264 k \u2264 K) for each time instance.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "Training samples are augmented by applying additive noise, with a power determined by a signal-to-noise ratio (SNR) value sampled from [-5,15] dB interval.", "startOffset": 135, "endOffset": 142}, {"referenceID": 16, "context": "We use the ADAM optimization algorithm for training [17], with a batch size of 64.", "startOffset": 52, "endOffset": 56}, {"referenceID": 6, "context": "The metrics we focus on are the false rejection rate (FRR) and false alarms (FA) per hour, typically fixing the latter at a desired value such as 1 FA/hr [7].", "startOffset": 154, "endOffset": 157}, {"referenceID": 6, "context": "Following [7], we choose the size limit as 250k (which is more than 6 times smaller than the architecture with CTC loss in [11]).", "startOffset": 10, "endOffset": 13}, {"referenceID": 10, "context": "Following [7], we choose the size limit as 250k (which is more than 6 times smaller than the architecture with CTC loss in [11]).", "startOffset": 123, "endOffset": 127}, {"referenceID": 6, "context": "We compare the performance with a CNN architecture based on [7].", "startOffset": 60, "endOffset": 63}, {"referenceID": 17, "context": "Even when implemented on modern smartphones without any approximations and special function units, our KWS model can achieve an inference time much faster than the time scale for reactive time for humans with auditory stimuli, which is ~280 ms [18].", "startOffset": 244, "endOffset": 248}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "The performance gap of CRNN architectures with CNN architectures (adapted from [7] as explained in Section 3.", "startOffset": 79, "endOffset": 82}, {"referenceID": 18, "context": "Significant deterioration in performance is observed especially in conjunction with higher noise, as also explained in [19].", "startOffset": 119, "endOffset": 123}], "year": 2017, "abstractText": "Keyword spotting (KWS) constitutes a major component of human-technology interfaces. Maximizing the detection accuracy at a low false alarm (FA) rate, while minimizing the footprint size, latency and complexity are the goals for KWS. Towards achieving them, we study Convolutional Recurrent Neural Networks (CRNNs). Inspired by large-scale state-ofthe-art speech recognition systems, we combine the strengths of convolutional layers and recurrent layers to exploit local structure and long-range context. We analyze the effect of architecture parameters, and propose training strategies to improve performance. With only ~230k parameters, our CRNN model yields acceptably low latency, and achieves 97.71% accuracy at 0.5 FA/hour for 5 dB signal-to-noise ratio.", "creator": "Word"}}}