{"id": "1703.05407", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play", "abstract": "We describe a simple scheme that allows an agent to explore its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on (nearly) reversible environments, or environments that can be reset, and Alice will \"propose\" the task by running a set of actions and then Bob must partially undo, or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When deployed on an RL task within the environment, this unsupervised training reduces the number of episodes needed to learn.", "histories": [["v1", "Wed, 15 Mar 2017 22:27:43 GMT  (1257kb,D)", "http://arxiv.org/abs/1703.05407v1", null], ["v2", "Wed, 19 Apr 2017 23:32:25 GMT  (1227kb,D)", "http://arxiv.org/abs/1703.05407v2", null], ["v3", "Sun, 4 Jun 2017 12:44:45 GMT  (1430kb,D)", "http://arxiv.org/abs/1703.05407v3", null], ["v4", "Sun, 29 Oct 2017 16:02:21 GMT  (1415kb,D)", "http://arxiv.org/abs/1703.05407v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sainbayar sukhbaatar", "zeming lin", "ilya kostrikov", "gabriel synnaeve", "arthur szlam"], "accepted": false, "id": "1703.05407"}, "pdf": {"name": "1703.05407.pdf", "metadata": {"source": "META", "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play", "authors": ["Sainbayar Sukhbaatar", "Ilya Kostrikov", "Arthur Szlam", "Rob Fergus"], "emails": ["Sukhbaatar<sainbar@cs.nyu.edu>."], "sections": [{"heading": null, "text": "We describe a simple scheme that allows an agent to explore his surroundings unsupervised. Our scheme pits two versions of the same agent, Alice and Bob, against each other. Alice suggests a task that Bob has to complete, and then Bob tries to complete the task. In this work, we focus on (almost) reversible environments or environments that can be reset, and Alice \"suggests\" the task by performing a series of actions, and Bob must partially undo or repeat them. Through an appropriate reward structure, Alice and Bob automatically create an exploration curriculum that allows the agent to train unsupervised. If this unsupervised training is used in the environment on an RL task, the number of episodes required to learn will be reduced."}, {"heading": "1. Introduction", "text": "In environments where reward is sparse, only a small fraction of agents \"experience is directly used to update policy, contributing to inefficiency.In this essay, we introduce a novel form of unsupervised training for an agent that allows for research and learning about the environment without any external reward that encourages agents to learn how to move between states as efficiently as possible. We show that this unsupervised training allows agents to learn new tasks within the environment faster than starting with a random policy.1Department of Computer Science, Courant Institute, New York University 2Facebook AI Research, New York. Answer: Sainbayar Sukhbaatar < sainbar @ cs.nyu.edu > STOPTION task that they first abandon."}, {"heading": "2. Approach", "text": "We look at environments with a single physical agent, but we allow two separate heads: Alice and Bob, each with their own goals and parameters. At a high level, Alice's job is to suggest a task for Bob to complete, and Bob's job is to complete the task. Our approach is limited to Bob Bob's behavior: (i) those that are (almost) reversible, or (ii) those that can be reset to their original state at least once. These limitations allow us to bypass complications to communicate the task and determine its difficulty (see Section 5 for further discussion). In these two scenarios, Alice starts in an initial state of s0 and suggests a task by doing it, i.e. executing a sequence of actions that puts the agent back into a state. She then issues a STOP action that hands over control of Bob. In reversible environments, Bob's goal is to bring Bob's goal back to a certain state of it (or a certain state of it)."}, {"heading": "2.1. Parameterizing Alice and Bob\u2019s actions", "text": "Alice and Bob each have political functions that take as input two observations of state variables and output a distribution of actions. In Alice's case, the function will be that of formaAlice = fA (st, s0), where s0 is the observation of the initial state of the environment and st is the observation of the current state. In Bob's case, the function will be beaBob = fB (s \u2032 t, s \u2032 0), where s \u2032 0 = s0 if we have a reversible environment. In a reconfigurable environment, s \u2032 0 is the state in which Alice performed the stop action. Note that the \"observations\" may include a parameterized model of raw observation. When a target task is presented, the political function of the agent aTarget = fB (s \u2032 t, e) is where e is a special observation corresponding to the target task. In the experiments below, we show our approach in settings where f is tabular; where it is a neural disk network, where it is an input network, and where it is continuous."}, {"heading": "2.2. Universal Bob in the tabular setting", "text": "We now show that in environments with finite states, tabular guidelines and Markovian transitions, we can interpret the reset and reverse games as training for agents to find strategies that can move from one state to another in the least expected number of steps. Note that, as discussed above, the policy table for both Alice and Bob is indexed by Bob, not just by si. In particular, in the above assumptions, this means that there is a policy \u03c0fast, so \u03c0fast (s0, sT) has the least expected number of steps to transition from s0 to sT. Let's call such a policy a quick policy. It is clear that \u03c0fast is a universal policy for Bob, so that for any Alice policy \u03c0a is optimal in relation to \u03c0a."}, {"heading": "3. Related Work", "text": "In fact, it is as if it were a way of getting an agent to learn about his surroundings in order to stay away from other goals. This is different from our scheme in which the reward is purely internal and the self-play is a way of expanding the reward derived from other goals. SELFPLAYEPISODE (REVERSE / REPEAT) is an example of how to explore an agent's motivation in order to expand the reward."}, {"heading": "4. Experiments", "text": "The following experiments examine our approach to self-play using a variety of tasks, both continuous and discrete, from the Mazebase (Sukhbaatar et al., 2015) and the RLab (Duan et al., 2016) environment.The same protocol is applied in all environments: self-play and target task episodes are mixed together and used to train the agent using discrete policy gradient. We evaluate both the reverse and repetitive version of self-play. We show that the self-play episodes help training in terms of the number of target task episodes required to learn the task (considering the self-play episodes as \"free\" as they do not use environmental rewards). In all experiments we use the policy gradient (Williams, 1992) with a baseline to optimize strategies. In the tabular task below we use a constant baseline; in all other tasks we use the strategies we use based on a neural network and a neural state."}, {"heading": "4.1. Long hallway", "text": "We first describe a simple toy designed to illustrate the function of asymmetric self-play. Although the environment consists of M states that we do not train with other tasks: {s1,..., sM} arranged in a chain. Alice and Bob have three possible actions, \"left,\" \"right,\" or \"stop.\" If the agent is with Bob with i 6 = 1, \"left\" he takes it to si \u2212 1; \"right\" analogously increases the state index and \"stop\" transfers control to Bob when Alice performs it and ends the episode when Bob performs it. We use \"return to the state of origin\" as a self-play task (i.e. reversal in algorithm 1). For the target task we randomly select a starting state and the target state, and the episode is considered successful when Bob goes to the target state and performs the stop action before a fixed number of maximum steps. In this case, the target task is essentially the same as the self-play task, and the self-play task runs."}, {"heading": "4.2. Mazebase", "text": "We describe experiments with the MazeBase environment (Sukhbaatar et al., 2015), which have discrete actions and states, but sufficiently combinatorial complexity that tabular methods cannot be used. They consist of various elements placed on a finite 2D network; the environment is automatically generated for each episode. For both self-representations and the target task, we use an environment in which the labyrinth contains a light switch, a key and a wall with a door (see fig. 1). An agent can open or close the door by turning the light on or off. When the light is off, the agent can only change the (glowing) light. There is also a target flag in the objective task.In self-play, an episode begins with Alice in control, which can be turned on by the light and the light is turned off."}, {"heading": "4.2.1. BIASING FOR OR AGAINST SELF-PLAY", "text": "The effectiveness of our approach depends in part on the similarity between the self-play and the target tasks. One way to explore this in our environment is to vary the probability that the light will be turned off first during the self-play episods1. Note that the light in the target task is normally turned off; if the light is turned on at the start of Alice's turn in the reverse direction, Bob will learn to turn it off, and then Bob will be inclined to turn it on again. On the other hand, if the light is normally turned off at the start of Alice's turn in the reverse direction, Bob is strongly inclined against turning the light on, and so the test task becomes particularly difficult. Thus, this probability changes to 1The initial state of the light should dramatically change the behavior of the agent: when it is, the agent can approach the button directly. Notice the similarity between the two tasks."}, {"heading": "4.3. RLLab: Mountain Car", "text": "We now apply our approach to the Mountain Car task in the RLab, where the agent controls a car trapped in a 1-D valley. It must learn to build momentum by moving alternately to the left and right and climbing higher up the valley walls until it is able to escape. Although the problem is portrayed as continuous, we discredit the 1-D action space into 5 containers (equally large) that allow us to use discrete political gradients as above. An observation of the condition consists of the position and speed of the car. As in (Houthooft et al., 2016; Tang et al., 2016), a reward of + 1 is only given if the car successfully climbs the mountain, and episodes are limited to tMax = 500 steps. We use the same hyperparameters as the macebase experiments, except the batch size is 128 and the reward for self-play is scaled up by one millimetre."}, {"heading": "4.4. RLLab: Swimmer", "text": "Finally, we applied our approach to the SwimmerGather task in the RLLab (which uses the Mujoco (Todorov et al., 2012) simulator), where the agent controls a worm with two flexible joints, swimming in a 2D viscous liquid. In the target task, the agent gets reward + 1 for eating green apples and -1 for touching red bombs that are not present during the self-play. Thus, the self-play task and the target tasks are different: in the former, the worm only swims around, but in the latter, he must learn to swim towards green apples and away from the red bombs. The observation state consists of a 13-dimensional vector describing the location and common angles of the worm, and a 20-dimensional vector for scanning nearby objects. The worm takes two real values as action, each controlling a common control."}, {"heading": "5. Discussion", "text": "In this paper, we have described a new method for intrinsically motivated learning, which we call asymmetrical self-play. Despite the conceptual simplicity of the method, we have seen that it can be effective in both discrete and continuous input settings with functional approximation to promote exploration and automatically generate curricula. In evaluating it against challenging benchmarks, our approach is comparable or better than current RL methods, which provide an incentive for exploration. In addition, it can theoretically be shown that in simple environments through asymmetrical self-play with reward functions, optimal agents can switch between (1) and (2) each pair of attainable states as efficiently as possible."}, {"heading": "5.1. Meta-exploration for Alice", "text": "We want Alice and Bob to explore the state (or state) space, and we want Bob to be exposed to many different tasks. Due to the form of the standard reinforcement learning goal (expectation of rewards), Alice only wants to find the most difficult for Bob and is not interested in the space of things that are difficult for Bob. In the fully tabular setting, with fully reversible dynamics or with provisions, and without the limitations of realistic optimization strategies, we saw in Section 2.2 that this forces Bob and Alice to make any state transition as efficient as possible. However, with more realistic optimization methods or environments, and with functional approximations, Bob and Alice can get stuck in sub-optimal minimizations. For example, let's follow the argument in the third paragraph of 2.2 and assume that Bob and Alice are in equilibrium (and that we are in the tabular, finite, Markovian setting)."}, {"heading": "5.2. Communicating via actions", "text": "In this work, we limited Alice to proposing tasks for Bob by performing them. This limitation is practical and effective in confined environments that allow resetting or are (almost) reversible. It provides a solution to three of the main difficulties in implementing the basic idea of \"Alice proposes tasks, Bob does them\": parameterizing the sample of tasks, presenting and communicating the tasks, and ensuring the appropriate degree of difficulty of the tasks. Each of these tasks is interesting in more general contexts. In this work, the tasks have incentives for efficient transitions. One can imagine other reward functions and task descriptions that provide incentives to discover statistics of states and state transitions, such as models of their causality or temporal order, cluster structure."}], "references": [{"title": "Intrinsic Motivation and Reinforcement Learning, pp. 17\u201347", "author": ["Barto", "Andrew G"], "venue": null, "citeRegEx": "Barto and G.,? \\Q2013\\E", "shortCiteRegEx": "Barto and G.", "year": 2013}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Bellemare", "Marc G", "Srinivasan", "Sriram", "Ostrovski", "Georg", "Schaul", "Tom", "Saxton", "David", "Munos", "R\u00e9mi"], "venue": "In NIPS,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Curriculum learning", "author": ["Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In ICML, pp", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Duan", "Yan", "Chen", "Xi", "Houthooft", "Rein", "Schulman", "John", "Abbeel", "Pieter"], "venue": "In ICML,", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Bengio", "Yoshua"], "venue": "In NIPS, pp", "citeRegEx": "Bengio and Yoshua.,? \\Q2014\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2014}, {"title": "Curiosity-driven exploration in deep reinforcement learning via bayesian neural networks", "author": ["Houthooft", "Rein", "Chen", "Xi", "Duan", "Yan", "Schulman", "John", "Turck", "Filip De", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Houthooft et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Houthooft et al\\.", "year": 2016}, {"title": "Empowerment: a universal agent-centric measure of control", "author": ["Klyubin", "Alexander S", "Polani", "Daniel", "Nehaniv", "Chrystopher L"], "venue": "In Proceedings of the IEEE Congress on Evolutionary Computation,", "citeRegEx": "Klyubin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Klyubin et al\\.", "year": 2005}, {"title": "Selfpaced learning for latent variable models", "author": ["M.P. Kumar", "Packer", "Benjamin", "Koller", "Daphne"], "venue": "In NIPS", "citeRegEx": "Kumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2010}, {"title": "Adversarial learning for neural dialogue generation", "author": ["Li", "Jiwei", "Monroe", "Will", "Shi", "Tianlin", "Ritter", "Alan", "Jurafsky", "Dan"], "venue": "arXiv 1701.06547,", "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Exploration in model-based reinforcement learning by empirically estimating learning progress", "author": ["Lopes", "Manuel", "Lang", "Tobias", "Toussaint", "Marc", "Oudeyer", "Pierre-Yves"], "venue": "In NIPS, pp", "citeRegEx": "Lopes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lopes et al\\.", "year": 2012}, {"title": "Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks", "author": ["Mescheder", "Lars M", "Nowozin", "Sebastian", "Geiger", "Andreas"], "venue": "arXiv abs/1701.04722,", "citeRegEx": "Mescheder et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mescheder et al\\.", "year": 2017}, {"title": "Reinforcement learning for robot soccer", "author": ["Riedmiller", "Martin", "Gabel", "Thomas", "Hafner", "Roland", "Lange", "Sascha"], "venue": "Autonomous Robots,", "citeRegEx": "Riedmiller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Riedmiller et al\\.", "year": 2009}, {"title": "Some studies in machine learning using the game of checkers", "author": ["Samuel", "Arthur L"], "venue": "IBM Journal of Research and Development,", "citeRegEx": "Samuel and L.,? \\Q1959\\E", "shortCiteRegEx": "Samuel and L.", "year": 1959}, {"title": "Universal value function approximators", "author": ["Schaul", "Tom", "Horgan", "Dan", "Gregor", "Karol", "Silver", "David"], "venue": "In ICML, pp", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Curious model-building control systems", "author": ["J. Schmidhuber"], "venue": "In Proc. Int. J. Conf. Neural Networks,", "citeRegEx": "Schmidhuber,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber", "year": 1991}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["Leach", "Madeleine", "Kavukcuoglu", "Koray", "Graepel", "Thore", "Hassabis", "Demis"], "venue": "search. Nature,", "citeRegEx": "Leach et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Leach et al\\.", "year": 2016}, {"title": "Intrinsically motivated reinforcement learning", "author": ["Singh", "Satinder P", "Barto", "Andrew G", "Chentanez", "Nuttapong"], "venue": "In NIPS, pp", "citeRegEx": "Singh et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2004}, {"title": "An analysis of model-based interval estimation for markov decision processes", "author": ["Strehl", "Alexander L", "Littman", "Michael L"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Strehl et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2008}, {"title": "Mazebase: A sandbox for learning from games", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Synnaeve", "Gabriel", "Chintala", "Soumith", "Fergus", "Rob"], "venue": "arXiv 1511.07401,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["Sutton", "Richard S", "Modayil", "Joseph", "Delp", "Michael", "Degris", "Thomas", "Pilarski", "Patrick M", "White", "Adam", "Precup", "Doina"], "venue": "In AAMAS", "citeRegEx": "Sutton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "exploration: A study of count-based exploration for deep reinforcement learning", "author": ["H. Tang", "R. Houthooft", "D. Foote", "A. Stooke", "X. Chen", "Y. Duan", "J. Schulman", "F. De Turck", "P. Abbeel"], "venue": "arXiv abs/1611.04717,", "citeRegEx": "Tang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "Temporal difference learning and td-gammon", "author": ["Tesauro", "Gerald"], "venue": "Commun. ACM,", "citeRegEx": "Tesauro and Gerald.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 1995}, {"title": "Lecture 6.5 - rmsprop, coursera: Neural networks for machine", "author": ["T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2012}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In IROS,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "In Machine Learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}], "referenceMentions": [{"referenceID": 11, "context": ", 2016), and in in multi-agent games such as RoboSoccer (Riedmiller et al., 2009).", "startOffset": 56, "endOffset": 81}, {"referenceID": 8, "context": "(Li et al., 2017) introduce an adversarial approach to dialogue generation, where a generator model is subjected to a form of \u201cTuring test\u201d by a discriminator network.", "startOffset": 0, "endOffset": 17}, {"referenceID": 10, "context": "(Mescheder et al., 2017) demonstrate how adversarial loss terms can be combined with variational auto-encoders to permit more accurate density modeling.", "startOffset": 0, "endOffset": 24}, {"referenceID": 16, "context": "There is a large body of work on intrinsic motivation (Barto, 2013; Singh et al., 2004; Klyubin et al., 2005; Schmidhuber, 1991) for self-supervised learning agents.", "startOffset": 54, "endOffset": 128}, {"referenceID": 6, "context": "There is a large body of work on intrinsic motivation (Barto, 2013; Singh et al., 2004; Klyubin et al., 2005; Schmidhuber, 1991) for self-supervised learning agents.", "startOffset": 54, "endOffset": 128}, {"referenceID": 14, "context": "There is a large body of work on intrinsic motivation (Barto, 2013; Singh et al., 2004; Klyubin et al., 2005; Schmidhuber, 1991) for self-supervised learning agents.", "startOffset": 54, "endOffset": 128}, {"referenceID": 14, "context": "One line in this direction is curiosity-driven exploration (Schmidhuber, 1991).", "startOffset": 59, "endOffset": 78}, {"referenceID": 1, "context": "These techniques can be applied in encouraging exploration in the context of reinforcement learning, for example (Bellemare et al., 2016; Strehl & Littman, 2008; Lopes et al., 2012; Tang et al., 2016); Roughly, these use some notion of the novelty of a state to give a reward.", "startOffset": 113, "endOffset": 200}, {"referenceID": 9, "context": "These techniques can be applied in encouraging exploration in the context of reinforcement learning, for example (Bellemare et al., 2016; Strehl & Littman, 2008; Lopes et al., 2012; Tang et al., 2016); Roughly, these use some notion of the novelty of a state to give a reward.", "startOffset": 113, "endOffset": 200}, {"referenceID": 20, "context": "These techniques can be applied in encouraging exploration in the context of reinforcement learning, for example (Bellemare et al., 2016; Strehl & Littman, 2008; Lopes et al., 2012; Tang et al., 2016); Roughly, these use some notion of the novelty of a state to give a reward.", "startOffset": 113, "endOffset": 200}, {"referenceID": 6, "context": "Another line of work on intrinsic motivation is a formalization of the notion of empowerment (Klyubin et al., 2005), or how much control the agent has over its environment.", "startOffset": 93, "endOffset": 115}, {"referenceID": 2, "context": "Curriculum learning (Bengio et al., 2009) is widely used in many machine learning approaches.", "startOffset": 20, "endOffset": 41}, {"referenceID": 7, "context": "Previous automatic approaches, such as (Kumar et al., 2010), rely on monitoring training error.", "startOffset": 39, "endOffset": 59}, {"referenceID": 19, "context": "Our basic paradigm of \u201cAlice proposing a task, and Bob doing it\u201d is related to the Horde architecture (Sutton et al., 2011) and (Schaul et al.", "startOffset": 102, "endOffset": 123}, {"referenceID": 13, "context": ", 2011) and (Schaul et al., 2015).", "startOffset": 12, "endOffset": 33}, {"referenceID": 18, "context": "The following experiments explore our self-play approach on a variety of tasks, both continuous and discrete, from the Mazebase (Sukhbaatar et al., 2015) and RLLab (Duan et al.", "startOffset": 128, "endOffset": 153}, {"referenceID": 3, "context": ", 2015) and RLLab (Duan et al., 2016) environments.", "startOffset": 18, "endOffset": 37}, {"referenceID": 18, "context": "We now describe experiments using the MazeBase environment (Sukhbaatar et al., 2015).", "startOffset": 59, "endOffset": 84}, {"referenceID": 5, "context": "As in (Houthooft et al., 2016; Tang et al., 2016), a reward of +1 is given only when the car succeeds in climbing the hill, 0.", "startOffset": 6, "endOffset": 49}, {"referenceID": 20, "context": "As in (Houthooft et al., 2016; Tang et al., 2016), a reward of +1 is given only when the car succeeds in climbing the hill, 0.", "startOffset": 6, "endOffset": 49}, {"referenceID": 5, "context": "7, we compare this to current state-of-the-art methods, namely VIME (Houthooft et al., 2016) and SimHash (Tang et al.", "startOffset": 68, "endOffset": 92}, {"referenceID": 20, "context": ", 2016) and SimHash (Tang et al., 2016).", "startOffset": 20, "endOffset": 39}, {"referenceID": 23, "context": "Finally, we applied our approach to the SwimmerGather task in RLLab (which uses the Mujoco (Todorov et al., 2012) simulator), where the agent controls a worm with two flexible joints, swimming in a 2D viscous fluid.", "startOffset": 91, "endOffset": 113}, {"referenceID": 5, "context": "A comparison of our self-play approach on MountainCar task with VIME (Houthooft et al., 2016) and SimHash (Tang et al.", "startOffset": 69, "endOffset": 93}, {"referenceID": 20, "context": ", 2016) and SimHash (Tang et al., 2016) (figure adapted from (Tang et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 20, "context": ", 2016) (figure adapted from (Tang et al., 2016)).", "startOffset": 29, "endOffset": 48}, {"referenceID": 5, "context": "The episode length is 500 steps for target tasks as in (Houthooft et al., 2016; Tang et al., 2016), and 600 for self-play.", "startOffset": 55, "endOffset": 98}, {"referenceID": 20, "context": "The episode length is 500 steps for target tasks as in (Houthooft et al., 2016; Tang et al., 2016), and 600 for self-play.", "startOffset": 55, "endOffset": 98}, {"referenceID": 5, "context": "8 shows the target task reward as a function of training iteration for our approach alongside VIME (Houthooft et al., 2016) and SimHash (Tang et al.", "startOffset": 99, "endOffset": 123}, {"referenceID": 20, "context": ", 2016) and SimHash (Tang et al., 2016).", "startOffset": 20, "endOffset": 39}, {"referenceID": 5, "context": "Evaluation on SwimmerGather target task, comparing to VIME (Houthooft et al., 2016) and SimHash (Tang et al.", "startOffset": 59, "endOffset": 83}, {"referenceID": 20, "context": ", 2016) and SimHash (Tang et al., 2016) (figure adapted from (Tang et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 20, "context": ", 2016) (figure adapted from (Tang et al., 2016)).", "startOffset": 29, "endOffset": 48}], "year": 2017, "abstractText": "We describe a simple scheme that allows an agent to explore its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on (nearly) reversible environments, or environments that can be reset, and Alice will \u201cpropose\u201d the task by running a set of actions and then Bob must partially undo, or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When deployed on an RL task within the environment, this unsupervised training reduces the number of episodes needed to learn.", "creator": "LaTeX with hyperref package"}}}