{"id": "1703.05446", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing", "abstract": "Human parsing has recently attracted a lot of research interests due to its huge application potentials. However existing datasets have limited number of images and annotations, and lack the variety of human appearances and the coverage of challenging cases in unconstrained environment. In this paper, we introduce a new benchmark \"Look into Person (LIP)\" that makes a significant advance in terms of scalability, diversity and difficulty, a contribution that we feel is crucial for future developments in human-centric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels, which are captured from a wider range of viewpoints, occlusions and background complexity. Given these rich annotations we perform detailed analyses of the leading human parsing approaches, gaining insights into the success and failures of these methods. Furthermore, in contrast to the existing efforts on improving the feature discriminative capability, we solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into parsing results without resorting to extra supervision (i.e., no need for specifically labeling human joints in model training). Our self-supervised learning framework can be injected into any advanced neural networks to help incorporate rich high-level knowledge regarding human joints from a global perspective and improve the parsing results. Extensive evaluations on our LIP and the public PASCAL-Person-Part dataset demonstrate the superiority of our method.", "histories": [["v1", "Thu, 16 Mar 2017 01:14:36 GMT  (2870kb,D)", "https://arxiv.org/abs/1703.05446v1", "Accepted to appear in CVPR 2017"], ["v2", "Fri, 28 Jul 2017 01:41:39 GMT  (2869kb,D)", "http://arxiv.org/abs/1703.05446v2", "Accepted to appear in CVPR 2017"]], "COMMENTS": "Accepted to appear in CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["ke gong", "xiaodan liang", "dongyu zhang", "xiaohui shen", "liang lin"], "accepted": false, "id": "1703.05446"}, "pdf": {"name": "1703.05446.pdf", "metadata": {"source": "CRF", "title": "Look into Person: Self-supervised Structure-sensitive Learning and A New Benchmark for Human Parsing", "authors": ["Ke Gong", "Xiaodan Liang", "Dongyu Zhang", "Xiaohui Shen", "Liang Lin"], "emails": ["gongk3@mail2.sysu.edu.cn,", "xiaodan1@cs.cmu.edu,", "zhangdy27@mail.sysu.edu.cn,", "linlng@mail.sysu.edu.cn,", "xshen@adobe.com"], "sections": [{"heading": "1. Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "1.1. Related Work", "text": "With 50,462 images in 20 categories, our LIP dataset is the largest and most comprehensive human parsing dataset to date. Some other datasets in the vision community were devoted to the tasks of dress recognition, recall [21, 24], and estimation of human poses [1, 13], while our LIP dataset focused only on human parsing. Human parsing approaches: Recently, much research has focused on human parsing [18, 33, 32, 26, 20, 30, 5]. Thus, Liang et al. [18] proposed a novel co-CNN architecture that integrates multiple levels of image contexts into a unified nerwork. Besides human parsing, there is also an increasing research interest in the sub-segmentation of other objects such as animals or cars [29], with the general attention of the human structure [23] being based on the proposed 4-point scale."}, {"heading": "2. Look into Person Benchmark", "text": "In this section, we introduce our new Look into Person (LIP), a new large-format dataset that focuses on the semantic understanding of human bodies and has several appealing features. Firstly, with 50,462 annotated images, LIP is an order of magnitude larger and more sophisticated than previous similar experiments [33, 6, 18]. Secondly, LIP is provided with sophisticated pixel-by-pixel annotations of 19 human semantic particles and a background label. Thirdly, the images collected from the real scenarios contain people with challenging poses and angles, heavy closures, different appearances, and a wide range of resolutions. Furthermore, the background of the images in the LIP dataset is also more complex and diverse than those in previous datasets. Some examples are shown in Figure 1. With the LIP dataset, we propose a new benchmark suite for human parsing, where the test set is kept secret to avoid mismatch."}, {"heading": "2.1. Image Annotation", "text": "The images in the LIP dataset are cropped individuals from Microsoft COCO [19] training and validation kits. We defined 19 human parts or clothing names for comment, including hat, hair, sunglasses, outerwear, dress, coat, socks, trousers, gloves, scarf, skirt, jumpsuit, face, right arm, left arm, right leg, left leg, right shoe, left shoe, and additionally a background label. We implemented a comment tool and generated [2] based superpixels of images to speed up commenting."}, {"heading": "2.2. Dataset splits", "text": "There are a total of 50,462 images in the LIP dataset, including 19,081 full body images, 13,672 torso images, 403 lower body images, 3,386 head images, 2,778 rear views and 21,028 occlusion images. We split the images into separate training, validation and test kits. After random selection, we arrive at a unique split consisting of 30,462 training and 10,000 validation images with publicly available annotations and 10,000 test images with annotations retained for benchmarking purposes."}, {"heading": "2.3. Dataset statistics", "text": "In this section, we analyze the images and categories in the LIP data set in detail. In general, the face, arms and legs are the most remarkable parts of a human body. However, the analysis of the human body aims to analyze all the detailed regions of a person, including different body parts and different categories of clothing. Therefore, we define 6 body parts and 13 clothing categories. Among these 6 body parts, we divide arms and legs into left and right for more detailed analysis, which also increases the difficulty of the task. As far as clothing classes are concerned, we have not only common garments such as outerwear, trousers and shoes, but also rare categories such as skirts and jumpsuits. In addition, smaller-scale accessories such as sunglasses, gloves and socks are taken into account. The number of images for each semantic part label is in Fig. 3The images in the LIP data set contain various human appearances, angles and moments of the human body. In addition, more than half of the images suffer different manifestations."}, {"heading": "3. Empirical study of state-of-the-arts", "text": "In this section, we analyze the performance of leading human analytical or semantic object segmentation approaches based on our benchmark. We use our rich annotations and perform a detailed analysis of various factors influencing the results, such as appearance, shortening and viewpoints. The objective of this analysis is to evaluate the robustness of current approaches to various challenges to human parsing and to identify existing constraints to stimulate further research advancements. In our analysis, we take fully into account Convolutionary Networks [22] (FCN-8s), a deep Convolutionary Encoder Architecture [3] (SegNet), deep Convolutionary Networks with atomic Folding and Multi-Scale [4] (DeepLabV2) and an Attention Mechanism [5] (Attention), all of which achieved excellent performance in semantic image segments [3] (SegNet) and have fully available codes. To allow a fair comparison, we evaluate each method based on our Validity Validity Validation for our 30-Training Validity Model and the Validity of the CR2."}, {"heading": "3.1. Overall performance evaluation", "text": "We begin our analysis with a report on the overall human parsing performance of each approach and summarize the results in Table 2 and Table 3. In LIP validation, Attention [5] achieves the best result of 54.39% average accuracy among the four approaches, benefiting from the attention model that gently weights the characteristics of multiple scales; in mean IoU, Attention [5] performs best at 42.92%, while both FCN-8 [22] (28.29%) and SegNet [3] (18.17%) perform significantly worse. Similar results are observed in the LIP test. Interestingly, the result of this comparison is that the performance achieved is significantly lower than the current best results on other segmentation benchmarks such as PASCAL VOC [11], suggesting that detailed human parsing is more demanding than the segmentation at the object level that deserves more attention in the future due to the small parts and various fine-grained labels."}, {"heading": "3.2. Performance evaluation under different challenges", "text": "In addition, we analyze the performance of each approach in relation to the following five challenging factors: occlusion, full body, torso, head and rear view (see Figure 5). We evaluate the above four approaches of the LIP validation set, which includes 4,277 occlusion images, 5,452 full body images, 793 torso images, 112 head and 661 rear views. As expected, performance varies across different factors. Rear view is clearly the most difficult case. For example, the IoU of attention [5] falls from 42.92% to 33.50%. The second most common factor is the appearance of the head. Values of all approaches are much lower than the average for head-missed images. Performance also suffers greatly from occlusion. And the results of whole body images are closest to the average level. In contrast, the torso is relatively the simplest case when there are fewer setic parts present and part of the face is larger than the human face or face."}, {"heading": "3.3. Per-class performance evaluation", "text": "To discuss and analyze each of the 20 labels in the LIP dataset in more detail, we report further on the performance of the IoU per class on the LIP validation set shown in Table 4. We observe that the results are much better for labels with larger regions such as face, outerwear, coats and pants than for labels with smaller regions such as sunglasses, scarf and skirt. Attention [5] and DeepLabV2 [4] perform better for small labels thanks to the use of multi-scale features."}, {"heading": "3.4. Visualization comparison", "text": "The qualitative comparisons of four approaches of our LIP validation group are illustrated in Fig. 8. We exemplify the analysis results of the five challenging factor scenarios. In the torso image (a) with slight occlusion, four approaches with fewer errors perform well. In the background image (b), all four methods incorrectly mark the right arm as the left arm. The worst results occur when it comes to the head-missing image (c). SegNet [3] and FCN-8s [22] do not recognize arms and legs, while DeepLabV2 [4] and Attention [5] have errors on the right and left sides of arms, legs and shoes. Furthermore, severe occlusion (d) also severely impairs performance. Whole body is less challenging, but the small objects in a whole body image (s) such as shoes are also difficult to predict, as some of the results are observed from the perspective of human body configuration, e.g. foot structures in which the human body structure is considered more strongly than the human body structure (b)."}, {"heading": "4. Self-supervised Structure-sensitive Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Overview", "text": "As already mentioned, a major limitation of existing approaches to human analysis is the lack of consideration of the human body configuration, which is mainly studied in the human estimation problem. Human analysis and pose estimation aims to mark each image with different granularities, that is, pixel-by-pixel semantic labeling versus hinged structure prediction. Pixel-by-pixel labeling can address more detailed information, while the hinged structure provides a higher structure. However, the results of modern estimation models still show many errors [34, 7]. The predicted joints do not have a high quality to guide human analysis, compared to the joints extracted from the analyzing notes. Moreover, the joints in the estimation are not consistent with the analyzing notes. For example, the arms are labeled as arms for analyzing notes only when they are not covered by clothing, while the notes in the pose are independent."}, {"heading": "4.2. Self-supervised Structure-sensitive Loss", "text": "It means that we need to find structural monitoring from the parsing notes, which cover the centers of the head, upper body, left arm, right leg, left leg, and right leg. The region of the head is characterized by the fusion of parsing labels, hair, and facial features."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Experimental Settings", "text": "Dataset: We evaluate the performance of our self-monitored, structure-sensitive learning method for the human parsing task on two challenging datasets. One is the public PASCAL partial dataset with 1,716 images for training and 1,817 images for the test, which draws attention to the human sub-segmentation commented on by [6]. Subsequently [5, 30], the annotations on six person sub-classes merge with a background class introduced and analyzed in Section 3.Network Architecture: We use the publicly available model, Attention [5], as the basic architecture for its leading accuracy and competitiveness. We train a network based on DeepLabV2 [4], which is based on reproduced VGG-16."}, {"heading": "5.2. Results and Comparisons", "text": "We compare the proposed method with the strong baselines on the two public dataset.PASCAL-Person-Part Dataset [6]. Table. 5 shows the performance of our models and comparisons with four state-of-the-art methods on the standard intersection via Union (IoU) criterion. Our method can significantly exceed four baselines, in particular. For example, our best model achieved 59.36%, 7.58% better than DeepLabLargeFOV [4] and 2.97% better than Attention [5]. This big improvement shows that our self-monitored strategy is significantly helpful for human analysis of the task. LIP Dataset: We report the results and the comparisons with four state-of-the-art methods set on LIP validation and test set in Table. 3. Set on validation, the proposed architecture can give a huge boost in the average IoU."}, {"heading": "5.3. Qualitative Comparison", "text": "The qualitative comparisons of the parsing results on the LIP validation set are shown in Fig. 8. As can be seen from these visualized comparisons, despite large variations in appearance and position, our self-learning structure delivers semantically more meaningful and accurate predictions than other four methods. For example, the small regions (e.g. left or right shoe) can be successfully segmented with our method for structure-sensitive losses, if you take (b) and (c), our approach can also successfully handle the confusing labels such as left arm versus right arm and left leg versus right leg. These regions with similar appearance can be identified and separated from each other by guiding joint structure information. For the most difficult head-forgotten images (c), our approach excellently corrects the left shoe, right shoe and part of the left leg. Generally, our approach delivers more sensible results by effectively exploiting self-monitored structure-sensitive losses to switch the labeling of the human task."}, {"heading": "5.4. Further experiments and analyses", "text": "For a better understanding of our LIP dataset, we evaluate the LIP-trained models and test them on ATR [18] according to common categories, as reported in Table 6 (links). Generally, the performance on ATR is better than on LIP, as the LIP dataset includes cases with more different poses, appearance patterns, occlusions and resolution problems, which is more consistent with real-life situations. Following the MSCOCO dataset [19], we performed an em-piric analysis for different object sizes, i.e. small (area < 1532), medium (area < 3212) and large (area \u2265 3212). Results from four baselines and the proposed SSL are shown in Table 6 (mean). It can be noted that our SSL has considerably superior performance for different object sizes."}, {"heading": "6. Conclusions", "text": "In this paper, we presented \"Look into Person (LIP),\" an extensive human parsing dataset and a carefully designed scale to trigger advances in human parsing. LIP contains 50,462 images richly labeled with 19 semantic sub-markers. Based on our rich annotations, we conducted detailed experimental analysis to identify the success and limitations of leading human parsing approaches. In addition, we are designing a new learning strategy, namely self-monitored structure-sensitive learning, to semantically align the parsing results produced with human joint structures."}], "references": [{"title": "2d human pose estimation: New benchmark and state of the art analysis", "author": ["M. Andriluka", "L. Pishchulin", "P. Gehler", "B. Schiele"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Contour detection and hierarchical image segmentation", "author": ["P. Arbelaez", "M. Maire", "C. Fowlkes", "J. Malik"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.00561", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "ICLR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Attention to scale: Scale-aware semantic image segmentation", "author": ["L.-C. Chen", "Y. Yang", "J. Wang", "W. Xu", "A.L. Yuille"], "venue": "CVPR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["X. Chen", "R. Mottaghi", "X. Liu", "S. Fidler", "R. Urtasun"], "venue": "Detect what you can: Detecting and representing objects using holistic models and body parts. In CVPR", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Articulated pose estimation by a graphical model with image dependent pairwise relations", "author": ["X. Chen", "A. Yuille"], "venue": "NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards unified human parsing and pose estimation", "author": ["J. Dong", "Q. Chen", "X. Shen", "J. Yang", "S. Yan"], "venue": "CVPR", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "A deformable mixture parsing model with parselets", "author": ["J. Dong", "Q. Chen", "W. Xia", "Z. Huang", "S. Yan"], "venue": "ICCV", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Predicting depth", "author": ["D. Eigen", "R. Fergus"], "venue": "surface normals and semantic labels with a common multi-scale convolutional architecture. In ICCV", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "and A", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn"], "venue": "Zisserman. The pascal visual object classes challenge 2010 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "G", "author": ["C. Gan", "M. Lin", "Y. Yang"], "venue": "de Melo, and A. G. Hauptmann. Concepts not alone: Exploring pairwise relationships for zero-shot video activity recognition. In AAAI", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments", "author": ["C. Ionescu", "D. Papava", "V. Olaru", "C. Sminchisescu"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Deep human parsing with active template regression", "author": ["X. Liang", "S. Liu", "X. Shen", "J. Yang", "L. Liu", "J. Dong", "L. Lin", "S. Yan"], "venue": "TPAMI", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards computational baby learning: A weakly-supervised approach for object detection", "author": ["X. Liang", "S. Liu", "Y. Wei", "L. Liu", "L. Lin", "S. Yan"], "venue": "ICCV", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic object parsing with local-global long short-term memory", "author": ["X. Liang", "X. Shen", "D. Xiang", "J. Feng", "L. Lin", "S. Yan"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Proposal-free network for instance-level object segmentation", "author": ["X. Liang", "Y. Wei", "X. Shen", "J. Yang", "L. Lin", "S. Yan"], "venue": "arXiv preprint arXiv:1509.02636", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Human parsing with contextualized convolutional neural network", "author": ["X. Liang", "C. Xu", "X. Shen", "J. Yang", "S. Liu", "J. Tang", "L. Lin", "S. Yan"], "venue": "ICCV", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft COCO: common objects in context", "author": ["T. Lin", "M. Maire", "S.J. Belongie", "L.D. Bourdev", "R.B. Girshick", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "CoRR, abs/1405.0312", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Matching-CNN Meets KNN: Quasi- Parametric Human Parsing", "author": ["S. Liu", "X. Liang", "L. Liu", "X. Shen", "J. Yang", "C. Xu", "L. Lin", "X. Cao", "S. Yan"], "venue": "CVPR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Deepfashion: Powering robust clothes recognition and retrieval with rich annotations", "author": ["Z. Liu", "P. Luo", "S. Qiu", "X. Wang", "X. Tang"], "venue": "CVPR", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Parsing semantic parts of cars using graphical models and segment appearance consistency", "author": ["W. Lu", "X. Lian", "A. Yuille"], "venue": "BMVC", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Xufeng Han", "author": ["S.L.A.C.B.T.L.B.M. Hadi Kiapour"], "venue": "Where to buy it:matching street clothing photos in online shops. In ICCV", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Flowing convnets for human pose estimation in videos", "author": ["T. Pfister", "J. Charles", "A. Zisserman"], "venue": "ICCV", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "A High Performance CRF Model for Clothes Parsing", "author": ["E. Simo-Serra", "S. Fidler", "F. Moreno-Noguer", "R. Urtasun"], "venue": "ACCV", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic part segmentation using compositional model combining shape and appearance", "author": ["J. Wang", "A. Yuille"], "venue": "CVPR", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Who blocks who: Simultaneous clothing segmentation for grouping images", "author": ["N. Wang", "H. Ai"], "venue": "ICCV", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Joint object and part segmentation using deep learned potentials", "author": ["P. Wang", "X. Shen", "Z. Lin", "S. Cohen", "B. Price", "A. Yuille"], "venue": "ICCV", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Zoom better to see clearer: Huamn part segmentation with auto zoom net", "author": ["F. Xia", "P. Wang", "L.-C. Chen", "A.L. Yuille"], "venue": "ECCV", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Pose-guided human parsing by an and/or graph using pose-context features", "author": ["F. Xia", "J. Zhu", "P. Wang", "A. Yuille"], "venue": "AAAI", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Paper doll parsing: Retrieving similar styles to parse clothing items", "author": ["K. Yamaguchi", "M. Kiapour", "T. Berg"], "venue": "ICCV", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Parsing clothing in fashion photographs", "author": ["K. Yamaguchi", "M. Kiapour", "L. Ortiz", "T. Berg"], "venue": "CVPR", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation", "author": ["W. Yang", "W. Ouyang", "H. Li", "X. Wang"], "venue": "CVPR", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Dynamic topic modeling for monitoring market competition from online text and image data", "author": ["H. Zhang", "G. Kim", "E.P. Xing"], "venue": "ACM SIGKDD. ACM", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised salience learning for person re-identification", "author": ["R. Zhao", "W. Ouyang", "X. Wang"], "venue": "CVPR", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P. Torr"], "venue": "ICCV", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 34, "context": "cn/lip many higher-level computer vision applications [35], such as person re-identification [36] and human behavior analysis [12, 17].", "startOffset": 54, "endOffset": 58}, {"referenceID": 35, "context": "cn/lip many higher-level computer vision applications [35], such as person re-identification [36] and human behavior analysis [12, 17].", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "cn/lip many higher-level computer vision applications [35], such as person re-identification [36] and human behavior analysis [12, 17].", "startOffset": 126, "endOffset": 134}, {"referenceID": 16, "context": "cn/lip many higher-level computer vision applications [35], such as person re-identification [36] and human behavior analysis [12, 17].", "startOffset": 126, "endOffset": 134}, {"referenceID": 13, "context": "Recently, Convolutional Neural Networks (CNNs) have achieved exciting success in human parsing [14, 18, 16].", "startOffset": 95, "endOffset": 107}, {"referenceID": 17, "context": "Recently, Convolutional Neural Networks (CNNs) have achieved exciting success in human parsing [14, 18, 16].", "startOffset": 95, "endOffset": 107}, {"referenceID": 15, "context": "Recently, Convolutional Neural Networks (CNNs) have achieved exciting success in human parsing [14, 18, 16].", "startOffset": 95, "endOffset": 107}, {"referenceID": 14, "context": "Nevertheless, as demonstrated in many other problems such as object detection [15] and semantic segmentation [37], the performance of those CNN-based approaches heavily rely on the availability of annotated images for training.", "startOffset": 78, "endOffset": 82}, {"referenceID": 36, "context": "Nevertheless, as demonstrated in many other problems such as object detection [15] and semantic segmentation [37], the performance of those CNN-based approaches heavily rely on the availability of annotated images for training.", "startOffset": 109, "endOffset": 113}, {"referenceID": 31, "context": "Although there exist training sets for special scenarios such as fashion pictures [32, 9, 14, 18] and people in constrained situations (e.", "startOffset": 82, "endOffset": 97}, {"referenceID": 8, "context": "Although there exist training sets for special scenarios such as fashion pictures [32, 9, 14, 18] and people in constrained situations (e.", "startOffset": 82, "endOffset": 97}, {"referenceID": 13, "context": "Although there exist training sets for special scenarios such as fashion pictures [32, 9, 14, 18] and people in constrained situations (e.", "startOffset": 82, "endOffset": 97}, {"referenceID": 17, "context": "Although there exist training sets for special scenarios such as fashion pictures [32, 9, 14, 18] and people in constrained situations (e.", "startOffset": 82, "endOffset": 97}, {"referenceID": 5, "context": ", upright) [6], these datasets are limited in their coverage and scalability, as shown in Fig.", "startOffset": 11, "endOffset": 14}, {"referenceID": 17, "context": "The largest public human parsing dataset [18] so far only contains 17,000 fashion images while others only include thousands of images.", "startOffset": 41, "endOffset": 45}, {"referenceID": 4, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 29, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 32, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 31, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 8, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 25, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 19, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 17, "context": "The recent progress on human parsing [5, 30, 33, 32, 9, 26, 20, 18] has been achieved by improving the feature representations using convolutional neural networks and recurrent neural networks.", "startOffset": 37, "endOffset": 67}, {"referenceID": 36, "context": ", Conditional Random Fields (CRFs)), similar to the general object segmentation approaches [37, 4, 29].", "startOffset": 91, "endOffset": 102}, {"referenceID": 3, "context": ", Conditional Random Fields (CRFs)), similar to the general object segmentation approaches [37, 4, 29].", "startOffset": 91, "endOffset": 102}, {"referenceID": 28, "context": ", Conditional Random Fields (CRFs)), similar to the general object segmentation approaches [37, 4, 29].", "startOffset": 91, "endOffset": 102}, {"referenceID": 2, "context": "on the new LIP dataset, the results of some existing methods [3, 22, 4, 5] are unsatisfactory.", "startOffset": 61, "endOffset": 74}, {"referenceID": 21, "context": "on the new LIP dataset, the results of some existing methods [3, 22, 4, 5] are unsatisfactory.", "startOffset": 61, "endOffset": 74}, {"referenceID": 3, "context": "on the new LIP dataset, the results of some existing methods [3, 22, 4, 5] are unsatisfactory.", "startOffset": 61, "endOffset": 74}, {"referenceID": 4, "context": "on the new LIP dataset, the results of some existing methods [3, 22, 4, 5] are unsatisfactory.", "startOffset": 61, "endOffset": 74}, {"referenceID": 33, "context": "The human body structural information has been previously well-explored in the human pose estimation [34, 7] where dense joint annotations are provided.", "startOffset": 101, "endOffset": 108}, {"referenceID": 6, "context": "The human body structural information has been previously well-explored in the human pose estimation [34, 7] where dense joint annotations are provided.", "startOffset": 101, "endOffset": 108}, {"referenceID": 32, "context": "Therefore in this work we generate approximated human joints directly from the parsing annotations and use them as the supervision signal for the structure-sensitive loss, which is hence called a \u201cselfsupervised\u201d strategy, noted as Self-supervised StructureDataset #Training #Validation #Test Categories Fashionista [33] 456 229 56 PASCAL-Person-Part [6] 1,716 1,817 7 ATR [18] 16,000 700 1,000 18 LIP 30,462 10,000 10,000 20", "startOffset": 316, "endOffset": 320}, {"referenceID": 5, "context": "Therefore in this work we generate approximated human joints directly from the parsing annotations and use them as the supervision signal for the structure-sensitive loss, which is hence called a \u201cselfsupervised\u201d strategy, noted as Self-supervised StructureDataset #Training #Validation #Test Categories Fashionista [33] 456 229 56 PASCAL-Person-Part [6] 1,716 1,817 7 ATR [18] 16,000 700 1,000 18 LIP 30,462 10,000 10,000 20", "startOffset": 351, "endOffset": 354}, {"referenceID": 17, "context": "Therefore in this work we generate approximated human joints directly from the parsing annotations and use them as the supervision signal for the structure-sensitive loss, which is hence called a \u201cselfsupervised\u201d strategy, noted as Self-supervised StructureDataset #Training #Validation #Test Categories Fashionista [33] 456 229 56 PASCAL-Person-Part [6] 1,716 1,817 7 ATR [18] 16,000 700 1,000 18 LIP 30,462 10,000 10,000 20", "startOffset": 373, "endOffset": 377}, {"referenceID": 5, "context": "Our proposed framework significantly surpasses the previous methods on both the existing PASCAL-Person-Part dataset [6] and our new LIP dataset.", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "(b): The parsing results by Attention-toscale [5] where the left-arm is wrongly labeled as right-arm.", "startOffset": 46, "endOffset": 49}, {"referenceID": 20, "context": "Some other datasets in the vision community were dedicated to the tasks of clothes recognition, retrieval [21, 24] and human pose estimation [1, 13], while our LIP dataset only focuses on human parsing.", "startOffset": 106, "endOffset": 114}, {"referenceID": 23, "context": "Some other datasets in the vision community were dedicated to the tasks of clothes recognition, retrieval [21, 24] and human pose estimation [1, 13], while our LIP dataset only focuses on human parsing.", "startOffset": 106, "endOffset": 114}, {"referenceID": 0, "context": "Some other datasets in the vision community were dedicated to the tasks of clothes recognition, retrieval [21, 24] and human pose estimation [1, 13], while our LIP dataset only focuses on human parsing.", "startOffset": 141, "endOffset": 148}, {"referenceID": 12, "context": "Some other datasets in the vision community were dedicated to the tasks of clothes recognition, retrieval [21, 24] and human pose estimation [1, 13], while our LIP dataset only focuses on human parsing.", "startOffset": 141, "endOffset": 148}, {"referenceID": 17, "context": "Human parsing approaches: Recently many research efforts have been devoted to human parsing [18, 33, 32, 26, 20, 30, 5].", "startOffset": 92, "endOffset": 119}, {"referenceID": 32, "context": "Human parsing approaches: Recently many research efforts have been devoted to human parsing [18, 33, 32, 26, 20, 30, 5].", "startOffset": 92, "endOffset": 119}, {"referenceID": 31, "context": "Human parsing approaches: Recently many research efforts have been devoted to human parsing [18, 33, 32, 26, 20, 30, 5].", "startOffset": 92, "endOffset": 119}, {"referenceID": 25, "context": "Human parsing approaches: Recently many research efforts have been devoted to human parsing [18, 33, 32, 26, 20, 30, 5].", "startOffset": 92, "endOffset": 119}, {"referenceID": 19, "context": "Human parsing approaches: Recently many research efforts have been devoted to human parsing [18, 33, 32, 26, 20, 30, 5].", "startOffset": 92, "endOffset": 119}, {"referenceID": 29, "context": "Human parsing approaches: Recently many research efforts have been devoted to human parsing [18, 33, 32, 26, 20, 30, 5].", "startOffset": 92, "endOffset": 119}, {"referenceID": 4, "context": "Human parsing approaches: Recently many research efforts have been devoted to human parsing [18, 33, 32, 26, 20, 30, 5].", "startOffset": 92, "endOffset": 119}, {"referenceID": 17, "context": "[18] proposed a novel Co-CNN architecture which integrates multiple levels of image contexts into a unified nerwork.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Besides human parsing, there has also been increasing research interest on the part segmentation of other objects such as animals or cars [27, 29, 23].", "startOffset": 138, "endOffset": 150}, {"referenceID": 28, "context": "Besides human parsing, there has also been increasing research interest on the part segmentation of other objects such as animals or cars [27, 29, 23].", "startOffset": 138, "endOffset": 150}, {"referenceID": 22, "context": "Besides human parsing, there has also been increasing research interest on the part segmentation of other objects such as animals or cars [27, 29, 23].", "startOffset": 138, "endOffset": 150}, {"referenceID": 3, "context": "To capture the rich structure information based on the advanced CNN architecture, common solutions inlcude the combination of CNNs and CRFs [4, 37] and the adoptions of multi-scale feature representations [4, 5, 30].", "startOffset": 140, "endOffset": 147}, {"referenceID": 36, "context": "To capture the rich structure information based on the advanced CNN architecture, common solutions inlcude the combination of CNNs and CRFs [4, 37] and the adoptions of multi-scale feature representations [4, 5, 30].", "startOffset": 140, "endOffset": 147}, {"referenceID": 3, "context": "To capture the rich structure information based on the advanced CNN architecture, common solutions inlcude the combination of CNNs and CRFs [4, 37] and the adoptions of multi-scale feature representations [4, 5, 30].", "startOffset": 205, "endOffset": 215}, {"referenceID": 4, "context": "To capture the rich structure information based on the advanced CNN architecture, common solutions inlcude the combination of CNNs and CRFs [4, 37] and the adoptions of multi-scale feature representations [4, 5, 30].", "startOffset": 205, "endOffset": 215}, {"referenceID": 29, "context": "To capture the rich structure information based on the advanced CNN architecture, common solutions inlcude the combination of CNNs and CRFs [4, 37] and the adoptions of multi-scale feature representations [4, 5, 30].", "startOffset": 205, "endOffset": 215}, {"referenceID": 4, "context": "[5] proposed an attention mechanism that learns to weight the multi-scale features at each pixel location.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Some previous works [8, 31] explored human pose information to guide human parsing by generating \u201cpose-guided\u201d part segment proposals.", "startOffset": 20, "endOffset": 27}, {"referenceID": 30, "context": "Some previous works [8, 31] explored human pose information to guide human parsing by generating \u201cpose-guided\u201d part segment proposals.", "startOffset": 20, "endOffset": 27}, {"referenceID": 32, "context": "First, with 50,462 annotated images, LIP is an order of magnitude larger and more challenging than previous similar attempts[33, 6, 18].", "startOffset": 124, "endOffset": 135}, {"referenceID": 5, "context": "First, with 50,462 annotated images, LIP is an order of magnitude larger and more challenging than previous similar attempts[33, 6, 18].", "startOffset": 124, "endOffset": 135}, {"referenceID": 17, "context": "First, with 50,462 annotated images, LIP is an order of magnitude larger and more challenging than previous similar attempts[33, 6, 18].", "startOffset": 124, "endOffset": 135}, {"referenceID": 18, "context": "The images in the LIP dataset are cropped person instances from Microsoft COCO [19] training and validation sets.", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "We implemented an annotation tool and generated multi-scale superpixels of images based on [2] to speed up the annotation.", "startOffset": 91, "endOffset": 94}, {"referenceID": 21, "context": "In our analysis, we consider fully convolutional networks [22] (FCN-8s), a deep convolutional encoderdecoder architecture [3] (SegNet), deep convolutional nets with atrous convolution and multi-scale [4] (DeepLabV2) and an attention mechanism [5] (Attention), which all achieved excellent performance on semantic image segMethod Overall accuracy Mean accuracy Mean IoU SegNet [3] 69.", "startOffset": 58, "endOffset": 62}, {"referenceID": 2, "context": "In our analysis, we consider fully convolutional networks [22] (FCN-8s), a deep convolutional encoderdecoder architecture [3] (SegNet), deep convolutional nets with atrous convolution and multi-scale [4] (DeepLabV2) and an attention mechanism [5] (Attention), which all achieved excellent performance on semantic image segMethod Overall accuracy Mean accuracy Mean IoU SegNet [3] 69.", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "In our analysis, we consider fully convolutional networks [22] (FCN-8s), a deep convolutional encoderdecoder architecture [3] (SegNet), deep convolutional nets with atrous convolution and multi-scale [4] (DeepLabV2) and an attention mechanism [5] (Attention), which all achieved excellent performance on semantic image segMethod Overall accuracy Mean accuracy Mean IoU SegNet [3] 69.", "startOffset": 200, "endOffset": 203}, {"referenceID": 4, "context": "In our analysis, we consider fully convolutional networks [22] (FCN-8s), a deep convolutional encoderdecoder architecture [3] (SegNet), deep convolutional nets with atrous convolution and multi-scale [4] (DeepLabV2) and an attention mechanism [5] (Attention), which all achieved excellent performance on semantic image segMethod Overall accuracy Mean accuracy Mean IoU SegNet [3] 69.", "startOffset": 243, "endOffset": 246}, {"referenceID": 2, "context": "In our analysis, we consider fully convolutional networks [22] (FCN-8s), a deep convolutional encoderdecoder architecture [3] (SegNet), deep convolutional nets with atrous convolution and multi-scale [4] (DeepLabV2) and an attention mechanism [5] (Attention), which all achieved excellent performance on semantic image segMethod Overall accuracy Mean accuracy Mean IoU SegNet [3] 69.", "startOffset": 376, "endOffset": 379}, {"referenceID": 21, "context": "17 FCN-8s [22] 76.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "29 DeepLabV2 [4] 82.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "64 Attention [5] 83.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "Method Overall accuracy Mean accuracy Mean IoU SegNet [3] 69.", "startOffset": 54, "endOffset": 57}, {"referenceID": 21, "context": "37 FCN-8s [22] 76.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "69 DeepLabV2 [4] 82.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "56 Attention [5] 83.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "Following [5, 30], we use the standard intersection over union (IoU) criterion and pixelwise accuracy for evaluation.", "startOffset": 10, "endOffset": 17}, {"referenceID": 29, "context": "Following [5, 30], we use the standard intersection over union (IoU) criterion and pixelwise accuracy for evaluation.", "startOffset": 10, "endOffset": 17}, {"referenceID": 4, "context": "On the LIP validation set, among the four approaches, Attention [5] achieves the best result of 54.", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": "For mean IoU, Attention [5] performs best with 42.", "startOffset": 24, "endOffset": 27}, {"referenceID": 21, "context": "92%, while both FCN-8s [22] (28.", "startOffset": 23, "endOffset": 27}, {"referenceID": 2, "context": "29%) and SegNet [3] (18.", "startOffset": 16, "endOffset": 19}, {"referenceID": 10, "context": "The interesting outcome of this comparison is that the achieved performance is substantially lower than the current best results on other segmentation benchmark such as PASCAL VOC [11].", "startOffset": 180, "endOffset": 184}, {"referenceID": 4, "context": "For example, the IoU of Attention [5] drops from 42.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "Method hat hair gloves sunglasses u-clothes dress coat socks pants jumpsuits scarf skirt face l-arm r-arm l-leg r-leg l-shoe r-shoe Bkg Avg SegNet [3] 26.", "startOffset": 147, "endOffset": 150}, {"referenceID": 21, "context": "17 FCN-8s [22] 39.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "29 DeepLabV2 [4] 57.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "64 Attention [5] 58.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "Attention [5] and DeepLabV2 [4] perform better on small labels thanks to the utilization of multi-scale features.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Attention [5] and DeepLabV2 [4] perform better on small labels thanks to the utilization of multi-scale features.", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "SegNet [3] and FCN-8s [22] fail to recognize arms and legs, while DeepLabV2 [4] and Attention [5] have errors on the right and left of arms, legs and shoes.", "startOffset": 7, "endOffset": 10}, {"referenceID": 21, "context": "SegNet [3] and FCN-8s [22] fail to recognize arms and legs, while DeepLabV2 [4] and Attention [5] have errors on the right and left of arms, legs and shoes.", "startOffset": 22, "endOffset": 26}, {"referenceID": 3, "context": "SegNet [3] and FCN-8s [22] fail to recognize arms and legs, while DeepLabV2 [4] and Attention [5] have errors on the right and left of arms, legs and shoes.", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "SegNet [3] and FCN-8s [22] fail to recognize arms and legs, while DeepLabV2 [4] and Attention [5] have errors on the right and left of arms, legs and shoes.", "startOffset": 94, "endOffset": 97}, {"referenceID": 33, "context": "However, the results of state-of-the-art pose estimation models [34, 7] still have many errors.", "startOffset": 64, "endOffset": 71}, {"referenceID": 6, "context": "However, the results of state-of-the-art pose estimation models [34, 7] still have many errors.", "startOffset": 64, "endOffset": 71}, {"referenceID": 24, "context": "Following [25], for each parsing result and corresponding ground truth, we compute the center points of regions to obtain joints represented as heatmaps for training more smoothly.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "One is the public PASCAL-Person-part dataset with 1,716 images for training and 1,817 for testing, which pays attention to the human part segmentation annotated by [6].", "startOffset": 164, "endOffset": 167}, {"referenceID": 4, "context": "Following [5, 30], the annotations are merge to be six person part classes and one background class which are Head, Torse, Upper / Lower arms and Upper / Lower legs.", "startOffset": 10, "endOffset": 17}, {"referenceID": 29, "context": "Following [5, 30], the annotations are merge to be six person part classes and one background class which are Head, Torse, Upper / Lower arms and Upper / Lower legs.", "startOffset": 10, "endOffset": 17}, {"referenceID": 3, "context": "Method head torso u-arms l-arms u-legs l-legs Bkg Avg DeepLab-LargeFOV [4] 78.", "startOffset": 71, "endOffset": 74}, {"referenceID": 29, "context": "78 HAZN [30] 80.", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "11 Attention [5] 81.", "startOffset": 13, "endOffset": 16}, {"referenceID": 15, "context": "39 LG-LSTM [16] 82.", "startOffset": 11, "endOffset": 15}, {"referenceID": 5, "context": "Table 5: Comparison of person part segmentation performance with four state-of-the-art methods on the PASCALPerson-Part dataset [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "Network architecture: We utilize the publicly available model, Attention [5], as the basic architecture due to its leading accuracy and competitive efficiency.", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "We also train a network based on DeepLabV2 [4], which employs re-purposed VGG-16 by atrous convolution, multi-scale inputs and atrous spatial pyramid pooling.", "startOffset": 43, "endOffset": 46}, {"referenceID": 3, "context": "Training: We use the pre-trained models and networks settings provided by DeepLabV2 [4].", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "The scale of the input images is fiexed as 321 \u00d7 321 for training networks based on Attention [5].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "Method ATR LIP small medium large 153 321 513 SegNet [3] 15.", "startOffset": 53, "endOffset": 56}, {"referenceID": 21, "context": "44 FCN-8s [22] 34.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "25 DeepLabV2 [4] 48.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "28 Attention [5] 49.", "startOffset": 13, "endOffset": 16}, {"referenceID": 5, "context": "PASCAL-Person-Part dataset [6].", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "58% better than DeepLabLargeFOV [4] and 2.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "97% better than Attention [5].", "startOffset": 26, "endOffset": 29}, {"referenceID": 3, "context": "09% better than DeepLabV2 [4] and 1.", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "81% better than Attention [5].", "startOffset": 26, "endOffset": 29}, {"referenceID": 17, "context": "For a better understanding of our LIP dataset, we evaluate the models trained on LIP and test on ATR [18] on the common categories, as reported in Table 6 (Left).", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "Following MSCOCO dataset [19], we have done an empirical analysis on different object sizes, i.", "startOffset": 25, "endOffset": 29}, {"referenceID": 4, "context": "The detailed analyses over different input sizes for all methods (except Attention [5] for its attention mechanism over scales) are presented in Table 6 (Right), which shows that our structure-sensitive learning is more robust for input size.", "startOffset": 83, "endOffset": 86}], "year": 2017, "abstractText": "Human parsing has recently attracted a lot of research interests due to its huge application potentials. However existing datasets have limited number of images and annotations, and lack the variety of human appearances and the coverage of challenging cases in unconstrained environment. In this paper, we introduce a new benchmark1 \u201cLook into Person (LIP)\u201d that makes a significant advance in terms of scalability, diversity and difficulty, a contribution that we feel is crucial for future developments in humancentric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels, which are captured from a wider range of viewpoints, occlusions and background complexity. Given these rich annotations we perform detailed analyses of the leading human parsing approaches, gaining insights into the success and failures of these methods. Furthermore, in contrast to the existing efforts on improving the feature discriminative capability, we solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into parsing results without resorting to extra supervision (i.e., no need for specifically labeling human joints in model training). Our self-supervised learning framework can be injected into any advanced neural networks to help incorporate rich high-level knowledge regarding human joints from a global perspective and improve the parsing results. Extensive evaluations on our LIP and the public PASCAL-PersonPart dataset demonstrate the superiority of our method.", "creator": "LaTeX with hyperref package"}}}