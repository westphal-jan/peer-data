{"id": "1703.05465", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Neobility at SemEval-2017 Task 1: An Attention-based Sentence Similarity Model", "abstract": "This paper describes a neural-network model which performed competitively (top 6) at the SemEval 2017 cross-lingual Semantic Textual Similarity (STS) task. Our system employs an attention-based recurrent neural network model that optimizes the sentence similarity. In this paper, we describe our participation in the multilingual STS task which measures similarity across English, Spanish, and Arabic.", "histories": [["v1", "Thu, 16 Mar 2017 03:15:22 GMT  (93kb,D)", "http://arxiv.org/abs/1703.05465v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenli zhuang", "ernie chang"], "accepted": false, "id": "1703.05465"}, "pdf": {"name": "1703.05465.pdf", "metadata": {"source": "CRF", "title": "Neobility at SemEval-2017 Task 1: An Attention-based Sentence Similarity Model", "authors": ["WenLi Zhuang", "Ernie Chang"], "emails": ["bibo9901@gmail.com", "cyc025@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "Semantic text similarity (STS) measures the degree of equivalence between the meanings of two text sequences (Agirre et al., 2016).The similarity of the text pair can be presented as discrete or continuous values ranging from irrelevance (1) to exact semantic equivalence (5).It is widely applicable to many NLP tasks, including summary (Wong et al., 2008; Nenkova et al., 2011), question generation and answer (Vo et al., 2015), paraphrase recognition (Fernando and Stevenson, 2008), and machine translation (Corley et Mihalcea, 2005).In this paper we describe a system capable of learning context-sensitive characteristics within sentences. Furthermore, we encode sequential information with recurring neural networks (RNN) and perform attention mechanisms (Bahau et al, 2015 based on NN)."}, {"heading": "2 Related Works", "text": "Most proposed approaches have used a hybrid of different text unit sizes in the past, ranging from character-based, symbolic and semantic levels to knowledge-based similarity measurements (Gomaa and Fahmy, 2013). Linguistic depths of these measures often vary between lexical, syntactic and semantic levels. Most solutions involve an interaction of modules using features from different unit sizes and depths. Newer approaches generally include the word embedding similarity (Liebeck et al., 2016; Brychc\u0131 \"n and Svoboda, 2016) as part of the final ensemble. The most powerful team in 2016 (Rychalska et al., 2016) uses a combination of multiple modules, including recursive autoencoder with WordNet and a monolingual aligner (Sultan et al., 2016). UMD-TTICUW (He et al, 2016, does not require the CNN MP6) to provide attention to the network."}, {"heading": "3 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Model", "text": "In the face of two sentences I1 = {w11, w12, w1n1} and I2 = [w21, w22,..., w2n2}, where we both recognize the Jth character of the sentence, which is comparable to a function that has transformed each token into a D-dimension trainable vector. (We have two sentences, both of which are encrypted with a D-dimension vector.) (We have two sentences, both of which are encrypted with a D-dimension vector. (We have encrypted two sentences with one attribute.) (We have encrypted two sentences with one attribute.) (We have encrypted two sentences with one sentence.), both of which are encrypted with one sentence. (We have encrypted two sentences with one sentence.) (We have encrypted two sentences with one sentence.)"}, {"heading": "3.2 Word Embedding", "text": "We examined the initialization of Word embeddings randomly or with pre-trained word2vec (Mikolov et al., 2013) of dimensions 50, 100 or 300. We found that the system works best with 300-dimensional word2vec embeddings."}, {"heading": "3.3 Optimization", "text": "Let pn, yn is the predicted probability density and the expected value, and y-n is the commented gold value of the n-th sample. Most of the previous learning-based models are designed to minimize the following targets on a series of N samples: \u2022 Negative log probability (NLL) of p and p (Aker et al., 2016). The task is considered a classification problem for 6 class levels. \u2212 LNLL = N-log pntn, where tn y is rounded to the nearest integer. \u2022 Mean square error (MSE) between yn and y-n (Brychc\u0131 n and Svoboda, 2016).LMSE = 1N N N N-N (yn \u2212 n) 2 \u2022 Kullback Leibler divergence (KLD) of pn and gold distribution p-n."}, {"heading": "4 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data", "text": "We collected data sets from SICK (Marelli et al., 2014) and past STS in 2012, 2013, 2014, 2015 and 2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016) for both cross-language and monolingual subtasks. We mix and divide them into training sets and validation sets according to the 80: 20 ratio. Table 2 indicates the size of the training set and validation set. All non-English sentences that occur in training, validation and testing sets are translated into English using Google Cloud Translation API."}, {"heading": "4.2 Experiments", "text": "We use the ADAM algorithm to optimize the parameters with mini-batches of 125. The learning rate is set to 10 \u2212 4 at the beginning and reduced by half for all 5 epochs. We trained the network for 15 epochs. Word Embedding In Table 3 we show that the system performs better with pre-trained word vectors (WI) than with randomly initialized (RI). Loss Function We show performance with systems optimized with KLD, MSE and PCC. It shows that our system not only performs best when using LPCC as a training target, but also converges fastest."}, {"heading": "4.3 Final System Results", "text": "We adjust the model to the validation set and select the set of hyperparameters that provide the best performance to get the values of the test data. We report the official preliminary results in Table 5. There is an apparent performance decline in track4b that happens to all teams. We suspected that the sentences in track4b (s) come from a specific area, as the number of words not included in the vocabulary in track 4b is many times higher than in other tracks."}, {"heading": "5 Conclusion and Future Work", "text": "In summary, we propose a simple neural system with novel optimization possibilities. We found that optimization directly to PCC achieved the best values, which allowed the model to compete on STS-2017. Furthermore, we showed that the use of randomly initialized word embedding does not harm performance, but allows it to achieve slightly higher values than pre-trained word embedding."}], "references": [{"title": "Semeval2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation", "author": ["Eneko Agirre", "Carmen Banea", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Rada Mihalcea", "German Rigau", "Janyce Wiebe."], "venue": "Proceed-", "citeRegEx": "Agirre et al\\.,? 2016", "shortCiteRegEx": "Agirre et al\\.", "year": 2016}, {"title": "Semeval-2012 task 6: A pilot on semantic textual similarity", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre."], "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Pro-", "citeRegEx": "Agirre et al\\.,? 2012", "shortCiteRegEx": "Agirre et al\\.", "year": 2012}, {"title": "sem 2013 shared task: Semantic textual similarity", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor GonzalezAgirre", "Weiwei Guo."], "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main", "citeRegEx": "Agirre et al\\.,? 2013", "shortCiteRegEx": "Agirre et al\\.", "year": 2013}, {"title": "Usfd at semeval-2016 task 1: Putting different state-of-the-arts into a box", "author": ["Ahmet Aker", "Frederic Blain", "Andres Duque", "Marina Fomicheva", "Jurica Seva", "Kashif Shah", "Daniel Beck."], "venue": "Proceedings of the 10th International Workshop on Semantic", "citeRegEx": "Aker et al\\.,? 2016", "shortCiteRegEx": "Aker et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Uwb at semeval-2016 task 1: Semantic textual similarity using lexical, syntactic, and semantic information", "author": ["Tom\u00e1\u0161 Brychc\u0131\u0301n", "Luk\u00e1\u0161 Svoboda"], "venue": "In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-", "citeRegEx": "Brychc\u0131\u0301n and Svoboda.,? \\Q2016\\E", "shortCiteRegEx": "Brychc\u0131\u0301n and Svoboda.", "year": 2016}, {"title": "Measuring the semantic similarity of texts", "author": ["Courtney Corley", "Rada Mihalcea."], "venue": "Proceedings of the ACL workshop on empirical modeling of semantic equivalence and entailment. Association for Computational Linguistics, pages 13\u201318.", "citeRegEx": "Corley and Mihalcea.,? 2005", "shortCiteRegEx": "Corley and Mihalcea.", "year": 2005}, {"title": "A semantic similarity approach to paraphrase detection", "author": ["Samuel Fernando", "Mark Stevenson."], "venue": "Proceedings of the 11th Annual Research Colloquium of the UK Special Interest Group for Computational Linguistics. Citeseer, pages 45\u201352.", "citeRegEx": "Fernando and Stevenson.,? 2008", "shortCiteRegEx": "Fernando and Stevenson.", "year": 2008}, {"title": "A survey of text similarity approaches", "author": ["Wael H Gomaa", "Aly A Fahmy."], "venue": "International Journal of Computer Applications 68(13).", "citeRegEx": "Gomaa and Fahmy.,? 2013", "shortCiteRegEx": "Gomaa and Fahmy.", "year": 2013}, {"title": "Umd-ttic-uw at semeval-2016 task 1: Attention-based multi-perspective convolutional neural networks for textual similarity measurement", "author": ["Hua He", "John Wieting", "Kevin Gimpel", "Jinfeng Rao", "Jimmy Lin."], "venue": "Proceedings of the 10th Interna-", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Combining local context and wordnet similarity for word sense identification", "author": ["Claudia Leacock", "Martin Chodorow."], "venue": "WordNet: An electronic lexical database 49(2):265\u2013283.", "citeRegEx": "Leacock and Chodorow.,? 1998", "shortCiteRegEx": "Leacock and Chodorow.", "year": 1998}, {"title": "Uta dlnlp at semeval2016 task 1: Semantic textual similarity: A unified framework for semantic processing and evaluation", "author": ["Peng Li", "Heng Huang."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). Associa-", "citeRegEx": "Li and Huang.,? 2016", "shortCiteRegEx": "Li and Huang.", "year": 2016}, {"title": "Hhu at semeval-2016 task 1: Multiple approaches to measuring semantic textual similarity", "author": ["Matthias Liebeck", "Philipp Pollack", "Pashutan Modaresi", "Stefan Conrad."], "venue": "Proceedings of the 10th International Workshop on Semantic Evalu-", "citeRegEx": "Liebeck et al\\.,? 2016", "shortCiteRegEx": "Liebeck et al\\.", "year": 2016}, {"title": "An information-theoretic definition of similarity", "author": ["Dekang Lin"], "venue": "ICML. Citeseer, volume 98, pages 296\u2013304.", "citeRegEx": "Lin,? 1998", "shortCiteRegEx": "Lin", "year": 1998}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli."], "venue": "Nicoletta Calzolari (Conference Chair), Khalid Choukri,", "citeRegEx": "Marelli et al\\.,? 2014", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller."], "venue": "Communications of the ACM 38(11):39\u2013", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "2016. Samsung poland nlp team at semeval-2016 task 1: Necessity for diversity; combining recursive autoencoders, wordnet and ensem", "author": ["Barbara Rychalska", "Katarzyna Pakulska", "Krystyna Chodorowska", "Wojciech Walczak", "Piotr Andruszkiewicz"], "venue": null, "citeRegEx": "Rychalska et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rychalska et al\\.", "year": 2016}, {"title": "Takelab: Systems for measuring semantic text similarity", "author": ["Frane \u0160ari\u0107", "Goran Glava\u0161", "Mladen Karan", "Jan \u0160najder", "Bojana Dalbelo Ba\u0161i\u0107."], "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceedings", "citeRegEx": "\u0160ari\u0107 et al\\.,? 2012", "shortCiteRegEx": "\u0160ari\u0107 et al\\.", "year": 2012}, {"title": "Dls$@$cu at semeval-2016 task 1: Supervised models of sentence similarity", "author": ["Arafat Md Sultan", "Steven Bethard", "Tamara Sumner."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). Associa-", "citeRegEx": "Sultan et al\\.,? 2016", "shortCiteRegEx": "Sultan et al\\.", "year": 2016}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computa-", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D Turney", "Patrick Pantel."], "venue": "Journal of artificial intelligence research 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Fbk-hlt: An application of semantic textual similarity for answer selection in community question answering", "author": ["Ngoc Phuoc An Vo", "Simone Magnolini", "Octavian Popescu."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation, Se-", "citeRegEx": "Vo et al\\.,? 2015", "shortCiteRegEx": "Vo et al\\.", "year": 2015}, {"title": "Takelab: Systems for measuring semantic text similarity", "author": ["Frane \u0160ari\u0107", "Goran Glava\u0161", "Mladen Karan", "Jan \u0160najder", "Bojana Dalbelo Ba\u0161i\u0107."], "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Pro-", "citeRegEx": "\u0160ari\u0107 et al\\.,? 2012", "shortCiteRegEx": "\u0160ari\u0107 et al\\.", "year": 2012}, {"title": "Extractive summarization using supervised and semi-supervised learning", "author": ["Kam-Fai Wong", "Mingli Wu", "Wenjie Li."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1. Association for Computa-", "citeRegEx": "Wong et al\\.,? 2008", "shortCiteRegEx": "Wong et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Semantic textual similarity (STS) measures the degree of equivalence between the meanings of two text sequences (Agirre et al., 2016).", "startOffset": 112, "endOffset": 133}, {"referenceID": 24, "context": "It is widely applicable to many NLP tasks including summarization (Wong et al., 2008; Nenkova et al., 2011), generation and question answering (Vo et al.", "startOffset": 66, "endOffset": 107}, {"referenceID": 22, "context": ", 2011), generation and question answering (Vo et al., 2015), paraphrase detection (Fernando and Stevenson, 2008), and machine translation (Corley and Mihalcea, 2005).", "startOffset": 43, "endOffset": 60}, {"referenceID": 7, "context": ", 2015), paraphrase detection (Fernando and Stevenson, 2008), and machine translation (Corley and Mihalcea, 2005).", "startOffset": 30, "endOffset": 60}, {"referenceID": 6, "context": ", 2015), paraphrase detection (Fernando and Stevenson, 2008), and machine translation (Corley and Mihalcea, 2005).", "startOffset": 86, "endOffset": 113}, {"referenceID": 4, "context": "Further, we encode the sequential information with Recurrent Neural Network (RNN) and perform attention mechanism (Bahdanau et al., 2015) on RNN outputs for both sentences.", "startOffset": 114, "endOffset": 137}, {"referenceID": 18, "context": "However, for this competition we include merely the TakeLab features (\u0160ari\u0107 et al., 2012).", "startOffset": 69, "endOffset": 89}, {"referenceID": 8, "context": "Most proposed approaches in the past adopted a hybrid of varying text unit sizes ranging from character-based, token-based, to knowledge-based similarity measure (Gomaa and Fahmy, 2013).", "startOffset": 162, "endOffset": 185}, {"referenceID": 12, "context": "More recent approaches generally include the word embedding-based similarity (Liebeck et al., 2016; Brychc\u0131\u0301n and Svoboda, 2016) as a component of the final ensemble.", "startOffset": 77, "endOffset": 128}, {"referenceID": 5, "context": "More recent approaches generally include the word embedding-based similarity (Liebeck et al., 2016; Brychc\u0131\u0301n and Svoboda, 2016) as a component of the final ensemble.", "startOffset": 77, "endOffset": 128}, {"referenceID": 17, "context": "Top performing team in 2016 (Rychalska et al., 2016) uses an ensemble of multiple modules including recursive autoencoders with WordNet and monolingual aligner (Sultan et al.", "startOffset": 28, "endOffset": 52}, {"referenceID": 19, "context": ", 2016) uses an ensemble of multiple modules including recursive autoencoders with WordNet and monolingual aligner (Sultan et al., 2016).", "startOffset": 115, "endOffset": 136}, {"referenceID": 9, "context": "UMD-TTICUW (He et al., 2016) proposes the MPCNN model that requires no feature engineering and managed to perform competitively at 6th place.", "startOffset": 11, "endOffset": 28}, {"referenceID": 18, "context": "Surface Features Inspired by the simple system described in (\u0160ari\u0107 et al., 2012), We also extract surface features from the sentence pair as following:", "startOffset": 60, "endOffset": 80}, {"referenceID": 16, "context": "\u2022Ngram Overlap Similarity: These are features drawn from external knowledge like WordNet (Miller, 1995) and Wikipedia.", "startOffset": 89, "endOffset": 103}, {"referenceID": 10, "context": "We use both PathLen similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin et al.", "startOffset": 31, "endOffset": 59}, {"referenceID": 18, "context": "We employed the suggested preprocessing step (\u0160ari\u0107 et al., 2012), and added both WordNet and corpus-based information", "startOffset": 45, "endOffset": 65}, {"referenceID": 18, "context": "\u2022Semantic Sentence Similarity: We also computed token-based alignment overlap and vector space sentence similarity (\u0160ari\u0107 et al., 2012).", "startOffset": 115, "endOffset": 135}, {"referenceID": 21, "context": "We obtained the weighted form of latent semantic analysis vectors (Turney and Pantel, 2010) for each word w, before computing the cosine similarity.", "startOffset": 66, "endOffset": 91}, {"referenceID": 15, "context": "We explored initializing word embeddings randomly or with pre-trained word2vec (Mikolov et al., 2013) of dimension 50, 100, 300, respectively.", "startOffset": 79, "endOffset": 101}, {"referenceID": 3, "context": "\u2022 Negative Log-likelihood (NLL) of p and p\u0302 (Aker et al., 2016).", "startOffset": 44, "endOffset": 63}, {"referenceID": 5, "context": "\u2022 Mean square error (MSE) between yn and \u0177n (Brychc\u0131\u0301n and Svoboda, 2016).", "startOffset": 44, "endOffset": 73}, {"referenceID": 11, "context": "(Li and Huang, 2016; Tai et al., 2015).", "startOffset": 0, "endOffset": 38}, {"referenceID": 20, "context": "(Li and Huang, 2016; Tai et al., 2015).", "startOffset": 0, "endOffset": 38}, {"referenceID": 14, "context": "We gathered dataset from SICK (Marelli et al., 2014) and past STS across years 2012, 2013, 2014, 2015, and 2016 (Agirre et al.", "startOffset": 30, "endOffset": 52}], "year": 2017, "abstractText": "This paper describes a neural-network model which performed competitively (top 6) at the SemEval 2017 cross-lingual Semantic Textual Similarity (STS) task. Our system employs an attention-based recurrent neural network model that optimizes the sentence similarity. In this paper, we describe our participation in the multilingual STS task which measures similarity across English, Spanish, and Arabic.", "creator": "LaTeX with hyperref package"}}}