{"id": "1703.05486", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Using Reinforcement Learning for Demand Response of Domestic Hot Water Buffers: a Real-Life Demonstration", "abstract": "This paper demonstrates a data-driven control approach for demand response in real-life residential buildings. The objective is to optimally schedule the heating cycles of the Domestic Hot Water (DHW) buffer to maximize the self-consumption of the local photovoltaic (PV) production. A model-based reinforcement learning technique is used to tackle the underlying sequential decision-making problem. The proposed algorithm learns the stochastic occupant behavior, predicts the PV production and takes into account the dynamics of the system. A real-life experiment with six residential buildings is performed using this algorithm. The results show that the self-consumption of the PV production is significantly increased, compared to the default thermostat control.", "histories": [["v1", "Thu, 16 Mar 2017 06:42:07 GMT  (61kb,D)", "http://arxiv.org/abs/1703.05486v1", "Submitted to IEEE ISGT Europe 2017"]], "COMMENTS": "Submitted to IEEE ISGT Europe 2017", "reviews": [], "SUBJECTS": "cs.SY cs.LG", "authors": ["oscar de somer", "ana soares", "tristan kuijpers", "koen vossen", "koen vanthournout", "fred spiessens"], "accepted": false, "id": "1703.05486"}, "pdf": {"name": "1703.05486.pdf", "metadata": {"source": "CRF", "title": "Using Reinforcement Learning for Demand Response of Domestic Hot Water Buffers: a Real-Life Demonstration", "authors": ["Oscar De Somer", "Ana Soares", "Tristan Kuijpers", "Koen Vossen", "Koen Vanthournout", "Fred Spiessens"], "emails": [], "sections": [{"heading": null, "text": "Over the last ten years, it has become clear that the share of renewable energies in the total population has been many times higher than the share of renewable energies in the total population. In the last ten years, the share of renewable energies in the total population has tripled. In the last ten years, the share of renewable energies in the total population has tripled. In the last ten years, the share of renewable energies in the total population has tripled. In the last ten years, the share of renewable energies in the total population has tripled. In the last ten years, the number of renewable energies has tripled, and in the last ten years the number of renewable energies has tripled. In the last ten years, the share of renewable energies in the total population has tripled."}, {"heading": "II. SET-UP", "text": "The field trial initially consists of six renovated houses, but in a later phase additional houses will be added. An intelligent heat pump is used in each house for space heating and for heating the drinking water buffer. The insulation of the houses and the number of PV modules are dimensioned in such a way that the annual energy production exceeds their consumption. In the following three sub-sections, the drinking water buffer, the available sensors and the control system are briefly described."}, {"heading": "A. DHW Buffer", "text": "The drinking water tank has a volume of 200 litres. The buffer remains properly layered when water is tapped, but is mixed when a heating cycle is started. As this mixture creates an unpredictable temperature distribution, only full Chargingar Xiv: 170 3.05 486v 1 [cs.S Y] 16 Mar 2cycles is permitted, i.e. the heating process is stopped only if the temperature distribution in the buffer is assumed to be uniform. This greatly simplifies the problem of estimating the energy content of the buffer."}, {"heading": "B. Sensors", "text": "The most important for this application are: a temperature sensor in the centre of the buffer, a flow meter for measuring the tapped amount of hot water and an electric meter for the heat pump."}, {"heading": "C. Controls", "text": "The charging cycles of the TW buffer can be controlled by two control commands of the smart heat pump: firstly, the target temperature of the buffer can be set (Tset), i.e. if the temperature measured by the sensor in the centre of the buffer falls below Tset, the heat pump starts charging on Tset. To ensure user comfort, the minimum allowable target temperature is Tmin = 45 \u02da C, and maximum Tmax = 55 \u00b1 C. Secondly, a control command is available to force the heating process. This control can be used when a significant portion of the hot water is tapped, but the cold water front is still below the temperature sensor in the centre of the buffer. In this case, the sensor does not yet measure the cold water, which would not automatically start the heating."}, {"heading": "III. MARKOV DECISION PROCESSES", "text": "In RL, a problem is usually formulated as a Markov decision process (MDP) [10], [11]. An MDP is defined by its state space X, its action space U, and a transition function f: xk + 1 = f (xk, uk, wk), k = 0, 1,.., T \u2212 1, (1), which describes the dynamics from xk to xk + 1, controlled action uk [U, and is subject to a random error wk. The number of control periods in an optimization horizon is represented by T. The error wk is generated by a conditional probability distribution pw (\u00b7 | x). The transition from k to k + 1 is associated with a cost: ck = selection (xk, uk, wk), k = 0, 1,."}, {"heading": "A. State Description", "text": "rf\u00fc ide rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc for the rf\u00fc"}, {"heading": "B. Action", "text": "As described in section II, the target temperature of the boiler can be set and a heating cycle set in motion. Measures considered include either delaying reheating by setting the target temperature to Tmin, or starting the heat pump and uniformly charging the buffer to a certain temperature. In the further course of this work, the scope of action consists of three options, i.e. u = 0, 1, 2}, with: \u2022 u = 0: Heating delay \u2022 u = 1: Start heating now to SoC equivalent buffer with uniform temperature equal to Tmin \u2022 u = 2: Start heating now to SoC = 1"}, {"heading": "C. Transition Function", "text": "This paper proposes a model-based reinforcement approach, which means that a model of the transition function is used to generate data as input for the control strategy described in Section IV. This model is learned from historical measurement data. It must describe the evolution in SoC over time, include stochastic user behavior, and incorporate losses into the environment. In addition, there is a backup controller (BUC) that can be used to generate violations of comfort limits. This BUC should also be included in Model1) Tap Water Model: User behavior is captured by constructing an approximation of the conditional probability distribution pv (\u00b7 | t), which can be used to generate samples vi of water at a given time step. It is vi that the WK from (1) is represented by incorporating the historical water consumption data into the containers corresponding to their timerized components."}, {"heading": "D. Cost Function", "text": "In this thesis, the aim is to maximize the self-consumption of the PV production. The cost function is defined as \u03c1 (xk, uk, wk) = Pinj \u2206 t, where Pinj is the average power fed into the grid during the time interval. It is defined as PV production minus PV production, which is covered by the heat pump consumption according to the chosen measurement. A forecast of the PV production is available, as described in section III-A3. Obviously, this cost function can be easily adjusted to provide more flexible demand response options. Note that uncontrollable loads are ignored here, as they are very difficult to predict for a single house.The power consumption of the heat pump depends on the chosen measure and the SoC of the buffer. However, the performance profile cannot be modulated, only the duration of consumption can be prolonged. This means that in view of the boiler temperature profile, the two measures can be fixed."}, {"heading": "IV. CONTROL STRATEGY", "text": "The central idea behind Loose Amplification Learning is to estimate the Q function (4) based on past observations. These observations are represented by 4-tuples (x, u, x, x, x, c) containing the state, the action, the next state and the corresponding costs. In this thesis, a model is first learned to generate observations and then this model is used to form a training set to learn policy. This approach reduces the number of interactions with the system. Note that neither the dynamics of the system nor the cost function are given in an analytical form. Optimal behavior in this highly stochastic environment should be learned by interacting with the model [13]. To simplify the estimation of the Q function, a separate approach is made to Q-N in each time step. Further simplification occurs by iterating backwards over time steps, i.e., starting at the end of the optimization horizon, the Q function is set to zero and running backwards in time."}, {"heading": "V. RESULTS", "text": "In this section, the results of the proposed algorithm are presented in a real-life demonstration, the experiment looks at six houses from the same district over a period of four months, using a one-hour time step to calculate the Q functions, dividing the state space into 25 equidistant grid points and taking 200 samples of the tap water model from each grid point. Every hour, the policy is retrained with the latest data for a shrinking horizon of 24 hours, and the best action is chosen.The next two subsections examine the learned control policies and some typical active control behaviors.In addition, the performance of the learning algorithm is compared with the standard scenario using static thermostat control."}, {"heading": "A. Control Policy", "text": "Fig. 2 shows an example of the learned regulatory guidelines over a period of 28 hours. It shows the best measures according to the guidelines on state space. It can be seen that the policy tries to avoid charging during the period that precedes the highest PV production. Depending on the state of charge and the time, it charges at night to the minimum temperature in order to avoid a charging cycle triggered by the reserve controller just before noon. If the PV production comes close to its maximum, it starts heating the boiler. Preferably, first to the minimum temperature (u = 1) and then to the maximum temperature (u = 2). When the PV production starts, it charges if possible to its maximum temperature (u = 2). An example of a real trajectory on a winter day is in Fig. 3.A more detailed analysis of the guidelines can be obtained by evaluating the Q functions over state space for each measure. Fig. 4 shows the same day, the last 12 hours evaluated for Q should be selected."}, {"heading": "B. Performance Indicators", "text": "In order to compare the PV self-consumption of the proposed control algorithm with the standard thermostat control, performance indicators were calculated for both cases. Over a period from September 2016 to January 2017, the scenarios alternated every week. All houses are controlled by the same algorithm. By switching the scenarios weekly, the seasonal influence of the meteorological conditions was minimized. Table I shows some performance indicators for the six houses combined. It can be seen that the share of PV production detected by heating the hot water treatment almost triples due to the use of the active control algorithm. The second indicator represents the share of PV production detected by the total electricity consumption. Uncontrollable loads were neglected in the control strategy, as they are difficult to predict for an individual house. However, in order to verify the practical importance of the algorithm, they are taken into account in the determination of the PV self-power by the total electricity consumption, as the increase in the PV self-efficiency clearly shows by 20%."}, {"heading": "VI. CONCLUSION", "text": "The aim of this work was to implement a data-driven control approach for the demand response in real residential buildings. The aim was to optimally plan the heating cycles of the DHW buffer in order to maximise the self-consumption of local PV production. To address the underlying sequential decision problem, a model-based learning algorithm was used to amplify it. The approach to learn stochastic user behaviour, predict PV production and take into account the dynamics of the system is discussed. A real experiment with the proposed approach over a period of four months with six residential buildings is analysed. Each house is equipped with an intelligent heat pump and PV modules designed to achieve annual energy neutrality. Results show that the algorithm used significantly increases PV self-consumption compared to thermostat control. In the future, additional houses will be added to the experiment and the results evaluated over a longer period."}, {"heading": "ACKNOWLEDGMENT", "text": "The research that led to these results was funded by the European Commission under the H2020 - EU.2.1.5.3 programme under funding agreement No. 680603 - REnnovates."}], "references": [{"title": "LV distribution network feeders in Belgium and power quality issues due to increasing PV penetration levels", "author": ["C. Gonzalez", "J. Geuns", "S. Weckx", "T. Wijnhoven", "P. Vingerhoets", "T. De Rybel", "J. Driesen"], "venue": "IEEE PES Innovative Smart Grid Technologies Conference Europe, pp. 1\u20138, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "The impacts of storing solar energy in the home to reduce reliance on the utility", "author": ["R.L. Fares", "M.E. Webber"], "venue": "Nature Energy, vol. 2, p. 17001, 1 2017.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Linear breakthrough project: Large-scale implementation of smart grid technologies in distribution grids", "author": ["B. Dupont", "P. Vingerhoets", "P. Tant", "K. Vanthournout", "W. Cardinaels", "T.D. Rybel", "E. Peeters", "R. Belmans"], "venue": "2012 3rd IEEE PES Innovative Smart Grid Technologies Europe (ISGT Europe), Oct 2012, pp. 1\u20138.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Generalizable occupant-driven optimization model for domestic hot water production in NZEB", "author": ["H. Kazmi", "S. D\u2019Oca", "C. Delmastro", "S. Lodeweyckx", "S.P. Corgnati"], "venue": "Applied Energy, vol. 175, pp. 1\u201315, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Beyond theory: Experimental results of a self-learning air conditioning unit", "author": ["T. Leurs", "B.J. Claessens", "F. Ruelens", "S. Weckx", "G. Deconinck"], "venue": "IEEE International Energy Conference, ENERGYCON, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Reinforcement learning versus model predictive control: A comparison on a power system problem", "author": ["D. Ernst", "M. Glavic", "F. Capitanescu", "L. Wehenkel"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 39, no. 2, pp. 517\u2013529, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Residential demand response of thermostatically controlled loads using batch reinforcement learning", "author": ["F. Ruelens", "B.J. Claessens", "S. Vandael", "B.D. Schutter", "R. Babuka", "R. Belmans"], "venue": "IEEE Transactions on Smart Grid, vol. PP, no. 99, pp. 1\u201311, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Residential demand response applications using batch reinforcement learning", "author": ["F. Ruelens", "B. Claessens", "S. Vandael", "B. De Schutter", "R. Babuska", "R. Belmans"], "venue": "arXiv preprint arXiv:1504.02125, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning: An introduction", "author": ["R. Sutton", "A. Barto"], "venue": "1998.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Tree-Based Batch Mode Reinforcement Learning", "author": ["D. Ernst", "P. Geurts", "L. Wehenkel"], "venue": "Journal of Machine Learning Research, vol. 6, no. 1, pp. 503\u2013556, 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Approximate value iteration in the reinforcement learning context. application to electrical power system control", "author": ["D. Ernst", "M. Glavic", "P. Geurts", "L. Wehenkel"], "venue": "International Journal of Emerging Electric Power Systems, vol. 3, no. 1, pp. 1066\u20131, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Extremely randomized trees", "author": ["P. Geurts", "D. Ernst", "L. Wehenkel"], "venue": "Machine learning, vol. 63, no. 1, pp. 3\u201342, 2006. 6", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "It accelerates transformer aging and may cause voltage problems on the distribution feeder [1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "Besides that, it does not induce as much energy losses as with grid connected battery systems [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "Field tests in the residential sector have been conducted in multiple countries in order to assess the response to various input signals and the resultant flexibility [3], [4], [5].", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "Field tests in the residential sector have been conducted in multiple countries in order to assess the response to various input signals and the resultant flexibility [3], [4], [5].", "startOffset": 172, "endOffset": 175}, {"referenceID": 4, "context": "Field tests in the residential sector have been conducted in multiple countries in order to assess the response to various input signals and the resultant flexibility [3], [4], [5].", "startOffset": 177, "endOffset": 180}, {"referenceID": 5, "context": "RL was designed to infer closedloop policies for stochastic optimal control problems from a sample of trajectories gathered from interaction with the real system or from simulations [6].", "startOffset": 182, "endOffset": 185}, {"referenceID": 4, "context": "RL has already been applied in several related test cases with promising results [5], [7], [8].", "startOffset": 81, "endOffset": 84}, {"referenceID": 6, "context": "RL has already been applied in several related test cases with promising results [5], [7], [8].", "startOffset": 86, "endOffset": 89}, {"referenceID": 7, "context": "RL has already been applied in several related test cases with promising results [5], [7], [8].", "startOffset": 91, "endOffset": 94}, {"referenceID": 8, "context": "In RL, a problem is usually formulated as a Markov Decision Process (MDP) [10], [11].", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "In RL, a problem is usually formulated as a Markov Decision Process (MDP) [10], [11].", "startOffset": 80, "endOffset": 84}, {"referenceID": 7, "context": "The relevant state space X of a domestic water heater can be broken down in a time-related component, a controllable component and a non-controllable exogenous component [8], [7].", "startOffset": 170, "endOffset": 173}, {"referenceID": 6, "context": "The relevant state space X of a domestic water heater can be broken down in a time-related component, a controllable component and a non-controllable exogenous component [8], [7].", "startOffset": 175, "endOffset": 178}, {"referenceID": 10, "context": "The optimal behavior in this strongly stochastic environment should be learned by interacting with the model [13].", "startOffset": 109, "endOffset": 113}, {"referenceID": 10, "context": "For each in the considered period, a grid is created over the state-action Algorithm 1 Model based fitted Q-iteration [13] Require: Grid X \u00d7 U over state-action space, transition function f and cost function \u03c1 Q\u0302T+1 \u2190 0 for N = T, \u00b7 \u00b7 \u00b7 , 1 do // Build training set T S = { (i, o) }|F| l=1 : l\u2190 0 for \u2200(x, u) \u2208 X \u00d7 U do l\u2190 l + 1 i \u2190 (x, u)", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "In this case, an ensemble of extremely randomized trees is used as supervised learning algorithm [14].", "startOffset": 97, "endOffset": 101}], "year": 2017, "abstractText": "This paper demonstrates a data-driven control approach for demand response in real-life residential buildings. The objective is to optimally schedule the heating cycles of the Domestic Hot Water (DHW) buffer to maximize the selfconsumption of the local photovoltaic (PV) production. A modelbased reinforcement learning technique is used to tackle the underlying sequential decision-making problem. The proposed algorithm learns the stochastic occupant behavior, predicts the PV production and takes into account the dynamics of the system. A real-life experiment with six residential buildings is performed using this algorithm. The results show that the self-consumption of the PV production is significantly increased, compared to the default thermostat control. Reinforcement Learning, Demand Response, Domestic Hot Water, Field Experiment March 1, 2017", "creator": "LaTeX with hyperref package"}}}