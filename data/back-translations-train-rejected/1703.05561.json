{"id": "1703.05561", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Fraternal Twins: Unifying Attacks on Machine Learning and Digital Watermarking", "abstract": "Machine learning is increasingly used in security-critical applications, such as autonomous driving, face recognition and malware detection. Most learning methods, however, have not been designed with security in mind and thus are vulnerable to different types of attacks. This problem has motivated the research field of adversarial machine learning that is concerned with attacking and defending learning methods. Concurrently, a different line of research has tackled a very similar problem: In digital watermarking information are embedded in a signal in the presence of an adversary. As a consequence, this research field has also extensively studied techniques for attacking and defending watermarking methods.", "histories": [["v1", "Thu, 16 Mar 2017 11:15:28 GMT  (1152kb,D)", "http://arxiv.org/abs/1703.05561v1", null]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["erwin quiring", "daniel arp", "konrad rieck"], "accepted": false, "id": "1703.05561"}, "pdf": {"name": "1703.05561.pdf", "metadata": {"source": "CRF", "title": "Fraternal Twins: Unifying Attacks on Machine Learning and Digital Watermarking", "authors": ["Erwin Quiring", "Daniel Arp"], "emails": [], "sections": [{"heading": null, "text": "Machine learning is increasingly used in safety-critical applications, such as autonomous driving, face recognition and malware detection, but most learning methods are not designed with security in mind and are therefore susceptible to various types of attack. This problem has motivated the field of research of opposing machine learning, which focuses on attacking and defensive learning methods. At the same time, another field of research has tackled a very similar problem: with digital watermarking, information is embedded in a signal in the presence of an adversary. As a result, this field of research has also extensively investigated techniques for attacking and defending watermarks.The two research communities have worked in parallel to date, developing similar attack and defense strategies unnoticed, and this paper is a first attempt to bring these communities together. To this end, we present a uniform notation of black box attacks against machine learning and watermarks that reveals the similarity of both settings. To demonstrate the effectiveness of this unified view, we are applying concepts of water-time learning to counter-attack learning, and vice versa way of water-time learning."}, {"heading": "1 Introduction", "text": "In fact it is so that we are able to assert ourselves in a position to assert that we are able to assert ourselves in the world, and that we are able to assert ourselves in the world, that we are able to live in the world, in the world, in the world, in the world, in the world, in the world, in which we live, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in"}, {"heading": "2 Background", "text": "When machine learning or digital watermarking is used in safety-critical applications, the presence of an attacker must be taken into account; this adversary may attempt to attack the learning / watermarking process, thereby compromising the confidentiality, integrity and availability of the application. This section provides a basic introduction to the motivational and threat scenarios in machine learning and digital watermarking before Section 3 systematizes them under a common notation. A reader familiar with either area can proceed directly to Section 3."}, {"heading": "2.1 Adversarial Machine Learning", "text": "The success of machine learning methods is based on their ability to deduce patterns and relationships from large amounts of data [see 17, 26]. However, this conclusion is generally not robust against attacks and can therefore be disturbed by an adversary. In this case, the adversary can be divided into three classes: intoxication, evasion and extraction of data. The latter are the focus of our work as they have concrete adversaries in the field of digital water marking."}, {"heading": "2.2 Digital Watermarking", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "3 Unifying Adversarial Learning and Digital Watermarking", "text": "From the previous section, it appears that attacks against learning and watermarking methods have similarities - an observation that was surprisingly overlooked by the two research communities [2]. During this section, we systematically identify the similarities and show that it is possible to transfer knowledge about attacks and defense mechanisms from one field to the other. An overview of this systematization is presented in Figure 3. We guide our systematization of machine learning and digital watermarking along the following four concepts: 1. Data representation. Machine learning and watermarking use similar data representations that allow appropriate learning and recognition methods to be placed in the same context (Figure 3.1) 2. Problem Assessment. Watermarks can be considered a special case of binary classification. Consequently, binary classifiers and watermarking techniques solve a similar problem (Section 3.2).3. Attacks. Due to the similar representation and problem setting, attacks between the two fields overlap, as we allow for evasion alternatives to be discussed (Section 3.3) and (detail)."}, {"heading": "3.1 Feature Space vs. Media Space", "text": "Machine Learning. Learning methods typically operate on a so-called attribute space, which captures the characteristics of the data to be analyzed and learned. These characteristics correspond to the vectors x-RN and, in the case of classification, are assigned to a class name y, which must be learned and predicted, such as C + and C \u2212 in Figure 3 (a). Note that attribute spaces in machine learning can also be constructed implicitly, for example by means of nonlinear maps and core functions [17, 48]. Digital watermark. Similar to machine learning, watermarking methods work on a signal present in an underlying media space, such as the pixels of an image or the audio waves of a recording. Without loss of generality, this signal can be described as a vector x-RN, and thus the media space corresponds to the attribute space used in machine learning. Note that advanced watermarking schemes often depict the signal as a subspatial area, such as vector or vector space."}, {"heading": "3.2 Classifier vs. Watermark Detector", "text": "After embedding the training data in a trait space, the actual learning process takes place using a learning method, e.g. a support vector machine or a neural network. In the case of classification, this learning method attempts to infer functional dependencies from the training data on different data points of different classes. These dependencies are described in a learning model w, which parameterizes a decision function fw (x). In the case of a vector x, the function fw (x) predicts a class name or a corresponding numerical prediction scale. Digital watermark. The media space in the watermark is divided into two separate subspaces, as shown in Figure 3 (d), where the marked and unmarked versions of the signal represent the two classes. Note that a robust watermark should ideally survive image processing steps, such as compression and denoization. Therefore, the media space in the watermark range in the watermark class is also implicitly covered by variations, just like the general machine learning."}, {"heading": "3.3 Evasion Attack vs. Oracle Attack", "text": "In this case, it is the way in which one sees oneself in a position to solve the problem, in which it is about the question to what extent it is actually about a problem, in which it is about a problem, which is not only about a problem, but also about a problem that affects the way, in which it is about a way, in which it is about an attack, in which it is about the way, in which it is about the way, in which it is about the way, in which it is about the way, in which it is about the way, in which it is about the way, in which it is about the way, in which it is about the way, in which it is about the way, in which it is about the way, in which it is about the way, in which it is about the way, in which it is about the way, in which it is about the way, in which it is about the way, in which it is about the way, in which it is about the way, in which way it is about the way, in which it is about the way, in which it is about the way, in which it is about the way in which it is about the way, in which it is about the way in which it is about the way, in which it is about the way in which it is about the way, in which it is about the way in which it is about the way, in which it is about the way in which it is about the way in which it is about the way, in which it is about the way in which it is about the way in which it is about the way, in which it is about the way in which it is about the way it is about the way in which it is about the way in which it is about the way, in which it is about the way in which it is about the way in which it is about the way it is about the way, in which it is about the way in which it is about which it is about the way it is about the way it is about the way it is about the way, in which it is about how it is about how it is about which is about how it is about which is about which is about it is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about which is about"}, {"heading": "3.4 Model Extraction", "text": "The second attack mapping, we look at the pair of model taking and watermark estimation. In the blackbox scenario, the opponent aims to compromise the confidentiality of a learning model or digital watermark by sending specifically designed objects to a particular classifier / detector and the respective binary output. Machine learning aims at an effective strategy of querying a classifier so that the underlying model can be reconstructed with a few queries. Thus, we have recently demonstrated this threat of stealing models from cloud platforms that offer machine learning as a service. In contrast, extracting the learning model w is able to reconstruct the decision function fw and apply it to arbitrary data."}, {"heading": "3.5 Defenses", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4 Transfer of Attacks and Defenses", "text": "As a first case study, we apply a concept proposed by Biggio et al. [6] to secure machine learning to a watermark detector. We show that the resulting detector mitigates a state-of-the-art oracle attack. In the second case study, we apply the concept of state recognition to a method of machine learning and show that this combination effectively counteracts model extraction attacks on decision trees. While these case studies focus on two specific defense mechanisms, we encourage communities to work together and therefore summarize further research directions in Section 5."}, {"heading": "4.1 From Machine Learning to Watermarking", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "4.2 From Watermarking to Machine Learning", "text": "It is about the question to what extent it is about a way, in which it is about the question, to what extent it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, and in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way, in which it is about which it is about a way, in which it is about which it is about a way, in which it is about which it is about which it is about a way, in which it is about which it is about which it is about a way, in which it is about which it is about which it is about which it is about which it is about a way, in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about a way, in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which is about which it is about which it is about which it is about which it is about which is about which it is about which it is about which it is about which is about which it is about which it is about which it is about which it is about which it is about which it is about which is about which is about which is about which is about which is about which is about which it is about which is about which is about which is about which"}, {"heading": "5 Discussion", "text": "In recent years, it has been shown that the knowledge gained from machine learning can help to mitigate attacks on water brands, and the presented unified view opens up interesting approaches for future work. Comparing the defense mechanisms between the two areas in Section 3.5 shows that government recognition strategies have not yet been taken into account in machine learning. While we have successfully transposed the safety margin approach in this paper, a unified view opens up interesting approaches for future work. Comparing the defense mechanisms between the two areas in Section 3.5 shows that government recognition strategies have not yet been considered."}, {"heading": "6 Conclusion", "text": "Developing methods of analysis for an adversarial environment is a challenging task: firstly, these methods must provide correct results even when parts of their input are manipulated, and secondly, these methods should protect against known and future attacks. In this paper, we show that the research areas of adversarial learning and digital watermarking have both risen to this challenge and developed a remarkable set of defense mechanisms to operate in an adversarial environment. In this paper, we show that the two research areas share similarities that were overlooked in previous work and can transfer concepts from one area to another. By systematizing attacks, we are able to transform defense mechanisms for learning methods in the area of watermarking and vice versa. This not only opens up new perspectives for designing common defense mechanisms, but also allows the combination of techniques from both areas that have not been dovetailed so far. In our analysis, we identify interesting directions of future research that will allow the two communities to learn from each other and combine the \"best of both worlds.\""}, {"heading": "A Linear Watermark Detector", "text": "This section illustrates the watermarking process with a simple watermarking scheme. The process can generally be divided into two phases: embedding and recognition. Consider, for example, the additive watermarking scheme, which is also the embedding scheme used in the example shown in Figure 2. In this scheme, the watermarking version x of a signal x is generated by plotting a watermark vector w-RN on x element by element, i.e. x-x + w. (4) The watermark w usually represents a random pattern. To determine whether a signal contains a certain watermark, a linear correlation detector can be used that uses the following decision function (x-ig) = x-w. (5) The output is a weighted sum between x-and the watermark w. If watermark and signal match, the correlation exceeds a predefined threshold. Geometrically, each signal corresponds to a point in a vector space determining the presence of a water mark, as the 8 is a decision boundary for the two."}, {"heading": "B Blind Newton Sensitivity Attack", "text": "This section briefly recapitulates the Blind Newton Sensitivity Attack (BNSA), which interprets watermark removal as a nonlinear optimization problem [12]: min d (t) (6) is subject to fw (x) = fw (x) + t) = \u03b7. (7) The objective function d (t) measures the changes in the image x). For example, the squared Euclidean standard d (t) = fw (x) 22 minimizes the length of t and thus the necessary pixel changes. At the same time, the optimal solution must fulfill the limitation that the detector does not recognize the watermark. A position at the limit is sufficient here, so that Equation (7) limits the decision function to \u03b7. However, the opponent does not know the function fw (x) because the watermark w is kept secret. Although only a binary detector output is possible, an attack is explicit."}, {"heading": "C Security Margin Construction", "text": "The construction of the safety margin works as follows: First, we select a tree region and select the training data located within that particular region; then, we estimate the distribution of the selected training data in each dimension by estimating the core density. In this way, no a priori assumptions about their distribution are required. Finally, the distribution in each dimension is used to define the margin at the boundary in that dimension. To this end, we set the margin to the characteristic value where the probability of occurrence is less than a certain threshold. In Figure 7 (a), for example, the upper right tree regions have a smaller safety margin because more training data is located near the boundary. On the contrary, the farthest left region has fewer training samples near the boundary, so that a larger margin can be defined. By controlling the safety margin in this statistical way, we can ensure that an honest query is within the margin. We repeat the process for each tree region."}, {"heading": "D BOWS Contest", "text": "\"Break Our Watermarking System,\" or BOWS, is a competition held twice in the watermark community. The latest contest is divided into three consecutive episodes, with only the last episode revealing the underlying watermark scheme. 1. At the beginning of the contest, 3 watermark images are available along with an online watermark detector that allows 30 calls per day. This episode models an attacker with limited knowledge and skills. Participants must operate with few queries and must construct their attacks carefully. 2. In the next episode, the daily rate is limited and participants can perform various forms of oracle and watermark evaluation attacks on the detector. The episode models a more powerful attacker, as only 3 images are available to derive the pattern of the watermark scheme. 3. Finally, the same watermark is embedded in 10,000 images and the underlying watermark scheme is published. This episode models a more powerful attacker, as there are only 3 images available to deduce the pattern of the watermark scheme. 3. Finally, the same watermark is embedded in 10,000 images and the underlying watermark scheme is released."}], "references": [{"title": "Are you threatening me?: Towards smart detectors in watermarking", "author": ["M. BARNI", "P. COMESA\u00d1A-ALFARO", "F. P\u00c9REZ-GONZ\u00c1LEZ", "B. TONDI"], "venue": "Proceedings of SPIE", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Coping with the enemy: Advances in adversary-aware signal processing", "author": ["M. BARNI", "F. P\u00c9REZ-GONZ\u00c1LEZ"], "venue": "In IEEE International Conference on Acoustics, Speech, and Signal Processing", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Putting reproducible signal processing into practice: A case study in watermarking", "author": ["M. BARNI", "F. P\u00c9REZ-GONZ\u00c1LEZ", "P. COMESA\u00d1A", "G. BAR- TOLI"], "venue": "In International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Two key estimation techniques for the broken arrows watermarking scheme", "author": ["P. BAS", "A. WESTFELD"], "venue": "In Proc. of ACM Workshop on Multimedia and Security", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "One-and-a-half-class multiple classifier systems for secure learning against evasion attacks at test time", "author": ["B. BIGGIO", "I. CORONA", "Z. HE", "P.P.K. CHAN", "G. GIACINTO", "D.S. YEUNG", "F. ROLI"], "venue": "In Proc. of International Workshop on Multiple Classifier Systems (MCS)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Evasion attacks against machine learning at test time", "author": ["B. BIGGIO", "I. CORONA", "D. MAIORCA", "B. NELSON", "N. \u0160RNDI\u0106", "P. LASKOV", "G. GIACINTO", "F. ROLI"], "venue": "In Machine Learning and Knowledge Discovery in Databases. Springer,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Adversarial pattern classification using multiple classifiers and randomisation", "author": ["B. BIGGIO", "G. FUMERA", "F. ROLI"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Support vector machines under adversarial label noise", "author": ["B. BIGGIO", "B. NELSON", "P. LASKOV"], "venue": "In Proc. of Asian Conference on Machine Learning (ACML)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Poisoning attacks against support vector machines", "author": ["B. BIGGIO", "B. NELSON", "P. LASKOV"], "venue": "In Proc. of International Conference on Machine Learning (ICML)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Noniterative algorithms for sensitivity analysis attacks", "author": ["M.E. CHOUBASSI", "P. MOULIN"], "venue": "IEEE Transactions on Information Forensics and Security", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Blind newton sensitivity attack", "author": ["P. COMESA\u00d1A", "L. P\u00c9REZ-FREIRE", "F. P\u00c9REZ-GONZ\u00c1LEZ"], "venue": "IEE Proceedings \u2013 Information Security 153,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Breaking the BOWS watermarking system: Key guessing and sensitivity attacks", "author": ["P. COMESA\u00d1A", "F. P\u00c9REZ-GONZ\u00c1LEZ"], "venue": "EURASIP Journal on Information Security 2007,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Public watermarks and resistance to tampering", "author": ["COX I. J", "LINNARTZ", "G.J.-P. M"], "venue": "In Proc. of IEEE International Conference on Image Processing (ICIP)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Digital watermarking and steganography", "author": ["I.J. COX", "M. MILLER", "J. BLOOM", "J. FRIDRICH", "T. KALKER"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Reverse-engineering a detector with false alarms", "author": ["CRAVER S", "YU"], "venue": "Proceedings of SPIE", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "P.E.HART, AND D.G.STORK. Pattern classification, second ed", "author": ["R. DUDA"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "On the fundamental tradeoff between watermark detection performance and robustness against sensitivity analysis attacks", "author": ["M. EL CHOUBASSI", "P. MOULIN"], "venue": "Proceedings of SPIE", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Evading network anomaly detection systems: formal reasoning and practical techniques", "author": ["FOGLA P", "LEE"], "venue": "In Proc. of ACM Conference on Computer and Communications Security (CCS)", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Polymorphic blending attacks", "author": ["P. FOGLA", "M. SHARIF", "R. PERDISCI", "O. KOLESNIKOV", "LEE"], "venue": "In Proc. of USENIX Security Symposium", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "NIS: Just another N-order side-informed watermarking scheme", "author": ["T. FURON", "B. MACQ", "N. HURLEY", "SILVESTRE", "G. JA"], "venue": "In Proc. of IEEE International Conference on Image Processing (ICIP) (2002),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Unified approach of asymmetric watermarking schemes", "author": ["T. FURON", "I. VENTURINI", "P. DUHAMEL"], "venue": "Proceedings of SPIE", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "The Dresden Image Database for benchmarking digital image forensics", "author": ["T. GLOE", "R. B\u00d6HME"], "venue": "Journal of Digital Forensic Practice", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["K. GROSSE", "N. PAPERNOT", "P. MANOHARAN", "M. BACKES", "P. MCDANIEL"], "venue": "Tech. Rep. abs/1606.04435,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "The Elements of Statistical Learning: data mining, inference and prediction. Springer series in statistics", "author": ["T. HASTIE", "R. TIBSHIRANI", "J. FRIEDMAN"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "Adversarial machine learning", "author": ["L. HUANG", "A.D. JOSEPH", "B. NELSON", "B.I.P. RUBINSTEIN", "J.D. TYGAR"], "venue": "In Proc. of ACM Workshop on Artificial Intelligence and Security (AISEC)", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Watermark estimation through detector analysis", "author": ["T. KALKER", "LINNARTZ", "J.-P.M. G", "M. VAN DIJK"], "venue": "In Proc. of IEEE International Conference on Image Processing (ICIP)", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1998}, {"title": "Revolver: An automated approach to the detection of evasive web-based malware", "author": ["A. KAPRAVELOS", "Y. SHOSHITAISHVILI", "M. COVA", "C. KRUEGEL", "G. VIGNA"], "venue": "In Proc. of USENIX Security Symposium (Aug", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Rozzle: De-cloaking internet malware", "author": ["C. KOLBITSCH", "B. LIVSHITS", "B. ZORN", "C. SEIFERT"], "venue": "In Proc. of IEEE Symposium on Security and Privacy", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Feature weighting for improved classifier robustness", "author": ["KO\u0141CZ A", "TEO", "H. C"], "venue": "In Proc. of Conference on Email and Anti- Spam (CEAS)", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Towards fully autonomous driving: Systems and algorithms", "author": ["J. LEVINSON", "J. ASKELAND", "J. BECKER", "J. DOLSON", "D. HELD", "S. KAMMEL", "J.Z. KOLTER", "D. LANGER", "O. PINK", "V. PRATT", "M. SOKOLSKY", "G. STANEK", "D.M. STAVENS", "A. TEICHMAN", "M. WERLING", "S. THRUN"], "venue": "In Proc. of IEEE Intelligent Vehicles Symposium (IV)", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Acing the ioc game: Toward automatic discovery and analysis of open-source cyber threat intelligence", "author": ["X. LIAO", "K. YUAN", "X. WANG", "Z. LI", "L. XING", "R.A. BEYAH"], "venue": "In Proc. of ACM Conference on Computer and Communications Security (CCS)", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Analysis of the sensitivity attack against electronic watermarks in images", "author": ["LINNARTZ", "J.-P.M. G", "M. VAN DIJK"], "venue": "In Proc. of Information Hiding Conference", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1998}, {"title": "Adversarial learning", "author": ["D. LOWD", "C. MEEK"], "venue": "In Proc. of ACM SIGKDD Conference on Knowledge Discovery in Data Mining (KDD)", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2005}, {"title": "Good word attacks on statistical spam filters", "author": ["D. LOWD", "C. MEEK"], "venue": "In Conference on Email and Anti-Spam", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Improving the security of watermark public detectors", "author": ["M.F. MANSOUR", "A.H. TEWFIK"], "venue": "In Proc. of International Conference on Digital Signal Processing (DSP)", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2002}, {"title": "LMS-based attack on watermark public detectors", "author": ["M.F. MANSOUR", "A.H. TEWFIK"], "venue": "In Proc. of International Conference on Image Processing (ICIP)", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2002}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["N. PAPERNOT", "P. MCDANIEL", "I. GOODFELLOW"], "venue": "Tech. rep.,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["N. PAPERNOT", "P. MCDANIEL", "I. GOODFELLOW", "S. JHA", "Z. BERKAY CELIK", "A. SWAMI"], "venue": "Tech. rep.,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. PAPERNOT", "P. MCDANIEL", "S. JHA", "M. FREDRIKSON", "Z.B. CELIK", "A. SWAMI"], "venue": "In Proc. of IEEE European Symposium on Security and Privacy", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Towards the science of security and privacy in machine learning", "author": ["N. PAPERNOT", "P. MCDANIEL", "A. SINHA", "M. WELLMAN"], "venue": "Tech. Rep. abs/1611.03814, Computing Research Repository (CoRR),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. PAPERNOT", "P.D. MCDANIEL", "X. WU", "S. JHA", "A. SWAMI"], "venue": "In Proc. of IEEE Symposium on Security and Privacy", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Using an ensemble of oneclass SVM classifiers to harden payload-based anomaly detection systems", "author": ["PERDISCI R", "GU G", "LEE"], "venue": "In Proc. of International Conference on Data Mining (ICDM)", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2006}, {"title": "Design and analysis of the first BOWS contest", "author": ["A. PIVA", "M. BARNI"], "venue": "EURASIP Journal on Information Security", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2007}, {"title": "Detecting steganographic content on the internet", "author": ["N. PROVOS", "P. HONEYMAN"], "venue": "In Proc. of Network and Distributed System Security Symposium (NDSS)", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2002}, {"title": "Secure kernel machines against evasion attacks", "author": ["P. RUSSU", "A. DEMONTIS", "B. BIGGIO", "G. FUMERA", "F. ROLI"], "venue": "In Proc. of ACM Workshop on Artificial Intelligence and Security (AISEC)", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}, {"title": "Learning with Kernels", "author": ["B. SCH\u00d6LKOPF", "A.J. SMOLA"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2002}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["F. SCHROFF", "D. KALENICHENKO", "J. PHILBIN"], "venue": "In Proc. of IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2015}, {"title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition", "author": ["M. SHARIF", "S. BHAGAVATULA", "L. BAUER", "M.K. REITER"], "venue": "In Proc. of ACM Conference on Computer and Communications Security (CCS)", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2016}, {"title": "Membership inference attacks against machine learning models", "author": ["R. SHOKRI", "M. STRONATI", "V. SHMATIKOV"], "venue": "Tech. rep.,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "On the infeasibility of modeling polymorphic shellcode", "author": ["Y. SONG", "M. LOCASTO", "A. STAVROU", "A. KEROMYTIS", "S. STOLFO"], "venue": "In Proc. of ACM Conference on Computer and Communications Security (CCS)", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2007}, {"title": "Practical evasion of a learningbased classifier: A case study", "author": ["N. SRNDIC", "P. LASKOV"], "venue": "In Proc. of IEEE Symposium on Security and Privacy", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2014}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. TAIGMAN", "M. YANG", "M.A. RANZATO", "L. WOLF"], "venue": "In Proc. of IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "On the effectiveness of meta-detection for countering oracle attacks in watermarking", "author": ["B. TONDI", "P. COMESA\u00d1A-ALFARO", "F. P\u00c9REZ-GONZ\u00c1LEZ", "M. BARNI"], "venue": "In Workshop on Information Forensics and Security (WIFS)", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2015}, {"title": "Stealing machine learning models via prediction apis", "author": ["F. TRAM\u00c8R", "F. ZHANG", "A. JUELS", "M.K. REITER", "T. RISTENPART"], "venue": "In Proc. of USENIX Security Symposium", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2016}, {"title": "Randomized detection for spread-spectrum watermarking: Defending against sensitivity and other attacks", "author": ["R. VENKATESAN", "M.H. JAKUBOWSKI"], "venue": "In Proc. of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2005}, {"title": "Oracle attacks and covert channels", "author": ["I. VENTURINI"], "venue": "In Proc. of International Workshop on Digital Watermarking (2005),", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2005}, {"title": "Practical evasion of a learningbased classifier: A case study", "author": ["N. \u0160RNDI\u0106", "P. LASKOV"], "venue": "In Proc. of IEEE Symposium on Security and Privacy", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2014}, {"title": "Anagram: A content anomaly detector resistant to mimicry attack", "author": ["K. WANG", "J.J. PAREKH", "S.J. STOLFO"], "venue": "In Proc. of International Symposium on Recent Advances in Intrusion Detection (RAID)", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2006}, {"title": "Lessons from the BOWS contest", "author": ["A. WESTFELD"], "venue": "In Workshop on Multimedia and Security (MM&Sec)", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2006}, {"title": "A workbench for the BOWS contest", "author": ["A. WESTFELD"], "venue": "EURASIP Journal on Information Security 2007,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2008}, {"title": "Fast determination of sensitivity in the presence of countermeasures in BOWS-2", "author": ["A. WESTFELD"], "venue": "In International Workshop on Information Hiding. Springer,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2009}, {"title": "Better security levels for broken arrows", "author": ["F. XIE", "T. FURON", "C. FONTAINE"], "venue": "Proceedings of SPIE", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2010}, {"title": "Automatically evading classifiers: A case study on pdf malware classifiers", "author": ["XU W", "QI Y", "EVANS D"], "venue": "In Proc. of Network and Distributed System Security Symposium (NDSS)", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2016}], "referenceMentions": [{"referenceID": 24, "context": "This problem has motivated the research field of adversarial machine learning which is concerned with the theory and practice of learning in an adversarial environment [27, 35, 42].", "startOffset": 168, "endOffset": 180}, {"referenceID": 32, "context": "This problem has motivated the research field of adversarial machine learning which is concerned with the theory and practice of learning in an adversarial environment [27, 35, 42].", "startOffset": 168, "endOffset": 180}, {"referenceID": 39, "context": "This problem has motivated the research field of adversarial machine learning which is concerned with the theory and practice of learning in an adversarial environment [27, 35, 42].", "startOffset": 168, "endOffset": 180}, {"referenceID": 7, "context": "As part of this research, several attacks and defenses have been proposed, for example, for poisoning support vector machines [9, 10], crafting adversarial samples against neural networks [40, 41, 43] or stealing models from decision trees [56].", "startOffset": 126, "endOffset": 133}, {"referenceID": 8, "context": "As part of this research, several attacks and defenses have been proposed, for example, for poisoning support vector machines [9, 10], crafting adversarial samples against neural networks [40, 41, 43] or stealing models from decision trees [56].", "startOffset": 126, "endOffset": 133}, {"referenceID": 37, "context": "As part of this research, several attacks and defenses have been proposed, for example, for poisoning support vector machines [9, 10], crafting adversarial samples against neural networks [40, 41, 43] or stealing models from decision trees [56].", "startOffset": 188, "endOffset": 200}, {"referenceID": 38, "context": "As part of this research, several attacks and defenses have been proposed, for example, for poisoning support vector machines [9, 10], crafting adversarial samples against neural networks [40, 41, 43] or stealing models from decision trees [56].", "startOffset": 188, "endOffset": 200}, {"referenceID": 40, "context": "As part of this research, several attacks and defenses have been proposed, for example, for poisoning support vector machines [9, 10], crafting adversarial samples against neural networks [40, 41, 43] or stealing models from decision trees [56].", "startOffset": 188, "endOffset": 200}, {"referenceID": 53, "context": "As part of this research, several attacks and defenses have been proposed, for example, for poisoning support vector machines [9, 10], crafting adversarial samples against neural networks [40, 41, 43] or stealing models from decision trees [56].", "startOffset": 240, "endOffset": 244}, {"referenceID": 13, "context": "Concurrently to adversarial machine learning, a different line of research has faced very similar problems: In digital watermarking information is embedded in a signal, such as an image, in the presence of an adversary [15, 46].", "startOffset": 219, "endOffset": 227}, {"referenceID": 43, "context": "Concurrently to adversarial machine learning, a different line of research has faced very similar problems: In digital watermarking information is embedded in a signal, such as an image, in the presence of an adversary [15, 46].", "startOffset": 219, "endOffset": 227}, {"referenceID": 37, "context": "[40, 41].", "startOffset": 0, "endOffset": 8}, {"referenceID": 38, "context": "[40, 41].", "startOffset": 0, "endOffset": 8}, {"referenceID": 59, "context": "By contrast, the right plot shows an oracle attack against a watermarking method, similar to the attacks developed by Westfeld [62] and Cox & Linnartz [14].", "startOffset": 127, "endOffset": 131}, {"referenceID": 12, "context": "By contrast, the right plot shows an oracle attack against a watermarking method, similar to the attacks developed by Westfeld [62] and Cox & Linnartz [14].", "startOffset": 151, "endOffset": 155}, {"referenceID": 53, "context": "First, we show that stateful defenses from digital watermarking can effectively mitigate model-extraction attacks against decision trees [56].", "startOffset": 137, "endOffset": 141}, {"referenceID": 4, "context": "Second, we show that techniques for hardening machine learning with classifier diversity [6] can be successfully applied to block oracle attacks against watermarks.", "startOffset": 89, "endOffset": 92}, {"referenceID": 39, "context": "These attacks can be roughly categorized into three classes: poisoning, evasion and model extraction [42].", "startOffset": 101, "endOffset": 105}, {"referenceID": 33, "context": "For example, in the case of spam filtering, the adversary may omit words from spam emails indicative for unsolicited content [36].", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "A common variant of this attack type are mimicry attacks, in which the adversary mimics characteristics of a particular class to hinder a correct prediction [19, 52].", "startOffset": 157, "endOffset": 165}, {"referenceID": 49, "context": "A common variant of this attack type are mimicry attacks, in which the adversary mimics characteristics of a particular class to hinder a correct prediction [19, 52].", "startOffset": 157, "endOffset": 165}, {"referenceID": 18, "context": "Evasion and mimicry attacks have been successfully applied against different learning-based systems, for example in network intrusion detection [20, 52], malware detection [25, 53, 65] and face recognition [50].", "startOffset": 144, "endOffset": 152}, {"referenceID": 49, "context": "Evasion and mimicry attacks have been successfully applied against different learning-based systems, for example in network intrusion detection [20, 52], malware detection [25, 53, 65] and face recognition [50].", "startOffset": 144, "endOffset": 152}, {"referenceID": 22, "context": "Evasion and mimicry attacks have been successfully applied against different learning-based systems, for example in network intrusion detection [20, 52], malware detection [25, 53, 65] and face recognition [50].", "startOffset": 172, "endOffset": 184}, {"referenceID": 50, "context": "Evasion and mimicry attacks have been successfully applied against different learning-based systems, for example in network intrusion detection [20, 52], malware detection [25, 53, 65] and face recognition [50].", "startOffset": 172, "endOffset": 184}, {"referenceID": 62, "context": "Evasion and mimicry attacks have been successfully applied against different learning-based systems, for example in network intrusion detection [20, 52], malware detection [25, 53, 65] and face recognition [50].", "startOffset": 172, "endOffset": 184}, {"referenceID": 47, "context": "Evasion and mimicry attacks have been successfully applied against different learning-based systems, for example in network intrusion detection [20, 52], malware detection [25, 53, 65] and face recognition [50].", "startOffset": 206, "endOffset": 210}, {"referenceID": 32, "context": "In the black-box setting, no information about the learning method and its training data are available and the adversary needs to guide her attack along the predicted classes of the classifier [35, 40, 60].", "startOffset": 193, "endOffset": 205}, {"referenceID": 37, "context": "In the black-box setting, no information about the learning method and its training data are available and the adversary needs to guide her attack along the predicted classes of the classifier [35, 40, 60].", "startOffset": 193, "endOffset": 205}, {"referenceID": 57, "context": "In the black-box setting, no information about the learning method and its training data are available and the adversary needs to guide her attack along the predicted classes of the classifier [35, 40, 60].", "startOffset": 193, "endOffset": 205}, {"referenceID": 5, "context": "With increasing knowledge of the method and data, the probability of a successful evasion rises [7].", "startOffset": 96, "endOffset": 99}, {"referenceID": 32, "context": "In this attack setting, the adversary actively probes a learning method and analyzes the returned output to reconstruct the underlying learning model [35].", "startOffset": 150, "endOffset": 154}, {"referenceID": 53, "context": "[56] enable reconstructing learning models from different publicly available machine learning services in black-box as well as white-box settings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "the reconstructed model [51].", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Similar to machine learning, watermarking methods need to account for the presence of an adversary and withstand different forms of attacks [14, 21].", "startOffset": 140, "endOffset": 148}, {"referenceID": 12, "context": "In this attack scenario, the adversary has access to a watermark detector that can be used to check whether a given media sample contains a watermark or not [14].", "startOffset": 157, "endOffset": 161}, {"referenceID": 9, "context": "In the second setting, the adversary also has access to a watermark detector, yet her goal is not only to remove the watermark from a target medium but to estimate its pattern [11, 38].", "startOffset": 176, "endOffset": 184}, {"referenceID": 35, "context": "In the second setting, the adversary also has access to a watermark detector, yet her goal is not only to remove the watermark from a target medium but to estimate its pattern [11, 38].", "startOffset": 176, "endOffset": 184}, {"referenceID": 1, "context": "It is evident from the previous section that attacks against learning and watermarking methods share similarities\u2014 an observation that has surprisingly been overlooked by the two research communities [2].", "startOffset": 200, "endOffset": 203}, {"referenceID": 15, "context": "Note that feature spaces in machine learning can also be constructed implicitly, for example using non-linear maps and kernel functions [17, 48].", "startOffset": 136, "endOffset": 144}, {"referenceID": 45, "context": "Note that feature spaces in machine learning can also be constructed implicitly, for example using non-linear maps and kernel functions [17, 48].", "startOffset": 136, "endOffset": 144}, {"referenceID": 13, "context": "Note that advanced watermarking schemes often map the signal to other spaces, such as frequency or random subspace domains [15, 21].", "startOffset": 123, "endOffset": 131}, {"referenceID": 19, "context": "Similar to machine learning, the function fw may induce a linear as well as non-linear boundary, such as a polynomial [22] or fractalized boundary [37].", "startOffset": 118, "endOffset": 122}, {"referenceID": 34, "context": "Similar to machine learning, the function fw may induce a linear as well as non-linear boundary, such as a polynomial [22] or fractalized boundary [37].", "startOffset": 147, "endOffset": 151}, {"referenceID": 5, "context": "For example, the adversary can perform a gradient descent in the direction of the decision boundary to determine the features that have the most effect on the classification [7, 41].", "startOffset": 174, "endOffset": 181}, {"referenceID": 38, "context": "For example, the adversary can perform a gradient descent in the direction of the decision boundary to determine the features that have the most effect on the classification [7, 41].", "startOffset": 174, "endOffset": 181}, {"referenceID": 32, "context": "Still, an adversary can perform a line search through the binary responses to locate the boundary\u2019s position [35].", "startOffset": 109, "endOffset": 113}, {"referenceID": 36, "context": "the original model [39, 40].", "startOffset": 19, "endOffset": 27}, {"referenceID": 37, "context": "the original model [39, 40].", "startOffset": 19, "endOffset": 27}, {"referenceID": 5, "context": "Instead, the modified sample also needs to be located inside the distribution of the target class [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 12, "context": "As for evasion, the adversary can perform an attack based on gradient descent to remove the watermark x\u0303 from the image with as little changes as possible [14].", "startOffset": 155, "endOffset": 159}, {"referenceID": 10, "context": "Take, for instance, the state-of-the-art Blind Newton Sensitivity Attack [12] that solves the optimization problem from Eq.", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "The optimal solution is guaranteed for convex boundaries, but suitable results are also reported for non-linear watermarking schemes by following the boundary\u2019s envelope [12, 13].", "startOffset": 170, "endOffset": 178}, {"referenceID": 11, "context": "The optimal solution is guaranteed for convex boundaries, but suitable results are also reported for non-linear watermarking schemes by following the boundary\u2019s envelope [12, 13].", "startOffset": 170, "endOffset": 178}, {"referenceID": 53, "context": "[56] have recently demonstrated this threat by stealing models from cloud platforms providing machine learning as a service.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "In the first case, an attacker collects a number of input-output pairs with queries either scattered over the feature space or created adaptively [39, 40, 56].", "startOffset": 146, "endOffset": 158}, {"referenceID": 37, "context": "In the first case, an attacker collects a number of input-output pairs with queries either scattered over the feature space or created adaptively [39, 40, 56].", "startOffset": 146, "endOffset": 158}, {"referenceID": 53, "context": "In the first case, an attacker collects a number of input-output pairs with queries either scattered over the feature space or created adaptively [39, 40, 56].", "startOffset": 146, "endOffset": 158}, {"referenceID": 32, "context": "For example, an adversary can reconstruct a linear classifier by performing a line search in each feature direction from a fixed position [35].", "startOffset": 138, "endOffset": 142}, {"referenceID": 53, "context": "The extraction against nonlinear classifiers such as decision trees also exploits localized boundary points for reconstruction [56].", "startOffset": 127, "endOffset": 131}, {"referenceID": 6, "context": "Randomization Random Subspace Method [8] Random Subspace Method [18, 57] Randomized Ensemble [8, 31] Randomized Boundary [21, 34] \u2014 Union of Watermarks [21]", "startOffset": 37, "endOffset": 40}, {"referenceID": 16, "context": "Randomization Random Subspace Method [8] Random Subspace Method [18, 57] Randomized Ensemble [8, 31] Randomized Boundary [21, 34] \u2014 Union of Watermarks [21]", "startOffset": 64, "endOffset": 72}, {"referenceID": 54, "context": "Randomization Random Subspace Method [8] Random Subspace Method [18, 57] Randomized Ensemble [8, 31] Randomized Boundary [21, 34] \u2014 Union of Watermarks [21]", "startOffset": 64, "endOffset": 72}, {"referenceID": 6, "context": "Randomization Random Subspace Method [8] Random Subspace Method [18, 57] Randomized Ensemble [8, 31] Randomized Boundary [21, 34] \u2014 Union of Watermarks [21]", "startOffset": 93, "endOffset": 100}, {"referenceID": 28, "context": "Randomization Random Subspace Method [8] Random Subspace Method [18, 57] Randomized Ensemble [8, 31] Randomized Boundary [21, 34] \u2014 Union of Watermarks [21]", "startOffset": 93, "endOffset": 100}, {"referenceID": 31, "context": "Randomization Random Subspace Method [8] Random Subspace Method [18, 57] Randomized Ensemble [8, 31] Randomized Boundary [21, 34] \u2014 Union of Watermarks [21]", "startOffset": 121, "endOffset": 129}, {"referenceID": 4, "context": "Complex Boundary Non-Linearity [6, 47] Non-Linearity [21\u201323, 37] Classifier Diversity [6] \u2014 \u2014 Snake Traps [21]", "startOffset": 31, "endOffset": 38}, {"referenceID": 44, "context": "Complex Boundary Non-Linearity [6, 47] Non-Linearity [21\u201323, 37] Classifier Diversity [6] \u2014 \u2014 Snake Traps [21]", "startOffset": 31, "endOffset": 38}, {"referenceID": 19, "context": "Complex Boundary Non-Linearity [6, 47] Non-Linearity [21\u201323, 37] Classifier Diversity [6] \u2014 \u2014 Snake Traps [21]", "startOffset": 53, "endOffset": 64}, {"referenceID": 20, "context": "Complex Boundary Non-Linearity [6, 47] Non-Linearity [21\u201323, 37] Classifier Diversity [6] \u2014 \u2014 Snake Traps [21]", "startOffset": 53, "endOffset": 64}, {"referenceID": 34, "context": "Complex Boundary Non-Linearity [6, 47] Non-Linearity [21\u201323, 37] Classifier Diversity [6] \u2014 \u2014 Snake Traps [21]", "startOffset": 53, "endOffset": 64}, {"referenceID": 4, "context": "Complex Boundary Non-Linearity [6, 47] Non-Linearity [21\u201323, 37] Classifier Diversity [6] \u2014 \u2014 Snake Traps [21]", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "Stateful Analysis \u2014 Security Margin [1, 55] \u2014 Line Search Detection [1] \u2014 Locality-Sensitive Hashing [58]", "startOffset": 36, "endOffset": 43}, {"referenceID": 52, "context": "Stateful Analysis \u2014 Security Margin [1, 55] \u2014 Line Search Detection [1] \u2014 Locality-Sensitive Hashing [58]", "startOffset": 36, "endOffset": 43}, {"referenceID": 0, "context": "Stateful Analysis \u2014 Security Margin [1, 55] \u2014 Line Search Detection [1] \u2014 Locality-Sensitive Hashing [58]", "startOffset": 68, "endOffset": 71}, {"referenceID": 55, "context": "Stateful Analysis \u2014 Security Margin [1, 55] \u2014 Line Search Detection [1] \u2014 Locality-Sensitive Hashing [58]", "startOffset": 101, "endOffset": 105}, {"referenceID": 9, "context": "Similar to the model extraction case, estimation attacks in the watermarking literature are based on localizing boundary points where the signal just crosses the detector\u2019s decision boundary [11, 37].", "startOffset": 191, "endOffset": 199}, {"referenceID": 34, "context": "Similar to the model extraction case, estimation attacks in the watermarking literature are based on localizing boundary points where the signal just crosses the detector\u2019s decision boundary [11, 37].", "startOffset": 191, "endOffset": 199}, {"referenceID": 9, "context": "Choubassi and Moulin present a variant of this estimation attack to find boundary points that reduce the effort of the subsequent watermark estimation [11].", "startOffset": 151, "endOffset": 155}, {"referenceID": 32, "context": "This approach already comes very close to the work of Lowd and Meek [35] from adversarial machine learning.", "startOffset": 68, "endOffset": 72}, {"referenceID": 6, "context": "In machine learning, randomized ensemble learning has been proposed for implementing this defense strategy [8, 44].", "startOffset": 107, "endOffset": 114}, {"referenceID": 41, "context": "In machine learning, randomized ensemble learning has been proposed for implementing this defense strategy [8, 44].", "startOffset": 107, "endOffset": 114}, {"referenceID": 6, "context": "As a consequence, the adversary has to attack different classifiers at the same time [8].", "startOffset": 85, "endOffset": 88}, {"referenceID": 57, "context": "Alternatively, the features selected to train each classifier can be randomized, such that an adversary cannot be sure whether a specific feature has an influence on the returned classifier output [60].", "startOffset": 197, "endOffset": 201}, {"referenceID": 31, "context": "In particular, a detector can be hardened by creating a randomized region around the decision boundary where the detector returns arbitrary outputs [21, 34].", "startOffset": 148, "endOffset": 156}, {"referenceID": 9, "context": "This misleads the inherent line search in attacks that localize the boundary in this way [11, 12].", "startOffset": 89, "endOffset": 97}, {"referenceID": 10, "context": "This misleads the inherent line search in attacks that localize the boundary in this way [11, 12].", "startOffset": 89, "endOffset": 97}, {"referenceID": 16, "context": "in machine learning, several works in the field of watermarking propose to randomly divide the image pixels into subsets and aggregate the classifier output from each subset [18, 57].", "startOffset": 174, "endOffset": 182}, {"referenceID": 54, "context": "in machine learning, several works in the field of watermarking propose to randomly divide the image pixels into subsets and aggregate the classifier output from each subset [18, 57].", "startOffset": 174, "endOffset": 182}, {"referenceID": 44, "context": "implement this defense strategy using non-linear kernel functions [47], while Biggio et al.", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "realize a tighter and more complex boundary through the combination of two-class and one-class models [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 19, "context": "Similarly, the watermarking community has examined non-linear boundaries to defend against oracle and watermark-estimation attacks [22, 23, 37].", "startOffset": 131, "endOffset": 143}, {"referenceID": 20, "context": "Similarly, the watermarking community has examined non-linear boundaries to defend against oracle and watermark-estimation attacks [22, 23, 37].", "startOffset": 131, "endOffset": 143}, {"referenceID": 34, "context": "Similarly, the watermarking community has examined non-linear boundaries to defend against oracle and watermark-estimation attacks [22, 23, 37].", "startOffset": 131, "endOffset": 143}, {"referenceID": 14, "context": "In addition, Furon and Bas have introduced small indents called snake traps at the decision boundary in order to stop attacks based on random walks along the detection region [16, 21].", "startOffset": 175, "endOffset": 183}, {"referenceID": 10, "context": "For example, boundaries based on fractals and snake traps block some of the attacks presented in Section 3, yet approximations of the decision boundary are still possible and might be used for successful attacks [12].", "startOffset": 212, "endOffset": 216}, {"referenceID": 0, "context": "While this concept has not yet been examined in adversarial machine learning, stateful analysis of queries has been successfully applied in digital watermarking for detecting oracle and watermark-estimation attacks [1, 55, 58].", "startOffset": 215, "endOffset": 226}, {"referenceID": 52, "context": "While this concept has not yet been examined in adversarial machine learning, stateful analysis of queries has been successfully applied in digital watermarking for detecting oracle and watermark-estimation attacks [1, 55, 58].", "startOffset": 215, "endOffset": 226}, {"referenceID": 55, "context": "While this concept has not yet been examined in adversarial machine learning, stateful analysis of queries has been successfully applied in digital watermarking for detecting oracle and watermark-estimation attacks [1, 55, 58].", "startOffset": 215, "endOffset": 226}, {"referenceID": 4, "context": "[6] for securing machine learning to a watermark detector.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "In our first case study, we consider a recent defense from machine learning that increases the complexity of the decision boundary by combining a two-class and oneclass model [6].", "startOffset": 175, "endOffset": 178}, {"referenceID": 60, "context": "Broadly speaking, \u201dthe image does not have to look nice\u201c in an attack [63].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "Our dataset for this evaluation consists of images from the publicly available Dresden Image Database [24], where 50 uncompressed Adobe Lightroom images from a Nikon D70 camera are used.", "startOffset": 102, "endOffset": 106}, {"referenceID": 2, "context": "5-class detector using the well-studied Blind Newton Sensitivity Attack [3, 12, 13] that successfully defeats several existing defenses (see Appendix B).", "startOffset": 72, "endOffset": 83}, {"referenceID": 10, "context": "5-class detector using the well-studied Blind Newton Sensitivity Attack [3, 12, 13] that successfully defeats several existing defenses (see Appendix B).", "startOffset": 72, "endOffset": 83}, {"referenceID": 11, "context": "5-class detector using the well-studied Blind Newton Sensitivity Attack [3, 12, 13] that successfully defeats several existing defenses (see Appendix B).", "startOffset": 72, "endOffset": 83}, {"referenceID": 53, "context": "[56] for a decision tree and then develop a stateful classifier as an effective countermeasure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[56] reconstruct decision trees by performing targeted queries on the APIs provided by the BigML service.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "As a countermeasure to this attack, we devise a defense that builds on a successful protection technique from digital watermarking\u2014a stateful detector [1, 55].", "startOffset": 151, "endOffset": 158}, {"referenceID": 52, "context": "As a countermeasure to this attack, we devise a defense that builds on a successful protection technique from digital watermarking\u2014a stateful detector [1, 55].", "startOffset": 151, "endOffset": 158}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The exact parameters of the security margin are derived through statistical properties of the decision function [1].", "startOffset": 112, "endOffset": 115}, {"referenceID": 53, "context": "[56].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "such as a line search detection as proposed for watermarking [1], can lower the chances of reconstructing a model in reasonable time.", "startOffset": 61, "endOffset": 64}, {"referenceID": 36, "context": "Although recent research in machine learning has started to also study black-box attacks more thoroughly [39, 40, 56], existing insights from digital watermarking potentially bring forth novel ideas.", "startOffset": 105, "endOffset": 117}, {"referenceID": 37, "context": "Although recent research in machine learning has started to also study black-box attacks more thoroughly [39, 40, 56], existing insights from digital watermarking potentially bring forth novel ideas.", "startOffset": 105, "endOffset": 117}, {"referenceID": 53, "context": "Although recent research in machine learning has started to also study black-box attacks more thoroughly [39, 40, 56], existing insights from digital watermarking potentially bring forth novel ideas.", "startOffset": 105, "endOffset": 117}, {"referenceID": 53, "context": "On the other side, the various strategies, such as adaptive re-learning [56], that have been used successfully against classifiers mark a potential threat for watermark detectors as well.", "startOffset": 72, "endOffset": 76}, {"referenceID": 56, "context": "\u0160rndi\u0107 and Laskov [59], for instance, demonstrate the feasibility to evade a publicly available PDF malware classifier with the insight that full knowledge of the classifier features is not necessary.", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "The identified similarities between both research fields can be seen as part of a bigger problem: Adversarial Signal Processing [2].", "startOffset": 128, "endOffset": 131}], "year": 2017, "abstractText": "Machine learning is increasingly used in security-critical applications, such as autonomous driving, face recognition and malware detection. Most learning methods, however, have not been designed with security in mind and thus are vulnerable to different types of attacks. This problem has motivated the research field of adversarial machine learning that is concerned with attacking and defending learning methods. Concurrently, a different line of research has tackled a very similar problem: In digital watermarking information are embedded in a signal in the presence of an adversary. As a consequence, this research field has also extensively studied techniques for attacking and defending watermarking methods. The two research communities have worked in parallel so far, unnoticeably developing similar attack and defense strategies. This paper is a first effort to bring these communities together. To this end, we present a unified notation of black-box attacks against machine learning and watermarking that reveals the similarity of both settings. To demonstrate the efficacy of this unified view, we apply concepts from watermarking to machine learning and vice versa. We show that countermeasures from watermarking can mitigate recent model-extraction attacks and, similarly, that techniques for hardening machine learning can fend off oracle attacks against watermarks. Our work provides a conceptual link between two research fields and thereby opens novel directions for improving the security of both, machine learning and digital watermarking.", "creator": "LaTeX with hyperref package"}}}