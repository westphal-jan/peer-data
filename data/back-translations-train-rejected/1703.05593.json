{"id": "1703.05593", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Convolutional neural network architecture for geometric matching", "abstract": "We address the problem of determining correspondences between two images in agreement with a geometric model such as an affine or thin-plate-spline transformation, and estimating its parameters. The contributions of this work are three-fold. First, we propose a convolutional neural network architecture for geometric matching. The architecture is based on three main components that mimic the standard steps of feature extraction, matching and simultaneous inlier detection and model parameter estimation, while being trainable end-to-end. Second, we demonstrate that the network parameters can be trained from synthetically generated imagery without the need for manual annotation and that our matching layer significantly increases generalization capabilities to never seen before images. Finally, we show that the same model can perform both instance-level and category-level matching giving state-of-the-art results on the challenging Proposal Flow dataset.", "histories": [["v1", "Thu, 16 Mar 2017 13:03:54 GMT  (4307kb,D)", "http://arxiv.org/abs/1703.05593v1", "To appear in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)"], ["v2", "Thu, 13 Apr 2017 22:32:43 GMT  (9428kb,D)", "http://arxiv.org/abs/1703.05593v2", "In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)"]], "COMMENTS": "To appear in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["ignacio rocco", "relja arandjelovi\\'c", "josef sivic"], "accepted": false, "id": "1703.05593"}, "pdf": {"name": "1703.05593.pdf", "metadata": {"source": "CRF", "title": "Convolutional neural network architecture for geometric matching", "authors": ["Ignacio Rocco", "Relja Arandjelovi\u0107", "Josef Sivic"], "emails": ["josef.sivic}@inria.fr"], "sections": [{"heading": "1. Introduction", "text": "Estimating correspondences between images is one of the fundamental problems in computer vision [15, 21] with applications ranging from large-scale 3D reconstruction [1] to image manipulation [17] and semantic segmentation [45]. Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affinity transformation are calculated by detecting and matching local characteristics (such as SIFT [33] or HOG [9, 18], followed by truncating false conformities using local geometric constraints [37, 40] and a robust estimate of global geometric transformations using algorithms such as RANSAC [14] or Hough transform [28, 29, 33]. This approach works well in many cases, but fails in situations where (i) large changes in the appearance represented by e.g. intra-class variation [18] or Hough transform [28, 29, 33] are not required or large changes in iouts."}, {"heading": "2. Related work", "text": "The classical approach to finding matches involves identifying points of interest and calculating local descriptors around these points [7, 8, 20, 32, 33, 34, 37]. One limitation of the classical approach is that while it works relatively well, the feature detectors and descriptors lack the generalization capability for categorical linkage. Recently, however, conventional neural networks have been used to learn powerful feature descriptors that are more resistant to appearance changes than the classical descriptors [6, 19, 24, 38, 44]. However, this work still divides the image into a number of local patches and uses the local patches for descriptor calculation, which are then compared with an appropriate distance measurement by directly identifying a similarity value [19, 44] or even by directly establishing a binary agreement with us - and not bringing about agreed decisions."}, {"heading": "3. Architecture for geometric matching", "text": "In this section, we present a new architecture of the Convolutionary Neural Network (CNN) for estimating the geometric transformation parameters between two input images; the architecture is designed to mimic the classic computer vision pipeline (e.g. [35]), while using differentiable modules so that it can be trained end-to-end for the geometry estimation task; the classical approach consists of the fol-colored stages: (i) local descriptors (e.g. SIFT) are extracted from both input images; (ii) the descriptors are matched across images to form a series of preliminary matches, which are then used to (iii) extract the parameters of the geometric model using RANSAC or Hough Voting.Our architecture, illustrated in Fig. 2, imitates this process by: (i) the passing of input images IA and Iii) the transmission of input images A by means of a continuous architecture, followed by a continuous parameters A, and IAB by three."}, {"heading": "3.1. Feature extraction", "text": "The first stage of the pipeline is feature extraction, for which we use a standard CNN architecture. A CNN without fully connected layers takes an input image and generates a feature map f-Rh-w-d, which can be interpreted as an h-w dense spatial grid of d-dimensional local descriptors. A similar interpretation was used earlier in retrieval work [3, 4, 5, 16] demonstrating a high discriminatory power of CNN-based descriptors. Therefore, for feature extraction we use the VGG-16 network [39], which was truncated at the Pool4 level (in front of the ReLU unit), followed by an L2 normalization per feature. As shown in Fig. 2, the feature extraction network is duplicated and arranged in a Siamese configuration so that the two input images are routed through two identical networks with common parameters."}, {"heading": "3.2. Matching network", "text": "This year it is as far as it has ever been until the next round."}, {"heading": "3.3. Regression network", "text": "The normalized correlation map is guided through a regression network that directly estimates the geometric transformation parameters relative to the two input images. In classical geometry estimation, this step consists in the robust estimation of the transformation from the list of preliminary correspondences. However, local geometric constraints are often used to further trim the list of preliminary correspondences [37, 40] by maintaining only similarities consistent with other similarities in its spatial neighborhood. Final geometry estimation is done by RANSAC [14] or by rough coordination [28, 29, 33]. We imitate the classical approach using a neural network in which we stack two blocks of revolutionary layers, followed by batch normalization [22] and ReLU nonlinearity, and add a final fully connected layer that relates to the transformation parameters as represented in the image."}, {"heading": "3.4. Hierarchy of transformations", "text": "Another commonly used approach to estimating image-to-image transformations is to first estimate a simple transformation and then gradually increase the complexity of the model, refining the estimates along the way [8, 32, 35]. The motivation behind this method is that estimating a very complex transformation could be difficult and mathematically inefficient, so a robust and fast rough estimate of a simpler transformation can be used as a starting point, which also governs the subsequent estimation of the more complex transformation.We follow the same good practice and start with estimating an affine transformation, a linear transformation with a degree of freedom of 6, capable of modelling transformation, rotation, non-isotropic scaling and shear. The estimated affine transformation is then used to align image B on image A, using an image resampling layer [23]. The aligned images are then guided through a second network of estimation 18, the final geometry of the transformation is primed."}, {"heading": "4. Training framework", "text": "In order to train the parameters of our geometric matching CNN, it is necessary to design the corresponding loss function and use appropriate training data. Next, we will discuss these two important points."}, {"heading": "4.1. Loss function", "text": "We assume a fully monitored environment in which the training data consists of image pairs and the desired results in the form of the parameters \u03b8GT of geometric transformation between soil and true.The loss function L is designed in such a way that it can be used for estimating the estimated transformation parameters, the loss function through the use of backpropagation and stochastic gradient descent.It is desirable that the loss is general and not specific to a particular type of geometric model, so that it can be used for estimating the mesh parameters, the loss function through the use of backpropagation and stochastic gradient descent.It is desirable that the loss is general and not specific to a particular type of geometric model, so that it can be used for estimating the affine, homography, thin plate spline or any other geometric transformation. Furthermore, the loss should be independent of the parameterization of the transformation and not operate directly on the parameter values themselves."}, {"heading": "4.2. Training from synthetic transformations", "text": "Training CNNs usually require a lot of data, and there is no public dataset that contains many pairs of images commented on with their geometric transformation. Therefore, we opt for training from synthetically generated data, which gives us the flexibility to collect as many training examples as necessary for any 2-D geometric transformation of interest. We create each example by scanning image A from a public dataset and generating image B by applying a random transformation T\u03b8GT to image A. Strictly speaking, image A is generated from the central section of the original image, while image B is generated by transforming the original image with additional symmetrical padding to avoid marginal artifacts; the procedure is shown in fig. 6."}, {"heading": "5. Experimental results", "text": "In this section, we provide details of training and evaluation data sets and implementation details, perform quantitative and qualitative assessments of our methodology and compare them with baselines and the state of the art. In addition, we provide further insights into the components of our architecture."}, {"heading": "5.1. Evaluation dataset and performance measure", "text": "The quantitative evaluation of our method is based on the proposal flow data set by Ham et al. [18]. The data set contains 900 pairs of images representing different instances of the same class, such as ducks and cars, but with large variations within the class, e.g. the cars are often of different fabrication, or the ducks may be of different subspecies. In addition, the images contain significant background confusion, as in Fig. 8. The task is to predict the positions of predefined key points from Fig. A in Fig. B by estimating a geometric transformation that distorts Fig. A in Fig. B, and apply the same transformation to the key point positions. We follow the standard evaluation metric used for this benchmark, i.e. the average probability of a correct key point (PCK) [43], which is the percentage of key points that correspond correctly."}, {"heading": "5.2. Training dataset", "text": "Two different training data sets are generated by sampling images from two publicly available image sets: the StreetView synth is generated from the training set of the Tokyo Time Machine [3], which contains Google Street View images of Tokyo, and the Pascal synth from the training set of Pascal VOC 2011 [12] images. Each synthetically generated data set contains 40k images, divided into 20k for training and 20k for validation. The parameters for the ground transformation were sampled independently of reasonable ranges, e.g. for affine transformation, we take the relative scale change of up to 2x, while for thin plate spline we randomly jitter a 3x3 grid of control points, translating each point independently by up to a quarter of the image size in all directions.To demonstrate the generalization capability of our approach, we use StreetView synth for all experiments, as the Street View images are very similar to Pascal images in nature to the flow."}, {"heading": "5.3. Implementation details", "text": "We use the MatConvNet [41] library and train the networks with stochastic gradient descent, with learning rate 10 \u2212 3, impulse 0.9, without weight decay and batch size 16. Instead of data augmentation, we can simply generate more synthetic training data. Input images are reduced to 227 \u00d7 227, generating 15 x 15 feature cards that are transferred into the appropriate layer. Networks are trained to convergence, which typically occurs after 10 epochs and takes 12 hours on a single GPU. Our final affine transformation estimation method uses a combination of two networks that independently spin back the parameters, which are then averaged to create the final affine estimation.The two networks were trained on different areas of affine transformations. As in Figure 5, the estimated affine transformation is used to warp image A and pass it along with image B to a single network that estimates the thin plate transformation."}, {"heading": "5.4. Comparisons to state-of-the-art", "text": "We compare our method with SIFT Flow [30], Graph matching cores (GMK) [11], Deformable spatial pyramid matching (DSP) [26], DeepFlow [36] and all three variants of Proposal Flow (NAM, PHM, LOM) [18]. As shown in Tab. 1, our method outperforms all others and sets the new state of the art by 1%. The best competing methods are based on Proposal Flow and use object suggestions that allow them to guide the allocation to regions of the images containing objects. Their performance is significantly different from the choice of object suggestion method, which demonstrates the importance of this guided allocation. On the contrary, our method does not use guidelines, but nevertheless manages to exceed even the best Proposal Flow and object suggestion combinations. Furthermore, we compare affine transformations calculated using RANSAC, which suggest the same descriptors as our VG (4%) by the threshold of the most wide."}, {"heading": "5.5. Discussions and ablation studies", "text": "In this section, we examine the importance of the various components of our architecture. Apart from training on the Streetview synthesis, which we also train on the Pascal synthesis, which contains images similar to those in the Benchmark Proposal Flow. Results of these ablation studies are summarized in Tab. 2. The correlation with the linkage of information on the one hand and subtraction on the other hand is clear."}, {"heading": "5.6. Qualitative results", "text": "Fig. 8 illustrates the effectiveness of our method in the area of category-level matching, where quite sophisticated pairs of images from the proposal-flow dataset [18] containing large class-internal variations are correctly aligned. Fig. 9 shows the quality of instance-level matching, in which different images of the same scene are correctly aligned. Images are taken from the data set of the Tokyo Time Machine [3] and are taken at different times that are months or years apart. Note that by automatically highlighting the differences (in the attribute space) between the aligned images, it is possible to detect changes in the scene, such as coincidences, changes in vegetation, structural differences, e.g. in new buildings."}, {"heading": "6. Conclusions", "text": "We have described a network architecture for geometric matching that is fully trainable from synthetic images without manual annotation. Thanks to our matching layer, the network generalizes well to never-before-seen images, achieving state-of-the-art results in the challenging proposal flow dataset for cross-category matching, opening the possibility to apply our architecture to other difficult correspondence problems such as matching across presentation styles.This work has been supported in part by the European Research Council (ERC Fellowship LEAPNr. 336845), the Agence Nationale de la Recherche (Semapolis Project, ANR-13-CORD-0003), the Inria CityLab IPL, the CIFAR Learning in Machines & Brains Programme and ESIF, OP Research, Development and Education Project IMPACT No. CZ.02.1.0 / 0.0 / 15 003 / 0000468."}], "references": [{"title": "Building rome in a day", "author": ["S. Agarwal", "N. Snavely", "I. Simon", "S.M. Seitz", "R. Szeliski"], "venue": "In Proc. ICCV,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Learning to match aerial images with deep attentive architectures", "author": ["H. Altwaijry", "E. Trulls", "J. Hays", "P. Fua", "S. Belongie"], "venue": "In Proc. CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "NetVLAD: CNN architecture for weakly supervised place recognition", "author": ["R. Arandjelovi\u0107", "P. Gronat", "A. Torii", "T. Pajdla", "J. Sivic"], "venue": "In Proc. CVPR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Factors of transferability from a generic ConvNet representation", "author": ["H. Azizpour", "A. Razavian", "J. Sullivan", "A. Maki", "S. Carlsson"], "venue": "CoRR, abs/1406.5774,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Aggregating local deep features for image retrieval", "author": ["A. Babenko", "V. Lempitsky"], "venue": "In Proc. ICCV,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Pn-net: Conjoined triple deep network for learning local image descriptors", "author": ["V. Balntas", "E. Johns", "L. Tang", "K. Mikolajczyk"], "venue": "arXiv preprint arXiv:1601.05030,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Surf: Speeded up robust features", "author": ["H. Bay", "T. Tuytelaars", "L. Van Gool"], "venue": "In Proc. ECCV,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Shape matching and object recognition using low distortion correspondence", "author": ["A. Berg", "T. Berg", "J. Malik"], "venue": "In Proc. CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Histogram of Oriented Gradients for Human Detection", "author": ["N. Dalal", "B. Triggs"], "venue": "In Proc. CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Deep image homography estimation", "author": ["D. DeTone", "T. Malisiewicz", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1606.03798,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "A graph-matching kernel for object categorization", "author": ["O. Duchenne", "A. Joulin", "J. Ponce"], "venue": "In Proc. ICCV,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "The PASCAL Visual Object Classes Challenge 2011", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["P. Fischer", "A. Dosovitskiy", "E. Ilg", "P. H\u00e4usser", "C. Haz\u0131rba\u015f", "V. Golkov", "P. van der Smagt", "D. Cremers", "T. Brox"], "venue": "In Proc. ICCV,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography", "author": ["M.A. Fischler", "R.C. Bolles"], "venue": "Comm. ACM,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1981}, {"title": "Computer vision: a modern approach", "author": ["D.A. Forsyth", "J. Ponce"], "venue": "Prentice Hall Professional Technical Reference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "In Proc. ECCV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Non-rigid dense correspondence with applications for  image enhancement", "author": ["Y. HaCohen", "E. Shechtman", "D.B. Goldman", "D. Lischinski"], "venue": "Proc. ACM SIGGRAPH,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Proposal flow", "author": ["B. Ham", "M. Cho", "C. Schmid", "J. Ponce"], "venue": "In Proc. CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Matchnet: Unifying feature and metric learning for patchbased matching", "author": ["X. Han", "T. Leung", "Y. Jia", "R. Sukthankar", "A.C. Berg"], "venue": "In Proc. CVPR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "A combined corner and edge detector", "author": ["C. Harris", "M. Stephens"], "venue": "In Alvey vision conference,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1988}, {"title": "Multiple view geometry in computer vision", "author": ["R. Hartley", "A. Zisserman"], "venue": "Cambridge university press,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman", "K. Kavukcuoglu"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Learned local descriptors for recognition and matching", "author": ["M. Jahrer", "M. Grabner", "H. Bischof"], "venue": "In Computer Vision Winter Workshop,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Warpnet: Weakly supervised matching for single-view reconstruction", "author": ["A. Kanazawa", "D.W. Jacobs", "M. Chandraker"], "venue": "In Proc. CVPR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Deformable spatial pyramid matching for fast dense correspondences", "author": ["J. Kim", "C. Liu", "F. Sha", "K. Grauman"], "venue": "In Proc. CVPR,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Object recognition by affine invariant matching", "author": ["Y. Lamdan", "J.T. Schwartz", "H.J. Wolfson"], "venue": "In Proc. CVPR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1988}, {"title": "Robust object detection with interleaved categorization and segmentation", "author": ["B. Leibe", "A. Leonardis", "B. Schiele"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Sift flow: Dense correspondence across scenes and its applications", "author": ["C. Liu", "J. Yuen", "A. Torralba"], "venue": "IEEE PAMI,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Do convnets learn correspondence", "author": ["J.L. Long", "N. Zhang", "T. Darrell"], "venue": "In NIPS,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Object recognition from local scale-invariant features", "author": ["D.G. Lowe"], "venue": "In Proc. ICCV,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1999}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}, {"title": "An affine invariant interest point detector", "author": ["K. Mikolajczyk", "C. Schmid"], "venue": "In Proc. ECCV,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2002}, {"title": "Object retrieval with large vocabularies and fast spatial matching", "author": ["J. Philbin", "O. Chum", "M. Isard", "J. Sivic", "A. Zisserman"], "venue": "In Proc. CVPR,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Deepmatching: Hierarchical deformable dense matching", "author": ["J. Revaud", "P. Weinzaepfel", "Z. Harchaoui", "C. Schmid"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Local grayvalue invariants for image retrieval", "author": ["C. Schmid", "R. Mohr"], "venue": "IEEE PAMI,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1997}, {"title": "Discriminative learning of deep convolutional feature point descriptors", "author": ["E. Simo-Serra", "E. Trulls", "L. Ferraz", "I. Kokkinos", "P. Fua", "F. Moreno-Noguer"], "venue": "In Proc. ICCV,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In Proc. ICLR,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Video Google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "In Proc. ICCV,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2003}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "In Proc. ACMM,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Deepflow: Large displacement optical flow with deep matching", "author": ["P. Weinzaepfel", "J. Revaud", "Z. Harchaoui", "C. Schmid"], "venue": "In Proc. ICCV,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "Articulated human detection with flexible mixtures of parts", "author": ["Y. Yang", "D. Ramanan"], "venue": "IEEE PAMI,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Learning to compare image patches via convolutional neural networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "In Proc. CVPR,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Flowweb: Joint image set alignment by weaving consistent, pixel-wise correspondences", "author": ["T. Zhou", "Y.J. Lee", "S.X. Yu", "A.A. Efros"], "venue": "In Proc. CVPR,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "Estimating correspondences between images is one of the fundamental problems in computer vision[15, 21] with applications ranging from large-scale 3D reconstruction [1] to image manipulation [17] and semantic segmentation [45].", "startOffset": 95, "endOffset": 103}, {"referenceID": 20, "context": "Estimating correspondences between images is one of the fundamental problems in computer vision[15, 21] with applications ranging from large-scale 3D reconstruction [1] to image manipulation [17] and semantic segmentation [45].", "startOffset": 95, "endOffset": 103}, {"referenceID": 0, "context": "Estimating correspondences between images is one of the fundamental problems in computer vision[15, 21] with applications ranging from large-scale 3D reconstruction [1] to image manipulation [17] and semantic segmentation [45].", "startOffset": 165, "endOffset": 168}, {"referenceID": 16, "context": "Estimating correspondences between images is one of the fundamental problems in computer vision[15, 21] with applications ranging from large-scale 3D reconstruction [1] to image manipulation [17] and semantic segmentation [45].", "startOffset": 191, "endOffset": 195}, {"referenceID": 44, "context": "Estimating correspondences between images is one of the fundamental problems in computer vision[15, 21] with applications ranging from large-scale 3D reconstruction [1] to image manipulation [17] and semantic segmentation [45].", "startOffset": 222, "endOffset": 226}, {"referenceID": 32, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 192, "endOffset": 196}, {"referenceID": 8, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 204, "endOffset": 211}, {"referenceID": 17, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 204, "endOffset": 211}, {"referenceID": 36, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 286, "endOffset": 294}, {"referenceID": 39, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 286, "endOffset": 294}, {"referenceID": 13, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 384, "endOffset": 388}, {"referenceID": 27, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 408, "endOffset": 420}, {"referenceID": 28, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 408, "endOffset": 420}, {"referenceID": 32, "context": "Traditionally, correspondences consistent with a geometric model such as epipolar geometry or planar affine transformation, are computed by detecting and matching local features (such as SIFT [33] or HOG [9, 18]), followed by pruning incorrect matches using local geometric constraints [37, 40] and robust estimation of global geometric transformation using algorithms such as RANSAC [14] or Hough transform [28, 29, 33].", "startOffset": 408, "endOffset": 420}, {"referenceID": 17, "context": "intra-class variation [18], or (ii) large changes of scene layout or non-rigid deformations that require complex geometric models with", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "First, we replace the standard local features with powerful trainable convolutional neural network features [27, 39], which allows us to cope with large changes of appearance between the matched images.", "startOffset": 108, "endOffset": 116}, {"referenceID": 38, "context": "First, we replace the standard local features with powerful trainable convolutional neural network features [27, 39], which allows us to cope with large changes of appearance between the matched images.", "startOffset": 108, "endOffset": 116}, {"referenceID": 32, "context": "Second, we develop trainable matching and transformation estimation layers that can cope with noisy and incorrect matches in a robust way, mimicking the good practices in feature matching such as the second nearest neighbor test [33], neighborhood consensus [37, 40] and Hough transform-like estimation [28, 29, 33].", "startOffset": 229, "endOffset": 233}, {"referenceID": 36, "context": "Second, we develop trainable matching and transformation estimation layers that can cope with noisy and incorrect matches in a robust way, mimicking the good practices in feature matching such as the second nearest neighbor test [33], neighborhood consensus [37, 40] and Hough transform-like estimation [28, 29, 33].", "startOffset": 258, "endOffset": 266}, {"referenceID": 39, "context": "Second, we develop trainable matching and transformation estimation layers that can cope with noisy and incorrect matches in a robust way, mimicking the good practices in feature matching such as the second nearest neighbor test [33], neighborhood consensus [37, 40] and Hough transform-like estimation [28, 29, 33].", "startOffset": 258, "endOffset": 266}, {"referenceID": 27, "context": "Second, we develop trainable matching and transformation estimation layers that can cope with noisy and incorrect matches in a robust way, mimicking the good practices in feature matching such as the second nearest neighbor test [33], neighborhood consensus [37, 40] and Hough transform-like estimation [28, 29, 33].", "startOffset": 303, "endOffset": 315}, {"referenceID": 28, "context": "Second, we develop trainable matching and transformation estimation layers that can cope with noisy and incorrect matches in a robust way, mimicking the good practices in feature matching such as the second nearest neighbor test [33], neighborhood consensus [37, 40] and Hough transform-like estimation [28, 29, 33].", "startOffset": 303, "endOffset": 315}, {"referenceID": 32, "context": "Second, we develop trainable matching and transformation estimation layers that can cope with noisy and incorrect matches in a robust way, mimicking the good practices in feature matching such as the second nearest neighbor test [33], neighborhood consensus [37, 40] and Hough transform-like estimation [28, 29, 33].", "startOffset": 303, "endOffset": 315}, {"referenceID": 6, "context": "The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [7, 8, 20, 32, 33, 34, 37].", "startOffset": 140, "endOffset": 166}, {"referenceID": 7, "context": "The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [7, 8, 20, 32, 33, 34, 37].", "startOffset": 140, "endOffset": 166}, {"referenceID": 19, "context": "The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [7, 8, 20, 32, 33, 34, 37].", "startOffset": 140, "endOffset": 166}, {"referenceID": 31, "context": "The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [7, 8, 20, 32, 33, 34, 37].", "startOffset": 140, "endOffset": 166}, {"referenceID": 32, "context": "The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [7, 8, 20, 32, 33, 34, 37].", "startOffset": 140, "endOffset": 166}, {"referenceID": 33, "context": "The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [7, 8, 20, 32, 33, 34, 37].", "startOffset": 140, "endOffset": 166}, {"referenceID": 36, "context": "The classical approach for finding correspondences involves identifying interest points and computing local descriptors around these points [7, 8, 20, 32, 33, 34, 37].", "startOffset": 140, "endOffset": 166}, {"referenceID": 5, "context": "Recently, convolutional neural networks have been used to learn powerful feature descriptors which are more robust to appearance changes than the classical descriptors [6, 19, 24, 38, 44].", "startOffset": 168, "endOffset": 187}, {"referenceID": 18, "context": "Recently, convolutional neural networks have been used to learn powerful feature descriptors which are more robust to appearance changes than the classical descriptors [6, 19, 24, 38, 44].", "startOffset": 168, "endOffset": 187}, {"referenceID": 23, "context": "Recently, convolutional neural networks have been used to learn powerful feature descriptors which are more robust to appearance changes than the classical descriptors [6, 19, 24, 38, 44].", "startOffset": 168, "endOffset": 187}, {"referenceID": 37, "context": "Recently, convolutional neural networks have been used to learn powerful feature descriptors which are more robust to appearance changes than the classical descriptors [6, 19, 24, 38, 44].", "startOffset": 168, "endOffset": 187}, {"referenceID": 43, "context": "Recently, convolutional neural networks have been used to learn powerful feature descriptors which are more robust to appearance changes than the classical descriptors [6, 19, 24, 38, 44].", "startOffset": 168, "endOffset": 187}, {"referenceID": 5, "context": "These are then compared with an appropriate distance measure [6, 24, 38], by directly outputting a similarity score [19, 44], or even by directly outputting a binary matching/non-matching decision [2].", "startOffset": 61, "endOffset": 72}, {"referenceID": 23, "context": "These are then compared with an appropriate distance measure [6, 24, 38], by directly outputting a similarity score [19, 44], or even by directly outputting a binary matching/non-matching decision [2].", "startOffset": 61, "endOffset": 72}, {"referenceID": 37, "context": "These are then compared with an appropriate distance measure [6, 24, 38], by directly outputting a similarity score [19, 44], or even by directly outputting a binary matching/non-matching decision [2].", "startOffset": 61, "endOffset": 72}, {"referenceID": 18, "context": "These are then compared with an appropriate distance measure [6, 24, 38], by directly outputting a similarity score [19, 44], or even by directly outputting a binary matching/non-matching decision [2].", "startOffset": 116, "endOffset": 124}, {"referenceID": 43, "context": "These are then compared with an appropriate distance measure [6, 24, 38], by directly outputting a similarity score [19, 44], or even by directly outputting a binary matching/non-matching decision [2].", "startOffset": 116, "endOffset": 124}, {"referenceID": 1, "context": "These are then compared with an appropriate distance measure [6, 24, 38], by directly outputting a similarity score [19, 44], or even by directly outputting a binary matching/non-matching decision [2].", "startOffset": 197, "endOffset": 200}, {"referenceID": 12, "context": "Related to us are also network architectures for estimating inter-frame motion in video [13] or instance-level homography estimation [10], however their goal is very different from ours targeting high-precision correspondence with very limited appearance variation and background clutter.", "startOffset": 88, "endOffset": 92}, {"referenceID": 9, "context": "Related to us are also network architectures for estimating inter-frame motion in video [13] or instance-level homography estimation [10], however their goal is very different from ours targeting high-precision correspondence with very limited appearance variation and background clutter.", "startOffset": 133, "endOffset": 137}, {"referenceID": 24, "context": "Closer to us is the network architecture of [25] who, however, target a different problem of fine-grained category-level matching (different species of birds) with limited background clutter and small translations and scale changes, as their objects are centered in the image.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "Some works, such as [18, 30, 31], have addressed the hard problem of category-level matching, but resort to the traditional non-trainable frameworks such as RANSAC, Hough voting or estimation of a dense deformation field, and guide the matching using object proposals [18].", "startOffset": 20, "endOffset": 32}, {"referenceID": 29, "context": "Some works, such as [18, 30, 31], have addressed the hard problem of category-level matching, but resort to the traditional non-trainable frameworks such as RANSAC, Hough voting or estimation of a dense deformation field, and guide the matching using object proposals [18].", "startOffset": 20, "endOffset": 32}, {"referenceID": 30, "context": "Some works, such as [18, 30, 31], have addressed the hard problem of category-level matching, but resort to the traditional non-trainable frameworks such as RANSAC, Hough voting or estimation of a dense deformation field, and guide the matching using object proposals [18].", "startOffset": 20, "endOffset": 32}, {"referenceID": 17, "context": "Some works, such as [18, 30, 31], have addressed the hard problem of category-level matching, but resort to the traditional non-trainable frameworks such as RANSAC, Hough voting or estimation of a dense deformation field, and guide the matching using object proposals [18].", "startOffset": 268, "endOffset": 272}, {"referenceID": 34, "context": "[35]), while using differentiable modules so that it trainable end-to-end for the geometry estimation task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "A similar interpretation has been used previously in instance retrieval works [3, 4, 5, 16] demonstrating high discriminative power of CNN-based descriptors.", "startOffset": 78, "endOffset": 91}, {"referenceID": 3, "context": "A similar interpretation has been used previously in instance retrieval works [3, 4, 5, 16] demonstrating high discriminative power of CNN-based descriptors.", "startOffset": 78, "endOffset": 91}, {"referenceID": 4, "context": "A similar interpretation has been used previously in instance retrieval works [3, 4, 5, 16] demonstrating high discriminative power of CNN-based descriptors.", "startOffset": 78, "endOffset": 91}, {"referenceID": 15, "context": "A similar interpretation has been used previously in instance retrieval works [3, 4, 5, 16] demonstrating high discriminative power of CNN-based descriptors.", "startOffset": 78, "endOffset": 91}, {"referenceID": 38, "context": "Thus, for feature extraction we use the VGG-16 network [39], cropped at the pool4 layer (before the ReLU unit), followed by per-feature L2-normalization.", "startOffset": 55, "endOffset": 59}, {"referenceID": 32, "context": "Furthermore, the second nearest neighbor test [33] prunes the matches further by requiring that the match strength is significantly stronger than the second best match involving the same descriptor, which is very effective at discarding ambiguous matches.", "startOffset": 46, "endOffset": 50}, {"referenceID": 32, "context": "Secondly, in the case of the descriptor fA matching multiple features in fB due to the existence of clutter or repetitive patterns, the matching scores will be downweighted similarly to the second nearest neighbor test [33].", "startOffset": 219, "endOffset": 223}, {"referenceID": 41, "context": "The first step of our matching layer, namely the correlation layer, is somewhat similar to layers used in DeepMatching [42] and FlowNet [13].", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "The first step of our matching layer, namely the correlation layer, is somewhat similar to layers used in DeepMatching [42] and FlowNet [13].", "startOffset": 136, "endOffset": 140}, {"referenceID": 41, "context": "However, DeepMatching [42] only uses deep RGB patches and no part of their architecture is trainable.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "FlowNet [13] uses a spatially constrained correlation layer such that similarities are are only computed in a restricted spatial neighborhood thus limiting the range of geometric transformations that can be captured.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "Previous works have used other matching layers to combine the descriptors across images, namely simple concatenation of descriptors or images themselves along the channel dimension [10] or subtraction [25].", "startOffset": 181, "endOffset": 185}, {"referenceID": 24, "context": "Previous works have used other matching layers to combine the descriptors across images, namely simple concatenation of descriptors or images themselves along the channel dimension [10] or subtraction [25].", "startOffset": 201, "endOffset": 205}, {"referenceID": 36, "context": "Local geometric constraints are often used to further prune the list of tentative matches [37, 40] by only retaining matches which are consistent with other matches in its spatial neighborhood.", "startOffset": 90, "endOffset": 98}, {"referenceID": 39, "context": "Local geometric constraints are often used to further prune the list of tentative matches [37, 40] by only retaining matches which are consistent with other matches in its spatial neighborhood.", "startOffset": 90, "endOffset": 98}, {"referenceID": 13, "context": "Final geometry estimation is done by RANSAC [14] or Hough voting [28, 29, 33].", "startOffset": 44, "endOffset": 48}, {"referenceID": 27, "context": "Final geometry estimation is done by RANSAC [14] or Hough voting [28, 29, 33].", "startOffset": 65, "endOffset": 77}, {"referenceID": 28, "context": "Final geometry estimation is done by RANSAC [14] or Hough voting [28, 29, 33].", "startOffset": 65, "endOffset": 77}, {"referenceID": 32, "context": "Final geometry estimation is done by RANSAC [14] or Hough voting [28, 29, 33].", "startOffset": 65, "endOffset": 77}, {"referenceID": 21, "context": "We again mimic the classical approach using a neural network, where we stack two blocks of convolutional layers, followed by batch-normalization [22] and the ReLU non-linearity, and add a final fully connected layer which regresses to the parameters of the transformation, as shown in Fig.", "startOffset": 145, "endOffset": 149}, {"referenceID": 36, "context": "The first convolutional layers can also enforce local neighborhood consensus [37, 40] by learning filters which only fire if nearby descriptors in image A are matched to nearby descriptors in image B, and we show qualitative evidence in Sec.", "startOffset": 77, "endOffset": 85}, {"referenceID": 39, "context": "The first convolutional layers can also enforce local neighborhood consensus [37, 40] by learning filters which only fire if nearby descriptors in image A are matched to nearby descriptors in image B, and we show qualitative evidence in Sec.", "startOffset": 77, "endOffset": 85}, {"referenceID": 12, "context": "equation (1)), and not just the local neighborhood as in [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 7, "context": "Another commonly used approach when estimating image to image transformations is to start by estimating a simple transformation and then progressively increase the model complexity, refining the estimates along the way [8, 32, 35].", "startOffset": 219, "endOffset": 230}, {"referenceID": 31, "context": "Another commonly used approach when estimating image to image transformations is to start by estimating a simple transformation and then progressively increase the model complexity, refining the estimates along the way [8, 32, 35].", "startOffset": 219, "endOffset": 230}, {"referenceID": 34, "context": "Another commonly used approach when estimating image to image transformations is to start by estimating a simple transformation and then progressively increase the model complexity, refining the estimates along the way [8, 32, 35].", "startOffset": 219, "endOffset": 230}, {"referenceID": 22, "context": "The estimated affine transformation is then used to align image B to image A using an image resampling layer [23].", "startOffset": 109, "endOffset": 113}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "the average probability of correct keypoint (PCK) [43], being the proportion of keypoints that are correctly matched.", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "Two different training datasets are generated by sampling images from two publicly available image datasets: StreetView-synth is created from the training set of the Tokyo Time Machine [3] dataset which contains Google Street View images of Tokyo, and Pascal-synth is created from the training set of Pascal VOC 2011 [12] images.", "startOffset": 185, "endOffset": 188}, {"referenceID": 11, "context": "Two different training datasets are generated by sampling images from two publicly available image datasets: StreetView-synth is created from the training set of the Tokyo Time Machine [3] dataset which contains Google Street View images of Tokyo, and Pascal-synth is created from the training set of Pascal VOC 2011 [12] images.", "startOffset": 317, "endOffset": 321}, {"referenceID": 40, "context": "We use the MatConvNet [41] library and train the networks with stochastic gradient descent, with learning rate 10\u22123, momentum 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 29, "context": "We compare our method against SIFT Flow [30], Graphmatching kernels (GMK) [11], Deformable spatial pyramid Methods PCK (%)", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "We compare our method against SIFT Flow [30], Graphmatching kernels (GMK) [11], Deformable spatial pyramid Methods PCK (%)", "startOffset": 74, "endOffset": 78}, {"referenceID": 35, "context": "DeepFlow [36] 20 GMK [11] 27 SIFT Flow [30] 38 DSP [26] 29 Proposal Flow NAM [18] 53 Proposal Flow PHM [18] 55 Proposal Flow LOM [18] 56 RANSAC with our features (affine) 47 Ours (affine) 49 Ours (affine + thin plate spline) 56 Ours (affine ensemble + thin plate spline) 57", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "DeepFlow [36] 20 GMK [11] 27 SIFT Flow [30] 38 DSP [26] 29 Proposal Flow NAM [18] 53 Proposal Flow PHM [18] 55 Proposal Flow LOM [18] 56 RANSAC with our features (affine) 47 Ours (affine) 49 Ours (affine + thin plate spline) 56 Ours (affine ensemble + thin plate spline) 57", "startOffset": 21, "endOffset": 25}, {"referenceID": 29, "context": "DeepFlow [36] 20 GMK [11] 27 SIFT Flow [30] 38 DSP [26] 29 Proposal Flow NAM [18] 53 Proposal Flow PHM [18] 55 Proposal Flow LOM [18] 56 RANSAC with our features (affine) 47 Ours (affine) 49 Ours (affine + thin plate spline) 56 Ours (affine ensemble + thin plate spline) 57", "startOffset": 39, "endOffset": 43}, {"referenceID": 25, "context": "DeepFlow [36] 20 GMK [11] 27 SIFT Flow [30] 38 DSP [26] 29 Proposal Flow NAM [18] 53 Proposal Flow PHM [18] 55 Proposal Flow LOM [18] 56 RANSAC with our features (affine) 47 Ours (affine) 49 Ours (affine + thin plate spline) 56 Ours (affine ensemble + thin plate spline) 57", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "DeepFlow [36] 20 GMK [11] 27 SIFT Flow [30] 38 DSP [26] 29 Proposal Flow NAM [18] 53 Proposal Flow PHM [18] 55 Proposal Flow LOM [18] 56 RANSAC with our features (affine) 47 Ours (affine) 49 Ours (affine + thin plate spline) 56 Ours (affine ensemble + thin plate spline) 57", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "DeepFlow [36] 20 GMK [11] 27 SIFT Flow [30] 38 DSP [26] 29 Proposal Flow NAM [18] 53 Proposal Flow PHM [18] 55 Proposal Flow LOM [18] 56 RANSAC with our features (affine) 47 Ours (affine) 49 Ours (affine + thin plate spline) 56 Ours (affine ensemble + thin plate spline) 57", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "DeepFlow [36] 20 GMK [11] 27 SIFT Flow [30] 38 DSP [26] 29 Proposal Flow NAM [18] 53 Proposal Flow PHM [18] 55 Proposal Flow LOM [18] 56 RANSAC with our features (affine) 47 Ours (affine) 49 Ours (affine + thin plate spline) 56 Ours (affine ensemble + thin plate spline) 57", "startOffset": 129, "endOffset": 133}, {"referenceID": 17, "context": "All the numbers apart from ours and RANSAC are taken from [18].", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "matching (DSP) [26], DeepFlow [36], and all three variants of Proposal Flow (NAM, PHM, LOM) [18].", "startOffset": 15, "endOffset": 19}, {"referenceID": 35, "context": "matching (DSP) [26], DeepFlow [36], and all three variants of Proposal Flow (NAM, PHM, LOM) [18].", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "matching (DSP) [26], DeepFlow [36], and all three variants of Proposal Flow (NAM, PHM, LOM) [18].", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "Replacing our correlation-based matching layer with feature concatenation or subtraction, as proposed in [10] and [25], respectively, incurs a large performance drop.", "startOffset": 105, "endOffset": 109}, {"referenceID": 24, "context": "Replacing our correlation-based matching layer with feature concatenation or subtraction, as proposed in [10] and [25], respectively, incurs a large performance drop.", "startOffset": 114, "endOffset": 118}, {"referenceID": 9, "context": "Concatenation [10] 26 29 Subtraction [25] 18 21 Ours without normalization 44 \u2013 Ours 49 45", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "Concatenation [10] 26 29 Subtraction [25] 18 21 Ours without normalization 44 \u2013 Ours 49 45", "startOffset": 37, "endOffset": 41}, {"referenceID": 9, "context": "Furthermore, it can be seen that the performance of feature concatenation [10] and subtraction [25] is, apart from being significantly worse than ours, much more affected by the choice of the training set.", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "Furthermore, it can be seen that the performance of feature concatenation [10] and subtraction [25] is, apart from being significantly worse than ours, much more affected by the choice of the training set.", "startOffset": 95, "endOffset": 99}, {"referenceID": 32, "context": "The step mimics the second nearest neighbor test used in classical feature matching [33], as discussed in Sec.", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "Note that [13] also uses a correlation layer, but they do not normalize the map in any way, which is clearly suboptimal.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "8 illustrates the effectiveness of our method in category-level matching, where quite challenging pairs of images from the Proposal Flow dataset [18], containing large intra-class variations, are aligned correctly.", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "The images are taken from the Tokyo Time Machine dataset [3] and are captured at different points in time which are months or years apart.", "startOffset": 57, "endOffset": 60}], "year": 2017, "abstractText": "We address the problem of determining correspondences between two images in agreement with a geometric model such as an affine or thin-plate-spline transformation, and estimating its parameters. The contributions of this work are three-fold. First, we propose a convolutional neural network architecture for geometric matching. The architecture is based on three main components that mimic the standard steps of feature extraction, matching and simultaneous inlier detection and model parameter estimation, while being trainable end-to-end. Second, we demonstrate that the network parameters can be trained from synthetically generated imagery without the need for manual annotation and that our matching layer significantly increases generalization capabilities to never seen before images. Finally, we show that the same model can perform both instance-level and category-level matching giving state-of-the-art results on the challenging Proposal Flow dataset.", "creator": "LaTeX with hyperref package"}}}