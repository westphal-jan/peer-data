{"id": "1703.05706", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Improving Document Clustering by Eliminating Unnatural Language", "abstract": "Technical documents contain a fair amount of unnatural language, such as tables, formulas, pseudo-codes, etc. Unnatural language can be an important factor of confusing existing NLP tools. This paper presents an effective method of distinguishing unnatural language from natural language, and evaluates the impact of unnatural language detection on NLP tasks such as document clustering. We view this problem as an information extraction task and build a multiclass classification model identifying unnatural language components into four categories. First, we create a new annotated corpus by collecting slides and papers in various formats, PPT, PDF, and HTML, where unnatural language components are annotated into four categories. We then explore features available from plain text to build a statistical model that can handle any format as long as it is converted into plain text. Our experiments show that removing unnatural language components gives an absolute improvement in document clustering up to 15%. Our corpus and tool are publicly available.", "histories": [["v1", "Thu, 16 Mar 2017 16:32:06 GMT  (1134kb,D)", "https://arxiv.org/abs/1703.05706v1", null], ["v2", "Fri, 17 Mar 2017 04:03:36 GMT  (1134kb,D)", "http://arxiv.org/abs/1703.05706v2", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["myungha jang", "jinho d choi", "james allan"], "accepted": false, "id": "1703.05706"}, "pdf": {"name": "1703.05706.pdf", "metadata": {"source": "CRF", "title": "Improving Document Clustering by Eliminating Unnatural Language", "authors": ["Myungha Jang", "Jinho D. Choi", "James Allan"], "emails": ["mhjang@cs.umass.edu,", "jinho.choi@emory.edu,", "allan@cs.umass.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Table Extraction", "text": "Various efforts have been made to extract tables by means of semi-supervised learning about the patterns of table layouts within ASCII text documents (Ng et al., 1999) (Pinto et al., 2003; Lerman et al., 2001; Zanibbi et al., 2004). Existing techniques use graphical features such as primitive geometric shapes, symbols, and lines to recognize table boundaries. (Khusro et al., 2015) introduce the most advanced table extraction techniques from PDF articles and compare them. However, there does not seem to be any work that has attempted to process plaintext extracted from richer formats where table layouts are intact."}, {"heading": "2.2 Formula Extraction", "text": "Lin et al. (2011) categorized existing approaches to the recognition of mathematical formulas by \"character-based\" and \"layout-based\" in relation to key features. (Chan and Yeung, 2000) provide a comprehensive overview of mathematical formula extraction using various layout functions available from image-based documents. Since we do not have access to layout information, character-based approaches are more relevant to our work, using features of mathematical symbols, operators and positions and their character size (Suzuki et al., 2003; Kacem et al., 2001)."}, {"heading": "2.3 Code Extraction", "text": "Tuarob et al. (2013) proposed three methods for extracting pseudo-code: a rules-based method, machine learning, and a combined method. Their rules-based approach detects the presence of pseudo-code labels using keyword matching. The machine learning approach recognizes a box surrounding a sparse region, and checks whether the box is pseudo-code or not."}, {"heading": "3 Problem Definition", "text": "Input to our task is simple text extracted from PDF or PPT documents. The goal is to assign a class name to each line of this plaintext and identify it as a natural language (regular text) or as one of the four types of non-natural language block components, tables, codes, formulas or other text. In this work, we focus on these four specific types because our observations lead us to believe that they are the most common components in PPT lecture slides and PPT articles. Illustrations are also a very common component, but we do not consider them because they are often images or drawings and cannot be easily extracted into text. In this section, we briefly discuss the characteristics of each component and the challenges of identifying them from the raw text."}, {"heading": "3.1 Table", "text": "Tables are predominant in almost all areas of technical documents. Tables are usually mediated by their two-dimensional layout and column and / or row headers (Khusro et al., 2015). Figure 2 shows a table in an original PDF document and the same table that contains the text extracted from Apache Tika1. Ta-1https: / / tika.apache.org / bles often merges multiple cells for the layout, making it particularly difficult to distinguish them as a table once they are converted into flat text."}, {"heading": "3.2 Mathematical Formula", "text": "Mathematical formulas exist in two ways: isolated formulas in their own lines, or as formulas embedded within a line of text. In this work, we treat both types as a formula component. Since not all mathematical symbols can be matched with Unicode characters, and because the extraction software may not convert them correctly, the extracted text tends to contain strangely formatted or even completely wrong characters. Superscripts and subscriptions are no longer distinguishable, and the original visual layout (e.g. multi-line mathematical symbols, such as \"and\") is lost."}, {"heading": "3.3 Code", "text": "Similar to mathematical formulas, they exist both in isolation and embedded, although most code components are isolated code blocks. As with formula components, we treat both types of code blocks as code components. We assume that even indentations, one of the strong visual references to the code, are not preserved in the extracted text, even though an extraction tool stores them, not to limit ourselves to the detailed performance of text extraction tools."}, {"heading": "3.4 Miscellaneous Non-text (Misc.)", "text": "In addition to the above-mentioned components, there are other types of non-natural language blocks that remain when converted to text and may produce incorrect sub-topic matches between documents. To make this possible, we call these components other text. An example of other text is the text and caption that are part of the diagrams on slides. Figure 3 shows an example of other text that has lost its structure and meaning while being converted to text without the original diagram."}, {"heading": "4 Corpus", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data Collection", "text": "We collected 1,561 slides from various computer science and electrical engineering courses available online, and 5,898 scientific papers from several years of ACL / EMNLP archive2. We divided the data set for various purposes: training the classification model, training Word embedding models for feature extraction, and clustering for extrinsic evaluation. The details of the data set we used are summarized in Table 1. We provide the data for download at http: / / people.cs.umass. edu / ~ mhjang / publications.html.For classification, we have compiled three sets of data from two different data sources: (1) Lecture slides (2) ACL papers, (3) which combine the two. We chose these two types of data sources because they have different ratios of unnatural language components and therefore complement each other for coverage. 2 Table shows the ratio of the four% of each data set, for example, 1.4% of each component in LTS2."}, {"heading": "4.2 Text Extraction", "text": "We extracted plain text from our records using an open source software package, Apache Tika. The package is available for text extraction in various formats such as PDF, PPT and HTML."}, {"heading": "4.3 Annotation", "text": "To train a statistical model, we need basic data. We created annotation guidelines for the four types of non-natural language components and commented on 35 slides of lectures (7,943 lines) and 35 ACL essays (25,686 lines). We developed a simple annotation tool to support the task and also to ensure that annotators comply with certain rules.3 For this task, we hired four annotators with knowledge of computer science."}, {"heading": "5 Features", "text": "We find that line-based predictions have more advantages over token-based predictions because they allow us to observe the syntactic structure of the line, how statistically common the grammar structure is, and how layout patterns are compared with adjacent lines. We in-3The guidelines and tool are available at http: / / people.cs.umass.edu / ~ mhjang / publication.html. We list five sets of features that are used to train our classifier and discuss the impact of each feature on accuracy."}, {"heading": "5.1 N-gram (N)", "text": "Unigrams and bigrams of each line are included as attributes."}, {"heading": "5.2 Parsing Features (P)", "text": "If we tried to analyze the unnatural language line, the resulting parsing tree would form an unusual syntactic structure. To grasp this insight, we analyzed each line with the dependency parser in ClearNLP (Choi and McCallum, 2013) and extracted features such as the set of dependency labels, the ratio of each POS tag and the POS tag of each dependent head pair from each parse tree."}, {"heading": "5.3 Table String Layout (T)", "text": "Text extracted from tables loses its visual layout as a table, but still retains implicit layout through its string patterns. Tables tend to transmit the same type of data along the same column or row. For example, if a column in a table prints numbers, it is more likely that number markers are contained in parallel at the same position on the rows of the table. Therefore, a row block is more likely to be a table if they share the same pattern. We encode each row by replacing each character either as S (string) or N (numeral), and then calculate the editing distance between adjacent rows, weighted by language modelling the probability calculated from the table body (Eq.1, 2)."}, {"heading": "5.4 Word Embedding Feature (E)", "text": "We train word embedding using TWORD2V EC using WORD2VEC (Mikolov et al., 2013). The training corpus comprised 278,719 words. Since we make a line-based prediction, we need a vector that represents the line, not every word. We consider three ways of line embedding: (1) averaging the vector of words, (2) calculating the paragraph vector introduced in (Le and Mikolov, 2014), (3) using both."}, {"heading": "5.5 Sequential Feature (S)", "text": "The sequential nature of the lines is also an important feature, as the component most likely occurs over a block of contiguous lines. We train two models: the first model uses the annotation for the class of the previous line, and then we train another model using the predicted label of the previous line, which represents the output of the first model."}, {"heading": "6 Classification Experiments", "text": "We used the classifier Liblinear Support Vector Machine (SVM) (Chang and Lin, 2011) for training and performed 5-fold cross-validation for evaluation. To improve the robustness of structured predictions, we used a search algorithm known as DAGGER to SVM (Ross et al., 2010). First, we introduce two baselines to compare accuracy with our statistical model."}, {"heading": "6.1 Baselines", "text": "Since no existing work is directly applicable to our scenario, we consider two simple basic lines. \u2022 Weighted random components (W-random) These assign the class of random components to each row. Instead of a uniform random prediction, we made more educated guesses based on the ratio of the components known from the annotated data set (Table 2). \u2022 Component Language Modeling (CLM) Among the five language models of a five-component class (the four non-textual components and the text component) generated from the annotations, we predict the component for each line by assigning the component whose language model gives the line the highest probability."}, {"heading": "6.2 Classification Result", "text": "Notes within each dataset, however, TSLIDES and TACL are split for training and testing using a 5x cross-validation scheme. Table 3 reports an F1 score for predicting the four components in the two datasets using our method and baselines. Table 4: Multi-domain classification improves singledomain classification in Table 3. Identifying categories with particularly low accuracy in each dataset (TABLE and FORMULA in TSLIDES and CODE in TACL) is improved to be as good as the other category.The proposed method has dramatically increased the prediction accuracy for all components against baselines. CLM baseline showed the highest accuracy on CODE among the four categories in both datasets. Since pseudo-codes use TSUSPS more controlled vocabulary than MUSPS (e.g. reserved words, common variable names), language itself becomes a distinguishing feature."}, {"heading": "6.3 Feature Analysis", "text": "We started from individual characteristics and combined them incrementally to observe performance (Figure 5). Characteristics are greedily added in such a way that first a feature is added that provides the higher accuracy when used alone. First, we compare the three possibilities for calculating sentence vectors mentioned in Section 5 (Figure 4). If we experiment only with embedding text vectors, they produce, on average, 9-12 times better results than paragraph vectors. If both characteristics are used, there are some gains in CODE and MISC. Components and losses in TABLE and FORMULA. However, if we experiment with all other characteristics besides embedding features, losses are covered by the other characteristics that ultimately yield the highest performance through combined vectors.N-gram characteristics (N) were the strongest feature with 68% of the F1 score, when used alone. The next useful table features are the ABE, the Parsing-T, and the Parsing-T."}, {"heading": "7 Removal Effects of Unnatural Language on NLP tools", "text": "We observe how the removal of unnatural language from documents affects the performance of two NLP tools, document similarity and document clustering. For the experiments, we prepared a gold standard cluster for each dataset, CDSA and COS."}, {"heading": "7.1 Document Similarity", "text": "To test whether the calculated similarity better reflects the actual relevance of the topic after the unnatural language has been removed, we performed regression analyses. We converted the gold standard clustering into pairs of binary relevance. If two documents are in the same basic truth cluster, they are relevant and otherwise irrelevant. We then mounted a log-linear model in R to predict the binary relevance from the cosmic similarity of the document pairs. Regression models that fit in R are evaluated using AIC (Akaike, 1974). AIC is a measure of model selection that measures the relative quality of statistical models learned from the given data. If AIC is smaller, the quality of the fit is better, and the smaller the complexity of the model, the fewer parameters it has to represent the model. Table 5 shows that AIC topics have been reduced by 53 and the natural documents cannot become comparable with the non-similarity."}, {"heading": "7.2 Document Clustering", "text": "This year, \"he said,\" we have to push ourselves to the limits, \"he said.\" We have to push ourselves to the limits, \"he said.\" We have to push ourselves to the limits, \"he said.\" We have to push ourselves to the limits, \"he said.\" We have to push ourselves to the limits of our possibilities, \"he said.\" We have to push ourselves to the limits of the possibilities we have set ourselves. \""}, {"heading": "8 Conclusion", "text": "In this paper, we argued that unnatural language in technical documents should be distinguished from natural language in order for NLP tools to work effectively. We presented an approach to identifying four types of unnatural language blocks from normal text that is not dependent on a document format. The presented method extracts five sets of line-based text features and showed an 82% F1 score for the four categories of unnatural language. We showed how existing NLP tools can work better on documents if we remove unnatural language from documents. In particular, we demonstrated that removing unnatural language improves clustering in many areas by at best 15% and 11% without significantly affecting the original clustering in any environment."}, {"heading": "Acknowledgments", "text": "This work has been supported in part by the Center for Intelligent Information Retrieval, in part by the NSF Scholarship # IIS-0910884, and in part by the NSF Scholarship # IIS1217281. All opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the Founder. Authors thank Kenneth W. Church for providing valuable comments and advice."}], "references": [{"title": "Semi-supervised clustering by seeding", "author": ["Basu et al.2002] Sugato Basu", "Arindam Banerjee", "Raymond J. Mooney"], "venue": "In Proceedings of the Nineteenth International Conference on Machine Learning,", "citeRegEx": "Basu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Basu et al\\.", "year": 2002}, {"title": "Mathematical expression recognition: A survey", "author": ["Chan", "Yeung2000] Kam-Fai Chan", "Dit-Yan Yeung"], "venue": null, "citeRegEx": "Chan et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2000}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "Lin2011] Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Transition-based dependency parsing with selectional branching", "author": ["Choi", "McCallum2013] Jinho D. Choi", "Andrew McCallum"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Choi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2013}, {"title": "Looking beyond text: Extracting figures, tables and captions from computer science papers", "author": ["Clark", "Divvala2015] Christopher Clark", "Santosh Divvala"], "venue": "In AAAI Workshops", "citeRegEx": "Clark et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2015}, {"title": "Automatic extraction of printed mathematical formulas using fuzzy logic and propagation of context", "author": ["Kacem et al.2001] Afef Kacem", "Abdel Bela\u00efd", "Mohamed Ben Ahmed"], "venue": "IJDAR,", "citeRegEx": "Kacem et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kacem et al\\.", "year": 2001}, {"title": "On methods and tools of table detection, extraction and annotation in pdf documents", "author": ["Khusro et al.2015] Shah Khusro", "Asima Latif", "Irfan Ullah"], "venue": "J. Inf. Sci.,", "citeRegEx": "Khusro et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Khusro et al\\.", "year": 2015}, {"title": "A search engine for mathematical formulae", "author": ["Kohlhase", "Sucan2006] Michael Kohlhase", "Ioan Sucan"], "venue": "AISC, volume 4120 of Lecture Notes in Computer Science,", "citeRegEx": "Kohlhase et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kohlhase et al\\.", "year": 2006}, {"title": "Distributed representations of sentences and documents. CoRR, abs/1405.4053", "author": ["Le", "Mikolov2014] Quoc V. Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Automatic data extraction from lists and tables in web sources", "author": ["Craig Knoblock", "Steven Minton"], "venue": "Proceedings of the workshop on Advances in Text Extraction and Mining (IJCAI-2001),", "citeRegEx": "Lerman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lerman et al\\.", "year": 2001}, {"title": "Mathematical Formula Identification in PDF Documents", "author": ["Lin et al.2011] Xiaoyan Lin", "Liangcai Gao", "Zhi Tang", "Xiaofan Lin", "Xuan Hu"], "venue": "In International Conference on Document Analysis and Recognition,", "citeRegEx": "Lin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2011}, {"title": "TableSeer: automatic table metadata extraction and searching in digital libraries", "author": ["Liu et al.2007] Ying Liu", "Kun Bai", "Prasenjit Mitra", "C. Lee Giles"], "venue": "In Joint Conference on Digital Library,", "citeRegEx": "Liu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2007}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning to recognize tables in free text", "author": ["Ng et al.1999] Hwee Tou Ng", "Chung Yong Lim", "Jessica Li Teng Koo"], "venue": "In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics,", "citeRegEx": "Ng et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "The document spectrum for page layout analysis", "author": ["L. O\u2019Gorman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "O.Gorman.,? \\Q1993\\E", "shortCiteRegEx": "O.Gorman.", "year": 1993}, {"title": "Table extraction using conditional random fields", "author": ["Pinto et al.2003] David Pinto", "Andrew McCallum", "Xing Wei", "W. Bruce Croft"], "venue": "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval,", "citeRegEx": "Pinto et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pinto et al\\.", "year": 2003}, {"title": "No-regret reductions for imitation learning and structured prediction. CoRR, abs/1011.0686", "author": ["Ross et al.2010] St\u00e9phane Ross", "Geoffrey J. Gordon", "J. Andrew Bagnell"], "venue": null, "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "A fast algorithm", "author": ["Simon et al.1997] Anik\u00f3 Simon", "Jean-Christophe Pret", "A. Peter Johnson"], "venue": null, "citeRegEx": "Simon et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Simon et al\\.", "year": 1997}, {"title": "Infty- an integrated ocr system for mathematical documents", "author": ["Fumikazu Tamari", "Ryoji Fukuda", "Seiichi Uchida", "Toshihiro Kanahori"], "venue": "In Proceedings of ACM Symposium on Document Engineering", "citeRegEx": "Suzuki et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Suzuki et al\\.", "year": 2003}, {"title": "Automatic detection of pseudocodes in scholarly documents using machine learning", "author": ["Sumit Bhatia", "Prasenjit Mitra", "C. Lee Giles"], "venue": "In Proceedings of the 2013 12th International Conference on Document Analysis and", "citeRegEx": "Tuarob et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tuarob et al\\.", "year": 2013}, {"title": "A survey of table recognition: Models, observations, transformations, and inferences", "author": ["Dorothea Blostein", "R. Cordy"], "venue": "Int. J. Doc. Anal. Recognit.,", "citeRegEx": "Zanibbi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zanibbi et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 14, "context": "Document layout analysis aiming to identify document format by classifying blocks into text, figures, tables, and such has been a long-studied problem (O\u2019Gorman, 1993; Simon et al., 1997).", "startOffset": 151, "endOffset": 187}, {"referenceID": 17, "context": "Document layout analysis aiming to identify document format by classifying blocks into text, figures, tables, and such has been a long-studied problem (O\u2019Gorman, 1993; Simon et al., 1997).", "startOffset": 151, "endOffset": 187}, {"referenceID": 13, "context": "Various efforts have been made for table extraction using semi-supervised learning on the patterns of table layouts within ASCII text documents (Ng et al., 1999) web documents (Pinto et al.", "startOffset": 144, "endOffset": 161}, {"referenceID": 15, "context": ", 1999) web documents (Pinto et al., 2003; Lerman et al., 2001; Zanibbi et al., 2004) PDF and OCR image documents (Clark and Divvala, 2015; Liu et al.", "startOffset": 22, "endOffset": 85}, {"referenceID": 9, "context": ", 1999) web documents (Pinto et al., 2003; Lerman et al., 2001; Zanibbi et al., 2004) PDF and OCR image documents (Clark and Divvala, 2015; Liu et al.", "startOffset": 22, "endOffset": 85}, {"referenceID": 20, "context": ", 1999) web documents (Pinto et al., 2003; Lerman et al., 2001; Zanibbi et al., 2004) PDF and OCR image documents (Clark and Divvala, 2015; Liu et al.", "startOffset": 22, "endOffset": 85}, {"referenceID": 11, "context": ", 2004) PDF and OCR image documents (Clark and Divvala, 2015; Liu et al., 2007).", "startOffset": 36, "endOffset": 79}, {"referenceID": 6, "context": "(Khusro et al., 2015) introduces and compares the state-of-the-art table extraction techniques from PDF articles.", "startOffset": 0, "endOffset": 21}, {"referenceID": 18, "context": "They use features of mathematical symbols, operators, and positions and their character sizes (Suzuki et al., 2003; Kacem et al., 2001).", "startOffset": 94, "endOffset": 135}, {"referenceID": 5, "context": "They use features of mathematical symbols, operators, and positions and their character sizes (Suzuki et al., 2003; Kacem et al., 2001).", "startOffset": 94, "endOffset": 135}, {"referenceID": 6, "context": "Tables are usually conveyed by its two-dimensional layout and its column and/or row headings (Khusro et al., 2015).", "startOffset": 93, "endOffset": 114}, {"referenceID": 12, "context": "We train word embeddings using TWORD2V EC using WORD2VEC (Mikolov et al., 2013).", "startOffset": 57, "endOffset": 79}, {"referenceID": 16, "context": "To improve the robustness of structured prediction, we adopted a learning to search algorithm known as DAGGER to SVM (Ross et al., 2010).", "startOffset": 117, "endOffset": 136}, {"referenceID": 19, "context": "PC-CB (Tuarob et al., 2013)* N/A 75.", "startOffset": 6, "endOffset": 27}, {"referenceID": 19, "context": "We also include the numbers reported by Tuarob et al. (2013) for comparison.", "startOffset": 40, "endOffset": 61}, {"referenceID": 0, "context": "In this experiment, we consider seeded K-means clustering algorithm (Basu et al., 2002) for teaching documents.", "startOffset": 68, "endOffset": 87}], "year": 2017, "abstractText": "Technical documents contain a fair amount of unnatural language, such as tables, formulas, pseudo-codes, etc. Unnatural language can be an important factor of confusing existing NLP tools. This paper presents an effective method of distinguishing unnatural language from natural language, and evaluates the impact of unnatural language detection on NLP tasks such as document clustering. We view this problem as an information extraction task and build a multiclass classification model identifying unnatural language components into four categories. First, we create a new annotated corpus by collecting slides and papers in various formats, PPT, PDF, and HTML, where unnatural language components are annotated into four categories. We then explore features available from plain text to build a statistical model that can handle any format as long as it is converted into plain text. Our experiments show that removing unnatural language components gives an absolute improvement in document clustering up to 15%. Our corpus and tool are publicly available.", "creator": "LaTeX with hyperref package"}}}