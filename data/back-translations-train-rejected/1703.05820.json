{"id": "1703.05820", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Particle Value Functions", "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.", "histories": [["v1", "Thu, 16 Mar 2017 21:08:31 GMT  (299kb,D)", "http://arxiv.org/abs/1703.05820v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["chris j maddison", "dieterich lawson", "george tucker", "nicolas heess", "arnaud doucet", "riy mnih", "yee whye teh"], "accepted": false, "id": "1703.05820"}, "pdf": {"name": "1703.05820.pdf", "metadata": {"source": "CRF", "title": "PARTICLE VALUE FUNCTIONS", "authors": ["Chris J. Maddison", "Dieterich Lawson", "George Tucker", "Nicolas Heess", "Arnaud Doucet", "Andriy Mnih", "Yee Whye Teh"], "emails": ["cmaddis@stats.ox.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "This type of risk sensitivity is desirable, for example, in real-world contexts such as financial trading or safety-critical applications where the risk required to achieve a particular return plays a major role. In this paper, we introduce a risk-sensitive value function based on a system of interacting trajectories called Particle Value Function (PVF), which is applicable to large-scale amplification of learning problems with non-linear functional approximation, inspired by recent advances in the conclusion of variations that limit the marginal probability of the Protocol on Importance Samples (Burda et al., 2016; Mnih & Rezende, 2016)."}, {"heading": "2 RISK SENSITIVITY AND EXPONENTIAL UTILITY", "text": "We consider a finite horizon Markov Decision Process (MDP) (\u03b2 = \u03b2 selection), in which Rt is the momentary reward generated by an agent following a non-stationary policy, see Appendix A. A supply function u: R \u2192 R is an immutable, non-diminishing function that defines a ranking of possible returns. The expected supply T [u] is the real number V \u03c01 (s, u), the benefit of which is the expected supply. (E) [u) (T, t = 1 Rt), a natural definition of the \"value\" of a state is the real number V (s, u), the benefit of which is the expected supply."}, {"heading": "3 PARTICLE VALUE FUNCTIONS", "text": "Algorithms for optimizing V \u03c0T (s, \u03b2) may suffer from numerical problems or high variance, see Appendix B. Instead, we define a value function that limits V \u03c0T (s, \u03b2) and approach it in the infinite sample boundary. We call it a particle filter because it assigns a value to a bootstrap particle filter that can be used to estimate the normalization constants in a hidden Markov model (HMM). Let (Xt) be the states of an HMM with transitions Xt-p (\u00b7 Xt \u2212 1) and emissions Yt."}, {"heading": "4 EXPERIMENTS", "text": "To highlight the benefits of using PVFs, we apply them to a variant of the Gridworld task called Cliffworld, see Appendix E for comparison with other methods and further details. We trained time-dependent tabular strategies using policy gradients from different PVFs for \u03b2-1, \u2212 0.5, 0, 0.5, 1, 2}. We tried K-1,..., 8} and learning rates of 1 x 10 \u2212 3, 5 x 10 \u2212 4, 1 x 10 \u2212 4, 5 x 10 \u2212 5}. In the \u03b2 = 0 case, we pursued K-independent, non-interacting paths and determined via a policy gradient with estimated bases. Figure 2 shows the density above the final state of the trained MDP under different \u03b2 treatments butK = 4. Note that the higher the risk parameter, the broader the policy is, with the agent ultimately solving the task. No \u03b2 = 0 corresponding to the standard REINFORCE solves this task, even after increasing the number of players to 64."}, {"heading": "5 CONCLUSION", "text": "We have introduced the particle value function, which approximates a risk-sensitive value function for a particular MDP. We will try to answer theoretical questions such as whether the PVF increases in \u03b2 and is monotonous in the number of particles. Furthermore, the PVF does not have an efficient tabular representation, so it would be valuable to understand the effects of efficient approximations. Experimentally, we hope to explore these ideas for complex sequential tasks with nonlinear function approximators. An obvious example of such tasks is the conclusion of variation via a sequential model."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank Re'mi Munos, Theophane Weber, David Silver, Marc G. Bellemare and Danilo J. Rezende for their helpful discussion and support in this project."}, {"heading": "A MARKOV DECISION PROCESSES", "text": "We look at decision problems where an agent selects actions and receives rewards in a stochastic environment. For the sake of presentation, we look at a finite horizon MDP consisting of: a finite state space S, a finite action space A, a stationary ambient transition core that fulfills the Markov property p (\u00b7 | St, At,..., S0, A0) = p (\u00b7 St, At), and reward functions rT \u2212 t: S \u00b7 A \u2192 R. At each step, the agent selects actions according to a policy \u03c0T \u2212 t (\u00b7 St) taking into account the current state. \u03c0T \u2212 t (\u00b7 St) is the action distribution and rT \u2212 t is the reward function if T \u2212 t steps are left. Overall, the MDP proceeds stochaically and produces a sequence of random variables (St, At) according to the following dynamics for T-N time steps."}, {"heading": "B RISK-SENSITIVE VALUE FUNCTION DETAILS", "text": "Usefulness theory gives us a language to describe the relative importance of high or low returns. A usefulness function u: \u03b2 \u03b2 = \u03b2 (1997) is an immutable, non-diminishing function that determines a ranking of possible returns. (...) The expected usefulness E [u (... T = \u03b2 = \u03b2 = \u03b2 = \u03b2 = \u03b2 = s] represents a ranking above politics (von Neumann & Morgenstern, 1953). The expected usefulness does not necessarily have an interpretable scale, since any affinity transformation of the usefulness function results in the same relative order of strategies or returns outcomes. Therefore, we define the value associated with a usefulness by attributing it to the scale of rewards defined by the MDP. For an agent following the \"value\" of a state, the real number V (s, u) whose usefulness is the expected usefulness is: s (u) (E) \u2212 1."}, {"heading": "C RELATED WORK", "text": "The idea of treating enhanced learning as a problem is not a new idea. (Von Neumann & Morgenstern, 1953; \u03b2 = 1988; Pratt, 1964; Arrow, 1974) It has been extensively studied for the governance of MDPs (Howard & Matheson, 1972; Coraluppi, 1997; Borkar & Meyn, 2002; Mihatsch & Neuneier, 2002; Ba \ufffd uerle & Rieder, 2013) In the field of enhanced learning, risk sensitivity has been studied (Koenig & Simmons, 1994; Neuneier & Mihatsch, 1998; Shen et al., 2014), although none of them takes into account the direct political gradient approach considered in this paper. Most of the methods considered are variants of a Q-learning approach or a political iteration. Nor is the idea of treating rewards as emissions of an HMM a new idea (Toussaint & Storkey, 2006; Rawlik et al, 2010). The idea of treating enhanced learning as a problem is not a new idea."}, {"heading": "D PARTICLE VALUE FUNCTION DETAILS", "text": "Remember algorithm 1 and the definition of MDP in Appendix A = \u03b2 = \u03b2 (\u03b2) (\u03b2) (\u03b2) (\u03b2) (\u03b2) (\u03b2) (\u03b2) t (S (i) t, A (i) t (17) the particle value function associated with the bootstrap particle filter dynamics: V \u03c0T (s (1),., s (K), s (K), s (S), s (S), s (S), s (S), s (S), s (S), s (S), s (S), s (S), s (S), s (S), s (S), s (S), s (S), s (S), s (S), S (S), S (S), S (S, S, S, S, S, S, S (S, S, S, S, S, S, S (S), S (S, S, S (S), S (S), S (S, S (S), S (S, S, S, S (S), S (S, S, S (S), S (S (S), S (S, S (S), S (S, S, S (S), S (S (S, S), S (S (S, S), S (S (S, S, S), S (S (S (S), S (S (S, S), S (S (S, S (S, S), S (S (S, S, S), S (S (S, S, S), S (S (S), S (S (S, S (S, S), S (S), S (S (S, S (S, S, S), S (S), S (S (S (S, S), S (S (S, S), S (S (S, S), S (S (S, S), S (S (S), S (S (S (S), S (S, S), S (S (S, S, S, S, S), S (S (S (S), S ("}, {"heading": "E CLIFFWORLD DETAILS", "text": "We have written down the flags, that we must attach ourselves to the flags, that we must attach ourselves to the flags, that we must attach ourselves to the flags, that we must attach ourselves to the flags, that we must attach ourselves to the flags, that we must attach ourselves to the flags, that we must attach ourselves to the flags, that we must attach ourselves to the flags, that we must attach ourselves to the flags of the flags of the flags, that we must attach ourselves to the flags, that we must attach ourselves to the flags, that we must write ourselves to the flags, that we must attach ourselves to the flags of the flags of the flags, the flags of the flags of the flags, the flags of the flags of the flags, the flags of the flags of the flags, the flags of the flags of the flags, the flags of the flags of the flags, the flags of the flags of the flags, the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags of the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags of the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags of the flags of the flags of the flags, the flags of the flags of the flags of the flags of the flags of the flags"}], "references": [{"title": "Logarithmic transformations for discrete-time, finite-horizon stochastic control problems", "author": ["Francesca Albertini", "Wolfgang J Runggaldier"], "venue": "Applied mathematics & optimization,", "citeRegEx": "Albertini and Runggaldier.,? \\Q1988\\E", "shortCiteRegEx": "Albertini and Runggaldier.", "year": 1988}, {"title": "Essays in the theory of risk-bearing", "author": ["Kenneth Joseph Arrow"], "venue": null, "citeRegEx": "Arrow.,? \\Q1974\\E", "shortCiteRegEx": "Arrow.", "year": 1974}, {"title": "More risk-sensitive markov decision processes", "author": ["Nicole B\u00e4uerle", "Ulrich Rieder"], "venue": "Mathematics of Operations Research,", "citeRegEx": "B\u00e4uerle and Rieder.,? \\Q2013\\E", "shortCiteRegEx": "B\u00e4uerle and Rieder.", "year": 2013}, {"title": "Risk-sensitive optimal control for markov decision processes with monotone cost", "author": ["Vivek S Borkar", "Sean P Meyn"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Borkar and Meyn.,? \\Q2002\\E", "shortCiteRegEx": "Borkar and Meyn.", "year": 2002}, {"title": "Risk sensitive path integral control", "author": ["Bart van den Broek", "Wim Wiegerinck", "Hilbert Kappen"], "venue": "arXiv preprint arXiv:1203.3523,", "citeRegEx": "Broek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Broek et al\\.", "year": 2012}, {"title": "Importance Weighted Autoencoder", "author": ["Yuri Burda", "Roger Grosse", "Ruslan Salakhutdinov"], "venue": "In ICLR,", "citeRegEx": "Burda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2016}, {"title": "Optimal control of Markov decision processes for performance and robustness", "author": ["Stefano Coraluppi"], "venue": "PhD thesis, University of Maryland,", "citeRegEx": "Coraluppi.,? \\Q1997\\E", "shortCiteRegEx": "Coraluppi.", "year": 1997}, {"title": "Using expectation-maximization for reinforcement learning", "author": ["Peter Dayan", "Geoffrey E Hinton"], "venue": "Neural Computation,", "citeRegEx": "Dayan and Hinton.,? \\Q1997\\E", "shortCiteRegEx": "Dayan and Hinton.", "year": 1997}, {"title": "Feynman-kac formulae. In Feynman-Kac Formulae, pp. 47\u201393", "author": ["Pierre Del Moral"], "venue": null, "citeRegEx": "Moral.,? \\Q2004\\E", "shortCiteRegEx": "Moral.", "year": 2004}, {"title": "A tutorial on particle filtering and smoothing: fiteen years", "author": ["Arnaud Doucet", "Adam M Johansen"], "venue": null, "citeRegEx": "Doucet and Johansen.,? \\Q2011\\E", "shortCiteRegEx": "Doucet and Johansen.", "year": 2011}, {"title": "Taming the noise in reinforcement learning via soft updates", "author": ["Roy Fox", "Ari Pakman", "Naftali Tishby"], "venue": "arXiv preprint arXiv:1512.08562,", "citeRegEx": "Fox et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2015}, {"title": "On solving general state-space sequential decision problems using inference algorithms", "author": ["Matt Hoffman", "Arnaud Doucet", "Nando De Freitas", "Ajay Jasra"], "venue": "Technical report, Technical Report TR2007-04,", "citeRegEx": "Hoffman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2007}, {"title": "Risk-sensitive Markov decision processes", "author": ["Ronald A Howard", "James E Matheson"], "venue": "Management science,", "citeRegEx": "Howard and Matheson.,? \\Q1972\\E", "shortCiteRegEx": "Howard and Matheson.", "year": 1972}, {"title": "Sequential decision making in general state space models", "author": ["Nikolas Kantas"], "venue": "PhD thesis, Citeseer,", "citeRegEx": "Kantas.,? \\Q2009\\E", "shortCiteRegEx": "Kantas.", "year": 2009}, {"title": "On particle methods for parameter estimation in state-space models", "author": ["Nikolas Kantas", "Arnaud Doucet", "Sumeetpal S Singh", "Jan Maciejowski", "Nicolas Chopin"], "venue": "Statistical science,", "citeRegEx": "Kantas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kantas et al\\.", "year": 2015}, {"title": "Linear theory for control of nonlinear stochastic systems", "author": ["Hilbert J Kappen"], "venue": "Physical review letters,", "citeRegEx": "Kappen.,? \\Q2005\\E", "shortCiteRegEx": "Kappen.", "year": 2005}, {"title": "Optimal control as a graphical model inference problem", "author": ["Hilbert J Kappen", "Vicen\u00e7 G\u00f3mez", "Manfred Opper"], "venue": "Machine learning,", "citeRegEx": "Kappen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kappen et al\\.", "year": 2012}, {"title": "Risk-sensitive planning with probabilistic decision graphs", "author": ["Sven Koenig", "Reid G Simmons"], "venue": "In Proceedings of the 4th international conference on principles of knowledge representation and reasoning,", "citeRegEx": "Koenig and Simmons.,? \\Q1994\\E", "shortCiteRegEx": "Koenig and Simmons.", "year": 1994}, {"title": "Risk sensitive markov decision processes", "author": ["Steven I Marcus", "Emmanual Fern\u00e1ndez-Gaucherand", "Daniel Hern\u00e1ndez-Hernandez", "Stefano Coraluppi", "Pedram Fard"], "venue": "In Systems and control in the twenty-first century,", "citeRegEx": "Marcus et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1997}, {"title": "Risk-sensitive reinforcement learning", "author": ["Oliver Mihatsch", "Ralph Neuneier"], "venue": "Machine learning,", "citeRegEx": "Mihatsch and Neuneier.,? \\Q2002\\E", "shortCiteRegEx": "Mihatsch and Neuneier.", "year": 2002}, {"title": "Variational Inference for Monte Carlo Objectives", "author": ["Andriy Mnih", "Danilo Rezende"], "venue": "In ICML,", "citeRegEx": "Mnih and Rezende.,? \\Q2016\\E", "shortCiteRegEx": "Mnih and Rezende.", "year": 2016}, {"title": "Risk sensitive reinforcement learning", "author": ["Ralph Neuneier", "Oliver Mihatsch"], "venue": "In Proceedings of the 11th International Conference on Neural Information Processing Systems,", "citeRegEx": "Neuneier and Mihatsch.,? \\Q1998\\E", "shortCiteRegEx": "Neuneier and Mihatsch.", "year": 1998}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["Andrew Y Ng", "Daishi Harada", "Stuart Russell"], "venue": "In ICML,", "citeRegEx": "Ng et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "On some properties of markov chain monte carlo simulation methods based on the particle filter", "author": ["Michael K Pitt", "Ralph dos Santos Silva", "Paolo Giordani", "Robert Kohn"], "venue": "Journal of Econometrics,", "citeRegEx": "Pitt et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pitt et al\\.", "year": 2012}, {"title": "Risk aversion in the small and in the large", "author": ["John W Pratt"], "venue": null, "citeRegEx": "Pratt.,? \\Q1964\\E", "shortCiteRegEx": "Pratt.", "year": 1964}, {"title": "An approximate inference approach to temporal optimization in optimal control. In Advances in neural information processing", "author": ["Konrad Rawlik", "Marc Toussaint", "Sethu Vijayakumar"], "venue": null, "citeRegEx": "Rawlik et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rawlik et al\\.", "year": 2011}, {"title": "Particle smoothing for hidden diffusion processes: Adaptive path integral smoother", "author": ["H-Ch Ruiz", "HJ Kappen"], "venue": "arXiv preprint arXiv:1605.00278,", "citeRegEx": "Ruiz and Kappen.,? \\Q2016\\E", "shortCiteRegEx": "Ruiz and Kappen.", "year": 2016}, {"title": "Curious model-building control systems", "author": ["J\u00fcrgen Schmidhuber"], "venue": "In Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1991}, {"title": "Risk-sensitive reinforcement learning", "author": ["Yun Shen", "Michael J Tobia", "Tobias Sommer", "Klaus Obermayer"], "venue": "Neural computation,", "citeRegEx": "Shen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Information theory of decisions and actions", "author": ["Naftali Tishby", "Daniel Polani"], "venue": "In Perception-action cycle,", "citeRegEx": "Tishby and Polani.,? \\Q2011\\E", "shortCiteRegEx": "Tishby and Polani.", "year": 2011}, {"title": "Linearly-solvable markov decision problems", "author": ["Emanuel Todorov"], "venue": "In NIPS, pp", "citeRegEx": "Todorov.,? \\Q2006\\E", "shortCiteRegEx": "Todorov.", "year": 2006}, {"title": "Probabilistic inference for solving discrete and continuous state markov decision processes", "author": ["Marc Toussaint", "Amos Storkey"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Toussaint and Storkey.,? \\Q2006\\E", "shortCiteRegEx": "Toussaint and Storkey.", "year": 2006}, {"title": "Theory of games and economic behavior", "author": ["John Von Neumann", "Oskar Morgenstern"], "venue": null, "citeRegEx": "Neumann and Morgenstern.,? \\Q1953\\E", "shortCiteRegEx": "Neumann and Morgenstern.", "year": 1953}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "For control variates, we used distinct baselines depending on whether \u03b2 = 0 or not. For \u03b2 = 0, we used a baseline that was an exponential moving average with smoothing factor 0.8. The baselines were also non-stationary, and with dimensionality 4 \u00d7 12 \u00d7 24. For \u03b2 6= 0 we used no baseline except for VIMCO\u2019s control variate (Mnih", "author": ["Monte Carlo returns"], "venue": null, "citeRegEx": "returns.,? \\Q2016\\E", "shortCiteRegEx": "returns.", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "The idea is inspired by recent advances in variational inference which bound the log marginal likelihood via importance sampling estimators (Burda et al., 2016; Mnih & Rezende, 2016), but takes an orthogonal approach to reward modifications, e.", "startOffset": 140, "endOffset": 182}, {"referenceID": 27, "context": "(Schmidhuber, 1991; Ng et al., 1999).", "startOffset": 0, "endOffset": 36}, {"referenceID": 22, "context": "(Schmidhuber, 1991; Ng et al., 1999).", "startOffset": 0, "endOffset": 36}, {"referenceID": 24, "context": "This choice is well-studied, and it is implied by the assumption that V \u03c0 T (s, u) is additive for deterministic translations of the reward function (Pratt, 1964; Howard & Matheson, 1972; Coraluppi, 1997).", "startOffset": 149, "endOffset": 204}, {"referenceID": 6, "context": "This choice is well-studied, and it is implied by the assumption that V \u03c0 T (s, u) is additive for deterministic translations of the reward function (Pratt, 1964; Howard & Matheson, 1972; Coraluppi, 1997).", "startOffset": 149, "endOffset": 204}, {"referenceID": 30, "context": "Note also that the literature on KL regularized control (Todorov, 2006; Kappen, 2005; Tishby & Polani, 2011) gives a different perspective on risk sensitive control, which mirrors the relationship between variational inference and maximum likelihood.", "startOffset": 56, "endOffset": 108}, {"referenceID": 15, "context": "Note also that the literature on KL regularized control (Todorov, 2006; Kappen, 2005; Tishby & Polani, 2011) gives a different perspective on risk sensitive control, which mirrors the relationship between variational inference and maximum likelihood.", "startOffset": 56, "endOffset": 108}, {"referenceID": 23, "context": "The result is an unbiased estimator \u220fT t=0(K \u22121\u2211K i=1 q(yt|X (i) t )) of the desired probability (Del Moral, 2004; Pitt et al., 2012).", "startOffset": 97, "endOffset": 133}, {"referenceID": 12, "context": "This is distinct, but related to Kantas (2009), which investigates particle filter algorithms for infinite horizon risk-sensitive control.", "startOffset": 33, "endOffset": 47}, {"referenceID": 24, "context": "This is a broadly studied choice that is implied by the assumption that the value function V \u03c0 T (s, u) is additive for deterministic translations of the return (Pratt, 1964; Howard & Matheson, 1972; Coraluppi, 1997).", "startOffset": 161, "endOffset": 216}, {"referenceID": 6, "context": "This is a broadly studied choice that is implied by the assumption that the value function V \u03c0 T (s, u) is additive for deterministic translations of the return (Pratt, 1964; Howard & Matheson, 1972; Coraluppi, 1997).", "startOffset": 161, "endOffset": 216}, {"referenceID": 6, "context": "From Coraluppi (1997). 2.", "startOffset": 5, "endOffset": 22}, {"referenceID": 6, "context": "From Coraluppi (1997). 2. From Coraluppi (1997). 3.", "startOffset": 5, "endOffset": 48}, {"referenceID": 6, "context": "From Coraluppi (1997). 2. From Coraluppi (1997). 3. From Coraluppi (1997). 4.", "startOffset": 5, "endOffset": 74}, {"referenceID": 6, "context": "As \u03b2 \u2192 \u2212\u221e it approaches the infimum, a worst-case value (Coraluppi, 1997).", "startOffset": 56, "endOffset": 73}, {"referenceID": 33, "context": "Even ignoring underflow/overflow issues, REINFORCE (Williams, 1992) style algorithms would find difficulties, because deriving unbiased estimators of the ratio exp(\u03b2QT\u2212t(St, At, \u03b2) \u2212 \u03b2V \u03c0 T (s, \u03b2)) from single trajectories of the MDP may be hard.", "startOffset": 51, "endOffset": 67}, {"referenceID": 14, "context": "(Kantas et al., 2015), but for large T the estimate suffers from high mean squared errors.", "startOffset": 0, "endOffset": 21}, {"referenceID": 24, "context": "Risk sensitivity originates in the study of utility and choice in economics (Von Neumann & Morgenstern, 1953; Pratt, 1964; Arrow, 1974).", "startOffset": 76, "endOffset": 135}, {"referenceID": 1, "context": "Risk sensitivity originates in the study of utility and choice in economics (Von Neumann & Morgenstern, 1953; Pratt, 1964; Arrow, 1974).", "startOffset": 76, "endOffset": 135}, {"referenceID": 6, "context": "It has been extensively studied for the control of MDPs (Howard & Matheson, 1972; Coraluppi, 1997; Marcus et al., 1997; Borkar & Meyn, 2002; Mihatsch & Neuneier, 2002; B\u00e4uerle & Rieder, 2013).", "startOffset": 56, "endOffset": 191}, {"referenceID": 18, "context": "It has been extensively studied for the control of MDPs (Howard & Matheson, 1972; Coraluppi, 1997; Marcus et al., 1997; Borkar & Meyn, 2002; Mihatsch & Neuneier, 2002; B\u00e4uerle & Rieder, 2013).", "startOffset": 56, "endOffset": 191}, {"referenceID": 28, "context": "In reinforcement learning, risk sensitivity has been studied (Koenig & Simmons, 1994; Neuneier & Mihatsch, 1998; Shen et al., 2014), although none of these consider the direct policy gradient approach considered in this work.", "startOffset": 61, "endOffset": 131}, {"referenceID": 15, "context": "The idea of treating reinforcement learning as an inference problem is not a new idea (see e.g. Albertini & Runggaldier, 1988; Dayan & Hinton, 1997; Kappen, 2005; Toussaint & Storkey, 2006; Hoffman et al., 2007; Tishby & Polani, 2011; Kappen et al., 2012).", "startOffset": 86, "endOffset": 255}, {"referenceID": 11, "context": "The idea of treating reinforcement learning as an inference problem is not a new idea (see e.g. Albertini & Runggaldier, 1988; Dayan & Hinton, 1997; Kappen, 2005; Toussaint & Storkey, 2006; Hoffman et al., 2007; Tishby & Polani, 2011; Kappen et al., 2012).", "startOffset": 86, "endOffset": 255}, {"referenceID": 16, "context": "The idea of treating reinforcement learning as an inference problem is not a new idea (see e.g. Albertini & Runggaldier, 1988; Dayan & Hinton, 1997; Kappen, 2005; Toussaint & Storkey, 2006; Hoffman et al., 2007; Tishby & Polani, 2011; Kappen et al., 2012).", "startOffset": 86, "endOffset": 255}, {"referenceID": 14, "context": "These are studied for example in Albertini & Runggaldier (1988); Kappen (2005); Tishby & Polani (2011); Kappen et al.", "startOffset": 65, "endOffset": 79}, {"referenceID": 14, "context": "These are studied for example in Albertini & Runggaldier (1988); Kappen (2005); Tishby & Polani (2011); Kappen et al.", "startOffset": 65, "endOffset": 103}, {"referenceID": 14, "context": "These are studied for example in Albertini & Runggaldier (1988); Kappen (2005); Tishby & Polani (2011); Kappen et al. (2012); Fox et al.", "startOffset": 65, "endOffset": 125}, {"referenceID": 10, "context": "(2012); Fox et al. (2015);", "startOffset": 8, "endOffset": 26}, {"referenceID": 4, "context": "Yet, in certain special cases, risk sensitive objectives can also be cast as solutions to path integral control problems (Broek et al., 2012).", "startOffset": 121, "endOffset": 141}, {"referenceID": 14, "context": "Ruiz & Kappen (2016). The observation is that in an MDP with fully controllable transition dynamics, optimizing a policy \u03c0\u2032, which completely specifies the transition dynamics, achieves the risk sensitive value at \u03c0: max \u03c0\u2032 V\u0302 \u03c0,\u03c0 \u2032 T (s, \u03b2) = V \u03c0 T (s, \u03b2) (16) Note that this has an interesting connection to Bayesian inference.", "startOffset": 7, "endOffset": 21}, {"referenceID": 23, "context": "and since the bootstrap particle filter is unbiased (Del Moral, 2004; Pitt et al., 2012), = V \u03c0 T (s, \u03b2) (26) For \u03b2 < 0 we get the reverse inequality, V \u03c0 T,K(s, \u03b2) \u2265 V \u03c0 T (s, \u03b2).", "startOffset": 52, "endOffset": 88}, {"referenceID": 33, "context": "For \u03b2 = 0, we used instead a REINFORCE (Williams, 1992) estimator, that was simply estimated from the Monte Carlo returns.", "startOffset": 39, "endOffset": 55}], "year": 2017, "abstractText": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent\u2019s experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.", "creator": "LaTeX with hyperref package"}}}