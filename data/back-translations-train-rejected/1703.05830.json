{"id": "1703.05830", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Automatically identifying wild animals in camera trap images with deep learning", "abstract": "Having accurate, detailed, and up-to-date information about wildlife location and behavior across broad geographic areas would revolutionize our ability to study, conserve, and manage species and ecosystems. Currently such data are mostly gathered manually at great expense, and thus are sparsely and infrequently collected. Here we investigate the ability to automatically, accurately, and inexpensively collect such data from motion sensor cameras. These camera traps enable pictures of wildlife to be collected inexpensively, unobtrusively, and at high-volume. However, identifying the animals, animal attributes, and behaviors in these pictures remains an expensive, time-consuming, manual task often performed by researchers, hired technicians, or crowdsourced teams of human volunteers. In this paper, we demonstrate that such data can be automatically extracted by deep neural networks (aka deep learning), which is a cutting-edge type of artificial intelligence. In particular, we use the existing human-labeled images from the Snapshot Serengeti dataset to train deep convolutional neural networks for identifying 48 species in 3.2 million images taken from Tanzania's Serengeti National Park. We train neural networks that automatically identify animals with over 92% accuracy. More importantly, we can choose to have our system classify only the images it is highly confident about, allowing valuable human time to be focused only on challenging images. In this case, our automatic animal identification system saves approximately ~8.3 years (at 40 hours per week) of human labeling effort (i.e. over 17,000 hours) while operating on a 3.2-million-image dataset at the same 96% accuracy level of crowdsourced teams of human volunteers. Those efficiency gains immediately highlight the importance of using deep neural networks to automate data extraction from camera trap images.", "histories": [["v1", "Thu, 16 Mar 2017 21:35:15 GMT  (7094kb,D)", "https://arxiv.org/abs/1703.05830v1", null], ["v2", "Wed, 22 Mar 2017 00:45:55 GMT  (7094kb,D)", "http://arxiv.org/abs/1703.05830v2", null], ["v3", "Thu, 30 Mar 2017 22:24:19 GMT  (7094kb,D)", "http://arxiv.org/abs/1703.05830v3", null], ["v4", "Wed, 5 Apr 2017 04:26:20 GMT  (7092kb,D)", "http://arxiv.org/abs/1703.05830v4", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["mohammed sadegh norouzzadeh", "anh nguyen", "margaret kosmala", "ali swanson", "craig packer", "jeff clune"], "accepted": false, "id": "1703.05830"}, "pdf": {"name": "1703.05830.pdf", "metadata": {"source": "CRF", "title": "Automatically identifying wild animals in camera-trap images with deep learning", "authors": ["Mohammed Sadegh Norouzzadeh", "Anh Nguyen", "Margaret Kosmala", "Ali Swanson", "Craig Packer", "Jeff Clune"], "emails": [], "sections": [{"heading": null, "text": "Automatic identification of wildlife in deep learning camera footage Mohammed Sadegh Norouzzadeh1, Anh Nguyen1, Margaret Kosmala2, Ali Swanson3, Craig Packer4, and Jeff Clune1,51 University of Wyoming; 2Harvard University; 3University of Oxford; 4University of Minnesota; 5Uber AI LabsLast edited on April 6, 2017Having accurate, detailed, and up-to-date information about wildlife location and behavior across wide geographic areas would revolutionize our ability to study, conserve, and manage species and ecosystems. Such data is usually collected with great effort and is therefore sparse and irregular. Here, we investigate the ability to automatically, accurately, and unexplicitly collect data that could turn many fields of biology, ecology, and zoology into \"big data.\""}, {"heading": "Deep Learning | Animal identification | Convolutional Neural Networks", "text": "To better understand the complexity of naturalistic ecosystems, as well as to best manage and protect them, it would be helpful to have detailed knowledge of animal numbers, locations and behaviors in natural ecosystems (1). A well-known method of collecting data from wildlife uses motion sensor cameras placed in natural habitats (3) that have revolutionized ecology and wildlife conservation over the past two decades (2). Camera traps have become an essential tool for ecologists to study population sizes and distributions (3), assess habitat use (4), identify new species (5), and so on. They can capture millions of images (6-8), extract knowledge from these camera images traditionally made by humans (i.e. experts or volunteers), and are so time-consuming and costly that much of the invaluable knowledge remains in these data repositories."}, {"heading": "Background and Related Works", "text": "There are a number of approaches that set out in search of new ways to understand the world and how it is about people, and how it is about people. (12) In the case of the automatic capture of images, as we do, the machine is equipped with many pairs of images, where the image is input and the correct name (e.g. \"lion\") is output. Deep learning allows the machine to extract multiple levels of abstraction from raw data (Fig. 3). Inspired by the mammalian visual cortex (14), deep convolutional neural networks are a class of feedforward deep neural networks, in which each layer of neurons uses revolutionary operations to extract information from the overlapping small regions (10). Deep neural networks have recently dramatically improved the state of art in many real-world problems (10)."}, {"heading": "Experiments and Results", "text": "The ultimate goal of this research is to shift the manual labeling efforts currently undertaken by human volunteers to the computer. Therefore, we address the following two classification tasks. The blank vs. animal task is to train a DNN to determine whether images contain animals. Since 75% of images of humans are designated as blank techniques, automating this task alone would save time or assign them more important, challenging tasks. The task of animal identification is to classify which animal species are present in the images that contain animals. In this paper, we only tackle the label classification (12) (one instead of identifying several species in an image), so we removed images labeled as containing more than one species from our training and test group (approximately 5% of the datasets). Both tasks are computer vision classification tasks for which the state-of-the-art approach is training a DNN (10, 20, containing 34 different architectures), for example, which they contain different architectures."}, {"heading": "Additional Results", "text": "In this section, we show additional important results for (1) how much human labor can be saved through our automatic identification system. (2) Experiments to further improve classification accuracy through modeling. In SI, we also document additional experiments to improve the imbalance of data sets (Sec. S4) and use our image classification techniques to classify events rather than individual images (Sec. S3). Saving human labor over prediction and trust thresholds. A major advantage of automated animal identification is eliminating the need for humans to label images. Here, we estimate the total amount of human labor that can be saved if our system matches the current classification accuracy of human volunteers. We create a two-step pipeline by classifying the best model from the empty vs. animal experiment, whether the image contains an animal and, if it does, with the best animal classification class."}, {"heading": "Discussion and Future Work", "text": "There are many approaches for future work, but here we mentioned four particularly promising projects. (1) We and others can experiment with more architectures, but more images are labeled per year (the current figure is about 9 million), and this additional data should come at a significant cost. (2) Given the rapid progress in this area, we should play by the rules. (3)"}, {"heading": "Conclusion", "text": "In this work, we tested the ability of the human workforce to be routed for other important scientific purposes. We have shown that training these networks from scratch on the SS dataset data can improve performance via the transfer learning approach of Gomez et al. (32). We have also shown that deep neural networks can work well on the full 48-class SS dataset, although performance for rare classes is worse. We have demonstrated other techniques that can increase performance at the expense of overall accuracy (Sec. p.4). Perhaps most importantly, our results show that the use of deep learning technology is beneficial to researchers in biology and the human volunteers who help them by tagging images. Specifically, our system can save 96.9% of manual work (over 17,000 hours) while working at the same level 96.6% accuracy as human volunteers."}, {"heading": "S.1. Pre-processing and Training", "text": "In this section we document the technical details for the pre-processing step and for the selection of hyperparameters in all experiments in paper.Pre-processing. The original images in the dataset are 2,048 x 1,536 pixels, which is too large for the current state of the art deep neural networks due to the increased computing costs for training and executing DNNs on high resolution images. We followed standard practices in reducing the images to 256 x 256 pixels. Although this can slightly distort the images as we do not get the aspect ratios of the images, it is a de facto standard in the deep learning community (10). The images in the dataset are color images in which each pixel has three values: one for each of the red, green and blue intensities. We point to all values for a particular color as a color channel."}, {"heading": "53 0.0001 0", "text": "Table p.2. The hyperparameters for the formation of dynamic neural networks for all experiments AlexNet NIN VGG GoogLeNet ResNet-18 ResNet-34 ResNet-50 ResNet-101 ResNet-152 020406080100P e rce n tA ccu rate89.9 91.0 91.3 92.3 92.3 92.7 92.5 92.897.6 97.8 98.2 96.3 98.2 98.2 98.4 98.2Top-1 Top-5Fig. p.2. The top-1 and top-5 accuracy of different architectures for total capture events (as opposed to frames) improves the accuracy for all images within a capture event. The best top-1 accuracy is ResNet-152 with 92.8% and the best top-5 accuracy is ResNet-50 with 98.4%."}, {"heading": "S.2. Prediction averaging", "text": "For each model we present here, there is a probability distribution across all classes; for each model, we have the top 5 conjectures (each of which includes a category and a trust probability); in all other models, we sum up these probabilities by categories and divide them by n to generate an aggregated vector V of probabilities (the length of which varies from 5 to 5n); the last aggregated predictions refer to the top 5 entries of V, p. 3 showing an example of this type of capture of events that are not contained in individual images; however, we are trained to classify images; we can anticipate predictions for individual images to predict events; we could also simply train the neural network to classify events directly, but there are challenges associated with capturing events."}], "references": [{"title": "Automatic storage and analysis of camera trap data. The Bulletin of the Ecological Society of America 91(3):352\u2013360", "author": ["G Harris", "R Thompson", "JL Childs", "JG Sanderson"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Camera traps in animal ecology: methods and analyses. (Springer Science & Business Media)", "author": ["AF O\u2019Connell", "JD Nichols", "KU Karanth"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Camera trap, line transect census and track surveys: a comparative evaluation", "author": ["L Silveira", "AT Jacomo", "JAF Diniz-Filho"], "venue": "Biological Conservation", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "The use of camera-trap data to model habitat use by antelope species in the udzungwa mountain forests, tanzania", "author": ["AE Bowkett", "F Rovero", "AR Marshall"], "venue": "African Journal of Ecology 46(4):479\u2013487", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "A previously unsurveyed forest in the rubeho mountains of tanzania reveals new species and range records", "author": ["F Rovero"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Data acquisition and management software for camera trap data: A case study from the team network. Ecological Informatics 6(6):345\u2013353", "author": ["EH Fegraus"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Software for minimalistic data management in large camera trap studies", "author": ["YS Krishnappa", "WC Turner"], "venue": "Ecological informatics 24:11\u201316", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A novel method to reduce time investment when processing videos from camera trap studies", "author": ["KRR Swinnen", "J Reijniers", "M Breno", "H Leirs"], "venue": "PLOS ONE 9(6):1\u20137", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Snapshot serengeti, high-frequency annotated camera trap images of 40 mammalian species in an african savanna", "author": ["A Swanson"], "venue": "Scientific data", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "A (2016) Deep learning. Book in preparation for", "author": ["I Goodfellow", "Y Bengio", "Courville"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Some studies in machine learning using the game of checkers", "author": ["AL Samuel"], "venue": "IBM Journal of research and development 3(3):210\u2013229", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1959}, {"title": "A (2012) Foundations of machine learning", "author": ["M Mohri", "A Rostamizadeh", "Talwalkar"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Deep convolutional neural networks for hyperspectral image classification", "author": ["W Hu", "Y Huang", "L Wei", "F Zhang", "H Li"], "venue": "Journal of Sensors", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G Hinton"], "venue": "IEEE Signal Processing Magazine 29(6):82\u201397", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "New types of deep neural network learning for speech recognition and related applications: An overview", "author": ["L Deng", "G Hinton", "B Kingsbury"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing. (IEEE),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "End-to-end attention-based large vocabulary speech recognition in 2016", "author": ["D Bahdanau", "J Chorowski", "D Serdyuk", "Y Bengio"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). (IEEE),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K Cho"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K He", "X Zhang", "S Ren", "J Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K Simonyan", "A Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V Mnih"], "venue": "Nature", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["MD Zeiler", "R Fergus"], "venue": "European conference on computer vision. (Springer),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Fast automatic detection of wildlife in images from trap cameras in Iberoamerican Congress on Pattern Recognition", "author": ["K Figueroa", "A Camarena-Ibarrola", "J Garc\u00eda", "HT Villela"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Master\u2019s thesis (University of Alberta)", "author": ["B Wang"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Automated identification of animal species in camera trap images", "author": ["X Yu"], "venue": "EURASIP Journal on Image and Video Processing", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Deep convolutional neural network based species recognition for wild animal monitoring", "author": ["G Chen", "TX Han", "Z He", "R Kays", "T Forrester"], "venue": "IEEE International Conference on Image Processing (ICIP). (IEEE),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Linear spatial pyramid matching using sparse coding for image classification in Computer Vision and Pattern Recognition", "author": ["J Yang", "K Yu", "Y Gong", "T Huang"], "venue": "IEEE Conference on. (IEEE),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Latent dirichlet allocation. Journal of machine Learning research 3(Jan):993\u20131022", "author": ["DM Blei", "AY Ng", "MI Jordan"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2003}, {"title": "A bayesian hierarchical model for learning natural scene categories", "author": ["L Fei-Fei", "P Perona"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905). (IEEE),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "A (2016) Animal identification in low quality camera-trap images using very deep convolutional neural networks and confidence thresholds", "author": ["A Gomez", "G Diez", "A Salazar", "Diaz"], "venue": "in International Symposium on Visual Computing", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "A (2016) Towards automatic wild animal monitoring: Identification of animal species in camera-trap images using very deep convolutional neural networks. arXiv preprint arXiv:1603.06169", "author": ["Gomez A", "Salazar"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J Deng"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks in Advances in neural information processing systems", "author": ["A Krizhevsky", "I Sutskever", "GE Hinton"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Identifying and attacking the saddle point problem in highdimensional non-convex optimization in Advances in neural information processing systems", "author": ["YN Dauphin"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Going deeper with convolutions in", "author": ["C Szegedy"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "How transferable are features in deep neural networks? in Advances in neural information processing systems", "author": ["J Yosinski", "J Clune", "Y Bengio", "H Lipson"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Transfer learning. Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques 1:242", "author": ["L Torrey", "J Shavlik"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}, {"title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks in Advances in Neural Information Processing Systems", "author": ["A Nguyen", "A Dosovitskiy", "J Yosinski", "T Brox", "J Clune"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks in Visualization for Deep Learning Workshop, ICML conference", "author": ["A Nguyen", "J Yosinski", "J Clune"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition in Neurocomputing", "author": ["JS Bridle"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1990}, {"title": "Efficient backprop in Neural networks: Tricks of the trade", "author": ["YA LeCun", "L Bottou", "GB Orr", "KR M\u00fcller"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "A convergence analysis of log-linear training in Advances in Neural Information Processing Systems", "author": ["S Wiesler", "H Ney"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Torch: a modular machine learning software library, (Idiap), Technical report", "author": ["R Collobert", "S Bengio", "J Mari\u00e9thoz"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "ni [1]", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "wi = fi \u221148 i=1 fi [2]", "startOffset": 19, "endOffset": 22}], "year": 2017, "abstractText": "Having accurate, detailed, and up-to-date information about wildlife location and behavior across broad geographic areas would revolutionize our ability to study, conserve, and manage species and ecosystems. Currently, such data are mostly gathered manually at great expense, and thus are sparsely and infrequently collected. Here we investigate the ability to automatically, accurately, and inexpensively collect such data, which could transform many fields of biology, ecology, and zoology into \u201cbig data\u201d sciences. Motion sensor cameras called \u201ccamera traps\u201d enable pictures of wildlife to be collected inexpensively, unobtrusively, and at high-volume. However, identifying the animals, animal attributes, and behaviors in these pictures remains an expensive, time-consuming, manual task often performed by researchers, hired technicians, or crowdsourced teams of human volunteers. In this paper, we demonstrate that such data can be automatically extracted by deep neural networks (aka deep learning), which is a cutting-edge type of artificial intelligence. In particular, we use the existing human-labeled, single-animal images from the Snapshot Serengeti dataset to train deep convolutional neural networks for identifying 48 species in 3.2 million images taken from Tanzania\u2019s Serengeti National Park. In this paper we train neural networks that automatically identify animals with over 92% accuracy, and we expect that number to improve rapidly in years to come. More importantly, we can choose to have our system classify only the images it is highly confident about, allowing valuable human time to be focused only on challenging images. In this case, our system can automate animal identification for 96.9% of the data while still performing at the same 96.6% accuracy level of crowdsourced teams of human volunteers, saving approximately \u223c8.2 years (at 40 hours per week) of human labeling effort (i.e. over 17,000 hours) on a 3.2million-image dataset. Those efficiency gains immediately highlight the importance of using deep neural networks to automate data extraction from camera-trap images. The improvements in accuracy we expect in years to come suggest that this technology could enable the inexpensive, unobtrusive, high-volume and perhaps even realtime collection of information about vast numbers of animals in the wild.", "creator": "LaTeX with hyperref package"}}}