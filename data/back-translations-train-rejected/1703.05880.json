{"id": "1703.05880", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2017", "title": "Empirical Evaluation of Parallel Training Algorithms on Acoustic Modeling", "abstract": "Deep learning models (DLMs) are state-of-the-art techniques in speech recognition. However, training good DLMs can be time consuming especially for production-size models and corpora. Although several parallel training algorithms have been proposed to improve training efficiency, there is no clear guidance on which one to choose for the task in hand due to lack of systematic and fair comparison among them. In this paper we aim at filling this gap by comparing four popular parallel training algorithms in speech recognition, namely asynchronous stochastic gradient descent (ASGD), blockwise model-update filtering (BMUF), bulk synchronous parallel (BSP) and elastic averaging stochastic gradient descent (EASGD), on 1000-hour LibriSpeech corpora using feed-forward deep neural networks (DNNs) and convolutional, long short-term memory, DNNs (CLDNNs). Based on our experiments, we recommend using BMUF as the top choice to train acoustic models since it is most stable, scales well with number of GPUs, can achieve reproducible results, and in many cases even outperforms single-GPU SGD. ASGD can be used as a substitute in some cases.", "histories": [["v1", "Fri, 17 Mar 2017 03:38:48 GMT  (298kb,D)", "https://arxiv.org/abs/1703.05880v1", null], ["v2", "Wed, 26 Jul 2017 06:29:54 GMT  (305kb,D)", "http://arxiv.org/abs/1703.05880v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["wenpeng li", "binbin zhang", "lei xie", "dong yu"], "accepted": false, "id": "1703.05880"}, "pdf": {"name": "1703.05880.pdf", "metadata": {"source": "CRF", "title": "Empirical Evaluation of Parallel Training Algorithms on Acoustic Modeling", "authors": ["Wenpeng Li", "Binbin Zhang", "Lei Xie", "Dong Yu"], "emails": ["lxie}@nwpu-aslp.org,", "dongyu@ieee.org"], "sections": [{"heading": "1. Introduction", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "2. Parallel training algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. BSP", "text": "The Bulk Synchronous Parallel Algorithm (GNP) [33] is often referred to as the model mean. In this model, the data is distributed across multiple workers. Each worker updates his local model independently using his own share of data with SGD. Periodically, the local models are averaged and the generated global model is synchronized across the workforce. We refer to joke as the local model of the i-th worker at the time. The global model w = t isar Xiv: 170 3.05 880v 2 [cs.C L] 26 July 2 017 calculates asw: t = w = 1N = 1 wit, (1) where N is the number of local workers and w = t is the average model of local models. This algorithm is easy to implement and can achieve linear acceleration if communication costs can be ignored (e.g. with large synchronous time) at the expense of detection accuracy, especially when the number of workers grows large."}, {"heading": "2.2. ASGD", "text": "The asynchronous stochastic gradient descent (ASGD) algorithm is the distributed version of SGD. [40] It has been proven that ASGD converges for convex problems. As shown in Figure 1, ASGD uses a parameter separator and several local workers. Each worker independently and asynchronously pulls the latest global model from the parameter server, calculates the gradient drop with a new minibatch and sends it back to the parameter server. The parameter server always keeps the current model. If it receives the gradient matching from worker i, it generates the new model t + k + 1 = w \u0430t + k \u2212 with (2) where it is the learning rate. Before worker i sends gradient deviations back to the parameter server, some other workers may have already added their local gradient deviations to the model and updated the model k \u2212 times to essentially achieve a \"delayed\" gradient deviation with the parameter server sometimes not stable on the basis of the SD."}, {"heading": "2.3. BMUF", "text": "The block-by-block Model Update Filter (BMUF) algorithm [32] can be considered an improved method of averaging the model, where the global Model Update is implemented as a filter.In the BMUF, the complete Training Set D is divided into non-overlapping M blocks and each block is further divided into non-overlapping N splits, with N being the number of manpower. Each worker updates his local model with its share of data. The N optimized local models are then averaged by equation. (1) Unlike BSP, where the average model w-t is treated as a global model, BMUF generates the global model w-t = w-t-1 + equilibrium value, (3) where T = 1 + equilibrium value, 0 \u2264 equilibrium value < 1, \u03b7 > 0, (4) the global model is w-t = w-t-1 + equilibrium value, (3) at T-T."}, {"heading": "2.4. EASGD", "text": "In the elastic mean stochastic gradient decrease (EASGD) [37], the loss function is defined as min w1t,..., w N t, w i = 1 f (D \u2212 wit) + \u03bb 2 | wit \u2212 w = 2 (7), where D is the training theorem, f (.) is the loss function for local sequential training, \u03bb is a hyperparameter for the square penalty term, joke represents the model for the i-th worker, and w \u00b2 t represents the global model. From Equation (7) we find that EASGD minimizes the loss that is added up over all workers, as well as the square difference between the global model and the local models."}, {"heading": "2.5. Relationships between algorithms", "text": "These four algorithms, though different, have relationships. First, ASGD and EASGD are asynchronous algorithms based on the client / server framework, in which the global model is stored and updated by a parameter server, and each worker calculates gradients and updates his local model independently of each other. Second, in ASGD, the global model is updated based on the local gradients calculated and sent by workers. In BSP, EASGD and BMUF, the global model is a weighted sum of local models instead of gradients. Third, EASGD and BMUF both introduce additional hyperparameters whose values can influence training behavior."}, {"heading": "3. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Experimental setup", "text": "In this thesis, all models are trained on the 1000hr LibriSpeech [39] dataset. 40-Dim FBANK functions, which are calculated on a 25ms window, are shifted by 10ms. Lexicon and Language Model (LM) are provided by the dataset. Specifically, the results here are all achieved with a full 3-gram LM. We used Test-Clean and Test-Other sets for evaluation. To evaluate the parallel training algorithms, we trained two types of DLMs: DNNs and CLDNNs [23]. Input to DNNs is the 40-Dim FBANK function with the first and second time derivatives and 11 frame contexts. Input to CLDNNs is the same as to DNNs, but without the second order time derivatives. The DNN has six hidden layers, each containing 1024 neurons."}, {"heading": "3.2. Experimental results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1. DNN results", "text": "Table 1 compares training acceleration and word error rate (WHO) on test sets with four parallel training algorithms on 4 GPUs and 8 GPUs. Using the same synchronization period, EASGD achieved the best training acceleration, followed by ASGD, BMUF and BSP. However, BMUF achieved the best WHO in 4 and 8 GPU cases and even exceeded the single GPU SGD. As Figure 2 shows, the convergence trend of BMUF is similar to the minibatch SGD with single GPU in the CV. To further verify our conclusions, we chose the most appropriate synchronization period for each algorithm based on literature. The results in Table 2 show that BMUF still performed best."}, {"heading": "3.2.2. CLDNN results", "text": "In the CLDNN training, we calculated gradients for 100 partial sequences of different expressions at the same time. To train the models, the shortened BPTT with a circumcision step of 20 was used. For each algorithm, the appropriate synchronization period was selected on the basis of the literature. Specifically, the synchronization periods for BSP, ASGD, BMUF or EASGD are 5, 5, 80 or 64 minbatches. Table 3 shows that BMUF achieved the best WHO with 4 GPUs and the best WHO with 4 GPUs and ASGD the best WHO."}, {"heading": "3.2.3. Synchronization period", "text": "As Table 4 shows, the WHO of the ASGD has gradually increased and the training has even diverged with increasing \u03c4. This is because the ASGD suffers from the problem of graduated gradient updating and longer synchronization periods lead to higher latency. In contrast, we did not observe any decrease in BMUF performance in the variation of the synchronization period. As far as training acceleration is concerned, parameter exchange between multi-GPUs is quite frequent with short duration of synchronization and the communication overhead is the biggest bottleneck in training speed. Therefore, the communication overhead (from 5 to 20 mini-batches in Table 4) decreases and training acceleration increases. If the value of the communication overhead continues to increase, the communication overhead decreases so that the computing speed becomes the biggest bottleneck. Therefore, the training acceleration remains virtually unchanged while the synchronization period increases (from 20 to 4 mini-batches)."}, {"heading": "3.2.4. Minibatch size", "text": "Table 5 compares three different minibatch sizes in BMUF. For a single GPU training, the training speed is lower if smaller minibatch sizes are used. This is because with a small minibatch size, the GPU performance is not fully utilized and the model is updated more frequently. Training speed in multi-GPU training is calculated ass = tsf (ts, N) + tc (15), where ts is the computing time through an epoch of data sets on single GPU, N is the number of GPUs, tc is the communication overhead, and f (ts, N) = \u03b1 ts N (16) is a decreasing function over N. If the minibatch can fill the GPU size, it is 1, otherwise \u03b1 is greater than 1. According to the equation. (15) and (16) we get = 1\u03b1 N + tc ts."}, {"heading": "4. Conclusions", "text": "The experimental results show that BMUF and ASGD consistently perform better than BSP and EASGD. In particular, BMUF achieved the best performance without frequent synchronization and in some cases even exceeded the single GPU SGD. We suspect that the dynamics used in the BMUF global model update correlates the global model not only with each local model, but also with the previous global model. ASGD also achieved fairly good performance and training acceleration in the warehouse with the same synchronization period. ASGD fully benefits from the asynchronous properties, is insensitive to the difference in workers \"computing capacity, but is sensitive to the synchronization period and suffers from poor reproducibility."}, {"heading": "5. Acknowledgements", "text": "This work is supported by the National Natural Science Foundation of China (grant no. 61571363)."}, {"heading": "6. References", "text": "[1] G. E. Dahl, D. Yu, L. Deng, and A. Acero, Hamid G. Omid. [Context-dependent Networks - Network-Pre-trained deep neural networks for large-vocabulary speech recognition, \"IEEE Transactions on Audio Speech & Language Processing, vol. 20, no. 1, pp. 30-42, 2012. [2] F. Seide, G. Li, and D. Yu, and D. Yu, A. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, and T. N. Sainath\" Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups, \"IEEE Signal Processing Magazine, vol. 29, pp. 82-97, Ji4, 2012. [Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups.\""}], "references": [{"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio Speech & Language Processing, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Conversational speech transcription using context-dependent deep neural networks.", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "in INTER- SPEECH,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition", "author": ["O. Abdel-Hamid", "A.-r. Mohamed", "H. Jiang", "G. Penn"], "venue": "ICASSP, 2012, pp. 4277\u20134280.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploring convolutional neural network structures and optimization techniques for speech recognition.", "author": ["O. Abdel-Hamid", "L. Deng", "D. Yu"], "venue": "INTERSPEECH,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Improvements to deep convolutional neural networks for LVCSR", "author": ["T.N. Sainath", "B. Kingsbury", "A.-r. Mohamed", "G.E. Dahl", "G. Saon", "H. Soltau", "T. Beran", "A.Y. Aravkin", "B. Ramabhadran"], "venue": "ASRU, 2013 IEEE Workshop on, 2013, pp. 315\u2013320.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep convolutional neural networks for LVCSR", "author": ["T.N. Sainath", "A.-r. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "ICASSP, 2013, pp. 8614\u20138618.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural networks for speech recognition", "author": ["O. Abdel-Hamid", "A.R. Mohamed", "H. Jiang", "L. Deng", "G. Penn", "D. Yu"], "venue": "IEEE/ACM Transactions on Audio Speech & Language Processing, vol. 22, no. 10, pp. 1533\u20131545, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural network acoustic models for ASR", "author": ["A.-r. Mohamed"], "venue": "Ph.D. dissertation, University of Toronto, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "arXiv preprint arXiv:1402.1128, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Simplifying long short-term memory acoustic models for fast training and decoding", "author": ["Y. Miao", "J. Li", "Y. Wang", "S.X. Zhang", "Y. Gong"], "venue": "ICASSP, 2016, pp. 2284\u20132288.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["H. Sak", "A. Senior", "K. Rao", "F. Beaufays"], "venue": "arXiv preprint arXiv:1507.06947, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning acoustic frame labeling for speech recognition with recurrent neural networks", "author": ["H. Sak", "A. Senior", "K. Rao", "O. Irsoy", "A. Graves", "F. Beaufays", "J. Schalkwyk"], "venue": "ICASSP, 2015, pp. 4280\u20134284.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Context dependent phone models for LSTM RNN acoustic modelling", "author": ["A. Senior", "H. Sak", "I. Shafran"], "venue": "ICASSP, 2015, pp. 4585\u20134589.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Training deep bidirectional LSTM acoustic model for LVCSR by a context-sensitive-chunk BPTT approach", "author": ["K. Chen", "Q. Huo"], "venue": "IEEE/ACM Transactions on Audio, Speech & Language Processing, vol. 24, no. 7, pp. 1185\u20131193, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Highway long short-term memory RNNs for distant speech recognition", "author": ["Y. Zhang", "G. Chen", "D. Yu", "K. Yao", "S. Khudanpur", "J. Glass"], "venue": "ICASSP, 2016, pp. 5755\u20135759.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep convolutional neural networks with layer-wise context expansion and attention", "author": ["D. Yu", "W. Xiong", "J. Droppo", "A. Stolcke", "G. Ye", "J. Li", "G. Zweig"], "venue": "INTERSPEECH, 2016, pp. 17\u201321.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "An unsupervised deep domain adaptation approach for robust speech recognition", "author": ["S. Sun", "B. Zhang", "L. Xie", "Y. Zhang"], "venue": "Neurocomputing, 2017.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Advances in very deep convolutional neural networks for LVCSR", "author": ["T. Sercu", "V. Goel"], "venue": "INTERSPEECH, 2016, pp. 3429\u20133433.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Feedforward sequential memory networks: A new structure to learn long-term dependency", "author": ["S. Zhang", "C. Liu", "H. Jiang", "S. Wei", "L. Dai", "Y. Hu"], "venue": "arXiv preprint arXiv:1512.08301, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Compact feedforward sequential memory networks for large vocabulary continuous speech recognition", "author": ["S. Zhang", "H. Jiang", "S. Xiong", "S. Wei", "L.R. Dai"], "venue": "INTERSPEECH, 2016, pp. 3389\u20133393.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "ICASSP, 2015, pp. 4580\u20134584.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Q.V. Le"], "venue": "NIPS, 2012, pp. 1223\u20131231.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning with COTS HPC systems", "author": ["A. Coates", "B. Huval", "T. Wang", "D.J. Wu", "A.Y. Ng", "B. Catanzaro"], "venue": "ICML, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs", "author": ["F. Seide", "H. Fu", "J. Droppo", "G. Li", "D. Yu"], "venue": "INTERSPEECH, 2014, pp. 1058\u20131062.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J.C. Duchi"], "venue": "NIPS, 2011, pp. 873\u2013881.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Asynchronous stochastic optimization for sequence training of deep neural networks", "author": ["G. Heigold", "E. Mcdermott", "V. Vanhoucke", "A. Senior"], "venue": "ICASSP, 2014, pp. 5587\u20135591.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Asynchronous stochastic gradient descent for DNN training", "author": ["S. Zhang", "C. Zhang", "Z. You", "R. Zheng"], "venue": "ICASSP, 2013, pp. 6660\u20136663.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "GPU asynchronous stochastic gradient descent to speed up neural network training", "author": ["T. Paine", "H. Jin", "J. Yang", "Z. Lin", "T. Huang"], "venue": "arXiv preprint arXiv:1312.6186, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering", "author": ["K. Chen", "Q. Huo"], "venue": "ICASSP, 2016, pp. 5880\u20135884.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "A bridging model for parallel computation", "author": ["L.G. Valiant"], "venue": "Communications of the ACM, vol. 33, no. 8, pp. 103\u2013111, 1990.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1990}, {"title": "Theano-mpi: a theano-based distributed training framework", "author": ["H. Ma", "F. Mao", "G.W. Taylor"], "venue": "arXiv preprint arXiv:1605.08325, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Parallel training of DNNs with natural gradient and parameter averaging", "author": ["D. Povey", "X. Zhang", "S. Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Experiments on parallel training of deep neural network using model averaging", "author": ["H. Su", "H. Chen"], "venue": "arXiv preprint arXiv:1507.01239, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning with elastic averaging SGD", "author": ["S. Zhang", "A.E. Choromanska", "Y. LeCun"], "venue": "NIPS, 2015, pp. 685\u2013693.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "ASRU, 2011 IEEE workshop on, no. EPFL-CONF-192584, 2011.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Librispeech: an ASR corpus based on public domain audio books", "author": ["V. Panayotov", "G. Chen", "D. Povey", "S. Khudanpur"], "venue": "ICASSP, 2015, pp. 5206\u20135210.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 12, no. 7, pp. 2121\u20132159, 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Asynchronous stochastic gradient descent with delay compensation for distributed deep learning", "author": ["S. Zheng", "Q. Meng", "T. Wang", "W. Chen", "N. Yu", "Z.-M. Ma", "T.-Y. Liu"], "venue": "arXiv preprint arXiv:1609.08326, 2016.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 152, "endOffset": 161}, {"referenceID": 1, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 152, "endOffset": 161}, {"referenceID": 2, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 152, "endOffset": 161}, {"referenceID": 3, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 325, "endOffset": 343}, {"referenceID": 4, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 325, "endOffset": 343}, {"referenceID": 5, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 325, "endOffset": 343}, {"referenceID": 6, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 325, "endOffset": 343}, {"referenceID": 7, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 325, "endOffset": 343}, {"referenceID": 8, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 325, "endOffset": 343}, {"referenceID": 9, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 408, "endOffset": 440}, {"referenceID": 10, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 408, "endOffset": 440}, {"referenceID": 11, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 408, "endOffset": 440}, {"referenceID": 12, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 408, "endOffset": 440}, {"referenceID": 13, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 408, "endOffset": 440}, {"referenceID": 14, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 408, "endOffset": 440}, {"referenceID": 15, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 408, "endOffset": 440}, {"referenceID": 16, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 460, "endOffset": 484}, {"referenceID": 17, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 460, "endOffset": 484}, {"referenceID": 18, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 460, "endOffset": 484}, {"referenceID": 19, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 460, "endOffset": 484}, {"referenceID": 20, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 460, "endOffset": 484}, {"referenceID": 21, "context": "Since 2010, the year in which deep neural networks (DNNs) were successfully applied to the large vocabulary continuous speech recognition (LCVSR) tasks [1, 2, 3] and led to significant recognition accuracy improvement over the then state of the art, various deep learning models, such as convolutional neural networks (CNNs) [4, 5, 6, 7, 8, 9], long short-term memory (LSTM) recurrent neural networks (RNNs) [10, 11, 12, 13, 14, 15, 16, 17] and their variants [18, 19, 20, 21, 22, 23], have been developed to further improve the performance of automatic speech recognition (ASR) systems.", "startOffset": 460, "endOffset": 484}, {"referenceID": 22, "context": ", [24, 25]), which exploits and splits the structure of neural networks to distribute computation across GPUs, and data parallelism (e.", "startOffset": 2, "endOffset": 10}, {"referenceID": 23, "context": ", [24, 25]), which exploits and splits the structure of neural networks to distribute computation across GPUs, and data parallelism (e.", "startOffset": 2, "endOffset": 10}, {"referenceID": 22, "context": ", [24, 26, 27]), which splits and distributes data across GPUs to achieve speedup.", "startOffset": 2, "endOffset": 14}, {"referenceID": 24, "context": ", [24, 26, 27]), which splits and distributes data across GPUs to achieve speedup.", "startOffset": 2, "endOffset": 14}, {"referenceID": 25, "context": ", [24, 26, 27]), which splits and distributes data across GPUs to achieve speedup.", "startOffset": 2, "endOffset": 14}, {"referenceID": 26, "context": "Several successful techniques, such as asynchronous stochastic gradient descent (ASGD) [29, 30, 31], blockwise modelupdate filtering (BMUF) [32], bulk synchronous parallel (BSP) [33, 34, 35, 36], 1-bit SGD [26] and elastic averaging stochastic gradient descent (EASGD) [37], have been proposed recently.", "startOffset": 87, "endOffset": 99}, {"referenceID": 27, "context": "Several successful techniques, such as asynchronous stochastic gradient descent (ASGD) [29, 30, 31], blockwise modelupdate filtering (BMUF) [32], bulk synchronous parallel (BSP) [33, 34, 35, 36], 1-bit SGD [26] and elastic averaging stochastic gradient descent (EASGD) [37], have been proposed recently.", "startOffset": 87, "endOffset": 99}, {"referenceID": 28, "context": "Several successful techniques, such as asynchronous stochastic gradient descent (ASGD) [29, 30, 31], blockwise modelupdate filtering (BMUF) [32], bulk synchronous parallel (BSP) [33, 34, 35, 36], 1-bit SGD [26] and elastic averaging stochastic gradient descent (EASGD) [37], have been proposed recently.", "startOffset": 87, "endOffset": 99}, {"referenceID": 29, "context": "Several successful techniques, such as asynchronous stochastic gradient descent (ASGD) [29, 30, 31], blockwise modelupdate filtering (BMUF) [32], bulk synchronous parallel (BSP) [33, 34, 35, 36], 1-bit SGD [26] and elastic averaging stochastic gradient descent (EASGD) [37], have been proposed recently.", "startOffset": 140, "endOffset": 144}, {"referenceID": 30, "context": "Several successful techniques, such as asynchronous stochastic gradient descent (ASGD) [29, 30, 31], blockwise modelupdate filtering (BMUF) [32], bulk synchronous parallel (BSP) [33, 34, 35, 36], 1-bit SGD [26] and elastic averaging stochastic gradient descent (EASGD) [37], have been proposed recently.", "startOffset": 178, "endOffset": 194}, {"referenceID": 31, "context": "Several successful techniques, such as asynchronous stochastic gradient descent (ASGD) [29, 30, 31], blockwise modelupdate filtering (BMUF) [32], bulk synchronous parallel (BSP) [33, 34, 35, 36], 1-bit SGD [26] and elastic averaging stochastic gradient descent (EASGD) [37], have been proposed recently.", "startOffset": 178, "endOffset": 194}, {"referenceID": 32, "context": "Several successful techniques, such as asynchronous stochastic gradient descent (ASGD) [29, 30, 31], blockwise modelupdate filtering (BMUF) [32], bulk synchronous parallel (BSP) [33, 34, 35, 36], 1-bit SGD [26] and elastic averaging stochastic gradient descent (EASGD) [37], have been proposed recently.", "startOffset": 178, "endOffset": 194}, {"referenceID": 33, "context": "Several successful techniques, such as asynchronous stochastic gradient descent (ASGD) [29, 30, 31], blockwise modelupdate filtering (BMUF) [32], bulk synchronous parallel (BSP) [33, 34, 35, 36], 1-bit SGD [26] and elastic averaging stochastic gradient descent (EASGD) [37], have been proposed recently.", "startOffset": 178, "endOffset": 194}, {"referenceID": 24, "context": "Several successful techniques, such as asynchronous stochastic gradient descent (ASGD) [29, 30, 31], blockwise modelupdate filtering (BMUF) [32], bulk synchronous parallel (BSP) [33, 34, 35, 36], 1-bit SGD [26] and elastic averaging stochastic gradient descent (EASGD) [37], have been proposed recently.", "startOffset": 206, "endOffset": 210}, {"referenceID": 34, "context": "Several successful techniques, such as asynchronous stochastic gradient descent (ASGD) [29, 30, 31], blockwise modelupdate filtering (BMUF) [32], bulk synchronous parallel (BSP) [33, 34, 35, 36], 1-bit SGD [26] and elastic averaging stochastic gradient descent (EASGD) [37], have been proposed recently.", "startOffset": 269, "endOffset": 273}, {"referenceID": 35, "context": "All the four algorithms were implemented in Kaldi toolkit [38] using message passing interface (MPI) for parameter exchange across GPUs.", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "To evaluate these algorithms, we train DNNs and CLDNNs [23] (an architecture that stacks CNNs, LSTMs and DNNs) on 1000hr LibriSpeech [39] corpus.", "startOffset": 55, "endOffset": 59}, {"referenceID": 36, "context": "To evaluate these algorithms, we train DNNs and CLDNNs [23] (an architecture that stacks CNNs, LSTMs and DNNs) on 1000hr LibriSpeech [39] corpus.", "startOffset": 133, "endOffset": 137}, {"referenceID": 30, "context": "The bulk synchronous parallel (BSP) [33] algorithm is often referred to as model averaging.", "startOffset": 36, "endOffset": 40}, {"referenceID": 37, "context": "It is proved [40] that ASGD converges for convex problems.", "startOffset": 13, "endOffset": 17}, {"referenceID": 38, "context": "Therefore ASGD essentially adds a \u201cdelayed\u201d gradient \u2207w t computed based on the model w\u0303t to the model w\u0303t+k [41].", "startOffset": 109, "endOffset": 113}, {"referenceID": 29, "context": "The blockwise model-update filtering (BMUF) algorithm [32] can be considered as an improved model averaging technique in which the global model update is implemented as a filter.", "startOffset": 54, "endOffset": 58}, {"referenceID": 29, "context": "We implemented CBM-BMUF [32] in this work.", "startOffset": 24, "endOffset": 28}, {"referenceID": 34, "context": "In elastic averaging stochastic gradient descent (EASGD) [37], the loss function is defined as", "startOffset": 57, "endOffset": 61}, {"referenceID": 36, "context": "Experimental setup In this work, all the models are trained on the 1000hr LibriSpeech [39] dataset.", "startOffset": 86, "endOffset": 90}, {"referenceID": 21, "context": "To evaluate the parallel training algorithms, we trained two types of DLMs: DNNs and CLDNNs [23].", "startOffset": 92, "endOffset": 96}], "year": 2017, "abstractText": "Deep learning models (DLMs) are state-of-the-art techniques in speech recognition. However, training good DLMs can be time consuming especially for production-size models and corpora. Although several parallel training algorithms have been proposed to improve training efficiency, there is no clear guidance on which one to choose for the task in hand due to lack of systematic and fair comparison among them. In this paper we aim at filling this gap by comparing four popular parallel training algorithms in speech recognition, namely asynchronous stochastic gradient descent (ASGD), blockwise model-update filtering (BMUF), bulk synchronous parallel (BSP) and elastic averaging stochastic gradient descent (EASGD), on 1000-hour LibriSpeech corpora using feed-forward deep neural networks (DNNs) and convolutional, long short-term memory, DNNs (CLDNNs). Based on our experiments, we recommend using BMUF as the top choice to train acoustic models since it is most stable, scales well with number of GPUs, can achieve reproducible results, and in many cases even outperforms singleGPU SGD. ASGD can be used as a substitute in some cases.", "creator": "LaTeX with hyperref package"}}}