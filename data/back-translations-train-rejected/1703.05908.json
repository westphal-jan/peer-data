{"id": "1703.05908", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2017", "title": "Learning Robust Visual-Semantic Embeddings", "abstract": "Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and few-shot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.", "histories": [["v1", "Fri, 17 Mar 2017 06:59:51 GMT  (8099kb,D)", "http://arxiv.org/abs/1703.05908v1", "12 pages"], ["v2", "Mon, 20 Mar 2017 00:28:07 GMT  (8100kb,D)", "http://arxiv.org/abs/1703.05908v2", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["yao-hung hubert tsai", "liang-kang huang", "ruslan salakhutdinov"], "accepted": false, "id": "1703.05908"}, "pdf": {"name": "1703.05908.pdf", "metadata": {"source": "CRF", "title": "Learning Robust Visual-Semantic Embeddings", "authors": ["Yao-Hung Hubert Tsai", "Liang-Kang Huang", "Ruslan Salakhutdinov"], "emails": ["yaohungt@cs.cmu.edu", "liangkah@andrew.cmu.edu", "rsalakhu@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2. Related Work", "text": "In this context, it is also worth mentioning the fact that the two are two groups that have been active in the most diverse areas of the world over the past two decades: in the USA, in Europe, in Europe, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA,"}, {"heading": "3. Proposed Method", "text": "s leave Vtr = {v (tr) i} Ntr i = 1 for labeled training images from Ctr classes, Vut = {v (ut) i} Nut i = 1 for unlabeled training images from possibly different classes and Vte = {v (te) i} Nte i = 1 for test images from novel classes from Cte. We designate these class-specific text properties according to [49, 50, 51, 2, 48, 7] as Ttr = {t (tr) c} Ctrc = 1, Tut = {t (ut) c} Cutc = 1, and Tte = {t (te) c} Ctec = 1 for labeled training and test classes."}, {"heading": "3.1. Basic Formulation", "text": "The goal of learning multimodal embedding can be formulated as learning transformation functions fv and ft, so that, given an image v and a textual attribute t fv (v), much of the previous work on learning multimodal embedding can be generalized to this formulation. For example, fv (\u00b7) in Cross-Modal Transfer (CMT) [41] can be considered a predefined feature extraction model, followed by a two-layered neural network, while ft (\u00b7) is placed on an identity matrix. To be more precise, [41] the aim is to learn a nonlinear projection directly of visual characteristics onto semantic word vectors. In recent years, it has been shown that deep architectures learn useful representations that build a high-grade semantics for both visual and textual data embedding \u00b7 v (to seek successful architectures), to apply this to large-scale (9) architectures."}, {"heading": "3.2. Reconstructing Features from Auto-Encoder", "text": "To be more precise, we propose to combine monitored and unattended learning objectives by incorporating auto-encoders [4] for both image and text data. In our model, the auto-encoders are added after the image and text data has been processed by the pre-trained networks. \u2212 For learning visual embedding, we use contractive auto-encoders that could be used to reconstruct the original input data. \u2212 In our model, the auto-encoders are added after the image and text data have been processed by the pre-trained networks. \u2212 In our model, we use contractive auto-encoders that are capable of learning more robust visual codes for images of the same class."}, {"heading": "3.3. Cross-Modality Distributions Matching", "text": "A common non-parametric method for analyzing and comparing distributions is the use of the criterion of maximum mean discrepancy (MMD) [11]. We can consider MMD as a two-sample test for v-h and th, and therefore its loss as LMMD = \u0445 Ep [? h (v-h)] \u2212 Eq [? (th)] 2Hk, (5) where p, q are the distributions of visual and textual embeddings (i.e. v-h-p and th-q), \u03c6 is the characteristic chart with canonical form \u03c6 (x) = k (x, \u00b7), and Hk is the reproducing core Hilbert space (RKHS), which is equipped with a characteristic core k."}, {"heading": "3.4. Learning", "text": "After deriving the hidden representations v-h and th, the transformation functions fv (\u00b7 q =) and ft (\u00b7) can be reformulated (\u00b7 q = 11), where f-v (\u00b7) and f-t-t (th), (7) where f-v (\u00b7) and f-t (\u00b7) are the mapping functions from the hidden representations on the visual and textual output. To minimize the verified information from the described Vtr educational images and the corresponding textual attributes Ttr, we use Lsupervised = \u2212 1Ntr Ntr-b \"i = 1 Ctr-f\" v (tr) h, i), f-t-t (tr) h (tr) h, c), (8) where Ii, c \"encoding of positive and negative classes and < > denotes a Dot product that we can accept."}, {"heading": "4. Experiments", "text": "In the experiments, we refer to our proposed method as ReViSE (Robust sEmi-supervised Visual-Semantic Embeddings). Extensive experiments to detect and retrieve images with zero and few settings are carried out with two benchmark datasets: Animals with Attributes (AwA) [21] and Caltech-UCSD Birds 200-2011 (CUB) [47]. CUB is a fine-grained dataset in which the objects are very similar both visually and semantically, while AwA is a more general concept dataset. We use the same training (+ validation) / test splits as in [2, 48]. Table 1 lists the statistics of the datasets. To verify the performance of our method, we consider two state-of-the-art deep embeddings methods: CMT [41] and DeViSE [9]."}, {"heading": "4.1. Network Design and Training Procedure", "text": "Please read in addition to the design details of ReViSE and its parameters. Please note that we report on an average of 10 random studies."}, {"heading": "4.2. Zero-Shot Learning", "text": "In fact, most of them are able to play by the rules."}, {"heading": "4.3. Transductive Zero-Shot Learning", "text": "In this section we expand our experiments to a transductive setting in which test data is available during the training. Therefore, the test data can now be considered as the unlabeled training data (Vtr = Vut and Ttr = Tut). To perform the experiments, we divide AwA data sets into 40 / 10 disjoint classes and CUB dataset into 150 / 50 disjoint classes for labeled training / test dataset. To evaluate the different components in ReViSE, we offer other variants: ReViSE \u2020 and ReViSE \u2020 \u2020 ReViSE is when we do not consider distributional agreement between the codes on modalities (\u03b2 = 0). ReViSE \u2020 \u2020 \u2020 is when we do not consider contractive loss in our visual auto encoders (\u03b2 = 0)."}, {"heading": "4.4. From Zero to Few-Shot Learning", "text": "In this subsection, we expand our experiments from transductive zero-shot learning to transductive low-shot learning. Compared to zero-shot learning, low-shot learning allows us to have a few labeled images in our test classes. Here, 3 images are randomly selected to be labeled per test category. We use the same performance comparison metrics as in Section 4.2 to report the outcomes. Transductive low-shot detection and retrieval: Tables 6 and 7 list the results of the transductive low-shot detection and retrieval tasks. In general, ReViSE performs best compared to its variants and other methods. In addition, as expected, when we compare the results with transductive zero-shot detection (Table 9) and retrieval (Table 5), each method achieves better results when only a few (i.e. 3) labeled images are observed in the test classes."}, {"heading": "4.5. t-SNE Visualization", "text": "Figure 5 also shows the t-SNE [26] visualization for the original CNN features, the reconstructed visual features rv (v \u207b (te)), and the visual codes v (te) h on AwA datasets with glo attributes under transductive zero-shot setting. First, it should be noted that both the reconstructed features and the visual codes have more separate clusters across different classes, suggesting that ReViSE has learned useful visualizations. Another interesting observation is that affinities between classes may change after learning visual codes. For example, the \"Leopard\" images (green dots) are located near \"humpback whale images\" (light violet dots) in the original CNN feature space. However, in the visual code space, leopard images are far removed from humpback images. One possible explanation is that we know that leopards are semantically distinguishable from humpback whale dots."}, {"heading": "5. Conclusion", "text": "In this paper, we have demonstrated how to complement a typical supervised formulation with unattended techniques for learning shared embedding of visual and textual data. We evaluate our proposed method empirically using both generic and fine-grained image classification data sets and compare it with modern methods in zero-shot and little-shot recognition and retrieval tasks, from inductive to transductive environments. In all experiments, our method consistently outperforms other methods and in some cases significantly improves performance. We believe that this work highlights the benefits of combining supervised and unsupervised learning techniques and takes a step toward learning more useful representations from multimodal data."}, {"heading": "6. Network Design", "text": "In all our experiments, GoogLeNet is pre-trained on ImageNet [1] images. Without fine-tuning, we directly extract the activations of the top layer (1024 dim) as our image input functions, followed by a common log (1 + v) pre-processing step. For the textual attributes, we process them through a standard l2 normalization. In ReViSE, we set \u03b1 = 1.0 in eq. (11), so that we attach the same importance to the monitored and unattended lenses (1 + v). For the visual auto-encoders, we edit the parameters of the contraction strength \u03b3 = 0.1 in eq. (2). The coding of visual features is parameterized by a two-layer layer, which we connect to a digital layer."}, {"heading": "7. Parameters Choice", "text": "We have four parameters in our architecture: \u03b1, \u03b2, \u03b3 and \u0443. We fix \u03b1 = 1.0, \u03b3 = 0.1, \u03ba = 32.0 for all experiments. Then, we set \u03bb = 0.0 (no unattended data fit conclusion) and perform cross validations as suggested in [3,46] to determine \u03b2 from {0.1, 1.0}. Next, we perform selected \u03b2 cross validations to select from {0.1, 1.0}. Table 8 lists the statistics of \u03b2 and \u03bb.Next, we examine the power of unattended information. We now take the CUB dataset with discount attributes to test the benefit of using unattended information, which can be considered tuning the \u03b1 parameter for the unattended target in Eq. (11). Originally, \u03b1 was set to 1.0, thus equally weighting the contribution of supervised and unattended losses."}, {"heading": "8. Precision-Recall Curve", "text": "Fig. 10 is the accuracy retrieval curve for zero-shot retrieval results on CUB datasets with att attributes."}, {"heading": "9. MMD Distance", "text": "For CUB datasets with att attributes in the transductive zero-shot experiment, we calculate the MMD distance (on the test codes) in our method with (ReViSE) and without (ReViSE \u2020) LMMD. The results of the MMD distance without the number of iterations are in Fig. 9. We clearly note that the red curve (ReViSE) consistently has a lower value than the blue curve (ReViSE \u2020). Furthermore, based on the previous results, ReViSE performs better than ReViSE \u2020. Therefore, an alignment of the distributions between visual and textual codes can lead to a better linkage of the modal information and thus to more robust visual-semantic embedding."}, {"heading": "10. Remarks on Contractive Loss", "text": "We find that adding a contractive loss to the textual autoencoder is of little use. One possible reason could be the limited number of textual attributes (200 for CUB). On the other hand, the number of visual attributes is large (11, 786 for CUB)."}, {"heading": "11. Comparing with recent state-of-the-art methods", "text": "In our main work we focus on comparison with specialization methods. In Table 9 we compare other methods for inductive and transductive zero-shot learning. Note that SMSESZSL uses ESZSL for its initialization."}], "references": [{"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng"], "venue": "IEEE CVPR", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["Abadi"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "ICLR 2015", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "A common strategy for deriving the visual-semantic embeddings is to make use of images and textual attributes in a supervised way [41, 2, 48, 49, 50, 22, 7].", "startOffset": 130, "endOffset": 156}, {"referenceID": 0, "context": "Zero-shot [7, 1, 2] and few-shot learning [8, 39, 20] are related problems, but somewhat different in the setting of the training data.", "startOffset": 10, "endOffset": 19}, {"referenceID": 1, "context": "Zero-shot [7, 1, 2] and few-shot learning [8, 39, 20] are related problems, but somewhat different in the setting of the training data.", "startOffset": 10, "endOffset": 19}, {"referenceID": 1, "context": "A number of similar methods learn transformations from input image representations to the semantic space for the recognition or retrieval purposes [2, 49, 1, 48, 7, 50, 51, 30, 37, 6, 12].", "startOffset": 147, "endOffset": 187}, {"referenceID": 0, "context": "A number of similar methods learn transformations from input image representations to the semantic space for the recognition or retrieval purposes [2, 49, 1, 48, 7, 50, 51, 30, 37, 6, 12].", "startOffset": 147, "endOffset": 187}, {"referenceID": 1, "context": "For each class, following [49, 50, 51, 2, 48, 7], its textual attributes are either provided from human annotated attributes [21] or learned from unsupervised text corpora (Wikipedia) [32].", "startOffset": 26, "endOffset": 48}, {"referenceID": 2, "context": "It can be viewed as a mixture of Batch Normalization [15] and Layer Normalization [3].", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "We use the same training (+validation)/ test splits as in [2, 48].", "startOffset": 58, "endOffset": 65}, {"referenceID": 1, "context": "We use the pre-extracted Word2Vec and Glove vectors from Wikipedia provided by [2, 48].", "startOffset": 79, "endOffset": 86}, {"referenceID": 1, "context": "Following the partitioning strategy of [2, 48], we split AwA dataset into 30/10/10 classes and CUB dataset into 100/50/50 classes for labeled training/ unlabeled training/ test data.", "startOffset": 39, "endOffset": 46}, {"referenceID": 1, "context": "Please see Supplementary material for more detailed comparisons to the following non-deep-embeddings methods: SOC [30], ConSE [29], SSE [49], SJE [2], ESZSL [37], JLSE [50], LatEm [48], Sync [7], MTE [6], TMV [10], and SMS [12].", "startOffset": 146, "endOffset": 149}, {"referenceID": 1, "context": "Expand the test-time search space: Note that most of the methods [9, 41, 29, 49, 2, 50, 48, 7, 10] consider that, at test time, queries come from only test classes.", "startOffset": 65, "endOffset": 98}], "year": 2017, "abstractText": "Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and fewshot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.", "creator": "LaTeX with hyperref package"}}}