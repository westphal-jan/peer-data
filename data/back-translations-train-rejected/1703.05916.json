{"id": "1703.05916", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2017", "title": "Construction of a Japanese Word Similarity Dataset", "abstract": "An evaluation of distributed word representation is generally conducted using a word similarity task and/or a word analogy task. There are many datasets readily available for these tasks in English. However, evaluating distributed representation in languages that do not have such resources (e.g., Japanese) is difficult. Therefore, as a first step toward evaluating distributed representations in Japanese, we constructed a Japanese word similarity dataset. To the best of our knowledge, our dataset is the first resource that can be used to evaluate distributed representations in Japanese. Moreover, our dataset contains various parts of speech and includes rare words in addition to common words.", "histories": [["v1", "Fri, 17 Mar 2017 07:53:03 GMT  (36kb,D)", "http://arxiv.org/abs/1703.05916v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yuya sakaizawa", "mamoru komachi"], "accepted": false, "id": "1703.05916"}, "pdf": {"name": "1703.05916.pdf", "metadata": {"source": "CRF", "title": "Construction of a Japanese Word Similarity Dataset", "authors": ["Yuya Sakaizawa"], "emails": ["sakaizawa-yuya@ed.tmu.ac.jp", "komachi@tmu.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "Traditionally, a word is presented as a sparse vector indicating the word itself (a hot vector) or the context of the word (a distribution vector), but both the one hot notation and the distribution notation suffer from data spareness because the dimensions of the word vector do not interact with each other. Distributed word representation addresses the data-sparse problem by constructing a dense fixed-length vector, with contexts divided (or distributed) across dimensions. Distributed word representation is known to improve the performance of many NLP applications, such as machine translation (Chen and Guo, 2015) and sensation analysis (Tai et al.), to name a few. The task of learning distributed representation is known as word representation."}, {"heading": "2 Related Work", "text": "In general, distributed word representations are evaluated using a word similarity task. For example, WordSim353 (Finkelstein et al., 2002), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al., 2012) have been used to evaluate word similarities in English. In addition, Baker et al. (2014) have created a verb similarity dataset (VSD) based on WordSim353 because there is no data set of verbs in the word similarity task. Furthermore, distributed representation of words is generally learned only from word information. Consequently, the distributed representation of low frequency words and unknown words cannot be learned well using conventional models. However, low frequency words and unknown words often consist of high frequency morphems (e.g. unpleasant \u2192 un + king + ly)."}, {"heading": "3 Construction of a Japanese Word Similarity Dataset", "text": "In fact, most of them will be able to move to another world, in which they will be able to understand the world and in which they will be able to change the world."}, {"heading": "4 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Comparison to Other Datasets", "text": "Table 5 shows how multiple resources differ. WordSim353 includes high-frequency words, so the variance tends to be low. In contrast, RW includes low-frequency words, unknown words, and complex words that consist of multiple morphemes, so the variance is large. VSD has many polysemic words that increase the variance. Despite the fact that our dataset, similar to the VSD and RW datasets, contains low-frequency and ambiguous words, its variance is 3.00. The level of variance is low compared to the other corpora. We thought that the examples of similarity in the task assignment reduced the variance level. We did not expect SCWS to have the largest variance in the datasets shown in Table 5 because it gave the context to the annotators during the annotation. Initially, we thought the context would serve to eliminate the ambiguity of the annotation and clarify the meaning of the word."}, {"heading": "4.2 Analysis", "text": "In this case, there is a pairing of \"fast\" and \"early.\") Although they are similar in time, they have nothing in common in speed. (Annotator A has assigned a rating of 10, but Annotator B has a rating of \"eager\" and \"demanding.\" Although the act is identical in the two verbs, there are some cases in which they express different degrees of emotion. Compared to \"eager,\" the pairing of \"eager\" and \"demanding\" indicates the similarity."}, {"heading": "5 Conclusion", "text": "In this study, we constructed the first Japanese word similarity dataset. It contains different parts of the language and includes rare words in addition to ordinary words. Crowdsourced annotators assigned similarities to word pairs during the word similarity task. We gave examples of similarities in the task to annotators, so we reduced the variance of each word pair. However, we did not restrict the attributes of words, such as the level of feeling, during the annotation. Error analysis revealed that the term similarity should be carefully defined when creating a similarity dataset. In the future, we plan to construct a word analog dataset in Japanese by translating an English dataset into Japanese. We hope that a Japanese database will facilitate research into Japanese distributed representations."}], "references": [{"title": "An Unsupervised Model for Instance Level Subcategorization Acquisition", "author": ["Simon Baker", "Roi Reichart", "Anna Korhonen."], "venue": "EMNLP.", "citeRegEx": "Baker et al\\.,? 2014", "shortCiteRegEx": "Baker et al\\.", "year": 2014}, {"title": "Representation Based Translation Evaluation Metrics", "author": ["Boxing Chen", "Hongyu Guo."], "venue": "ACL.", "citeRegEx": "Chen and Guo.,? 2015", "shortCiteRegEx": "Chen and Guo.", "year": 2015}, {"title": "Investigations on word senses and word usages", "author": ["Katrin Erk", "Diana McCarthy", "Nicholas Gaylord."], "venue": "ACL.", "citeRegEx": "Erk et al\\.,? 2009", "shortCiteRegEx": "Erk et al\\.", "year": 2009}, {"title": "Placing Search in Context: The Concept Revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "ACM.", "citeRegEx": "Finkelstein et al\\.,? 2002", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2002}, {"title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "ACL.", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Development of the Japanese WordNet", "author": ["Hitoshi Isahara", "Fransis Bond", "Kiyotaka Uchimoto", "Masao Utiyama", "Kyoko Kanzaki."], "venue": "LREC.", "citeRegEx": "Isahara et al\\.,? 2008", "shortCiteRegEx": "Isahara et al\\.", "year": 2008}, {"title": "Controlled and Balanced Dataset for Japanese Lexical Simplification", "author": ["Tomonori Kodaira", "Tomoyuki Kajiwara", "Mamoru Komachi."], "venue": "ACL Student Research Workshop.", "citeRegEx": "Kodaira et al\\.,? 2016", "shortCiteRegEx": "Kodaira et al\\.", "year": 2016}, {"title": "Better Word Representations with Recursive Neural Networks for Morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D. Manning."], "venue": "CoNLL.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "NIPS.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: A Lexical Database for English", "author": ["George A. Miller."], "venue": "Commun. ACM .", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Contextual Correlates of Semantic Similarity", "author": ["George A Miller", "Walter G Charles."], "venue": "Language and Cognitive Processes .", "citeRegEx": "Miller and Charles.,? 1991", "shortCiteRegEx": "Miller and Charles.", "year": 1991}, {"title": "Contextual Correlates of Synonymy", "author": ["Herbert Rubenstein", "John B. Goodenough."], "venue": "Commun. ACM .", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Unsupervised Morphology Induction Using Word Embeddings", "author": ["Radu Soricut", "Franz Och."], "venue": "NAACL.", "citeRegEx": "Soricut and Och.,? 2015", "shortCiteRegEx": "Soricut and Och.", "year": 2015}, {"title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "ACL.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Distributed word representation is known to improve the performance of many NLP applications such as machine translation (Chen and Guo, 2015) and sentiment analysis (Tai et al.", "startOffset": 121, "endOffset": 141}, {"referenceID": 13, "context": "Distributed word representation is known to improve the performance of many NLP applications such as machine translation (Chen and Guo, 2015) and sentiment analysis (Tai et al., 2015) to name a few.", "startOffset": 165, "endOffset": 183}, {"referenceID": 6, "context": "Figure 1: An example of the dataset from a previous study (Kodaira et al., 2016).", "startOffset": 58, "endOffset": 80}, {"referenceID": 3, "context": "For instance, WordSim353 (Finkelstein et al., 2002), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 25, "endOffset": 51}, {"referenceID": 10, "context": ", 2002), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 12, "endOffset": 38}, {"referenceID": 11, "context": ", 2002), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 43, "endOffset": 76}, {"referenceID": 4, "context": ", 2002), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al., 2012) have been used to evaluate word similarities in English.", "startOffset": 87, "endOffset": 107}, {"referenceID": 0, "context": "Moreover, Baker et al. (2014) built a verb similarity dataset (VSD) based on WordSim353 because there is no dataset of verbs in the word-similarity task.", "startOffset": 10, "endOffset": 30}, {"referenceID": 7, "context": "Some previous studies take advantage of the morphological information to provide a suitable representation for low-frequency words and unknown words (Luong et al., 2013; Soricut and Och, 2015).", "startOffset": 149, "endOffset": 192}, {"referenceID": 12, "context": "Some previous studies take advantage of the morphological information to provide a suitable representation for low-frequency words and unknown words (Luong et al., 2013; Soricut and Och, 2015).", "startOffset": 149, "endOffset": 192}, {"referenceID": 7, "context": "We followed the procedure used to construct the Stanford Rare Word Similarity Dataset (RW) (Luong et al., 2013).", "startOffset": 91, "endOffset": 111}, {"referenceID": 6, "context": "We extracted Japanese word pairs from the Evaluation Dataset of Japanese Lexical Simplification (Kodaira et al., 2016).", "startOffset": 96, "endOffset": 118}, {"referenceID": 5, "context": "nyther reason why we did not employ the synset from the Japanese WordNet (Isahara et al., 2008) was because its quality was not as good as the English WordNet except for concrete nouns2.", "startOffset": 73, "endOffset": 95}, {"referenceID": 5, "context": "We observed that the similarity of the pairs extracted from the dataset of Kodaira et al. (2016) was low without providing contexts; thus, we did not augment the dataset by inserting pseudonegative instances from WordNet\u2019s synsets, as was done in the RW corpus.", "startOffset": 75, "endOffset": 97}, {"referenceID": 6, "context": "Although (Kodaira et al., 2016) gave the annotators the context during annotation, we removed the context and gave only pairs to annotators.", "startOffset": 9, "endOffset": 31}], "year": 2017, "abstractText": "An evaluation of distributed word representation is generally conducted using a word similarity task and/or a word analogy task. There are many datasets readily available for these tasks in English. However, evaluating distributed representation in languages that do not have such resources (e.g., Japanese) is difficult. Therefore, as a first step toward evaluating distributed representations in Japanese, we constructed a Japanese word similarity dataset. To the best of our knowledge, our dataset is the first resource that can be used to evaluate distributed representations in Japanese. Moreover, our dataset contains various parts of speech and includes rare words in addition to common words.", "creator": "LaTeX with hyperref package"}}}