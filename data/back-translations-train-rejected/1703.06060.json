{"id": "1703.06060", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2017", "title": "Online Learning for Offloading and Autoscaling in Energy Harvesting Mobile Edge Computing", "abstract": "Mobile edge computing (a.k.a. fog computing) has recently emerged to enable in-situ processing of delay-sensitive applications at the edge of mobile networks. Providing grid power supply in support of mobile edge computing, however, is costly and even infeasible (in certain rugged or under-developed areas), thus mandating on-site renewable energy as a major or even sole power supply in increasingly many scenarios. Nonetheless, the high intermittency and unpredictability of renewable energy make it very challenging to deliver a high quality of service to users in energy harvesting mobile edge computing systems. In this paper, we address the challenge of incorporating renewables into mobile edge computing and propose an efficient reinforcement learning-based resource management algorithm, which learns on-the-fly the optimal policy of dynamic workload offloading (to the centralized cloud) and edge server provisioning to minimize the long-term system cost (including both service delay and operational cost). Our online learning algorithm uses a decomposition of the (offline) value iteration and (online) reinforcement learning, thus achieving a significant improvement of learning rate and run-time performance when compared to standard reinforcement learning algorithms such as Q-learning. We prove the convergence of the proposed algorithm and analytically show that the learned policy has a simple monotone structure amenable to practical implementation. Our simulation results validate the efficacy of our algorithm, which significantly improves the edge computing performance compared to fixed or myopic optimization schemes and conventional reinforcement learning algorithms.", "histories": [["v1", "Fri, 17 Mar 2017 15:54:36 GMT  (1241kb)", "http://arxiv.org/abs/1703.06060v1", "arXiv admin note: text overlap witharXiv:1701.01090by other authors"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1701.01090by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.NI", "authors": ["jie xu", "lixing chen", "shaolei ren"], "accepted": false, "id": "1703.06060"}, "pdf": {"name": "1703.06060.pdf", "metadata": {"source": "CRF", "title": "Online Learning for Offloading and Autoscaling in Energy Harvesting Mobile Edge Computing", "authors": ["Jie Xu", "Lixing Chen"], "emails": ["jiexu@miami.edu,", "lx.chen@miami.edu.", "sren@ece.ucr.edu"], "sections": [{"heading": null, "text": "This year, it has come to the point that it will only be once before there is such a process, in which there is such a process."}, {"heading": "II. RELATED WORK", "text": "In fact, you are able to manoeuvre yourself into a situation where you put yourself at the centre, where you put yourself at the centre."}, {"heading": "III. SYSTEM MODEL", "text": "As an important application method of mobile edge computing [11] we consider an edge system consisting of a base station and a series of edge servers physically adjacent to each other and sharing the same power supply in the cell location."}, {"heading": "A. Workload model", "text": "We consider a discrete time model by dividing the operating time into slots of the same length indexed by t = 0, 1,..., each time model having a duration corresponding to the timeframe in which the peripheral device can adjust its computing power (i.e. the number of active servers). We use x-L to represent a location coordinate in service area L. Let \u03bb (x, t) represent the arrival rate of the workload in site x, and \u03b8 (x, t) the maximum arrival rate. The system determines the amount of workload \u00b5 (t) \u2264 (t) to be processed locally. The remaining workload \u043c (t) \u2012 \u00b5 (t) is outsourced to the cloud for processing."}, {"heading": "B. Delay cost model", "text": "The average utilization of the Base Unit is \u03c1 (t) = \u2211 x \u03bb (x, t) / \u03b8 (x, t), resulting in a total wireless access and transmission delay of cwi (t) = \u2211 x \u03bb (x, t) / [\u03b8 (x, t) (1 \u2212 \u03c1 (t))) by following the literature and modelling the Base Unit as a queue [14]. Next, we model the delay in the workload occurring at the edge of the server. To quantify the delay performance of the services, the delay time clo (t) is mainly processing delay (t) due to the limited computing capacity on the local edge servers. The transmission delay from the peripheral device to the local servers is negligible due to the physical co-location. To quantify the delay performance of the services, such as average delay and final delay (e.g. 95 percent latency), the delay size of our model is (without a specific service size)."}, {"heading": "C. Power model", "text": "We use interchangeable power and energy, because the energy consumption during each time frame is the product of the (average) power and duration of each time frame that is kept constant in our model. The total power demand of the peripheral system within a time frame consists of two parts: First, basic operation and transmission power demand by peripheral devices (base station in our study); and second, computing power by peripheral servers. The first part is independent of the outsourcing or autoscale policy that is modeled as Dop (t) = dsta + ddyn (t), where dsta is the static power consumption and ddyn (t) the dynamic power consumption depending on the amount of total work. The computing energy requirement depends on the number of active servers and the locally processed workload. We use a generic function dcom (m (t), \u00b5 (t))), which is not determined by the daily power."}, {"heading": "D. Battery model", "text": "In a solar / wind system, photovoltaic modules and wind turbines can combine their power to power the peripheral system and charge the batteries. [6] When their combined efforts are insufficient, the batteries take over the continuous operation of the peripheral system. We refer to the battery state at the beginning of the time window t by b (t) by b (0, B], B (in power units), where B is the battery capacity. For system protection reasons, the battery unit must be separated from the load as soon as its terminal voltage is below a certain threshold for charging. We assign b (t) = 0 to this threshold voltage in order to ensure the basic operation of the system. As the eco-electricity balance is unpredictable and therefore unknown at the beginning of the time window t, the peripheral system adopts a conservative policy, the dcom (t), \u00b5 (t) = max (t) \u2212 dop (t) \u2212 fic."}, {"heading": "IV. PROBLEM FORMULATION", "text": "In this section, we formulate the dynamic offloading and autoscaling problem as an online learning problem (to minimize system costs), which is described by a tuple of s (t), (t), e (t), h (t), b (t) that is observable at the beginning of each time window (t). Among the four state elements, the workload rate (t), the environmental state e (t), the state of the backbone network h (t) are exogenous states that are independent of the offloading and autoscaling actions, while the battery state b (t) evolves according to offloading and autoscaling actions and the realization of renewable energies. To understand the stochastic control problem, they are assumed to have finite value spaces (t), e (t), h (t), evolve as finite-state chains."}, {"heading": "V. POST-DECISION STATE BASED ONLINE LEARNING", "text": "If all probability distributions were known from the outset, the optimal policy could be solved offline using traditional algorithms for solving Bellman equations, such as value teration and policy iteration [10]. In the problem under consideration, all these probability distributions are a priori unknown and therefore these algorithms are impractical. In this section, we propose an online learning algorithm for amplification to derive the optimal policy approach \u03c0 on the fly. Our solution is based on the idea of the Post-Decision State (PDS), which takes advantage of partially known information about system dynamics and allows the edge system to integrate this information into its learning process to accelerate learning. Compared to conventional online learning algorithms for amplification, such as Q-Learning, the proposed PDS-based learning algorithm significantly improves its convergence speed and runtime performance. In the rest of this section, we first define the PDS algorithm and then describe the proposed PDS-14 algorithm."}, {"heading": "A. Post-Decision State", "text": "We first introduce the term PDS, which is the most critical idea of our proposed algorithm. (In our problem, PDS is the intermediate state of the system after the peripheral system assumes the computing power-demand action a (t), but before the eco-electricity budget g (t) is realized. Figure 3 illustrates the relationship between a normal state s (t) and its PDS s (t). Specifically, the PDS is in the time slot t t t, denoted by s (t), (t), e (t), h (t), b), b (t), defined as a normal state s (t), as an independent state s (t), e (t), e (t), h), (t), andb), b), b (t), b (t), b (t), b (t), if the decision dop (t) > b (t) > b (t), that the decision (t), max (t) \u2212 dop (t) \u2212 dot (p) \u2212 dot (p) (p)."}, {"heading": "B. The algorithm", "text": "The algorithm maintains and updates a series of variables in each time window. These variables are (as follows) (as follows): (as high) (as high) (as high) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as (as) (as) (as) (as) (as) (as) (as) (as) (as) (as) (as (as) (as) (as) (as (as) (as) (as) (as) (as (as) (as) (as) (as) (as (as) (as) (as (as) (as) (as) (as) (as (as) (as) (as) (as) (as (as) (as) (as) (as) (as (as) (as) (as) (as) ((as) ((as) (as) ((as) (as (as) (as) (as) (as) () ((as) () (as) ((as) (as) (((as) (as) () () ((as) ((as) (((as) ()) ((("}, {"heading": "VI. ALGORITHM ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Convergence of the PDS learning algorithm", "text": "Theorem 1. The online learning algorithm based on the PDS converges with the optimal postdecision value function V * (s) when the sequence of learning rates n = 0 * t = 1 and n = 0 (s, a) + V (s, a) 2 < p (s).18 Evidence follows. The proof follows [24]. For each PDS function, we define a function on its value function as follows: Fs (V) = min a (c (s, a) + V (s)) (21), where s and a are such that, \u03bb = e (s), h = h (s) and b = b (s).a) For each value of V (s), Fs (V) maps to a real number. Based on this, we define F: R | S | R | S | S as a figure that collects Fs (s) for all S. It has been proven that convergence (S) is the optimal convergence (S)."}, {"heading": "B. Structure of the Optimal Policy", "text": "Next, we characterize the structure of the optimal policy. First, we show that the single slot cost function (1) convex (1) in the electricity demand action. Lemma 1. Suppose that both clo (m, \u00b5) and dcom (m, \u00b5) together convex in (m, \u00b5) for each given s, then the single slot cost function c (s, a) convex in a given situation. Let us remember that c (s, a) = c delay (s, a) + Eg delay (max.) max."}, {"heading": "A. Simulation Setup", "text": "We consider each time window as 15 minutes. The arrival space of the workload is determined as follows: \u0432 = {10 units / sec, 20 units / sec,..., 100 units / sec}. The ambient space is E = {Low, Medium, High}. For each ambient state, the green power is realized according to a normal distraction by different means: g (t | e = Low) \u0445 N (200W, 102), g (t | e = Medium) \u0445 N (400W, 102), g (t | e = High). The battery capacity is set as B = 2 kWh. The static power consumption of the base station is dsta = 300W. The maximum number of activated edge servers is M = 15. The power consumption of each edge server is 150 W. The maximum service rate of each edge server is 20 units / sec. Other important problems are determined as follows: Q cost cement scheme follows: Q cost cement scheme is set as follows: Q cost cost scheme is set as 150 units / sec of each MD."}, {"heading": "B. Run-time Performance Comparison", "text": "Figure 4 compares the runtime performance of our scheme with the three benchmark schemes for 10,000 time frames. As can be seen, the proposed PDS-based learning algorithm incurs significantly lower costs than all benchmark schemes. \u2022 The short-sighted optimisation incurs high average time costs because it ignores the time correlation of decision-making and is often forced to activate backup performance in subsequent time frames. \u2022 The fixed power scheme exhibits huge performance differences depending on the fixed value used, which implies that it is sensitive to system parameters. Figure 4 shows two fixed values (1.0 kW and 0.4 kW) for illustration, with 1.0 kW being the best fixed value determined by our extensive simulations. As system dynamics are unknown and may change over time, the use of a fixed computing power scheme can cause significant power losses. \u2022 The performance of Q-Learning is much worse than that of our system because it is very slow to adapt due to the large size of the PagenDS."}, {"heading": "C. Learned Optimal Policy", "text": "Figure 5 goes on to explain why the proposed algorithm outperforms the short-sighted solution by showing the optimal policy learned. If the workload is low and the network is not overloaded, the policy learned by the proposed algorithm tends to use local computing power conservatively with low battery levels, thus saving more energy for the future when the workload is high and the state of network overload deteriorates, lowering long-term system costs. On the other hand, the short-sighted policy ignores this timely correlation and activates local servers to handle the workload even when the battery level is not that high. As a result, even though it achieves a slight improvement in the current window, it consumes electricity to reduce potentially significant costs in the future. Figure 5 also confirms our theoretical results in Theorem 2 on the structure of optimal policy: Optimal power demand increases at the battery level 24."}, {"heading": "D. Battery State Distribution", "text": "Figure 6 (a) shows the distribution of the battery state over the simulated 10,000 time windows in a representative simulation run for the various schemes, and Figure 6 (b) shows the adapted curves (polynomial adaptation) for better inspection. As can be seen, short-sighted optimization leads to a large part of the time when the system is in the insufficient battery zone, resulting in significant reserve current costs. If a too low fixed power requirement is used (e.g. 0.4 kW), the battery state can spend a considerable time in the high battery state zone (i.e. 0.7 - 1 kWh), implying that much of the green electricity cannot be harvested and is therefore wasted due to the limited battery capacity limitation. Furthermore, the use of a smaller fixed current for the calculation does not guarantee that it has a lower chance of reaching the insufficient battery zone. This is because the battery condition is slightly higher than that of the basic level of operation can be supported by the insufficient."}, {"heading": "E. Cost Composition", "text": "Figure 7 shows the cost composition of the PDS-based algorithm and the short-sighted optimization in 10,000 time windows. It can be observed that the proposed PDS-based algorithm significantly lowers the reserve current costs by taking conservative measures at low battery states, thus avoiding the use of reserve current. In contrast, the short-sighted optimization often leads the battery state into the insufficient zone, as shown in Figure 6, and results in significant reserve current costs. Figure 8 shows the composition of the average time costs at the end of the simulation. It can be observed that the PDS-based algorithm and Q-leaning reduce the total costs by taking into account future system dynamics and causing low reserve current costs for 12.6% and 16.7% of the total cost, respectively."}, {"heading": "F. Optimal Offloading Strategy", "text": "Finally, we illustrate the optimal workload strategy in Figure 9 with a fixed number of active servers under different system states. As the network becomes increasingly congested and there are more 26active edge servers, the optimal strategy chooses to process more workload on the local edge system. However, as the workload arrival rate increases, the amount of workload that can be processed on the local edge system is saturated after a certain rate of workload arrival."}, {"heading": "VIII. CONCLUSION", "text": "In this paper, we investigated the problem of joint discharge and autoscale of energy harvesting MEC systems. We found that foresight and adaptability are key to the reliable and efficient operation of MEC with renewable energies. In order to enable rapid learning in the presence of a priori unknown system parameters, a PDS-based reinforcement learning algorithm was developed to learn the optimal discharge and autoscale policy by exploiting the specific structure of the problem under consideration. Our simulations showed that the proposed scheme can significantly improve computing performance even when operated with intermittent and unpredictable renewable energies."}], "references": [{"title": "Gartner says the internet of things will transform the data center", "author": ["J. Rivera", "R. van der Meulen"], "venue": "Retrieved August, vol. 5, p. 2014, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Mobile edge computing: Challenges for future virtual network embedding algorithms", "author": ["M.T. Beck", "M. Maier"], "venue": "The Eighth International Conference on Advanced Engineering Computing and Applications in Sciences (ADVCOMP). IARIA. Citeseer, 2014, pp. 65\u201370.  27", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Finding your way in the fog: Towards a comprehensive definition of fog computing", "author": ["L.M. Vaquero", "L. Rodero-Merino"], "venue": "ACM SIGCOMM Computer Communication Review, vol. 44, no. 5, pp. 27\u201332, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Edge computing: Vision and challenges", "author": ["W. Shi", "J. Cao", "Q. Zhang", "Y. Li", "L. Xu"], "venue": "IEEE Internet of Things Journal, vol. 3, no. 5, pp. 637\u2013646, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Fog and iot: An overview of research opportunities", "author": ["M. Chiang", "T. Zhang"], "venue": "IEEE Internet of Things Journal, vol. 3, no. 6, pp. 854\u2013864, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards sustainable in-situ server systems in the big data era", "author": ["C. Li", "Y. Hu", "L. Liu", "J. Gu", "M. Song", "X. Liang", "J. Yuan", "T. Li"], "venue": "ACM SIGARCH Computer Architecture News, vol. 43, no. 3. ACM, 2015, pp. 14\u201326.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Traffic load balancing framework for software-defined radio access networks powered by hybrid energy sources", "author": ["T. Han", "N. Ansari"], "venue": "IEEE/ACM Transactions on Networking, vol. pp, no. 99, March 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Energy harvesting sensor nodes: Survey and implications", "author": ["S. Sudevalayam", "P. Kulkarni"], "venue": "IEEE Communications Surveys & Tutorials, vol. 13, no. 3, pp. 443\u2013461, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Energy harvesting wireless communications: A review of recent advances", "author": ["S. Ulukus", "A. Yener", "E. Erkip", "O. Simeone", "M. Zorzi", "P. Grover", "K. Huang"], "venue": "IEEE Journal on Selected Areas in Communications, vol. 33, no. 3, pp. 360\u2013381, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Mobile-edge computing introductory technical white paper", "author": ["M. Patel", "B. Naughton", "C. Chan", "N. Sprecher", "S. Abeta", "A. Neal"], "venue": "White Paper, Mobile-edge Computing (MEC) industry initiative, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Mobile edge computing: Survey and research outlook", "author": ["Y. Mao", "C. You", "J. Zhang", "K. Huang", "K.B. Letaief"], "venue": "arXiv preprint arXiv:1701.01090, 2017.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Data offloading with renewable energy powered base station connected to a microgrid", "author": ["Y.-K. Chia", "C.K. Ho", "S. Sun"], "venue": "Global Communications Conference (GLOBECOM), 2014 IEEE. IEEE, 2014, pp. 2721\u20132726.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamic base station switching-on/off strategies for green cellular networks", "author": ["E. Oh", "K. Son", "B. Krishnamachari"], "venue": "IEEE Transactions on Wireless Communications, vol. 12, no. 5, pp. 2126\u20132136, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Toward dynamic energy-efficient operation of cellular network infrastructure", "author": ["E. Oh", "B. Krishnamachari", "X. Liu", "Z. Niu"], "venue": "IEEE Communications Magazine, vol. 49, no. 6, pp. 56\u201361, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Dynamic right-sizing for power-proportional data centers", "author": ["M. Lin", "A. Wierman", "L.L.H. Andrew", "E. Thereska"], "venue": "IEEE Infocom, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "iswitch: Coordinating and optimizing renewable energy powered server clusters", "author": ["C. Li", "A. Qouneh", "T. Li"], "venue": "ISCA, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Greenslot: scheduling energy consumption in green datacenters", "author": ["I. Goiri", "R. Beauchea", "K. Le", "T.D. Nguyen", "M.E. Haque", "J. Guitart", "J. Torres", "R. Bianchini"], "venue": "SuperComputing, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Dynamic computation offloading for mobile-edge computing with energy harvesting devices", "author": ["Y. Mao", "J. Zhang", "K.B. Letaief"], "venue": "IEEE Journal on Selected Areas in Communications, vol. 34, no. 12, pp. 3590\u20133605, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Stochastic network optimization with application to communication and queueing systems", "author": ["M.J. Neely"], "venue": "Synthesis Lectures on Communication Networks, vol. 3, no. 1, pp. 1\u2013211, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards power consumption-delay tradeoff by workload allocation in cloud-fog computing", "author": ["R. Deng", "R. Lu", "C. Lai", "T.H. Luan"], "venue": "Communications (ICC), 2015 IEEE International Conference on. IEEE, 2015, pp. 3909\u20133914.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Online learning for offloading and autoscaling in renewable-powered mobile edge computing", "author": ["J. Xu", "S. Ren"], "venue": "2016 IEEE Global Communications Conference (GLOBECOM), Dec 2016, pp. 1\u20136.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Managing cost, performance and reliability tradeoffs for energy-aware server provisioning", "author": ["B. Guenter", "N. Jain", "C. Williams"], "venue": "IEEE Infocom, 2011.  28", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Structure-aware stochastic storage management in smart grids", "author": ["Y. Zhang", "M. van der Schaar"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 8, no. 6, pp. 1098\u20131110, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "The ode method for convergence of stochastic approximation and reinforcement learning", "author": ["V.S. Borkar", "S.P. Meyn"], "venue": "SIAM Journal on Control and Optimization, vol. 38, no. 2, pp. 447\u2013469, 2000.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2000}, {"title": "Dynamic programming and optimal control", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific Belmont, MA,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1995}, {"title": "Puterman, Markov decision processes: discrete stochastic dynamic programming", "author": ["L. M"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Although cloud computing enables convenient access to a centralized pool of configurable computing resources, moving all the distributed data and computing-intensive applications to clouds (which are often physically located in remote megascale data centers) is simply out of the question, as it would not only pose an extremely heavy burden on today\u2019s already-congested backbone networks [1] but also result in (sometimes intolerable) large transmission latencies that degrade the quality of service [2]\u2013[4].", "startOffset": 389, "endOffset": 392}, {"referenceID": 1, "context": "Although cloud computing enables convenient access to a centralized pool of configurable computing resources, moving all the distributed data and computing-intensive applications to clouds (which are often physically located in remote megascale data centers) is simply out of the question, as it would not only pose an extremely heavy burden on today\u2019s already-congested backbone networks [1] but also result in (sometimes intolerable) large transmission latencies that degrade the quality of service [2]\u2013[4].", "startOffset": 501, "endOffset": 504}, {"referenceID": 3, "context": "Although cloud computing enables convenient access to a centralized pool of configurable computing resources, moving all the distributed data and computing-intensive applications to clouds (which are often physically located in remote megascale data centers) is simply out of the question, as it would not only pose an extremely heavy burden on today\u2019s already-congested backbone networks [1] but also result in (sometimes intolerable) large transmission latencies that degrade the quality of service [2]\u2013[4].", "startOffset": 505, "endOffset": 508}, {"referenceID": 1, "context": "As a remedy to the above limitations, mobile edge computing (MEC) [2]\u2013[4] (a.", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "As a remedy to the above limitations, mobile edge computing (MEC) [2]\u2013[4] (a.", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": ", fog computing [5]) has recently emerged to enable in-situ processing of (some) workloads locally at the network edge without moving them to the cloud.", "startOffset": 16, "endOffset": 19}, {"referenceID": 5, "context": "However, providing reliable and stable grid power supply in remote areas and hazardous locations can be extremely costly and even infeasible since construction and operation of transmission lines are often prohibitive, and grid-tied servers can violate environmental quality regulations in rural areas that are ecologically sensitive [6].", "startOffset": 334, "endOffset": 337}, {"referenceID": 6, "context": "For instance, in many developing countries, the majority of base stations have to be powered by continuously operating diesel generators because the electric grid is too unreliable [7].", "startOffset": 181, "endOffset": 184}, {"referenceID": 7, "context": "sole power supply for edge systems in the field, thanks to the recent advancements of energy harvesting techniques [8], [9].", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "sole power supply for edge systems in the field, thanks to the recent advancements of energy harvesting techniques [8], [9].", "startOffset": 120, "endOffset": 123}, {"referenceID": 9, "context": "It is well-known that MDP suffers from the so-called \u201ccurse of dimensionality\u201d problem when the state space is large [10].", "startOffset": 117, "endOffset": 121}, {"referenceID": 9, "context": "The proposed PDS-based learning algorithm exploits the special structure of state transitions of the considered energy harvesting MEC system to conquer this challenge, thereby significantly improving both the learning convergence speed and the run-time performance compared with conventional online reinforcement learning algorithms such as Q-learning [10].", "startOffset": 352, "endOffset": 356}, {"referenceID": 10, "context": "The concept of MEC was proposed in 2014 as a new platform that provides IT and cloudcomputing capabilities within the radio access network in close proximity to mobile subscribers [11].", "startOffset": 180, "endOffset": 184}, {"referenceID": 3, "context": "Recently, the definition of edge devices gets broader, encompassing any devices that have computing resources along the path between data sources and cloud data centers [4].", "startOffset": 169, "endOffset": 172}, {"referenceID": 4, "context": "Fog computing [5] is a related concept that refers to the same computing paradigm.", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "Compared with MCC, MEC has the advantages of achieving lower latency, saving energy, supporting context-aware computing, and enhancing privacy and security for mobile applications [4].", "startOffset": 180, "endOffset": 183}, {"referenceID": 11, "context": ", what/when/how to offload a user\u2019s workload from its device to the edge system or cloud (see [12] and references therein).", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "MEC servers are small-scale data centers and consume substantially less energy than the conventional cloud mega-scale data center [12].", "startOffset": 130, "endOffset": 134}, {"referenceID": 7, "context": "Off-grid renewable energy, such as solar radiation and wind energy, has recently emerged as a viable and promising power source for various IT systems thanks to the recent advancement of energy harvesting techniques [8], [9].", "startOffset": 216, "endOffset": 219}, {"referenceID": 8, "context": "Off-grid renewable energy, such as solar radiation and wind energy, has recently emerged as a viable and promising power source for various IT systems thanks to the recent advancement of energy harvesting techniques [8], [9].", "startOffset": 221, "endOffset": 224}, {"referenceID": 12, "context": "However, designing green MEC powered by renewable energy is much more challenging compared to green communication systems [13]\u2013[15] or green data center networks [16] [7], [17], [18] since the radio and computation resources have to be jointly managed, whereas prior research typically only considers one of the two decisions.", "startOffset": 122, "endOffset": 126}, {"referenceID": 14, "context": "However, designing green MEC powered by renewable energy is much more challenging compared to green communication systems [13]\u2013[15] or green data center networks [16] [7], [17], [18] since the radio and computation resources have to be jointly managed, whereas prior research typically only considers one of the two decisions.", "startOffset": 127, "endOffset": 131}, {"referenceID": 15, "context": "However, designing green MEC powered by renewable energy is much more challenging compared to green communication systems [13]\u2013[15] or green data center networks [16] [7], [17], [18] since the radio and computation resources have to be jointly managed, whereas prior research typically only considers one of the two decisions.", "startOffset": 162, "endOffset": 166}, {"referenceID": 6, "context": "However, designing green MEC powered by renewable energy is much more challenging compared to green communication systems [13]\u2013[15] or green data center networks [16] [7], [17], [18] since the radio and computation resources have to be jointly managed, whereas prior research typically only considers one of the two decisions.", "startOffset": 167, "endOffset": 170}, {"referenceID": 16, "context": "However, designing green MEC powered by renewable energy is much more challenging compared to green communication systems [13]\u2013[15] or green data center networks [16] [7], [17], [18] since the radio and computation resources have to be jointly managed, whereas prior research typically only considers one of the two decisions.", "startOffset": 172, "endOffset": 176}, {"referenceID": 17, "context": "However, designing green MEC powered by renewable energy is much more challenging compared to green communication systems [13]\u2013[15] or green data center networks [16] [7], [17], [18] since the radio and computation resources have to be jointly managed, whereas prior research typically only considers one of the two decisions.", "startOffset": 178, "endOffset": 182}, {"referenceID": 15, "context": "right-sizing) in data centers [16] dynamically controls the number of active servers, but the control knob of offloading to the cloud is not available in the context of data centers.", "startOffset": 30, "endOffset": 34}, {"referenceID": 18, "context": "For energy harvesting mobile devices, a dynamic computation offloading policy was proposed in [19] using Lyapunov optimization techniques [20] based on both information of the wireless channel and energy.", "startOffset": 94, "endOffset": 98}, {"referenceID": 19, "context": "For energy harvesting mobile devices, a dynamic computation offloading policy was proposed in [19] using Lyapunov optimization techniques [20] based on both information of the wireless channel and energy.", "startOffset": 138, "endOffset": 142}, {"referenceID": 20, "context": "Another study relevant to our work is [21], which also studies workload allocation/offloading in a cloud-fog computing system.", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "A key advantage of our proposed algorithm is that it exploits the partial information of the edge computing system and the structure of the resource management problem, and thus it converges much faster than conventional reinforcement learning algorithms such as Q-learning [10].", "startOffset": 274, "endOffset": 278}, {"referenceID": 21, "context": "To the best knowledge of the authors, the conference version of this paper [22] was the first to study resource management for energy harvesting MEC servers (see related discussions in a recent comprehensive survey paper [12]).", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "To the best knowledge of the authors, the conference version of this paper [22] was the first to study resource management for energy harvesting MEC servers (see related discussions in a recent comprehensive survey paper [12]).", "startOffset": 221, "endOffset": 225}, {"referenceID": 21, "context": "The present paper extents our findings in [22].", "startOffset": 42, "endOffset": 46}, {"referenceID": 10, "context": "SYSTEM MODEL As a major deployment method of mobile edge computing [11], we consider an edge system consisting of a base station and a set of edge servers, which are physically co-located and share the same power supply in the cell site.", "startOffset": 67, "endOffset": 71}, {"referenceID": 13, "context": "x \u03bb(x, t)/[\u03b8(x, t)(1\u2212\u03c1(t))] by following the literature and modeling the base station as a queueing system [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "In a solar+wind system, photovoltaic modules and wind turbines can combine their output to power the edge system and charge the batteries [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 22, "context": "[23], [24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[23], [24].", "startOffset": 6, "endOffset": 10}, {"referenceID": 9, "context": "the value iteration and the policy iteration [10], in an offline manner.", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "The proof follows [24].", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "It is proven in [25] that the convergence of our proposed algorithm is equivalent to the convergence of the associated ordinary differential equation (O.", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "is guaranteed [26].", "startOffset": 14, "endOffset": 18}, {"referenceID": 26, "context": "If the above is true, then we can apply [27] (Section 4.", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "The proposed PDS-based learning algorithm is compared with three benchmark schemes: \u2022 Q-learning [10]: Q-learning is a famous model-free reinforcement learning technique for solving MDP problems.", "startOffset": 97, "endOffset": 101}], "year": 2017, "abstractText": "Mobile edge computing (a.k.a. fog computing) has recently emerged to enable in-situ processing of delay-sensitive applications at the edge of mobile networks. Providing grid power supply in support of mobile edge computing, however, is costly and even infeasible (in certain rugged or under-developed areas), thus mandating on-site renewable energy as a major or even sole power supply in increasingly many scenarios. Nonetheless, the high intermittency and unpredictability of renewable energy make it very challenging to deliver a high quality of service to users in energy harvesting mobile edge computing systems. In this paper, we address the challenge of incorporating renewables into mobile edge computing and propose an efficient reinforcement learning-based resource management algorithm, which learns on-the-fly the optimal policy of dynamic workload offloading (to the centralized cloud) and edge server provisioning to minimize the long-term system cost (including both service delay and operational cost). Our online learning algorithm uses a decomposition of the (offline) value iteration and (online) reinforcement learning, thus achieving a significant improvement of learning rate and runtime performance when compared to standard reinforcement learning algorithms such as Q-learning. We prove the convergence of the proposed algorithm and analytically show that the learned policy has a simple monotone structure amenable to practical implementation. Our simulation results validate the efficacy of our algorithm, which significantly improves the edge computing performance compared to fixed or myopic optimization schemes and conventional reinforcement learning algorithms. J. Xu and L. Chen are with the Department of Electrical and Computer Engineering, University of Miami. Email: jiexu@miami.edu, lx.chen@miami.edu. S. Ren is with the Department of Electrical and Computer Engineering, University of California, Riverside. Email: sren@ece.ucr.edu", "creator": "LaTeX with hyperref package"}}}