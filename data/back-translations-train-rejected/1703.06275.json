{"id": "1703.06275", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2017", "title": "Evolving Game Skill-Depth using General Video Game AI Agents", "abstract": "Most games have, or can be generalised to have, a number of parameters that may be varied in order to provide instances of games that lead to very different player experiences. The space of possible parameter settings can be seen as a search space, and we can therefore use a Random Mutation Hill Climbing algorithm or other search methods to find the parameter settings that induce the best games. One of the hardest parts of this approach is defining a suitable fitness function. In this paper we explore the possibility of using one of a growing set of General Video Game AI agents to perform automatic play-testing. This enables a very general approach to game evaluation based on estimating the skill-depth of a game. Agent-based play-testing is computationally expensive, so we compare two simple but efficient optimisation algorithms: the Random Mutation Hill-Climber and the Multi-Armed Bandit Random Mutation Hill-Climber. For the test game we use a space-battle game in order to provide a suitable balance between simulation speed and potential skill-depth. Results show that both algorithms are able to rapidly evolve game versions with significant skill-depth, but that choosing a suitable resampling number is essential in order to combat the effects of noise.", "histories": [["v1", "Sat, 18 Mar 2017 09:04:05 GMT  (1417kb,D)", "http://arxiv.org/abs/1703.06275v1", "9 pages, 17 figures, CEC2017"]], "COMMENTS": "9 pages, 17 figures, CEC2017", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jialin liu", "julian togelius", "diego perez-liebana", "simon m lucas"], "accepted": false, "id": "1703.06275"}, "pdf": {"name": "1703.06275.pdf", "metadata": {"source": "CRF", "title": "Evolving Game Skill-Depth using General Video Game AI Agents", "authors": ["Jialin Liu", "Julian Togelius", "Simon M. Lucas"], "emails": ["jialin.liu@essex.ac.uk", "julian.togelius@nyu.edu", "dperez@essex.ac.uk", "sml@essex.ac.uk"], "sections": [{"heading": null, "text": "This year it is so far that it will only take a few days to reach an agreement."}, {"heading": "II. AUTOMATIC GAME DESIGN AND DEPTH ESTIMATION", "text": "In fact, it's as if it's a way in which people are able to determine themselves what they want and what they don't want. (...) In fact, it's as if they are able to determine themselves. (...) It's as if people are able to determine themselves. (...) It's as if they were able to determine themselves. (...) It's as if they were able to determine themselves. (...) It's as if they were able to determine themselves. (...) It's as if they were able to determine themselves. (...) It's as if they were able to determine themselves. (...) It's as if they were able to determine themselves what they want. (...) It's as if they do it, as if they do it, as if they do it, as if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it."}, {"heading": "III. FRAMEWORK", "text": "We customize the two-player space battle game, adapted to the GVG AI framework by Liu et al. [6], then we use RMHC and MABRMHC to develop game parameters to provide some game instances that result in a high win rate for GVG AI sample MCTS agents. The main difference in the modified space battle game used in this work is the introduction of the weapons system. Each ship has the choice to fire a rocket after its cooldown time has expired. From now on, we use the term \"game\" to refer to a game instance, i.e. a specific configuration of game parameters. a) Spaceship: Each player / agent controller has a spaceship that has maximum speed vs units distance per game, and slows down over time. At each game tick, the player can choose to do nothing or make an action under {RotateClockwise, Red Throne}."}, {"heading": "IV. OPTIMISERS", "text": "We compare a Random Mutation Hill-Climber with a MultiArmed Bandit Random Mutation Hill-Climber in evolving instances of space battle games. This section is organized as follows: Section IV-A briefly reminds us of the Random Mutation Hill-Climber. Section IV-B presents the Multi-Armed Bandit Random Mutation Hill-Climber and its selection and mutation rules."}, {"heading": "A. Random Mutation Hill-Climber", "text": "The Random Mutation Hill-Climber (RMHC) is a simple but efficient derivative-free optimization method that is predominantly used in discrete domains [18], [19]. The pseudo-code of the RMHC is in algorithm 1. For each generation, a progeny is generated based on the best genome (parent) to date by mutating exactly one randomly selected gene. The best genome to date is updated when the fitness value of the offspring matches better or the best so far."}, {"heading": "B. Multi-Armed Bandit Random Mutation Hill-Climber", "text": "The question that arises is whether this is a value that leads to the maximum reward (equation 3). (Algorithm 1 Random Mutation Hill Climber (RMHC) with the maximum urgency (equation 2), in order to then change the parameter in dimension d to the value (arm), which leads to the maximum reward. (Algorithm 1 Random Mutation Hill Climber (RMHC) with the maximum urgency (equation 2). Require: X: Search Space Require: D = | X: Problem dimension (genome length) Require: F 7 \u2192 [0, 1]: Fitness Function1: Random Climber (RMHC). Require: X: Search Space Require: D = | X."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "First, we use the sample agent with an OpenLoop Monte Carlo tree search algorithm for two players provided by the GVG-AI framework, which uses the difference in results (Eq. 1) of both players as heuristic algorithm (referred to as OLMCTS), as player 1. No modification or adjustment has been made to this example agent. We implement a consistently rotating and shooting agent (referred to as RAS) as player 2. Specifically, the RAS is a deterministic agent, and according to Eq. 1, the OLMCTS aims to maximize (100 \u00d7 nbk (1) \u2212 c \u00d7 nbm (1) \u2212 (100 \u00d7 nbk (2) \u2212 c \u00d7 nbm (2)), where nbk (1) and nbk (2) the number of lives subtracted from the RAS, and OLMCTS the number of missiles subtracted from the RAS, RHC-Z RHC-Z RHC-Z RHC-Z RHC-Z RHC-Z RHC-Z RHC-Z RHC-RHC-Z RHC-Z RHC-Z RHC-Z RHC-Z RHC-Z RHC-RHC-Z RHC-Z RHC-Z RHC-RHC-Z RHC-Z RHC-Z RHC-Z RHC-Z RHC-Z RHC-Z RHC-Z-RHC-Z RHC-Z RHC-RHC-Z-Z-Z RHC-Z RHC-Z-RHC-Z RHC-Z-Z RHC-Z-Z RHC-RHC-Z RHC-Z-Z RHC-Z-Z-RHC-Z RHC-Z RHC-Z-Z RHC-Z RHC-Z-RHC-Z-RHC-Z RHC-Z-RHC-Z-Z RHC-Z-Z-Z-RHC-Z RHC-RHC-Z RHC-Z-Z RHC-Z-Z-RHC-RHC-RHC-Z-Z RHC-Z-Z-Z-RHC-Z-RHC-RHC-Z-RHC-Z-RHC-"}, {"heading": "A. Winning rate distribution", "text": "We use an OLMCTS agent as player 1 and an RAS agent as player 2. At each game tick, each agent is assigned 10 ms to decide an action. The average number of iterations performed by OLMCTS against RAS is 350. The time to return an action to RAS is negligible. Average win rates for 11 and 69 repeated attempts of all 14,400 legal games played by OLMCTS against RAS are in Fig. 2. The win rate for 69 attempts in each game instance varies between 20% and 100%. Of all cases, the OLMCTS does not achieve a 100% win rate in more than 5,000 games. Fig. 3 shows how the win rate varies with the change in each parameter. Maximum ship speed and thrust speed have a negligible impact on the average win rate of the OLMCTS. The higher the maximum rocket speed or shorter the cooldown time, the higher the average win rate is."}, {"heading": "B. Evolving games by RMHC and MABRMHC using different resampling numbers", "text": "In this context, it should be noted that the case is an accident."}, {"heading": "C. Evolving games in a larger search space", "text": "s radius are 10, 20, 30, 40 and 50. Thus, the search space is 6-dimensional and the total number of possible games has grown to 72,000 (5 times greater). Instead of a smart agent and a deterministic agent, we play the same OLMCTS agent (with 350 iterations) that was previously used in sections V-A and V-B against two of its instances: an OLMCTS with 700 iterations and an OLMCTS with 175 iterations, called OLMCTS700 and OLMCTS175. The same optimization process with RMHC and MABRMHC is repeated separately."}, {"heading": "D. But what are the evolved games actually like?", "text": "In order to understand the results of the optimization process, we examined a random sample of games that were found to have high fitness in the optimization process, and compared these with several games with low fitness. We can see some patterns in the high fitness games, one of which is simply having very high costs for firing rockets. This is somewhat disappointing, as it means that the OLMCTS agent achieves higher scores simply by staying far away from the RAS agent, who will quickly reach large negative scores. A more interesting pattern was low rocket costs, slow rockets, fast rotation speed, and fast launchers. This resulted in behavior in which the OLMCTS agent circles mostly in a straight line around the screen, spending most of the time outside the range of the RAS agent's missile. When approaching the RAS agent, the OLMCTS agent turns to intercept rockets and have low cost salvos (which typically hit all of the rockets and then have multiple fit) in the optimization process."}, {"heading": "VI. CONCLUSION AND FURTHER WORK", "text": "In this context, it is worth mentioning that this project is a project that moves towards the needs of people in developing countries and which aims to put the needs of people at the heart of society."}], "references": [{"title": "An experiment in automatic game design.", "author": ["J. Togelius", "J. Schmidhuber"], "venue": "Proceedings of the 2008 IEEE Conference on Computational Intelligence and Games,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Evolutionary game design", "author": ["C. Browne", "F. Maire"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games, vol. 2, no. 1, pp. 1\u201316, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Aesthetic considerations for automated platformer design.", "author": ["M. Cook", "S. Colton", "A. Pease"], "venue": "The Eighth Annual AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Procedural content generation in games: A textbook and an overview of current research", "author": ["N. Shaker", "J. Togelius", "M.J. Nelson"], "venue": "Procedural Content Generation in Games: A Textbook and an Overview of Current Research, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Discovering unique game variants", "author": ["A. Isaksen", "D. Gopstein", "J. Togelius", "A. Nealen"], "venue": "Computational Creativity and Games Workshop at the 2015 International Conference on Computational Creativity, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Rolling horizon coevolutionary planning for two-player video games", "author": ["J. Liu", "D. P\u00e9rez-Li\u00e9bana", "S.M. Lucas"], "venue": "Proceedings of the IEEE Computer Science and Electronic Engineering Conference (CEEC), 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Metagame in symmetric chess-like games", "author": ["B. Pell"], "venue": "1992.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1992}, {"title": "Towards automatic personalised content creation for racing games", "author": ["J. Togelius", "R. De Nardi", "S.M. Lucas"], "venue": "2007 IEEE Symposium on Computational Intelligence and Games. IEEE, 2007, pp. 252\u2013259.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Searchbased procedural content generation: A taxonomy and survey", "author": ["J. Togelius", "G.N. Yannakakis", "K.O. Stanley", "C. Browne"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games, vol. 3, no. 3, pp. 172\u2013186, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic content generation in the galactic arms race video game", "author": ["E.J. Hastings", "R.K. Guha", "K.O. Stanley"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games, vol. 1, no. 4, pp. 245\u2013 263, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards a generic framework for automated video game level creation", "author": ["N. Sorenson", "P. Pasquier"], "venue": "European Conference on the Applications of Evolutionary Computation. Springer, 2010, pp. 131\u2013 140.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic generation of game elements via evolution", "author": ["D. Ashlock"], "venue": "Proceedings of the 2010 IEEE Conference on Computational Intelligence and Games. IEEE, 2010, pp. 289\u2013296.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-faceted evolution of simple arcade games.", "author": ["M. Cook", "S. Colton"], "venue": "Proceedings of the 2011 IEEE Conference on Computational Intelligence and Games,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Towards automated game design", "author": ["M.J. Nelson", "M. Mateas"], "venue": "Congress of the Italian Association for Artificial Intelligence. Springer, 2007, pp. 626\u2013637.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Automated tweaking of levels for casual creation of mobile games", "author": ["E.J. Powley", "S. Gaudl", "S. Colton", "M.J. Nelson", "R. Saunders", "M. Cook"], "venue": "2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Depth in strategic games", "author": ["F. Lantz", "A. Isaksen", "A. Jaffe", "A. Nealen", "J. Togelius"], "venue": "under review, 2017.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "General video game evaluation using relative algorithm performance profiles", "author": ["T.S. Nielsen", "G.A. Barros", "J. Togelius", "M.J. Nelson"], "venue": "European Conference on the Applications of Evolutionary Computation. Springer, 2015, pp. 369\u2013380.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning DFA: Evolution versus Evidence Driven State Merging", "author": ["S.M. Lucas", "T.J. Reynolds"], "venue": "Evolutionary Computation, 2003. CEC\u201903. The 2003 Congress on, vol. 1. IEEE, 2003, pp. 351\u2013358.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning deterministic finite automata with a smart state labeling evolutionary algorithm", "author": ["\u2014\u2014"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 27, no. 7, pp. 1063\u20131074, 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Bandit-based random mutation hill-climbing", "author": ["J. Liu", "D. Pe\u0155ez-Liebana", "S.M. Lucas"], "venue": "arXiv preprint arXiv:1606.06041, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Bandit-based random mutation hill-climbing", "author": ["D.P.-L. Jialin Liu", "S.M. Lucas"], "venue": "Evolutionary Computation, 2017. CEC\u201917. The 2017 Congress on. IEEE, 2017.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Optimal resampling for the noisy onemax problem", "author": ["J. Liu", "M. Fairbank", "D. P\u00e9rez-Li\u00e9bana", "S.M. Lucas"], "venue": "arXiv preprint arXiv:1607.06641, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Portfolio methods in uncertain contexts", "author": ["J. Liu"], "venue": "Ph.D. dissertation, Universit\u00e9 Paris-Saclay, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "The n-tuple bandit evolutionary algorithm for automatic game improvement", "author": ["K. Kunanusont", "R.D. Gaina", "J. Liu", "D. Perez-Liebana", "S.M. Lucas"], "venue": "Evolutionary Computation, 2017. CEC\u201917. The 2017 Congress on. IEEE, 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "There have been various attempts to automate or part-automate the game generation process, as this is an interesting challenge for AI and computational creativity [1], [2], [3].", "startOffset": 163, "endOffset": 166}, {"referenceID": 1, "context": "There have been various attempts to automate or part-automate the game generation process, as this is an interesting challenge for AI and computational creativity [1], [2], [3].", "startOffset": 168, "endOffset": 171}, {"referenceID": 2, "context": "There have been various attempts to automate or part-automate the game generation process, as this is an interesting challenge for AI and computational creativity [1], [2], [3].", "startOffset": 173, "endOffset": 176}, {"referenceID": 3, "context": "Many video games require content to be produced for them, and recent years have seen a surge in AI-based procedural content generation [4].", "startOffset": 135, "endOffset": 138}, {"referenceID": 4, "context": "Recent work has demonstrated the potential of this approach, automatically generating distinct and novel variants of the minimalist mobile game Flappy Bird [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 5, "context": "[6] introduced a two-player space-battle game,", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] to the GVG-AI framework, then uses the Random Mutation Hill Climber (RMHC) and Multi-Armed Bandit RMHC (MABRMHC) to evolve game parameters to provide some game instances that lead to high winning rates for GVGAI sample MCTS agents.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Attempts to automatically design complete games go back to Barney Pell, who generated rules for chess-like games [7].", "startOffset": 113, "endOffset": 116}, {"referenceID": 7, "context": "[8] evolved racing tracks in a car racing game using a simple multi-objective evolutionary algorithm called Cascading Elitism.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "This can be seen as an early form of experience-driven procedural content generation [9], where game content is generated through search in content space using evolutionary computation or some other form of stochastic optimisation.", "startOffset": 85, "endOffset": 88}, {"referenceID": 9, "context": "Similar methods have since been used to generate many types of game content, such as particle systems for weapons in a space shooter [10], platform game levels [11] or puzzles [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "Similar methods have since been used to generate many types of game content, such as particle systems for weapons in a space shooter [10], platform game levels [11] or puzzles [12].", "startOffset": 160, "endOffset": 164}, {"referenceID": 11, "context": "Similar methods have since been used to generate many types of game content, such as particle systems for weapons in a space shooter [10], platform game levels [11] or puzzles [12].", "startOffset": 176, "endOffset": 180}, {"referenceID": 0, "context": "Togelius and Schmidhuber [1] used a simple hill-climber to generate single-player", "startOffset": 25, "endOffset": 28}, {"referenceID": 12, "context": "[13], [3], who used search-based methods to design rulesets, maps and", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[13], [3], who used search-based methods to design rulesets, maps and", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "In a similar vein, Browne and Maire [2] developed a system", "startOffset": 36, "endOffset": 39}, {"referenceID": 13, "context": "Nelson and Mateas [14], who use reasoning methods to create Wario Ware-style minigames out of verb-noun relations and common minigame design patterns.", "startOffset": 18, "endOffset": 22}, {"referenceID": 4, "context": "on generating playable Flappy Bird variants [5].", "startOffset": 44, "endOffset": 47}, {"referenceID": 14, "context": "[15] optimise the parameters of an abstract touch-based mobile game, showing that parameter changes to a single ruleset can give rise to what feels and plays like different games.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "This property is universally considered desirable by game designers, yet it is hard to define properly; some of the definitions build on the idea of a skill chain, where deeper games simply have more things that can be learned [16].", "startOffset": 227, "endOffset": 231}, {"referenceID": 16, "context": "Therefore, we can use game-playing agents of different strengths to play the same game, and the bigger the difference in outcome the greater the depth [17].", "startOffset": 151, "endOffset": 155}, {"referenceID": 5, "context": "[6] to the GVG-AI framework, then use RMHC and MABRMHC to evolve game parameters to provide some game instances that lead to high winning rate for GVG-AI sample", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "The Random Mutation Hill-Climber (RMHC) is a simple but efficient derivative-free optimisation method mostly used in discrete domains [18], [19].", "startOffset": 134, "endOffset": 138}, {"referenceID": 18, "context": "The Random Mutation Hill-Climber (RMHC) is a simple but efficient derivative-free optimisation method mostly used in discrete domains [18], [19].", "startOffset": 140, "endOffset": 144}, {"referenceID": 19, "context": "Multi-Armed Bandit Random Mutation Hill-Climber (MABRMHC), derived from the 2-armed bandit-based RMHC [20], [21], uses both UCB-style selection and mutation rules.", "startOffset": 102, "endOffset": 106}, {"referenceID": 20, "context": "Multi-Armed Bandit Random Mutation Hill-Climber (MABRMHC), derived from the 2-armed bandit-based RMHC [20], [21], uses both UCB-style selection and mutation rules.", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "Require: X : search space Require: D = |X |: problem dimension (genome length) Require: f : X 7\u2192 [0, 1]: fitness function 1: Randomly initialise a genome x \u2208 X 2: bestF itSoFar \u2190 0 3: M \u2190 0 .", "startOffset": 97, "endOffset": 103}, {"referenceID": 0, "context": "Require: X : search space Require: D = |X |: problem dimension (genome length) Require: f : X 7\u2192 [0, 1]: fitness function 1: Randomly initialise a genome x \u2208 X 2: bestF itSoFar \u2190 0 3: M \u2190 0 .", "startOffset": 97, "endOffset": 103}, {"referenceID": 21, "context": "A recent work applied the RMHC and a two-armed banditbased RMHC with resamplings to a noisy variant of the OneMax problem, and showed both theoretically and practically the importance of choosing a suitable resampling number to accelerate the convergence to the optimum [22], [20], [21].", "startOffset": 270, "endOffset": 274}, {"referenceID": 19, "context": "A recent work applied the RMHC and a two-armed banditbased RMHC with resamplings to a noisy variant of the OneMax problem, and showed both theoretically and practically the importance of choosing a suitable resampling number to accelerate the convergence to the optimum [22], [20], [21].", "startOffset": 276, "endOffset": 280}, {"referenceID": 20, "context": "A recent work applied the RMHC and a two-armed banditbased RMHC with resamplings to a noisy variant of the OneMax problem, and showed both theoretically and practically the importance of choosing a suitable resampling number to accelerate the convergence to the optimum [22], [20], [21].", "startOffset": 282, "endOffset": 286}, {"referenceID": 22, "context": "Using dynamic non-adaptive or adaptive resampling numbers increasing with the generation number, such as the resampling rules discussed in [23], to take the strength of both small and big numbers of resamplings will be favourable.", "startOffset": 139, "endOffset": 143}, {"referenceID": 23, "context": "A N-Tuple Bandit Evolutionary Algorithm [24] is proposed to handle such case.", "startOffset": 40, "endOffset": 44}, {"referenceID": 4, "context": "[5] used Euclidean distance for measuring distance between game instances of Flappy Bird and discovered that such a simple measure can be misleading, since the difference between game instances does not always reflect the difference between their parameter values.", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "Most games have, or can be generalised to have, a number of parameters that may be varied in order to provide instances of games that lead to very different player experiences. The space of possible parameter settings can be seen as a search space, and we can therefore use a Random Mutation Hill Climbing algorithm or other search methods to find the parameter settings that induce the best games. One of the hardest parts of this approach is defining a suitable fitness function. In this paper we explore the possibility of using one of a growing set of General Video Game AI agents to perform automatic playtesting. This enables a very general approach to game evaluation based on estimating the skill-depth of a game. Agent-based playtesting is computationally expensive, so we compare two simple but efficient optimisation algorithms: the Random Mutation HillClimber and the Multi-Armed Bandit Random Mutation HillClimber. For the test game we use a space-battle game in order to provide a suitable balance between simulation speed and potential skill-depth. Results show that both algorithms are able to rapidly evolve game versions with significant skill-depth, but that choosing a suitable resampling number is essential in order to combat the effects of noise.", "creator": "LaTeX with hyperref package"}}}