{"id": "1703.06283", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2017", "title": "Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters", "abstract": "As autonomous vehicles become an every-day reality, high-accuracy pedestrian detection is of paramount practical importance. Pedestrian detection is a highly researched topic with mature methods, but most datasets focus on common scenes of people engaged in typical walking poses on sidewalks. But performance is most crucial for dangerous scenarios, such as children playing in the street or people using bicycles/skateboards in unexpected ways. Such \"in-the-tail\" data is notoriously hard to observe, making both training and testing difficult. To analyze this problem, we have collected a novel annotated dataset of dangerous scenarios called the Precarious Pedestrian dataset. Even given a dedicated collection effort, it is relatively small by contemporary standards (around 1000 images). To explore large-scale data-driven learning, we explore the use of synthetic data generated by a game engine. A significant challenge is selected the right \"priors\" or parameters for synthesis: we would like realistic data with realistic poses and object configurations. Inspired by Generative Adversarial Networks, we generate a massive amount of synthetic data and train a discriminative classifier to select a realistic subset, which we deem Synthetic Imposters. We demonstrate that this pipeline allows one to generate realistic training data by making use of rendering/animation engines. Interestingly, we also demonstrate that such data can be used to rank algorithms, suggesting that Synthetic Imposters can also be used for \"in-the-tail\" validation at test-time, a notoriously difficult challenge for real-world deployment.", "histories": [["v1", "Sat, 18 Mar 2017 10:52:53 GMT  (1970kb)", "http://arxiv.org/abs/1703.06283v1", "To appear in CVPR 2017"], ["v2", "Mon, 10 Apr 2017 14:59:25 GMT  (3308kb,D)", "http://arxiv.org/abs/1703.06283v2", "To appear in CVPR 2017"]], "COMMENTS": "To appear in CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["shiyu huang", "deva ramanan"], "accepted": false, "id": "1703.06283"}, "pdf": {"name": "1703.06283.pdf", "metadata": {"source": "CRF", "title": "Recognition in-the-Tail: Training Detectors for Unusual Pedestrians with Synthetic Imposters", "authors": ["Shiyu Huang", "Deva Ramanan"], "emails": ["huangsy13@mails.tsinghua.edu.cn", "deva@cs.cmu.edu"], "sections": [{"heading": null, "text": "Pedestrian detection is a highly researched subject with sophisticated methods, but most datasets focus on shared scenes of people taking typical walking positions on sidewalks. However, performance is critical for dangerous scenarios, such as children playing in the street or people using bicycles / skateboards in unexpected ways. Such \"in-the-tail\" data is notoriously difficult to observe, making both training and testing difficult. To analyze this problem, we have collected a novel annotated dataset of dangerous scenarios called the Precious Pedestrian Dataset. Even with a dedicated collection effort, they are relatively small by today's standards (about 1,000 images). To explore data-driven learning on a larger scale, we are investigating the use of synthetic data generated by a game machine. A significant challenge is selecting the right \"priorities\" or creating parameters for realistic objects that we would classify realistically from synthetic data."}, {"heading": "1. Introduction", "text": "In fact, it is such that most of them are able to survive themselves without there being a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, in which there is a process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process of the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process of the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the process, the"}, {"heading": "2. Related work", "text": "Synthetic data sets were used to train and evaluate the performance of computer vision algorithms. Some types of basic truths are hard to obtain from hand labeling, such as the optical flow, but easily accessible through the computer. Adam et al. [20] used a 3D game machine to generate synthetic data, and learned a physical intuition to predict the falling progress of Block Tower. Mayer et al. [23] issued a benchmark for synthetic data. They designed a computer program to generate synthetic images with disparity, and rated the FlowNet [13]. SawNet et al. [32] used synthetic data to improve image segment performance."}, {"heading": "3. Dataset", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Synthetic Dataset", "text": "In this project, we use Unity 3D as our basic tool for generating synthetic data. Figure 2 shows that we can avoid types of human gestures that include women, men, cyclists and skateboarders. Unlike Javier et al., who use a single virtual city, we can achieve the variety of wallpapers so that we also get types of human gestures. We design a scene that captures 2D images. Unlike Javier et al., who use a single virtual city, our synthetic data can be achieved by rendering real images."}, {"heading": "3.2. Precarious Dataset", "text": "We collect precarious datasets from Google Images and Baidu Images. We also add a few selected images from MPII Dataset [2]. Finally, there are 951 reasonable images in the precarious dataset. We then label bouncing boxes for each image manually. Precarious datasets contain scenes and people, such as children walking on the street, people watching their mobile phone cross the road, people slipping, motorcyclists making dangerous movements, overloaded motorcycles, and people with umbrellas. According to a rough statistic, there are about four hundred cyclists, fifty motorcycles, five hundred pedestrians in the precarious dataset. Figure 6 shows some examples in the precarious dataset. Compared with Caltech Dataset in Figure 6, precarious datasets contain more dangerous situations, and these situations will be a very difficult task for person detectors. Figure 5 shows the comparison of precarious datasets and Caltech dataset in terms of human types."}, {"heading": "3.3. Data Pre-Processing", "text": "Our original synthetic data and precarious data differ in size. For convenience, we reduce all images to a standard size with a resolution of 960 x 720. Firstly, we maintain the aspect ratio of the original images and reduce them to a maximum width of 720 pixels or a maximum height of 540 pixels. Secondly, we randomly scale down images to a black background with a resolution of 960 x 720. The final images are the same size. As the original image size of Caltech Dataset is 640 x 480 and has the same aspect ratio with the target size, we reduce them directly to 960 x 720. All standardized image data can be trained and tested with our detectors."}, {"heading": "4. Proposed Method", "text": "In this section, we will introduce a novel framework called a selection model that can transfer our RCNN-based detector from synthetic data to real data."}, {"heading": "4.1. Selecting Imposters", "text": "We start with the definitions of the terms 6 = PT (QxS), QxS (QxS), QS (QS), QS (XS), QS (XS), QS (XS), QS (XS), DS (XS) and real data (XS). We start with the definitions of the terminologies 6 = PT (QxS), QS (QxS), QS (XS), QS (XS), QS (XS), QS (QxS), QxS (QxS), QS (QxS), QS (XS), QS (XS), QS (XS), QS (XS). We start with the definitions of the terminologies 6 = PT (QxS), QxS (QxS)."}, {"heading": "4.2. RPN+", "text": "Our detection model, called RPN +, is based on RPN. Figure 4 shows the architecture of RPN +. RPN + is a fully revolutionary neural network implemented with TensorFlow. We concatenate multiple layers at different levels to improve the ability to locate people at different resolutions. We use 9 anchors (reference boxes with 3 scales and aspect ratios) at each sliding position. Standard boxes are tuned to the truth of the Earth with Jaccard overlaps of more than 50%. A box is treated as a negative box if its Jaccard overlap with the truth of the ground is less than 20%. To speed up training time, we use a pre-programmed model VGG-16 as a network initialization. We also freeze the first two revolutionary layers during training."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Evaluation Metrics", "text": "The standard overlap is 50%, but we also include the results of overlap criteria of 70%. The average error rate is used as a comprehensive assessment index. Baselines: We compare proposed detectors with the following basic approaches for precarious test data sets. ACF: An aggregate channel has a detector [8] trained on Caltech pedestrian data sets. LDCF: An LDCF detector [26] trained on Caltech pedestrian data sets. HOG + Cascade: A cascade of enhanced classifiers working with HOG functions [42]. HARR + Cascade: A cascade of increased classifiers working with hairlike functions [39, 21]. RPN / BF: The RPN detection model working with enhanced forest functions."}, {"heading": "5.2. Evaluation for Detectors", "text": "We plot ROC curves of different detectors with 50% and 70% overlap criteria, both shown in Figure 6. The curves show that our proposed detector is significantly superior to other detectors. Table 3 shows the average error rate of all detectors. Our proposed method exceeds all baselines with an average error rate of 42.47%. Our RPN + detector trained on the Caltech Pedestrian Data Set achieves the second best result with an average error rate of 58.82%. Figure 9 shows the selected visualized results of the proposed detector and the RPN + detector trained only on Caltech. These examples illustrate that our method can improve the ability to locate people and is more flexible in aspect ratios. We also test our RPN + model on Caltech dataset, and Figure 7 shows the results of different detectors on Caltech dataset. All detectors are trained on Caltech dataset. As a reference, our RPN 68 model would currently rank on Caltech datasets."}, {"heading": "5.3. Effect of the Selector and Imposter Number", "text": "In this section, we will examine some experiments to show how our selector can improve detection performance. Detection models in the three steps of the selection model are tested on precarious datasets, and we will also train a model without the second step (marginal distribution adjustment), which means that we will refine the model trained on synthetic datasets directly. In addition, we will train a model based only on precarious training datasets. Table 4 shows the effects of different training strategies. The model that finishes all three steps achieves the best result, which is 42.47%, and the model that has been refined on stackers and real training data achieves the second best result, which is 45.97%. Comparing the results of NoStep2 (no marginal distribution adjustment) and Step3 (our proposed method), it shows that without cheaters, a reduction in detection performance of about 6% is replicated on detection performance."}, {"heading": "5.4. Evaluation on Synthetic Imposter Dataset", "text": "The RPN + and ACF detectors are trained on appropriate training data sets for test data sets (e.g., the detector that tests on real data is trained only on real training data). Figure 8 and Figure 10 show that performance on both real test data and synthetic test data has the same ranking. We think the synthetic data can be used as a test data set to evaluate the detectors when real test data is missing."}, {"heading": "6. Conclusion", "text": "Motivated by the fact that rare but dangerous scenes are precisely the scenarios on which visual recognition should be characterized, we first analyze existing data sets and illustrate that they do not contain sufficient rare scenarios (because, of course, they focus on ordinary or typical urban scenes).To close this gap, we have compiled our own data set of precarious people, which we will publish in order to advance further research on this important (but not yet researched) problem. Precarious scenes are difficult because little data is available for both evaluation and education. To meet this challenge, we propose the use of synthetic data generated with a game engine. However, it is difficult to ensure that the synthesized data matches the statistics of real precarious scenarios. Inspired by generative opposing networks, we induce the use of a discriminatory classifier (trained to distinguish real from synthetic data) to implicate this data."}], "references": [{"title": "A local basis representation for estimating human pose from cluttered images", "author": ["A. Agarwal", "B. Triggs"], "venue": "In Asian Conference on Computer Vision,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "A database-based framework for gesture recognition", "author": ["V. Athitsos", "H. Wang", "A. Stefan"], "venue": "Personal and Ubiquitous Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Model-based validation approaches and matching techniques for automotive vision based pedestrian detection", "author": ["A. Broggi", "A. Fascioli", "P. Grisleri", "T. Graf", "M. Meinecke"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel"], "venue": "arXiv preprint arXiv:1606.03657,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "R. Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Fast feature pyramids for object detection", "author": ["P. Doll\u00e1r", "R. Appel", "S. Belongie", "P. Perona"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Pedestrian detection: A benchmark", "author": ["P. Doll\u00e1r", "C. Wojek", "B. Schiele", "P. Perona"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Pedestrian detection: An evaluation of the state of the art", "author": ["P. Doll\u00e1r", "C. Wojek", "B. Schiele", "P. Perona"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Monocular pedestrian detection: Survey and experiments", "author": ["M. Enzweiler", "D.M. Gavrila"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "A mobile vision system for robust multi-person tracking", "author": ["A. Ess", "B. Leibe", "K. Schindler", "L. van Gool"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["P. Fischer", "A. Dosovitskiy", "E. Ilg", "P. H\u00e4usser", "C. Haz\u0131rba\u015f", "V. Golkov", "P. van der Smagt", "D. Cremers", "T. Brox"], "venue": "arXiv preprint arXiv:1504.06852,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D.Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Inferring 3d structure with a statistical image-based shape model", "author": ["K. Grauman", "G. Shakhnarovich", "T. Darrell"], "venue": "In Computer Vision,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Discriminative decorrelation for clustering and classification", "author": ["B. Hariharan", "J. Malik", "D. Ramanan"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Learning scene-specific pedestrian detectors without real data", "author": ["H. Hattori", "V. Naresh Boddeti", "K.M. Kitani", "T. Kanade"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Analysis by synthesis: 3d object recognition by object reconstruction", "author": ["M. Hejrati", "D. Ramanan"], "venue": "In 2014 IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Unsupervised feature learning for 3d scene labeling", "author": ["K. Lai", "L. Bo", "D. Fox"], "venue": "In 2014 IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Learning physical intuition of block towers by example", "author": ["A. Lerer", "S. Gross", "R. Fergus"], "venue": "arXiv preprint arXiv:1603.01312,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "An extended set of haar-like features for rapid object detection", "author": ["R. Lienhart", "J. Maydt"], "venue": "In Image Processing", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Learning appearance in virtual scenarios for pedestrian detection", "author": ["J. Marin", "D. V\u00e1zquez", "D. Ger\u00f3nimo", "A.M. L\u00f3pez"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation", "author": ["N. Mayer", "E. Ilg", "P. H\u00e4usser", "P. Fischer", "D. Cremers", "A. Dosovitskiy", "T. Brox"], "venue": "arXiv preprint arXiv:1512.02134,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": "arXiv preprint arXiv:1411.1784,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "3d pose-by-detection of vehicles via discriminatively reduced ensembles of correlation filters", "author": ["Y. Movshovitz-Attias", "Y. Sheikh", "V.N. Boddeti", "Z. Wei"], "venue": "In BMVC,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Local decorrelation for improved pedestrian detection", "author": ["W. Nam", "P. Doll\u00e1r", "J.H. Han"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Teaching 3d geometry to deformable part models", "author": ["B. Pepik", "M. Stark", "P. Gehler", "B. Schiele"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Learning people detection models from few training samples", "author": ["L. Pishchulin", "A. Jain", "C. Wojek", "M. Andriluka", "T. Thorm\u00e4hlen", "B. Schiele"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Nearest neighbor search methods for handshape recognition", "author": ["M. Potamias", "V. Athitsos"], "venue": "In Proceedings of the 1st international conference on PErvasive Technologies Related to Assistive Environments,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Playing for data: Ground truth from computer games", "author": ["S.R. Richter", "V. Vineet", "S. Roth", "V. Koltun"], "venue": "arXiv preprint arXiv:1608.02192,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Hands in action: real-time 3d reconstruction of hands in interaction with objects", "author": ["J. Romero", "H. Kjellstr\u00f6m", "D. Kragic"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes", "author": ["G. Ros", "L. Sellart", "J. Materzynska", "D. Vazquez", "A.M. Lopez"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Improved techniques for training gans", "author": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": "arXiv preprint arXiv:1606.03498,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "Data-driven scene understanding from 3d models", "author": ["S. Satkin", "J. Lin", "M. Hebert"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "From virtual to reality: Fast adaptation of virtual object detectors to real domains", "author": ["B. Sun", "K. Saenko"], "venue": "In BMVC,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Virtual and real world adaptation for pedestrian detection", "author": ["D. Vazquez", "A.M. Lopez", "J. Marin", "D. Ponsa", "D. Geronimo"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. Viola", "M. Jones"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2001}, {"title": "Multi-cue onboard pedestrian detection", "author": ["C.Wojek", "S.Walk", "B. Schiele"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "Is faster r-cnn doing well for pedestrian detection", "author": ["L. Zhang", "L. Lin", "X. Liang", "K. He"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Fast human detection using a cascade of histograms of oriented gradients", "author": ["Q. Zhu", "M.-C. Yeh", "K.-T. Cheng", "S. Avidan"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2006}], "referenceMentions": [{"referenceID": 6, "context": "poses on sidewalks [9, 6, 12, 11, 40].", "startOffset": 19, "endOffset": 37}, {"referenceID": 9, "context": "poses on sidewalks [9, 6, 12, 11, 40].", "startOffset": 19, "endOffset": 37}, {"referenceID": 8, "context": "poses on sidewalks [9, 6, 12, 11, 40].", "startOffset": 19, "endOffset": 37}, {"referenceID": 37, "context": "poses on sidewalks [9, 6, 12, 11, 40].", "startOffset": 19, "endOffset": 37}, {"referenceID": 17, "context": "Synthetic training data is an actively explored topic because it provides a potentially infinite well of annotated data for training data-hungry architectures [20, 23, 13, 17, 32, 34].", "startOffset": 159, "endOffset": 183}, {"referenceID": 20, "context": "Synthetic training data is an actively explored topic because it provides a potentially infinite well of annotated data for training data-hungry architectures [20, 23, 13, 17, 32, 34].", "startOffset": 159, "endOffset": 183}, {"referenceID": 10, "context": "Synthetic training data is an actively explored topic because it provides a potentially infinite well of annotated data for training data-hungry architectures [20, 23, 13, 17, 32, 34].", "startOffset": 159, "endOffset": 183}, {"referenceID": 14, "context": "Synthetic training data is an actively explored topic because it provides a potentially infinite well of annotated data for training data-hungry architectures [20, 23, 13, 17, 32, 34].", "startOffset": 159, "endOffset": 183}, {"referenceID": 29, "context": "Synthetic training data is an actively explored topic because it provides a potentially infinite well of annotated data for training data-hungry architectures [20, 23, 13, 17, 32, 34].", "startOffset": 159, "endOffset": 183}, {"referenceID": 31, "context": "Synthetic training data is an actively explored topic because it provides a potentially infinite well of annotated data for training data-hungry architectures [20, 23, 13, 17, 32, 34].", "startOffset": 159, "endOffset": 183}, {"referenceID": 11, "context": "Synthetic imposters: We address both concerns with a novel variant of Generative Adversarial Networks (GANs) [14], a method for synthesizing data from latent noise vectors.", "startOffset": 109, "endOffset": 113}, {"referenceID": 38, "context": "We find surprisingly good results with a (to our knowledge) novel variant of region proposal network (RPN) [41] tuned for particular objects (precarious people) rather than a general class of objectness detections.", "startOffset": 107, "endOffset": 111}, {"referenceID": 28, "context": "Instead of classifying a sparse set of proposed windows (as nearly all contemporary object detection systems based on RCNN do [31]), this network returns a dense heatmap of pedestrian detections, along with regressed bounding box location for each pixel location in the heatmap.", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "[20] used a 3D game engine to generate synthetic data and learned a physical intuition to predict the falling progress of block tower.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] gave out a benchmark of synthetic data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "They designed a computer program to generate synthetic images with disparity, optical flow and disparity change, and evaluate the FlowNet[13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 29, "context": "[32] used synthetic data to improve image segmentation performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[34] used Unity Development Platform to generate a synthetic urban scene dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Previous work with 3D computer graphics models has widely been used for modeling kinds of human body shapes [15, 4, 1, 22, 29, 3, 33].", "startOffset": 108, "endOffset": 133}, {"referenceID": 2, "context": "Previous work with 3D computer graphics models has widely been used for modeling kinds of human body shapes [15, 4, 1, 22, 29, 3, 33].", "startOffset": 108, "endOffset": 133}, {"referenceID": 0, "context": "Previous work with 3D computer graphics models has widely been used for modeling kinds of human body shapes [15, 4, 1, 22, 29, 3, 33].", "startOffset": 108, "endOffset": 133}, {"referenceID": 19, "context": "Previous work with 3D computer graphics models has widely been used for modeling kinds of human body shapes [15, 4, 1, 22, 29, 3, 33].", "startOffset": 108, "endOffset": 133}, {"referenceID": 26, "context": "Previous work with 3D computer graphics models has widely been used for modeling kinds of human body shapes [15, 4, 1, 22, 29, 3, 33].", "startOffset": 108, "endOffset": 133}, {"referenceID": 1, "context": "Previous work with 3D computer graphics models has widely been used for modeling kinds of human body shapes [15, 4, 1, 22, 29, 3, 33].", "startOffset": 108, "endOffset": 133}, {"referenceID": 30, "context": "Previous work with 3D computer graphics models has widely been used for modeling kinds of human body shapes [15, 4, 1, 22, 29, 3, 33].", "startOffset": 108, "endOffset": 133}, {"referenceID": 24, "context": "Moreover, 3D simulation can also been used for car detection [27, 25, 18] and scene understanding [36, 19].", "startOffset": 61, "endOffset": 73}, {"referenceID": 22, "context": "Moreover, 3D simulation can also been used for car detection [27, 25, 18] and scene understanding [36, 19].", "startOffset": 61, "endOffset": 73}, {"referenceID": 15, "context": "Moreover, 3D simulation can also been used for car detection [27, 25, 18] and scene understanding [36, 19].", "startOffset": 61, "endOffset": 73}, {"referenceID": 33, "context": "Moreover, 3D simulation can also been used for car detection [27, 25, 18] and scene understanding [36, 19].", "startOffset": 98, "endOffset": 106}, {"referenceID": 16, "context": "Moreover, 3D simulation can also been used for car detection [27, 25, 18] and scene understanding [36, 19].", "startOffset": 98, "endOffset": 106}, {"referenceID": 19, "context": "[22] used a game engine to generate synthetic training data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[28] used 8 HD cameras to scan human body and built real 3D human models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] used 3D modelling software to build a special scene and randomly put 3D models on a special background for pedestrian detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Sun and Saenko [37] used 3D models to train detectors for real objects.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "[16] and can be used only for feature-based detectors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[38] used synthetic pedestrian data to generate robust real-world detectors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "GAN [14] is a deep network which can generate fake images from latent code and train a neural network to discriminate whether one image is real or generated.", "startOffset": 4, "endOffset": 8}, {"referenceID": 21, "context": "Recent works [24, 7, 30, 35, 5] on GAN used neural networks to generate realistic data from latent code.", "startOffset": 13, "endOffset": 31}, {"referenceID": 4, "context": "Recent works [24, 7, 30, 35, 5] on GAN used neural networks to generate realistic data from latent code.", "startOffset": 13, "endOffset": 31}, {"referenceID": 27, "context": "Recent works [24, 7, 30, 35, 5] on GAN used neural networks to generate realistic data from latent code.", "startOffset": 13, "endOffset": 31}, {"referenceID": 32, "context": "Recent works [24, 7, 30, 35, 5] on GAN used neural networks to generate realistic data from latent code.", "startOffset": 13, "endOffset": 31}, {"referenceID": 3, "context": "Recent works [24, 7, 30, 35, 5] on GAN used neural networks to generate realistic data from latent code.", "startOffset": 13, "endOffset": 31}, {"referenceID": 2, "context": "Number of 3D models [4, 8] Index of background images [0, 1726) Index of 3D models [0, 20) Position of 3D models Within the field of vision Index of Animations [0,maxnumber) Time of animation [0, 1] Model\u2019s angle on the x axis [\u221290, 90] Model\u2019s angle on the y axis [\u2212180, 180] Model\u2019s angle on the z axis [\u221290, 90] Light intensity [0.", "startOffset": 20, "endOffset": 26}, {"referenceID": 5, "context": "Number of 3D models [4, 8] Index of background images [0, 1726) Index of 3D models [0, 20) Position of 3D models Within the field of vision Index of Animations [0,maxnumber) Time of animation [0, 1] Model\u2019s angle on the x axis [\u221290, 90] Model\u2019s angle on the y axis [\u2212180, 180] Model\u2019s angle on the z axis [\u221290, 90] Light intensity [0.", "startOffset": 20, "endOffset": 26}, {"referenceID": 0, "context": "Number of 3D models [4, 8] Index of background images [0, 1726) Index of 3D models [0, 20) Position of 3D models Within the field of vision Index of Animations [0,maxnumber) Time of animation [0, 1] Model\u2019s angle on the x axis [\u221290, 90] Model\u2019s angle on the y axis [\u2212180, 180] Model\u2019s angle on the z axis [\u221290, 90] Light intensity [0.", "startOffset": 192, "endOffset": 198}, {"referenceID": 19, "context": "[22] who used a single virtual urban city, our synthetic data can achieve the diversity of background by rendering real images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17], we don\u2019t set a special mode to generate synthetic data, and our parameters are consecutive and randomly generated.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "RPN [41] is the state-of-the-art pedestrian detector on Caltech dataset.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "Evaluation method of Caltech pedestrian dataset [10] is used as our 2D bounding box evaluation.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "ACF: An aggregate channel features detector[8] trained on Caltech pedestrian dataset.", "startOffset": 43, "endOffset": 46}, {"referenceID": 23, "context": "LDCF: A LDCF detector [26] trained on Caltech Pedestrian dataset.", "startOffset": 22, "endOffset": 26}, {"referenceID": 39, "context": "HOG+Cascade: A cascade of boosted classifiers working with HOG features[42].", "startOffset": 71, "endOffset": 75}, {"referenceID": 36, "context": "HARR+Cascade: A cascade of boosted classifiers working with haar-like features[39, 21].", "startOffset": 78, "endOffset": 86}, {"referenceID": 18, "context": "HARR+Cascade: A cascade of boosted classifiers working with haar-like features[39, 21].", "startOffset": 78, "endOffset": 86}, {"referenceID": 38, "context": "RPN/BF: The RPN detection model trained with boosted forest[41].", "startOffset": 59, "endOffset": 63}, {"referenceID": 36, "context": "HAAR [39, 21] 95.", "startOffset": 5, "endOffset": 13}, {"referenceID": 18, "context": "HAAR [39, 21] 95.", "startOffset": 5, "endOffset": 13}, {"referenceID": 39, "context": "4% HOG [42] 89.", "startOffset": 7, "endOffset": 11}, {"referenceID": 5, "context": "28% ACF[8] 73.", "startOffset": 7, "endOffset": 10}, {"referenceID": 23, "context": "50% LDCF [26] 71.", "startOffset": 9, "endOffset": 13}, {"referenceID": 38, "context": "37% RPN/BF [41] 54.", "startOffset": 11, "endOffset": 15}], "year": 2017, "abstractText": "As autonomous vehicles become an every-day reality, high-accuracy pedestrian detection is of paramount practical importance. Pedestrian detection is a highly researched topic with mature methods, but most datasets focus on common scenes of people engaged in typical walking poses on sidewalks. But performance is most crucial for dangerous scenarios, such as children playing in the street or people using bicycles/skateboards in unexpected ways. Such \u201cinthe-tail\u201d data is notoriously hard to observe, making both training and testing difficult. To analyze this problem, we have collected a novel annotated dataset of dangerous scenarios called the Precarious Pedestrian dataset. Even given a dedicated collection effort, it is relatively small by contemporary standards (\u2248 1000 images). To explore largescale data-driven learning, we explore the use of synthetic data generated by a game engine. A significant challenge is selected the right \u201cpriors\u201d or parameters for synthesis: we would like realistic data with realistic poses and object configurations. Inspired by Generative Adversarial Networks, we generate a massive amount of synthetic data and train a discriminative classifier to select a realistic subset, which we deem Synthetic Imposters. We demonstrate that this pipeline allows one to generate realistic training data by making use of rendering/animation engines. Interestingly, we also demonstrate that such data can be used to rank algorithms, suggesting that Synthetic Imposters can also be used for \u201cin-the-tail\u201d validation at test-time, a notoriously difficult challenge for real-world deployment.", "creator": "LaTeX with hyperref package"}}}