{"id": "1703.06284", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2017", "title": "Multi-talker Speech Separation with Utterance-level Permutation Invariant Training of Deep Recurrent Neural Networks", "abstract": "Despite the significant progress made in the recent years in dictating single-talker speech, the progress made in speaker independent multi-talker mixed speech separation and tracing, often referred to as the cocktail-party problem, has been less impressive. In this paper we propose a novel technique for attacking this problem. The core of our technique is permutation invariant training (PIT), which aims at minimizing the source stream reconstruction error no matter how labels are ordered. This is achieved by aligning labels to the output streams automatically during the training time. This strategy effectively solves the label permutation problem observed in deep learning based techniques for speech separation. More interestingly, our approach can integrate speaker tracing in the PIT framework so that separation and tracing can be carried out in one step and trained end-to-end. This is achieved using recurrent neural networks (RNNs) by forcing separated frames belonging to the same speaker to be aligned to the same output layer during training. Furthermore, the computational cost introduced by PIT is very small compared to the RNN computation during training and is zero during separation. We evaluated PIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks and found that it compares favorably to non-negative matrix factorization (NMF), computational auditory scene analysis (CASA), deep clustering (DPCL) and deep attractor network (DANet), and generalizes well over unseen speakers and languages.", "histories": [["v1", "Sat, 18 Mar 2017 10:59:03 GMT  (2955kb,D)", "https://arxiv.org/abs/1703.06284v1", null], ["v2", "Tue, 11 Jul 2017 12:02:01 GMT  (3523kb,D)", "http://arxiv.org/abs/1703.06284v2", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["morten kolb{\\ae}k", "dong yu", "zheng-hua tan", "jesper jensen"], "accepted": false, "id": "1703.06284"}, "pdf": {"name": "1703.06284.pdf", "metadata": {"source": "CRF", "title": "Multi-talker Speech Separation with Utterance-level Permutation Invariant Training of Deep Recurrent Neural Networks", "authors": ["Morten Kolb\u00e6k", "Dong Yu", "Zheng-Hua Tan", "Jesper Jensen"], "emails": ["mok@es.aau.dk;", "zt@es.aau.dk).", "dongyu@ieee.org).", "jje@es.aau.dk;", "jesj@oticon.com)."], "sections": [{"heading": null, "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "II. MONAURAL SPEECH SEPARATION", "text": "The goal of the monaural reflection is to estimate the individual source signals (n = 1). (n = 1) In the real world the received signals will be in the real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real n n n n n n n n n n) real real real real real real real real real real real real real real real real real real real real real real real real real) real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real real"}, {"heading": "III. MASKS AND TRAINING CRITERIA", "text": "Since masks can be considered an intermediate step in estimating the size spectrum of the source signals, we are not represented in the following three popular masks defined for separating individual conversations from sound. (5) When the Y mask phase is used for reconstruction, the IRM reaches the highest signal for distortion ratio (SDR) [44] when all sources have the same phase (which is an illegal assumption in general). IRMs are limited to 0 \u2264 M irms (t, f)."}, {"heading": "D. Training Criterion", "text": "Since we first estimate masks that can estimate the size spectrum of each source, the model parameters can be optimized to minimize the Mean Squared Error (MSE) between the estimated mask M and one of the above target masks. This approach poses two problems: First, in the silence segments, | Xs (t, f) | = 0 and | Y (t, f) | = 0, so that the target masks M (t, f) are not well defined. Second, what really matters to us is the error between the reconstructed source signal and the true source signal. To overcome these limitations, recent work [27] is directly minimized as nitrate S (s)."}, {"heading": "IV. PERMUTATION INVARIANT TRAINING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Conventional Multi-Talker Separation", "text": "A natural and commonly used approach to language separation based on deep learning is to present the problem as a multi-class problem [30], [35], [46], as in Fig. 1. In this traditional two-talker separation model, J-frames of mixed signal Y feature vectors are used as input for a deep learning model, e.g. for a supplying Deep Neural Network (DNN), Convolutional Neural Network (CNN) or LSTM RNN, to generate M-frames of masks for each talker. In particular, if M = 1, the output of the model can be described by the vector u = [m-T1, i m-T 2, i] T and the sources are separated as 1, i = m-1, i-ri and a-2, i-ri, i-ri, for sources s = 1, 2."}, {"heading": "B. The Label Permutation Problem", "text": "During the training, the error (e.g. using Equation (11) between the clean size spectra a1, i and a2, i and their estimated counterparts a, 1, i and a, i must be calculated. However, since the model estimates the masks m, 1, i and m, 2 simultaneously and they depend on the same input mix, it is not known in advance whether the resulting output vector u, i, i, T, i or u, i, i, i, i, i, i, i, i, i, i, i, i, T, T, T, i.e. the permutation of the output masks is unknown. A naive approach to forming a deep learning separation model without exact knowledge of the permutation of the output 5 masks is to use a constant permutation as illustrated by Fig. 1. Although such a training approach works for simple cases, e.g. female speakers are mixed with male speakers without exact knowledge of the output 5 masks, the output work can be so that the masculine or the output 5 masks can be masked."}, {"heading": "C. Permutation Invariant Training", "text": "rE \"s tis rf\u00fc ide r\u00fc the front for the back for the front for the back for the front for the back for the front for the back for the front for the back for the front for the back for the front for the back for the front for the back for the front for the back for the front for the back for the front for the back for the front for the front for the back for the front for the back for the front for the front for the back for the front for the front for the back for the front for the front for the back for the front for the back for the front for the back for the front for the back for the front for the front for the back for the front for the front for the front for the front for the back.\" rE \"s tis tis rf\u00fc ide for the front for the front for the front for the front for the back."}, {"heading": "V. UTTERANCE-LEVEL PIT", "text": "There are several ways you can get used to an earlier problem in an earlier framework. (...) There are several ways you can solve the problem in order to solve it. (...) There are many ways you can solve the problem. (...) There are many ways you can solve the problem. (...) There are few ways you can solve the problem. (...) There are few ways you can solve the problem. (...) There are few ways you can solve the problem. (...) There are few ways you can solve the problem. (...) There are many ways you can solve the problem. (...) There are few ways you can solve the problem. (...) There are many ways you can solve the problem. (...) There are few ways you can solve the problem. (...) But there are two major problems. (...) Firstly, it requires a separate step that can complicate the model. (...) Secondly, because the permutation depends on later models. (...)"}, {"heading": "VI. EXPERIMENTAL RESULTS", "text": "We investigated uPIT at various levels and all models were implemented with the Microsoft Cognitive Toolkit (CNTK) [47], [48] 2, assessed for their potential to improve the Signal-to-Distortion Ratio (SDR) [44] and the Perceptual Evaluation of Speech Quality (PESQ) [49] scores, both indicators commonly used to evaluate speech enhancement performance for multi-talker speech separation.2Available at: https: / / www.cntk.ai /"}, {"heading": "A. Datasets", "text": "We evaluated uPIT on the WSJ0-2mix, WSj0-3mix3 and Danish-2mix datasets with 129-dimensional STFT magnitude spectra calculated with a sampling frequency of 8 kHz, an image size of 32 ms and an image shift of 16 ms. The WSJ0-2mix dataset was introduced in [36] and was derived from the WSJ0 corpus [50]. The 30h training set and the 10h validation set contain mixtures of two speakers selected by randomly selecting 49 male and 51 female speakers and expressions from the WSJ0 training set si tr s and mixing them at different signal-to-noise ratios (SNRs)."}, {"heading": "B. Permutation Invariant Training", "text": "This year it is more than ever before."}, {"heading": "C. Utterance-level Permutation Invariant Training", "text": "This year it is more than ever before in the history of the city."}, {"heading": "D. Three-Talker Speech Separation", "text": "In Fig. 5, we present the uPIT training progress measured by MSE on the three speaker training and validation sets WSJ0-3mix. We note that, similar to the two-speaker scenario in Fig. 4, a low training progress is achieved, although the validation progress is somewhat higher. A better balance between training and validation of MSEs can be achieved through hyperparameter tuning. We also observe that increasing the model size reduces both the training and validation of MSE, which is expected due to the greater variability in the datasets.5 [37], [39] we did not use the SDR measure from [44]. Instead, a related variant called scale invariant SNR was used. 10In Table VII, we summarize the SDR improvement in dB from various uPIT separation configurations for trilingual mixed language, in closed state (CC) and combined state (OC)."}, {"heading": "E. Combined Two- and Three-Talker Speech Separation", "text": "To illustrate the flexibility of uPIT, Table VIII summarizes the performance of the three uPIT-BLSTM and uPIT-BLSTM-ST loudspeakers (from Table VII) when trained and tested on both sides of the WSJ0-2mix and WSJ0-3mix datasets, i.e. on both sides of the speaker mixtures. To train the three loudspeakers with the WSJ0-2mix dataset, we expand WSJ0-2mix with a third \"silent\" channel consisting of white noise with an energy level 70 dB below the remaining speaker spectral in the mix."}, {"heading": "VII. CONCLUSION AND DISCUSSION", "text": "In this paper, we introduced the permutation invariant training (uPIT) technique for speech-independent speech separation of multitalkers. We consider uPIT to be an interesting step towards solving the important problem of the cocktail party in a real-world environment where the number of speakers during the training is unknown. Our experiments with bilingual and trilingual mixed language separation tasks suggest that uPIT can actually deal more effectively with the problem of label permutation. These experiments show that bi-directional Long Short-Term Memory (LSTM) 11Recurrent Neural Networks (RNNs) perform better than unidirectional LSTMs and Phase Sensitive Masks (PSMs) are better training criteria than amplitude masks (AM). Our results also suggest that the acoustic stimuli learned from the model are largely speaker and language independent, as the models PSTMs are good for invisible speakers and generate more speech."}, {"heading": "ACKNOWLEDGMENT", "text": "We would like to thank Dr. John Hershey of MERL and Zhuo Chen of Columbia University for sharing the WSJ02mix and WSJ0-3mix datasets, and Dr. Hakan Erdogan of Microsoft Research for discussing PSM."}], "references": [{"title": "The Cocktail Party Problem", "author": ["S. Haykin", "Z. Chen"], "venue": "Neural Comput., vol. 17, no. 9, pp. 1875\u20131902, 2005.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1875}, {"title": "The Cocktail Party Phenomenon: A Review of Research on Speech Intelligibility in Multiple-Talker Conditions", "author": ["A.W. Bronkhorst"], "venue": "Acta Acust united Ac, vol. 86, no. 1, pp. 117\u2013128, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Some Experiments on the Recognition of Speech, with One and with Two Ears", "author": ["E.C. Cherry"], "venue": "J. Acoust. Soc. Am., vol. 25, no. 5, pp. 975\u2013 979, Sep. 1953.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1953}, {"title": "Monaural Speech Separation and Recognition Challenge", "author": ["M. Cooke", "J.R. Hershey", "S.J. Rennie"], "venue": "Comput. Speech Lang., vol. 24, no. 1, pp. 1\u201315, Jan. 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Speech Separation by Humans and Machines", "author": ["P. Divenyi"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Prediction-driven computational auditory scene analysis", "author": ["D.P.W. Ellis"], "venue": "Ph.D. dissertation, Massachusetts Institute of Technology, 1996.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1996}, {"title": "Modelling Auditory Processing and Organisation", "author": ["M. Cooke"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Computational Auditory Scene Analysis: Principles, Algorithms, and Applications", "author": ["D. Wang", "G.J. Brown"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Model-based sequential organization in cochannel speech", "author": ["Y. Shao", "D. Wang"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 14, no. 1, pp. 289\u2013298, Jan. 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "An Unsupervised Approach to Cochannel Speech Separation", "author": ["K. Hu", "D. Wang"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 21, no. 1, pp. 122\u2013131, Jan. 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Single-Channel Speech Separation using Sparse Non-Negative Matrix Factorization", "author": ["M.N. Schmidt", "R.K. Olsson"], "venue": "Proc. INTER- SPEECH, 2006, pp. 2614\u20132617.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Convolutive Speech Bases and Their Application to Supervised Speech Separation", "author": ["P. Smaragdis"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 15, no. 1, pp. 1\u201312, Jan. 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Algorithms for Non-negative Matrix Factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "NIPS, 2000, pp. 556\u2013562.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Super-human multi-talker speech recognition: the IBM 2006 speech separation challenge system", "author": ["T.T. Kristjansson"], "venue": "Proc. INTER- SPEECH, 2006, pp. 97\u2013100.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Speech Recognition Using Factorial Hidden Markov Models for Separation in the Feature Space", "author": ["T. Virtanen"], "venue": "Proc. INTERSPEECH, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Source-Filter-Based Single- Channel Speech Separation Using Pitch Information", "author": ["M. Stark", "M. Wohlmayr", "F. Pernkopf"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 19, no. 2, pp. 242\u2013255, Feb. 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Factorial Hidden Markov Models", "author": ["Z. Ghahramani", "M.I. Jordan"], "venue": "Machine Learning, vol. 29, no. 2-3, pp. 245\u2013273, 1997.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Roles of Pre-Training and Fine- Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition", "author": ["D. Yu", "L. Deng", "G.E. Dahl"], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Context-Dependent Pre- Trained Deep Neural Networks for Large-Vocabulary Speech Recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 20, no. 1, pp. 30\u201342, Jan. 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "Proc. INTERSPEECH, 2011, pp. 437\u2013440.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups", "author": ["G. Hinton"], "venue": "IEEE Sig. Process. Mag., vol. 29, no. 6, pp. 82\u201397, Nov. 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Achieving Human Parity in Conversational Speech Recognition", "author": ["W. Xiong"], "venue": "arXiv:1610.05256 [cs], 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "English Conversational Telephone Speech Recognition by Humans and Machines", "author": ["G. Saon"], "venue": "arXiv:1703.02136 [cs], 2017.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2017}, {"title": "Towards Scaling Up Classification-Based Speech Separation", "author": ["Y. Wang", "D. Wang"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 21, no. 7, pp. 1381\u20131390, Jul. 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "On Training Targets for Supervised Speech Separation", "author": ["Y. Wang", "A. Narayanan", "D. Wang"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 22, no. 12, pp. 1849\u20131858, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1849}, {"title": "An Experimental Study on Speech Enhancement Based on Deep Neural Networks", "author": ["Y. Xu", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "IEEE Sig. Process. Let., vol. 21, no. 1, pp. 65\u201368, Jan. 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech Enhancement with LSTM Recurrent Neural Networks and Its Application to Noise-Robust ASR", "author": ["F. Weninger"], "venue": "LVA/ICA. Springer, 2015, pp. 91\u201399.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint Optimization of Masks and Deep Recurrent Neural Networks for Monaural Source Separation", "author": ["P.-S. Huang", "M. Kim", "M. Hasegawa-Johnson", "P. Smaragdis"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 23, no. 12, pp. 2136\u20132147, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale training to increase speech intelligibility for hearing-impaired listeners in novel noises", "author": ["J. Chen"], "venue": "J. Acoust. Soc. Am., vol. 139, no. 5, pp. 2604\u20132612, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech Intelligibility Potential of General and Specialized Deep Neural Network Based Speech Enhancement Systems", "author": ["M. Kolb\u00e6k", "Z.H. Tan", "J. Jensen"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 25, no. 1, pp. 153\u2013167, 2017.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2017}, {"title": "Speech separation of a target speaker based on deep neural networks", "author": ["J. Du"], "venue": "ICSP, 2014, pp. 473\u2013477.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech enhancement based on neural networks improves speech intelligibility in noise for cochlear implant users", "author": ["T. Goehring"], "venue": "Hearing Research, vol. 344, pp. 183\u2013194, 2017.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep Neural Networks for Single-Channel Multi-Talker Speech Recognition", "author": ["C. Weng", "D. Yu", "M.L. Seltzer", "J. Droppo"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 23, no. 10, pp. 1670\u20131679, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep clustering: Discriminative embeddings for segmentation and separation", "author": ["J.R. Hershey", "Z. Chen", "J.L. Roux", "S. Watanabe"], "venue": "Proc. ICASSP, 2016, pp. 31\u201335.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep attractor network for singlemicrophone speaker separation", "author": ["Z. Chen", "Y. Luo", "N. Mesgarani"], "venue": "Proc. ICASSP, 2017, pp. 246\u2013250.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2017}, {"title": "Permutation Invariant Training of Deep Models for Speaker-Independent Multi-talker Speech Separation", "author": ["D. Yu", "M. Kolb\u00e6k", "Z.-H. Tan", "J. Jensen"], "venue": "Proc. ICASSP, 2017, pp. 241\u2013245.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2017}, {"title": "Single-Channel Multi-Speaker Separation Using Deep Clustering", "author": ["Y. Isik"], "venue": "Proc. INTERSPEECH, 2016, pp. 545\u2013549.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Single Channel Auditory Source Separation with Neural Network", "author": ["Z. Chen"], "venue": "Ph.D., Columbia University, United States \u2013 New York, 2017.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2017}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1997}, {"title": "Complex Ratio Masking for Monaural Speech Separation", "author": ["D.S. Williamson", "Y. Wang", "D. Wang"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 24, no. 3, pp. 483\u2013492, Mar. 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Recurrent Networks for Separation and Recognition of Single Channel Speech in Non-stationary Background Audio", "author": ["H. Erdogan", "J.R. Hershey", "S. Watanabe", "J.L. Roux"], "venue": "New Era for Robust Speech Recognition: Exploiting Deep Learning. Springer, 2017.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2017}, {"title": "Performance measurement in blind audio source separation", "author": ["E. Vincent", "R. Gribonval", "C. Fevotte"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 14, no. 4, pp. 1462\u20131469, 2006.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks", "author": ["H. Erdogan", "J.R. Hershey", "S. Watanabe", "J.L. Roux"], "venue": "Proc. ICASSP, 2015, pp. 708\u2013712.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural network based speech separation for robust speech recognition", "author": ["Y. Tu"], "venue": "ICSP, 2014, pp. 532\u2013536.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "The Computational Network Toolkit", "author": ["D. Yu", "K. Yao", "Y. Zhang"], "venue": "IEEE Sig. Process. Mag., vol. 32, no. 6, pp. 123\u2013126, Nov. 2015.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["A. Agarwal"], "venue": "Microsoft Technical Report {MSR- TR}-2014-112, Tech. Rep., 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs", "author": ["A. Rix", "J. Beerends", "M. Hollier", "A. Hekstra"], "venue": "Proc. ICASSP, vol. 2, 2001, pp. 749\u2013752.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2001}, {"title": "CSR-I (WSJ0) Complete LDC93s6a", "author": ["J. Garofolo", "D. Graff", "P. Doug", "D. Pallett"], "venue": "1993, philadelphia: Linguistic Data Consortium.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1993}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Y. Gal", "Z. Ghahramani"], "venue": "arXiv:1512.05287, Dec. 2015.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2015}, {"title": "A Deep Ensemble Learning Method for Monaural Speech Separation", "author": ["X.L. Zhang", "D. Wang"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 24, no. 5, pp. 967\u2013977, May 2016.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep stacking networks with time series for speech separation", "author": ["S. Nie", "H. Zhang", "X. Zhang", "W. Liu"], "venue": "Proc. ICASSP, 2014, pp. 6667\u2013 6671.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent Deep Stacking Networks for Supervised Speech Separation", "author": ["Z.-Q. Wang", "D. Wang"], "venue": "Proc. ICASSP, 2017, pp. 71\u201375.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "HAVING a conversation in a complex acoustic environment, with multiple noise sources and competing background speakers, is a task humans are remarkably good at [1], [2].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "HAVING a conversation in a complex acoustic environment, with multiple noise sources and competing background speakers, is a task humans are remarkably good at [1], [2].", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "The problem that humans solve when they focus their auditory attention towards one audio signal in a complex mixture of signals is commonly known as the cocktail party problem [1], [2].", "startOffset": 176, "endOffset": 179}, {"referenceID": 1, "context": "The problem that humans solve when they focus their auditory attention towards one audio signal in a complex mixture of signals is commonly known as the cocktail party problem [1], [2].", "startOffset": 181, "endOffset": 184}, {"referenceID": 0, "context": "Despite intense research for more than half a century, a general machine based solution to the cocktail party problem is yet to be discovered [1]\u2013[4].", "startOffset": 142, "endOffset": 145}, {"referenceID": 3, "context": "Despite intense research for more than half a century, a general machine based solution to the cocktail party problem is yet to be discovered [1]\u2013[4].", "startOffset": 146, "endOffset": 149}, {"referenceID": 2, "context": "Since the cocktail party problem was initially formalized [3], a large number of potential solutions have been proposed [5], and the most popular techniques originate from the field of Computational Auditory Scene Analysis (CASA) [6]\u2013[10].", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "Since the cocktail party problem was initially formalized [3], a large number of potential solutions have been proposed [5], and the most popular techniques originate from the field of Computational Auditory Scene Analysis (CASA) [6]\u2013[10].", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "Since the cocktail party problem was initially formalized [3], a large number of potential solutions have been proposed [5], and the most popular techniques originate from the field of Computational Auditory Scene Analysis (CASA) [6]\u2013[10].", "startOffset": 230, "endOffset": 233}, {"referenceID": 9, "context": "Since the cocktail party problem was initially formalized [3], a large number of potential solutions have been proposed [5], and the most popular techniques originate from the field of Computational Auditory Scene Analysis (CASA) [6]\u2013[10].", "startOffset": 234, "endOffset": 238}, {"referenceID": 10, "context": "Another popular technique for multi-talker speech separation is Non-negative Matrix Factorization (NMF) [11]\u2013[14].", "startOffset": 104, "endOffset": 108}, {"referenceID": 12, "context": "Another popular technique for multi-talker speech separation is Non-negative Matrix Factorization (NMF) [11]\u2013[14].", "startOffset": 109, "endOffset": 113}, {"referenceID": 3, "context": "For multi-talker speech separation, both CASA and NMF have led to limited success [4], [5] and the most successful techniques, before the deep learning era, are based on probabilistic models [15]\u2013[17], such as factorial GMM-HMM [18], that model the temporal dynamics and the complex interactions of the target and competing speech signals.", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "For multi-talker speech separation, both CASA and NMF have led to limited success [4], [5] and the most successful techniques, before the deep learning era, are based on probabilistic models [15]\u2013[17], such as factorial GMM-HMM [18], that model the temporal dynamics and the complex interactions of the target and competing speech signals.", "startOffset": 87, "endOffset": 90}, {"referenceID": 13, "context": "For multi-talker speech separation, both CASA and NMF have led to limited success [4], [5] and the most successful techniques, before the deep learning era, are based on probabilistic models [15]\u2013[17], such as factorial GMM-HMM [18], that model the temporal dynamics and the complex interactions of the target and competing speech signals.", "startOffset": 191, "endOffset": 195}, {"referenceID": 15, "context": "For multi-talker speech separation, both CASA and NMF have led to limited success [4], [5] and the most successful techniques, before the deep learning era, are based on probabilistic models [15]\u2013[17], such as factorial GMM-HMM [18], that model the temporal dynamics and the complex interactions of the target and competing speech signals.", "startOffset": 196, "endOffset": 200}, {"referenceID": 16, "context": "For multi-talker speech separation, both CASA and NMF have led to limited success [4], [5] and the most successful techniques, before the deep learning era, are based on probabilistic models [15]\u2013[17], such as factorial GMM-HMM [18], that model the temporal dynamics and the complex interactions of the target and competing speech signals.", "startOffset": 228, "endOffset": 232}, {"referenceID": 17, "context": "More recently, a large number of techniques based on deep learning [19] have been proposed, especially for Automatic Speech Recognition (ASR) [20]\u2013[25], and speech enhancement [26]\u2013[34].", "startOffset": 142, "endOffset": 146}, {"referenceID": 22, "context": "More recently, a large number of techniques based on deep learning [19] have been proposed, especially for Automatic Speech Recognition (ASR) [20]\u2013[25], and speech enhancement [26]\u2013[34].", "startOffset": 147, "endOffset": 151}, {"referenceID": 23, "context": "More recently, a large number of techniques based on deep learning [19] have been proposed, especially for Automatic Speech Recognition (ASR) [20]\u2013[25], and speech enhancement [26]\u2013[34].", "startOffset": 176, "endOffset": 180}, {"referenceID": 31, "context": "More recently, a large number of techniques based on deep learning [19] have been proposed, especially for Automatic Speech Recognition (ASR) [20]\u2013[25], and speech enhancement [26]\u2013[34].", "startOffset": 181, "endOffset": 185}, {"referenceID": 27, "context": ", [30]), although successful work has, similarly to NMF and CASA, mainly", "startOffset": 2, "endOffset": 6}, {"referenceID": 32, "context": "To the authors knowledge only four deep learning based works [35]\u2013[38] exist, that have tried to address and solve the harder speaker independent multi-talker speech separation task.", "startOffset": 61, "endOffset": 65}, {"referenceID": 35, "context": "To the authors knowledge only four deep learning based works [35]\u2013[38] exist, that have tried to address and solve the harder speaker independent multi-talker speech separation task.", "startOffset": 66, "endOffset": 70}, {"referenceID": 32, "context": "[35], which proposed the best performing system in the 2006 monaural speech separation and recognition challenge [4], the instantaneous energy was used to ar X iv :1 70 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[35], which proposed the best performing system in the 2006 monaural speech separation and recognition challenge [4], the instantaneous energy was used to ar X iv :1 70 3.", "startOffset": 113, "endOffset": 116}, {"referenceID": 33, "context": "[36] have made significant progress with their Deep Clustering (DPCL) technique.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "To further improve the model [39], another RNN is stacked on top of the first DPCL RNN to estimate continuous masks for each target speaker.", "startOffset": 29, "endOffset": 33}, {"referenceID": 34, "context": "[37], [40] proposed a related technique called Deep Attractor Network (DANet).", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[37], [40] proposed a related technique called Deep Attractor Network (DANet).", "startOffset": 6, "endOffset": 10}, {"referenceID": 35, "context": "Recently, we proposed the Permutation Invariant Training (PIT) technique1 [38] for attacking the speaker independent multi-talker speech separation problem and showed that PIT effectively solves the label permutation problem.", "startOffset": 74, "endOffset": 78}, {"referenceID": 35, "context": "Specifically, uPIT extends the frame-level PIT technique [38] with an utterance-level training criterion that effectively eliminates the need for additional speaker tracing or very large input/output contexts, which is otherwise required by the original PIT [38].", "startOffset": 57, "endOffset": 61}, {"referenceID": 35, "context": "Specifically, uPIT extends the frame-level PIT technique [38] with an utterance-level training criterion that effectively eliminates the need for additional speaker tracing or very large input/output contexts, which is otherwise required by the original PIT [38].", "startOffset": 258, "endOffset": 262}, {"referenceID": 38, "context": "We achieve this using deep Long Short-Term Memory (LSTM) RNNs [41] that, during training, minimize the utterance-level separation error, hence forcing separated frames belonging to the same speaker to be aligned to the same output stream.", "startOffset": 62, "endOffset": 66}, {"referenceID": 33, "context": "1In [36], a related permutation free technique, which is similar to PIT for exactly two-speakers, was evaluated with negative results and conclusion.", "startOffset": 4, "endOffset": 8}, {"referenceID": 39, "context": "This is because phase estimation is still an open problem in the speech separation setup [42], [43].", "startOffset": 89, "endOffset": 93}, {"referenceID": 40, "context": "This is because phase estimation is still an open problem in the speech separation setup [42], [43].", "startOffset": 95, "endOffset": 99}, {"referenceID": 24, "context": ", [27], [43]), that better results can be achieved if, instead of estimating Z directly, we first estimate a set of masks Ms(t, f), s = 1, .", "startOffset": 2, "endOffset": 6}, {"referenceID": 40, "context": ", [27], [43]), that better results can be achieved if, instead of estimating Z directly, we first estimate a set of masks Ms(t, f), s = 1, .", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "The Ideal Ratio Mask (IRM) [27] for each source is defined as", "startOffset": 27, "endOffset": 31}, {"referenceID": 41, "context": "When the phase of Y is used for reconstruction, the IRM achieves the highest Signal to Distortion Ratio (SDR) [44], when all sources have the same phase, (which is an invalid assumption in general).", "startOffset": 110, "endOffset": 114}, {"referenceID": 28, "context": "Nevertheless, we report IRM results as an upper performance bound since the IRM is a commonly used training target for deep learning based monaural speech separation [31], [32].", "startOffset": 166, "endOffset": 170}, {"referenceID": 29, "context": "Nevertheless, we report IRM results as an upper performance bound since the IRM is a commonly used training target for deep learning based monaural speech separation [31], [32].", "startOffset": 172, "endOffset": 176}, {"referenceID": 24, "context": "Another applicable mask is the Ideal Amplitude Mask (IAM) (known as FFT-mask in [27]), or simply Amplitude Mask (AM), when estimated by a deep learning model.", "startOffset": 80, "endOffset": 84}, {"referenceID": 40, "context": "The Ideal Phase Sensitive Mask (IPSM) [43], [45]", "startOffset": 38, "endOffset": 42}, {"referenceID": 42, "context": "The Ideal Phase Sensitive Mask (IPSM) [43], [45]", "startOffset": 44, "endOffset": 48}, {"referenceID": 24, "context": "To overcome these limitations, recent works [27] directly minimize the MSE", "startOffset": 44, "endOffset": 48}, {"referenceID": 40, "context": "(11) is used as a cost function, the IPSM is the upper bound achievable on the task [43].", "startOffset": 84, "endOffset": 88}, {"referenceID": 27, "context": "A natural, and commonly used, approach for deep learning based speech separation is to cast the problem as a multi-class [30], [35], [46] regression problem as depicted in Fig.", "startOffset": 121, "endOffset": 125}, {"referenceID": 32, "context": "A natural, and commonly used, approach for deep learning based speech separation is to cast the problem as a multi-class [30], [35], [46] regression problem as depicted in Fig.", "startOffset": 127, "endOffset": 131}, {"referenceID": 43, "context": "A natural, and commonly used, approach for deep learning based speech separation is to cast the problem as a multi-class [30], [35], [46] regression problem as depicted in Fig.", "startOffset": 133, "endOffset": 137}, {"referenceID": 32, "context": "This problem is referred to as the label permutation (or ambiguity) problem in [35], [36].", "startOffset": 79, "endOffset": 83}, {"referenceID": 33, "context": "This problem is referred to as the label permutation (or ambiguity) problem in [35], [36].", "startOffset": 85, "endOffset": 89}, {"referenceID": 35, "context": "2 and is referred to as Permutation Invariant Training (PIT) [38].", "startOffset": 61, "endOffset": 65}, {"referenceID": 35, "context": "However, this usually leads to unsatisfactory results as reported in [38].", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "For example, in CASA a related problem referred to as the Sequential Organization Problem has been addressed using a model-based sequential grouping algorithm [9].", "startOffset": 159, "endOffset": 162}, {"referenceID": 44, "context": "We evaluated uPIT on various setups and all models were implemented using the Microsoft Cognitive Toolkit (CNTK) [47], [48]2.", "startOffset": 113, "endOffset": 117}, {"referenceID": 45, "context": "We evaluated uPIT on various setups and all models were implemented using the Microsoft Cognitive Toolkit (CNTK) [47], [48]2.", "startOffset": 119, "endOffset": 123}, {"referenceID": 41, "context": "The models were evaluated on their potential to improve the Signal-to-Distortion Ratio (SDR) [44] and the Perceptual Evaluation of Speech Quality (PESQ) [49] score, both of which are metrics widely used to evaluate speech enhancement performance for multi-talker speech separation tasks.", "startOffset": 93, "endOffset": 97}, {"referenceID": 46, "context": "The models were evaluated on their potential to improve the Signal-to-Distortion Ratio (SDR) [44] and the Perceptual Evaluation of Speech Quality (PESQ) [49] score, both of which are metrics widely used to evaluate speech enhancement performance for multi-talker speech separation tasks.", "startOffset": 153, "endOffset": 157}, {"referenceID": 33, "context": "The WSJ0-2mix dataset was introduced in [36] and was derived from the WSJ0 corpus [50].", "startOffset": 40, "endOffset": 44}, {"referenceID": 47, "context": "The WSJ0-2mix dataset was introduced in [36] and was derived from the WSJ0 corpus [50].", "startOffset": 82, "endOffset": 86}, {"referenceID": 33, "context": "In our study, the validation set is used to find initial hyperparameters and to evaluate closed-condition (CC) (seen speaker) performance, similarly to [36], [38], [39].", "startOffset": 152, "endOffset": 156}, {"referenceID": 35, "context": "In our study, the validation set is used to find initial hyperparameters and to evaluate closed-condition (CC) (seen speaker) performance, similarly to [36], [38], [39].", "startOffset": 158, "endOffset": 162}, {"referenceID": 36, "context": "In our study, the validation set is used to find initial hyperparameters and to evaluate closed-condition (CC) (seen speaker) performance, similarly to [36], [38], [39].", "startOffset": 164, "endOffset": 168}, {"referenceID": 35, "context": "We first evaluated the original frame-level PIT on the twotalker separation dataset WSJ0-2mix, and differently from [38], we fixed the input dimension to 51 frames, to isolate the effect of a varying output dimension.", "startOffset": 116, "endOffset": 120}, {"referenceID": 33, "context": "The WSJ0-2mix dataset, used in [36], was designed such that speaker one was always assigned the most energy, and consequently speaker two the lowest, when scaling to a given SNR.", "startOffset": 31, "endOffset": 35}, {"referenceID": 32, "context": "Previous work [35] has shown that such speaker energy patterns are an effective discriminative feature, which is clearly seen in Fig.", "startOffset": 14, "endOffset": 18}, {"referenceID": 32, "context": "3, where the CONV-DNN model achieves considerably lower training and validation MSE than the CONVDNN-RAND model, which hardly decreases in either training or validation MSE due to the label permutation problem [35], [36].", "startOffset": 210, "endOffset": 214}, {"referenceID": 33, "context": "3, where the CONV-DNN model achieves considerably lower training and validation MSE than the CONVDNN-RAND model, which hardly decreases in either training or validation MSE due to the label permutation problem [35], [36].", "startOffset": 216, "endOffset": 220}, {"referenceID": 36, "context": "Note that, since we used Nvidia\u2019s cuDNN implementation of LSTMs, to speed up training, we were unable to apply dropout across time steps, which was adopted by the best DPCL model [39] and is known to be more effective, both theoretically and empirically, than the simple dropout strategy used in this work [51].", "startOffset": 179, "endOffset": 183}, {"referenceID": 48, "context": "Note that, since we used Nvidia\u2019s cuDNN implementation of LSTMs, to speed up training, we were unable to apply dropout across time steps, which was adopted by the best DPCL model [39] and is known to be more effective, both theoretically and empirically, than the simple dropout strategy used in this work [51].", "startOffset": 306, "endOffset": 310}, {"referenceID": 36, "context": "3) Two-stage Models and Reduced Dropout Rate: It is well known that cascading DNNs can improve performance for certain deep learning based applications [39], [52]\u2013[54].", "startOffset": 152, "endOffset": 156}, {"referenceID": 49, "context": "3) Two-stage Models and Reduced Dropout Rate: It is well known that cascading DNNs can improve performance for certain deep learning based applications [39], [52]\u2013[54].", "startOffset": 158, "endOffset": 162}, {"referenceID": 51, "context": "3) Two-stage Models and Reduced Dropout Rate: It is well known that cascading DNNs can improve performance for certain deep learning based applications [39], [52]\u2013[54].", "startOffset": 163, "endOffset": 167}, {"referenceID": 33, "context": "These results agree with breakdowns from other works [36], [39] and generally indicate that same-gender mixed speech separation is a harder task.", "startOffset": 53, "endOffset": 57}, {"referenceID": 36, "context": "These results agree with breakdowns from other works [36], [39] and generally indicate that same-gender mixed speech separation is a harder task.", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "Furthermore, we note that the PESQ improvements are similar to what have been reported for DNN based speech enhancement systems [32].", "startOffset": 128, "endOffset": 132}, {"referenceID": 33, "context": "Oracle NMF [36] - - 5.", "startOffset": 11, "endOffset": 15}, {"referenceID": 33, "context": "1 CASA [36] - - 2.", "startOffset": 7, "endOffset": 11}, {"referenceID": 33, "context": "1 DPCL [36] - - 5.", "startOffset": 7, "endOffset": 11}, {"referenceID": 34, "context": "8 DPCL+ [37] - - - 9.", "startOffset": 8, "endOffset": 12}, {"referenceID": 34, "context": "1 DANet [37] - - - 9.", "startOffset": 8, "endOffset": 12}, {"referenceID": 34, "context": "6 DANet\u2021 [37] - - - 10.", "startOffset": 9, "endOffset": 13}, {"referenceID": 36, "context": "5 DPCL++ [39] - - - 9.", "startOffset": 9, "endOffset": 13}, {"referenceID": 36, "context": "4 DPCL++\u2021 [39] - - - 10.", "startOffset": 10, "endOffset": 14}, {"referenceID": 33, "context": "From the table we can observe that the models trained with PIT already achieve similar or better SDR than the original DPCL [36], respectively, with DNNs and CNNs.", "startOffset": 124, "endOffset": 128}, {"referenceID": 34, "context": "Using the uPIT training criteria, we improve on PIT and achieve comparable performance with DPCL+, DPCL++ and DANet models5 reported in [37], [39], which used curriculum training [55], and recurrent dropout [51].", "startOffset": 136, "endOffset": 140}, {"referenceID": 36, "context": "Using the uPIT training criteria, we improve on PIT and achieve comparable performance with DPCL+, DPCL++ and DANet models5 reported in [37], [39], which used curriculum training [55], and recurrent dropout [51].", "startOffset": 142, "endOffset": 146}, {"referenceID": 48, "context": "Using the uPIT training criteria, we improve on PIT and achieve comparable performance with DPCL+, DPCL++ and DANet models5 reported in [37], [39], which used curriculum training [55], and recurrent dropout [51].", "startOffset": 207, "endOffset": 211}, {"referenceID": 34, "context": "5 [37], [39] did not use the SDR measure from [44].", "startOffset": 2, "endOffset": 6}, {"referenceID": 36, "context": "5 [37], [39] did not use the SDR measure from [44].", "startOffset": 8, "endOffset": 12}, {"referenceID": 41, "context": "5 [37], [39] did not use the SDR measure from [44].", "startOffset": 46, "endOffset": 50}, {"referenceID": 33, "context": "Oracle NMF [36] 4.", "startOffset": 11, "endOffset": 15}, {"referenceID": 36, "context": "5 - - DPCL++\u2021 [39] - - - 7.", "startOffset": 14, "endOffset": 18}, {"referenceID": 37, "context": "1 DANet [40] - - - 7.", "startOffset": 8, "endOffset": 12}, {"referenceID": 34, "context": "7 DANet\u2021 [37] - - - 8.", "startOffset": 9, "endOffset": 13}, {"referenceID": 36, "context": "This is of great practical importance, since a priori knowledge about the number of speakers is not needed at test time, as required by competing methods such as DPCL++ [39] and DANet [37], [40].", "startOffset": 169, "endOffset": 173}, {"referenceID": 34, "context": "This is of great practical importance, since a priori knowledge about the number of speakers is not needed at test time, as required by competing methods such as DPCL++ [39] and DANet [37], [40].", "startOffset": 184, "endOffset": 188}, {"referenceID": 37, "context": "This is of great practical importance, since a priori knowledge about the number of speakers is not needed at test time, as required by competing methods such as DPCL++ [39] and DANet [37], [40].", "startOffset": 190, "endOffset": 194}, {"referenceID": 33, "context": "The proposed uPIT technique is algorithmically simpler yet performs on par with DPCL [36], [39] and comparable to DANets [37], [40], both of which involve separate embedding and clustering stages during inference.", "startOffset": 85, "endOffset": 89}, {"referenceID": 36, "context": "The proposed uPIT technique is algorithmically simpler yet performs on par with DPCL [36], [39] and comparable to DANets [37], [40], both of which involve separate embedding and clustering stages during inference.", "startOffset": 91, "endOffset": 95}, {"referenceID": 34, "context": "The proposed uPIT technique is algorithmically simpler yet performs on par with DPCL [36], [39] and comparable to DANets [37], [40], both of which involve separate embedding and clustering stages during inference.", "startOffset": 121, "endOffset": 125}, {"referenceID": 37, "context": "The proposed uPIT technique is algorithmically simpler yet performs on par with DPCL [36], [39] and comparable to DANets [37], [40], both of which involve separate embedding and clustering stages during inference.", "startOffset": 127, "endOffset": 131}], "year": 2017, "abstractText": "In this paper we propose the utterance-level Permutation Invariant Training (uPIT) technique. uPIT is a practically applicable, end-to-end, deep learning based solution for speaker independent multi-talker speech separation. Specifically, uPIT extends the recently proposed Permutation Invariant Training (PIT) technique with an utterance-level cost function, hence eliminating the need for solving an additional permutation problem during inference, which is otherwise required by frame-level PIT. We achieve this using Recurrent Neural Networks (RNNs) that, during training, minimize the utterance-level separation error, hence forcing separated frames belonging to the same speaker to be aligned to the same output stream. In practice, this allows RNNs, trained with uPIT, to separate multi-talker mixed speech without any prior knowledge of signal duration, number of speakers, speaker identity or gender. We evaluated uPIT on the WSJ0 and Danish twoand three-talker mixed-speech separation tasks and found that uPIT outperforms techniques based on Non-negative Matrix Factorization (NMF) and Computational Auditory Scene Analysis (CASA), and compares favorably with Deep Clustering (DPCL) and the Deep Attractor Network (DANet). Furthermore, we found that models trained with uPIT generalize well to unseen speakers and languages. Finally, we found that a single model, trained with uPIT, can handle both two-speaker, and three-speaker speech mixtures.", "creator": "LaTeX with hyperref package"}}}