{"id": "1703.06452", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2017", "title": "Deep Neural Networks for Semantic Segmentation of Multispectral Remote Sensing Imagery", "abstract": "A semantic segmentation algorithm must assign a label to every pixel in an image. Recently, semantic segmentation of RGB imagery has advanced significantly due to deep learning. Because creating datasets for semantic segmentation is laborious, these datasets tend to be significantly smaller than object recognition datasets. This makes it difficult to directly train a deep neural network for semantic segmentation, because it will be prone to overfitting. To cope with this, deep learning models typically use convolutional neural networks pre-trained on large-scale image classification datasets, which are then fine-tuned for semantic segmentation. For non-RGB imagery, this is currently not possible because large-scale labeled non-RGB datasets do not exist. In this paper, we developed two deep neural networks for semantic segmentation of multispectral remote sensing imagery. Prior to training on the target dataset, we initialize the networks with large amounts of synthetic multispectral imagery. We show that this significantly improves results on real-world remote sensing imagery, and we establish a new state-of-the-art result on the challenging Hamlin Beach State Park Dataset.", "histories": [["v1", "Sun, 19 Mar 2017 15:21:32 GMT  (3445kb,D)", "http://arxiv.org/abs/1703.06452v1", "9 pages, submitted to ICCV 2017"], ["v2", "Thu, 21 Sep 2017 13:45:12 GMT  (5442kb,D)", "http://arxiv.org/abs/1703.06452v2", null]], "COMMENTS": "9 pages, submitted to ICCV 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["ronald kemker", "christopher kanan"], "accepted": false, "id": "1703.06452"}, "pdf": {"name": "1703.06452.pdf", "metadata": {"source": "CRF", "title": "Deep Neural Networks for Semantic Segmentation of Multispectral Remote Sensing Imagery", "authors": ["Ronald Kemker", "Christopher Kanan", "Chester F. Carlson"], "emails": ["kanan}@rit.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, most people who are able are able to decide for themselves what they want and what they don't want, \"he told the German Press Agency.\" I don't think they want that, \"he said.\" But I don't think they want that. \"He added,\" I don't think they want that. \"He added,\" I don't think they want that. \"He added,\" I don't think they want that. I don't think they want that, but I don't think they want that. \""}, {"heading": "2. Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Semantic Segmentation with Deep Nets", "text": "In fact, most of them will be able to play by the rules that they have established in recent years, and they will be able to play by the rules that they have imposed on themselves."}, {"heading": "2.2. Deep Learning for Non-RGB Sensors", "text": "The lack of labeled data available for these sensor modalities has led researchers to adopt uncontrolled methods of extracting features such as autoencoders, the most successful of which are stacked autoencoders [19, 18, 36, 22, 33]. An autoencoder is an uncontrolled neural network that learns efficient encoding of training data. These autoencoders can be stacked together to learn higher characteristic representations. DCNNs have been successful in RGB applications due to the large amount of labeled data available, but DCNNs have not yet reached the state of the art for which they are used."}, {"heading": "2.3. Deep-Learning with Synthetic Data", "text": "Synthetic data has been used to increase the amount of available training data for many applications, including object recognition [24], estimation [5], face and handwriting recognition [35], and semantic segmentation [28]. Previous work used various methods to generate large amounts of synthetic data, including geometric / color transformations, 3D modeling, and virtual reality emulators. The great advantage of synthetic images is that they are usually cheaper and easier to obtain than images that are manually annotated by humans; however, the difference in the distributions of features and space, also known as the synthetic gap, makes it difficult to transfer features from synthetic to real images. Researchers have introduced domain matching techniques [3] to mitigate this phenomenon, including training of auto encoders to shift the distribution of synthetic data to the distribution of real data [8, 35] allowing a data set to then be trained to classify and pre-tell."}, {"heading": "3. Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Multispectral Sensor", "text": "The Hamlin Beach State Park dataset used for the analysis was acquired by the Tetracam Micro-MCA6 multispectral image sensor. Micro-MCA6 has six spectral bands, including three color (RGB) and three near infrared (NIR) bands, and was calibrated with an integrated sphere to accurately model sensor reaction in the simulated DIRSIG environment. See Table 1 for further technical specifications."}, {"heading": "3.2. DIRSIG", "text": "Good DCNN initialization requires a large amount of training data with sufficient spatial / spectral variability to extract discriminatory features to learn filters. There are no publicly available ImageNet datasets for non-RGB sensor modalities, so we use DIRSIG to build a large synthetic dataset for semantic segmentation of air layers. DIRSIG is a software tool widely used in the remote sensing industry to model image sensors prior to development. It can be used to simulate education platforms and sensor designs, including monochromatic, multispectral, hyperspectral, thermal and light acquisition and reach (LIDAR).DIRSIG images model an object with physical transmission / propagation."}, {"heading": "3.3. Segmentation Networks", "text": "In this paper, we adapted the semantic segmentation models Sharpmask [25] and RefineNet [16] to our requirements, as they reconstruct the response of the downsampled features by passing on features of previous layers. These networks also learn a sharp classification mask in an end-to-end framework, resulting in higher classification performance. Our goal is to measure the benefit of a pre-training with synthetic images on these two different networks. The folding network for both of these models was ResNet-50 [9] with the improved network architecture scheme proposed in [10], where batch normalization and ReLU are applied before each folding. This DCNN was pre-trained with the synthetic DIRSIG images. Both segmentation models in this paper were developed using Theano / Keras [6]."}, {"heading": "3.3.1 Sharpmask", "text": "The Sharpmask model used for this paper is illustrated in Fig. 2. The network is divided into the subnetworks Folding, Bridge and Segmentation. The folding network is identical to the first four folding blocks in the ResNet-50 DCNN. Because our model is rebuilt from scratch, we use batch normalization to regulate, whereas the original Sharpmask model did not. The bridge network is an M x 1 x 1 folding layer between the folding and segmentation networks, where M is selected as the performance-speed trade-off. The main goal of this network is to add some variability to the features fed into the segmentation network in Refinement Module # 3. We use a value of M = 512 that worked well in preliminary experiments.The Segmentation network uses refinement modules to return the output of the bridge layer to the original dimensionality of the input data."}, {"heading": "3.3.2 RefineNet", "text": "The RefineNet model used in this paper (Fig. 4) follows the same basic structure as the Sharpmask model with a few minor changes. The finishing block in Fig. 3 is replaced by a more complex block called RefineNet (Fig. 5 (a), which is divided into three main components: Residual Folding Units (RCUs), Multi-Resolution Fusion (MRF), and Chained Restpooling (CRP). In addition, our regularization model uses a stack normalization that was not performed with the original RefineNet. The folding net mimics the first five blocks of ResNet-50.The RCUs (Fig. 5 (b)) are used to spread the gradient over short and long-distance connections, making the holistic training more effective and efficient. MRF is used to combine features on multiple scales that will be two features in this paper. The multiple features (CRP) are different for multiple features (CRP)."}, {"heading": "4. Experimental Procedure", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Data", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Synthetic DIRSIG Data", "text": "We used DIRSIG to generate synthetic multispectral images for pre-training of our semantic segmentation algorithms. We used the synthetic scene shown in Figure 6, which resembles Trona, an incorporated area in Southern California. It is an industrial and residential scene with 109 marked classes, including buildings, vehicles, swimming pools, terrain, and desert plants. We used calibrated radiation data captured by an integrating sphere to model the response of the Tetracam sensor in DIRSIG. Trona is accurate only to a GSD of 0.5 meters, and the generation of images with smaller GSDs generates unrealistic images based on artifacts. We generated data using multiple GSDs (0.5, 0.75, and 1 meters) to learn invariant features in our network. This GSD is 10 times larger than the sets of Hamlin State Park's synthetic images, and we flew over the intersectional area with some of the drone objects. \"We used DIRSIG to generate synthetic multispectral images to pre-train our semantic segmentation algorithms."}, {"heading": "4.1.2 Hamlin Beach State Park Dataset", "text": "This year, the time has come to cut it by half, but no longer by half."}, {"heading": "4.2. Network Initialization", "text": "We compare both models with and without pre-training on synthetic DIRSIG images. For the pre-trained model, the ResNet-50 DCNN was initialized with 80 x 80 pixel fields, corresponding to approximately 4.7 million DIRSIG training images. The network was initialized with a mini-batch size of 128 and a weight decay of 1e-4. Weights were randomly initialized from a normal distribution of 0 x 80 pixel fields. We calculated the channel mean and standard deviation using the entire DIRSIG training set, and these parameters were used to scale each image to mean and unit variance. During training, the images are mixed in each epoch, and we use random horizontal and vertical flips for data augmentation. Since the class distribution for the DIRSIG data is unbalanced, we use the class weights for the entire training set to generate samples (per pixel) of the network, lowering the class weights by 1 millimeter."}, {"heading": "4.3. Network Fine-Tuning", "text": "We have fine-tuned the weights of the Sharpmask and RefineNet segmentation networks based on the pre-trained networks to assign them to the pre-trained ResNet DCNN. Due to the high resolution of the orthomosaic, the data is split into 160 x 160 fields and fed into the segmentation frame. Our models are fine-tuned in two stages, using the Nadam optimizer, batch size 32, weight drop of 1e-4 and class weight parameters of \u00b5 = 0.25. First, the folding network is frozen and the remaining layers are trained to match the weights of the segmentation network to the pre-trained weights in the folding network. An initial learning rate of 2e-3 is used for this stage, and then it is dropped by a factor of 10 at the validation loss plate.Second, all weights are trained with the pre-trained weights in the folding network, using an initial learning rate of 5-2 and another 2-3 times."}, {"heading": "5. Experimental Results", "text": "In recent years, it has become clear that the number of unemployed who are able to do their jobs continues to increase, while the number of unemployed who are able to do their jobs continues to increase. In recent years, the number of unemployed who have fallen into the poverty trap has multiplied, and the number of unemployed has multiplied. In recent years, the number of unemployed who have fallen into the poverty trap has multiplied."}, {"heading": "6. Discussion and Conclusions", "text": "In this paper, we used synthetic multispectral data to boot a DCNN dataset for semantic segmentation, and the characteristics we learned from the synthetic data were successfully applied to real-world images. Our RefineNet Sim model was evaluated at the Hamlin Beach State Park dataset, where it exceeded the previous accuracy of the middle class by 23.6%. This work will allow remote sensing researchers to leverage advances in deep learning that were not available to them due to the lack of labeled data. In the future, we hope to improve our model by 1) enhancing the exploration of deeper ResNet models; 2) by using newer state-of-theart conversion (ResNeXt [34]) and segmentation models; and 3) enhancing the inherent GSD of the DIRSIG scene; and 4) including additional diversified classes of the synthetic data, these modelling techniques should be used to enhance the RSIG's ability to generate large quantities of data."}, {"heading": "Acknowledgements", "text": "We thank Michael Gartley, Carl Salvaggio and the RIT Signature Interdisciplinary Research Area, UAS Research Laboratory for their support."}], "references": [{"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "Machine learning, 79(1-2):151\u2013175", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets", "author": ["L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "atrous convolution, and fully connected crfs. arXiv preprint, arXiv:1606.00915", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Synthesizing training images for boosting human 3d pose estimation", "author": ["W. Chen", "H. Wang", "Y. Li", "H. Su", "Z. Wang", "C. Tu", "D. Lischinski", "D. Cohen-Or", "B. Chen"], "venue": "3D Vision, pages 479\u2013488. IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/ keras", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast r-cnn", "author": ["R. Girshick"], "venue": "ICCV", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised and transfer learning challenge", "author": ["I. Guyon", "G. Dror", "V. Lemaire", "G. Taylor", "D.W. Aha"], "venue": "Neural Networks (IJCNN), pages 793\u2013800. IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ECCV", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep convolutional neural networks for hyperspectral image classification", "author": ["W. Hu", "Y. Huang", "L. Wei", "F. Zhang", "H. Li"], "venue": "Journal of Sensors, 2015", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Advances in wide area hyperspectral image simulation", "author": ["E. Ientilucci", "S. Brown"], "venue": "Proc SPIE, 5075:110\u2013121", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Self-taught feature learning for hyperspectral image classification", "author": ["R. Kemker", "C. Kanan"], "venue": "IEEE Transactions on Geoscience and Remote Sensing", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "High-resolution multispectral dataset for semantic segmentation", "author": ["R. Kemker", "C. Salvaggio", "C. Kanan"], "venue": "arXiv preprint, arXiv:1703.01918", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2017}, {"title": "Efficient inference in fully connected crfs with gaussian edge potentials", "author": ["P. Kr\u00e4henb\u00fchl", "V. Koltun"], "venue": "NIPS", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "RefineNet: Multipath refinement networks for high-resolution semantic segmentation", "author": ["G. Lin", "A. Milan", "C. Shen", "I.D. Reid"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Microsoft COCO: Common objects in context", "author": ["T. Lin", "M. Maire", "S.J. Belongie", "L.D. Bourdev", "R.B. Girshick", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Spectral-spatial classification of hyperspectral image using autoencoders", "author": ["Z. Lin"], "venue": "In Information, Communications and Signal Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Hyperspectral classification via deep networks and superpixel segmentation", "author": ["Y. Liu"], "venue": "International Journal of Remote Sensing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "F", "author": ["F. Luus", "B. Salmon"], "venue": "van den Bergh, and B. Maharaj. Multiview deep learning for land-use classification. IEEE  Geoscience and Remote Sensing Letters, 12(12):2448\u20132452", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Hyperspectral image classification via contextual deep learning", "author": ["X. Ma"], "venue": "EURASIP Journal on Image and Video Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "ICCV", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning deep object detectors from 3d models", "author": ["X. Peng", "B. Sun", "K. Ali", "K. Saenko"], "venue": "ICCV, pages 1278\u20131286", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to refine object segments", "author": ["P.H.O. Pinheiro", "T. Lin", "R. Collobert", "P. Doll\u00e1r"], "venue": "ECCV", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Selftaught learning: transfer learning from unlabeled data", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng"], "venue": "ICML. ACM", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "You only look once: Unified", "author": ["J. Redmon", "S.K. Divvala", "R.B. Girshick", "A. Farhadi"], "venue": "real-time object detection. In CVPR", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes", "author": ["G. Ros", "L. Sellart", "J. Materzynska", "D. Vazquez", "A.M. Lopez"], "venue": "CVPR, pages 3234\u20133243", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "The isprs benchmark on urban object classification and 3d building reconstruction", "author": ["F. Rottensteiner", "G. Sohn", "J. Jung", "M. Gerke", "C. Baillard", "S. Benitez", "U. Breitkopf"], "venue": "ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci, 1(3):293\u2013 298", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M.S. Bernstein", "A.C. Berg", "F. Li"], "venue": "IJCV, 115(3):211\u2013252", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "An advanced synthetic image generation model and its application to multi/hyperspectral algorithm development", "author": ["J. Schott", "S. Brown", "R. Raqueo", "H. Gross", "G. Robinson"], "venue": "Canadian Journal of Remote Sensing, 25(2):99\u2013111", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1999}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised spectral\u2013 spatial feature learning with stacked sparse autoencoder for hyperspectral imagery classification", "author": ["C. Tao", "H. Pan", "Y. Li", "Z. Zou"], "venue": "IEEE Geoscience and Remote Sensing Letters, 12(12):2438\u20132442", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Aggregated residual transformations for deep neural networks", "author": ["S. Xie", "R.B. Girshick", "P. Doll\u00e1r", "Z. Tu", "K. He"], "venue": "CVPR", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning from synthetic data using a stacked multichannel autoencoder", "author": ["X. Zhang", "Y. Fu", "S. Jiang", "L. Sigal", "G. Agam"], "venue": "ICML, pages 461\u2013464. IEEE", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "On combining multiscale deep learning features for the classification of hyperspectral remote sensing imagery", "author": ["W. Zhao", "Z. Guo", "J. Yue", "X. Zhang", "L. Luo"], "venue": "International Journal of Remote Sensing, 36(13):3368\u20133379", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 29, "context": ", VGG-16 has 138 million parameters [32].", "startOffset": 36, "endOffset": 40}, {"referenceID": 4, "context": "the network, this approach has enabled state-of-the-art performance for many other computer vision tasks that use smaller datasets, including object detection [7, 27] and semantic segmentation [20, 23].", "startOffset": 159, "endOffset": 166}, {"referenceID": 24, "context": "the network, this approach has enabled state-of-the-art performance for many other computer vision tasks that use smaller datasets, including object detection [7, 27] and semantic segmentation [20, 23].", "startOffset": 159, "endOffset": 166}, {"referenceID": 17, "context": "the network, this approach has enabled state-of-the-art performance for many other computer vision tasks that use smaller datasets, including object detection [7, 27] and semantic segmentation [20, 23].", "startOffset": 193, "endOffset": 201}, {"referenceID": 20, "context": "the network, this approach has enabled state-of-the-art performance for many other computer vision tasks that use smaller datasets, including object detection [7, 27] and semantic segmentation [20, 23].", "startOffset": 193, "endOffset": 201}, {"referenceID": 14, "context": "28 million training images), fine-tune it for semantic segmentation on the COCO dataset (80K training images) [17], and then fine-tune it again on PACAL VOC (1,464 training images)[4, 16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 1, "context": "28 million training images), fine-tune it for semantic segmentation on the COCO dataset (80K training images) [17], and then fine-tune it again on PACAL VOC (1,464 training images)[4, 16].", "startOffset": 180, "endOffset": 187}, {"referenceID": 13, "context": "28 million training images), fine-tune it for semantic segmentation on the COCO dataset (80K training images) [17], and then fine-tune it again on PACAL VOC (1,464 training images)[4, 16].", "startOffset": 180, "endOffset": 187}, {"referenceID": 11, "context": "Contributions: Our paper makes three major contributions: 1) we are the first to use recent fully-convolutional neural networks for semantic segmentation with multispectral remote sensing imagery; 2) we show that pre-training these networks on synthetic DIRSIG imagery for semantic segmentation significantly increases their performance; and 3) it surpasses the previous state-of-the-art mean-class accuracy on the Hamlin Beach State Park dataset [14] by 23.", "startOffset": 447, "endOffset": 451}, {"referenceID": 17, "context": "The convolution network is usually a pre-trained DCNN designed to classify images from ImageNet [20, 23, 4, 25], and current state-ofthe-art performers use VGG-16 [32] or ResNet [30].", "startOffset": 96, "endOffset": 111}, {"referenceID": 20, "context": "The convolution network is usually a pre-trained DCNN designed to classify images from ImageNet [20, 23, 4, 25], and current state-ofthe-art performers use VGG-16 [32] or ResNet [30].", "startOffset": 96, "endOffset": 111}, {"referenceID": 1, "context": "The convolution network is usually a pre-trained DCNN designed to classify images from ImageNet [20, 23, 4, 25], and current state-ofthe-art performers use VGG-16 [32] or ResNet [30].", "startOffset": 96, "endOffset": 111}, {"referenceID": 22, "context": "The convolution network is usually a pre-trained DCNN designed to classify images from ImageNet [20, 23, 4, 25], and current state-ofthe-art performers use VGG-16 [32] or ResNet [30].", "startOffset": 96, "endOffset": 111}, {"referenceID": 29, "context": "The convolution network is usually a pre-trained DCNN designed to classify images from ImageNet [20, 23, 4, 25], and current state-ofthe-art performers use VGG-16 [32] or ResNet [30].", "startOffset": 163, "endOffset": 167}, {"referenceID": 27, "context": "The convolution network is usually a pre-trained DCNN designed to classify images from ImageNet [20, 23, 4, 25], and current state-ofthe-art performers use VGG-16 [32] or ResNet [30].", "startOffset": 178, "endOffset": 182}, {"referenceID": 17, "context": "The first fully-convolutional network (FCN) designed for semantic segmentation [20] used the VGG-16 network [32], which has approximately 138 million parameters.", "startOffset": 79, "endOffset": 83}, {"referenceID": 29, "context": "The first fully-convolutional network (FCN) designed for semantic segmentation [20] used the VGG-16 network [32], which has approximately 138 million parameters.", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "The net\u2019s major disadvantage was that VGG-16\u2019s 5 max-pooling layers shrunk the original image by a factor of 32, resulting in a coarse label map [20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 20, "context": "In [23], this FCN model was improved by building a symmetric (deconvolution) network using spatial unpooling and deconvolution layers.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "As a post-processing step, the authors used a conditional random field (CRF) to sharpen the classification boundaries [15].", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "The major downside to this deconvolution network was that it required more memory and time to train compared to [20].", "startOffset": 112, "endOffset": 116}, {"referenceID": 1, "context": "The DeepLab network [4] was built with the ResNet DCNN.", "startOffset": 20, "endOffset": 23}, {"referenceID": 22, "context": "Two recent models that did this, Sharpmask [25] and RefineNet [16], used skip-connections to incorporate image refinement into the end-to-end model.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "Two recent models that did this, Sharpmask [25] and RefineNet [16], used skip-connections to incorporate image refinement into the end-to-end model.", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "The most successful of these feature extraction methods is the stacked autoencoder [19, 18, 36, 22, 33].", "startOffset": 83, "endOffset": 103}, {"referenceID": 15, "context": "The most successful of these feature extraction methods is the stacked autoencoder [19, 18, 36, 22, 33].", "startOffset": 83, "endOffset": 103}, {"referenceID": 33, "context": "The most successful of these feature extraction methods is the stacked autoencoder [19, 18, 36, 22, 33].", "startOffset": 83, "endOffset": 103}, {"referenceID": 19, "context": "The most successful of these feature extraction methods is the stacked autoencoder [19, 18, 36, 22, 33].", "startOffset": 83, "endOffset": 103}, {"referenceID": 30, "context": "The most successful of these feature extraction methods is the stacked autoencoder [19, 18, 36, 22, 33].", "startOffset": 83, "endOffset": 103}, {"referenceID": 18, "context": ", [21, 18, 11]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 15, "context": ", [21, 18, 11]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 8, "context": ", [21, 18, 11]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 30, "context": "In [33], stacked sparse autoencoders (SSAE) were used to learn feature extracting filters from one hyperspectral image, and then these filters were used to classify a separate target dataset.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "In [13], the discriminative power of these unsupervised feature extracting frameworks was improved by training the model on large quantities of unlabeled hyperspectral data, which is known as self-taught learning [26].", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "In [13], the discriminative power of these unsupervised feature extracting frameworks was improved by training the model on large quantities of unlabeled hyperspectral data, which is known as self-taught learning [26].", "startOffset": 213, "endOffset": 217}, {"referenceID": 21, "context": "Synthetic data has been used to increase the quantity of available training data for many applications, including object detection [24], pose estimation [5], face and handwriting recognition [35], and semantic segmentation [28].", "startOffset": 131, "endOffset": 135}, {"referenceID": 2, "context": "Synthetic data has been used to increase the quantity of available training data for many applications, including object detection [24], pose estimation [5], face and handwriting recognition [35], and semantic segmentation [28].", "startOffset": 153, "endOffset": 156}, {"referenceID": 32, "context": "Synthetic data has been used to increase the quantity of available training data for many applications, including object detection [24], pose estimation [5], face and handwriting recognition [35], and semantic segmentation [28].", "startOffset": 191, "endOffset": 195}, {"referenceID": 25, "context": "Synthetic data has been used to increase the quantity of available training data for many applications, including object detection [24], pose estimation [5], face and handwriting recognition [35], and semantic segmentation [28].", "startOffset": 223, "endOffset": 227}, {"referenceID": 0, "context": "Researchers have adopted domain adaptation techniques [3] to mitigate this phenomenon, including training autoencoders to shift the distribution of the synthetic data to the distribution of the real data [8, 35].", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "Researchers have adopted domain adaptation techniques [3] to mitigate this phenomenon, including training autoencoders to shift the distribution of the synthetic data to the distribution of the real data [8, 35].", "startOffset": 204, "endOffset": 211}, {"referenceID": 32, "context": "Researchers have adopted domain adaptation techniques [3] to mitigate this phenomenon, including training autoencoders to shift the distribution of the synthetic data to the distribution of the real data [8, 35].", "startOffset": 204, "endOffset": 211}, {"referenceID": 25, "context": "In [28], the authors built a synthetic dataset using a virtual reality generator for the semantic segmentation of autonomous driving datasets.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "DIRSIG images an object using physics-based radiative transfer/propagation modeling [12, 31].", "startOffset": 84, "endOffset": 92}, {"referenceID": 28, "context": "DIRSIG images an object using physics-based radiative transfer/propagation modeling [12, 31].", "startOffset": 84, "endOffset": 92}, {"referenceID": 22, "context": "In this paper, we adapted the Sharpmask [25] and RefineNet [16] semantic segmentation frameworks to work for our requirements.", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "In this paper, we adapted the Sharpmask [25] and RefineNet [16] semantic segmentation frameworks to work for our requirements.", "startOffset": 59, "endOffset": 63}, {"referenceID": 6, "context": "The convolution network for both of these models was ResNet-50 [9] with the improved network architecture scheme proposed in [10], where batch-normalization and ReLU are applied prior to each convolution.", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "The convolution network for both of these models was ResNet-50 [9] with the improved network architecture scheme proposed in [10], where batch-normalization and ReLU are applied prior to each convolution.", "startOffset": 125, "endOffset": 129}, {"referenceID": 3, "context": "Both segmentation models in this paper were developed using Theano/Keras [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 22, "context": "Since our model is being trained from scratch, we use batch normalization for regularization, whereas the original Sharpmask model did not [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "These parameters differ slightly from [25] because the", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "We evaluated our deep network initialization technique on the challenging Hamlin Beach State Park dataset [14], which consists of training (9,393\u00d75,642), validation (8,833\u00d76,918), and test (12,446\u00d77,654) images.", "startOffset": 106, "endOffset": 110}, {"referenceID": 26, "context": "The International Society for Photogrammetry and Remote Sensing (ISPRS) has two semantic segmentation challenges that uses multispectral imagery with a GSD of 5-9 cm but only six class labels [29].", "startOffset": 192, "endOffset": 196}, {"referenceID": 11, "context": "Previous attempts to classify the Hamlin Beach State Park dataset resulted in mediocre performance [14].", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "The state-ofthe-art performer in terms of mean-class accuracy was a spatial-spectral feature extracting framework called multiscale independent component analysis (MICA) [13].", "startOffset": 170, "endOffset": 174}, {"referenceID": 11, "context": "We compare our results against MICA, the previous state-of-the-art method on this dataset [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 11, "context": "The black panel has a small number of training samples; however, [14] demonstrated that simple target detection algorithms may be more appropriate for finding both wooden panels.", "startOffset": 65, "endOffset": 69}, {"referenceID": 31, "context": "In the future, we hope to improve our model by 1) exploring deeper ResNet models; 2) using newer state-of-theart convolution (ResNeXt [34]) and segmentation models; and 3) improving the inherent GSD of the DIRSIG scene; and 4) including additional diverse classes to the synthetic data.", "startOffset": 134, "endOffset": 138}], "year": 2017, "abstractText": "A semantic segmentation algorithm must assign a label to every pixel in an image. Recently, semantic segmentation of RGB imagery has advanced significantly due to deep learning. Because creating datasets for semantic segmentation is laborious, these datasets tend to be significantly smaller than object recognition datasets. This makes it difficult to directly train a deep neural network for semantic segmentation, because it will be prone to overfitting. To cope with this, deep learning models typically use convolutional neural networks pre-trained on large-scale image classification datasets, which are then fine-tuned for semantic segmentation. For non-RGB imagery, this is currently not possible because large-scale labeled non-RGB datasets do not exist. In this paper, we developed two deep neural networks for semantic segmentation of multispectral remote sensing imagery. Prior to training on the target dataset, we initialize the networks with large amounts of synthetic multispectral imagery. We show that this significantly improves results on real-world remote sensing imagery, and we establish a new state-of-the-art result on the challenging Hamlin Beach State Park Dataset.", "creator": "LaTeX with hyperref package"}}}