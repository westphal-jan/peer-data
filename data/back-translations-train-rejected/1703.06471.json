{"id": "1703.06471", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2017", "title": "Multi-Timescale, Gradient Descent, Temporal Difference Learning with Linear Options", "abstract": "Deliberating on large or continuous state spaces have been long standing challenges in reinforcement learning. Temporal Abstraction have somewhat made this possible, but efficiently planing using temporal abstraction still remains an issue. Moreover using spatial abstractions to learn policies for various situations at once while using temporal abstraction models is an open problem. We propose here an efficient algorithm which is convergent under linear function approximation while planning using temporally abstract actions. We show how this algorithm can be used along with randomly generated option models over multiple time scales to plan agents which need to act real time. Using these randomly generated option models over multiple time scales are shown to reduce number of decision epochs required to solve the given task, hence effectively reducing the time needed for deliberation.", "histories": [["v1", "Sun, 19 Mar 2017 17:31:13 GMT  (578kb,D)", "http://arxiv.org/abs/1703.06471v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["peeyush kumar", "doina precup"], "accepted": false, "id": "1703.06471"}, "pdf": {"name": "1703.06471.pdf", "metadata": {"source": "CRF", "title": "Multi-Timescale, Gradient Descent, Temporal Difference Learning with Linear Options", "authors": ["Peeyush Kumar"], "emails": ["agaron@uw.edu", "dprecup@cs.mcgill.ca"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.06 471v 1"}, {"heading": "1 Introduction", "text": "The use of higher, time-extended measures enables an AI agent to solve an entire task in a smaller number of decision-making eras. Like many other AI systems, Temporal Abstraction is often used in reinforcement models to efficiently solve major problems. Various methods have been proposed to reduce the computational load and reduce data complexity in learning by allowing planning in AI systems to calculate good policies for many situations at once. In environments with continuously evaluated observations or others with a large number of states, some form of generalization is necessary. Linear methods are perhaps the most common and best understood class of generalization mechanisms. Linear methods include a wide range of approaches, including lookup tables, state aggregation methods, radial functions with fixed foundations, etc. Frameworks for temporary abstractions generally do not contain an intrinsic structure to describe such temporal abstractions. In order to achieve abstractions over time, abstractions are commonly abstracted."}, {"heading": "2 Background", "text": "An MDP is a tuple (S, A, P, R, \u03b3) consisting of a state set S, an action set A, a transition function P: S \u00b7 A \u00b7 S 7 \u2192 [0, 1], an expected reward function R: S \u00b7 A 7 \u2192 IR and a discount factor \u03b3 [0, 1]. At each MDP time step, the actor takes an action in the state st, and the environment reacts with a reward rt and the resulting next state s't = st + 1. The Bellman equation for defining the optimal action value function Q \u0445 (s, a) = R (s, a) = R (s, a) = R (s, a) is the expected discounted return that is achieved by trading in state s and then following it. The Bellman equation defining the optimal action value function Q \u0445 is: Q \u0445 (s, a) = R (s, a) = R (s, a) = R (s, a) = R (s, R = R (s, a), a (a) is maedy."}, {"heading": "2.1 Off Policy Temporal Difference Learning", "text": "Of the various methods of learning in RL, temporal difference methods are undoubtedly the most important and novel. Temporal difference methods have the advantage that they can learn directly from the raw data without having to make any demands on the environmental model, and they can start by updating estimates that are partly based on other estimates learned. Of the methods of time difference (TD), the off-policy TD control algorithm known as Q-Learning is the most central. Its simplest form, single-stage Q-Learning, is defined by Q (st, at) = Q (st, at) + \u03b1 [rt + 1 + tacmaxa Q (st + 1, a) \u2212 Q (st, at)."}, {"heading": "2.2 Linear Methods for MDPs", "text": "Instead of using the tabular representation of states, states are represented in the form of n-dimensional attribute vectors \u03c6 (s) and IRn. Linear methods approach the value function for a policy \u03c0 as a linear combination of state attribute vectors. Convergence cannot be guaranteed for such methods (Baird (1995)) Several non-gradient descending approaches to this problem have been developed, but none is completely satisfactory. Second-order methods, such as LSTD (Bradtke et al. (1996), Boyan (2002), show that the convergence between the two methods is stable, but their computational complexity is square in the size of the state."}, {"heading": "2.3 Options and SMDP", "text": "Options are extended-time actions. They can be regarded as fixed measures that can be applied in certain states and that are terminated according to a termination condition. Formally, an option is a triple (I, \u00b5, \u03b2), with the sentence I, S being the termination probability of the option. MDP together with the options describe a Semi-Markov decision-making process (SMDP). SMDP is like an MDP, except that the options take different amounts of time. In each SMDP time step, the agent selects an option and follows the option policy until completion. In each state, one must select an option from the options available in that state. When the system enters a new subset of state space, a new group of options becomes available."}, {"heading": "2.4 Linear Options", "text": "Linear options are a direct extension of the options box from a tabular representation to a linear representation. Linear options are defined by states that are represented as n-dimensional feature vectors. A linear option is a tuple (I, \u00b5, \u03b2) in which I-IRn is the initiation behavior, \u00b5: IRn 7 \u2192 A is the option policy, and \u03b2: IRn 7 \u2192 [0, 1] is the termination condition.Sorg and Singh (2010) proposed the idea of linear options as linear option representations. They proposed an extension of the MDP-LSTD method for policy evaluation in the primitive action case to include options by defining the SMDP-LSTD method for behavior policy evaluation. The solution given in SMDP-LSTD is 0 = t-K-K-K method for interpreting options."}, {"heading": "3 Gradient Descent TD algorithm for linear op-", "text": "The developed LSTD algorithms may be stable under general conditions, but they are square in terms of computational complexity. We propose here a method of derivation of gradients (SMDP-TDC) for planning in the field of attachment learning using a non-political TD framework. SMDP-TDC is the direct extension of the TDC algorithm (equations 1, 2 and 3) to the SMDP setting. The option value function is approximately V \u03c0 (s) = maxoQ\u03c0 (s, o). We aim to minimize the medium-sized square projected bellman error (MSPBE\u03b8 = VTB). \u2212 For the linear option architecture VTB-TDC-2D \u2212 \u2212 D, this is the matrix whose series are. This method is the projection operator that the projection operators are (SDDDP)."}, {"heading": "3.1 Convergence Proof for SMDP-TDC", "text": "Theorem 1 Let us consider the iterations 9 and 10 of the SMDP TDC algorithm. Let us leave the step size sequences \u03b1k and \u03b2k, k \u2265 0, in this case: - and this is indeed k = 0, p = 0, p = 0, p = 0, p = 0, p = 0, p = 0, p = 0, p = 2, p = 0, p = 2, p = 0, p = 0, p = 0, p = 0, p = 0, p = 0, p = 0, p = 0, p = 2, p = 0, p = 0, p = 0, p = 0, p = 0, p = 0, p =, p =, p =, p =, p =, p =, p =, p, p =, p =, p, p =, p =, p =, p =, p =."}, {"heading": "4 Multiple Time Scales and Real Time RL", "text": "Although planning to act on multiple time scales requires more computing time (real-time systems do not have much time for thought, so a lot of research has been done to construct just the right number of options by optimizing the computing time around models to accommodate multiple time scales (Stolle and Precup (2002), Wolfe and Barto (2005)). Is it necessary to be so careful, or can we handle many option models? The ideas that are central to planning seek and learn during the search. We show empirically that it is much better to search with many option models than with a few handmade ones. We use the sample search to search through the state space. The search is done by picking up a quasi-random model and sampling the next state down to a certain depth."}, {"heading": "5 Empirical Results", "text": "In order to evaluate the practical benefits of the proposed framework, we show here some experiments where the system learns linear option models on multiple time scales, updating the estimates using the SMDP-TDC algorithm. We also show the time scales on which the system acts depending on its position in the environment and its immediate neighbors. Results are demonstrated in two areas: the first domain is a grid world domain, as shown in Figure 1, and the second domain is the continuous domain of large dimensional spaces."}, {"heading": "5.1 Grid World Domain", "text": "The system starts from every state in the state, it gets a reward of + 10 when it reaches the target state (blue field in Figure 1), while it gets a reward of -3 when it hits the wall, and a reward of -1 for all other transitions. The system has four primitive measures while acting in the environment with 15% noise. We demonstrate two experimental conditions in which option models are composed over multiple timescales and different random measures. The first experiment consists of two time scales - \u03b2 = 0.5, 1 and distorted random measures defined as follows - choose a probability x (0, 1), choose an action a, the system selects action a with a probability x and all other actions equally with the probability 1 \u2212 x. The system has the same termination conditions and selects primitive measures with respect to the same policy for all states. Likewise, there are 40 strategies, giving the system 80 options."}, {"heading": "5.2 Continuous Domain", "text": "The domain (Figure 6), which is 10 \u00b7 10 Continuous Space World, consists of rooms separated by walls as shown. In addition, the floor of each room is dyed with one of the 4 colors (P). (G) reen, (Y) yellow, (B) lue. In the figure, the floor colors are indicated by the respective first letter, and the rooms are separated by dashed lines. (The walls are represented by fixed lines.) The agent controls a circular wheel robot for navigation, which is able to observe its current global position. (X, y] 2 and its current global orientation in relation to the angle. (0, 360] In addition, he can recognize the color of the floor below him."}, {"heading": "6 Conclusion", "text": "Systems that need to act in real time do not have much time for thought. For such systems, we suggest the use of many option models that are composed over multiple time scales. We also propose an SMDP learning algorithm that is proven to be convergent with linear option models, while achieving linear time efficiency and storage requirements at the same time. We demonstrate our results in the area of the networking world and a continuous space area. We observe that systems that work with the SMDP TDC algorithm over multiple time scales when learning are performing better in all aspects than other methods. They exhibit recursive optimal behavior that is reasonably good compared to the optimal solution. Moreover, they achieve goals more efficiently compared to other methods that are considered in each step. It will be interesting to see the extension of the SMDP TDC algorithm using multiple time steps and permissions tracing."}], "references": [{"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning, pages 30\u201337. Morgan Kaufmann.", "citeRegEx": "Baird,? 1995", "shortCiteRegEx": "Baird", "year": 1995}, {"title": "Stochastic approximation with two time scales", "author": ["V.S. Borkar"], "venue": "Systems amp; Control Letters.", "citeRegEx": "Borkar,? 1997", "shortCiteRegEx": "Borkar", "year": 1997}, {"title": "The o.d.e. method for convergence of stochastic approximation and reinforcement learning", "author": ["V.S. Borkar", "S.P. Meyn"], "venue": "SIAM J. CONTROL OPTIM", "citeRegEx": "Borkar and Meyn,? \\Q2000\\E", "shortCiteRegEx": "Borkar and Meyn", "year": 2000}, {"title": "Technical update: Least-squares temporal difference learning", "author": ["J.A. Boyan"], "venue": "Machine Learning. 13", "citeRegEx": "Boyan,? 2002", "shortCiteRegEx": "Boyan", "year": 2002}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["S.J. Bradtke", "A.G. Barto", "P. Kaelbling"], "venue": "Machine Learning, pages 22\u2013", "citeRegEx": "Bradtke et al\\.,? 1996", "shortCiteRegEx": "Bradtke et al\\.", "year": 1996}, {"title": "Convergent activation dynamics in continuous time networks", "author": ["M.W. Hirsch"], "venue": "Neural Netw.", "citeRegEx": "Hirsch,? 1989", "shortCiteRegEx": "Hirsch", "year": 1989}, {"title": "Bandit based monte-carlo planning", "author": ["L. Kocsis", "C. Szepesvri"], "venue": "In: ECML-06. Number 4212 in LNCS.", "citeRegEx": "Kocsis and Szepesvri,? 2006", "shortCiteRegEx": "Kocsis and Szepesvri", "year": 2006}, {"title": "Linear options", "author": ["J. Sorg", "S. Singh"], "venue": "AAMAS \u201910.", "citeRegEx": "Sorg and Singh,? 2010", "shortCiteRegEx": "Sorg and Singh", "year": 2010}, {"title": "Learning options in reinforcement learning", "author": ["M. Stolle", "D. Precup"], "venue": "Lecture Notes in Computer Science.", "citeRegEx": "Stolle and Precup,? 2002", "shortCiteRegEx": "Stolle and Precup", "year": 2002}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Fast gradient-descent methods for temporaldifference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesvri", "E. Wiewiora"], "venue": "In Proceedings of the 26th International Conference on Machine Learning.", "citeRegEx": "Sutton et al\\.,? 2009", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "A convergent o(n) algorithm for off-policy temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "C. Szepesvri", "H.R. Maei"], "venue": "Advances in Neural Information Processing Systems 21. MIT Press.", "citeRegEx": "Sutton et al\\.,? 2001", "shortCiteRegEx": "Sutton et al\\.", "year": 2001}, {"title": "Reinforcement learning via aixi approximation", "author": ["J. Veness", "K.S. Ng", "M. Hutter", "D. Silver"], "venue": "AAAI\u201910.", "citeRegEx": "Veness et al\\.,? 2010", "shortCiteRegEx": "Veness et al\\.", "year": 2010}, {"title": "Identifying useful subgoals in reinforcement learning by local graph partitioning", "author": ["A.P. Wolfe", "A.G. Barto"], "venue": "ICML.", "citeRegEx": "Wolfe and Barto,? 2005", "shortCiteRegEx": "Wolfe and Barto", "year": 2005}], "referenceMentions": [{"referenceID": 8, "context": "Markov Decision Process (MDP) models provide a formal method of planning in controlled dynamical systems (Sutton and Barto (1998)).", "startOffset": 106, "endOffset": 130}, {"referenceID": 7, "context": "We use the linear option models (Sorg and Singh (2010)) for learning and planning.", "startOffset": 33, "endOffset": 55}, {"referenceID": 10, "context": "Recently various temporal difference algorithms were proposed which are compatible with both linear function approximation and off-policy training using primitive action in MDP setting (GTD algorithm Sutton et al. (2001), GTD2 and TDC algorithms Sutton et al.", "startOffset": 200, "endOffset": 221}, {"referenceID": 10, "context": "Recently various temporal difference algorithms were proposed which are compatible with both linear function approximation and off-policy training using primitive action in MDP setting (GTD algorithm Sutton et al. (2001), GTD2 and TDC algorithms Sutton et al. (2009)).", "startOffset": 200, "endOffset": 267}, {"referenceID": 0, "context": "As a result convergence cannot be guaranteed for such methods (Baird (1995)).", "startOffset": 63, "endOffset": 76}, {"referenceID": 0, "context": "As a result convergence cannot be guaranteed for such methods (Baird (1995)). Several non-gradient-descent approaches to this problem have been developed, but none has been completely satisfactory . Second-order methods, such as LSTD (Bradtke et al. (1996), Boyan (2002)), can be guaranteed stable under general conditions, but their computational complexity is quadratic in the size of the state space.", "startOffset": 63, "endOffset": 257}, {"referenceID": 0, "context": "As a result convergence cannot be guaranteed for such methods (Baird (1995)). Several non-gradient-descent approaches to this problem have been developed, but none has been completely satisfactory . Second-order methods, such as LSTD (Bradtke et al. (1996), Boyan (2002)), can be guaranteed stable under general conditions, but their computational complexity is quadratic in the size of the state space.", "startOffset": 63, "endOffset": 271}, {"referenceID": 7, "context": "Sorg and Singh (2010) proposed the idea of linear options as linear representations of options.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Convergence proof of SMDP-TDC can be carried out in a similar way as in Sutton et al. (2009). Hence it can be shown that SMDP-TDC converges to the TD fixed point with probability one.", "startOffset": 72, "endOffset": 93}, {"referenceID": 1, "context": "Proof The proof of this theorem is based on a two time scale difference in the step-size schedule {\u03b1k} and {\u03b2k}; refer Borkar (1997) for a convergence analysis of the general two timescale stochastic approximation recursions.", "startOffset": 119, "endOffset": 133}, {"referenceID": 5, "context": "By the Hirsch lemma (Hirsch (1989)), it follow that \u2016\u03b8k \u2212 \u03b8\u2016 \u2192 0 almost surely as k \u2192\u221e for the same \u03b8 that depends on the initial condition \u03b80 of recursion 9.", "startOffset": 7, "endOffset": 35}, {"referenceID": 1, "context": "The assumptions are now verified and by Borkar and Meyn (2000) Theorem 2.", "startOffset": 40, "endOffset": 63}, {"referenceID": 8, "context": "Hence a lot of research has been done to construct just the right number of options by optimizing the computational time around models to accommodate multiple time scales (Stolle and Precup (2002), Wolfe and Barto (2005)).", "startOffset": 172, "endOffset": 197}, {"referenceID": 8, "context": "Hence a lot of research has been done to construct just the right number of options by optimizing the computational time around models to accommodate multiple time scales (Stolle and Precup (2002), Wolfe and Barto (2005)).", "startOffset": 172, "endOffset": 221}, {"referenceID": 6, "context": "Typically a large number of rollouts is performed, and values lower in the search tree are used to update the values higher up in the search tree (Kocsis and Szepesvri (2006), Veness et al.", "startOffset": 147, "endOffset": 175}, {"referenceID": 6, "context": "Typically a large number of rollouts is performed, and values lower in the search tree are used to update the values higher up in the search tree (Kocsis and Szepesvri (2006), Veness et al. (2010)).", "startOffset": 147, "endOffset": 197}, {"referenceID": 7, "context": "We ran our second set of experiments on a continuous navigation domain as described in Sorg and Singh (2010). The domain (Figure 6), which is 10 \u00d7 10 continuous room world consists of rooms which are separated by walls as shown.", "startOffset": 87, "endOffset": 109}], "year": 2017, "abstractText": "Deliberating on large or continuous state spaces have been long standing challenges in reinforcement learning. Temporal Abstraction have somewhat made this possible, but efficiently planing using temporal abstraction still remains an issue. Moreover using spatial abstractions to learn policies for various situations at once while using temporal abstraction models is an open problem. We propose here an efficient algorithm which is convergent under linear function approximation while planning using temporally abstract actions. We show how this algorithm can be used along with randomly generated option models over multiple time scales to plan agents which need to act real time. Using these randomly generated option models over multiple time scales are shown to reduce number of decision epochs required to solve the given task, hence effectively reducing the time needed for deliberation. ar X iv :1 70 3. 06 47 1v 1 [ cs .A I] 1 9 M ar 2 01 7", "creator": "LaTeX with hyperref package"}}}