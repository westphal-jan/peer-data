{"id": "1703.06492", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2017", "title": "VQABQ: Visual Question Answering by Basic Questions", "abstract": "Taking image and question as the input of our method, it can output the text-based answer of the query question about the given image, so called Visual Question Answering (VQA). There are two main modules in our algorithm. Given a natural language question about an image, the first module takes the question as input and then outputs the basic questions of the main question, given question. The second module takes the main question, image and these basic questions as input and then outputs the text-based answer of the main question. We formulate the basic questions generation problem as a LASSO optimization problem, and also propose a criterion about how to exploit these basic questions to help answer main question. Our method is evaluated on the challenging VQA dataset, and yields the competitive performance compared to state-of-the-art.", "histories": [["v1", "Sun, 19 Mar 2017 19:14:55 GMT  (1013kb,D)", "https://arxiv.org/abs/1703.06492v1", "Submitted ICCV 2017"], ["v2", "Mon, 28 Aug 2017 22:40:19 GMT  (1013kb,D)", "http://arxiv.org/abs/1703.06492v2", "Accepted by CVPR 2017 VQA Challenge Workshop. (Tables updated)"]], "COMMENTS": "Submitted ICCV 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["jia-hong huang", "modar alfadly", "bernard ghanem"], "accepted": false, "id": "1703.06492"}, "pdf": {"name": "1703.06492.pdf", "metadata": {"source": "CRF", "title": "VQABQ: Visual Question Answering by Basic Questions", "authors": ["Jia-Hong Huang", "Modar Alfadly", "Bernard Ghanem"], "emails": ["bernard.ghanem}@kaust.edu.sa"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that one is able to find a solution that is capable of finding a solution that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution."}, {"heading": "2. Motivations", "text": "The following two important reasons motivate us to do Visual Question Answering by Basic Questions (VQABQ). Firstly, lately, most VQA work only emphasizes the image part, the visual features, but puts less effort on the question part, the text properties. However, image and question functions are both important for VQA. If we focus on just one of them, we are unlikely to achieve the good performance of VQA in the near future. [14] Therefore, we should focus our efforts on both at the same time. In [14] they proposed a novel co-attention mechanism that jointly performs image-oriented question attention and question-oriented image attention for VQA. [14] We also proposed a hierarchical architecture to represent the question and create image-question cards at the word level, phrase level and question level. Then, these shared features are combined with word level, phrase level and question level to recursively use them for the pre-prediction of the question."}, {"heading": "3. Related Work", "text": "There are many papers [1, 26, 2, 9, 15, 25, 36, 29] that raise the question of how to solve the problem of VQA. Our method covers various areas of machine learning, natural language processing (NLP) and computer visualization. Below, we discuss what we are doing to tackle the problem of VQA. However, there are also a number of methods that deal with the issue of VQA, such as machine translation of texts into language. Authors use the Flexible Length Sequences (LSTM). It is a special variant of RNN and in the natural language, such as the translation of images, which LSTM is a successful application. In this case, the authors use RNN and the conventional neural network to ask a question and ask a question."}, {"heading": "4. Basic Question Dataset", "text": "We propose a new dataset, called Basic Question Dataset (BQD), generated by our basic question generation algorithm. BQD is the first basic question dataset. With respect to BQD, the dataset format is {Image, MQ, 3 (BQ + corresponding similarity value)}. All our images are taken from the test images of the MS COCO dataset [13], the MQ, main questions, are taken from the test questions of the VQA, open dataset [1], the BQ, basic questions, from the training and validation questions of the VQA dataset [1], and the corresponding similarity value of the BQ is generated by our basic question generation method, which refers to section 5. In addition, we also take the multiple-choice questions in VQA dataset [1] to do the same as above. Note that we remove the repeated questions in the Total Questions V7A-Q8Q is the Q8Q."}, {"heading": "5. Methodology", "text": "In Section 5, we will mainly discuss how to encode questions and generate BQ, and why we use the Co-Attention Mechanism VQA algorithm [14] to answer the query, and the overall architecture of our VQABQ model can be seen in Figure 2. The model consists of two main parts, Module 1 and Module 2. Regarding Module 1, it takes the encoded MQ as input and uses the matrix of the encoded BQ to output the BQ of the query, then Module 2 is a VQA algorithm with the Co-Attention mechanism [14], and it takes the output of Module 1, MQ and the given image as input, and then outputs the final answer from MQ. The detailed architecture of Module 1 can be referred to Figure 2."}, {"heading": "5.1. Question encoding", "text": "There are many popular text encoders, such as Word2Vec [20], GloVe [23], and Skip-Thoughts [11]. In these encoders, Skip-Thoughts can focus not only on the word-to-word meaning, but also on the semantic meaning of the whole sentence. Therefore, we choose Skip-Thoughts as our question encoding method. In the Skip-Thoughts model, it uses an RNN encoder with GRU enabled [4], and then we use this encoder to turn an English sentence into a vector. With respect to GRU, it has been shown to work just as well as LSTM [7] in sequence modeling applications, but is conceptually easier because GRU units have only 2 gates and not the use of a cell.Question encoder: w1i,... wNi, the words in question si and N are the total number of words in question \u2212 note that the word si-hidden is the question."}, {"heading": "5.2. Problem Formulation", "text": "Our idea is the BQ generation for MQ, and at the same time we just want the minimum number of BQ to represent the MQ, so modelling our problem as a LASSO optimization problem is an appropriate method: min x1, 2, 3, 4, 5, 5, where A is the matrix of the coded BQ, b is the coded MQ, and \u03bb is a parameter of the regularization term."}, {"heading": "5.3. Basic Question Generation", "text": "We will now describe how to generate the BQ of a query, illustrated by Figure 2. Note that in the following we will only describe the case of the open question, because the case of the multiple choice question is the same as the case of the open question. According to Section 5.2, we can encode all questions from the training and validation questions of the VQA dataset [1] by Skip-Thought Vectors, and then we have the matrix of these encoded basic questions. Each column of the matrix is the vector representation of a basic question, 4800 times 1 dimensions, and we have 215623 columns. That is, the dimension of the BQ matrix, called A, is 4800 times 215623. In addition, we will encode the query as a column vector, 4800 times 1 dimensions, by Skip-Thought Vectors, called b. Now we can solve the BASSO optimization problem, which is mentioned in Section 5.3 to solve the solution."}, {"heading": "5.4. Basic Question Concatenation", "text": "In this section, we propose a criterion to use this BQ. In BQD, each MQ has three corresponding BQ with values. We can have the following format: {MQ, (BQ1, Score1), (BQ2, Score2), (BQ3, Score3), and these values are all between 0 and 1 in the following order, Score1 / Score2 \u2265 Score3 (6), and we define 3 thresholds, s1, s2, and s3. In addition, we calculate the following 3 averages (avg) and 3 standard deviations (std) to Score1, Score2 / Score1, or Score3 / Score2, and then use avg \u00b1 std, based on Table 3, to be the first estimate of the correct thresholds. The BQ usage process can be explained as Table 1. Detailed discussion of the BQ chaining algorithm is described in Section 6.4."}, {"heading": "5.5. Co-Attention Mechanism", "text": "There are two types of co-attention mechanism [14], parallel and alternative. In our VQABQ model, we only use the VQA algorithm with alternative co-attention mechanism to be our VQA module, which refers to Figure 2, because in [14] the alternative co-attention mechanism VQA can be more accurate than the parallel mechanism. In addition, this mechanism consists of three main steps: \u2022 Firstly, the input request is summarized in a single vector q. \u2022 Secondly, one has to take care of the given image, which depends on q. \u2022 Thirdly, one has to take care of the question which depends on the image visited."}, {"heading": "6. Experiment", "text": "In section 6 we describe the details of our implementation and discuss the results of the experiments on the proposed method."}, {"heading": "6.1. Datasets", "text": "We are conducting our experiments with the VQA [1] dataset. The VQA dataset is based on the MS COCO dataset [13] and contains the largest number of questions. There are questions, 248349 for training, 121512 for validation, and 244302 for testing. In the VQA dataset, each question is associated with 10 answers commented by different people from Amazon Mechanical Turk (AMT). Approximately 98% of the answers do not exceed 3 words, and 90% of the answers have single words. Note that we test our method only on the open case in the VQA dataset because it has the most open questions of all available datasets, and we think that open tasks are closer to the real situation than multiple choice datasets."}, {"heading": "6.2. Setup", "text": "In order to prove our claim that BQ can contribute to accuracy and can be compared with the state-of-the-art VQA method [14], we therefore use the same setting, data set and source code in Module 2 as in [14]. Then Module 1 in the VQABQ model is our basic question generation module. In other words, in our model, the only difference to [14] is our Module 1, illustrated in Figure 2."}, {"heading": "6.3. Evaluation Metrics", "text": "The VQA dataset provides an open-ended multiple-choice task for evaluation. In view of an open-ended task, the answer can be any phrase or word. However, in a multiple-choice task, an answer should be selected from 18 candidate answers. In both cases, the answers are evaluated on the basis of accuracy that can reflect human consensus. Accuracy is given by the following information: Accuracy V QA = 1N N N \u2211 i = 1 min {\u2211 t-Ti I [ai = t] 3, 1} (10), where N is the total number of examples, I [\u00b7] stands for an indicator function, ai is the predicted answer, and Ti is a collection of responses from the ith example. That is, a predicted answer is considered correct if at least 3 commentators agree, and the score depends on the total number of matches if the predicted answer is incorrect."}, {"heading": "6.4. Results and Analysis", "text": "Here we describe our final results and analysis through the following parts: Does Basic Question Help Accuracy? The answer is yes. Here we discuss only the open case. In our experiment, we use the avg \u00b1 std, based on Table 3, to get the initial guess of s1, s2 and s3, in Table 1, s2 = 0.82 and s3 = 0.53, we can get better use of the BQ. The threshold, s1 = 0.43, can be considered as 43% of the test questions of VQA, which do not find the basic question of training and validation of the records of VQA, and only 57% of the test questions can be found. The threshold, s1 = 0.43% of the test questions of VQA record, which are not found."}, {"heading": "7. Conclusion and Future Work", "text": "In this paper, we propose a VQABQ model for answering visual questions. The VQABQ model consists of two main modules: Basic Question Generation Modules and Co-Attention VQA Modules. The former can generate the basic questions for the query, and the latter can use the image, basic and query queries as input modules and then output the text-based answer to the query. According to Section 6.4, since the basic question record generated from the VQA dataset is not good enough, we only have the 57% of all test questions that can benefit from the basic questions. Nevertheless, we can increase 28 correctly answered questions compared to the state of the art. We believe that if our basic question record is good enough, the increase in accuracy will be much higher. According to the prior state of the art in VQA, they will all achieve the highest accuracy in the yes / no question."}, {"heading": "Acknowledgements", "text": "This work is supported by competitive research grants from the King Abdullah University of Science and Technology (KAUST). We would also like to pay tribute to Fabian Caba, Humam Alwassel and Adel Bibi, who are always helpful to us in discussing this work."}], "references": [{"title": "Vqa: Visual question  answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2425\u20132433,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Abc-cnn: An attention based convolutional neural network for visual question answering", "author": ["K. Chen", "J. Wang", "L.-C. Chen", "H. Gao", "W. Xu", "R. Nevatia"], "venue": "arXiv preprint arXiv:1511.05960,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "Advances in Neural Information Processing Systems, pages 2296\u20132304,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "A focused dynamic attention model for visual question answering", "author": ["I. Ilievski", "S. Yan", "J. Feng"], "venue": "arXiv preprint arXiv:1604.01485,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Answer-type prediction for visual question answering", "author": ["K. Kafle", "C. Kanan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4976\u2013 4984,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings  Figure 3. Some future work examples. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R.R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "NIPS, pages 3294\u20133302,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual question answering with question rep-  resentation update (qru)", "author": ["R. Li", "J. Jia"], "venue": "NIPS, pages 4655\u20134663,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV, pages 740\u2013755. Springer,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "NIPS, pages 289\u2013297,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["L. Ma", "Z. Lu", "H. Li"], "venue": "arXiv preprint arXiv:1506.00333,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["L. Ma", "Z. Lu", "H. Li"], "venue": "AAAI, page 16,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "Advances in Neural Information Processing Systems, pages 1682\u20131690,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1\u20139,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask your neurons: A deep learning approach to visual question answering", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, pages 3111\u20133119,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["H. Noh", "P. Hongsuck Seo", "B. Han"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 30\u2013 38,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "Advances in Neural Information Processing Systems, pages 2953\u20132961,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "Advances in Neural Information Processing Systems, pages 2953\u20132961,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4613\u20134621,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, pages 3104\u20133112,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156\u20133164,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask me anything: Free-form visual question answering  based on knowledge from external sources", "author": ["Q. Wu", "P. Wang", "C. Shen", "A. Dick", "A. van den Hengel"], "venue": "In CVPR,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "arXiv, 1603,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["H. Xu", "K. Saenko"], "venue": "ECCV, pages 451\u2013466. Springer,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "ICML, volume 14, pages 77\u201381,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A. Smola"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 21\u201329,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation", "author": ["J. Yao", "S. Fidler", "R. Urtasun"], "venue": "CVPR 2012, pages 702\u2013709. IEEE,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "arXiv preprint arXiv:1512.02167,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual7w: Grounded question answering in images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4995\u20135004,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Our method is evaluated on the challenging VQA dataset [1] and yields state-of-the-art accuracy, 60.", "startOffset": 55, "endOffset": 58}, {"referenceID": 33, "context": "Visual Question Answering (VQA) is a challenging and young research field, which can help machines achieve one of the ultimate goals in computer vision, holistic scene understanding [34].", "startOffset": 182, "endOffset": 186}, {"referenceID": 16, "context": "Since 2014, there has been a lot of progress in designing systems with the VQA ability [17, 1, 18, 24, 16, 6].", "startOffset": 87, "endOffset": 109}, {"referenceID": 0, "context": "Since 2014, there has been a lot of progress in designing systems with the VQA ability [17, 1, 18, 24, 16, 6].", "startOffset": 87, "endOffset": 109}, {"referenceID": 17, "context": "Since 2014, there has been a lot of progress in designing systems with the VQA ability [17, 1, 18, 24, 16, 6].", "startOffset": 87, "endOffset": 109}, {"referenceID": 23, "context": "Since 2014, there has been a lot of progress in designing systems with the VQA ability [17, 1, 18, 24, 16, 6].", "startOffset": 87, "endOffset": 109}, {"referenceID": 15, "context": "Since 2014, there has been a lot of progress in designing systems with the VQA ability [17, 1, 18, 24, 16, 6].", "startOffset": 87, "endOffset": 109}, {"referenceID": 5, "context": "Since 2014, there has been a lot of progress in designing systems with the VQA ability [17, 1, 18, 24, 16, 6].", "startOffset": 87, "endOffset": 109}, {"referenceID": 13, "context": "However, recently there are some works [14, 12] that try to do more effort on the question part.", "startOffset": 39, "endOffset": 47}, {"referenceID": 11, "context": "However, recently there are some works [14, 12] that try to do more effort on the question part.", "startOffset": 39, "endOffset": 47}, {"referenceID": 11, "context": "In [12], authors proposed a Question Representation Update (QRU) mechanism to up-", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In [14], the authors proposed a Co-Attention mechanism, jointly utilizing information about visual and question attention, for VQA and achieved the state-of-the-art accuracy.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "We take the query question, called the main question (MQ), encoded by Skip-Thought Vectors [11], as the input of Module 1.", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "In the Module 1, we encode all of the questions, also by Skip-Thought Vectors, from the training and validation sets of VQA [1] dataset as a 4800 by 215623 dimension basic question (BQ) matrix, and then solve the LASSO optimization problem, with MQ, to find the 3 BQ of MQ.", "startOffset": 124, "endOffset": 127}, {"referenceID": 13, "context": "In [14], they proposed a novel co-attention mechanism that jointly performs image-guided question attention and question-guided image attention for VQA.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "[14] also proposed a hierarchical architecture to represent the question, and construct image-question co-attention maps at the word level, phrase level and question level.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] is also a recent work focusing on the text-based question part, text feature.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "In [12], they presented a reasoning network to update the question representation iteratively after the question interacts with image content each time.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Both of [14, 12] yield better performance than previous works by doing more effort on the question part.", "startOffset": 8, "endOffset": 16}, {"referenceID": 11, "context": "Both of [14, 12] yield better performance than previous works by doing more effort on the question part.", "startOffset": 8, "endOffset": 16}, {"referenceID": 0, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 25, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 1, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 8, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 14, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 24, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 35, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 28, "context": "Recently, there are many papers [1, 26, 2, 9, 15, 25, 36, 29] have proposed methods to solve the VQA issue.", "startOffset": 32, "endOffset": 61}, {"referenceID": 6, "context": "Long Short Term Memory (LSTM) [7] is a particular variant of RNN and in natural language tasks, such as machine translation [27, 3], LSTM is a successful application.", "startOffset": 30, "endOffset": 33}, {"referenceID": 26, "context": "Long Short Term Memory (LSTM) [7] is a particular variant of RNN and in natural language tasks, such as machine translation [27, 3], LSTM is a successful application.", "startOffset": 124, "endOffset": 131}, {"referenceID": 2, "context": "Long Short Term Memory (LSTM) [7] is a particular variant of RNN and in natural language tasks, such as machine translation [27, 3], LSTM is a successful application.", "startOffset": 124, "endOffset": 131}, {"referenceID": 24, "context": "In [25], the authors exploit RNN and Convolutional Neural Network (CNN) to build a question generation algorithm, but the generated question sometimes has invalid grammar.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "The input in [18] is the concatenation of each word embedding with the same feature vector of image.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "[6] encodes the input question sentence by LSTM and join the image feature to the final output.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15] groups the neighbouring word and image features by doing convolution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "In [21], the question is encoded by Gated Recurrent Unit (GRU) [4] similar to LSTM and the authors also introduce a dynamic parameter layer in CNN whose weights are adaptively predicted by the encoded question feature.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "In [21], the question is encoded by Gated Recurrent Unit (GRU) [4] similar to LSTM and the authors also introduce a dynamic parameter layer in CNN whose weights are adaptively predicted by the encoded question feature.", "startOffset": 63, "endOffset": 66}, {"referenceID": 22, "context": "In order to analyze the relationship among words, phrases and sentences, several works, such as [23, 11, 20], proposed methods about how to map text into vector space.", "startOffset": 96, "endOffset": 108}, {"referenceID": 10, "context": "In order to analyze the relationship among words, phrases and sentences, several works, such as [23, 11, 20], proposed methods about how to map text into vector space.", "startOffset": 96, "endOffset": 108}, {"referenceID": 19, "context": "In order to analyze the relationship among words, phrases and sentences, several works, such as [23, 11, 20], proposed methods about how to map text into vector space.", "startOffset": 96, "endOffset": 108}, {"referenceID": 22, "context": "[23, 20] try to map words to vector space, and if the words share common contexts in the corpus, their encoded vectors will close to each other in the vector space.", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[23, 20] try to map words to vector space, and if the words share common contexts in the corpus, their encoded vectors will close to each other in the vector space.", "startOffset": 0, "endOffset": 8}, {"referenceID": 10, "context": "In [11], the authors propose a framework of encoder-decoder models, called skip-thoughts.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "In this model, the authors exploit an RNN encoder with GRU activations [4] and an RNN decoder with a conditional GRU [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "In this model, the authors exploit an RNN encoder with GRU activations [4] and an RNN decoder with a conditional GRU [4].", "startOffset": 117, "endOffset": 120}, {"referenceID": 31, "context": "In some sense, VQA is related to image captioning [32, 10, 28, 5].", "startOffset": 50, "endOffset": 65}, {"referenceID": 9, "context": "In some sense, VQA is related to image captioning [32, 10, 28, 5].", "startOffset": 50, "endOffset": 65}, {"referenceID": 27, "context": "In some sense, VQA is related to image captioning [32, 10, 28, 5].", "startOffset": 50, "endOffset": 65}, {"referenceID": 4, "context": "In some sense, VQA is related to image captioning [32, 10, 28, 5].", "startOffset": 50, "endOffset": 65}, {"referenceID": 4, "context": "[5] uses a language model to combine a set of possible words detected in several regions of the image and generate image description.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "In [28], the authors use CNN to extract the high-level image features and considered them as the first input of the recurrent network to generate the caption of image.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "[32] proposes an algorithm to generate one word at a time by paying attention to local image regions related to the currently predicted word.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "In [10], the deep neural network can learn to embed language and visual information into a common multi-modal space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "However, the current image captioning algorithms only can generate the rough description of image and there is no so called proper metric to evaluate the quality of image caption , even though BLEU [22] can be used to evaluate the image caption.", "startOffset": 198, "endOffset": 202}, {"referenceID": 25, "context": "There are several VQA models have ability to focus on specific image regions related to the input question by integrating the image attention mechanism [26, 2, 33, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 1, "context": "There are several VQA models have ability to focus on specific image regions related to the input question by integrating the image attention mechanism [26, 2, 33, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 32, "context": "There are several VQA models have ability to focus on specific image regions related to the input question by integrating the image attention mechanism [26, 2, 33, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 11, "context": "There are several VQA models have ability to focus on specific image regions related to the input question by integrating the image attention mechanism [26, 2, 33, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 11, "context": "In [12], in the pooling step, the authors exploit an image attention mechanism to help determine the relevance between original questions and updated ones.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Before [14], no work applied language attention mechanism to VQA, but the researchers in NLP they had modeled language attention.", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "In [14], the authors propose a co-attention mechanism that jointly performs language attention and image attention.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "All of our images are from the testing images of MS COCO dataset [13], the MQ, main questions, are from the testing questions of VQA, open-ended, dataset [1], the BQ, basic questions, are from the training and validation questions of VQA, open-ended, dataset [1], and the corresponding similarity score of BQ is generated by our basic question generation method, referring to Section 5.", "startOffset": 65, "endOffset": 69}, {"referenceID": 0, "context": "All of our images are from the testing images of MS COCO dataset [13], the MQ, main questions, are from the testing questions of VQA, open-ended, dataset [1], the BQ, basic questions, are from the training and validation questions of VQA, open-ended, dataset [1], and the corresponding similarity score of BQ is generated by our basic question generation method, referring to Section 5.", "startOffset": 154, "endOffset": 157}, {"referenceID": 0, "context": "All of our images are from the testing images of MS COCO dataset [13], the MQ, main questions, are from the testing questions of VQA, open-ended, dataset [1], the BQ, basic questions, are from the training and validation questions of VQA, open-ended, dataset [1], and the corresponding similarity score of BQ is generated by our basic question generation method, referring to Section 5.", "startOffset": 259, "endOffset": 262}, {"referenceID": 0, "context": "Moreover, we also take the multiplechoice questions in VQA dataset [1] to do the same thing as above.", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "Note that we remove the repeated questions in the VQA dataset, so the total number of questions is slightly less than VQA dataset [1].", "startOffset": 130, "endOffset": 133}, {"referenceID": 13, "context": "In Section 5, we mainly discuss how to encode questions and generate BQ and why we exploit the Co-Attention Mechanism VQA algorithm [14] to answer the query question.", "startOffset": 132, "endOffset": 136}, {"referenceID": 13, "context": "Then, the Module 2 is a VQA algorithm with the Co-Attention Mechanism [14], and it takes the output of Module 1, MQ, and the given image as input and then outputs the final answer of MQ.", "startOffset": 70, "endOffset": 74}, {"referenceID": 19, "context": "There are many popular text encoders, such as Word2Vec [20], GloVe [23] and Skip-Thoughts [11].", "startOffset": 55, "endOffset": 59}, {"referenceID": 22, "context": "There are many popular text encoders, such as Word2Vec [20], GloVe [23] and Skip-Thoughts [11].", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "There are many popular text encoders, such as Word2Vec [20], GloVe [23] and Skip-Thoughts [11].", "startOffset": 90, "endOffset": 94}, {"referenceID": 3, "context": "In Skip-Thoughts model, it uses an RNN encoder with GRU [4] activations, and then we use this encoder to map an English sentence into a vector.", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "Regarding GRU, it has been shown to perform as well as LSTM [7] on the sequence modeling applications but being conceptually simpler because GRU units only have 2 gates and do not need the use of a cell.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "2, we can encode the all questions from the training and validation questions of VQA dataset [1] by Skip-Thought Vectors, and then we have the matrix of these encoded basic questions.", "startOffset": 93, "endOffset": 96}, {"referenceID": 13, "context": "There are two types of Co-Attention Mechanism [14] , Parallel and Alternating.", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "In our VQABQ model, we only use the VQA algorithm with Alternating Co-Attention Mechanism to be our VQA module, referring to Figure 2, because, in [14], Alternating Co-Attention Mechanism VQA module can get the higher accuracy than the Parallel one.", "startOffset": 147, "endOffset": 151}, {"referenceID": 13, "context": "Moreover, we want to compare with the VQA method, Alternating one, with higher accuracy in [14].", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "We only show the open-ended case of VQA dataset [1], and \u201d# Q\u201d denoted number of questions.", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "We conduct our experiments on VQA [1] dataset.", "startOffset": 34, "endOffset": 37}, {"referenceID": 12, "context": "VQA dataset is based on the MS COCO dataset [13] and it contains the largest number of questions.", "startOffset": 44, "endOffset": 48}, {"referenceID": 13, "context": "In order to prove our claim that BQ can help accuracy and compare with the state-of-the-art VQA method [14], so, in our Module 2, we use the same setting, dataset and source code mentioned in [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 13, "context": "In order to prove our claim that BQ can help accuracy and compare with the state-of-the-art VQA method [14], so, in our Module 2, we use the same setting, dataset and source code mentioned in [14].", "startOffset": 192, "endOffset": 196}, {"referenceID": 13, "context": "In other words, in our model ,the only difference compared to [14] is our Module 1, illustrated by Figure 2.", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "LSTM Q+I [1] 36.", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "2 BOWIMG [1] 33.", "startOffset": 9, "endOffset": 12}, {"referenceID": 34, "context": "6 iBOWIMG [35] 35.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "9 DPPnet [21] 37.", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "4 FDA [8] 36.", "startOffset": 6, "endOffset": 9}, {"referenceID": 32, "context": "5 SAN [33] 36.", "startOffset": 6, "endOffset": 10}, {"referenceID": 30, "context": "9 SMem [31] 37.", "startOffset": 7, "endOffset": 11}, {"referenceID": 29, "context": "2 DMN+ [30] 36.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "4 Refined-Neurons [19] 36.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "4 QRU [12] 37.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "8 CoAtt+VGG [14] 38.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "5 CoAtt+ResNet [14] 38.", "startOffset": 15, "endOffset": 19}, {"referenceID": 0, "context": "Evaluation results on VQA dataset [1].", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "LSTM Q+I [8] 36.", "startOffset": 9, "endOffset": 12}, {"referenceID": 13, "context": "18 CoAtt+VGG [14] 38.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "Re-run evaluation results on VQA dataset [1].", "startOffset": 41, "endOffset": 44}, {"referenceID": 13, "context": "Note that the result of [14] in Table 5 is lower than in Table 4, and CoAtt+VGG is same as our VGGNet.", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "Although we only have 57% of testing questions can benefit from the basic questions, our method still can improve the state-of-theart accuracy [14] from 60.", "startOffset": 143, "endOffset": 147}, {"referenceID": 13, "context": "Recently, [14] proposed the Co-Attention Mechanism in VQA and got the state-of-the-art accuracy.", "startOffset": 10, "endOffset": 14}], "year": 2017, "abstractText": "Taking an image and question as the input of our method, it can output the text-based answer of the query question about the given image, so called Visual Question Answering (VQA). There are two main modules in our algorithm. Given a natural language question about an image, the first module takes the question as input and then outputs the basic questions of the main given question. The second module takes the main question, image and these basic questions as input and then outputs the text-based answer of the main question. We formulate the basic questions generation problem as a LASSO optimization problem, and also propose a criterion about how to exploit these basic questions to help answer main question. Our method is evaluated on the challenging VQA dataset [1] and yields state-of-the-art accuracy, 60.34% in open-ended task.", "creator": "LaTeX with hyperref package"}}}