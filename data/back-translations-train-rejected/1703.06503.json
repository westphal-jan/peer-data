{"id": "1703.06503", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2017", "title": "CLTune: A Generic Auto-Tuner for OpenCL Kernels", "abstract": "This work presents CLTune, an auto-tuner for OpenCL kernels. It evaluates and tunes kernel performance of a generic, user-defined search space of possible parameter-value combinations. Example parameters include the OpenCL workgroup size, vector data-types, tile sizes, and loop unrolling factors. CLTune can be used in the following scenarios: 1) when there are too many tunable parameters to explore manually, 2) when performance portability across OpenCL devices is desired, or 3) when the optimal parameters change based on input argument values (e.g. matrix dimensions). The auto-tuner is generic, easy to use, open-source, and supports multiple search strategies including simulated annealing and particle swarm optimisation. CLTune is evaluated on two GPU case-studies inspired by the recent successes in deep learning: 2D convolution and matrix-multiplication (GEMM). For 2D convolution, we demonstrate the need for auto-tuning by optimizing for different filter sizes, achieving performance on-par or better than the state-of-the-art. For matrix-multiplication, we use CLTune to explore a parameter space of more than two-hundred thousand configurations, we show the need for device-specific tuning, and outperform the clBLAS library on NVIDIA, AMD and Intel GPUs.", "histories": [["v1", "Sun, 19 Mar 2017 20:10:00 GMT  (221kb,D)", "http://arxiv.org/abs/1703.06503v1", "8 pages, published in MCSoC '15, IEEE 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC), 2015"]], "COMMENTS": "8 pages, published in MCSoC '15, IEEE 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC), 2015", "reviews": [], "SUBJECTS": "cs.PF cs.AI cs.DC", "authors": ["cedric nugteren", "valeriu codreanu"], "accepted": false, "id": "1703.06503"}, "pdf": {"name": "1703.06503.pdf", "metadata": {"source": "CRF", "title": "CLTune: A Generic Auto-Tuner for OpenCL Kernels", "authors": ["Cedric Nugteren", "Valeriu Codreanu"], "emails": ["mail@cedricnugteren.nl", "valeriu.codreanu@surfsara.nl"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. RELATED WORK", "text": "In the first category of related work, we will discuss cases where autotunners have been introduced to solve a specific OpenCL or GPU-related problem. Examples are autotunners for folding [21], sparse matrix vector multiplications [5], dense matrix multiplications [14], [16] and FFTs [15]. Unlike CLTune, such tuners are tailored to a specific problem and often do not apply beyond these problems. A more general OpenCL autotunner is Maestro Data-Orchestration Tuner [20], but it is orthogonal for this work as it works on data transfers and not on kernels. We are not aware of any existing generic tuners for OpenCL cores (although there are existing tuners that address GPUs but work at a higher level than OpenCL part)."}, {"heading": "III. CLTUNE: AN AUTO-TUNER FOR OPENCL KERNELS", "text": "CLTune is an autotunner written in C + + 11 that targets the OpenCL kernel. It is open source and freely available under the APACHE 2.0 license on GitHub1, which includes the two case studies of this work as sample codes. It is implemented as a library with a C + + API and can therefore be used independently for offline tuning (compilation time) or integrated into existing source code for on-line tuning (runtime).The API is designed to hide all OpenCL API calls from the user, i.e. CLTune takes care of tasks such as OpenCL device initialization, kernel call, and device memory management. To illustrate the usability of the tuner, we briefly discuss a simple example. Consider the example OpenCL kernel at the top of Fig. 1, which uses the WPT parameter to vary the value per thread."}, {"heading": "A. Advanced usage", "text": "As we have seen, using CLTune can be quite simple. Nevertheless, the tuner is also equipped with advanced features introduced in this section: 1) Manipulation 1CLTune on GitHub: https: / / github.com / CNugteren / CLTunedes Thread and Working Group Configuration, 2) Parameter Limitations and 3) Result Verification. The copy example of Fig. 1 could be expanded by adding tuning the local workgroup size, which is just as easy as introducing a new WG parameter with values of, for example, {32, 64, 128, 256}, changing the old 64 in 1 workgroup size, and using MulLocalSize ({\"WG\"}) to multiply 1 by WG. The reason that the thread dimensions in the example code are within parentheses is the following: The example uses a 1D arrangement, but they can be expanded to 2D or 3D.Users of the Auto Tuner."}, {"heading": "B. Search strategies and search-space properties", "text": "In this case, this is the case if the search result space is too large, but also if it is limited by the repetition of the search result process. Three alternatives to the search result process are available: searching for the search result credential, searching for the search result credential, searching for the search result credential, searching for the search result credential, searching for the search result credential, searching for the search result credential, searching for the search result credential, searching for the search result credential, searching for the search result credential, searching for the search result credential, searching for the search result credential, searching for the search result credential and searching for the search result credential."}, {"heading": "C. Simulated annealing", "text": "Simulated annealing (SA) is a heuristic search method inspired by metallurgical annealing [9] that moves iteratively through the search space from neighbor to neighbor and ends after a fixed number of iterations or upon reaching a certain criterion. In principle, the simulated annealing only moves to neighbors with better performance (lower energy when annealing), but in order to avoid getting stuck in a local optimum, the heurist has a certain probability of taking a step toward a worse configuration. This probability decreases over time as the annealing temperature drops: at the end of the search, the probability of moving toward the global optimum is higher than at the beginning. This probability continues to decrease depending on the performance difference between the current and a neighboring configuration. The simulated annealing of heuristics is implemented in CLTune. The search is initialized in a random configuration and continues until a defined number of configurations have been researched."}, {"heading": "D. Particle swarm optimisation", "text": "Particle swarm optimization (PSO) is an evolutionary search strategy in which a swarm of S communicating particles explores the search space [7]. These particles are randomly positioned at first and receive a random velocity. At each step t, the new positions xt + 1i of each particle i are calculated based on their current position xti and their velocity v t i. These velocities are in turn updated depending on their current velocities, their best configuration pti to date, and the overall best known configuration gt. A variant of the PSO is accelerated PSO [22], where the new position in each dimension d is calculated directly on the old position, removing the concept of velocity: xt + 1i, d = \u03b1 d + \u03b2p t i, d + d + (1 \u2212 \u03b2 \u2212 GP) xgtti, in which it is probable and CLwith a random number within the given range d."}, {"heading": "IV. EXPERIMENTAL SETUP", "text": "In sections V and VI, we will conduct experiments with OpenCL GPUs from various manufacturers, selected for their great architectural diversity, these devices and their relevant properties are listed in Table I. We plan to extend our experiments to non-GPU OpenCL devices (CPUs, MIC) in the future. All data types in this essay are floating-point numbers with uniform precision. In addition, simulated annealing is configured with a variable temperature T = {2, 4, 6}, and PSO is configured with \u03b1 = 0.4, \u03b2 = 0 (not a locally best influence as claimed in [22]), \u03b3 = 0.4, and a variable swarm size S = {3, 6}."}, {"heading": "V. CASE-STUDY: 2D CONVOLUTION", "text": "Our first case study is 2D folding, which is important, for example, for image filters such as blur or edge detection. This case study is motivated, on the one hand, by the conceptual simplicity of folding, and, on the other, by the renewed interest in GPU-based folding in the context of Convolutionary Neural Networks for Deep Learning. Folding is used in production as well as research tools and libraries such as Theano [1], Caffe [6], cuDNN, Torch7, and cuda-convennet [11]. In Convolutionary Neural Networks, many 2D folds need to be performed, typically with a bandwidth of fixed filter sizes ranging from 3 x 3 to 11 x 11. An auto-tuner can be used to optimize the folding code for each specific filter size, as happens with Theano, for example."}, {"heading": "A. Tuning parameters", "text": "We implemented a highly customizable implementation of 2D convolution in OpenCL, inspired by [18] and [21]. Details of our implementation are not the focus of this work, but the source is available for inspection in the context of the CLTune examples and further discussions can be found in related work [21]. We will discuss the selected tuning parameters: \u2022 The total amount of work is configurable in both dimensions, resulting in a rectangular thread block of Ywg size Xwg, each of which can be tuned individually, as shown in Fig. 3. \u2022 The workload per thread Xwpt and Ywpt is configurable in both dimensions, resulting in a rectangular thread block of loads, calculations and memory per thread. Advantages could come from improved data locality and reuse of filter coefficients and index variables. \u2022 The parameter L $implements 3 caching strategies: no hardware-based caching only)."}, {"heading": "B. Evaluation of the search strategies", "text": "Before evaluating the performance of the best results found for 2D confrontation, we analyze the search strategies of CLTune. Within a search space of 3424 permutations (see Table II for values), random search, random annexation and PSO, the individual configurations are configured. We use an image of 8192 by 4096 and a filter of 7x7 (later also 3x3 and 11x11).To illustrate the behavior of the search strategies, we present the search progress on a single device in the figure. On the left, we see the traces of 3 runs (in different colors) of the random strategy."}, {"heading": "C. Best-found results", "text": "The tuning parameters and their possible values are set out on the left side in Table II. The right side provides the best-found results for 3 different filter sizes and for two devices. These results once again demonstrate the usefulness of auto-tuning: the optimal parameters vary between different filter sizes and between different devices, even those from the same endorse.To show that our 2D folding case study is realistic and useful, we also examine the absolute performance and compare it against related work. Figure 6 first shows the performance in relation to the theoretical architectural peak values in both GFLOPS and memory bandwidth2. We observe that our implementation follows the mathematical property of the convolution that it becomes more energy-intensive as the filter sizes are increased. We also point out that the GTX480 and HD7970 work relatively better than the K40m due to their better balanced architecture. The Iris GPU is limited by its small memory width for 3x11."}, {"heading": "VI. CASE-STUDY: MATRIX-MULTIPLICATION", "text": "Our second case study concerns generalized matrix multiplication (GEMM) as found in the linear BLAS algebra libraries. This case study is partly motivated by the same reasons as the 2D convolution: Matrix multiplication is one of the key components of deep learning and other machine learning algorithms, and is also a popular target for autotuning due to its broad applicability and FLOP-heavy computation. Some recent OpenCL autotuning work on GEMM is [14], [16] and the clBLAS library. In this case study, we also consider the multiplication of two matrices A and B accordingly: C = \u03b1ATB + \u03b2Cin the \u03b1 and \u03b2 constants, and AT is the transposed version of A. Apart from the expectation of transposed input, we also assume that the matrix dimensions are multiples (see 2 and multiples below)."}, {"heading": "A. Tuning parameters", "text": "We have implemented a highly tunable parallel version of matrix multiplication in OpenCL, inspired by [16] and the clBLAS library. Where possible, we use the same parameter names as in [16]. The tuning parameters are shown in Fig. 8 and described below: 1) The local workgroup size is tunable at workgroup level with the parameters Mwg, Nwg, Kwg according to the M, N and K matrix dimensions. 2) The local workgroup size is enforceable in 2 dimensions: MdimC and NdimC. Combined with the two corresponding tile parameters, this also defines the amount of work per thread in the M and N dimensions: Mwi = Mwg / MdimC and Nwi = NdimC."}, {"heading": "B. Evaluation of the search strategies", "text": "Regarding 2D convolution, we first evaluate the search strategies of CLTune. We use the same devices as shown in Table I and use the same configurations of the same set of search strategies. For matrix multiplication, our search space consists of 241,600 unique configurations: 71 times larger than for 2D convolution. The values tested are on the left in Table IV. To keep our search experiments comparable to 2D convolution in terms of runtime, we decide to examine only 1 / 2048 of the search space: 117 configurations. Our experiments take into account square matrices of 2048 times 2048 (M = N = K = 2048).As before, we evaluate the search strategies by running each search 128 times. The infringement charts will be shown in Fig. 7. In addition to the observations for the conversion, we conclude that it is easier to find a good configuration compared to 2D configuration by showing the average performance of the search strategy in Figure 7."}, {"heading": "C. Best-found results", "text": "The best results are compared with the theoretical peak capabilities of the devices and with two libraries: cuBLAS 7.0 for K40m and 5.5 for GTX480 and clBLAS2.4.0 after running the included tuner. The results are shown in Fig. 9, which shows that our matrix multiplication exceeds the clBLAS library in all cases, including the AMD GPU for which clBLAS was originally designed. We are not able to compare cuBLAS on the K40m, as it uses assembly-level optimizations to reduce register pressure and eliminate register bank conflicts [12]. Furthermore, we are not able to use CUDA's ldg instruction. To demonstrate the merits of tuning for a specific device, we are roughly comparing the results of the GTIV 79% table of available 440M parameters."}, {"heading": "VII. CONCLUSIONS", "text": "This work introduced the CLTune autotunner for OpenCL kernels. We saw that the generic and open source CLTune is easy to use and can be used for offline or online tuning. Based on two case studies and an evaluation on 4 different devices (Tesla K40m, GeForce GTX480, Radeon HD7970, Iris 5100), we conclude that the two advanced search strategies that simulate the glow and optimization of particle swarms both have their merits, although their effectiveness depends on the problem at hand. Furthermore, we demonstrated the merits of CLTune by 1) exploring a search space of more than two hundred thousand configurations, 2) identifying significant differences between the best-case parameters between devices and 3) showing that an OpenCL 2D configuration code can be optimized to enter arguments."}], "references": [{"title": "et al", "author": ["J. Bergstra", "F. Bastien", "O. Breuleux", "P. Lamblin", "R. Pascanu", "O. Delalleau", "G. Desjardins", "D. Warde-Farley", "I. Goodfellow", "A. Bergeron"], "venue": "Theano: Deep Learning on GPUs with Python. In NIPS \u201911: Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "From CUDA to OpenCL: Towards a Performance-Portable Solution for Multi-Platform GPU Programming", "author": ["P. Du", "R. Weber", "P. Luszczek", "S. Tomov", "G. Peterson", "J. Dongarra"], "venue": "Parallel Computing, 38(8):391 \u2013 407", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards a Tunable Multi-Backend Skeleton Programming Framework for Multi-GPU Systems", "author": ["J. Enmyren", "C. Kessler"], "venue": "MCC-3: Swedish Workshop on Multicore Computing", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Auto-tuning a High-Level Language Targeted to GPU Codes", "author": ["S. Grauer-Gray", "L. Xu", "R. Searles", "S. Ayalasomayajula", "J. Cavazos"], "venue": "INPAR: Workshop on Innovative Parallel Computing", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatically Generating and Tuning GPU Code for Sparse Matrix-Vector Multiplication from a High- Level Representation", "author": ["D. Grewe", "A. Lokhmotov"], "venue": "GPGPU-4: General Purpose Processing on Graphics Processing Units. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "MM \u201914: Int. Conf. on Mult. ACM", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Particle Swarm Optimization", "author": ["J. Kennedy", "R. Eberhart"], "venue": "International Conference on Neural Networks. IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1995}, {"title": "A Script- Based Autotuning Compiler System to Generate High-Performance CUDA Code", "author": ["M. Khan", "P. Basu", "G. Rudy", "M. Hall", "C. Chen", "J. Chame"], "venue": "ACM TACO,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Optimization by Simulated Annealing", "author": ["S. Kirkpatrick", "C.D. Gelatt", "M.P. Vecchi"], "venue": "Science, 220(4598):671\u2013680", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1983}, {"title": "Optimization by Direct Search: New Perspectives on some Classical and Modern Methods", "author": ["T.G. Kolda", "R.M. Lewis", "V. Torczon"], "venue": "SIAM review, 45(3):385\u2013482", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Imagenet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS \u201912: Advances in Neural Information Processing Systems, pages 1097\u20131105", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Performance Upper Bound Analysis and Optimization of SGEMM on Fermi and Kepler GPUs", "author": ["J. Lai", "A. Seznec"], "venue": "CGO \u201913: Code Generation and Optimization. IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "OpenMPC: Extended OpenMP Programming and Tuning for GPUs", "author": ["S. Lee", "R. Eigenmann"], "venue": "SC: Int. Conf. on High Performance Computing Networking, Storage and Analysis. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "A Note on Auto-tuning GEMM for GPUs", "author": ["Y. Li", "J. Dongarra", "S. Tomov"], "venue": "ICCS: Int. Conf. on Computational Science. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "MPFFT: An Auto-Tuning FFT Library for OpenCL GPUs", "author": ["Y. Li", "Y.-Q. Zhang", "Y.-Q. Liu", "G.-P. Long", "H.-P. Jia"], "venue": "Journal of Computer Science and Technology, 28(1):90\u2013105", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Performance Tuning of Matrix Multiplication in OpenCL on Different GPUs and CPUs", "author": ["K. Matsumoto", "N. Nakasato", "S. Sedukhin"], "venue": "SC Companion. IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Implementing Level-3 BLAS Routines in OpenCL on Different Processing Units", "author": ["K. Matsumoto", "N. Nakasato", "S. Sedukhin"], "venue": "Technical Report TR 2014-001, The University of Aizu", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Image Convolution with CUDA", "author": ["V. Podlozhnyuk"], "venue": "Technical report, NVIDIA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Convolution Engine: Balancing Efficiency & Flexibility in Specialized Computing", "author": ["W. Qadeer", "R. Hameed", "O. Shacham", "P. Venkatesan", "C. Kozyrakis", "M.A. Horowitz"], "venue": "ISCA-40: International Symposium on Computer Architecture. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Maestro: Data Orchestration and Tuning for OpenCL Devices", "author": ["K. Spafford", "J. Meredith", "J. Vetter"], "venue": "Euro-Par \u201910. Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimizing Convolution Operations on GPUs Using Adaptive Tiling", "author": ["B. Van Werkhoven", "J. Maassen", "H.E. Bal", "F.J. Seinstra"], "venue": "Future Gener. Comput. Syst.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Accelerated Particle Swarm Optimization and Support Vector Machine for Business Optimization and Applications", "author": ["X.-S. Yang", "S. Deb", "S. Fong"], "venue": "Communications in Computer and Information Science, 136:53\u201366", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "Despite these advancements, achieving close-to-peak performance remains a task for expert programmers in many cases [2].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "Although OpenCL code is portable across devices, it is definitely not performance portable: to achieve good performance it is necessary to tune design parameters, adjust the hierarchy of parallelism, and explore different algorithms [2].", "startOffset": 233, "endOffset": 236}, {"referenceID": 1, "context": "However, beyond that, even for devices coming from the same vendor or with the same architectural family, it might be worthwhile to explore workgroup configurations, loop unroll factors, or vector widths [2].", "startOffset": 204, "endOffset": 207}, {"referenceID": 14, "context": "[15]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Second, we present two case-studies inspired by the recent successes with convolutional neural networks for deep-learning on GPUs [6], [11]: 2D convolution and matrix-multiplication (GEMM).", "startOffset": 130, "endOffset": 133}, {"referenceID": 10, "context": "Second, we present two case-studies inspired by the recent successes with convolutional neural networks for deep-learning on GPUs [6], [11]: 2D convolution and matrix-multiplication (GEMM).", "startOffset": 135, "endOffset": 139}, {"referenceID": 20, "context": "For 2D convolution, our tuner matches the CUDAbased state-of-the-art [21] and achieves up to 1658 GFLOPS on a 11x11 filter and 207GB/s on a 3x3 filter running on an AMD HD7970 GPU.", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "We also tune matrix-multiplication, matching the stateof-the-art [17], and achieving better performance compared to the clBLAS library.", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "Examples are auto-tuners for convolution [21], sparse matrixvector multiplications [5], dense matrix-matrix multiplications [14], [16], and FFTs [15].", "startOffset": 41, "endOffset": 45}, {"referenceID": 4, "context": "Examples are auto-tuners for convolution [21], sparse matrixvector multiplications [5], dense matrix-matrix multiplications [14], [16], and FFTs [15].", "startOffset": 83, "endOffset": 86}, {"referenceID": 13, "context": "Examples are auto-tuners for convolution [21], sparse matrixvector multiplications [5], dense matrix-matrix multiplications [14], [16], and FFTs [15].", "startOffset": 124, "endOffset": 128}, {"referenceID": 15, "context": "Examples are auto-tuners for convolution [21], sparse matrixvector multiplications [5], dense matrix-matrix multiplications [14], [16], and FFTs [15].", "startOffset": 130, "endOffset": 134}, {"referenceID": 14, "context": "Examples are auto-tuners for convolution [21], sparse matrixvector multiplications [5], dense matrix-matrix multiplications [14], [16], and FFTs [15].", "startOffset": 145, "endOffset": 149}, {"referenceID": 19, "context": "A more generic OpenCL auto-tuner is Maestro data-orchestration tuner [20], however it is orthogonal to this work since it works on data transfers rather than kernel.", "startOffset": 69, "endOffset": 73}, {"referenceID": 2, "context": "Examples include tuners for a GPU skeleton programming framework [3], as part of a parallelizing compiler [8], for OpenMP 4.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "Examples include tuners for a GPU skeleton programming framework [3], as part of a parallelizing compiler [8], for OpenMP 4.", "startOffset": 106, "endOffset": 109}, {"referenceID": 12, "context": "0 directives [13], for HMPP directives [4], and for mathematical expressions in Theano [1].", "startOffset": 13, "endOffset": 17}, {"referenceID": 3, "context": "0 directives [13], for HMPP directives [4], and for mathematical expressions in Theano [1].", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "0 directives [13], for HMPP directives [4], and for mathematical expressions in Theano [1].", "startOffset": 87, "endOffset": 90}, {"referenceID": 9, "context": "Even derivative free methods such as direct search [10] are not suitable, since they assume that it is relatively cheap to explore all neighbours of a particular configuration.", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "Simulated annealing Simulated annealing (SA) is a heuristic search method inspired by annealing in metallurgy [9] which iteratively moves through the search-space from neighbour to neighbour and ends after a fixed number of iterations or when a certain criterion is reached.", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "Particle swarm optimisation Particle swarm optimisation (PSO) is an evolutionary search strategy in which a swarm of S communicating particles explore the search-space [7].", "startOffset": 168, "endOffset": 171}, {"referenceID": 21, "context": "A variant of PSO is accelerated PSO [22], in which the new position in each dimension d is directly calculated based on the old position, removing the concept of velocity: x i,d = \u03b1 d + \u03b2p t i,d + \u03b3g t d + (1\u2212 \u03b1\u2212 \u03b2 \u2212 \u03b3)xi,d in which \u03b1, \u03b2 and \u03b3 are probability parameters and d represents a random number within the range of the parameter in dimension d.", "startOffset": 36, "endOffset": 40}, {"referenceID": 21, "context": "4, \u03b2 = 0 (no local-best influence as argued by [22]), \u03b3 = 0.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "Convolution is used in production as well as research tools and libraries, such as Theano [1], Caffe [6], cuDNN, Torch7, and cuda-convnet [11].", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "Convolution is used in production as well as research tools and libraries, such as Theano [1], Caffe [6], cuDNN, Torch7, and cuda-convnet [11].", "startOffset": 101, "endOffset": 104}, {"referenceID": 10, "context": "Convolution is used in production as well as research tools and libraries, such as Theano [1], Caffe [6], cuDNN, Torch7, and cuda-convnet [11].", "startOffset": 138, "endOffset": 142}, {"referenceID": 18, "context": "Convolution is also important outside the context of deep-learning, as is discussed in [19], in which it is also shown that convolution can be generalized to other image and video processing operations such as SAD and SIFT.", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "Tuning parameters We implemented a highly tunable implementation of 2D convolution in OpenCL inspired by [18] and [21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 20, "context": "Tuning parameters We implemented a highly tunable implementation of 2D convolution in OpenCL inspired by [18] and [21].", "startOffset": 114, "endOffset": 118}, {"referenceID": 20, "context": "Details of our implementation are not the focus of this work, but the source is available for inspection as part of the CLTune examples and further discussion can be found in related work [21].", "startOffset": 188, "endOffset": 192}, {"referenceID": 20, "context": "The state-of-the-art in 2D convolution uses adaptive-tiling, a form of auto-tuning [21].", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "An alternative to 2D convolution is an FFT in frequency space, but as discussed in [21], 2D convolution is significantly faster for filter sizes below 19 by 19 compared to the highly optimized cuFFT library.", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": "Some examples of recent OpenCL auto-tuning work on GEMM are [14], [16] and the clBLAS library.", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "Some examples of recent OpenCL auto-tuning work on GEMM are [14], [16] and the clBLAS library.", "startOffset": 66, "endOffset": 70}, {"referenceID": 15, "context": "These assumptions can be resolved by a relatively-cheap pre-processing kernel, as is also suggested in [16].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "Tuning parameters We implemented a highly tunable parallel version of matrix-multiplication in OpenCL, inspired by [16] and the clBLAS library.", "startOffset": 115, "endOffset": 119}, {"referenceID": 15, "context": "As far as possible, we use the same parameter names as in [16].", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "We are not able to match cuBLAS on the K40m, as it uses assembly-level optimisations to reduce register pressure and remove registerbank conflicts [12].", "startOffset": 147, "endOffset": 151}, {"referenceID": 15, "context": "[16], [17]: they reach 2913 GFLOPS on a 18% lower clocked HD7970.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[16], [17]: they reach 2913 GFLOPS on a 18% lower clocked HD7970.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "0 by using assembly-level optimisations for NVIDIA GPUs [12], which are now integrated in the tested versions of cuBLAS.", "startOffset": 56, "endOffset": 60}], "year": 2017, "abstractText": "This work presents CLTune, an auto-tuner for OpenCL kernels. It evaluates and tunes kernel performance of a generic, user-defined search space of possible parametervalue combinations. Example parameters include the OpenCL workgroup size, vector data-types, tile sizes, and loop unrolling factors. CLTune can be used in the following scenarios: 1) when there are too many tunable parameters to explore manually, 2) when performance portability across OpenCL devices is desired, or 3) when the optimal parameters change based on input argument values (e.g. matrix dimensions). The auto-tuner is generic, easy to use, open-source, and supports multiple search strategies including simulated annealing and particle swarm optimisation. CLTune is evaluated on two GPU case-studies inspired by the recent successes in deep learning: 2D convolution and matrixmultiplication (GEMM). For 2D convolution, we demonstrate the need for auto-tuning by optimizing for different filter sizes, achieving performance on-par or better than the state-of-the-art. For matrix-multiplication, we use CLTune to explore a parameter space of more than two-hundred thousand configurations, we show the need for device-specific tuning, and outperform the clBLAS library on NVIDIA, AMD and Intel GPUs.", "creator": "LaTeX with hyperref package"}}}