{"id": "1703.06514", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2017", "title": "Recurrent Collective Classification", "abstract": "We propose a new method for training iterative collective classifiers for labeling nodes in network data. The iterative classification algorithm (ICA) is a canonical method for incorporating relational information into classification. Yet, existing methods for training ICA models rely on the assumption that relational features reflect the true labels of the nodes. This unrealistic assumption introduces a bias that is inconsistent with the actual prediction algorithm. In this paper, we introduce recurrent collective classification (RCC), a variant of ICA analogous to recurrent neural network prediction. RCC accommodates any differentiable local classifier and relational feature functions. We provide gradient-based strategies for optimizing over model parameters to more directly minimize the loss function. In our experiments, this direct loss minimization translates to improved accuracy and robustness on real network data. We demonstrate the robustness of RCC in settings where local classification is very noisy, settings that are particularly challenging for ICA.", "histories": [["v1", "Sun, 19 Mar 2017 21:19:04 GMT  (389kb,D)", "http://arxiv.org/abs/1703.06514v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuangfei fan", "bert huang"], "accepted": false, "id": "1703.06514"}, "pdf": {"name": "1703.06514.pdf", "metadata": {"source": "CRF", "title": "Recurrent Collective Classification", "authors": ["Shuangfei Fan", "Bert Huang"], "emails": [], "sections": [{"heading": null, "text": "We propose a new method for building iterative collective classifiers to identify nodes in network data. The iterative classification algorithm (ICA) is a canonical method for incorporating relational information into classification. However, existing methods for building ICA models are based on the assumption that the relational characteristics reflect the true names of the nodes. This unrealistic assumption leads to a distortion that is inconsistent with the actual prediction algorithm. In this paper, we introduce recurrent collective classification (RCC), a variant of ICA that is analogous to recursive prediction of neural networks. RCC takes into account any differentiable local classification and relational feature functions. We offer gradient-based strategies for optimizing using model parameters to minimize the loss function more directly. In our experiments, this direct loss minimization leads to improved accuracy and robustness based on network data, which we demonstrate is very local in RCC classification."}, {"heading": "1 Introduction", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "2 Related Work", "text": "One of the basic tasks in the analysis of network data (Getoor and Diehl, 2005; Lindamood et al., 2009) is that collective classification addresses this task by making common classifications of interconnected nodes (Kong et al., 2011; Taskar et al., 2002; Neville and Jensen, 2003).Gibbs sampling (GS) is another approach to collective classifications, using the iterative classification system (McDowell et al., 2004; Sen et al., 2008), which introduces randomization into iterative classification. ICA and GS have repeatedly demonstrated that effective frameworks are used for collective classification (Sen et al., 2008; Jensen et al., 2003; Lu and Getoor, 2003; McDowell et al.)."}, {"heading": "3 Iterative Classification", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in"}, {"heading": "4 Recurrent Collective Classification", "text": "The RCC framework calculates derivatives for the levels of collective classification and provides a general scheme for calculating the progression of the loss function in relation to the classification parameters. At test time, a collective classification parameter is often given only local characteristics of nodes connected in a network. Thus, all relational characteristics are derived from predicted neighbor labels. A neighbor label should take into account common patterns of misclassification of neighbor labels. Unlike the ICA training processes that invite the classifiers to rely on the reliability of neighbor els.Prediction in RCC is analogous to ICA values."}, {"heading": "4.1 Derivative Structure", "text": "Key aspects in understanding the efficient computability of these gradients are the independence relations between the input and output variables. These independence relations are universal for each local classifier and for each relational feature. Since the local classifier operates independently of each node, there is no dependence in any iteration between the classification of a node and the relational characteristics of each other node. Since the relational characteristics are entirely determined by the neighbors of each particular node, there is only a dependence between the predictions of any node j \u2212 \u2212 and the relational characteristic of the node if they are neighbors in the graph. The matrix derivative of the relational function i (source row pi.Since relational characteristics are entirely determined by the neighbors of each particular node, there is only one dependence between the predictions of any node j \u2212 and the relational characteristic of the node if it has neighbors in the graph function of the output line."}, {"heading": "4.2 Example Local Classifiers", "text": "One advantage of ICA is that it can use all local classifiers and relational characteristics. Similarly, given Section 4.1, we can also use many local classifiers f and relational characteristics g in RCC. In this section, we will give some examples of these configurations.First, we will consider linear classifiers with activation functions, where the linear product forecasts are squeezed by various activation functions. A common activation function is the logistic sigmoid function: f (xi, ri (t \u2212 1); also, we will consider linear classifiers with activation functions, where the linear product forecasts (\u2212 1) are squeezed by various activation functions. (7) A common activation function is the logistic sigmoid function: f (xi, ri \u2212) to specify the horizontal concatenation of the row vectors xi and ri."}, {"heading": "4.3 Example Relational Features", "text": "There are many aggregation operators that can be used to define relational characteristics. Frequently used characteristics include the sum of probabilities for each class, the proportion of each class in the neighborhood, the mode that is and exists as the class label with the highest probability among neighbors, which is an indicator for each class name (Sen et al., 2008). The choice which of these to use depends on the application. Here, we will discuss three of them and provide their derivatives. The sum aggregation function g can be written as ri = g (P (t); ai) = a > i P (t), where ai is the adjacency vector of the node i. The Jacobic of the sum characteristic is g \u2032 i (p (t) j) = aijIk, where Ik is the identity matrix. The proportional operator is similar to the sum (except that we scale the adjacency matrix A by normalizing each line vector, where A is an A = an operator."}, {"heading": "4.4 Computational Complexity", "text": "The prediction in RCC (and ICA) therefore requires the calculation of relational characteristics and the prediction of T-times. For most relational characteristics, each node must perform O (k) work per neighbor, which is equivalent to a total calculation of O (| E | k). Assuming a constant number of relational characteristics, the linear local classifier O (d + k) does the work. Therefore, any prediction conductor O (| E | k + d) requires time, and the complete prediction requires O (E | k + d) time. Gradient-based optimization of the RCC objective function is a non-convex program, so the total number of iterations for learning is not trivial to analyze in general. However, we can analyze the computational complexity of each gradient calculation. Calculation of the gradient effect requires three phases: the calculation of the classifier and relative characteristic derivatives, the formula."}, {"heading": "5 Experiments", "text": "In this section, we describe experiments that test whether the proposed training method for RCC is capable of improving the existing methods of training. We examine scenarios in which the local classifier produces inaccurate predictions and the erroneous assumptions implied by the training of ICA and Gibbs, with their relational characteristics calculated on the basis of the actual labels. Results show that our hypothesis is correct and identifies a variety of settings in which RCC better optimizes the training goal and produces more accurate predictions about data held. We experimented with four data sets, two bibliographic data sets, a social network and an image dataset. The Cora dataset is a collection of 2,708 machine learning publications that are ratified into seven classes (Sen et al, 2008). Each publication in the datasets is described by a 0 / 1-rated word vector that indicates the absence / presence of the corresponding word book from the dictionary."}, {"heading": "6 Conclusion and Discussion", "text": "We have introduced recurrent collective classification, a variant of the iterative classification algorithm that uses differentiated operations that allow error gradients to be backpropagated to directly optimize model parameters. Collective classification has long been understood as a principal approach to classifying nodes in networks, but in practice often suffers from small improvements or no improvements at all. One reason for this may be the flawed training method that we correct in this paper. Our experiments show dramatic improvements in training accuracy that translate into significant but less dramatic improvements in test performance. RCC is an important step toward fully realizing the power of collective classification, so one important aspect to further improve the effectiveness of collective classifiers is the generalization behavior of collective models. A future direction of research is to investigate how more direct training loss minimization interacts with known generalization analyses, which may lead to further algorithm improvements."}], "references": [{"title": "An evolutionary algorithm that constructs recurrent neural networks", "author": ["Peter J Angeline", "Gregory M Saunders", "Jordan B Pollack"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Angeline et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Angeline et al\\.", "year": 1994}, {"title": "Combining collective classification and link prediction", "author": ["Mustafa Bilgic", "Galileo Mark Namata", "Lise Getoor"], "venue": "In Data Mining Workshops,", "citeRegEx": "Bilgic et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bilgic et al\\.", "year": 2007}, {"title": "Learning to segment", "author": ["Eran Borenstein", "Shimon Ullman"], "venue": "In European conference on computer vision,", "citeRegEx": "Borenstein and Ullman.,? \\Q2004\\E", "shortCiteRegEx": "Borenstein and Ullman.", "year": 2004}, {"title": "Recurrent neural networks and robust time series prediction", "author": ["Jerome T Connor", "R Douglas Martin", "Les E Atlas"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Connor et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Connor et al\\.", "year": 1994}, {"title": "Learning graphical model parameters with approximate marginal inference", "author": ["Justin Domke"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Domke.,? \\Q2013\\E", "shortCiteRegEx": "Domke.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Training iterative collective classifiers with back-propagation", "author": ["Shuangfei Fan", "Bert Huang"], "venue": "In 12th International Workshop on Mining and Learning with Graphs, San Francisco,", "citeRegEx": "Fan and Huang.,? \\Q2016\\E", "shortCiteRegEx": "Fan and Huang.", "year": 2016}, {"title": "Link mining: a survey", "author": ["Lise Getoor", "Christopher P Diehl"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Getoor and Diehl.,? \\Q2005\\E", "shortCiteRegEx": "Getoor and Diehl.", "year": 2005}, {"title": "Supervised sequence labelling", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Graves and Jaitly.,? \\Q2014\\E", "shortCiteRegEx": "Graves and Jaitly.", "year": 2014}, {"title": "Gradient flow in recurrent nets: the difficulty of learning", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "J\u00fcrgen Schmidhuber"], "venue": "long-term dependencies,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Structural learning with forgetting", "author": ["Masumi Ishikawa"], "venue": "Neural Networks,", "citeRegEx": "Ishikawa.,? \\Q1996\\E", "shortCiteRegEx": "Ishikawa.", "year": 1996}, {"title": "Why collective inference improves relational classification", "author": ["David Jensen", "Jennifer Neville", "Brian Gallagher"], "venue": "In SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Jensen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jensen et al\\.", "year": 2004}, {"title": "Multilabel collective classification", "author": ["Xiangnan Kong", "Xiaoxiao Shi", "S Yu Philip"], "venue": "In SDM,", "citeRegEx": "Kong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kong et al\\.", "year": 2011}, {"title": "Inferring private information using social network data", "author": ["Jack Lindamood", "Raymond Heatherly", "Murat Kantarcioglu", "Bhavani Thuraisingham"], "venue": "In Proceedings of the 18th international conference on World wide web,", "citeRegEx": "Lindamood et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lindamood et al\\.", "year": 2009}, {"title": "Collective classification of network data", "author": ["Ben London", "Lise Getoor"], "venue": "Data Classification: Algorithms and Applications. CRC Press,", "citeRegEx": "London and Getoor.,? \\Q2013\\E", "shortCiteRegEx": "London and Getoor.", "year": 2013}, {"title": "Link-based classification", "author": ["Qing Lu", "Lise Getoor"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Lu and Getoor.,? \\Q2003\\E", "shortCiteRegEx": "Lu and Getoor.", "year": 2003}, {"title": "Learning to discover social circles in ego networks", "author": ["J.J. McAuley", "J. Leskovec"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "McAuley and Leskovec.,? \\Q2012\\E", "shortCiteRegEx": "McAuley and Leskovec.", "year": 2012}, {"title": "Cautious inference in collective classification", "author": ["Luke K McDowell", "Kalyan Moy Gupta", "David W Aha"], "venue": "In AAAI,", "citeRegEx": "McDowell et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McDowell et al\\.", "year": 2007}, {"title": "Birds of a feather: Homophily in social networks", "author": ["Miller McPherson", "Lynn Smith-Lovin", "James M Cook"], "venue": "Annual Review of Sociology,", "citeRegEx": "McPherson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "McPherson et al\\.", "year": 2001}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Deep collective inference", "author": ["John Moore", "Jennifer Neville"], "venue": "In Proceedings of the 31st AAAI Conference on Artificial Intelligence,", "citeRegEx": "Moore and Neville.,? \\Q2017\\E", "shortCiteRegEx": "Moore and Neville.", "year": 2017}, {"title": "Iterative classification in relational data", "author": ["Jennifer Neville", "David Jensen"], "venue": "In Proc. AAAI-2000 Workshop on Learning Statistical Models from Relational Data,", "citeRegEx": "Neville and Jensen.,? \\Q2000\\E", "shortCiteRegEx": "Neville and Jensen.", "year": 2000}, {"title": "Collective classification with relational dependency networks", "author": ["Jennifer Neville", "David Jensen"], "venue": "In Proceedings of the Second International Workshop on Multi-Relational Data Mining,", "citeRegEx": "Neville and Jensen.,? \\Q2003\\E", "shortCiteRegEx": "Neville and Jensen.", "year": 2003}, {"title": "Column networks for collective classification", "author": ["Trang Pham", "Truyen Tran", "Dinh Phung", "Svetha Venkatesh"], "venue": "arXiv preprint arXiv:1609.04508,", "citeRegEx": "Pham et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2016}, {"title": "Collective classification in network data", "author": ["Prithviraj Sen", "Galileo Namata", "Mustafa Bilgic", "Lise Getoor", "Brian Galligher", "Tina Eliassi-Rad"], "venue": "AI Magazine,", "citeRegEx": "Sen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sen et al\\.", "year": 2008}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Discriminative probabilistic models for relational data", "author": ["Ben Taskar", "Pieter Abbeel", "Daphne Koller"], "venue": "In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Taskar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2002}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Relational learning with one network: An asymptotic analysis", "author": ["R. Xiang", "J. Neville"], "venue": "In International Conference on Artificial Intelligence & Statistics (AISTATS),", "citeRegEx": "Xiang and Neville.,? \\Q2011\\E", "shortCiteRegEx": "Xiang and Neville.", "year": 2011}], "referenceMentions": [{"referenceID": 22, "context": "Existing learning algorithms for iterative classification resort to an approximation based on the unrealistic assumption that the predicted labels of neighbors are their true classes (Neville and Jensen, 2000; London and Getoor, 2013).", "startOffset": 183, "endOffset": 234}, {"referenceID": 15, "context": "Existing learning algorithms for iterative classification resort to an approximation based on the unrealistic assumption that the predicted labels of neighbors are their true classes (Neville and Jensen, 2000; London and Getoor, 2013).", "startOffset": 183, "endOffset": 234}, {"referenceID": 28, "context": "Finally, because the same base-classifier parameters should be used at all iterations of ICA, we can use methods for recurrent neural networks such as back-propagation through time (BPTT) (Werbos, 1990) to compute the combined gradient.", "startOffset": 188, "endOffset": 202}, {"referenceID": 7, "context": "Node classification is one of the fundamental tasks in analysis of network data (Getoor and Diehl, 2005; Lindamood et al., 2009).", "startOffset": 80, "endOffset": 128}, {"referenceID": 14, "context": "Node classification is one of the fundamental tasks in analysis of network data (Getoor and Diehl, 2005; Lindamood et al., 2009).", "startOffset": 80, "endOffset": 128}, {"referenceID": 13, "context": "Collective classification addresses this task by making joint classifications of connected nodes (Kong et al., 2011; Taskar et al., 2002; Neville and Jensen, 2003).", "startOffset": 97, "endOffset": 163}, {"referenceID": 27, "context": "Collective classification addresses this task by making joint classifications of connected nodes (Kong et al., 2011; Taskar et al., 2002; Neville and Jensen, 2003).", "startOffset": 97, "endOffset": 163}, {"referenceID": 23, "context": "Collective classification addresses this task by making joint classifications of connected nodes (Kong et al., 2011; Taskar et al., 2002; Neville and Jensen, 2003).", "startOffset": 97, "endOffset": 163}, {"referenceID": 18, "context": "Gibbs sampling (GS) is another approach for collective classification using the iterative classification framework (McDowell et al., 2007; Sen et al., 2008), that introduces randomization into the iterative classification.", "startOffset": 115, "endOffset": 156}, {"referenceID": 25, "context": "Gibbs sampling (GS) is another approach for collective classification using the iterative classification framework (McDowell et al., 2007; Sen et al., 2008), that introduces randomization into the iterative classification.", "startOffset": 115, "endOffset": 156}, {"referenceID": 25, "context": "ICA and GS have been shown repeatedly to be effective frameworks for collective classification (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003; McDowell et al., 2007).", "startOffset": 95, "endOffset": 204}, {"referenceID": 12, "context": "ICA and GS have been shown repeatedly to be effective frameworks for collective classification (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003; McDowell et al., 2007).", "startOffset": 95, "endOffset": 204}, {"referenceID": 22, "context": "ICA and GS have been shown repeatedly to be effective frameworks for collective classification (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003; McDowell et al., 2007).", "startOffset": 95, "endOffset": 204}, {"referenceID": 16, "context": "ICA and GS have been shown repeatedly to be effective frameworks for collective classification (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003; McDowell et al., 2007).", "startOffset": 95, "endOffset": 204}, {"referenceID": 18, "context": "ICA and GS have been shown repeatedly to be effective frameworks for collective classification (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003; McDowell et al., 2007).", "startOffset": 95, "endOffset": 204}, {"referenceID": 19, "context": "One of the more natural motivations for collective classification comes from the study of social networks, where the phenomenon of homophily\u2014the tendency of individuals to interact with other similar individuals\u2014has been an important concept (McPherson et al., 2001; Bilgic et al., 2007).", "startOffset": 242, "endOffset": 287}, {"referenceID": 1, "context": "One of the more natural motivations for collective classification comes from the study of social networks, where the phenomenon of homophily\u2014the tendency of individuals to interact with other similar individuals\u2014has been an important concept (McPherson et al., 2001; Bilgic et al., 2007).", "startOffset": 242, "endOffset": 287}, {"referenceID": 29, "context": "In transductive settings, the classifier is both trained and tested on partially labeled networks, or possibly in the same network with a different set of labels known during each phase (Xiang and Neville, 2011).", "startOffset": 186, "endOffset": 211}, {"referenceID": 0, "context": "RNNs were introduced decades ago (Angeline et al., 1994; Connor et al., 1994), but they have recently become prominent because of their effectiveness at modeling sequences, such as those occurring in natural language processing, e.", "startOffset": 33, "endOffset": 77}, {"referenceID": 3, "context": "RNNs were introduced decades ago (Angeline et al., 1994; Connor et al., 1994), but they have recently become prominent because of their effectiveness at modeling sequences, such as those occurring in natural language processing, e.", "startOffset": 33, "endOffset": 77}, {"referenceID": 26, "context": ", (Socher et al., 2011; Mikolov et al., 2010; Graves and Jaitly, 2014; Graves, 2012).", "startOffset": 2, "endOffset": 84}, {"referenceID": 20, "context": ", (Socher et al., 2011; Mikolov et al., 2010; Graves and Jaitly, 2014; Graves, 2012).", "startOffset": 2, "endOffset": 84}, {"referenceID": 9, "context": ", (Socher et al., 2011; Mikolov et al., 2010; Graves and Jaitly, 2014; Graves, 2012).", "startOffset": 2, "endOffset": 84}, {"referenceID": 8, "context": ", (Socher et al., 2011; Mikolov et al., 2010; Graves and Jaitly, 2014; Graves, 2012).", "startOffset": 2, "endOffset": 84}, {"referenceID": 28, "context": "A now standard method for gradient optimization of RNN parameters is known as back-propagation through time (Werbos, 1990; Hochreiter et al., 2001; Ishikawa, 1996), which unrolls recurrent networks and computes gradients for the parameters separately before combining them into a single update.", "startOffset": 108, "endOffset": 163}, {"referenceID": 10, "context": "A now standard method for gradient optimization of RNN parameters is known as back-propagation through time (Werbos, 1990; Hochreiter et al., 2001; Ishikawa, 1996), which unrolls recurrent networks and computes gradients for the parameters separately before combining them into a single update.", "startOffset": 108, "endOffset": 163}, {"referenceID": 11, "context": "A now standard method for gradient optimization of RNN parameters is known as back-propagation through time (Werbos, 1990; Hochreiter et al., 2001; Ishikawa, 1996), which unrolls recurrent networks and computes gradients for the parameters separately before combining them into a single update.", "startOffset": 108, "endOffset": 163}, {"referenceID": 6, "context": "After the first version of our manuscript was published (Fan and Huang, 2016), two other groups have independently and concurrently pursued very similar directions, applying deep learning to collective classification (Pham et al.", "startOffset": 56, "endOffset": 77}, {"referenceID": 24, "context": "After the first version of our manuscript was published (Fan and Huang, 2016), two other groups have independently and concurrently pursued very similar directions, applying deep learning to collective classification (Pham et al., 2016; Moore and Neville, 2017).", "startOffset": 217, "endOffset": 261}, {"referenceID": 21, "context": "After the first version of our manuscript was published (Fan and Huang, 2016), two other groups have independently and concurrently pursued very similar directions, applying deep learning to collective classification (Pham et al., 2016; Moore and Neville, 2017).", "startOffset": 217, "endOffset": 261}, {"referenceID": 24, "context": "Pham et al. (2016) proposed another deep learning model for collective classification.", "startOffset": 0, "endOffset": 19}, {"referenceID": 25, "context": "In practice, researchers have used naive Bayes, logistic regression, support vector machines as classifiers, and they have used averages, sums, and presence as relational features (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003).", "startOffset": 180, "endOffset": 266}, {"referenceID": 12, "context": "In practice, researchers have used naive Bayes, logistic regression, support vector machines as classifiers, and they have used averages, sums, and presence as relational features (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003).", "startOffset": 180, "endOffset": 266}, {"referenceID": 22, "context": "In practice, researchers have used naive Bayes, logistic regression, support vector machines as classifiers, and they have used averages, sums, and presence as relational features (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003).", "startOffset": 180, "endOffset": 266}, {"referenceID": 16, "context": "In practice, researchers have used naive Bayes, logistic regression, support vector machines as classifiers, and they have used averages, sums, and presence as relational features (Sen et al., 2008; Jensen et al., 2004; Neville and Jensen, 2000; Lu and Getoor, 2003).", "startOffset": 180, "endOffset": 266}, {"referenceID": 25, "context": "which is the class label with the highest probability among neighbors and exists, which is an indicator for each class label (Sen et al., 2008).", "startOffset": 125, "endOffset": 143}, {"referenceID": 25, "context": "The Cora data set is a collection of 2,708 machine learning publications categorized into seven classes (Sen et al., 2008).", "startOffset": 104, "endOffset": 122}, {"referenceID": 25, "context": "tion of 3,312 research publications crawled from the CiteSeer repository (Sen et al., 2008).", "startOffset": 73, "endOffset": 91}, {"referenceID": 17, "context": "The social network data we use is the Facebook ego network data (McAuley and Leskovec, 2012), which includes users\u2019 personal information, friend lists, and ego networks.", "startOffset": 64, "endOffset": 92}, {"referenceID": 2, "context": "The image dataset we use is the Weizmann horse-image segmentation set (Borenstein and Ullman, 2004).", "startOffset": 70, "endOffset": 99}, {"referenceID": 2, "context": "The image dataset we use is the Weizmann horse-image segmentation set (Borenstein and Ullman, 2004). We subsample 20 images of horses on various backgrounds. We use features described by Domke (2013) for each pixel: We expand the RGB values of each pixel and the normalized vertical and horizontal positions into 64 features using sinusoidal expansion.", "startOffset": 71, "endOffset": 200}, {"referenceID": 5, "context": "For each of the learning objectives, we optimize using the adagrad approach (Duchi et al., 2011), in which gradients are rescaled based on the magnitude of previously seen gradients.", "startOffset": 76, "endOffset": 96}], "year": 2017, "abstractText": "We propose a new method for training iterative collective classifiers for labeling nodes in network data. The iterative classification algorithm (ICA) is a canonical method for incorporating relational information into classification. Yet, existing methods for training ICA models rely on the assumption that relational features reflect the true labels of the nodes. This unrealistic assumption introduces a bias that is inconsistent with the actual prediction algorithm. In this paper, we introduce recurrent collective classification (RCC), a variant of ICA analogous to recurrent neural network prediction. RCC accommodates any differentiable local classifier and relational feature functions. We provide gradient-based strategies for optimizing over model parameters to more directly minimize the loss function. In our experiments, this direct loss minimization translates to improved accuracy and robustness on real network data. We demonstrate the robustness of RCC in settings where local classification is very noisy, settings that are particularly challenging for ICA.", "creator": "TeX"}}}