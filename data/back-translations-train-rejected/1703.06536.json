{"id": "1703.06536", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2017", "title": "The Relationship Between Agnostic Selective Classification Active Learning and the Disagreement Coefficient", "abstract": "A selective classifier (f,g) comprises a classification function f and a binary selection function g, which determines if the classifier abstains from prediction, or uses f to predict. The classifier is called pointwise-competitive if it classifies each point identically to the best classifier in hindsight (from the same class), whenever it does not abstain. The quality of such a classifier is quantified by its rejection mass, defined to be the probability mass of the points it rejects. A \"fast\" rejection rate is achieved if the rejection mass is bounded from above by O(1/m) where m is the number of labeled examples used to train the classifier (and O hides logarithmic factors). Pointwise-competitive selective (PCS) classifiers are intimately related to disagreement-based active learning and it is known that in the realizable case, a fast rejection rate of a known PCS algorithm (called Consistent Selective Strategy) is equivalent to an exponential speedup of the well-known CAL active algorithm.", "histories": [["v1", "Sun, 19 Mar 2017 23:22:31 GMT  (726kb,D)", "http://arxiv.org/abs/1703.06536v1", null], ["v2", "Thu, 30 Mar 2017 11:08:28 GMT  (252kb,D)", "http://arxiv.org/abs/1703.06536v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["roei gelbhart", "ran el-yaniv"], "accepted": false, "id": "1703.06536"}, "pdf": {"name": "1703.06536.pdf", "metadata": {"source": "CRF", "title": "The Relationship Between Agnostic Selective Classification Active Learning and the Disagreement Coefficient", "authors": ["Roei Gelbhart", "Ran El-Yaniv"], "emails": ["ROEIGE@CS.TECHNION.AC.IL", "RANI@CS.TECHNION.AC.IL"], "sections": [{"heading": null, "text": "We focus on the agnostic setting, for which there is a well-known algorithm called LESS, which learns a PCS classifier and achieves a fast rejection rate (depending on Hanneke's inconsistency coefficient) under strong assumptions. We present an improved PCS learning algorithm called ILESS, for which we show a fast rejection rate (depending on Hanneke's inconsistency coefficient) without assumptions. Our rejection rate smoothly interpolates the feasible and agnostic settings. The main result of this work is an equivalence between the following three units: (i) the existence of a fast rejection rate for each PCS learning algorithm (such as ILESS); (ii) a poly-logarithmic limit for Hanneke's inconsistency coefficient; and (iii) an exponential acceleration for a new disunity-based active learning algorithm called active-ILESS."}, {"heading": "1. Introduction", "text": "It is not as if it is a real problem if it is a real problem. (F) The general performance of a selective classifier is quantified in terms of its coverage and risk, where coverage is the most likely mass of unrejected instances and the risk isar Xiv: 170 3.06 536v 1 [cs.L G] 19 Mthe normalized average loss of f-limited f-instances. (F) Let f-instances be a real risk problem for the given problem. (F) 19 Mthe normalized average loss of f-instances. (F) Lass f-instances is (unknown) real risk problem for non-rejected instances, and Risk isar Xiv: 170 3.06 536v 1 [cs.L G] 19 Mthe normalized average loss of f-instances."}, {"heading": "2. Definitions", "text": "A learning problem is defined by a hypothesis of class F and an unknown probability distribution. (...) A learning problem is defined by a hypothesis of class F and an unforeseen probability distribution PX, Y. (...) A learning problem is defined by a hypothesis of class F. (...) A learning problem is defined by a hypothesis of class F. (...) A learning problem is defined by a hypothesis of class F. (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (..., (...), (...), (..., (...), (..., (...), (...,...,..., (...), (..., (...), (...,...,..., (...), (...), (..., (...), (...,..., (...), (...,..., (...), (...,...,...), (...,...,..., (...), (...,...,..., (...,...,...,...,), (..., (...), (...), (..., (..., (...), (...,..., (...), (..., (...,), (...), (..., (...), (..., (...), (..., (..., (...), (..., (...), (...), (..., (..., (...), (..., (..., (...), (..., (..., (...), (..., (..., (..., (...,),), (..., (...), (...), (..., (...), (..., (..., (...), (...), (..., (..., (...), (...), (..., (..., (..., (...,), (...), (..., (...), (...,), (..., (...,), (...,), (..., (...,"}, {"heading": "3. Convergence Bounds and LESS", "text": "Definition Convergence, those of [18, 23]. Definition Convergence, those of [18, 23]. Definition Convergence, those of [18, 23]. Definition Convergence, those of [18, 23]. Definition Convergence, those of [18, 23]. Definition Convergence, those of [18, 23]. Definition Convergence, those of [18, 23]. Definition Convergence, those of [18, 23]. Definition Convergence, those of [18]. Definition Convergence, those of [18]. Definition Convergence, those of [18]. Definition Convergence, those of [18]. Definition, those of [18, 23]. Definition Convergence, those of [18]. Definition, those of Convergence, those of [18]. Definition, definition, those of Convergence, those of, those of [18]. Definition, those of Convergence, those of, those of [18]. Definition Convergence, those of, those of, those of [18]."}, {"heading": "4. ILESS", "text": "(D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (D), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F), (F (F), (F), (F), (F), (F), (F (F), (F), (F), (F (F), (F), (F), (F (F), (F), (F), (F), (F), (F (F), (F), (F), (F), (F), (F), (F), (F), (F (F), (F), (F), (F), (F), (F), (F), (), (F), (F), (F), (, (F), (, (F), (, (F), (F), (F), (), (F), (, (, (F), (), (F), (F), (, (F), (F), (F), (F), (F), ("}, {"heading": "5. From Selective Classification to the Disagreement Coefficient", "text": "We must now show a reduction of the selective classification to detect the inconsistency resulting from the random forecast. (7) Let F be a hypothesis class with a finite UK dimension d, and let us allow PX, Y an unknown distribution. (7) Let us allow the PK class (7) Sp (7) Sp (7) Sp (6) Sp (7) Sp (7) Sp (7) Sp (7) Sp (7) Sp (7) Sp (6) Sp (6) p (6) p (6) p (6) p) p (6) p) p (6) p (6) p (6) p) p (6) p (6) p (6) p (6) p (6) p (6) p (6) p (p) p (6) p (6) p (p) p (6) p (p) p (6) p (p) p (6) p (p) p (6) p (6) p (p) p (6) p (p) p (6) p (p) p (6) p (p) p (6) p (p (6) p (p) p (p (6) p (p) p (p) p (p (6) p (p) p (p (p) p (p (6) p (p) p (p) p (p (p) p (p) p (6 p (p) p (p) p (p) p (p (p) p (p) p 3 p (p) p 3 p (p) p (p) p 3 p (p) p 3 p (p) p 3 p (p) p (p) p (p) p (6) p (p (6) p) p (p (p (p) p (6) p) p (p (p (p) p (6) p) p (p (6) p (p) p (6) p (p (6) p (p (p) p (6) p) p (p (6) p) p (p (6) p (p (p (6) p (p (p) p (p) p) p) p (p"}, {"heading": "6. Active-ILESS", "text": "\"We have not managed to solve the problem.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" \"We.\" \"\" We. \"\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\" We. \"We.\""}, {"heading": "7. A Reduction from Active-iLess to Batch-ILESS", "text": "In strategy 4, we define a selective classification for ILESS, called batch ILESS, as it is simpler than ILESS, which uses Active-ILESS as its motor. Faced with an example Sm described, Batch-ILESS simulates the active algorithm by applying it in a simple way via a uniform random arrangement of Sm (i.e., it introduces an unlabeled example sequentially to the active algorithm and discloses the label only when the active algorithm requests it). Upon termination, after the active algorithm has consumed all the examples, our batch algorithm receives f from the active algorithm and uses its last low error rate to define its selection functions. Lemma 11 implies that Batch-ILESS is pointedly competitive. We note that Lemma 4, Theorem 5 and Theorem 8, which are detectable for ILESS."}, {"heading": "8. From the Disagreement Coefficient to Active Learning", "text": "In this section, we show that if the Active-ILESS is already dependent on the ILS, we already have it for the ESM. < M > R > R (f) for some specific PX, Y, then the complexity of Active-ILESS under the same PX, Y depends only on the distribution of Polylog1 (1 / r), so if someone (r) has a Polylog1 (1 / r) for all r > 0, d, 1 / r), where the parameters of Polylog2 and Polylog3 depend only on Polylog1 (1 / r), if he (r) has Polylog1 (1 / r) for all r > 0, we get this Active-ILESS class. This direction has already been shown in [9, 10] for agnostic CAL and A2."}, {"heading": "9. Concluding Remarks", "text": "In this paper, we focus on inconsistency-based methodologies, which means that we always have an equivalent response to a low error rate within a subset of hypotheses w.h.p., and we have made decisions based on inconsistencies. We have therefore introduced a new selective classification called ILESS, the rejection of which, however, uses sharp generalization boundaries (which depend on the Bernstein premise).Our analysis shows that ILESS sometimes has significantly better distribution guarantees relative to the best known selective strategy of the [4].In addition, the guarantees we provide for ILESS are not at all dependent on the Bernstein premise. For the general agnostic attitude, we have shown an equivalence relationship between pointedly-competing distributions, active learning and the inconsistency coefficient (see Figure 1).This equivalence is formulated in relation to a rapid R-R-4.R-exponential and 6.3 definitions."}, {"heading": "Appendix A.", "text": "We now show that functions that are true risk minimizers of PX, Y (Gt \u2212 1) are within Gt \u2212 1. We point to a true risk minimizer according to PX, Y (Gt \u2212 1) according to PX, Y (Gt \u2212 1), using inequality (49) and the definition of inequality (Gt \u2212 1), R (f \u2212 R), RPX, Y (Gt \u2212 1), Y (Gt \u2212 1), R (f \u2212 X), RPX \u2212 R, Y (f \u2212 1), R (Gt), Y (f \u2212 2), R (Gt), R (f \u2212 2), Y (Gt), R (n), R n (R), R n (n), R (R), R (n), R (R), R (R), R (n, R), R (R), R (n, R), R (R), R (n (R), R (n, R), R (n, Gt), R (f (t), R (n, R, n (n, n, R), G (n (n, n, n, n (f, n, n, n, n, G, n), G (t), G (t), R (R (r \u2212 R), R (r \u2212 R), R (n (r \u2212 R), R (n (n, R), R (r), R (n (n, R), R (n (n, R), R (n, R), R (n (n, R), R (n (n, R), R (n (n, R), R (n, R), R (n (n, R), G (n (n, R), G (n (n, R), G (n (n (n, n, G (n, R), G (n, R), G (f \u2212 R), G (f \u2212 R), R (f \u2212 R), R), R (f \u2212 R (r), R (r), R (n (r), R (n (r), R), R ("}], "references": [{"title": "On optimum recognition error and reject trade-off", "author": ["C. Chow"], "venue": "IEEE Trans. on Information Theory, vol. 16, pp. 41\u201336, 1970.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1970}, {"title": "Algorithmic Learning in a Random World", "author": ["V. Vovk", "A. Gammerman", "G. Shafer"], "venue": "New York: Springer,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "On the foundations of noise-free selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 1605\u20131641, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Agnostic pointwise-competitive selective classification", "author": ["Y. Wiener", "R. El-Yaniv"], "venue": "Journal of AI Research, vol. 52, pp. 171\u2013201, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning with rejection", "author": ["C. Cortes", "G. DeSalvo", "M. Mohri"], "venue": "International Conference on Algorithmic Learning Theory, pp. 67\u201382, Springer, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Agnostic selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": "Neural Information Processing Systems (NIPS), pp. 1665\u20131673, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Local complexities for empirical risk minimization", "author": ["P. Bartlett", "S. Mendelson", "P. Philips"], "venue": "COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann Publishers, 2004. 31", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimal aggregation of classifiers in statistical learning", "author": ["A. Tsybakov"], "venue": "The Annals of Mathematical Statistics, vol. 32, pp. 135\u2013166, 2004.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["S. Hanneke"], "venue": "Proceedings of the 24th International Conference on Machine Learning (ICML), pp. 353\u2013360, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Algorithms for Active Learning", "author": ["D. Hsu"], "venue": "PhD thesis, Department of Computer Science and Engineering, School of Engineering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Active learning using smooth relative regret approximations with applications", "author": ["N. Ailon", "R. Begleiter", "E. Ezra"], "venue": "25th Annual Conference on Learning Theory (COLT), 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "A compression technique for analyzing disagreement-based active learning", "author": ["Y. Wiener", "S. Hanneke", "R. El-Yaniv"], "venue": "Journal of Machine Learning Research, vol. 16, pp. 713\u2013 745, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "On the version space compression set size and its applications", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": "Measures of Complexity, pp. 341\u2013357, Springer International Publishing, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Agnostic active learning", "author": ["M.-F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "Proceedings of the 23rd International Conference on Machine Learning, pp. 65\u201372, ACM, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Activized learning: Transforming passive to active with improved label complexity", "author": ["S. Hanneke"], "venue": "The Journal of Machine Learning Research, vol. 13, no. 5, pp. 1469\u20131587, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Theory of disagreement-based active learning", "author": ["S. Hanneke"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning, vol. 7, no. 2-3, pp. 131\u2013309, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Surrogate losses in passive and active learning", "author": ["S. Hanneke", "L. Yang"], "venue": "arXiv:1207.3772, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "A general agnostic active learning algorithm", "author": ["S. Dasgupta", "D.J. Hsu", "C. Monteleoni"], "venue": "Advances in Neural Information Processing Systems 20, pp. 353\u2013360, 2007.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient and parsimonious agnostic active learning", "author": ["T.-K. Huang", "A. Agarwal", "D.J. Hsu", "J. Langford", "R.E. Schapire"], "venue": "Advances in Neural Information Processing Systems 28 (C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, eds.), pp. 2755\u20132763, Curran Associates, Inc., 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Active learning via perfect selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": "Journal of Machine Learning Research, vol. 13, pp. 255\u2013279, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Theoretical Foundations of Selective Prediction", "author": ["Y. Wiener"], "venue": "PhD thesis, the Technion \u2014 Israel Institute of Technology,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Machine Learning, vol. 15, no. 2, pp. 201\u2013221, 1994.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "Introduction to statistical learning theory", "author": ["O. Bousquet", "S. Boucheron", "G. Lugosi"], "venue": "Advanced Lectures on Machine Learning, vol. 3176 of Lecture Notes in Computer Science, pp. 169\u2013207, Springer, 2003. 32", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Agnostic selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": "Neural Information Processing Systems (NIPS), 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Theory of Active Learning.", "author": ["S. Hanneke"], "venue": "http://www.stevehanneke.com,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Active learning in the non-realizable case", "author": ["M. K\u00e4\u00e4ri\u00e4inen"], "venue": "Lecture Notes in Computer Science, vol 4264. Springer, Berlin, Heidelberg, 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Coarse sample complexity bounds for active learning", "author": ["S. Dasgupta"], "venue": "Advances in Neural Information Processing Systems 18, pp. 235\u2013242, 2005.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Y. Freund", "H. Seung", "E. Shamir", "N. Tishby"], "venue": "Machine Learning, vol. 28, pp. 133\u2013168, 1997.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "Efficient active learning of halfspaces: an aggressive approach", "author": ["A. Gonen", "S. Sabato", "S. Shalev-Shwartz"], "venue": "Journal of Machine Learning Research, vol. 14, no. 1, pp. 2583\u20132615, 2013. 33", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Introduction Selective classification is a unique and extreme instance of the broader concept of confidencerated prediction [1, 2].", "startOffset": 124, "endOffset": 130}, {"referenceID": 1, "context": "Introduction Selective classification is a unique and extreme instance of the broader concept of confidencerated prediction [1, 2].", "startOffset": 124, "endOffset": 130}, {"referenceID": 2, "context": "Given a training sample consisting of m labeled instances, the learning algorithm is required to output a selective classifier [3], defined to be a pair ( f ,g), where f is a prediction function, chosen from some hypothesis class F , and g : X \u2192 {0,1} is a selection function, serving as a qualifier for f as follows: for any x, if g(x) = 1, the classifier predicts f (x), and otherwise it abstains.", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "The selective classifier ( f ,g) is said to be pointwisecompetitive if, for each x with g(x) = 1, it must hold that f (x) = f \u2217(x) for all f \u2217 \u2208 F [4].", "startOffset": 147, "endOffset": 150}, {"referenceID": 4, "context": "The scenario of a predefined decision functions hypothesis class is investigated in [5].", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "Pointwise-competitive selective classification (PCS) was first considered in the realizable case [3], for which a simple consistent selective strategy (CSS) was shown to achieve a bounded and monotonically increasing (with m) coverage in various non-trivial settings.", "startOffset": 97, "endOffset": 100}, {"referenceID": 3, "context": "These results were recently extended to the agnostic setting [4, 6] with a related but different algorithm called low-error selective strategy (LESS), for which a number of coverage bounds were shown.", "startOffset": 61, "endOffset": 67}, {"referenceID": 5, "context": "These results were recently extended to the agnostic setting [4, 6] with a related but different algorithm called low-error selective strategy (LESS), for which a number of coverage bounds were shown.", "startOffset": 61, "endOffset": 67}, {"referenceID": 6, "context": "These bounds relied on the fact that the underlying probability distribution and the hypothesis class F will satisfy the so-called \u201c(\u03b21,\u03b22)-Bernstein property\u201d [7].", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "The coverage bounds in [4, 6] are dependent on the parameters \u03b21,\u03b22.", "startOffset": 23, "endOffset": 29}, {"referenceID": 5, "context": "The coverage bounds in [4, 6] are dependent on the parameters \u03b21,\u03b22.", "startOffset": 23, "endOffset": 29}, {"referenceID": 6, "context": "This Bernstein property assumption (as presented in [7]), which allows for better concentration, can be problematic.", "startOffset": 52, "endOffset": 55}, {"referenceID": 3, "context": "It was mentioned in [4] that, under the Tsybakov noise condition [8], the desired property holds, but this is guaranteed only for cases in which the Bayes classifier is within F , which is a fairly strong assumption in itself.", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "It was mentioned in [4] that, under the Tsybakov noise condition [8], the desired property holds, but this is guaranteed only for cases in which the Bayes classifier is within F , which is a fairly strong assumption in itself.", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "Hanneke\u2019s disagreement coefficient [9] (see Definition 2.", "startOffset": 35, "endOffset": 38}, {"referenceID": 8, "context": "1), is a well-known parameter of the hypothesis class and the marginal distribution; it is used in most of the known label complexity bounds [9, 10, 11].", "startOffset": 141, "endOffset": 152}, {"referenceID": 9, "context": "1), is a well-known parameter of the hypothesis class and the marginal distribution; it is used in most of the known label complexity bounds [9, 10, 11].", "startOffset": 141, "endOffset": 152}, {"referenceID": 10, "context": "1), is a well-known parameter of the hypothesis class and the marginal distribution; it is used in most of the known label complexity bounds [9, 10, 11].", "startOffset": 141, "endOffset": 152}, {"referenceID": 11, "context": "Note that in principle, the disagreement coefficient can be replaced by another important quantity, namely, the version space compression set size, recently shown to be equivalent to it [12, 13].", "startOffset": 186, "endOffset": 194}, {"referenceID": 12, "context": "Note that in principle, the disagreement coefficient can be replaced by another important quantity, namely, the version space compression set size, recently shown to be equivalent to it [12, 13].", "startOffset": 186, "endOffset": 194}, {"referenceID": 13, "context": "This querying strategy, which is often termed \u201cdisagreementbased,\u201d has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18].", "startOffset": 166, "endOffset": 170}, {"referenceID": 14, "context": "This querying strategy, which is often termed \u201cdisagreementbased,\u201d has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18].", "startOffset": 209, "endOffset": 217}, {"referenceID": 15, "context": "This querying strategy, which is often termed \u201cdisagreementbased,\u201d has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18].", "startOffset": 209, "endOffset": 217}, {"referenceID": 16, "context": "This querying strategy, which is often termed \u201cdisagreementbased,\u201d has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18].", "startOffset": 222, "endOffset": 226}, {"referenceID": 17, "context": "This querying strategy, which is often termed \u201cdisagreementbased,\u201d has been used in a number of stream-based AL algorithms such as A2 (Agnostic Active), developed in [14], RobustCAL, studied by the authors of [15, 16] and [17], or the general agnostic AL algorithm of [18].", "startOffset": 268, "endOffset": 272}, {"referenceID": 18, "context": "In [19], a computationally efficient algorithm for disagreement based AL.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "The first formal relationship between PCS classification and AL was proposed in [20, 21], where the aforementioned CSS algorithm was shown to be equivalent to the well-known CAL AL algorithm of [22], in the sense that a fast coverage rate for CSS was proven to be equivalent to exponential label complexity speedup for CAL.", "startOffset": 80, "endOffset": 88}, {"referenceID": 20, "context": "The first formal relationship between PCS classification and AL was proposed in [20, 21], where the aforementioned CSS algorithm was shown to be equivalent to the well-known CAL AL algorithm of [22], in the sense that a fast coverage rate for CSS was proven to be equivalent to exponential label complexity speedup for CAL.", "startOffset": 80, "endOffset": 88}, {"referenceID": 21, "context": "The first formal relationship between PCS classification and AL was proposed in [20, 21], where the aforementioned CSS algorithm was shown to be equivalent to the well-known CAL AL algorithm of [22], in the sense that a fast coverage rate for CSS was proven to be equivalent to exponential label complexity speedup for CAL.", "startOffset": 194, "endOffset": 198}, {"referenceID": 8, "context": "the red arrow indicates a previously known result (from [9, 10]) (and can also be deduced from the other arrows).", "startOffset": 56, "endOffset": 63}, {"referenceID": 9, "context": "the red arrow indicates a previously known result (from [9, 10]) (and can also be deduced from the other arrows).", "startOffset": 56, "endOffset": 63}, {"referenceID": 3, "context": "We acquire the following definitions from [4].", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "The disagreement set [9] and agreement set [3] w.", "startOffset": 21, "endOffset": 24}, {"referenceID": 2, "context": "The disagreement set [9] and agreement set [3] w.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "(5) In selective classification [3], the learning algorithm receives Sm and is required to output a selective classifier, defined to be a pair ( f ,g), where f \u2208 F is a classifier, and g : X \u2192 {0,1} is a selection function, serving as a qualifier for f as follows.", "startOffset": 32, "endOffset": 35}, {"referenceID": 8, "context": "Then, Hanneke\u2019s disagreement coefficient [9] of a classifier f \u2208 F with respect to the target distribution PX is", "startOffset": 41, "endOffset": 44}, {"referenceID": 15, "context": "For more on the disagreement coefficient, and examples of probabilities distributions and hypothesis classes for which it is bounded, see [16].", "startOffset": 138, "endOffset": 142}, {"referenceID": 17, "context": "Convergence Bounds and LESS We use a uniform convergence bound from [18, 23].", "startOffset": 68, "endOffset": 76}, {"referenceID": 22, "context": "Convergence Bounds and LESS We use a uniform convergence bound from [18, 23].", "startOffset": 68, "endOffset": 76}, {"referenceID": 17, "context": "Lemma 1 ([18]) Let F be a hypothesis class with VC-dimension d.", "startOffset": 9, "endOffset": 13}, {"referenceID": 3, "context": "(14) Strategy 1 is the LESS algorithm of [4].", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "In the original lemma from [18], there appears S(H ,n), the growth function.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "We plug in Sauer\u2019s Lemma, S(H ,n)\u2264 ( em d ) d , into Lemma 1 from [18] to get our lemma.", "startOffset": 66, "endOffset": 70}, {"referenceID": 3, "context": "Remark 2 The original definition of pointwise-competitiveness from [4] requires a single f \u2217.", "startOffset": 67, "endOffset": 70}, {"referenceID": 15, "context": "In many cases, the disagreement coefficient, \u03b8(r), is bounded by a constant, or by O(polylog(1/r)) for all r > 0 (see [16]).", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "For example, it was shown in [12], that for linear separators under mixture of Gaussians, and for axis-aligned rectangles under product densities over Rk, \u03b8(r) is bounded by O(polylog(1/r)) for all r > 0.", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "5, let X = [0,1], and F = { f1, f2} where", "startOffset": 11, "endOffset": 16}, {"referenceID": 0, "context": "Let PX be the uniform distribution over [0,1].", "startOffset": 40, "endOffset": 45}, {"referenceID": 1, "context": "and for r in [2 ,1], \u2206B( f \u2217,r) r \u2264 1 1/2 = 2, (45)", "startOffset": 13, "endOffset": 19}, {"referenceID": 0, "context": "and for r in [2 ,1], \u2206B( f \u2217,r) r \u2264 1 1/2 = 2, (45)", "startOffset": 13, "endOffset": 19}, {"referenceID": 9, "context": "Active-ILESS is very similar to Agnostic CAL [10], Algorithm 4.", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "2 on page 36, and A2 [14].", "startOffset": 21, "endOffset": 25}, {"referenceID": 23, "context": "Moreover, while Agnostic CAL requires calculation of an ERM with many constraints (defined by the function LEARN in HSU\u2019s thesis), Active-ILESS requires a calculation of the ERM with only one constraint, as seen from the disbelief principle [24], already discussed in Section 4.", "startOffset": 241, "endOffset": 245}, {"referenceID": 9, "context": "In [10], Hsu introduced the agnostic CAL algorithm and showed (Theorem 4.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "The leading term of this bound is R( f \u2217)2 \u03b52 , which is also the case for A 2 [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 8, "context": "This direction has been shown before in [9, 10] for agnostic CAL and A2.", "startOffset": 40, "endOffset": 47}, {"referenceID": 9, "context": "This direction has been shown before in [9, 10] for agnostic CAL and A2.", "startOffset": 40, "endOffset": 47}, {"referenceID": 15, "context": "As a preparation for the theorem, we present Lemma 17 (shown before in [16]), in which we introduce a small feature of the disagreement coefficient that will serve us later.", "startOffset": 71, "endOffset": 75}, {"referenceID": 24, "context": "We found this useful bound in [26] (Theorem 5.", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "Our analysis proves that ILESS has sometimes significantly better rejection guarantees relative to the best known pointwisecompetitive selective strategy of [4].", "startOffset": 157, "endOffset": 160}, {"referenceID": 26, "context": "Such algorithms can be seen in [28, 29, 30], for the realizable and the low error scenarios.", "startOffset": 31, "endOffset": 43}, {"referenceID": 27, "context": "Such algorithms can be seen in [28, 29, 30], for the realizable and the low error scenarios.", "startOffset": 31, "endOffset": 43}, {"referenceID": 28, "context": "Such algorithms can be seen in [28, 29, 30], for the realizable and the low error scenarios.", "startOffset": 31, "endOffset": 43}], "year": 2017, "abstractText": "A selective classifier ( f ,g) comprises a classification function f and a binary selection function g, which determines if the classifier abstains from prediction, or uses f to predict. The classifier is called pointwise-competitive if it classifies each point identically to the best classifier in hindsight (from the same class), whenever it does not abstain. The quality of such a classifier is quantified by its rejection mass, defined to be the probability mass of the points it rejects. A \u201cfast\u201d rejection rate is achieved if the rejection mass is bounded from above by \u00d5(1/m) where m is the number of labeled examples used to train the classifier (and \u00d5 hides logarithmic factors). Pointwise-competitive selective (PCS) classifiers are intimately related to disagreement-based active learning and it is known that in the realizable case, a fast rejection rate of a known PCS algorithm (called Consistent Selective Strategy) is equivalent to an exponential speedup of the well-known CAL active algorithm. We focus on the agnostic setting, for which there is a known algorithm called LESS that learns a PCS classifier and achieves a fast rejection rate (depending on Hanneke\u2019s disagreement coefficient) under strong assumptions. We present an improved PCS learning algorithm called ILESS for which we show a fast rate (depending on Hanneke\u2019s disagreement coefficient) without any assumptions. Our rejection bound smoothly interpolates the realizable and agnostic settings. The main result of this paper is an equivalence between the following three entities: (i) the existence of a fast rejection rate for any PCS learning algorithm (such as ILESS); (ii) a poly-logarithmic bound for Hanneke\u2019s disagreement coefficient; and (iii) an exponential speedup for a new disagreement-based active learner called Active-ILESS.", "creator": "LaTeX with hyperref package"}}}