{"id": "1703.06554", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "Object category understanding via eye fixations on freehand sketches", "abstract": "The study of eye gaze fixations on photographic images is an active research area. In contrast, the image subcategory of freehand sketches has not received as much attention for such studies. In this paper, we analyze the results of a free-viewing gaze fixation study conducted on 3904 freehand sketches distributed across 160 object categories. Our analysis shows that fixation sequences exhibit marked consistency within a sketch, across sketches of a category and even across suitably grouped sets of categories. This multi-level consistency is remarkable given the variability in depiction and extreme image content sparsity that characterizes hand-drawn object sketches. In our paper, we show that the multi-level consistency in the fixation data can be exploited to (a) predict a test sketch's category given only its fixation sequence and (b) build a computational model which predicts part-labels underlying fixations on objects. We hope that our findings motivate the community to deem sketch-like representations worthy of gaze-based studies vis-a-vis photographic images.", "histories": [["v1", "Mon, 20 Mar 2017 01:13:33 GMT  (734kb,D)", "http://arxiv.org/abs/1703.06554v1", "Accepted for publication in Transactions on Image Processing (this http URL)"]], "COMMENTS": "Accepted for publication in Transactions on Image Processing (this http URL)", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["ravi kiran sarvadevabhatla", "sudharshan suresh", "r venkatesh babu"], "accepted": false, "id": "1703.06554"}, "pdf": {"name": "1703.06554.pdf", "metadata": {"source": "META", "title": "Object category understanding via eye fixations on freehand sketches", "authors": ["Ravi Kiran Sarvadevabhatla", "Sudharshan Suresh"], "emails": ["(ravika@gmail.com)", "(venky@cds.iisc.ac.in)"], "sections": [{"heading": null, "text": "In fact, most of them will be able to play by the rules that they need for their policies, and they will be able to play by the rules that they need for their policies."}, {"heading": "II. RELATED WORK", "text": "The study of eye fixations on images is a very active area of research in the related fields of neuroscience and computer vision [1] - [5], [22]. However, the broad category of images containing sketchy representations has probably not received as much attention because of a lack of usable image content with which to correlate the fixation data. In the available literature, there are four broad categories of studies that include eye and sketchy representations. The first category of studies uses eye tracking to understand how people copy and draw line drawings and simple shapes [23], [24]. The second uses eye tracking to investigate differences between the perception of photographic image contents and corresponding line representations [25], [26]. In the third category, eye tracking is used to characterize the semantic plausibility of objects in line drawings of scenes. [27] The fourth category of studies uses eye tracking to sketch objects."}, {"heading": "III. DATA GATHERING PROTOCOL", "text": "This year it has come to the point where it will be able to retaliate, \"he said.\" We have to go in search, \"he said.\" We have to go in search of a solution, \"he said.\" We have to go in search, \"he said."}, {"heading": "IV. CONSTRUCTING THE FIXATION MAP FOR A SKETCH", "text": "The fixation map is then outlined by a Gaussian core, whose standard deviation is set to 1 visualization angle (36 starting points).The fixation map is viewed by multiple subjects looking at the same image (sketch) and used as a basic truth to evaluate fixation methods (IOC sketch).We construct fixation maps for sketches based on the approach described by O'Connell et al. [2], which summarizes all fixation locations we look at across all subjects who have viewed the sketch.See the corresponding fixation times.The fixation map is initialized as the sum of impulse-oriented functions at each fixation location and is weighted by the fixation duration duration.This map is then outlined by a Gaussian core, whose standard deviation is set to 1 visualization angle (36 starting points)."}, {"heading": "VI. FIXATION MAPS : A CATEGORY-LEVEL PERSPECTIVE", "text": "To get a categorical perspective of fixation, the category-specific fixation maps Uc are calculated by averaging the standardized fixation maps of the sketches within a category (equation 2). Let's name the number of fixation maps in category c. We have: Uc = 1cnc [n] s common to all categories (e.g.), marginalized versions of the category maps are calculated by subtracting the average of all category-related maps."}, {"heading": "VII. OBJECT CATEGORIZATION: HOW WELL CAN WE PREDICT CATEGORY FROM FIXATIONS ALONE?", "text": "The high level of the IOC observed in Section V suggests reliable consistency between the fixation sequences and raises a fascinating question: Given a fixation sequence, is it possible to predict the category of the sketch to which it corresponds, only from fixation locations? In this respect, good prediction performance would allow finer, object-based analyses based on a correct prediction of the underlying category (see Section IX). To answer the above question, we follow the method of O'Connell et al. We consider the fixation data of a subject for testing and use the fixation data from the rest of the subjects (Leave-One-Subject-Out method) to build a category of smart fixation maps as described in Section VI. Let's assume the subject whose data was used for testing, M viewed sketches and used the subject's sketches to use the fixation data from the rest of the fixation data."}, {"heading": "A. Ablative Experiments", "text": "We also conducted a series of ablative experiments on (a) the regimes - primed and unrounded (b), using the regimes alternately for training / testing and (c) including duration in the creation of fixation maps. We do not include the results of these experiments in Figure 6 for clarity, however, we discovered the following trends: \u2022 Models trained on primed fixation sequences predict better and relatively better chances for test fixation sequences regardless of the regime (p < 0.005 for unrounded regimes and p < 0.05 for unrounded regimes, character test) \u2022 A whole range of categories are misclassified when models are trained on unrounded fixation sequences. We believe this is due to confusion resulting from an increased number of classes in unrounded regimes (160 vs 13), the similarity between categories belonging to the same \"fixation map\" (section 6)."}, {"heading": "VIII. FIXATIONS AND SEMANTIC OBJECT-PARTS", "text": "One hypothesis for this deeper phenomenon is the following: In a random order (i.e. when object category is announced), subjects fix on regions that consistently correspond to the \"signature\" of semantic object parts, with the fixation order reflecting the relative meaning of the parts. To verify this hypothesis, we used object part contour annotations that are available for the sketches considered in the compressed regime [18]. As a general approach, we have assigned fixations to the corresponding object part regions - sequence order. This assignment of fixations to sub-regions entails a certain degree of ambiguity with overlapping partial annotations. To minimize this, we used a multi-level spatial analysis algorithm to determine the assignment of fixations to sub-regions."}, {"heading": "IX. A COMPUTATIONAL MODEL FOR FIXATION-BASED OBJECT PART PREDICTION", "text": "Building on the encouraging trends in the similarity of fixation sequences at the fixation (section V) and the partial marker level (section VIII), we ask: Is it possible to predict the object labels of each fixation in the sequence given a fixation sequence? Such predictions could allow applications such as object contour annotations of freehand sketches [35] from fixation sequences alone7 in analogy to the approaches to object limitation described in Papadopoulos et al. [36] and Yun et al. [33]. Furthermore, the additional partial presence information (available for sketches commented on via fixations) can be used to refine the results of sketch-based image restoration approaches. [37] The consistency of the fixation sequences at the sketch and category level certainly suggests that the ordinal position and spatial position of a fixation sequence within a part of the fixation sequence depends on the {1} probability of a high fixation component within its fixation sequence."}, {"heading": "A. Computational Models", "text": "To represent the sequence of fixation characteristics, we use a Hidden Markov Model (HMM).Q = hidden states. Using terminology relevant to HMMs, let us specify the observation sequence as f1: T = (f1, f2,. ft,.. fT), where ft-Rd (i.e. continuous observations) and the corresponding sequence of hidden states is specified as f1: T sequence. HMM is typically specified by three components \u03bb = (A, B). A is the state transition matrix such as Aij = p (yt + 1 = j | yt = i). B is the observation model, i.e. Bt = p (ft) is the probability vector of the initial states, i.e. B is the state transition matrix such that we represent F = p (f1, f2, fM) the sequence of the fixation characteristics."}, {"heading": "B. Experimental Setup", "text": "For our experiments, we used 60% of the randomly selected fixation sequences of the category to construct the individual training models. To ensure sufficient training data, we extended the data. For each reference training sequence, we generated 50 extended sequences, randomly disturbing the fixation position, ensuring that the disturbance resulted in a deviation of the angle of view position of less than 1 degree for each reference fixation. For each test sequence, we calculated the proportion of correctly predicted partial markers (sequence prediction accuracy), and the categorically predictive accuracy of the partial visitation sequence was averaged over 10 trials9. To verify whether the performance of the model was better than the randomly assigned partial markers for fixations of a sequence."}, {"heading": "C. Alternate models", "text": "In this section we provide a brief overview of the alternative models we use for the partial prediction calculation model1) DTW: In the DTW model we use the spatial locations of the fixations. For each sequence of test fixations we find the training fixation sequence with the optimal alignment distance d \u0445. In determining the optimality criteria we use Euclidean metrics as the base distance between the fixation locations via test and traction sequences. After we determine the training sequence with the smallest distance d \u0445, we determine sub-label predictions for the test sequence as follows: Suppose p and q contain the indices for test and \"optimal-matching\" traction sequences, as defined by the DTW Procedure.9In each study, the training amount was selected randomly. Initially, all test fixations p are considered unachieved."}, {"heading": "D. Results for our computational model", "text": "We compared our HMM-based calculation model with alternative models commonly used for sequence-based prediction tasks. In particular, we performed comparative evaluations using HMM Viterbi decoding (HMM-Viterbi), Dynamic Time Warping (DTW), and Recurrent Neural Networks (RNN), as in Section IX-C. For the evaluation method, the experimental setup is the same (Section IX-B). To verify that the performance of our model was better than chance, we calculated predictions using randomly assigned sub-markers to fix a sequence. All of the aforementioned models work better than random. However, the performance of our PMAP-based model is superior to the alternative models (see Figure 8) and significantly better than random sub-mark mapping models - on average, zero calculations of the sequence are calculated, with at least 61% of sequence markers predicting our object orientation through expliability."}, {"heading": "X. DISCUSSION", "text": "The motivation for studying fixations on objects stems from the work of Einhauser et al. [43] and Nuthmann et al. [44] who claim that people represent and understand more than just what they see, namely visual content with respect to objects (although contradictory results with non-fixed approaches have been reported). Our observations (Section VI) indicate a categorical regularity with regard to the eye, the fixation patterns on sketches of objects in the open air. (Section VI) show that these are images that represent a scene and a line of drawings that represent virtually the same image. However, is it also possible that eye mechanisms are used for photos and sketches."}, {"heading": "XI. CONCLUSION", "text": "In this paper, we have presented the first large-scale study of object fixation data on hands-free sketches. Analysis of data from our user study has shown that eye fixations on hands-free sketches are not mere shapes - they are strongly conditioned by the categorical aspect of the sketch content (object). In fact, this conditioning is strong enough to predict the object category on the basis of the fixation sequence alone. More dramatically, our results show that the sequencing of fixations corresponds to an implicit sequence of parts that constitute the object, although the parts themselves cannot be described in the sketch stroke data. In our work, we have also shown how consistency in visions can best be used to create object-specific mathematical models that are capable of predicting the male object parts that are implicitly subject to fixations."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank Prof. Veni Madhavan (Indian Institute of Science) for the access to SMI Eye Tracker devices and NVIDIA for donating the Tesla K40 GPU."}], "references": [{"title": "Learning to predict where humans look", "author": ["T. Judd", "K. Ehinger", "F. Durand", "A. Torralba"], "venue": "IEEE International Conference on Computer Vision, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Dissociation of salience-driven and content-driven spatial attention to scene category with predictive decoding of gaze patterns", "author": ["T.P. O\u2019Connell", "D.B. Walther"], "venue": "Journal of Vision, vol. 15, no. 5, p. 20, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting eye fixations using convolutional neural networks", "author": ["N. Liu", "J. Han", "D. Zhang", "S. Wen", "T. Liu"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Prediction of search targets from fixations in open-world settings", "author": ["H. Sattar", "S. M\u00fcller", "M. Fritz", "A. Bulling"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of scores, datasets, and models in visual saliency prediction", "author": ["A. Borji", "H. Tavakoli", "D. Sihite", "L. Itti"], "venue": "IEEE International Conference on Computer Vision, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Centreof-gravity fixations in visual search: When looking at nothing helps to find something", "author": ["D. Venini", "R.W. Remington", "G. Horstmann", "S.I. Becker"], "venue": "Journal of Ophthalmology, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Eye movement control during visual object processing: effects of initial fixation position and semantic constraint.", "author": ["J.M. Henderson"], "venue": "Canadian Journal of Experimental Psychology,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "The influence of image characteristics on image recognition: a comparison of photographs and line drawings", "author": ["S. Heuer"], "venue": "Aphasiology, vol. 30, no. 8, pp. 943\u2013961, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Parallel object activation and attentional gating of information: Evidence from eye movements in the multiple object naming paradigm.", "author": ["E.R. Schotter", "V.S. Ferreira", "K. Rayner"], "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Cognitive determinants of fixation location during picture viewing.", "author": ["G.R. Loftus", "N.H. Mackworth"], "venue": "Journal of Experimental Psychology: Human perception and performance,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1978}, {"title": "The effects of semantic consistency on eye movements during complex scene viewing.", "author": ["J.M. Henderson", "P.A. Weeks Jr.", "A. Hollingworth"], "venue": "Journal of experimental psychology: Human perception and performance,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "The relation between gaze behavior and categorization: Does where we look determine what we see?", "author": ["M.O. Hartendorp", "S. Van der Stigchel", "I. Hooge", "J. Mostert", "T. de Boer", "A. Postma"], "venue": "Journal of Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Predicting human gaze beyond pixels", "author": ["J. Xu", "M. Jiang", "S. Wang", "M.S. Kankanhalli", "Q. Zhao"], "venue": "Journal of vision, vol. 14, no. 1, pp. 28\u201328, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "How do humans sketch objects?", "author": ["M. Eitz", "J. Hays", "M. Alexa"], "venue": "ACM Trans. Graph. (Proc. SIGGRAPH),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations", "author": ["S.S.S. Kruthiventi", "K. Ayush", "R. Venkatesh Babu"], "venue": "CoRR, vol. abs/1510.02927, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Cat2000: A large scale fixation dataset for boosting saliency research", "author": ["A. Borji", "L. Itti"], "venue": "CoRR, vol. abs/1505.03581, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Saliency unified: A deep architecture for simultaneous eye fixation prediction and salient object segmentation", "author": ["S.S. Kruthiventi", "V. Gudisa", "J.H. Dholakiya", "R. Venkatesh Babu"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Analyzing structural characteristics of object category representations from their semanticpart distributions", "author": ["R.K. Sarvadevabhatla", "R. Venkatesh Babu"], "venue": "ACM Multimedia Conference, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Where do people draw lines?", "author": ["F. Cole", "A. Golovinskiy", "A. Limpaecher", "H.S. Barros", "A. Finkelstein", "T. Funkhouser", "S. Rusinkiewicz"], "venue": "in ACM Transactions on Graphics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Impact of product design representation on customer judgment", "author": ["T.N. Reid", "E.F. MacDonald", "P. Du"], "venue": "Journal of Mechanical Design, vol. 135, no. 9, p. 091008, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Swiden: Convolutional neural networks for depiction invariant object recognition", "author": ["R.K. Sarvadevabhatla", "S. Surya", "S.S.S. Kruthiventi", "R. Venkatesh Babu"], "venue": "ACM Multimedia Conference, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to predict sequences of human visual fixations", "author": ["M. Jiang", "X. Boix", "G. Roig", "J. Xu", "L. Van Gool", "Q. Zhao"], "venue": "IEEE transactions on neural networks and learning systems, vol. 27, no. 6, pp. 1241\u20131252, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Visuomotor characterization of eye movements in a drawing task", "author": ["R. Coen-Cagli", "P. Coraggio", "P. Napoletano", "O. Schwartz", "M. Ferraro", "G. Boccignone"], "venue": "Vision Research, vol. 49, no. 8, pp. 810 \u2013 818, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Segmentation and accuracy in copying and drawing: Experts and beginners", "author": ["J. Tchalenko"], "venue": "Vision Research, vol. 49, no. 8, pp. 791 \u2013 800, 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Chapter 29 - absence of scene context effects in object detection and eye gaze capture", "author": ["L. Gareze", "J.M. Findlay"], "venue": "Eye Movements. Oxford: Elsevier, 2007, pp. 617 \u2013 637.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Looking at anything that is green when hearing frog: How object surface colour and stored object colour knowledge influence language-mediated overt attention", "author": ["F. Huettig", "G.T. Altmann"], "venue": "The Quarterly Journal of Experimental Psychology, vol. 64, no. 1, pp. 122\u2013145, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Attentional orienting and scene semantics", "author": ["P. De Graef", "J. Lauwereyns", "K. Verfaillie"], "venue": "Psychological Reports, no. 268, p. 2, 2000.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Fixations on objects in natural scenes: dissociating importance from salience", "author": ["B.M. t Hart", "H.C.E.F. Schmidt", "C. Roth", "W. Einhauser"], "venue": "Frontiers in psychology, vol. 4, p. 455, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Object-based saccadic selection during scene perception: Evidence from viewing position effects", "author": ["M. Pajak", "A. Nuthmann"], "venue": "Journal of vision, vol. 13, no. 5, pp. 2\u20132, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Computational visual attention systems and their cognitive foundations: A survey", "author": ["S. Frintrop", "E. Rome", "H.I. Christensen"], "venue": "ACM Transactions on Applied Perception (TAP), vol. 7, no. 1, p. 6, 2010.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Eye of the dragon: Exploring discriminatively minimalist sketch-based abstractions for object categories", "author": ["R.K. Sarvadevabhatla", "R. Venkatesh Babu"], "venue": "ACM Multimedia Conference, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Sun: A bayesian framework for saliency using natural statistics", "author": ["L. Zhang", "M.H. Tong", "T.K. Marks", "H. Shan", "G.W. Cottrell"], "venue": "Journal of Vision, vol. 8, no. 7, p. 32, 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Studying relationships between human gaze, description, and computer vision", "author": ["K. Yun", "Y. Peng", "D. Samaras", "G.J. Zelinsky", "T. Berg"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 739\u2013746.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "A general method applicable to the search for similarities in the amino acid sequence of two proteins", "author": ["S.B. Needleman", "C.D. Wunsch"], "venue": "Journal of Molecular Biology, vol. 48, no. 3, pp. 443 \u2013 453, 1970.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1970}, {"title": "Data-driven segmentation and labeling of freehand sketches", "author": ["Z. Huang", "H. Fu", "R.W.H. Lau"], "venue": "ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2014), 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Training object class detectors from eye tracking data", "author": ["D.P. Papadopoulos", "A.D.F. Clarke", "F. Keller", "V. Ferrari"], "venue": "European Conference on Computer Vision, 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Intra-category sketchbased image retrieval by matching deformable part models", "author": ["Y. Li", "T. Hospedales", "Y.-Z. Song", "S. Gong"], "venue": "British Machine Vision Conference, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimal decoding of linear codes for minimizing symbol error rate (corresp.)", "author": ["L. Bahl", "J. Cocke", "F. Jelinek", "J. Raviv"], "venue": "IEEE Transactions on Information Theory, vol. 20, no. 2, pp. 284\u2013287, 1974.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1974}, {"title": "Bridging viterbi and posterior decoding: a generalized risk approach to hidden path inference based on hidden markov models", "author": ["J. Lember", "A.A. Koloydenko"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1\u201358, 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence labeling: Generative and discriminative approaches", "author": ["H. Erdogan"], "venue": "iCMLA 2010 Tutorial. http://www.icmla-conference.org/ icmla10/CFP Tutorial files/hakan.pdf.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Introduction to Statistical Pattern Recognition (2Nd Ed.)", "author": ["K. Fukunaga"], "venue": "San Diego, CA, USA: Academic Press Professional,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1990}, {"title": "Algorithms for maximumlikelihood bandwidth selection in kernel density estimators", "author": ["J.M. Leiva-Murillo", "A. Art\u00e9s-Rodr\u0131\u0301guez"], "venue": "Pattern Recognition Letters, vol. 33, no. 13, pp. 1717\u20131724, 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Objects predict fixations better than early saliency", "author": ["W. Einhauser", "M. Spain", "P. Perona"], "venue": "Journal of Vision, vol. 8, no. 14, pp. 1\u201326, 2008.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Object-based attentional selection in scene viewing", "author": ["A. Nuthmann", "J.M. Henderson"], "venue": "Journal of vision, vol. 10, no. 8, p. 20, 2010.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Visual scenes are categorized by function.", "author": ["M.R. Greene", "C. Baldassano", "A. Esteva", "D.M. Beck", "L. Fei-Fei"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "Simple line drawings suffice for functional mri decoding of natural scene categories", "author": ["D.B. Walther", "B. Chai", "E. Caddigan", "D.M. Beck", "L. Fei-Fei"], "venue": "Proceedings of the National Academy of Sciences, vol. 108, no. 23, pp. 9661\u20139666, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Saliency filters: Contrast based filtering for salient region detection", "author": ["F. Perazzi", "P. Kr\u00e4henb\u00fchl", "Y. Pritch", "A. Hornung"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2012.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}, {"title": "Can computers learn from humans to see better?: inferring scene semantics from viewers\u2019 eye movements", "author": ["R. Subramanian", "V. Yanulevskaya", "N. Sebe"], "venue": "ACM Multimedia Conference, 2011.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2011}, {"title": "13-part-based representations of visual shape and implications for visual cognition", "author": ["M. Singh", "D.D. Hoffman"], "venue": "Advances in psychology, vol. 130, pp. 401\u2013459, 2001.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2001}, {"title": "Object representations in ventral and dorsal visual streams: fmri repetition effects depend on attention and part\u2013whole configuration", "author": ["V. Thoma", "R.N. Henson"], "venue": "Neuroimage, vol. 57, no. 2, pp. 513\u2013525, 2011.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Cviu special issue on parts and attributes: Mid-level representation for object recognition, scene classification and object detection", "author": ["T. Darrell", "V. Ferrari", "F. Jurie", "V. Lepetit"], "venue": "Computer Vision and Image Understanding, vol. 138, pp. 85 \u2013, 2015.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2015}, {"title": "An algebra for the analysis of object encoding", "author": ["C.W. Tyler", "L.T. Likova"], "venue": "NeuroImage, vol. 50, no. 3, pp. 1243\u20131250, 2010.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "Hierarchical structure in perceptual representation", "author": ["S.E. Palmer"], "venue": "Cognitive Psychology, vol. 9, no. 4, pp. 441 \u2013 474, 1977.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1977}, {"title": "Scrutinizing visual images: The role of gaze in mental imagery and memory", "author": ["B. Laeng", "I.M. Bloem", "S. DAscenzo", "L. Tommasi"], "venue": "Cognition, vol. 131, no. 2, pp. 263\u2013283, 2014.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Fixation location and fixation duration as indices of cognitive processing", "author": ["D.E. Irwin"], "venue": "The interface of language, vision, and action: Eye movements and the visual world, pp. 105\u2013134, 2004.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2004}, {"title": "Regarding scenes", "author": ["J.M. Henderson"], "venue": "Current directions in psychological science, vol. 16, no. 4, pp. 219\u2013222, 2007.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2007}, {"title": "Quantifying the contribution of low-level saliency to human eye movements in dynamic scenes", "author": ["L. Itti"], "venue": "Visual Cognition, vol. 12, pp. 1093\u2013 1123, 2005.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2005}, {"title": "The cross-depiction problem: Computer vision algorithms for recognising objects in artwork and in photographs", "author": ["H. Cai", "Q. Wu", "T. Corradi", "P. Hall"], "venue": "CoRR, vol. abs/1505.00110, 2015.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Multiple studies [1]\u2013[5] have demonstrated that this fixation mechanism is bottom-up, predominantly driven by image content and richness of detail (color, texture etc.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "Multiple studies [1]\u2013[5] have demonstrated that this fixation mechanism is bottom-up, predominantly driven by image content and richness of detail (color, texture etc.", "startOffset": 21, "endOffset": 24}, {"referenceID": 5, "context": "nothing\u201d phenomenon [6]\u2013[9], wherein the eye fixations on the same stimulus by multiple subjects fall on empty regions, yet exhibit enough regularity to make gaze-based inferences.", "startOffset": 20, "endOffset": 23}, {"referenceID": 8, "context": "nothing\u201d phenomenon [6]\u2013[9], wherein the eye fixations on the same stimulus by multiple subjects fall on empty regions, yet exhibit enough regularity to make gaze-based inferences.", "startOffset": 24, "endOffset": 27}, {"referenceID": 9, "context": "semantics [10] and the regularity in rest of the fixations is a statistical anomaly.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "However, a more intriguing explanation is that these empty region fixations aim to implicitly verify the overall consistency of the scene content depicted in the sketch [11], [12].", "startOffset": 169, "endOffset": 173}, {"referenceID": 11, "context": "However, a more intriguing explanation is that these empty region fixations aim to implicitly verify the overall consistency of the scene content depicted in the sketch [11], [12].", "startOffset": 175, "endOffset": 179}, {"referenceID": 13, "context": "[14] used in our eye-fixation user study.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "predicted from gaze patterns alone [2], [8].", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "predicted from gaze patterns alone [2], [8].", "startOffset": 40, "endOffset": 43}, {"referenceID": 12, "context": "Do the gaze patterns exhibit correlation with any object-level attributes such as semantic-parts? [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "To examine these questions and related issues, we conducted an eye fixation study on a large database containing handdrawn sketches of objects across 160 categories [14].", "startOffset": 165, "endOffset": 169}, {"referenceID": 14, "context": "(3904 compared to the previous largest set of 200) can be used in benchmarking generalized1 saliency prediction models [15]\u2013[17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 16, "context": "(3904 compared to the previous largest set of 200) can be used in benchmarking generalized1 saliency prediction models [15]\u2013[17].", "startOffset": 124, "endOffset": 128}, {"referenceID": 17, "context": "\u2022 We map eye fixation sequences to semantic object-part label sequences by utilizing part contour annotations of sketches [18].", "startOffset": 122, "endOffset": 126}, {"referenceID": 18, "context": "underlying category in 3D model renderings [19], art displays,", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "[20] use eye-gaze data to interpret customer selections among sketch-like consumer product renderings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "the complexity of the study design [20].", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "Combined with data from similar studies on other modalities [13], our sketch gaze data can contribute towards new insights into cross-modal (photo, brush art, line drawing)", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "object representations [21].", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "The study of eye-fixations on images is a very active research area in the inter-related fields of neuroscience and computer vision [1]\u2013[5], [22].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "The study of eye-fixations on images is a very active research area in the inter-related fields of neuroscience and computer vision [1]\u2013[5], [22].", "startOffset": 136, "endOffset": 139}, {"referenceID": 21, "context": "The study of eye-fixations on images is a very active research area in the inter-related fields of neuroscience and computer vision [1]\u2013[5], [22].", "startOffset": 141, "endOffset": 145}, {"referenceID": 22, "context": "The first category of studies use gaze tracking to understand how people copy and draw line-drawings and simple shapes [23], [24].", "startOffset": 119, "endOffset": 123}, {"referenceID": 23, "context": "The first category of studies use gaze tracking to understand how people copy and draw line-drawings and simple shapes [23], [24].", "startOffset": 125, "endOffset": 129}, {"referenceID": 24, "context": "perception of photographic image content and corresponding line-drawing representations [25], [26].", "startOffset": 88, "endOffset": 92}, {"referenceID": 25, "context": "perception of photographic image content and corresponding line-drawing representations [25], [26].", "startOffset": 94, "endOffset": 98}, {"referenceID": 26, "context": "In the third category, gaze tracking is used to characterize semantic plausibility of objects in line-drawings of scenes [27].", "startOffset": 121, "endOffset": 125}, {"referenceID": 6, "context": "The fourth category of studies use gaze tracking to explore sketch-like depictions of objects [7]\u2013[9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "The fourth category of studies use gaze tracking to explore sketch-like depictions of objects [7]\u2013[9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 27, "context": "Although objects make the scenes meaningful in many instances [28], [29], their role has been studied in a limited con-", "startOffset": 62, "endOffset": 66}, {"referenceID": 28, "context": "Although objects make the scenes meaningful in many instances [28], [29], their role has been studied in a limited con-", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] summarize prevailing theories of the relationship between attention and object recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Recently, Ali and Itti [16] constructed a large-scale dataset", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "for sketch images originally belonging to the same sketch dataset we have used [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 30, "context": "For our analysis, we used 3904 sketches spread across 160 object categories studied in the work of Sarvadevabhatla and Babu [31].", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "[14] used to construct epitomic versions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "However, the images were chosen only from 13 selected categories for which semantic object-part annotations exist [18].", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "sketch) displaying the sketch\u2019s category as a text string representing the category label as provided by the creators of the freehand sketch dataset [14].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "Fixation maps summarize the fixation information averaged over multiple subjects viewing the same image (sketch) and are used as ground-truth for evaluating saliency prediction methods [1], [3], [15], [16].", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "Fixation maps summarize the fixation information averaged over multiple subjects viewing the same image (sketch) and are used as ground-truth for evaluating saliency prediction methods [1], [3], [15], [16].", "startOffset": 190, "endOffset": 193}, {"referenceID": 14, "context": "Fixation maps summarize the fixation information averaged over multiple subjects viewing the same image (sketch) and are used as ground-truth for evaluating saliency prediction methods [1], [3], [15], [16].", "startOffset": 195, "endOffset": 199}, {"referenceID": 15, "context": "Fixation maps summarize the fixation information averaged over multiple subjects viewing the same image (sketch) and are used as ground-truth for evaluating saliency prediction methods [1], [3], [15], [16].", "startOffset": 201, "endOffset": 205}, {"referenceID": 1, "context": "[2] which we summarize next.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "(1) Typical approaches normalize F \u2032 with respect to the maximum value or to lie between 0 and 1 [1], [16].", "startOffset": 97, "endOffset": 100}, {"referenceID": 15, "context": "(1) Typical approaches normalize F \u2032 with respect to the maximum value or to lie between 0 and 1 [1], [16].", "startOffset": 102, "endOffset": 106}, {"referenceID": 1, "context": "[2] suggest standardizing F \u2032 to obtain a zero mean, unit standard deviation map F .", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "[32]", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "A high value for IOC indicates consistency among the fixation sequences and is commonly observed for natural images, particularly with a central object [1].", "startOffset": 152, "endOffset": 155}, {"referenceID": 32, "context": "[33], \u201cfunctional\u201d categories (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "observed for regular image datasets as well [1], [16].", "startOffset": 44, "endOffset": 47}, {"referenceID": 15, "context": "observed for regular image datasets as well [1], [16].", "startOffset": 49, "endOffset": 53}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] averages around 35% (only 6 (scene) categories, 22 subjects, 216 photographic images).", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "To verify this hypothesis, we utilized object-part contour annotations available for the sketches viewed in primed regime [18].", "startOffset": 122, "endOffset": 126}, {"referenceID": 33, "context": "To determine similarity between pairs of part-name token sequences, we used the Needleman-Wunsch algorithm [34] with a 0 \u2212 1 cost model (Cost(P,Q) = 0 if Part P = Part Q, 1 otherwise) and a gap penalty of 0.", "startOffset": 107, "endOffset": 111}, {"referenceID": 34, "context": "Such predictions could enable applications such as object-part contour annotations of freehand sketches [35] from fixations alone7, in analogy with the object-bounding-boxesfrom-eye-fixations approaches described in Papadopoulos et", "startOffset": 104, "endOffset": 108}, {"referenceID": 35, "context": "[36] and Yun et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "In addition, the additional part presence information (available for sketches annotated via fixations) can be utilized for refining results of sketch-based image retrieval approaches [37].", "startOffset": 183, "endOffset": 187}, {"referenceID": 37, "context": "Essentially, PMAP maximizes the posterior probability p(li|A) of each individual hidden state given the entire observation sequence [38].", "startOffset": 132, "endOffset": 136}, {"referenceID": 38, "context": "Our choice of PMAP is motivated by the fact that PMAP maximizes the expected number of correctly estimated states and hence provides better state estimates overall compared to Viterbi decoding [39].", "startOffset": 193, "endOffset": 197}, {"referenceID": 39, "context": "In such a scenario, the maximum-likelihood training of HMMs reduces to a counting process wherein the observation, transition and initial state models can be modeled and estimated independently [40].", "startOffset": 194, "endOffset": 198}, {"referenceID": 40, "context": "In particular, we model the distribution via Kernel Density Estimation [41] with the bandwidth automatically selected using single-dimensional likelihood-based search [42].", "startOffset": 71, "endOffset": 75}, {"referenceID": 41, "context": "In particular, we model the distribution via Kernel Density Estimation [41] with the bandwidth automatically selected using single-dimensional likelihood-based search [42].", "startOffset": 167, "endOffset": 171}, {"referenceID": 21, "context": "[22], related to learning an explicit prediction model for eye-fixation sequences, would", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43] and Nuthmann et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[44] who posit that humans more likely than not, represent and understand visual content in terms of objects (although", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "contradictory results have been reported with non-fixation based approaches [45]).", "startOffset": 76, "endOffset": 80}, {"referenceID": 1, "context": "This is in line with similar studies on photographic images [2].", "startOffset": 60, "endOffset": 63}, {"referenceID": 45, "context": "fMRI-based studies have shown that the neural response of the visual system to images of a scene and line drawings depicting the scene category is virtually the same [46].", "startOffset": 166, "endOffset": 170}, {"referenceID": 46, "context": "Current theories and computational models of saliency rely on features specific to photo images such as color-contrast and edge-contrast [47].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "Combined with data from similar studies on other modalities [13], our sketch gaze data", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "can contribute towards new insights into cross-modal (photo, brush art, line drawing) object representations [21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 47, "context": "[48] utilize eye gaze", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "Part-based representations of objects, semantic or otherwise, are well supported by multiple studies in neuroscience and computer vision [49]\u2013[51].", "startOffset": 137, "endOffset": 141}, {"referenceID": 50, "context": "Part-based representations of objects, semantic or otherwise, are well supported by multiple studies in neuroscience and computer vision [49]\u2013[51].", "startOffset": 142, "endOffset": 146}, {"referenceID": 51, "context": "[52] suggest that humans tap into generic concepts", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "Furthermore, studies by Palmer [53] have shown that when parts correspond to a \u2018good\u2019 segmentation of a figure (e.", "startOffset": 31, "endOffset": 35}, {"referenceID": 53, "context": "These studies [54] suggest that the eye fixations are an external manifestation which aim to verify an implicit, internal part-based representation for the object.", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": "[13] perform a study which evaluates the ability of eye gaze statistics in predicting attributes of objects such as shape and size.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "importance among the parts [12].", "startOffset": 27, "endOffset": 31}, {"referenceID": 34, "context": "exploiting part-whole object semantics) can enable object-part contour annotations of freehand sketches [35] as mentioned previously (Section IX).", "startOffset": 104, "endOffset": 108}, {"referenceID": 1, "context": "indicated in eye-fixation and saliency literature [2], [48], [55].", "startOffset": 50, "endOffset": 53}, {"referenceID": 47, "context": "indicated in eye-fixation and saliency literature [2], [48], [55].", "startOffset": 55, "endOffset": 59}, {"referenceID": 54, "context": "indicated in eye-fixation and saliency literature [2], [48], [55].", "startOffset": 61, "endOffset": 65}, {"referenceID": 55, "context": "In fact, Henderson [56] argues that a complete model of", "startOffset": 19, "endOffset": 23}, {"referenceID": 56, "context": "However, duration is typically not considered essential in most saliency prediction approaches [57].", "startOffset": 95, "endOffset": 99}, {"referenceID": 35, "context": "More broadly, our sketch object eye-fixations data lays the ground for uncovering connections between eye fixation patterns on objects in photographic images [36] and their sketched versions.", "startOffset": 158, "endOffset": 162}, {"referenceID": 57, "context": "new insights into cross-modal (photo, brush art, line drawing) object representations [58].", "startOffset": 86, "endOffset": 90}], "year": 2017, "abstractText": "The study of eye gaze fixations on photographic images is an active research area. In contrast, the image subcategory of freehand sketches has not received as much attention for such studies. In this paper, we analyze the results of a freeviewing gaze fixation study conducted on 3904 freehand sketches distributed across 160 object categories. Our analysis shows that fixation sequences exhibit marked consistency within a sketch, across sketches of a category and even across suitably grouped sets of categories. This multi-level consistency is remarkable given the variability in depiction and extreme image content sparsity that characterizes hand-drawn object sketches. In our paper, we show that the multi-level consistency in the fixation data can be exploited to (a) predict a test sketch\u2019s category given only its fixation sequence and (b) build a computational model which predicts part-labels underlying fixations on objects. We hope that our findings motivate the community to deem sketch-like representations worthy of gaze-based studies vis-avis photographic images.", "creator": "LaTeX with hyperref package"}}}