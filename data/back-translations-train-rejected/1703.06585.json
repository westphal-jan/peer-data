{"id": "1703.06585", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning", "abstract": "We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward.", "histories": [["v1", "Mon, 20 Mar 2017 03:50:57 GMT  (5750kb,D)", "http://arxiv.org/abs/1703.06585v1", "11 pages, 4 figures, 2 tables, webpage:this http URL"], ["v2", "Tue, 21 Mar 2017 17:41:23 GMT  (5750kb,D)", "http://arxiv.org/abs/1703.06585v2", "11 pages, 4 figures, 2 tables, webpage:this http URL"]], "COMMENTS": "11 pages, 4 figures, 2 tables, webpage:this http URL", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.LG", "authors": ["abhishek das", "satwik kottur", "jos\\'e m f moura", "stefan lee", "dhruv batra"], "accepted": false, "id": "1703.06585"}, "pdf": {"name": "1703.06585.pdf", "metadata": {"source": "CRF", "title": "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning", "authors": ["Abhishek Das", "Satwik Kottur", "Jos\u00e9 M.F. Moura", "Stefan Lee", "Dhruv Batra"], "emails": [], "sections": [{"heading": null, "text": "We introduce the first targeted training for visual questioners and dialogue agents. Specifically, we introduce a cooperative \"image guessing game\" between two agents - Q-BOT and A-BOT - who communicate in dialogue with natural language, so that Q-BOT can select an invisible image from a series of images. We use deep reinforcement learning (RL) to learn the guidelines of these agents end-to-end - from pixels to multi-agent multiround dialogues with game rewards. We show two experimental results: First, as a demonstration of pure RL (from the ground up), we show results in a synthetic world where agents communicate in non-round vocabularies, i.e. symbols without predefined meanings (X, Y, Z). We find that two bots invent their own communication protocol (from the ground up) and begin to use certain symbols to answer certain visual attributes (shape / size)."}, {"heading": "1. Introduction", "text": "In fact, it is the case that the two are a very complex story, in which it is a matter of putting people in the centre. (...) It is not the case that people are pushed into the background. (...) It is the case that people are pushed into the background. (...) It is the case that people are pushed into the background. (...) It is the case that people are pushed into the background. (...) It is the case that people are pushed into the background. (...) It is the case that people are pushed into the background. (...) It is the case that people are pushed into the background. (...) It is the case that people are pushed into the background. (...) It is the case that people are pushed into the background. (...) It is the case that people are pushed into the centre. (...) It is the case that people are put into the centre. (...) It is the case that people are put into the centre."}, {"heading": "2. Related Work", "text": "In fact, most of them will be able to put themselves centre stage without having to put themselves centre stage."}, {"heading": "3. Cooperative Image Guessing Game:", "text": "The game includes two cooperative actors - a questioner bot (Q-BOT) and an answer bot (ABOT) - with an information asymmetry. A-BOT sees an image I, Q-BOT does not. Q-BOT is primed with a 1-sentence description c of the invisible image and asks \"questions\" (sequence of discrete symbols via a vocabulary V), which ABOT answers with a different sequence of symbols. Communication takes place for a certain number of rounds. Game object in general. In addition to communication, Q-BOT must provide a \"description\" of the unknown image I, which is based only on the dialog story, and both players receive a reward from the environment that is inversely proportional to the error in this description."}, {"heading": "4. Reinforcement Learning for Dialog Agents", "text": "In this case, it is as if it is a matter of a way in which people are able to put themselves and the world in the centre. (...) It is not as if people in the world are able to understand the world. (...) It is as if people in the world are able to understand the world. (...) It is as if people in the world put themselves in the centre. (...) It is as if people in the world are placed in the centre. (...) It is as if people are placed in the centre of the world. (...) It is as if people are placed in the centre of the world. (...) It is as if people are placed in the centre of the world. (...) It is as if people are placed in the centre of the world. (...) It is as if people are placed in the centre of the world. (...) It is as if people are placed in the centre of the world. (...) It is as if people are placed in the centre. (...) It is as if people are placed in the centre. (...) It is as if people are placed in the centre of the world. (...) It is as if people are placed in the centre of the world. (...)."}, {"heading": "4.2. Joint Training with Policy Gradients", "text": "To train these agents, we use the REINFORCE [34] algorithm, which updates the political parameters (\u03b8Q, \u03c0A, \u03b8f) in response to experience and effectiveness. Remember that our agents take measures - communication (qt, at) and prediction (feature prediction y) - and that our goal is to maximize the expected reward according to the agents \"strategies, summed up throughout the dialogue: min \u03b8A, \u03b8g J (\u03b8A, \u03b8Q, \u03b8g), where (3) J (\u03b8A, \u03c0Q, \u03b8srt) = E perspecsQ [T = 1 rt (sQt, yt)]] (4) While the above goal is a natural one, we note that viewing the entire dialogue as a single RL episode does not make any difference between individual good or bad expeditions within the same."}, {"heading": "5. Emergence of Grounded Dialog", "text": "To succeed, Q-BOT and A-BOT must complete a series of challenging sub-tasks - they must learn a common language (do you understand what I mean when I say \"person\"?) and develop mappings between symbols and image representations (what does \"person\" mean?), i.e. we must learn to learn the language in visual perception in order to answer questions, and QBOT must learn to anticipate plausible image representations - all in a way that assumes a remote reward function. Before diving into the full task on real images, we perform a \"sanitary check\" on a synthetic dataset with perfect perception, it is even possible to do so. As shown in Fig 3, we consider a synthetic world with images represented as a triplet of attributes - 4 shapes, 4 colors, 4 styles - for a total of 64 unique images."}, {"heading": "6. Experiments", "text": "This year, the time has come for us to be able to go to a place where we can go to a place where we are able to move."}, {"heading": "7. Conclusions", "text": "In summary, we present a novel educational framework for visually grounded dialogue agents by organizing a cooperative \"picture-guessing game\" between two agents. We use in-depth reinforcement learning to learn the policies of these agents end-to-end - from pixels to multi-agent multiround dialogues to game rewards. We find that two bots invent their own communication protocol without human supervision. We start this game on the VisDial [4] dataset, where we pretrain with supervised dialogue data. We find that Amazon's finely tuned agents are not only significantly better than SL agents, but learn to play each other's strengths while remaining open to outsiders to interpretation.We thank Devi Parikh for helpful discussions."}], "references": [{"title": "VQA: Visual Question Answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "ICCV,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "VizWiz: Nearly Real-time Answers to Visual Questions", "author": ["J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "R.C. Miller", "R. Miller", "A. Tatarowicz", "B. White", "S. White", "T. Yeh"], "venue": "UIST,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Mind\u2019s Eye: A Recurrent Visual Representation for Image Caption Generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "CVPR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual Dialog", "author": ["A. Das", "S. Kottur", "K. Gupta", "A. Singh", "D. Yadav", "J.M. Moura", "D. Parikh", "D. Batra"], "venue": "CVPR,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "GuessWhat?! visual object discovery through multi-modal dialogue", "author": ["H. de Vries", "F. Strub", "S. Chandar", "O. Pietquin", "H. Larochelle", "A. Courville"], "venue": "In CVPR, 2017", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2017}, {"title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "From Captions to Visual Concepts and Back", "author": ["H. Fang", "S. Gupta", "F.N. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "NIPS,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative Adversarial Nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Emergence of language with multiagent games: Learning to communicate with sequences of symbols", "author": ["S. Havrylov", "I. Titov"], "venue": "ICLR Workshop,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes", "author": ["S. Kazemzadeh", "V. Ordonez", "M. Matten", "T.L. Berg"], "venue": "EMNLP,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-agent cooperation and the emergence of (natural) language", "author": ["A. Lazaridou", "A. Peysakhovich", "M. Baroni"], "venue": "10  ICLR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "Convention: A philosophical study", "author": ["D. Lewis"], "venue": "John Wiley & Sons,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep Reinforcement Learning for Dialogue Generation", "author": ["J. Li", "W. Monroe", "A. Ritter", "M. Galley", "J. Gao", "D. Jurafsky"], "venue": "EMNLP,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial learning for neural dialogue generation", "author": ["J. Li", "W. Monroe", "T. Shi", "A. Ritter", "D. Jurafsky"], "venue": "arXiv preprint arXiv:1701.06547,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Emergence of grounded compositional language in multi-agent populations", "author": ["I. Mordatch", "P. Abbeel"], "venue": "arXiv preprint arXiv:1703.04908,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}, {"title": "Evolution of Communication and Language in Embodied Agents", "author": ["S. Nolfi", "M. Mirolli"], "venue": "Springer Publishing Company, Incorporated, 1st edition,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Exploring Models and Data for Image Question Answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A. Courville", "J. Pineau"], "venue": "AAAI,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues", "author": ["I.V. Serban", "A. Sordoni", "R. Lowe", "L. Charlin", "J. Pineau", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1605.06069,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree search. Nature, 2016", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "MovieQA: Understanding Stories in Movies through Question-Answering", "author": ["M. Tapaswi", "Y. Zhu", "R. Stiefelhagen", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "CVPR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint Video and Text Parsing for Understanding Events and Answering Queries", "author": ["K. Tu", "M. Meng", "M.W. Lee", "T.E. Choe", "S.C. Zhu"], "venue": "IEEE MultiMedia,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to Sequence - Video to Text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R.J. Mooney", "T. Darrell", "K. Saenko"], "venue": "ICCV,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating Videos to Natural Language Using Deep Recurrent Neural Networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R.J. Mooney", "K. Saenko"], "venue": "NAACL HLT,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning, 8(3-4):229\u2013256,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1992}, {"title": "Using artificial intelligence to help blind people \u2018see\u2019 facebook", "author": ["S. Wu", "H. Pique", "J. Wieland"], "venue": "http://newsroom.fb.com/news/2016/04/using-artificialintelligence-to-help-blind-people-see-facebook/,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Second, we conduct large-scale real-image experiments on the VisDial dataset [4], where we pretrain with supervised dialog data and show that the RL \u2018fine-tuned\u2019 agents significantly outperform SL agents.", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": ", helping visually impaired users understand their surroundings [2] or social media content [35] (\u2018Who is in the photo? Dave.", "startOffset": 64, "endOffset": 67}, {"referenceID": 34, "context": ", helping visually impaired users understand their surroundings [2] or social media content [35] (\u2018Who is in the photo? Dave.", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 108, "endOffset": 127}, {"referenceID": 10, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 108, "endOffset": 127}, {"referenceID": 32, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 108, "endOffset": 127}, {"referenceID": 35, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 108, "endOffset": 127}, {"referenceID": 0, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 151, "endOffset": 170}, {"referenceID": 19, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 151, "endOffset": 170}, {"referenceID": 22, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 151, "endOffset": 170}, {"referenceID": 28, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 151, "endOffset": 170}, {"referenceID": 29, "context": "Despite rapid progress at the intersection of vision and language, in particular, in image/video captioning [3, 11, 31\u2013 33, 36] and question answering [1, 20, 23, 29, 30], it is clear we are far from this grand goal of a visual dialog agent.", "startOffset": 151, "endOffset": 170}, {"referenceID": 3, "context": "Two recent works [4, 5] have proposed studying this task of visually-grounded dialog.", "startOffset": 17, "endOffset": 23}, {"referenceID": 4, "context": "Two recent works [4, 5] have proposed studying this task of visually-grounded dialog.", "startOffset": 17, "endOffset": 23}, {"referenceID": 3, "context": "works [4, 5] first collect a dataset of human-human dialog, i.", "startOffset": 6, "endOffset": 12}, {"referenceID": 4, "context": "works [4, 5] first collect a dataset of human-human dialog, i.", "startOffset": 6, "endOffset": 12}, {"referenceID": 0, "context": "Despite significant popular interest in VQA (over 200 works citing [1] since 2015), all previous approaches have been based on supervised learning, making this the first instance of goaldriven training for visual question answering / dialog.", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "Second, we conduct large-scale real-image experiments on the VisDial dataset [4].", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 19, "endOffset": 33}, {"referenceID": 6, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 19, "endOffset": 33}, {"referenceID": 11, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 19, "endOffset": 33}, {"referenceID": 32, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 19, "endOffset": 33}, {"referenceID": 0, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 71, "endOffset": 89}, {"referenceID": 7, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 71, "endOffset": 89}, {"referenceID": 18, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 71, "endOffset": 89}, {"referenceID": 19, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 71, "endOffset": 89}, {"referenceID": 22, "context": ", image captioning [6, 7, 12, 33], and visual question answering (VQA) [1, 8, 19, 20, 23].", "startOffset": 71, "endOffset": 89}, {"referenceID": 3, "context": "Most related to this paper are two recent works on visually-grounded dialog [4, 5].", "startOffset": 76, "endOffset": 82}, {"referenceID": 4, "context": "Most related to this paper are two recent works on visually-grounded dialog [4, 5].", "startOffset": 76, "endOffset": 82}, {"referenceID": 3, "context": "[4] proposed the task of Visual Dialog, collected the VisDial dataset by pairing two subjects on Amazon Mechanical Turk to chat about an image (with assigned roles of \u2018Questioner\u2019 and \u2018Answerer\u2019), and trained neural visual dialog answering models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] extended the Referit game [13] to a \u2018GuessWhat\u2019 game, where one person asks questions about an image to guess which object has been \u2018selected\u2019, and the second person answers questions in \u2018yes\u2019/\u2018no\u2019/NA (natural language answers are disallowed).", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[5] extended the Referit game [13] to a \u2018GuessWhat\u2019 game, where one person asks questions about an image to guess which object has been \u2018selected\u2019, and the second person answers questions in \u2018yes\u2019/\u2018no\u2019/NA (natural language answers are disallowed).", "startOffset": 30, "endOffset": 34}, {"referenceID": 25, "context": "In our work, we take inspiration from the AlphaGo [26] approach of supervision from human-expert games and reinforcement learning from self-play.", "startOffset": 50, "endOffset": 54}, {"referenceID": 15, "context": "More formally, it is a generalization of the Lewis Signaling (LS) [16] game, widely studied in economics and game theory.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "[17] have proposed using RL for training dialog systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In contrast, taking a cue from adversarial learning [9, 18], we set up a cooperative game between two agents, such that we do not need to hand-define what a \u2018good\u2019 dialog looks like \u2013 a \u2018good\u2019 dialog is one that leads to a successful image-guessing play.", "startOffset": 52, "endOffset": 59}, {"referenceID": 17, "context": "In contrast, taking a cue from adversarial learning [9, 18], we set up a cooperative game between two agents, such that we do not need to hand-define what a \u2018good\u2019 dialog looks like \u2013 a \u2018good\u2019 dialog is one that leads to a successful image-guessing play.", "startOffset": 52, "endOffset": 59}, {"referenceID": 21, "context": "There is a long history of work on language emergence in multi-agent systems [22].", "startOffset": 77, "endOffset": 81}, {"referenceID": 9, "context": "The more recent resurgence has focused on deep RL [10,15,21].", "startOffset": 50, "endOffset": 60}, {"referenceID": 14, "context": "The more recent resurgence has focused on deep RL [10,15,21].", "startOffset": 50, "endOffset": 60}, {"referenceID": 20, "context": "The more recent resurgence has focused on deep RL [10,15,21].", "startOffset": 50, "endOffset": 60}, {"referenceID": 3, "context": "For our large-scale real-image results, we do not want our bots to invent their own uninterpretable language and use pretraining on VisDial [4] to achieve \u2018alignment\u2019 with English.", "startOffset": 140, "endOffset": 143}, {"referenceID": 3, "context": "Both the agent policies are modeled via Hierarchical Recurrent Encoder-Decoder neural networks, which have recently been proposed for dialog modeling [4, 24, 25].", "startOffset": 150, "endOffset": 161}, {"referenceID": 23, "context": "Both the agent policies are modeled via Hierarchical Recurrent Encoder-Decoder neural networks, which have recently been proposed for dialog modeling [4, 24, 25].", "startOffset": 150, "endOffset": 161}, {"referenceID": 24, "context": "Both the agent policies are modeled via Hierarchical Recurrent Encoder-Decoder neural networks, which have recently been proposed for dialog modeling [4, 24, 25].", "startOffset": 150, "endOffset": 161}, {"referenceID": 26, "context": "- State/History Encoder is an LSTM that takes as input at each round t \u2013 the encoded question Qt , the image features from VGG [27] y, and the previous fact encoding F t\u22121 \u2013 to produce a state encoding, i.", "startOffset": 127, "endOffset": 131}, {"referenceID": 33, "context": "In order to train these agents, we use the REINFORCE [34] algorithm that updates policy parameters (\u03b8Q, \u03b8A, \u03b8f ) in response to experienced rewards.", "startOffset": 53, "endOffset": 57}, {"referenceID": 27, "context": "During training, we use -greedy policies [28], ensuring an action probability of 0.", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "Image + Caption Human-Human dialog [4] SL-pretrained Q-BOT-A-BOT dialog RL-full-QAf Q-BOT A-BOT dialog", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "We leverage the recently introduced VisDial dataset [4] that contains (as of the publicly released v0.", "startOffset": 52, "endOffset": 55}, {"referenceID": 26, "context": "3 \u2013 specifically at each round t, Q-BOT needs to regress to the vector embedding \u0177t of image I corresponding to the fc7 (penultimate fully-connected layer) output from VGG16 [27].", "startOffset": 174, "endOffset": 178}, {"referenceID": 3, "context": "We first train both agents in a supervised manner on the train split of VisDial [4] v0.", "startOffset": 80, "endOffset": 83}, {"referenceID": 13, "context": "We use Adam [14] with a learning rate of 10\u22123, and clamp gradients to [\u22125, 5] to avoid explosion.", "startOffset": 12, "endOffset": 16}, {"referenceID": 3, "context": "performance on VisDial dataset [4]).", "startOffset": 31, "endOffset": 34}, {"referenceID": 11, "context": "We present each image + an automatically generated caption [12] to the agents, and allow them to communicate over 10 rounds of dialog.", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "These observations are consistent with recent literature in text-only dialog [17].", "startOffset": 77, "endOffset": 81}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "We go on to instantiate this game on the VisDial [4] dataset, where we pretrain with supervised dialog data.", "startOffset": 49, "endOffset": 52}], "year": 2017, "abstractText": "We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative \u2018image guessing\u2019 game between two agents \u2013 Q-BOT and A-BOT\u2013 who communicate in natural language dialog so that Q-BOT can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end \u2013 from pixels to multi-agent multi-round dialog to game reward. We demonstrate two experimental results. First, as a \u2018sanity check\u2019 demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/size). Thus, we demonstrate the emergence of grounded language and communication among \u2018visual\u2019 dialog agents with no human supervision at all. Second, we conduct large-scale real-image experiments on the VisDial dataset [4], where we pretrain with supervised dialog data and show that the RL \u2018fine-tuned\u2019 agents significantly outperform SL agents. Interestingly, the RL Q-BOT learns to ask questions that A-BOT is good at, ultimately resulting in more informative dialog and a better team.", "creator": "LaTeX with hyperref package"}}}