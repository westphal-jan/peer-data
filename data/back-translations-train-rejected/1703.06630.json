{"id": "1703.06630", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "Automatic Text Summarization Approaches to Speed up Topic Model Learning Process", "abstract": "The number of documents available into Internet moves each day up. For this reason, processing this amount of information effectively and expressibly becomes a major concern for companies and scientists. Methods that represent a textual document by a topic representation are widely used in Information Retrieval (IR) to process big data such as Wikipedia articles. One of the main difficulty in using topic model on huge data collection is related to the material resources (CPU time and memory) required for model estimate. To deal with this issue, we propose to build topic spaces from summarized documents. In this paper, we present a study of topic space representation in the context of big data. The topic space representation behavior is analyzed on different languages. Experiments show that topic spaces estimated from text summaries are as relevant as those estimated from the complete documents. The real advantage of such an approach is the processing time gain: we showed that the processing time can be drastically reduced using summarized documents (more than 60\\% in general). This study finally points out the differences between thematic representations of documents depending on the targeted languages such as English or latin languages.", "histories": [["v1", "Mon, 20 Mar 2017 08:19:43 GMT  (547kb,D)", "http://arxiv.org/abs/1703.06630v1", "16 pages, 4 tables, 8 figures"]], "COMMENTS": "16 pages, 4 tables, 8 figures", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["mohamed morchid", "juan-manuel torres-moreno", "richard dufour", "javier ram\\'irez-rodr\\'iguez", "georges linar\\`es"], "accepted": false, "id": "1703.06630"}, "pdf": {"name": "1703.06630.pdf", "metadata": {"source": "CRF", "title": "Automatic Text Summarization Approaches to Speed up Topic Model Learning Process", "authors": ["Mohamed Morchid", "Juan-Manuel Torres-Moreno", "Richard Dufour", "Javier Ram\u0131\u0301rez-Rod\u0155\u0131guez", "Georges Linar\u00e8s"], "emails": ["firstname.lastname@univ-avignon.fr", "jararo@azc.uam.mx"], "sections": [{"heading": "1 Introduction", "text": "The number of documents available on the Internet is growing exponentially every day, so processing this amount of information effectively and expressively has become a major concern for businesses and academics. Finally, an important part of the information is conveyed through textual documents such as blogs or microblogs, general or promotional websites, and encyclopaedic documents. This last type of textual data increases every day with new articles that convey large and heterogeneous information. Wikipedia, the most famous and widely used collaborative Internet encyclopedia, is enriched by volunteers around the world. It is the 12th most visited website in the United States, with approximately 10.65 million users each day dealing with new articles that convey large and heterogeneous information. Wikipedia is the most famous and frequently used topic, enriched by volunteers around the world."}, {"heading": "2 Related work", "text": "Several methods have been proposed by Information Retrieval (IR) researchers to process large corpus of documents such as Wikipedia Encyclopedia. All these methods consider documents as a bag of words [1] in which the word order is not taken into account. However, this approach shows that each document is reduced from a discrete space (words and documents) to a vector of numerical values that is counted by the word (number of occurrences) in the document called TF-IDF [4]. This approach shows its effectiveness in various tasks, namely more precisely in the basic identification of discriminatory words for a document [5]. However, this method has many weaknesses, such as the reduction of the description length, or the weakness of the intrastatistical structure of documents in the texts."}, {"heading": "3 Overview of the proposed approach", "text": "Figure 1 describes the approach proposed in this paper to evaluate the quality of a topic model presentation with and without automatic text collection systems. The latent dirichlet allocation approach (LDA) described in detail in the next section is used for the topic presentation in conjunction with various state-of-the-art summary systems presented in Section 3.2."}, {"heading": "3.1 Topic representation: latent Dirichlet allocation", "text": "Unlike a multinomial hybrid model, LDA considers that a topic is associated with each occurrence of a word that composes the document, rather than linking a topic with the full document, which allows a document to move from one word to another. However, word occurrences are linked by a latent variable that controls the global distribution of the topics in the document. These latent topics are characterized by a distribution of the associated word probabilities. PLSA and LDA models have shown that they generally exceed LSA in IR tasks [14]. In addition, LDA provides a direct estimate of the relevance of a topic that knows a phrase. Figure 2 shows the LDA formalism. For each document d of a corpus D, a first parameter is drawn using a set of parameters."}, {"heading": "3.2 Automatic Text Summarization systems", "text": "Two basic systems as well as the ARTEX state-of-the-art summary system are therefore presented in this paragraph.Baseline first (or Leadbase) selects the n first sentences of the documents, in which n is determined by a compression rate. Although this method is a strong starting point for the performance of any automatic summary system [21, 22], this very old and very simple sentence weighting does not contain any terms at all: it attaches the highest weight to the first sentences of the text. Texts of some genres, such as news reports or scientific papers, are specifically designed for this heuristic topic: e.g., every scientific paper contains a ready-made summary at the beginning. This gives a baseline [23] that proves to be very hard to beat."}, {"heading": "4 Evaluation of LDA model quality", "text": "In the previous section, various summary systems were described to reduce the size of the train body and retain only relevant information in the train documents. In this section, a number of metrics are proposed to evaluate the quality of thematic spaces generated from summaries of the train documents. Firstly, helplessness. This value is the most popular. We also propose to examine another metric to evaluate the dispersion of each word into a specific theme space. This metric is called the Jensen-Shannon divergence (JS)."}, {"heading": "4.1 Perplexity", "text": "A subject model Z is effective if it can correctly predict an invisible document from the test collection. Perplexity used in language modeling decreases monotonously in the probability of the test data and is algebraically equivalent to the inversion of the geometric mean per word probability. A lower perplexity value indicates a better generalization performance [2]: Confusion topic (B) = Exp {\u2212 1 NB M \u2211 d = 1 logP (w)}} (6) withNB = M \u2211 d = 1 Nd (7), where NB is the combined length of all M test terms and Nd is the number of words in document zi; P (w) is the probability that the generative model is associated with an unseen word w of a document d in the test collection. Quantity within the exponent is referred to as entropy of the test zi."}, {"heading": "5 Experiments", "text": "In this section, the experiments carried out to evaluate the relevance and effectiveness of the proposed system of rapid and robust creation of topics in space are presented, starting with the experimental protocol, followed by a qualitative analysis of the results obtained using evaluation metrics described in Section 4."}, {"heading": "5.1 Experimental protocol", "text": "A large corpus of documents is required to train thematic spaces. Three corpus were used. Each corpus C is in a specific language (English, Spanish and French) and consists of a training set A and a test set B. The corpus consists of articles from Wikipedia. Thus, for each of the three languages, a set of 100,000 documents is collected. 90% of the corpus is summarized and used to create thematic spaces, while 10% is used to evaluate each model (no need to be summarized). Table 1 shows that the Latin languages (French and Spanish) have a similar size (a difference of less than 4% is observed), while the English theme is larger than the others (English text corpus is 1.37 times larger than French or Spanish corpus). Despite the size difference of the corpus, both have a similar size (a difference of less than 4%)."}, {"heading": "5.2 Results", "text": "The experiments carried out in this paper are subject-related: each measurement proposed in Section 4 (Perplexity and JS) is applied to each language (English, Spanish and French), each room size ({5, 10, 50, 100, 200, 400}), and finally to each compression rate during the summary process (10% to 50% of the original size of the documents); Figures 5 and 6 present results obtained by varying the number of topics (Figure (a) to (c)) and the percentage of the summary (Figure 6) or the measures of perplexity and Jensen-Shannon (JS); the results are calculated by averaging between the different subject areas and an average between the various reduced summaries; in addition, each language was examined separately to show differences in theme room quality depending on the language."}, {"heading": "6 Discussions", "text": "This year it is so far that it is only a matter of time until it is so far, until it is so far, until it is so far."}, {"heading": "7 Conclusions", "text": "This paper proposes a qualitative study on the impact of document summaries on learning in the theme space. Basically, the idea that learning theme spaces from compressed documents takes less time than learning theme spaces from the complete documents is the main advantage of using the full text document in the text corpus to build theme spaces is to shift the semantic variability in each topic up and then increase the divergence between these themes. Experiments show that theme spaces with sufficient theme size exhibit more or less the same divergence; the only disadvantage of theme spaces with a large number of themes, i.e. appropriate knowledge of the size of the corpus (in our case more than 200 themes), is less perplexity, better divergence between themes and less time consuming during the LDA learning process; the only disadvantage of theme spaces that are learned from text corpus of rare documents that disappear when the number of themes suitable for the language is taken into account."}], "references": [{"title": "Automatic text processing: the transformation", "author": ["G. Salton"], "venue": "Analysis and Retrieval of Information by Computer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "Latent dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "The Journal of Machine Learning Research 3", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Modern information retrieval", "author": ["R. Baeza-Yates", "B Ribeiro-Neto"], "venue": "Volume 463. ACM press New York", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Introduction to modern information retrieval", "author": ["G. Salton", "M.J. McGill"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1983}, {"title": "On the specification of term values in automatic indexing", "author": ["G. Salton", "C.S. Yang"], "venue": "Journal of documentation 29", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1973}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S. Dumais", "G. Furnas", "T. Landauer", "R. Harshman"], "venue": "Journal of the American society for information science 41", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "A latent semantic analysis framework for large-span language modeling", "author": ["J. Bellegarda"], "venue": "Fifth European Conference on Speech Communication and Technology.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Proc. of Uncertainty in Artificial Intelligence, UAI \u2019 99, Citeseer", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Exploiting latent semantic information in statistical language modeling", "author": ["J. Bellegarda"], "venue": "Proceedings of the IEEE 88", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Keyword extraction using term-domain interdependence for dictation of radio news", "author": ["Y. Suzuki", "F. Fukumoto", "Y. Sekiguchi"], "venue": "17th international conference on Computational linguistics. Volume 2., ACL", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Probabilistic models for unified collaborative and content-based recommendation in sparse-data environments", "author": ["A. Popescul", "D.M. Pennock", "S. Lawrence"], "venue": "Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, Morgan Kaufmann Publishers Inc.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Automatically Evaluating Content Selection in Summarization without Human Models", "author": ["A. Louis", "A. Nenkova"], "venue": "Empirical Methods in Natural Language Processing, Singapore", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Divergence Measures based on the Shannon Entropy", "author": ["J. Lin"], "venue": "IEEE Transactions on Information Theory 37", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1991}, {"title": "Unsupervised learning by probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Machine Learning 42", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Expectation-propagation for the generative aspect model", "author": ["T. Minka", "J. Lafferty"], "venue": "Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence, Morgan Kaufmann Publishers Inc.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National academy of Sciences of the United States of America 101", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1984}, {"title": "Parameter estimation for text analysis", "author": ["G. Heinrich"], "venue": "Web: http://www. arbylon. net/publications/text-est. pdf", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Automatic Text Summarization", "author": ["J.M. Torres-Moreno"], "venue": "Wiley and Sons", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Artex is another text summarizer", "author": ["J.M. Torres-Moreno"], "venue": "arxiv:1210.3312 [cs.ir]", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Terms derived from frequent sequences for extractive text summarization", "author": ["Y. Ledeneva", "A. Gelbukh", "R.A. Gar\u0107\u0131a-Hern\u00e1ndez"], "venue": "Computational Linguistics and Intelligent Text Processing. Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Sch\u00fctze"], "venue": "The MIT Press, Cambridge, Massachusetts", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "Cortex : un algorithme pour la condensation automatique des textes", "author": ["J.M. Torres-Moreno", "P. Vel\u00e1zquez-Morales", "J.G. Meunier"], "venue": "ARCo\u201901. Volume 2., Lyon, France", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Summary evaluation with and without references", "author": ["J.M. Torres-Moreno", "H. Saggion", "Cunha", "I.d.", "E. SanJuan", "P. Vel\u00e1zquez-Morales"], "venue": "Polibits", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "The author-topic model for authors and documents", "author": ["M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth"], "venue": "Proceedings of the 20th conference on Uncertainty in artificial intelligence, AUAI Press", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Mallet: A machine learning for language toolkit", "author": ["A.K. McCallum"], "venue": "http://mallet.cs.umass.edu", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Different classical representations of a document, such as term-frequency based representation [1], have been proposed to extract word-level information from this large amount of data in a limited time.", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "The most known and used one is the latent Dirichlet allocation (LDA) [2] approach which outperforms classical methods in many NLP tasks.", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "All these methods consider documents as a bag-of-words [1] where the word order is not taken into account.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "Among the first methods proposed in IR, [3] propose to reduce each document from a discrete space (words and documents) to a vector of numeral values represented by the word counts (number of occurrences) in the document named TF-IDF [4].", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "Among the first methods proposed in IR, [3] propose to reduce each document from a discrete space (words and documents) to a vector of numeral values represented by the word counts (number of occurrences) in the document named TF-IDF [4].", "startOffset": 234, "endOffset": 237}, {"referenceID": 4, "context": "This approach showed its effectiveness in different tasks, and more precisely in the basic identification of discriminative words for a document [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 5, "context": "To substantiate the claims regarding TF-IDF method, IR researchers have proposed several other dimensionality reductions such as Latent Semantic Analysis (LSA) [6, 7] which uses a singular value decomposition (SVD) to reduce the space dimension.", "startOffset": 160, "endOffset": 166}, {"referenceID": 6, "context": "To substantiate the claims regarding TF-IDF method, IR researchers have proposed several other dimensionality reductions such as Latent Semantic Analysis (LSA) [6, 7] which uses a singular value decomposition (SVD) to reduce the space dimension.", "startOffset": 160, "endOffset": 166}, {"referenceID": 7, "context": "This method was improved by [8] which proposed a Probabilistic LSA (PLSA).", "startOffset": 28, "endOffset": 31}, {"referenceID": 8, "context": "This method demonstrated its performance on various tasks, such as sentence [9] or keyword [10] extraction.", "startOffset": 76, "endOffset": 79}, {"referenceID": 9, "context": "This method demonstrated its performance on various tasks, such as sentence [9] or keyword [10] extraction.", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "However, to address this shortcoming, a tempering heuristic is used to smooth the parameter of PLSA models for acceptable predictive performance: the authors in [11] showed that overfitting can occur even if tempering process is used.", "startOffset": 161, "endOffset": 165}, {"referenceID": 1, "context": "To overcome these two issues, the latent Dirichlet allocation (LDA) [2] method was proposed.", "startOffset": 68, "endOffset": 71}, {"referenceID": 11, "context": "The authors in [12] evaluated the effectiveness of the Jensen-Shannon (JS) theoretic measure [13] in predicting systems ranks in two summarization tasks: query-focused and update summarization.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "The authors in [12] evaluated the effectiveness of the Jensen-Shannon (JS) theoretic measure [13] in predicting systems ranks in two summarization tasks: query-focused and update summarization.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "LDA is a generative model which considers a document, seen as a bag-of-words [1], as a mixture of latent topics.", "startOffset": 77, "endOffset": 80}, {"referenceID": 13, "context": "PLSA and LDA models have been shown to generally outperform LSA on IR tasks [14].", "startOffset": 76, "endOffset": 80}, {"referenceID": 1, "context": "This allows to obtain a parameter binding all the documents together [2].", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "Several techniques have been proposed to estimate LDA parameters, such as Variational Methods [2], Expectation-propagation [15] or Gibbs Sampling [16].", "startOffset": 94, "endOffset": 97}, {"referenceID": 14, "context": "Several techniques have been proposed to estimate LDA parameters, such as Variational Methods [2], Expectation-propagation [15] or Gibbs Sampling [16].", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "Several techniques have been proposed to estimate LDA parameters, such as Variational Methods [2], Expectation-propagation [15] or Gibbs Sampling [16].", "startOffset": 146, "endOffset": 150}, {"referenceID": 16, "context": "Gibbs Sampling is a special case of Markov-chain Monte Carlo (MCMC) [17] and gives a simple algorithm to approximate inference in high-dimensional models such as LDA [18].", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "Gibbs Sampling is a special case of Markov-chain Monte Carlo (MCMC) [17] and gives a simple algorithm to approximate inference in high-dimensional models such as LDA [18].", "startOffset": 166, "endOffset": 170}, {"referenceID": 15, "context": "The first use of Gibbs Sampling for estimating LDA is reported in [16] and a more comprehensive description of this method can be found in [18].", "startOffset": 66, "endOffset": 70}, {"referenceID": 17, "context": "The first use of Gibbs Sampling for estimating LDA is reported in [16] and a more comprehensive description of this method can be found in [18].", "startOffset": 139, "endOffset": 143}, {"referenceID": 18, "context": "Various text summarization systems have been proposed over the years [19].", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "Two baseline systems as well as the ARTEX summarization system, that reaches state-of-the-art performance [20], are presented in this section.", "startOffset": 106, "endOffset": 110}, {"referenceID": 20, "context": "Although very simple, this method is a strong baseline for the performance of any automatic summarization system [21, 22].", "startOffset": 113, "endOffset": 121}, {"referenceID": 21, "context": "Although very simple, this method is a strong baseline for the performance of any automatic summarization system [21, 22].", "startOffset": 113, "endOffset": 121}, {"referenceID": 20, "context": "Baseline random (BR) The Baseline random [21] randomly selects n sentences of the documents, where n is also determined by a compression rate.", "startOffset": 41, "endOffset": 45}, {"referenceID": 19, "context": "ARTEX AnotheR TEXt (ARTEX) algorithm [20] is another simple extractive algorithm.", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "The w(\u2022) computed by Equation 4 must be normalized between the interval [0, 1].", "startOffset": 72, "endOffset": 78}, {"referenceID": 19, "context": "The element a\u03bc is only a scale factor that does not modify \u03b1 [20]:", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "This summarization system outperforms the CORTEX [24] one with the FRESA [25] measure.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "This summarization system outperforms the CORTEX [24] one with the FRESA [25] measure.", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "ARTEX is evaluated with several corpus such as the Medecina Clinica [20].", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "A lower perplexity score indicates better generalization performance [2]:", "startOffset": 69, "endOffset": 72}, {"referenceID": 24, "context": "Following a classical study of LDA topic spaces quality [26], the number of topics by model is fixed to {5, 10, 50, 100, 200, 400}.", "startOffset": 56, "endOffset": 60}, {"referenceID": 25, "context": "These topic spaces are built with the MALLET toolkit [27].", "startOffset": 53, "endOffset": 57}], "year": 2017, "abstractText": "The number of documents available into Internet moves each day up. For this reason, processing this amount of information effectively and expressibly becomes a major concern for companies and scientists. Methods that represent a textual document by a topic representation are widely used in Information Retrieval (IR) to process big data such as Wikipedia articles. One of the main difficulty in using topic model on huge data collection is related to the material resources (CPU time and memory) required for model estimate. To deal with this issue, we propose to build topic spaces from summarized documents. In this paper, we present a study of topic space representation in the context of big data. The topic space representation behavior is analyzed on different languages. Experiments show that topic spaces estimated from text summaries are as relevant as those estimated from the complete documents. The real advantage of such an approach is the processing time gain: we showed that the processing time can be drastically reduced using summarized documents (more than 60% in general). This study finally points out the differences between thematic representations of documents depending on the targeted languages such as English or latin languages.", "creator": "TeX"}}}