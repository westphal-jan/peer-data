{"id": "1703.06676", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "I2T2I: Learning Text to Image Synthesis with Textual Data Augmentation", "abstract": "Translating information between text and image is a fundamental problem in artificial intelligence that connects natural language processing and computer vision. In the past few years, performance in image caption generation has seen significant improvement through the adoption of recurrent neural networks (RNN). Meanwhile, text-to-image generation begun to generate plausible images using datasets of specific categories like birds and flowers. We've even seen image generation from multi-category datasets such as the Microsoft Common Objects in Context (MSCOCO) through the use of generative adversarial networks (GANs). Synthesizing objects with a complex shape, however, is still challenging. For example, animals and humans have many degrees of freedom, which means that they can take on many complex shapes. We propose a new training method called Image-Text-Image (I2T2I) which integrates text-to-image and image-to-text (image captioning) synthesis to improve the performance of text-to-image synthesis. We demonstrate that %the capability of our method to understand the sentence descriptions, so as to I2T2I can generate better multi-categories images using MSCOCO than the state-of-the-art. We also demonstrate that I2T2I can achieve transfer learning by using a pre-trained image captioning module to generate human images on the MPII Human Pose", "histories": [["v1", "Mon, 20 Mar 2017 11:11:38 GMT  (3792kb,D)", "http://arxiv.org/abs/1703.06676v1", null], ["v2", "Mon, 8 May 2017 18:46:42 GMT  (3792kb,D)", "http://arxiv.org/abs/1703.06676v2", null], ["v3", "Sat, 3 Jun 2017 22:46:46 GMT  (3792kb,D)", "http://arxiv.org/abs/1703.06676v3", "International Conference on Image Processing (ICIP) 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["hao dong", "jingqing zhang", "douglas mcilwraith", "yike guo"], "accepted": false, "id": "1703.06676"}, "pdf": {"name": "1703.06676.pdf", "metadata": {"source": "META", "title": "I2T2I: LEARNING TEXT TO IMAGE SYNTHESIS WITH TEXTUAL DATA AUGMENTATION", "authors": ["Hao Dong", "Jingqing Zhang", "Douglas McIlwraith", "Yike Guo"], "emails": [], "sections": [{"heading": null, "text": "Index terms - in-depth learning, GAN, image synthesis"}, {"heading": "1. INTRODUCTION", "text": "In fact, it is that we see ourselves as being able to assert ourselves, that we are able, that we are able to put ourselves at the top, and that we are able, that we will be able to assert ourselves, that we are able to put ourselves at the top, \"he said."}, {"heading": "2. PRELIMINARIES", "text": "In this section, we briefly describe the previous work on which our modules are based."}, {"heading": "2.1. Image Captioning Module", "text": "The caption on deep CNN and RNN has undergone significant improvements in recent years [2]. Deep CNNs can completely render an image X by embedding it in a vector of fixed length. Then, in particular, RNN decodes the fixed length vector to a desired output set S = (s1,..., st) by iterating the repeat relationship for t defined in Equation (1) to (4): h0, c0 = LSTM (\u03b8, We (CNN (X))) (1) ht, ct = LSTM (ht \u2212 1, ct \u2212 1, Ws (st \u2212 1) (2) pt = softmax (htWho + bo) (3) st = sample (pt, k) (4), which are the initial hidden and cell states of LSTM, which should all be zeros, s0 is the special start token; h0 is the sample (k, k) (4), representing the initial hidden and cell states of LSTM, which should all be the zeros; s0 is the special start token; 0 is the probability that the image is at the beginning of the equilibrium (the equal) of each of the two (the two)."}, {"heading": "2.2. Image-Text Mapping Module", "text": "Visual semantic embedding creates vector representation that embeds images and sentences in a common space [14]. Much work has been done to learn the multimodal representation of images and sentences, in which images and sentences can be mapped into the same embedding space using CNN and RNN encoders [15]. The adjustment costs between images and sentences can be defined as equation (5): min.: c is cosine similarity of two embedded vectors, x is embedded output of CNN encoders, v is embedded output of RNN encoders, v is embedded output of RNN encoders, xk and vk are embedded output of mismatched images and sentences."}, {"heading": "2.3. Generative adversarial network Module", "text": "Generative opposing networks (GAN) consist of a generator G and a discriminator D, which are trained in a competitive manner [6]. D learns to distinguish real images from fake images generated by G, while G learns to generate fake images from latent variables z and tries to cheat D. The cost function can be defined as equation (6): min G max D V (D, G) = Ex \u0445 Psample (x) [logD (x)] + Ez imed Pz (z) [log (1 \u2212 D (G (z))))] (6) On this basis, GAN-CLS [9] achieve a text-image synthesis by linking features of text embedding in latent variables of generator G and the last layer of discriminator D, as Fig. 3 shows."}, {"heading": "3. METHOD", "text": "In this section we present an important contribution to our work - the extension of text data by captions. We show how this is done by using a CNN and RNN and how this is used in text-image synthesis."}, {"heading": "3.1. Textual data augmentation via image captioning", "text": "For each real image Xi there is a set of Si that has a limited number of human annotated captions that cannot cover all possible descriptions of the images since each sentence has many synonymous sentences. For example, \"a man riding a surfboard on a wave\" is comparable to \"a person riding a surfboard on a wave,\" \"a surfer riding a big wave in the ocean,\" etc. Fortunately, the Softmax edition of RNN-based captions can exploit uncertainty to generate synonymous sentences. Therefore, k greater than 1 in Equation (4) can generate a massive number of textual descriptions from a single image X. Given an image X, the probability distribution of sentences S can be defined as an equation (7): P (S | X) = P (s1 | X) P (s2 | h1, s1)..."}, {"heading": "3.2. Network structure", "text": "Our method includes three previously presented modules: 1) a captioning module, 2) a caption module, and 3) a conditional generative network for text-image synthesis (GAN-CLS). For our captioning module, we use a state-of-the-art inception model [16] followed by an LSTM decoder as described in [1]. To improve the variety of captions generated, we use k = 3 in Equation (4). To obtain image-text mapping (embedding), we follow the cost function defined in Equation (5) and use Inception V3 as an image encoder and LSTM as a sentence encoder. Since image-text mapping requires both matching and non-matching sentences, we synthesize a batch of matching sentences for a series of images that we use as STSTM mapping after the caption module."}, {"heading": "3.3. Training", "text": "Since synthetic sentences are generated in each iteration, the RNN encoder for image and text embedding is trained in our method synchronously with the GAN-CLS module. In addition, we perform a data augmentation for images with random rotation, flip and cutout in each iteration. For the training of the RNN encoder, we use a learning rate of 0.0001 and the Adam optimization [17] with an impulse of 0.5. For the training of GAN-CLS, we use an initial learning rate of 0.0002 with a dynamic of 0.5. Both learning rates of RNN encoder and GAN-CLS decrease by 0.5 every 100 epochs. We use a stack size of 64 and train 600 epochs."}, {"heading": "4. EXPERIMENTS", "text": "Two datasets were used in our experiments: The MSCOCO [4] dataset contains 82783 training and 40504 validation images, each of which is commented with 5 sets of different annotators; the MPII Human Pose dataset (MHP) [12] contains approximately 25K images of 410 human activities; there is no sentence description, which makes it suitable for evaluating transfer learning."}, {"heading": "4.1. Qualitative comparison", "text": "In both cases, the generator and discriminator use the same architectures, i.e. GAN-CLS [9]. The results of the validation set can be compared in Fig. 1 and Fig. 6. Both GAN-CLS and I2T2I generate a plausible background, but I2T2I performs better for certain classes such as \"people,\" which has proved difficult for previous approaches [18]. In addition, static object-based images such as windows, toilets, buses, etc., have been found."}, {"heading": "4.2. Transfer learning", "text": "To demonstrate transfer learning, we used MSCOCO's pre-trained captioning module to train a text-to-picture module on the MPII Human Pose dataset (MHP). Since all images in this dataset contain people, we can successfully synthesize images of human activity, and the image quality is also better than GAN-CLS, as Fig. 7 shows. This experiment shows that I2T2I can achieve text-to-picture synthesis on unlabeled datasets by using a pre-labeled captioning module from multiple datasets."}, {"heading": "5. CONCLUSION", "text": "In this paper, we propose a new training method I2T2I. Qualitative comparative results show that training text-image synthesis with text data enlargement can help to obtain higher quality images. Furthermore, we demonstrate the synthesis of images that affect humans - which has proven difficult for existing methods [18]. In the immediate future, we plan to combine stackGAN [19] to improve image resolution."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "The authors thank Chao Wu and Simiao Yu for their helpful comments and suggestions. Hao Dong is supported by the OPTIMISE portal. Jingqing Zhang is supported by LexisNexis."}, {"heading": "7. REFERENCES", "text": "[1] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan 2016, \"Show and tell: Lessons learned from the 2015 mscoco image captioning challenge,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016. [2] Andrej Karpathy and Li Fei-Fei-Fei, \"Deep visual-semantic alignments for generating image descriptions,\" CVPR, 2015. [3] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S Zemiele, and Yoshua Bengio Bengio, \"Show and tell: Neural image caption generation with visual attention,\" ICML, 2015] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla."}], "references": [{"title": "Show and tell: Lessons learned from the 2015 mscoco image captioning challenge", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "CVPR, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "ICML, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "ECCV, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Oriol Vinyals", "Lasse Espeholt", "Alex Graves", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1606.05328, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "NIPS, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "ICLR, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional image synthesis with auxiliary classifier gans", "author": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "venue": "arXiv preprint arXiv:1610.09585, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial text to image synthesis", "author": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee"], "venue": "ICML, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating interpretable images with controllable structure", "author": ["Scott Reed", "A van den Oord", "N Kalchbrenner", "V Bapst", "M Botvinick", "N de Freitas"], "venue": "ICLR, 2017.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning what and where to draw", "author": ["Scott Reed", "Zeynep Akata", "Santosh Mohan", "Samuel Tenka", "Bernt Schiele", "Honglak Lee"], "venue": "NIPS, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "2d human pose estimation: New benchmark and state of the art analysis", "author": ["Mykhaylo Andriluka", "Leonid Pishchulin", "Peter Gehler", "Bernt Schiele"], "venue": "CVPR, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "S Hochreiter", "J\u00fcrgen Schmidhuber", "J Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u201380, 1997.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Fei Fei F Li"], "venue": "NIPS, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel"], "venue": "TACL, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "CVPR, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "International Conference on Learning Representations, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Plug & play generative networks: Conditional iterative generation of images in latent space", "author": ["Anh Nguyen", "Jason Yosinski", "Yoshua Bengio", "Alexey Dosovitskiy", "Jeff Clune"], "venue": "NIPS, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks", "author": ["Han Zhang", "Tao Xu", "Hongsheng Li", "Shaoting Zhang", "Xiaolei Huang", "Xiaogang Wang", "Dimitris Metaxas"], "venue": "arXiv preprint arXiv:1612.03242, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In the past few years, we\u2019ve witnessed a breakthrough in the application of recurrent neural networks (RNN) to generating textual descriptions conditioned on images [1, 2], with Xu et al.", "startOffset": 165, "endOffset": 171}, {"referenceID": 1, "context": "In the past few years, we\u2019ve witnessed a breakthrough in the application of recurrent neural networks (RNN) to generating textual descriptions conditioned on images [1, 2], with Xu et al.", "startOffset": 165, "endOffset": 171}, {"referenceID": 2, "context": "showing that the multimodality problem can be decomposed sequentially [3].", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "However, the lack of datasets with diversity descriptions of images limits the performance of text-to-image synthesis on multicategories dataset like MSCOCO [4].", "startOffset": 157, "endOffset": 160}, {"referenceID": 4, "context": "PixelCNN [5] learns a conditional image generator which is able to generate realistic scenes representing different objects and portraits of people.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "Generative adversarial networks (GAN) [6], specifically DCGAN [7] have been applied to a variety of datasets and shown plausible results.", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "Generative adversarial networks (GAN) [6], specifically DCGAN [7] have been applied to a variety of datasets and shown plausible results.", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "[8] has shown the capability of GANs to generate high-quality images conditioned on classes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] proposed GAN-CLS which bridges advances of DCGAN and an RNN encoder to generate images from an latent variables and embedding of image descriptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Besides these, recent studies learned to generate images by conditions [10, 11].", "startOffset": 71, "endOffset": 79}, {"referenceID": 10, "context": "Besides these, recent studies learned to generate images by conditions [10, 11].", "startOffset": 71, "endOffset": 79}, {"referenceID": 11, "context": "Furthermore, we demonstrate the flexibility and generality of I2T2I on MPII Human Pose dataset (MHP) [12] by using a pre-trained image captioning module from MSCOCO.", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "Image captioning via deep CNN and RNN has seen significant improvements in recent years [2].", "startOffset": 88, "endOffset": 91}, {"referenceID": 12, "context": "Then RNN especially LSTM [13] decodes the fixedlength vector to a desired output sentence S = (s1, .", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "Visual-semantic embedding generates vector representation that embeds images and sentences into a common space [14].", "startOffset": 111, "endOffset": 115}, {"referenceID": 14, "context": "A lot of work has been done to learn the multi-modal representation of images and sentences, where images and sentences can be mapped into the same embedding space by using CNN and RNN encoders [15].", "startOffset": 194, "endOffset": 198}, {"referenceID": 5, "context": "Generative adversarial networks (GAN) are composed of a generator G and a discriminator D, which are trained in a competitive manner [6].", "startOffset": 133, "endOffset": 136}, {"referenceID": 8, "context": "Based on this, GAN-CLS [9] achieve text-to-image synthesis by concatenating text embedding features into latent variables of the generator G and the last layer of discriminator D, as Fig.", "startOffset": 23, "endOffset": 26}, {"referenceID": 15, "context": "For our image captioning module, we use a state-of-the-art Inception model [16] followed by LSTM decoder as described in [1].", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "For our image captioning module, we use a state-of-the-art Inception model [16] followed by LSTM decoder as described in [1].", "startOffset": 121, "endOffset": 124}, {"referenceID": 8, "context": "We adopt GAN-CLS as image generation module, and reuse the LSTM encoder of image-to-text mapping module, then concatenate its embedded output into latent variables [9].", "startOffset": 164, "endOffset": 167}, {"referenceID": 12, "context": "For the RNN encoder, we use LSTM [13] with a hidden size of 256.", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "For GAN-CLS, in order to have a qualitative comparison, we use the same architecture describe in [9].", "startOffset": 97, "endOffset": 100}, {"referenceID": 16, "context": "0001, and the Adam optimization [17] with momentum of 0.", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "The MSCOCO [4] dataset contains 82783 training and 40504 validation images, each of which is annotated with 5 sentences from different annotators.", "startOffset": 11, "endOffset": 14}, {"referenceID": 11, "context": "The MPII Human Pose dataset (MHP) [12] has around 25K images covering 410 human activities.", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "GAN-CLS [9].", "startOffset": 8, "endOffset": 11}, {"referenceID": 17, "context": "Both GAN-CLS and I2T2I generate a plausible background, but I2T2I performs better for certain classes such as \u201cpeople\u201d which has proved to be challenging for previous approaches [18].", "startOffset": 178, "endOffset": 182}, {"referenceID": 17, "context": "Furthermore, we demonstrate the synthesis of images relating to humans \u2013 which has proved difficult for existing methods [18].", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "In the immediate future, we plan to combine stackGAN [19] to improve image resolution.", "startOffset": 53, "endOffset": 57}], "year": 2017, "abstractText": "Translating information between text and image is a fundamental problem in artificial intelligence that connects natural language processing and computer vision. In the past few years, performance in image caption generation has seen significant improvement through the adoption of recurrent neural networks (RNN). Meanwhile, text-to-image generation begun to generate plausible images using datasets of specific categories like birds and flowers. We\u2019ve even seen image generation from multi-category datasets such as the Microsoft Common Objects in Context (MSCOCO) through the use of generative adversarial networks (GANs). Synthesizing objects with a complex shape, however, is still challenging. For example, animals and humans have many degrees of freedom, which means that they can take on many complex shapes. We propose a new training method called Image-Text-Image (I2T2I) which integrates text-to-image and image-to-text (image captioning) synthesis to improve the performance of textto-image synthesis. We demonstrate that I2T2I can generate better multi-categories images using MSCOCO than the stateof-the-art. We also demonstrate that I2T2I can achieve transfer learning by using a pre-trained image captioning module to generate human images on the MPII Human Pose dataset (MHP) without using sentence annotation.", "creator": "LaTeX with hyperref package"}}}