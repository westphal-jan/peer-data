{"id": "1703.06683", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "A Systematic Study of Online Class Imbalance Learning with Concept Drift", "abstract": "As an emerging research topic, online class imbalance learning often combines the challenges of both class imbalance and concept drift. It deals with data streams having very skewed class distributions, where concept drift may occur. It has recently received increased research attention; however, very little work addresses the combined problem where both class imbalance and concept drift coexist. As the first systematic study of handling concept drift in class-imbalanced data streams, this paper first provides a comprehensive review of current research progress in this field, including current research focuses and open challenges. Then, an in-depth experimental study is performed, with the goal of understanding how to best overcome concept drift in online learning with class imbalance. Based on the analysis, a general guideline is proposed for the development of an effective algorithm.", "histories": [["v1", "Mon, 20 Mar 2017 11:22:10 GMT  (1437kb,D)", "http://arxiv.org/abs/1703.06683v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuo wang", "leandro l minku", "xin yao"], "accepted": false, "id": "1703.06683"}, "pdf": {"name": "1703.06683.pdf", "metadata": {"source": "CRF", "title": "A Systematic Study of Online Class Imbalance Learning with Concept Drift", "authors": ["Shuo Wang", "Leandro L. Minku", "Xin Yao"], "emails": ["X.Yao}@cs.bham.ac.uk.", "leandro.minku@leicester.ac.uk."], "sections": [{"heading": null, "text": "In fact, it is so that most of them are able to abide by the rules that they have imposed on themselves. (...) Indeed, it is so that they are able to survive themselves. (...) It is not so that they are able to survive themselves. (...) It is so that they are able to survive themselves. (...) It is so that they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) (...) (...) () (...) (). () (...) () (). () () (). () (). () ()). (). () (). (). () (). (). () (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). ()). (). (). ()). (). ()). (). ()). ()). (). ()). (). ()). (). (). ()). (). ()). (). ()). (). (). ().). ()."}, {"heading": "II. ONLINE LEARNING FRAMEWORK WITH CLASS IMBALANCE AND CONCEPT DRIFT", "text": "In data stream applications, over time, data arrives in streams of examples or stacks of examples. The information up to a certain time step t is used to build / update predictive models that then predict the new example (s) that arrives in time step t + 1. Learning under such conditions requires chunkbased learning or online learning algorithms, depending on how many training examples are available in each time step. By most agreed definitions [6], chuncontrolled learning algorithms process a stack of data examples at each time step, such as the case of daily Internet use by a number of users; online learning algorithms process examples one by one and the predictive model is updated after receiving each example [15], as is the case of sensor readings every second in technical systems. The term \"incremental learning\" is also commonly used in this scenario, as an algorithm used in each algorithm that processes data streams to process data streams into certain criteria."}, {"heading": "A. Learning Procedure", "text": "In fact, it is a purely mental game, which is about changing the world and changing the world."}, {"heading": "B. Problem Descriptions", "text": "This year is the highest in the history of the country."}, {"heading": "III. OVERCOMING CLASS IMBALANCE AND CONCEPT DRIFT SIMULTANEOUSLY", "text": "Following the review of class imbalance and concept drift in Section II, this section examines the combined problem, including example applications and existing solutions. If both exist, one problem affects the treatment of the other. For example, the drift detection algorithms based on the traditional classification error may be sensitive to unbalanced degrees and become less effective; the class imbalance techniques must be adaptive to altered P (y), otherwise the class receiving preferential treatment may not be the right minority class at the present time. Therefore, their mutual effects should be considered during algorithm design. Illustrative application problems of concept drift and class imbalance have been found in many real applications. Three examples are given to help us understand each type of concept drift."}, {"heading": "B. Approaches to Tackling Both Class Imbalance and Concept Drift", "text": "Some research efforts have been undertaken to address the common problem of concept drift and class imbalance, due to the increasing need for practical problems [86]. Uncorrelated bagging is one of the earliest algorithms to build a group of classifiers trained on a more balanced set of data, although based on a concept of passive weighing and overcoming the weighing of basic classification methods based on their discriminatory power, but equipped with a \"smart\" verification technology. Selective recursive approaches SERA [90] and REA [91] use similar ideas such as uncorrelated bagging to build an ensemble of weighted classifiers, but with a \"smarter\" oversampling technique. Learn + +.CDS and Learn +.NIE are newer algorithms that tackle class imbalances through the verification techniques in the verification technique of a subset-up [32]."}, {"heading": "IV. PERFORMANCE ANALYSIS", "text": "With a full review of the learning of class imbalances, we aim to gain an in-depth understanding of concept drift detection in unbalanced data flows and the performance of existing approaches, which are explored in Section III-B. Three research questions are explored through experimental analysis: 1) What are the difficulties in detecting each type of concept drift? Little work has been done on separate discussions of the three basic types of concept drift, particularly the P (y) drift. It is important to understand their differences so that the most appropriate approaches for best performance can be used. 2) Among the existing approaches developed for unbalanced flows of concept drift data, which approach is better and when? Although some approaches have been proposed to overcome concept drift and class imbalance, it is still unclear how well they work for each type of concept drift. 3) Whether and how class imbalance techniques affect concept drift detection and the online prediction of class imbalance?"}, {"heading": "A. Data Sets", "text": "For accurate analysis and comparable results, we select two most commonly used artificial data generators, SINE1 [79] and SEA [100], to produce unbalanced data streams with three simulated types of concept drift. This is one of the few studies that will individually discuss P (y), p (x | y) and P (y | x) types of concept drift in depth. Furthermore, each generator produces two data streams with different drift speeds - abrupt and gradual drifts. Drift speed is defined as the inversion of the time needed for a new concept to completely replace the old concept [72]. Depending on the speed, drifts can either be abrupt if the generation function is completely changed in just one time step, or gradual, otherwise. Data streams with a gradual concept drift are referred to by \"g\" in the following experiment, i.e. SINE1g [76] and SEAg \"Each data stream has a beginning of 3000 time steps, where the concept is a time step."}, {"heading": "B. Experimental and Evaluation Settings", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "C. Comparative Study on Artificial Data", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "D. Comparative Study on Real-World Data", "text": "After detailed analysis of the three types of concept drift, we will now consider the performance of the above learning models on the three real data sets (PAKDD [101], Weather [75] and Tweet [102]) described in Section IV-A. Based on the experimental results of the artificial data, we will focus on the best active (PAUC + OOB) and best passive concept drift detection (ESOS-ELM) methods for clear observation compared to OOB. The three methods use the same parameter settings as before. The initialization and validation data required by ESOS-ELM are the first 2% examples of each data set. Without knowing the true concept, we drift into real data, calculate and track the temporal G-mean by setting the decay factor to 0.995, meaning that the old performance at the 0.5% rate is forgotten."}, {"heading": "E. Further Discussions", "text": "This year it is more than ever before."}, {"heading": "V. CONCLUSION", "text": "In this context, it should be noted that this is a very complex matter."}], "references": [{"title": "A new dynamic modeling framework for credit risk assessment", "author": ["M.R. Sousa", "J. Gama", "E. Brand\u00e3o"], "venue": "Expert Systems with Applications, vol. 45, p. 341351, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Fault diagnosis using a timed discrete-event approach based on interval observers: Application to sewer networks", "author": ["J. Meseguer", "V. Puig", "T. Escobet"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans, vol. 40, no. 5, pp. 900\u2013916, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Online class imbalance learning and its applications in fault detection", "author": ["S. Wang", "L.L. Minku", "X. Yao"], "venue": "International Journal of Computational Intelligence and Applications, vol. 12, no. 4, pp. 1 340 001(1\u2013 19), 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Online ensemble learning of data streams with gradually evolved classes", "author": ["Y. Sun", "K. Tang", "L.L. Minku", "S. Wang", "X. Yao"], "venue": "IEEE Transaction on Knowledge and Data Engineering, vol. (Accepted), 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning from imbalanced data", "author": ["H. He", "E.A. Garcia"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 21, no. 9, pp. 1263\u20131284, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Online ensemble learning in the presence of concept drift", "author": ["L.L. Minku"], "venue": "Ph.D. dissertation, School of Computer Science, The University of Birmingham, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A survey on concept drift adaptation", "author": ["J. a. Gama", "I. \u017dliobait\u0117", "A. Bifet", "M. Pechenizkiy", "A. Bouchachia"], "venue": "ACM Comput. Surv., vol. 46, no. 4, pp. 44:1\u201344:37, Mar. 2014. [Online]. Available: http://doi.acm.org/10.1145/2523813", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Concept drift detection for online class imbalance learning", "author": ["S. Wang", "L.L. Minku", "D. Ghezzi", "D. Caltabiano", "P. Tino", "X. Yao"], "venue": "International Joint Conference on Neural Networks (IJCNN \u201913), 2013, pp. 1\u20138.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Concept drift detection for streaming data", "author": ["H. Wang", "Z. Abraham"], "venue": "International Joint Conference of Neural Networks, 2015, pp. 1\u20139.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Prequential auc for classifier evaluation and drift detection in evolving data streams", "author": ["D. Brzezinski", "J. Stefanowski"], "venue": "New Frontiers in Mining Complex Patterns, vol. 8983, pp. 87\u2013101, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Resampling-based ensemble methods for online class imbalance learning", "author": ["S. Wang", "L.L. Minku", "X. Yao"], "venue": "IEEE Transactions on Knowledge and Data Engineering, no. 5, pp. 1356 \u2013 1368, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Recursive least square perceptron model for non-stationary and imbalanced data stream classification", "author": ["A. Ghazikhani", "R. Monsefi", "H.S. Yazdi"], "venue": "Evolving Systems, vol. 4, no. 2, pp. 119\u2013131, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Ensemble of subset online sequential extreme learning machine for class imbalance and concept drift", "author": ["B. Mirza", "Z. Lin", "N. Liu"], "venue": "Neurocomputing, vol. 149, pp. 316\u2013329, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning in nonstationary environments: A survey", "author": ["G. Ditzler", "M. Roveri", "C. Alippi", "R. Polikar"], "venue": "IEEE Computational Intelligence Magazine, vol. 10, no. 4, pp. 12 \u2013 25, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Experimental comparisons of online and batch versions of bagging and boosting", "author": ["N.C. Oza", "S. Russell"], "venue": "Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2001, pp. 359\u2013364.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Learn++: an incremental learning algorithm for supervised neural networks", "author": ["R. Polikar", "L. Udpa", "S.S. Udpa", "V. Honavar"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), vol. 31, no. 4, pp. 497 \u2013 508, 2001.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Nonlinear neural networks: Principles, mechanisms, and architectures", "author": ["S. Grossber"], "venue": "Neural Networks, vol. 1, no. 1, p. 1761, 1988.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1988}, {"title": "A learning framework for online class imbalance learning", "author": ["S. Wang", "L.L. Minku", "X. Yao"], "venue": "IEEE Symposium on Computational Intelligence and Ensemble Learning (CIEL), 2013, pp. 36\u201345.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Detecting sudden concept drift with knowledge of human behavior", "author": ["K. Nishida", "S. Shimada", "S. Ishikawa", "K. Yamauchi"], "venue": "IEEE International Conference on Systems, Man and Cybernetics, 2008, pp. 3261\u20133267.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "The class imbalance problem: A systematic study", "author": ["N. Japkowicz", "S. Stephen"], "venue": "Intelligent Data Analysis, vol. 6, no. 5, pp. 429 \u2013 449, 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning from streaming data with concept drift and imbalance: an overview", "author": ["T.R. Hoens", "R. Polikar", "N.V. Chawla"], "venue": "Progress in Artificial Intelligence, vol. 1, no. 1, pp. 89\u2013101, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Wind power forecasting : state-of-the-art 2009", "author": ["C. Monteiro", "R. Bessa", "V. Miranda", "A. Botterud", "J. Wang", "G. Conzelmann"], "venue": "Technical Report (ANL/DIS-10-1), Argonne National Laboratory (ANL), 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Machine learning for the detection of oil spills in satellite radar images", "author": ["M. Kubat", "R.C. Holte", "S. Matwin"], "venue": "Machine Learning, vol. 30, no. 2-3, pp. 195\u2013215, 1998.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Issues in mining imbalanced data sets a review paper", "author": ["S. Visa", "A. Ralescu"], "venue": "Proceedings of the Sixteen Midwest Artificial Intelligence and Cognitive Science Conference, 2005, pp. 67\u201373.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Addressing the curse of imbalanced training sets: One-sided selection", "author": ["M. Kubat", "S. Matwin"], "venue": "Proceedings of the 14th International Conference on Machine Learning, 1997, pp. 179\u2013186.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "A study of the behavior of several methods for balancing machine learning training data", "author": ["G.E.A.P.A. Batista", "R.C. Prati", "M.C. Monard"], "venue": "Special issue on learning from imbalanced datasets, Sigkdd Explorations, vol. 6, no. 1, pp. 20\u201329, 2004.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "knn approach to unbalanced data distributions: A case study involving information extraction", "author": ["J. Zhang", "I. Mani"], "venue": "Workshop on Learning from Imbalanced Datasets II, ICML, 2003, pp. 42\u201348.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "On predicting rare classes with svm ensembles in scene classification", "author": ["R. Yan", "Y. Liu", "R. Jin", "A. Hauptmann"], "venue": "IEEE International  IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. XX, NO. X, FEBRUARY 2017  17 Conference on Acoustics, Speech, and Signal Processing, 2003, pp. III \u2013 21\u20134 vol.3.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2017}, {"title": "Class-boundary alignment for imbalanced dataset learning", "author": ["G. Wu", "E.Y. Chang"], "venue": "Workshop on Learning from Imbalanced Datasets II, ICML, 2003, pp. 49\u201356.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Experimental perspectives on learning from imbalanced data", "author": ["J.V. Hulse", "T.M. Khoshgoftaar", "A. Napolitano"], "venue": "Proceedings of the 24th international conference on Machine learning, 2007, pp. 935\u2013942.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics", "author": ["V. Lopez", "A. Fernandez", "S. Garcia", "V. Palade", "F. Herrera"], "venue": "Information Sciences, vol. 250, pp. 113\u2013141, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Smote: Synthetic minority over-sampling technique", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "Journal of Artificial Intelligence Research, vol. 16, pp. 341\u2013378, 2002.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Living in an imbalanced world", "author": ["T.R. Hoens"], "venue": "Ph.D. dissertation, Graduate School of the University of Notre Dame, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Balancing strategies and class overlapping", "author": ["G.E. Batista", "R.C. Prati", "M.C. Monard"], "venue": "Advances in Intelligent Data Analysis, vol. 3646, pp. 24\u201335, 2005.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2005}, {"title": "Class imbalances versus class overlapping: An analysis of a learning system behavior", "author": ["R.C. Prati", "G.E. Batista", "M.C. Monard"], "venue": "Lecture Notes in Computer Science, vol. 2972, pp. 312\u2013321, 2004.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "Class imbalances versus small disjuncts", "author": ["T. Jo", "N. Japkowicz"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 6, 2004, pp. 40\u201349.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "Class imbalances: are we focusing on the right issue", "author": ["N. Japkowicz"], "venue": "Workshop on Learning from Imbalanced Data Sets II, ICML, 2003, pp. 17\u201323.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2003}, {"title": "Identification of different types of minority class examples in imbalanced data", "author": ["K. Napierala", "J. Stefanowski"], "venue": "Hybrid Artificial Intelligent Systems, vol. 7209, pp. 139\u2013150, 2012.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Types of minority class examples and their influence on learning classifiers from imbalanced data", "author": ["\u2014\u2014"], "venue": "Journal of Intelligent Information Systems, vol. 46, no. 3, p. 563597, 2016.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Borderline-smote: A new oversampling method in imbalanced data sets learning", "author": ["H. Han", "W.-Y. Wang", "B.-H. Mao"], "venue": "International Conference on Intelligent Computing (ICIC), 2005, pp. 878\u2013887.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2005}, {"title": "Adasyn: Adaptive synthetic sampling approach for imbalanced learning", "author": ["H. He", "Y. Bai", "E.A. Garcia", "S. Li"], "venue": "2008, pp. 1322\u20131328.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "MWMOTE - majority weighted minority oversampling technique for imbalanced data set learning", "author": ["S. Barua", "M.M. Islam", "X. Yao", "K. Murase"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 26, no. 2, pp. 405\u2013425, 2014, an oversampling method by generating new examples.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Two modifications of cnn", "author": ["I. Tomek"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, vol. 6, no. 11, pp. 769\u2013772, 1976.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1976}, {"title": "Learning when negative examples abound", "author": ["M. Kubat", "R. Holte", "S. Matwin"], "venue": "9th European Conference on Machine Learning Prague, vol. 1224, 1997, pp. 146\u2013153.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1997}, {"title": "Improving identification of difficult small classes by balancing class distribution", "author": ["L. Jorma"], "venue": "8th Conference on Artificial Intelligence in Medicine in Europe, AIME 2001, vol. 2101, 2001, pp. 63\u201366.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2001}, {"title": "An efficient algorithm coupled with synthetic minority over-sampling technique to classify imbalanced pubchem bioassay data", "author": ["M. Hao", "Y. Wang", "S.H. Bryant"], "venue": "Analytica Chimica Acta, vol. 806, no. 2, p. 117127, 2014.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "A multiple resampling method for learning from imbalanced data sets", "author": ["A. Estabrooks", "T. Jo", "N. Japkowicz"], "venue": "Computational Intelligence, vol. 20, no. 1, pp. 18\u201336, 2004.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2004}, {"title": "Analyzing the oversampling of different classes and types of examples in multi-class imbalanced datasets", "author": ["J.A. S\u00e1ez", "B. Krawczyk", "M. Wo\u017aniak"], "venue": "Pattern Recognition, vol. (In press), 2016.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Online sequential classification of imbalanced data by combining extreme learning machine and improved smote algorithm", "author": ["W. Mao", "J. Wang", "L. Wang"], "venue": "2015 International Joint Conference on Neural Networks (IJCNN), 2015, pp. 1\u20138.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "A novelty detection approach to classification", "author": ["N. Japkowicz", "C. Myers", "M.A. Gluck"], "venue": "Proceedings of the 14th international joint conference on Artificial intelligence, 1995, pp. 518\u2013523.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1995}, {"title": "The influence of class imbalance on cost-sensitive learning: An empirical study", "author": ["X.-Y. Liu", "Z.-H. Zhou"], "venue": "Sixth International Conference on Data Mining (ICDM\u201906), 2006, pp. 970\u2013974.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning when training data are costly: the effect of class distribution on tree induction", "author": ["G.M. Weiss", "F. Provost"], "venue": "Journal of Artificial Intelligence Research, 2003, pp. 315\u2013354, 2003.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2003}, {"title": "Cost-sensitive online classification", "author": ["J. Wang", "P. Zhao", "S.C. Hoi"], "venue": "IEEE Transaction on Knowledge and Data Engineering, vol. 26, no. 10, pp. 2425 \u2013 2438, 2014.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Ensemble based systems in decision making", "author": ["R. Polikar"], "venue": "IEEE Circuits and Systems Magazine, vol. 6, no. 3, pp. 21\u201345, 2006.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2006}, {"title": "A review on ensembles for the class imbalance problem: Bagging- , boosting-, and hybrid-based approaches", "author": ["M. Galar", "A. Fernandez", "E. Barrenechea", "H. Bustince", "F. Herrera"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, vol. PP, pp. 1\u201322, 2011.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2011}, {"title": "Classifying imbalanced data using a bagging ensemble variation", "author": ["C. Li"], "venue": "Proceedings of the 45th annual southeast regional conference, 2007, pp. 203\u2013208.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2007}, {"title": "Exploratory undersampling for class imbalance learning", "author": ["X.-Y. Liu", "J. Wu", "Z.-H. Zhou"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, vol. 39, no. 2, pp. 539\u2013550, 2009.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2009}, {"title": "SMOTE- Boost: Improving prediction of the minority class in boosting", "author": ["N.V. Chawla", "A. Lazarevic", "L.O. Hall", "K.W. Bowyer"], "venue": "Knowledge Discovery in Databases: PKDD 2003, vol. 2838, 2003, pp. 107\u2013119.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2003}, {"title": "Neighbourhood sampling in bagging for imbalanced data", "author": ["J. B\u0142aszczy\u0144ski", "J. Stefanowski"], "venue": "Special Issue on Information Processing and Machine Learning for Applications of Engineering, Neurocomputing, vol. 150, no. Part B, p. 529542, 2015.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluating boosting algorithms to classify rare classes: Comparison and improvements", "author": ["M.V. Joshi", "V. Kumar", "R.C. Agarwal"], "venue": "Proceedings IEEE International Conference on Data Mining, 2001, pp. 257\u2013264.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2001}, {"title": "Exploiting diversity in ensembles: Improving the performance on unbalanced datasets", "author": ["N.V. Chawla", "J. Sylvester"], "venue": "Multiple Classifier Systems, vol. 4472, pp. 397\u2013406, 2007.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning from imbalanced data sets with boosting and data generation: the databoost-im approach", "author": ["H. Guo", "H.L. Viktor"], "venue": "SIGKDD Explor. Newsl., vol. 6, no. 1, pp. 30\u201339, 2004.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2004}, {"title": "Adacost: Misclassification cost-sensitive boosting", "author": ["W. Fan", "S.J. Stolfo", "J. Zhang", "P.K. Chan"], "venue": "Proceedings of the 16th International Conference on Machine Learning, 1999, pp. 97\u2013105.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1999}, {"title": "Online bagging and boosting", "author": ["N.C. Oza"], "venue": "IEEE International Conference on Systems, Man and Cybernetics, pp. 2340\u20132345, 2005.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2005}, {"title": "Weighted online sequential extreme learning machine for class imbalance learning", "author": ["B. Mirza", "Z. Lin", "K.-A. Toh"], "venue": "Neural Processing Letters, vol. 38, no. 3, pp. 465\u2013486, 2013.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning when data sets are imbalanced and when costs are unequal and unknown", "author": ["M.A. Maloof"], "venue": "Workshop on Learning from Imbalanced Data Sets II, ICML, 2003.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2003}, {"title": "An introduction to roc analysis", "author": ["T. Fawcett"], "venue": "Pattern Recognition Letters, vol. 27, no. 8, pp. 861\u2013874, 2006.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2006}, {"title": "A systematic analysis of performance measures for classification tasks", "author": ["M. Sokolova", "G. Lapalme"], "venue": "Information Processing and Management, vol. 45, no. 4, p. 427437, 2009.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2009}, {"title": "Boosting for learning multiple classes with imbalanced class distribution", "author": ["Y. Sun", "M.S. Kamel", "Y. Wang"], "venue": "Sixth International Conference on Data Mining (ICDM\u201906), 2006, pp. 592\u2013602.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2006}, {"title": "A simple generalisation of the area under the roc curve for multiple class classification problems", "author": ["D.J. Hand", "R.J. Till"], "venue": "Machine Learning, vol. 45, no. 2, pp. 171\u2013186, 2001.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2001}, {"title": "DDD: A new ensemble approach for dealing with concept drift", "author": ["L.L. Minku", "X. Yao"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 24, no. 4, pp. 619 \u2013633, 2012.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2012}, {"title": "The impact of diversity on online ensemble learning in the presence of concept drift", "author": ["L.L. Minku", "A.P. White", "X. Yao"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 5, pp. 730\u2013742, 2010.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2010}, {"title": "The impact of changing populations on classifier performance", "author": ["M.G. Kelly", "D.J. Hand", "N.M. Adams"], "venue": "Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1999, pp. 367\u2013371.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 1999}, {"title": "Incremental learning of concept drift in nonstationary environments", "author": ["R. Elwell", "R. Polikar"], "venue": "IEEE Transactions on Neural Networks, vol. 22, no. 10, pp. 1517\u20131531, 2011.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2011}, {"title": "Incremental learning of concept drift from streaming imbalanced data", "author": ["G. Ditzler", "R. Polikar"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 10, pp. 2283 \u2013 2301, 2013.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2013}, {"title": "Early drift detection method", "author": ["M. Baena-Garcia", "J. del Campo-Avila", "R. Fidalgo", "A. Bifet", "R. Gavalda", "R. Morales-Bueno"], "venue": "2006.  IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. XX, NO. X, FEBRUARY 2017  18", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2006}, {"title": "Characterizing concept drift", "author": ["G.I. Webb", "R. Hyde", "H. Cao", "H.L. Nguyen", "F. Petitjean"], "venue": "Data Mining and Knowledge Discovery, vol. (In print), 2016.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2016}, {"title": "Real-time data mining of non-stationary data streams from sensor networks", "author": ["L. Cohen", "G. Avrahami-Bakish", "M. Last", "A. Kandel", "O. Kipersztok"], "venue": "Information Fusion, vol. 9, no. 3, pp. 344\u2013353, 2008.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning with drift detection", "author": ["J. Gama", "P. Medas", "G. Castillo", "P. Rodrigues"], "venue": "Advances in Artificial Intelligence, vol. 3171, pp. 286\u2013295, 2004.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2004}, {"title": "Concept drift detection through resampling", "author": ["M. Harel", "S. Mannor", "R. El-Yaniv", "K. Crammer"], "venue": "Proceedings of The 31st International Conference on Machine Learning, 2014, p. 10091017.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2014}, {"title": "Reacting to different types of concept drift: The accuracy updated ensemble algorithm", "author": ["D. Brzezinski", "J. Stefanowski"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 25, no. 1, pp. 81 \u2013 94, 2014.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical change-detection tests", "author": ["C. Alippi", "G. Boracchi", "M. Roveri"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 28, no. 2, pp. 246 \u2013 258, 2017.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2017}, {"title": "On evaluating stream learning algorithms", "author": ["J. Gama", "R. Sebasti\u00e3o", "P.P. Rodrigues"], "venue": "Machine Learning, vol. 90, no. 3, pp. 317\u2013346, 2013.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2013}, {"title": "Handling concept drift in text data stream constrained by high labelling cost", "author": ["P. Lindstrom", "S.J. Delany", "B.M. Namee"], "venue": "Proceeding of the 23rd International Florida Artificial Intelligence Research Society Conference, 2010.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards social user profiling: unified and discriminative influence model for inferring home locations", "author": ["R. Li", "S. Wang", "H. Deng", "R. Wang", "K.C.-C. Chang"], "venue": "KDD, 2012, pp. 1023\u20131031.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2012}, {"title": "Graph ensemble boosting for imbalanced noisy graph stream classification", "author": ["S. Pan", "J. Wu", "X. Zhu", "C. Zhang"], "venue": "IEEE Transactions on Cybernetics, vol. 45, no. 5, pp. 954 \u2013 968, 2015.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2015}, {"title": "Classifying data streams with skewed class distributions and concept drifts", "author": ["J. Gao", "B. Ding", "J. Han", "W. Fan", "P.S. Yu"], "venue": "IEEE Internet Computing, vol. 12, no. 6, pp. 37\u201349, 2008.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2008}, {"title": "A general framework for mining concept-drifting data streams with skewed distributions", "author": ["J. Gao", "W. Fan", "J. Han", "P.S. Yu"], "venue": "Proceedings of SIAM ICDM, 2007, pp. 3\u201314.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2007}, {"title": "Classifying imbalanced data streams via dynamic feature group weighting with importance sampling", "author": ["K. Wu", "A. Edwards", "W. Fan", "J. Gao", "K. Zhang"], "venue": "Proceedings of the 2014 SIAM International Conference on Data Mining, 2014, pp. 722\u2013730.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2014}, {"title": "Sera: Selectively recursive approach towards nonstationary imbalanced stream data mining", "author": ["S. Chen", "H. He"], "venue": "International Joint Conference on Neural Networks, 2009, pp. 522\u2013529.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards incremental learning of nonstationary imbalanced data stream: a multiple selectively recursive approach", "author": ["\u2014\u2014"], "venue": "Evolving Systems, vol. 2, no. 1, pp. 35\u201350, 2011.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2011}, {"title": "Incremental learning of concept drift from streaming imbalanced data", "author": ["G. Ditzler", "R. Polikar"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 10, pp. 2283 \u2013 2301, 2013.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning in non-stationary environments with class imbalance", "author": ["T.R. Hoens", "N.V. Chawla"], "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, 2013, pp. 168\u2013176.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2013}, {"title": "Heuristic updatable weighted random subspaces for non-stationary environments", "author": ["T.R. Hoens", "N.V. Chawla", "R. Polikar"], "venue": "IEEE 11th International Conference on Data Mining (ICDM), 2011, pp. 241\u2013 250.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2011}, {"title": "Using hddt to avoid instances propagation in unbalanced and evolving data streams", "author": ["A.D. Pozzolo", "R. Johnson", "O. Caelen"], "venue": "International Joint Conference on Neural Networks (IJCNN), 2014, pp. 588 \u2013 594.", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2014}, {"title": "Prequential auc: properties of the area under the roc curve for data streams with concept drift", "author": ["D. Brzezinski", "J. Stefanowski"], "venue": "Knowledge and Information Systems, pp. 1\u201332, 2017.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2017}, {"title": "Continuous inspection schemes", "author": ["E.S. Page"], "venue": "Biometrika, vol. 41, no. 1/2, pp. 100\u2013115, 1954.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 1954}, {"title": "Online neural network model for non-stationary and imbalanced data stream classification", "author": ["A. Ghazikhani", "R. Monsefi", "H.S. Yazdi"], "venue": "International Journal of Machine Learning and Cybernetics, vol. 5, no. 1, pp. 51\u201362, 2014.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2014}, {"title": "A fast and accurate online sequential learning algorithm for feedforward networks", "author": ["N. ying Liang", "G. bin Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Neural Networks, vol. 17, no. 6, pp. 1411 \u2013 1423, 2006.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2006}, {"title": "A streaming ensemble algorithm (sea) for large-scale classification", "author": ["W.N. Street", "Y. Kim"], "venue": "pp. 377\u2013382, 2001.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2001}, {"title": "PAKDD data mining competition 2009: New ways of using known methods", "author": ["C. Linhart", "G. Harari", "S. Abramovich", "A. Buchris"], "venue": "New Frontiers in Applied Data Mining, Lecture Notes in Computer Science, vol. 5669, pp. 99\u2013105, 2010.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards social user profiling: unified and discriminative influence model for inferring home locations", "author": ["R. Li", "S. Wang", "H. Deng", "R. Wang", "K. Chang"], "venue": "Proceedings of the 18th ACM SIGKDD, 2012, pp. 1023\u20131031.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2012}, {"title": "Dealing with multiple classes in online class imbalance learning", "author": ["S. Wang", "L.L. Minku", "X. Yao"], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16), 2016, pp. 2118\u20132124.", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Applications in various domains such as risk management [1], anomaly detection [2], software engineering [3], and social media mining [4] are affected by both", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "Applications in various domains such as risk management [1], anomaly detection [2], software engineering [3], and social media mining [4] are affected by both", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "Applications in various domains such as risk management [1], anomaly detection [2], software engineering [3], and social media mining [4] are affected by both", "startOffset": 105, "endOffset": 108}, {"referenceID": 3, "context": "Applications in various domains such as risk management [1], anomaly detection [2], software engineering [3], and social media mining [4] are affected by both", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": ", at least one category is minority compared to other categories [5].", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "when learning from data streams [6].", "startOffset": 32, "endOffset": 35}, {"referenceID": 6, "context": "real concept drift [7], changes in P (y | x)).", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "resampling methods) facilitate concept drift detection and online prediction? Six recent approaches, DDM-OCI [8], LFR [9], PAUC-PH [10] [96], OOB [11], RLSACP [12] and ESOS-ELM [13], are compared and analyzed in depth under each of the three fundamental types of concept drift (i.", "startOffset": 109, "endOffset": 112}, {"referenceID": 8, "context": "resampling methods) facilitate concept drift detection and online prediction? Six recent approaches, DDM-OCI [8], LFR [9], PAUC-PH [10] [96], OOB [11], RLSACP [12] and ESOS-ELM [13], are compared and analyzed in depth under each of the three fundamental types of concept drift (i.", "startOffset": 118, "endOffset": 121}, {"referenceID": 9, "context": "resampling methods) facilitate concept drift detection and online prediction? Six recent approaches, DDM-OCI [8], LFR [9], PAUC-PH [10] [96], OOB [11], RLSACP [12] and ESOS-ELM [13], are compared and analyzed in depth under each of the three fundamental types of concept drift (i.", "startOffset": 131, "endOffset": 135}, {"referenceID": 95, "context": "resampling methods) facilitate concept drift detection and online prediction? Six recent approaches, DDM-OCI [8], LFR [9], PAUC-PH [10] [96], OOB [11], RLSACP [12] and ESOS-ELM [13], are compared and analyzed in depth under each of the three fundamental types of concept drift (i.", "startOffset": 136, "endOffset": 140}, {"referenceID": 10, "context": "resampling methods) facilitate concept drift detection and online prediction? Six recent approaches, DDM-OCI [8], LFR [9], PAUC-PH [10] [96], OOB [11], RLSACP [12] and ESOS-ELM [13], are compared and analyzed in depth under each of the three fundamental types of concept drift (i.", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "resampling methods) facilitate concept drift detection and online prediction? Six recent approaches, DDM-OCI [8], LFR [9], PAUC-PH [10] [96], OOB [11], RLSACP [12] and ESOS-ELM [13], are compared and analyzed in depth under each of the three fundamental types of concept drift (i.", "startOffset": 159, "endOffset": 163}, {"referenceID": 12, "context": "resampling methods) facilitate concept drift detection and online prediction? Six recent approaches, DDM-OCI [8], LFR [9], PAUC-PH [10] [96], OOB [11], RLSACP [12] and ESOS-ELM [13], are compared and analyzed in depth under each of the three fundamental types of concept drift (i.", "startOffset": 177, "endOffset": 181}, {"referenceID": 5, "context": "According to the most agreed definitions [6] [14], chunk-based learning algorithms process a batch of data examples at each time step, such as the case of daily internet usage from a set of users; online learning algorithms process examples one by one and the predictive model is updated after receiving each example [15], such as the case of sensor readings at", "startOffset": 41, "endOffset": 44}, {"referenceID": 13, "context": "According to the most agreed definitions [6] [14], chunk-based learning algorithms process a batch of data examples at each time step, such as the case of daily internet usage from a set of users; online learning algorithms process examples one by one and the predictive model is updated after receiving each example [15], such as the case of sensor readings at", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "According to the most agreed definitions [6] [14], chunk-based learning algorithms process a batch of data examples at each time step, such as the case of daily internet usage from a set of users; online learning algorithms process examples one by one and the predictive model is updated after receiving each example [15], such as the case of sensor readings at", "startOffset": 317, "endOffset": 321}, {"referenceID": 15, "context": "It is usually referred to as any algorithm that can process data streams with certain criteria met [16].", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "They both build and continuously update a learning model to accommodate newly available data, and simultaneously maintain its performance on old data, giving rise to the stability-plasticity dilemma [17].", "startOffset": 199, "endOffset": 203}, {"referenceID": 5, "context": "On the other hand, the way of designing online and chunkbased learning algorithms can be very different [6].", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "Class imbalance aggravates the learning difficulty [5] and complicates the data status [18].", "startOffset": 51, "endOffset": 54}, {"referenceID": 17, "context": "Class imbalance aggravates the learning difficulty [5] and complicates the data status [18].", "startOffset": 87, "endOffset": 91}, {"referenceID": 18, "context": "imbalance is an important data feature, commonly seen in applications such as spam filtering [19] and fault diagnosis [2] [3].", "startOffset": 93, "endOffset": 97}, {"referenceID": 1, "context": "imbalance is an important data feature, commonly seen in applications such as spam filtering [19] and fault diagnosis [2] [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 2, "context": "imbalance is an important data feature, commonly seen in applications such as spam filtering [19] and fault diagnosis [2] [3].", "startOffset": 122, "endOffset": 125}, {"referenceID": 19, "context": "It has been wellstudied in offline learning [20], and has attracted growing", "startOffset": 44, "endOffset": 48}, {"referenceID": 20, "context": "attention in data stream learning in recent years [21].", "startOffset": 50, "endOffset": 54}, {"referenceID": 21, "context": "In many applications, such as energy forecasting and climate data analysis [22], the data generator operates in nonstationary environments.", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "in [18] as a guide for algorithm design.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "1: Learning framework for online class imbalance learning [18].", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "of an online class imbalance algorithm can be described as \u201crecognizing minority-class data effectively, adaptively and timely without sacrificing the performance on the majority class\u201d [18].", "startOffset": 186, "endOffset": 190}, {"referenceID": 22, "context": "In reality, it is common to see that the majority class has accuracy close to 100% and the minority class has very low accuracy between 0%-10% [23].", "startOffset": 143, "endOffset": 147}, {"referenceID": 19, "context": "The negative effect of class imbalance on classifiers, such as decision trees [20], neural networks [24], k-Nearest Neighbour (kNN) [25] [26] [27] and SVM [28] [29], has been studied.", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "The negative effect of class imbalance on classifiers, such as decision trees [20], neural networks [24], k-Nearest Neighbour (kNN) [25] [26] [27] and SVM [28] [29], has been studied.", "startOffset": 100, "endOffset": 104}, {"referenceID": 24, "context": "The negative effect of class imbalance on classifiers, such as decision trees [20], neural networks [24], k-Nearest Neighbour (kNN) [25] [26] [27] and SVM [28] [29], has been studied.", "startOffset": 132, "endOffset": 136}, {"referenceID": 25, "context": "The negative effect of class imbalance on classifiers, such as decision trees [20], neural networks [24], k-Nearest Neighbour (kNN) [25] [26] [27] and SVM [28] [29], has been studied.", "startOffset": 137, "endOffset": 141}, {"referenceID": 26, "context": "The negative effect of class imbalance on classifiers, such as decision trees [20], neural networks [24], k-Nearest Neighbour (kNN) [25] [26] [27] and SVM [28] [29], has been studied.", "startOffset": 142, "endOffset": 146}, {"referenceID": 27, "context": "The negative effect of class imbalance on classifiers, such as decision trees [20], neural networks [24], k-Nearest Neighbour (kNN) [25] [26] [27] and SVM [28] [29], has been studied.", "startOffset": 155, "endOffset": 159}, {"referenceID": 28, "context": "The negative effect of class imbalance on classifiers, such as decision trees [20], neural networks [24], k-Nearest Neighbour (kNN) [25] [26] [27] and SVM [28] [29], has been studied.", "startOffset": 160, "endOffset": 164}, {"referenceID": 29, "context": "percentage of the minority class in the data set [30], the size ratio between classes [31], or simply a list of the number of examples in each class [32].", "startOffset": 49, "endOffset": 53}, {"referenceID": 30, "context": "percentage of the minority class in the data set [30], the size ratio between classes [31], or simply a list of the number of examples in each class [32].", "startOffset": 86, "endOffset": 90}, {"referenceID": 31, "context": "percentage of the minority class in the data set [30], the size ratio between classes [31], or simply a list of the number of examples in each class [32].", "startOffset": 149, "endOffset": 153}, {"referenceID": 32, "context": "The coefficient of variance is used in [33], which is less straightforward.", "startOffset": 39, "endOffset": 43}, {"referenceID": 17, "context": "To define the imbalanced degree suitable for online learning, a real-time indicator was proposed \u2013 time-decayed class size [18], expressing the size percentage of each class in the data stream.", "startOffset": 123, "endOffset": 127}, {"referenceID": 10, "context": "Based on this, a class imbalance detector was proposed to determine which classes should be regarded as the minority/majority and how imbalanced the current data stream is, and then used for designing better online classifiers [11] [3].", "startOffset": 227, "endOffset": 231}, {"referenceID": 2, "context": "Based on this, a class imbalance detector was proposed to determine which classes should be regarded as the minority/majority and how imbalanced the current data stream is, and then used for designing better online classifiers [11] [3].", "startOffset": 232, "endOffset": 235}, {"referenceID": 33, "context": "complexity comprises issues such as overlapping [34] [35] and small disjuncts [36].", "startOffset": 48, "endOffset": 52}, {"referenceID": 34, "context": "complexity comprises issues such as overlapping [34] [35] and small disjuncts [36].", "startOffset": 53, "endOffset": 57}, {"referenceID": 35, "context": "complexity comprises issues such as overlapping [34] [35] and small disjuncts [36].", "startOffset": 78, "endOffset": 82}, {"referenceID": 36, "context": "The small disjunct problem is associated with the within-class imbalance [37].", "startOffset": 73, "endOffset": 77}, {"referenceID": 4, "context": "In other words, the rarity of the minority class can be in a relative or absolute sense in terms of the number of available examples [5].", "startOffset": 133, "endOffset": 136}, {"referenceID": 37, "context": "In particular, authors in [38] [39] distinguished and analysed four types of data distributions in the minority class \u2013 safe, borderline, outliers and rare examples.", "startOffset": 26, "endOffset": 30}, {"referenceID": 38, "context": "In particular, authors in [38] [39] distinguished and analysed four types of data distributions in the minority class \u2013 safe, borderline, outliers and rare examples.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "be the harder cases in online applications [11].", "startOffset": 43, "endOffset": 47}, {"referenceID": 31, "context": "For example, SMOTE [32] is a widely used oversampling method, which generates new minorityclass data points based on the similarities between original minority-class examples in the feature space.", "startOffset": 19, "endOffset": 23}, {"referenceID": 39, "context": "Other smart oversampling techniques include Borderline-SMOTE [40], ADASYN [41], MWMOTE [42], to name but a few.", "startOffset": 61, "endOffset": 65}, {"referenceID": 40, "context": "Other smart oversampling techniques include Borderline-SMOTE [40], ADASYN [41], MWMOTE [42], to name but a few.", "startOffset": 74, "endOffset": 78}, {"referenceID": 41, "context": "Other smart oversampling techniques include Borderline-SMOTE [40], ADASYN [41], MWMOTE [42], to name but a few.", "startOffset": 87, "endOffset": 91}, {"referenceID": 42, "context": "undersampling techniques include Tomek links [43], Onesided selection [44], Neighbourhood cleaning rule [45], etc.", "startOffset": 45, "endOffset": 49}, {"referenceID": 43, "context": "undersampling techniques include Tomek links [43], Onesided selection [44], Neighbourhood cleaning rule [45], etc.", "startOffset": 70, "endOffset": 74}, {"referenceID": 44, "context": "undersampling techniques include Tomek links [43], Onesided selection [44], Neighbourhood cleaning rule [45], etc.", "startOffset": 104, "endOffset": 108}, {"referenceID": 45, "context": "The effectiveness of resampling techniques have been proved in real-world applications [46].", "startOffset": 87, "endOffset": 91}, {"referenceID": 46, "context": "rate [47], which is relatively easy for two-class data sets, but becomes more complicated for multi-class data sets [48].", "startOffset": 5, "endOffset": 9}, {"referenceID": 47, "context": "rate [47], which is relatively easy for two-class data sets, but becomes more complicated for multi-class data sets [48].", "startOffset": 116, "endOffset": 120}, {"referenceID": 29, "context": "Empirical studies have been carried out to compare different resampling methods [30].", "startOffset": 80, "endOffset": 84}, {"referenceID": 48, "context": "Some initial effort has been made recently, to extend smart resampling techniques to online learning [49].", "startOffset": 101, "endOffset": 105}, {"referenceID": 49, "context": "ifying their training mechanism with the direct goal of better accuracy on the minority class, including one-class learning [50], cost-sensitive learning [51] and threshold methods [52].", "startOffset": 124, "endOffset": 128}, {"referenceID": 50, "context": "ifying their training mechanism with the direct goal of better accuracy on the minority class, including one-class learning [50], cost-sensitive learning [51] and threshold methods [52].", "startOffset": 154, "endOffset": 158}, {"referenceID": 51, "context": "ifying their training mechanism with the direct goal of better accuracy on the minority class, including one-class learning [50], cost-sensitive learning [51] and threshold methods [52].", "startOffset": 181, "endOffset": 185}, {"referenceID": 52, "context": "Some online cost-sensitive methods have been proposed, such as CSOGD [53] and RLSACP [12].", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "Some online cost-sensitive methods have been proposed, such as CSOGD [53] and RLSACP [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 53, "context": "Finally, ensemble learning (also known as multiple classifier systems) [54] has become a major category of approaches to", "startOffset": 71, "endOffset": 75}, {"referenceID": 54, "context": "handling class imbalance [55].", "startOffset": 25, "endOffset": 29}, {"referenceID": 55, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 107, "endOffset": 111}, {"referenceID": 56, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 112, "endOffset": 116}, {"referenceID": 57, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 117, "endOffset": 121}, {"referenceID": 58, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 122, "endOffset": 126}, {"referenceID": 59, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 172, "endOffset": 176}, {"referenceID": 60, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 177, "endOffset": 181}, {"referenceID": 61, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 182, "endOffset": 186}, {"referenceID": 62, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 187, "endOffset": 191}, {"referenceID": 10, "context": "ance learning, such as OOB and UOB [11] applying random oversampling and undersampling in Online Bagging [64], and WOS-ELM [65] training a set of cost-sensitive online extreme learning machines.", "startOffset": 35, "endOffset": 39}, {"referenceID": 63, "context": "ance learning, such as OOB and UOB [11] applying random oversampling and undersampling in Online Bagging [64], and WOS-ELM [65] training a set of cost-sensitive online extreme learning machines.", "startOffset": 105, "endOffset": 109}, {"referenceID": 64, "context": "ance learning, such as OOB and UOB [11] applying random oversampling and undersampling in Online Bagging [64], and WOS-ELM [65] training a set of cost-sensitive online extreme learning machines.", "startOffset": 123, "endOffset": 127}, {"referenceID": 43, "context": "[44] proposed to use G-mean to replace overall accuracy:", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "According to [5], any metric that uses values from both", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "estimating the area under the curve, varying in [0, 1].", "startOffset": 48, "endOffset": 54}, {"referenceID": 65, "context": "AUC is usually generated by varying the classification decision threshold for separating positive and negative classes in the testing data set [66] [67].", "startOffset": 143, "endOffset": 147}, {"referenceID": 66, "context": "AUC is usually generated by varying the classification decision threshold for separating positive and negative classes in the testing data set [66] [67].", "startOffset": 148, "endOffset": 152}, {"referenceID": 9, "context": "Although a recent study has modified AUC for evaluating online classifiers [10], it still needs to collect recently received examples.", "startOffset": 75, "endOffset": 79}, {"referenceID": 67, "context": "Their multi-class versions have been developed [68] [69] [70].", "startOffset": 47, "endOffset": 51}, {"referenceID": 68, "context": "Their multi-class versions have been developed [68] [69] [70].", "startOffset": 52, "endOffset": 56}, {"referenceID": 69, "context": "Their multi-class versions have been developed [68] [69] [70].", "startOffset": 57, "endOffset": 61}, {"referenceID": 67, "context": "Measures Multi-class Online Sensitive to Imbalance recall yes yes no precision no [68] yes yes Fm no [68] yes yes Gm yes [69] yes no AUC no (See MAUC [70]) no (See PAUC [10]) no", "startOffset": 82, "endOffset": 86}, {"referenceID": 67, "context": "Measures Multi-class Online Sensitive to Imbalance recall yes yes no precision no [68] yes yes Fm no [68] yes yes Gm yes [69] yes no AUC no (See MAUC [70]) no (See PAUC [10]) no", "startOffset": 101, "endOffset": 105}, {"referenceID": 68, "context": "Measures Multi-class Online Sensitive to Imbalance recall yes yes no precision no [68] yes yes Fm no [68] yes yes Gm yes [69] yes no AUC no (See MAUC [70]) no (See PAUC [10]) no", "startOffset": 121, "endOffset": 125}, {"referenceID": 69, "context": "Measures Multi-class Online Sensitive to Imbalance recall yes yes no precision no [68] yes yes Fm no [68] yes yes Gm yes [69] yes no AUC no (See MAUC [70]) no (See PAUC [10]) no", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "Measures Multi-class Online Sensitive to Imbalance recall yes yes no precision no [68] yes yes Fm no [68] yes yes Gm yes [69] yes no AUC no (See MAUC [70]) no (See PAUC [10]) no", "startOffset": 169, "endOffset": 173}, {"referenceID": 6, "context": "2) Concept drift: Concept drift is said to occur when the joint probability P (x, y) changes [7] [71] [72].", "startOffset": 93, "endOffset": 96}, {"referenceID": 70, "context": "2) Concept drift: Concept drift is said to occur when the joint probability P (x, y) changes [7] [71] [72].", "startOffset": 97, "endOffset": 101}, {"referenceID": 71, "context": "2) Concept drift: Concept drift is said to occur when the joint probability P (x, y) changes [7] [71] [72].", "startOffset": 102, "endOffset": 106}, {"referenceID": 72, "context": "more challenging? Concept drift can manifest three fundamental forms of changes corresponding to the three major variables in the Bayes\u2019 theorem [73]: 1) a change in prior probability P (y); 2) a change in class-conditional pdf p (x | y); 3) a change in posterior probability P (y | x).", "startOffset": 145, "endOffset": 149}, {"referenceID": 73, "context": "Elwell and Polikar claimed that this type of drift is the result of an incomplete representation of the true distribution in current data, which simply requires providing supplemental data information to the learning model [74].", "startOffset": 223, "endOffset": 227}, {"referenceID": 20, "context": "The other two types belong to virtual concept drift [21], which does not change the decision (class) boundaries.", "startOffset": 52, "endOffset": 56}, {"referenceID": 6, "context": "on feedback about the performance of the classifier, while techniques for handling virtual drift can operate without such feedback [7].", "startOffset": 131, "endOffset": 134}, {"referenceID": 71, "context": "A detailed and mutually exclusive categorization can be found in [72].", "startOffset": 65, "endOffset": 69}, {"referenceID": 74, "context": "to old data [75], gradual drifts are often more difficult, because the slow change can delay or hide the hint left by the drift.", "startOffset": 12, "endOffset": 16}, {"referenceID": 75, "context": "We can see some drift detection methods specifically designed for gradual concept drift, such as Early Drift Detection method (EDDM) [76].", "startOffset": 133, "endOffset": 137}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "\u2019s taxonomy based on the four modules of an adaptive learning system [7], and Webb et al.", "startOffset": 69, "endOffset": 72}, {"referenceID": 76, "context": "\u2019s quantitative characterization [77].", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "[14] for its simplicity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "general observation is that, while active approaches are quite effective in detecting abrupt drift, passive approaches are very good at overcoming gradual drift [74] [14].", "startOffset": 161, "endOffset": 165}, {"referenceID": 13, "context": "general observation is that, while active approaches are quite effective in detecting abrupt drift, passive approaches are very good at overcoming gradual drift [74] [14].", "startOffset": 166, "endOffset": 170}, {"referenceID": 77, "context": "drift based on the change in the classification error, including OLIN [78], DDM [79] and PERM [80].", "startOffset": 70, "endOffset": 74}, {"referenceID": 78, "context": "drift based on the change in the classification error, including OLIN [78], DDM [79] and PERM [80].", "startOffset": 80, "endOffset": 84}, {"referenceID": 79, "context": "drift based on the change in the classification error, including OLIN [78], DDM [79] and PERM [80].", "startOffset": 94, "endOffset": 98}, {"referenceID": 80, "context": "Some other algorithms are specifically designed for data streams coming in batches, such as AUE [81] and the Learn++ family [74].", "startOffset": 96, "endOffset": 100}, {"referenceID": 73, "context": "Some other algorithms are specifically designed for data streams coming in batches, such as AUE [81] and the Learn++ family [74].", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "See [14] for the full list of techniques under each category.", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "Wang and Abraham [9] use a histogram to visualize the distribution of detection points from the drift detection approach over multiple runs.", "startOffset": 17, "endOffset": 20}, {"referenceID": 81, "context": "A very recent algorithm, Hierarchical Change-Detection Tests (HCDTs), was proposed to explicitly deal with the tradeoff [82].", "startOffset": 120, "endOffset": 124}, {"referenceID": 6, "context": "There are two common ways to depict such performance over time \u2013 holdout and prequential evaluation [7].", "startOffset": 100, "endOffset": 103}, {"referenceID": 82, "context": "This problem can be solved by using a sliding window or a time-based fading factor that weigh observations [83].", "startOffset": 107, "endOffset": 111}, {"referenceID": 2, "context": "predict faults in sensors accurately and timely [3].", "startOffset": 48, "endOffset": 51}, {"referenceID": 83, "context": "2) Spam filtering with p (x | y) drift: Spam filtering is a typical classification problem involving class imbalance and concept drift [84].", "startOffset": 135, "endOffset": 139}, {"referenceID": 6, "context": "For example, one of the spamming behaviours is to change email content and presentation in disguise, implying a possible classconditional pdf (p (x | y)) change [7].", "startOffset": 161, "endOffset": 164}, {"referenceID": 84, "context": "Machine learning algorithms can be used to discover who is interested in the product from the large amount of tweets [85].", "startOffset": 117, "endOffset": 121}, {"referenceID": 85, "context": "Some research efforts have been made to address the joint problem of concept drift and class imbalance, due to the rising need from practical problems [86] [1].", "startOffset": 151, "endOffset": 155}, {"referenceID": 0, "context": "Some research efforts have been made to address the joint problem of concept drift and class imbalance, due to the rising need from practical problems [86] [1].", "startOffset": 156, "endOffset": 159}, {"referenceID": 86, "context": "Bagging is one of the earliest algorithms, which builds an ensemble of classifiers trained on a more balanced set of data through resampling and overcomes concept drift passively by weighing the base classifier based on their discriminative power [87] [88] [89].", "startOffset": 247, "endOffset": 251}, {"referenceID": 87, "context": "Bagging is one of the earliest algorithms, which builds an ensemble of classifiers trained on a more balanced set of data through resampling and overcomes concept drift passively by weighing the base classifier based on their discriminative power [87] [88] [89].", "startOffset": 252, "endOffset": 256}, {"referenceID": 88, "context": "Bagging is one of the earliest algorithms, which builds an ensemble of classifiers trained on a more balanced set of data through resampling and overcomes concept drift passively by weighing the base classifier based on their discriminative power [87] [88] [89].", "startOffset": 257, "endOffset": 261}, {"referenceID": 89, "context": "SERA [90] and REA [91] use similar ideas to Uncorrelated Bagging of building an ensemble of weighted classifiers, but with a \u201csmarter\u201d oversampling technique.", "startOffset": 5, "endOffset": 9}, {"referenceID": 90, "context": "SERA [90] and REA [91] use similar ideas to Uncorrelated Bagging of building an ensemble of weighted classifiers, but with a \u201csmarter\u201d oversampling technique.", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": "NIE are more recent algorithms, which tackle class imbalance through the oversampling technique SMOTE [32] or a sub-ensemble technique, and overcome concept drift through a dynamic weighting strategy [92].", "startOffset": 102, "endOffset": 106}, {"referenceID": 91, "context": "NIE are more recent algorithms, which tackle class imbalance through the oversampling technique SMOTE [32] or a sub-ensemble technique, and overcome concept drift through a dynamic weighting strategy [92].", "startOffset": 200, "endOffset": 204}, {"referenceID": 92, "context": "IP [93] improves HUWRS [94] to deal with imbalanced data streams by introducing an instance propagation scheme based on a Na\u0131\u0308ve Bayes classifier, and uses Hellinger distance as a", "startOffset": 3, "endOffset": 7}, {"referenceID": 93, "context": "IP [93] improves HUWRS [94] to deal with imbalanced data streams by introducing an instance propagation scheme based on a Na\u0131\u0308ve Bayes classifier, and uses Hellinger distance as a", "startOffset": 23, "endOffset": 27}, {"referenceID": 94, "context": "So, Hellinger Distance Decision Tree (HDDT) was proposed to use Hellinger distance as the decision tree splitting criteria that is imbalance-insensitive [95].", "startOffset": 153, "endOffset": 157}, {"referenceID": 13, "context": "Developing a true online algorithm for concept drift is very challenging because of the difficulties in measuring minority-class statistics using only one example at a time [14].", "startOffset": 173, "endOffset": 177}, {"referenceID": 7, "context": "Drift Detection Method for Online Class Imbalance (DDM-OCI) [8] is one of the very first algorithms detecting concept drift actively in imbalanced data streams online.", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "the confusion matrix \u2013 minority-class recall and precision and majority-class recall and precision, with statistically-supported bounds for drift detection [9].", "startOffset": 156, "endOffset": 159}, {"referenceID": 9, "context": "Instead of tracking several performance rates for each class, prequential AUC (PAUC) [10] [96] was proposed as an overall performance", "startOffset": 85, "endOffset": 89}, {"referenceID": 95, "context": "Instead of tracking several performance rates for each class, prequential AUC (PAUC) [10] [96] was proposed as an overall performance", "startOffset": 90, "endOffset": 94}, {"referenceID": 96, "context": "measure for online scenarios, and was used as the concept drift indicator in Page-Hinkley (PH) test [97].", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "algorithms RLSACP [12], ONN [98] and ESOS-ELM [13] adapt the classification model to non-stationary environments passively, and involve mechanisms to overcome class imbalance.", "startOffset": 18, "endOffset": 22}, {"referenceID": 97, "context": "algorithms RLSACP [12], ONN [98] and ESOS-ELM [13] adapt the classification model to non-stationary environments passively, and involve mechanisms to overcome class imbalance.", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "algorithms RLSACP [12], ONN [98] and ESOS-ELM [13] adapt the classification model to non-stationary environments passively, and involve mechanisms to overcome class imbalance.", "startOffset": 46, "endOffset": 50}, {"referenceID": 7, "context": "Approaches Category? Class Access to Additional data? Multi-class? P (y) drift? imbalance? old data? DDM-OCI [8] Active (change detection test + windowing) No No No No No LFR [9] Active (change detection test + windowing) No No No No No PAUC-PH [10] Active (change detection test + windowing) No Yes No No No RLSACP [12]/ONN [98] Passive (single classifier) Yes Yes No No Yes ESOS-ELM [13] Passive+Active (ensemble) Yes No Yes No No OOB/UOB using CID [11] Active (weighting) Yes No No No Yes", "startOffset": 109, "endOffset": 112}, {"referenceID": 8, "context": "Approaches Category? Class Access to Additional data? Multi-class? P (y) drift? imbalance? old data? DDM-OCI [8] Active (change detection test + windowing) No No No No No LFR [9] Active (change detection test + windowing) No No No No No PAUC-PH [10] Active (change detection test + windowing) No Yes No No No RLSACP [12]/ONN [98] Passive (single classifier) Yes Yes No No Yes ESOS-ELM [13] Passive+Active (ensemble) Yes No Yes No No OOB/UOB using CID [11] Active (weighting) Yes No No No Yes", "startOffset": 175, "endOffset": 178}, {"referenceID": 9, "context": "Approaches Category? Class Access to Additional data? Multi-class? P (y) drift? imbalance? old data? DDM-OCI [8] Active (change detection test + windowing) No No No No No LFR [9] Active (change detection test + windowing) No No No No No PAUC-PH [10] Active (change detection test + windowing) No Yes No No No RLSACP [12]/ONN [98] Passive (single classifier) Yes Yes No No Yes ESOS-ELM [13] Passive+Active (ensemble) Yes No Yes No No OOB/UOB using CID [11] Active (weighting) Yes No No No Yes", "startOffset": 245, "endOffset": 249}, {"referenceID": 11, "context": "Approaches Category? Class Access to Additional data? Multi-class? P (y) drift? imbalance? old data? DDM-OCI [8] Active (change detection test + windowing) No No No No No LFR [9] Active (change detection test + windowing) No No No No No PAUC-PH [10] Active (change detection test + windowing) No Yes No No No RLSACP [12]/ONN [98] Passive (single classifier) Yes Yes No No Yes ESOS-ELM [13] Passive+Active (ensemble) Yes No Yes No No OOB/UOB using CID [11] Active (weighting) Yes No No No Yes", "startOffset": 316, "endOffset": 320}, {"referenceID": 97, "context": "Approaches Category? Class Access to Additional data? Multi-class? P (y) drift? imbalance? old data? DDM-OCI [8] Active (change detection test + windowing) No No No No No LFR [9] Active (change detection test + windowing) No No No No No PAUC-PH [10] Active (change detection test + windowing) No Yes No No No RLSACP [12]/ONN [98] Passive (single classifier) Yes Yes No No Yes ESOS-ELM [13] Passive+Active (ensemble) Yes No Yes No No OOB/UOB using CID [11] Active (weighting) Yes No No No Yes", "startOffset": 325, "endOffset": 329}, {"referenceID": 12, "context": "Approaches Category? Class Access to Additional data? Multi-class? P (y) drift? imbalance? old data? DDM-OCI [8] Active (change detection test + windowing) No No No No No LFR [9] Active (change detection test + windowing) No No No No No PAUC-PH [10] Active (change detection test + windowing) No Yes No No No RLSACP [12]/ONN [98] Passive (single classifier) Yes Yes No No Yes ESOS-ELM [13] Passive+Active (ensemble) Yes No Yes No No OOB/UOB using CID [11] Active (weighting) Yes No No No Yes", "startOffset": 385, "endOffset": 389}, {"referenceID": 10, "context": "Approaches Category? Class Access to Additional data? Multi-class? P (y) drift? imbalance? old data? DDM-OCI [8] Active (change detection test + windowing) No No No No No LFR [9] Active (change detection test + windowing) No No No No No PAUC-PH [10] Active (change detection test + windowing) No Yes No No No RLSACP [12]/ONN [98] Passive (single classifier) Yes Yes No No Yes ESOS-ELM [13] Passive+Active (ensemble) Yes No Yes No No OOB/UOB using CID [11] Active (weighting) Yes No No No Yes", "startOffset": 451, "endOffset": 455}, {"referenceID": 98, "context": "ESOS-ELM is an ensemble approach, maintaining a set of online sequential extreme learning machines (OS-ELM) [99].", "startOffset": 108, "endOffset": 112}, {"referenceID": 64, "context": "ELM-store maintains a pool of weighted extreme learning machines (WELM) [65] to retain old information.", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "With a different goal of concept drift detection from the above, a class imbalance detection (CID) approach was proposed, aiming at P (y) changes [18].", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "When a new example xt arrives, w (t) k is incrementally updated by the following equation [18]:", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "in OOB and UOB [11] for deciding the resampling rate adaptively and overcoming class imbalance effectively over time.", "startOffset": 15, "endOffset": 19}, {"referenceID": 63, "context": "OOB and UOB integrate oversampling and undersampling respectively into ensemble algorithm Online Bagging (OB) [64].", "startOffset": 110, "endOffset": 114}, {"referenceID": 29, "context": "Oversampling and undersampling are one of the simplest and most effective techniques of tackling class imbalance [30].", "startOffset": 113, "endOffset": 117}, {"referenceID": 78, "context": "For an accurate analysis and comparable results, we choose two most commonly used artificial data generators, SINE1 [79] and SEA [100], to produce imbalanced data streams containing three simulated types of concept drift.", "startOffset": 116, "endOffset": 120}, {"referenceID": 99, "context": "For an accurate analysis and comparable results, we choose two most commonly used artificial data generators, SINE1 [79] and SEA [100], to produce imbalanced data streams containing three simulated types of concept drift.", "startOffset": 129, "endOffset": 134}, {"referenceID": 71, "context": "The drifting speed is defined as the inverse of the time taken for a new concept to completely replace the old one [72].", "startOffset": 115, "endOffset": 119}, {"referenceID": 75, "context": "SINE1g [76] and SEAg.", "startOffset": 7, "endOffset": 11}, {"referenceID": 100, "context": "drift, three real-world data sets are included in our experiment with unknown concept drift, which are PAKDD 2009 credit card data (PAKDD) [101], Weather data [75] and UDI TweeterCrawl data [102].", "startOffset": 139, "endOffset": 144}, {"referenceID": 74, "context": "drift, three real-world data sets are included in our experiment with unknown concept drift, which are PAKDD 2009 credit card data (PAKDD) [101], Weather data [75] and UDI TweeterCrawl data [102].", "startOffset": 159, "endOffset": 163}, {"referenceID": 101, "context": "drift, three real-world data sets are included in our experiment with unknown concept drift, which are PAKDD 2009 credit card data (PAKDD) [101], Weather data [75] and UDI TweeterCrawl data [102].", "startOffset": 190, "endOffset": 195}, {"referenceID": 102, "context": "We choose a time interval, containing 8774 examples and covering seven tweet topics [103].", "startOffset": 84, "endOffset": 89}, {"referenceID": 63, "context": "OB) [64] and OOB with CID [11] respectively for classification.", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "OB) [64] and OOB with CID [11] respectively for classification.", "startOffset": 26, "endOffset": 30}, {"referenceID": 10, "context": "UOB is not chosen, for the consideration that undersampling may cause unstable performance which may indirectly affect our observation [11].", "startOffset": 135, "endOffset": 139}, {"referenceID": 100, "context": "After the detailed analysis of the three types of concept drift, we now look into the performance of the above learning models on the three real-world data sets (PAKDD [101], Weather [75] and Tweet [102]) described in Section IV-A.", "startOffset": 168, "endOffset": 173}, {"referenceID": 74, "context": "After the detailed analysis of the three types of concept drift, we now look into the performance of the above learning models on the three real-world data sets (PAKDD [101], Weather [75] and Tweet [102]) described in Section IV-A.", "startOffset": 183, "endOffset": 187}, {"referenceID": 101, "context": "After the detailed analysis of the three types of concept drift, we now look into the performance of the above learning models on the three real-world data sets (PAKDD [101], Weather [75] and Tweet [102]) described in Section IV-A.", "startOffset": 198, "endOffset": 203}, {"referenceID": 10, "context": "For example, the training of OS-ELM in ESOS-ELM requires initialisation and validation data sets reflecting the correct data concepts, and the weighted OS-ELM was found to over-emphasize the minority class and present large performance variance sometimes in earlier studies [11].", "startOffset": 274, "endOffset": 278}, {"referenceID": 7, "context": "DDM-OCI [8], LFR [9], PAUC-PH [10], OOB [11], RLSACP [12] and ESOS-ELM [13], which one performs better for which type of concept drift? 3) Would applying class imbalance techniques (e.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "DDM-OCI [8], LFR [9], PAUC-PH [10], OOB [11], RLSACP [12] and ESOS-ELM [13], which one performs better for which type of concept drift? 3) Would applying class imbalance techniques (e.", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "DDM-OCI [8], LFR [9], PAUC-PH [10], OOB [11], RLSACP [12] and ESOS-ELM [13], which one performs better for which type of concept drift? 3) Would applying class imbalance techniques (e.", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "DDM-OCI [8], LFR [9], PAUC-PH [10], OOB [11], RLSACP [12] and ESOS-ELM [13], which one performs better for which type of concept drift? 3) Would applying class imbalance techniques (e.", "startOffset": 40, "endOffset": 44}, {"referenceID": 11, "context": "DDM-OCI [8], LFR [9], PAUC-PH [10], OOB [11], RLSACP [12] and ESOS-ELM [13], which one performs better for which type of concept drift? 3) Would applying class imbalance techniques (e.", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "DDM-OCI [8], LFR [9], PAUC-PH [10], OOB [11], RLSACP [12] and ESOS-ELM [13], which one performs better for which type of concept drift? 3) Would applying class imbalance techniques (e.", "startOffset": 71, "endOffset": 75}], "year": 2017, "abstractText": "As an emerging research topic, online class imbalance learning often combines the challenges of both class imbalance and concept drift. It deals with data streams having very skewed class distributions, where concept drift may occur. It has recently received increased research attention; however, very little work addresses the combined problem where both class imbalance and concept drift coexist. As the first systematic study of handling concept drift in class-imbalanced data streams, this paper first provides a comprehensive review of current research progress in this field, including current research focuses and open challenges. Then, an in-depth experimental study is performed, with the goal of understanding how to best overcome concept drift in online learning with class imbalance. Based on the analysis, a general guideline is proposed for the development of an effective algorithm.", "creator": "LaTeX with hyperref package"}}}