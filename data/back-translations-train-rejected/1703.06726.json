{"id": "1703.06726", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "On the effect of pooling on the geometry of representations", "abstract": "In machine learning and neuroscience, certain computational structures and algorithms are known to yield disentangled representations without us understanding why, the most striking examples being perhaps convolutional neural networks and the ventral stream of the visual cortex in humans and primates. As for the latter, it was conjectured that representations may be disentangled by being flattened progressively and at a local scale. An attempt at a formalization of the role of invariance in learning representations was made recently, being referred to as I-theory. In this framework and using the language of differential geometry, we show that pooling over a group of transformations of the input contracts the metric and reduces its curvature, and provide quantitative bounds, in the aim of moving towards a theoretical understanding on how to disentangle representations.", "histories": [["v1", "Mon, 20 Mar 2017 13:18:56 GMT  (17kb)", "http://arxiv.org/abs/1703.06726v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gary b\\'ecigneul"], "accepted": false, "id": "1703.06726"}, "pdf": {"name": "1703.06726.pdf", "metadata": {"source": "CRF", "title": "On the effect of pooling on the geometry of representations", "authors": ["Gary B\u00e9cigneul"], "emails": ["GARY.BECIGNEUL@INF.ETHZ.CH"], "sections": [{"heading": null, "text": "ar Xiv: 170 3.06 726v 1 [cs.L G] 20 Mar 2to produce disentangled representations without us understanding why, the most striking examples are perhaps convolutional neuronal networks and the ventral stream of the visual cortex in human and primates. As for the latter, it has been suggested that representations can be untangled by flattening them progressively and locally (DiCarlo and Cox, 2007). Within this framework, and using the language of differential geometry, we show that the bundling of a group of transformations of input draws the metric together and reduces its curvature and provides quantitative limits, with the aim of reaching a theoretical understanding of how to unravel representations."}, {"heading": "1. Introduction", "text": "On the other hand, we are probably able to find a comprehensive system in which the different representations are reflected. In machine learning and neuroscience, Carlo Carlo Carlo 2016 has two basic interpretations, and they are closely related; the first is geometric: look at two sheets of paper of different colors, place one of the two on the other side, and fragment them together in a paper ball; now it might look difficult to separate the two sheets from a third: they are confused, a color sheet represents a class of a classification problem; the second is analytical: look at a dataset parameterized by a series of coordinates {xi} i, like images parameterized by pixels, and a classification task that represents a class of images; on the one hand, we cannot find a subset {xi} i, the J of this coordinated system is that a variation of this class would not alter the class of an element, while still encompassing a set of reasonable images."}, {"heading": "2. Some background material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Groups and geometry", "text": "G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G & G &"}, {"heading": "3. Main results: formal framework and theorems", "text": "This defines an action (Lgf) (x) = f (g) x (x) x (x) x (x) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x) x (G) x (G) x (G) x (G) x (G) x (G) x) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G) x (G (G) x (G) x (G) x (G) x (G) x (G (G) x (G) x (G) x (G (G) x (G) x (G) x (G (G) x (G) x (G (G) x (G) x (G) x (G (G) x (G (G) x) x (G (G) x (G) x (G (G) x (G (G) x (G) x) x (G (G) x) x (G (G (G) x) x (G (G (G (G) x) x) x (G (G) x) x (G (G (G) x (G (G) x) x ("}, {"heading": "Corollary.", "text": "For all f-X, for all f-II, for all f-II, for all f-II, for all f-II, for all f-II, for all f-II, for all f-II, for all f-II, for all f-II, for all f-II, for all f-II, for all f-II, for all G-II, for all G-II, for all G-II, for all G-II, for all G-II, for all G-II, for all G-II, for all G-II, for all G-II, for all G-II, for all G-II, for all G-II, for all G-II, for all G-II, for all G-II, for all G-II, for G, for G, for G, for G, for G, for all G-II, for all G-II, for all G-II, for all G-II, for all G-II, for all G-II-II, for all G-II, for all G-II-II, for all G-II, for all G-II-II, for all G-II, for all G-II, for all G, for all G-II-II, for all G, for all G-II-II, for all, for all G, for G-II-II, for all, for G-II-II, for all, for all, for G-II-II, for all, for all, for G-II-II, for all, for G, for all, for G-II, for all, for G-II-II, for all, for all, for G-II, for all, for G, for all, for all, for all, II-II-II-II, for all II, for all, for all, for all, for all, for all, for all, II-II-II, for all, for G, for all, for all, for all, II, for all, for all, for all, II-II, for all, II-II, for all, for all, for all, II-II, for all, for all, for all, II, for all, for all, II, for all, for all, for all, for all, for all"}, {"heading": "4. Conclusion", "text": "The ability to untangle highly convoluted representations is a very important and challenging problem in machine learning. Particularly in the field of deep learning, there are successful algorithms that can untangle highly convoluted representations in some situations without us understanding why. Similarly, the ventral flow of the visual cortex in humans and primates seems to perform such untangling of representations, but even here the reasons for this process are difficult to understand. It is thought that the proliferation of representations in relation to some annoying deformations, as well as their spatial flattening, could be helpful or even an essential part of the untangling process. As our theorems show, there is a link between these two intuitions in the sense that reaching a higher degree of inventory degree in relation to some group transformations may represent representations in the direction of the tangled space corresponding to the Lie algebra generators of these transformations."}, {"heading": "Acknowledgments", "text": "We thank Simon Janin and Victor Godet for interesting conversations."}, {"heading": "Appendix A. Proofs of Theorem 1, Theorem 2 and Lemma", "text": "Theorem 1. For all f-L2 (R2), for all g-G, for all g-G, for all g-G, for all g-G, for all g-G, for all g-G, for all g-G, for all g-G and for all g-G."}, {"heading": "Proof:", "text": "We have\u00b5G (G0) 2 (G0G) 2 (G0G) 2 (G0G) 1 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 1 (G0G) 1 (G0G) 1 (G0G) 1 (G0G) 1 (G0G) 1 (G0G) 1 (G0G) 1 (1 G) 1 (1 G0G) 1 (1 G) 1 (1 G0G) 1 (1 G0G) 1 (1 G) 1 (1 G0G) 1 (1 G0G) 1 (1 G0G) 1 (1 G0G) 1 (1 G0G) 1 (1 G0G) 1 (1 G0G) 1 (2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G) 2 (G0G)"}, {"heading": "Proof:", "text": "If we transfer a diffeomorphism from G \u00b7 f to his image, and if we recognize its difference from Lemma, we have this for the whole vector field X to G \u00b7 f, E \u00b7 (X) (E) (G) (E) (E) (G) (G) (G) (G) (G) (X (F)) = X (F) = [E) (E) (E) (G) (E) (G) (G) (E) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G) (G), G (G) (G) (G) (G), G (G) (G) (G), G (G) (G) (G) (G), G (G) (G) (G) (G), G (G) (G) (G) (G), G (G) (G) (G) (G) (G), G (G (G) (G) (G), G (G (G) (G) (G) (G), G (G (G) (G) (G) (G (G), G (G) (G (G) (G), G (G (G) (G) (G (G), G (G (G) (G), G (G (=), G (G (G), G (G (G) (G (G), G (G (G), G (G (G), G (=), G (G (G), G (G (G (G), G (G) (G (G), G (G (G), G (G (G), G (G (G), G (G (G), G (=), G (G (G)) (G (G (G (G), G (G)), G (G (G (=), G (G (G), G () (G (G), G) (G ("}, {"heading": "Lemma.", "text": "For all f-X and f-g applies: ddt | t = 0 \u03a6 (Lexp (t\u0441) f) = \u03a6 (ddt | t = 0 (Lexp (t\u0441) f))."}, {"heading": "Proof:", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "Appendix B. Supplementary material", "text": "The next three sentences are taken from the publicly available French textbook Paulin (2014), in which they are each numbered as E.7, 1.60, 1.62. Proposition B.0. G is a lie group, and the stabilizer Gv is an embedded lie subgroup of G.Proposition B.1. G is a lie group, H is an embedded lie subgroup of G, and \u03c0: G \u2192 G / H is the canonical projection. There is one and only a smooth manifold structure on the topological quotient space G / H, which turns into a smooth subgroup of G / H. Furthermore, the action of G on G / H is a smooth lie subgroup of G / H, and if H is normal in G, then G / H is a smooth lie group."}], "references": [{"title": "The Ricci flow in Riemannian geometry: a complete proof of the differentiable 1/4-pinching sphere", "author": ["Ben Andrews", "Christopher Hopper"], "venue": null, "citeRegEx": "Andrews and Hopper.,? \\Q2010\\E", "shortCiteRegEx": "Andrews and Hopper.", "year": 2010}, {"title": "Representation learning in sensory cortex: a theory", "author": ["Fabio Anselmi", "Tomaso Poggio"], "venue": "Technical report, Center for Brains, Minds and Machines (CBMM),", "citeRegEx": "Anselmi and Poggio.,? \\Q2014\\E", "shortCiteRegEx": "Anselmi and Poggio.", "year": 2014}, {"title": "Magic materials: a theory of deep hierarchical architectures for learning sensory representations", "author": ["Fabio Anselmi", "Joel Z Leibo", "Lorenzo Rosasco", "Jim Mutch", "Andrea Tacchetti", "Tomaso Poggio"], "venue": "CBCL paper,", "citeRegEx": "Anselmi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anselmi et al\\.", "year": 2013}, {"title": "Unsupervised learning of invariant representations in hierarchical architectures", "author": ["Fabio Anselmi", "Joel Z Leibo", "Lorenzo Rosasco", "Jim Mutch", "Andrea Tacchetti", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1311.4158,", "citeRegEx": "Anselmi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anselmi et al\\.", "year": 2013}, {"title": "On invariance and selectivity in representation learning", "author": ["Fabio Anselmi", "Lorenzo Rosasco", "Tomaso Poggio"], "venue": "Information and Inference,", "citeRegEx": "Anselmi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Anselmi et al\\.", "year": 2016}, {"title": "Understanding deep features with computer-generated imagery", "author": ["Mathieu Aubry", "Bryan C Russell"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Aubry and Russell.,? \\Q2015\\E", "shortCiteRegEx": "Aubry and Russell.", "year": 2015}, {"title": "Deep learning of representations: Looking forward", "author": ["Yoshua Bengio"], "venue": "In International Conference on Statistical Language and Speech Processing,", "citeRegEx": "Bengio.,? \\Q2013\\E", "shortCiteRegEx": "Bengio.", "year": 2013}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Invariant scattering convolution networks", "author": ["Joan Bruna", "St\u00e9phane Mallat"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Bruna and Mallat.,? \\Q2013\\E", "shortCiteRegEx": "Bruna and Mallat.", "year": 2013}, {"title": "Learning stable group invariant representations with convolutional networks", "author": ["Joan Bruna", "Arthur Szlam", "Yann LeCun"], "venue": "arXiv preprint arXiv:1301.3537,", "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "Transformation properties of learned visual representations", "author": ["Taco S Cohen", "Max Welling"], "venue": "arXiv preprint arXiv:1412.7659,", "citeRegEx": "Cohen and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Cohen and Welling.", "year": 2014}, {"title": "Group equivariant convolutional networks", "author": ["Taco S Cohen", "Max Welling"], "venue": "arXiv preprint arXiv:1602.07576,", "citeRegEx": "Cohen and Welling.,? \\Q2016\\E", "shortCiteRegEx": "Cohen and Welling.", "year": 2016}, {"title": "Steerable cnns", "author": ["Taco S Cohen", "Max Welling"], "venue": "arXiv preprint arXiv:1612.08498,", "citeRegEx": "Cohen and Welling.,? \\Q2016\\E", "shortCiteRegEx": "Cohen and Welling.", "year": 2016}, {"title": "Untangling invariant object recognition", "author": ["James J DiCarlo", "David D Cox"], "venue": "Trends in cognitive sciences,", "citeRegEx": "DiCarlo and Cox.,? \\Q2007\\E", "shortCiteRegEx": "DiCarlo and Cox.", "year": 2007}, {"title": "How does the brain solve visual object recognition? Neuron", "author": ["James J DiCarlo", "Davide Zoccolan", "Nicole C Rust"], "venue": null, "citeRegEx": "DiCarlo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "DiCarlo et al\\.", "year": 2012}, {"title": "Exploiting cyclic symmetry in convolutional neural networks", "author": ["Sander Dieleman", "Jeffrey De Fauw", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.02660,", "citeRegEx": "Dieleman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2016}, {"title": "Deep symmetry networks", "author": ["Robert Gens", "Pedro M Domingos"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Gens and Domingos.,? \\Q2014\\E", "shortCiteRegEx": "Gens and Domingos.", "year": 2014}, {"title": "Measuring invariances in deep networks. In Advances in neural information processing", "author": ["Ian Goodfellow", "Honglak Lee", "Quoc V Le", "Andrew Saxe", "Andrew Y Ng"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2009}, {"title": "An introduction to Lie groups and Lie algebras, volume 113", "author": ["Alexander Kirillov"], "venue": null, "citeRegEx": "Kirillov.,? \\Q2008\\E", "shortCiteRegEx": "Kirillov.", "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Understanding image representations by measuring their equivariance and equivalence", "author": ["Karel Lenc", "Andrea Vedaldi"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "Lenc and Vedaldi.,? \\Q2015\\E", "shortCiteRegEx": "Lenc and Vedaldi.", "year": 2015}, {"title": "Group invariant scattering", "author": ["St\u00e9phane Mallat"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Mallat.,? \\Q2012\\E", "shortCiteRegEx": "Mallat.", "year": 2012}, {"title": "Understanding deep convolutional networks", "author": ["St\u00e9phane Mallat"], "venue": "Phil. Trans. R. Soc. A,", "citeRegEx": "Mallat.,? \\Q2015\\E", "shortCiteRegEx": "Mallat.", "year": 2015}, {"title": "Learning with group invariant features: A kernel perspective", "author": ["Youssef Mroueh", "Stephen Voinea", "Tomaso A Poggio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mroueh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mroueh et al\\.", "year": 2015}, {"title": "Differential geometry and statistics, volume 48", "author": ["Michael K Murray", "John W Rice"], "venue": null, "citeRegEx": "Murray and Rice.,? \\Q1993\\E", "shortCiteRegEx": "Murray and Rice.", "year": 1993}, {"title": "Deep roto-translation scattering for object classification", "author": ["Edouard Oyallon", "St\u00e9phane Mallat"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Oyallon and Mallat.,? \\Q2015\\E", "shortCiteRegEx": "Oyallon and Mallat.", "year": 2015}, {"title": "Groupes et g\u00e9om\u00e9trie", "author": ["Fr\u00e9d\u00e9ric Paulin"], "venue": "Notes de cours,", "citeRegEx": "Paulin.,? \\Q2014\\E", "shortCiteRegEx": "Paulin.", "year": 2014}, {"title": "The neurogeometry of pinwheels as a sub-riemannian contact structure", "author": ["Jean Petitot"], "venue": "Journal of Physiology-Paris,", "citeRegEx": "Petitot.,? \\Q2003\\E", "shortCiteRegEx": "Petitot.", "year": 2003}, {"title": "Neurog\u00e9om\u00e9trie de la vision", "author": ["Jean Petitot"], "venue": "Modeles mathe\u0301matiques et physiques des architectures fonctionelles. Paris: E\u0301d. E\u0301cole Polytech,", "citeRegEx": "Petitot.,? \\Q2008\\E", "shortCiteRegEx": "Petitot.", "year": 2008}, {"title": "The computational magic of the ventral stream: sketch of a theory (and why some deep architectures", "author": ["Tomaso Poggio", "JimMutch", "Joel Leibo", "Lorenzo Rosasco", "Andrea Tacchetti"], "venue": null, "citeRegEx": "Poggio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Poggio et al\\.", "year": 2012}, {"title": "Local group invariant representations via orbit embeddings", "author": ["Anant Raj", "Abhishek Kumar", "Youssef Mroueh", "P Thomas Fletcher"], "venue": null, "citeRegEx": "Raj et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Raj et al\\.", "year": 1988}, {"title": "Ecole polytechnique, cmap phd thesis rigid-motion scattering for image classification", "author": ["Laurent Sifre", "Stphane Mallat"], "venue": null, "citeRegEx": "Sifre and Mallat.,? \\Q2014\\E", "shortCiteRegEx": "Sifre and Mallat.", "year": 2014}, {"title": "comprehensive introduction to differential geometry", "author": ["Michael Spivak"], "venue": "vol. iv.[a]", "citeRegEx": "Spivak.,? \\Q1981\\E", "shortCiteRegEx": "Spivak.", "year": 1981}, {"title": "Improving invariance and equivariance properties of convolutional neural networks", "author": ["Christopher Tensmeyer", "Tony Martinez"], "venue": null, "citeRegEx": "Tensmeyer and Martinez.,? \\Q2016\\E", "shortCiteRegEx": "Tensmeyer and Martinez.", "year": 2016}, {"title": "A mathematical theory of deep convolutional neural networks for feature extraction", "author": ["Thomas Wiatowski", "Helmut B\u00f6lcskei"], "venue": "arXiv preprint arXiv:1512.06293,", "citeRegEx": "Wiatowski and B\u00f6lcskei.,? \\Q2015\\E", "shortCiteRegEx": "Wiatowski and B\u00f6lcskei.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "As for the latter, it was conjectured that representations may be disentangled by being flattened progressively and at a local scale (DiCarlo and Cox, 2007).", "startOffset": 133, "endOffset": 156}, {"referenceID": 13, "context": "Why is disentangling representations important? On the physiological side, the brains of humans and primates alike have been observed to solve object recognition tasks by progressively disentangling their representations via the visual stream, from V1 to the IT cortex (DiCarlo and Cox, 2007; DiCarlo et al., 2012).", "startOffset": 269, "endOffset": 314}, {"referenceID": 14, "context": "Why is disentangling representations important? On the physiological side, the brains of humans and primates alike have been observed to solve object recognition tasks by progressively disentangling their representations via the visual stream, from V1 to the IT cortex (DiCarlo and Cox, 2007; DiCarlo et al., 2012).", "startOffset": 269, "endOffset": 314}, {"referenceID": 19, "context": "can yield very good accuracy (Krizhevsky et al., 2012).", "startOffset": 29, "endOffset": 54}, {"referenceID": 6, "context": "Conversely, disentangling representations might be sufficient to pre-solve practically any task relevant to the observed data (Bengio, 2013).", "startOffset": 126, "endOffset": 140}, {"referenceID": 13, "context": "How can we design algorithms in order to move towards more disentangled representations? Although it was conjectured that the visual stream might disentangle representations by flattening them locally, thus inducing a decrease in the curvature globally (DiCarlo and Cox, 2007), the mechanisms underlying such a disentanglement, whether it be for the brain or deep learning architectures, remain very poorly understood (DiCarlo and Cox, 2007; Bengio, 2013).", "startOffset": 253, "endOffset": 276}, {"referenceID": 13, "context": "How can we design algorithms in order to move towards more disentangled representations? Although it was conjectured that the visual stream might disentangle representations by flattening them locally, thus inducing a decrease in the curvature globally (DiCarlo and Cox, 2007), the mechanisms underlying such a disentanglement, whether it be for the brain or deep learning architectures, remain very poorly understood (DiCarlo and Cox, 2007; Bengio, 2013).", "startOffset": 418, "endOffset": 455}, {"referenceID": 6, "context": "How can we design algorithms in order to move towards more disentangled representations? Although it was conjectured that the visual stream might disentangle representations by flattening them locally, thus inducing a decrease in the curvature globally (DiCarlo and Cox, 2007), the mechanisms underlying such a disentanglement, whether it be for the brain or deep learning architectures, remain very poorly understood (DiCarlo and Cox, 2007; Bengio, 2013).", "startOffset": 418, "endOffset": 455}, {"referenceID": 17, "context": "Indeed, on the one hand, deep convolutional networks have been noticed to naturally learn more invariant features with deeper layers (Goodfellow et al., 2009; Lenc and Vedaldi, 2015; Tensmeyer and Martinez, 2016).", "startOffset": 133, "endOffset": 212}, {"referenceID": 20, "context": "Indeed, on the one hand, deep convolutional networks have been noticed to naturally learn more invariant features with deeper layers (Goodfellow et al., 2009; Lenc and Vedaldi, 2015; Tensmeyer and Martinez, 2016).", "startOffset": 133, "endOffset": 212}, {"referenceID": 33, "context": "Indeed, on the one hand, deep convolutional networks have been noticed to naturally learn more invariant features with deeper layers (Goodfellow et al., 2009; Lenc and Vedaldi, 2015; Tensmeyer and Martinez, 2016).", "startOffset": 133, "endOffset": 212}, {"referenceID": 27, "context": "On the other hand, the V1 part of the brain similarly achieves invariance to translations and rotations via a \u201cpinwheels\u201d structure, which can be seen as a principal fiber bundle (Petitot, 2003; Poggio et al., 2012).", "startOffset": 179, "endOffset": 215}, {"referenceID": 29, "context": "On the other hand, the V1 part of the brain similarly achieves invariance to translations and rotations via a \u201cpinwheels\u201d structure, which can be seen as a principal fiber bundle (Petitot, 2003; Poggio et al., 2012).", "startOffset": 179, "endOffset": 215}, {"referenceID": 21, "context": "To the best of our knowledge, the main theoretical efforts in this direction include the theory of scattering operators (Mallat, 2012; Wiatowski and B\u00f6lcskei, 2015) as well as I-theory (Anselmi et al.", "startOffset": 120, "endOffset": 164}, {"referenceID": 34, "context": "To the best of our knowledge, the main theoretical efforts in this direction include the theory of scattering operators (Mallat, 2012; Wiatowski and B\u00f6lcskei, 2015) as well as I-theory (Anselmi et al.", "startOffset": 120, "endOffset": 164}, {"referenceID": 1, "context": "To the best of our knowledge, the main theoretical efforts in this direction include the theory of scattering operators (Mallat, 2012; Wiatowski and B\u00f6lcskei, 2015) as well as I-theory (Anselmi et al., 2013b,a; Anselmi and Poggio, 2014; Anselmi et al., 2016).", "startOffset": 185, "endOffset": 258}, {"referenceID": 4, "context": "To the best of our knowledge, the main theoretical efforts in this direction include the theory of scattering operators (Mallat, 2012; Wiatowski and B\u00f6lcskei, 2015) as well as I-theory (Anselmi et al., 2013b,a; Anselmi and Poggio, 2014; Anselmi et al., 2016).", "startOffset": 185, "endOffset": 258}, {"referenceID": 23, "context": "In particular, I-theory permits to use the whole apparatus of kernel theory to build invariant features (Mroueh et al., 2015; Raj et al., 2016).", "startOffset": 104, "endOffset": 143}, {"referenceID": 29, "context": "I-theory I-theory aims at understanding how to compute a representation of an image I that is both unique and invariant under some deformations of a groupG, and how to build such representations in a hierarchical way (Poggio et al., 2012; Anselmi et al., 2013b,a; Anselmi and Poggio, 2014; Anselmi et al., 2016).", "startOffset": 217, "endOffset": 311}, {"referenceID": 1, "context": "I-theory I-theory aims at understanding how to compute a representation of an image I that is both unique and invariant under some deformations of a groupG, and how to build such representations in a hierarchical way (Poggio et al., 2012; Anselmi et al., 2013b,a; Anselmi and Poggio, 2014; Anselmi et al., 2016).", "startOffset": 217, "endOffset": 311}, {"referenceID": 4, "context": "I-theory I-theory aims at understanding how to compute a representation of an image I that is both unique and invariant under some deformations of a groupG, and how to build such representations in a hierarchical way (Poggio et al., 2012; Anselmi et al., 2013b,a; Anselmi and Poggio, 2014; Anselmi et al., 2016).", "startOffset": 217, "endOffset": 311}, {"referenceID": 13, "context": "For more on Lie groups, Lie algebras, Lie brackets and group representations, see Kirillov (2008), and for a rapid and clear presentation of the notions of sectional curvature and Riemannian curvature, see Andrews and Hopper (2010).", "startOffset": 82, "endOffset": 98}, {"referenceID": 0, "context": "For more on Lie groups, Lie algebras, Lie brackets and group representations, see Kirillov (2008), and for a rapid and clear presentation of the notions of sectional curvature and Riemannian curvature, see Andrews and Hopper (2010). 2.", "startOffset": 206, "endOffset": 232}, {"referenceID": 9, "context": "For more on this, see (Bruna et al., 2013; Mallat, 2016).", "startOffset": 22, "endOffset": 56}], "year": 2017, "abstractText": "In machine learning and neuroscience, certain computational structures and algorithms are known to yield disentangled representations without us understanding why, the most striking examples being perhaps convolutional neural networks and the ventral stream of the visual cortex in humans and primates. As for the latter, it was conjectured that representations may be disentangled by being flattened progressively and at a local scale (DiCarlo and Cox, 2007). An attempt at a formalization of the role of invariance in learning representations was made recently, being referred to as I-theory (Anselmi et al., 2013b). In this framework and using the language of differential geometry, we show that pooling over a group of transformations of the input contracts the metric and reduces its curvature, and provide quantitative bounds, in the aim of moving towards a theoretical understanding on how to disentangle representations.", "creator": "LaTeX with hyperref package"}}}