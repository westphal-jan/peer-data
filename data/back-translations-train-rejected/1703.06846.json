{"id": "1703.06846", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions", "abstract": "Expressive efficiency is a concept that allows formally reasoning about the representational capacity of deep network architectures. A network architecture is expressively efficient with respect to an alternative architecture if the latter must grow super-linearly in order to represent functions realized by the former. A well-known example is the exponential expressive efficiency of depth, namely, that in many cases shallow networks must grow exponentially large in order to represent functions realized by deep networks. In this paper we study the expressive efficiency brought forth by the architectural feature of connectivity, motivated by the observation that nearly all state of the art networks these days employ elaborate connection schemes, running layers in parallel while splitting and merging them in various ways. A formal treatment of this question would shed light on the effectiveness of modern connectivity schemes, and in addition, could provide new tools for network design. We focus on dilated convolutional networks, a family of deep models gaining increased attention, underlying state of the art architectures like Google's WaveNet and ByteNet. By introducing and studying the concept of mixed tensor decompositions, we prove that interconnecting dilated convolutional networks can lead to expressive efficiency. In particular, we show that a single connection between intermediate layers can already lead to an almost quadratic gap, which in large-scale settings typically makes the difference between a model that is practical and one that is not.", "histories": [["v1", "Mon, 20 Mar 2017 17:05:38 GMT  (2236kb,D)", "http://arxiv.org/abs/1703.06846v1", null], ["v2", "Mon, 17 Apr 2017 18:22:33 GMT  (2238kb,D)", "http://arxiv.org/abs/1703.06846v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["nadav cohen", "ronen tamari", "amnon shashua"], "accepted": false, "id": "1703.06846"}, "pdf": {"name": "1703.06846.pdf", "metadata": {"source": "CRF", "title": "Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions", "authors": ["Nadav Cohen", "Ronen Tamari", "Amnon Shashua", "TAMARI SHASHUA"], "emails": ["COHENNADAV@CS.HUJI.AC.IL", "RONENT@CS.HUJI.AC.IL", "SHASHUA@CS.HUJI.AC.IL"], "sections": [{"heading": null, "text": "A well-known example is the exponential expressive efficiency of depth, namely that in many cases flat networks must become exponentially large in order to represent functions realized by deep networks. In this paper, we examine the expressive efficiency produced by the architectural characteristics of connectivity, motivated by the observation that almost all modern networks nowadays use sophisticated connectivity schemes that run parallel layers, splitting and merging them in different ways. Formal treatment of this question would shed light on the effectiveness of modern connectivity schemes and, in addition, provide new tools for network design. We focus on advanced Convolutionary Networks, a family of deep models that are gaining increasing attention and underlie state-of-the-art architectures such as Google's WaveNet and ByteNet. By introducing and studying the concept of mixed connectivity or decomposition, we prove that networking between large networks, which are usually the only one major difference in efficiency."}, {"heading": "1. Introduction", "text": "This phenomenon has attracted considerable attention in recent years."}, {"heading": "2. Preliminaries", "text": "The constructions and analyses provided in this paper are based on tensor analysis concepts."}, {"heading": "3. Dilated Convolutional Networks", "text": "Since the work of Krizhevsky et al. (2012), almost all state-of-the-art systems for image and video processing in both science and industry have relied heavily on revolutionary networks (see, for example, Szegedy et al. (2015); Taigman et al. (2014); Karpathy et al. (2014); Long et al. (2015). In their basic form, revolutionary networks consist of successive layers, each encompassing convolutions with multiple filters, followed by point-by-point activation (non-linearity), followed by spatial pooling (decimation). Recently, an alternative form of revolutionary networks has emerged - dictated revolutionary networks. These models are achieved by following spatial pooling (decimation) on spatial pooling (decimation)."}, {"heading": "3.1. Baseline Architecture", "text": "The advanced convolutional architecture considered as the basis in this work is the one underlying WaveNet model represented in Fig. II. (The input to the network is a sequence of vectors (x [t]), where t is a natural time index. (A size-2 Convolutionary Layer with Dilation-1, i.e. with continuous filters, maps this input into the hidden sequence (h (1))), where the input situation [r1] of h (1) [t] is obtained by applying the filter formed by a1. (a1), a1, \u03b3, II, Rr0 on times t-1, t of the input: h (1) [t]."}, {"heading": "3.2. Dilations and Mode Trees", "text": "In this subsection we generalize the underlying tree structure and show that the resulting decompositions capture the meshes with different decompositions in their different convolutional layers. We start by defining a general (binary) tree using tensor modes: Definition 1: A binary mode tree2 via [N] is a complete binary mode tree3: \u2022 Each node is characterized by a subset of [N] sheets."}, {"heading": "4. Mixed Tensor Decompositions", "text": "Leave T and T two binary mode trees above [N] (def. 1). Consider the tree decomposition of grid tensors induced by T (eq. 3). This decomposition iteratively assigns a group of tensors (s). The tree decomposition induced by T) works similarly, but for distinction we use {T, based on weight vectors (s). [T, based on weight vectors (s). [T, based on weight vectors (s)."}, {"heading": "5. Expressive Efficiency Analysis", "text": "It is not possible for us to realize any function that can be realized by N or N. (ii) There are functions that can be realized by N or N. (ii) There are functions that are realizable. (ii) There are functions that are not realizable by M or N. (ii) There are functions that can be realized by N or N, unless their size is linear. (ii) There are functions that can be realized by M or N. (or a summation thereof) There are functions that can be realized. (or a summation thereof) There are functions that cannot be realized. (or) There are functions that cannot be realized. (ii) There are functions that cannot be realized. (or a summation thereof) Unless their size (number of revolutionary channels) is allowed to grow. (ii) There are functions that can be realized by M. (or a mation thereof) It cannot be realized."}, {"heading": "In words, \u0398(I;T ) is a set of nodes in T whose disjoint union gives I, where each node is maximal,", "text": "It is not difficult to see that for each type of tree a different tree can be used than the tree whose tree population is generated by the tree decomposition of T when these are matricized. I.Theorem 7 Let T be a binary mode tree over [N] (def. 1), and consider the corresponding tree decompositions (eq. 3) with discretizers v (1).. v (M) spanning Rr. Assume that g (\u00b7) is the product operator (g, b) = a \u00b7 b), and guess the generated grid sors."}, {"heading": "6. Experiment", "text": "To assess the practical implications of expression efficiency caused by mixing dilational networks, a simple experiment was conducted for a hybrid form. - We trained a baseline of dilated constellations (1). - We trained a base of dilated constellations (2) to classify the individual phonemes in TIMIT. - We also trained the accompanying network N (2), which we created by swapping even and odd layers (so that layer l \u2212 2 if l is even, and 2l if l is strange). As discussed in sec. 4, the mode trees corresponding to these networks (illustrated in fig. 2) - T and T, share interior nodes, meaning any subset of these nodes can serve as a hybrid form."}, {"heading": "7. Summary", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "Acknowledgments", "text": "This work is supported by the Intel Fellowship ICRI-CI # 9-2012-6133, the ISF Center 1790 / 12 and the European Research Council (TheoryDL Project). Nadav Cohen is supported by a Google Doctoral Fellowship in Machine Learning."}, {"heading": "Appendix A. Derivation of the Baseline Decomposition", "text": "In this appendix we derive the base decomposition (eq. 2) - a parameterization of the network tensors (eq. 1), which dilated the baseline input-output mappings of the base line convolutional network (fig. 1). As discussed in Fig. 3.1, o [t] - the network output at the time t (eq. 1) is a function of x [t-N + 1].. x [t] - its input over the last N: = 2L times. We would like to show that for each d1.. dN [M], input (d1,., dN) of a tensor Ay generated by eq. 2, the coordinate of the network output o [t] is equal to the following input: x [t-N + 1] = v (d1),."}, {"heading": "Appendix B. Deferred Proofs", "text": "B. 1. Proof of Claim 5 We initiate the proof by introducing notations that enable a more compact representation (i.e., we leave {aH,.I,.II,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H,.H"}, {"heading": "Appendix C. Maximality of Matricization Ranks", "text": "In the proof for Theorem 7 (App. B.2), and in the derivation of Korollar8 (Sec. 5), we have used the fact that a tree or a mixed decomposition (eq. 3 or 4 respectively), with a product operator g (\u00b7), is set maximum matricization ranks (which depend both on decomposition and on me) for all configurations of weights ({a\u03bd, \u03b3, a\u03bd, II} ranks of grids tensors {Ay} y when matrified w.r.t. I achieve their maximum possible values (which depend on both decomposition and I) for all configurations of weights ({a\u03bd, \u03b3, I} ranks of grids, II} ranks of trees decomposed w.r.t. I achieve their maximum possible values (which depend on both decomposition and I)."}], "references": [{"title": "Introduction to matrix analysis, volume 960", "author": ["Richard Bellman"], "venue": null, "citeRegEx": "Bellman.,? \\Q1970\\E", "shortCiteRegEx": "Bellman.", "year": 1970}, {"title": "The zero set of a polynomial", "author": ["Richard Caron", "Tim Traynor"], "venue": "WSMR Report 05-02,", "citeRegEx": "Caron and Traynor.,? \\Q2005\\E", "shortCiteRegEx": "Caron and Traynor.", "year": 2005}, {"title": "Convolutional rectifier networks as generalized tensor decompositions", "author": ["Nadav Cohen", "Amnon Shashua"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Cohen and Shashua.,? \\Q2016\\E", "shortCiteRegEx": "Cohen and Shashua.", "year": 2016}, {"title": "Deep simnets", "author": ["Nadav Cohen", "Or Sharir", "Amnon Shashua"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Cohen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "On the expressive power of deep learning: A tensor analysis", "author": ["Nadav Cohen", "Or Sharir", "Amnon Shashua"], "venue": "Conference On Learning Theory (COLT),", "citeRegEx": "Cohen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "Shallow vs. deep sum-product networks", "author": ["Olivier Delalleau", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Delalleau and Bengio.,? \\Q2011\\E", "shortCiteRegEx": "Delalleau and Bengio.", "year": 2011}, {"title": "The power of depth for feedforward neural networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1512.03965,", "citeRegEx": "Eldan and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Eldan and Shamir.", "year": 2015}, {"title": "Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1", "author": ["John S Garofolo", "Lori F Lamel", "William M Fisher", "Jonathon G Fiscus", "David S Pallett"], "venue": "NASA STI/Recon technical report n,", "citeRegEx": "Garofolo et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Garofolo et al\\.", "year": 1993}, {"title": "Tensor Spaces and Numerical Tensor Calculus, volume 42 of Springer Series in Computational Mathematics", "author": ["Wolfgang Hackbusch"], "venue": null, "citeRegEx": "Hackbusch.,? \\Q2012\\E", "shortCiteRegEx": "Hackbusch.", "year": 2012}, {"title": "Heterogeneous acoustic measurements and multiple classifiers for speech recognition", "author": ["Andrew K Halberstadt"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Halberstadt.,? \\Q1998\\E", "shortCiteRegEx": "Halberstadt.", "year": 1998}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q Weinberger", "Laurens van der Maaten"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Q Weinberger"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods", "author": ["Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar"], "venue": "CoRR abs/1506.08473,", "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In Proceedings of the 22nd ACM international conference on Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Neural machine translation in linear time", "author": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Alex Graves", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1610.10099,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Largescale video classification with convolutional neural networks", "author": ["Andrej Karpathy", "George Toderici", "Sanketh Shetty", "Thomas Leung", "Rahul Sukthankar", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "LeCun and Bengio.,? \\Q1995\\E", "shortCiteRegEx": "LeCun and Bengio.", "year": 1995}, {"title": "Speaker-independent phone recognition using hidden markov models", "author": ["K-F Lee", "H-W Hon"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Lee and Hon.,? \\Q1989\\E", "shortCiteRegEx": "Lee and Hon.", "year": 1989}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Learning real and boolean functions: When is deep better than shallow", "author": ["Hrushikesh Mhaskar", "Qianli Liao", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1603.00988,", "citeRegEx": "Mhaskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mhaskar et al\\.", "year": 2016}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "author": ["Razvan Pascanu", "Guido Montufar", "Yoshua Bengio"], "venue": "arXiv preprint arXiv,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "I-theory on depth vs width: hierarchical function composition", "author": ["Tomaso Poggio", "Fabio Anselmi", "Lorenzo Rosasco"], "venue": "Technical report, Center for Brains, Minds and Machines (CBMM),", "citeRegEx": "Poggio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Poggio et al\\.", "year": 2015}, {"title": "Training input-output recurrent neural networks through spectral methods", "author": ["Hanie Sedghi", "Anima Anandkumar"], "venue": "arXiv preprint arXiv:1603.00954,", "citeRegEx": "Sedghi and Anandkumar.,? \\Q2016\\E", "shortCiteRegEx": "Sedghi and Anandkumar.", "year": 2016}, {"title": "Tensorial mixture models", "author": ["Or Sharir", "Ronen Tamari", "Nadav Cohen", "Amnon Shashua"], "venue": "arXiv preprint arXiv:1610.04167,", "citeRegEx": "Sharir et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sharir et al\\.", "year": 2016}, {"title": "Going Deeper with Convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": null, "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "DeepFace: Closing the Gap to HumanLevel Performance in Face Verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Taigman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Taigman et al\\.", "year": 2014}, {"title": "Representation benefits of deep feedforward networks", "author": ["Matus Telgarsky"], "venue": "arXiv preprint arXiv:1509.08101,", "citeRegEx": "Telgarsky.,? \\Q2015\\E", "shortCiteRegEx": "Telgarsky.", "year": 2015}, {"title": "Wavenet: A generative model for raw audio", "author": ["A\u00e4ron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "venue": "CoRR abs/1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Fisher Yu", "Vladlen Koltun"], "venue": "arXiv preprint arXiv:1511.07122,", "citeRegEx": "Yu and Koltun.,? \\Q2015\\E", "shortCiteRegEx": "Yu and Koltun.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Delalleau and Bengio (2011); Pascanu et al.", "startOffset": 0, "endOffset": 28}, {"referenceID": 2, "context": "Delalleau and Bengio (2011); Pascanu et al. (2013); Montufar et al.", "startOffset": 0, "endOffset": 51}, {"referenceID": 2, "context": "Delalleau and Bengio (2011); Pascanu et al. (2013); Montufar et al. (2014); Telgarsky (2015); Eldan and Shamir (2015); Cohen et al.", "startOffset": 0, "endOffset": 75}, {"referenceID": 2, "context": "Delalleau and Bengio (2011); Pascanu et al. (2013); Montufar et al. (2014); Telgarsky (2015); Eldan and Shamir (2015); Cohen et al.", "startOffset": 0, "endOffset": 93}, {"referenceID": 2, "context": "Delalleau and Bengio (2011); Pascanu et al. (2013); Montufar et al. (2014); Telgarsky (2015); Eldan and Shamir (2015); Cohen et al.", "startOffset": 0, "endOffset": 118}, {"referenceID": 2, "context": "(2014); Telgarsky (2015); Eldan and Shamir (2015); Cohen et al. (2016b); Cohen and Shashua (2016); Poggio et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al.", "startOffset": 9, "endOffset": 34}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al.", "startOffset": 9, "endOffset": 56}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.", "startOffset": 9, "endOffset": 79}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results).", "startOffset": 9, "endOffset": 345}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al.", "startOffset": 9, "endOffset": 623}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al.", "startOffset": 9, "endOffset": 641}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b,a)) deviate from the simple feed-forward approach, running layers in parallel with various connectivity (split/merge) schemes. Whether or not this relates to expressive efficiency remains to be an open question. A specific family of deep networks gaining increased attention in the deep learning community is that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al.", "startOffset": 9, "endOffset": 1089}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b,a)) deviate from the simple feed-forward approach, running layers in parallel with various connectivity (split/merge) schemes. Whether or not this relates to expressive efficiency remains to be an open question. A specific family of deep networks gaining increased attention in the deep learning community is that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016)) architectures, which provide state of the art performance in audio and text processing tasks.", "startOffset": 9, "endOffset": 1130}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b,a)) deviate from the simple feed-forward approach, running layers in parallel with various connectivity (split/merge) schemes. Whether or not this relates to expressive efficiency remains to be an open question. A specific family of deep networks gaining increased attention in the deep learning community is that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016)) architectures, which provide state of the art performance in audio and text processing tasks. Dilated convolutional networks are typically applied to sequence data, and consist of multiple succeeding convolutional layers, each comprising non-contiguous filters with a different dilation (distance between neighboring elements). The choice of dilations directly affects the space of functions that may be realized by a network, and while no choice is expressively efficient w.r.t. another, we show in this work that interconnecting networks with different dilations leads to expressive efficiency, and by this demonstrate that connectivity indeed bears the potential to enhance the expressiveness of deep networks. Our analysis follows several recent works utilizing tensor decompositions for theoretical studies of deep learning (see for example Janzamin et al. (2015); Sedghi and Anandkumar (2016)), and in particular, builds on the equivalence between hierarchical tensor decompositions and convolutional networks established in Cohen et al.", "startOffset": 9, "endOffset": 2000}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b,a)) deviate from the simple feed-forward approach, running layers in parallel with various connectivity (split/merge) schemes. Whether or not this relates to expressive efficiency remains to be an open question. A specific family of deep networks gaining increased attention in the deep learning community is that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016)) architectures, which provide state of the art performance in audio and text processing tasks. Dilated convolutional networks are typically applied to sequence data, and consist of multiple succeeding convolutional layers, each comprising non-contiguous filters with a different dilation (distance between neighboring elements). The choice of dilations directly affects the space of functions that may be realized by a network, and while no choice is expressively efficient w.r.t. another, we show in this work that interconnecting networks with different dilations leads to expressive efficiency, and by this demonstrate that connectivity indeed bears the potential to enhance the expressiveness of deep networks. Our analysis follows several recent works utilizing tensor decompositions for theoretical studies of deep learning (see for example Janzamin et al. (2015); Sedghi and Anandkumar (2016)), and in particular, builds on the equivalence between hierarchical tensor decompositions and convolutional networks established in Cohen et al.", "startOffset": 9, "endOffset": 2030}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b,a)) deviate from the simple feed-forward approach, running layers in parallel with various connectivity (split/merge) schemes. Whether or not this relates to expressive efficiency remains to be an open question. A specific family of deep networks gaining increased attention in the deep learning community is that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016)) architectures, which provide state of the art performance in audio and text processing tasks. Dilated convolutional networks are typically applied to sequence data, and consist of multiple succeeding convolutional layers, each comprising non-contiguous filters with a different dilation (distance between neighboring elements). The choice of dilations directly affects the space of functions that may be realized by a network, and while no choice is expressively efficient w.r.t. another, we show in this work that interconnecting networks with different dilations leads to expressive efficiency, and by this demonstrate that connectivity indeed bears the potential to enhance the expressiveness of deep networks. Our analysis follows several recent works utilizing tensor decompositions for theoretical studies of deep learning (see for example Janzamin et al. (2015); Sedghi and Anandkumar (2016)), and in particular, builds on the equivalence between hierarchical tensor decompositions and convolutional networks established in Cohen et al. (2016b) and Cohen and Shashua (2016).", "startOffset": 9, "endOffset": 2183}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b,a)) deviate from the simple feed-forward approach, running layers in parallel with various connectivity (split/merge) schemes. Whether or not this relates to expressive efficiency remains to be an open question. A specific family of deep networks gaining increased attention in the deep learning community is that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016)) architectures, which provide state of the art performance in audio and text processing tasks. Dilated convolutional networks are typically applied to sequence data, and consist of multiple succeeding convolutional layers, each comprising non-contiguous filters with a different dilation (distance between neighboring elements). The choice of dilations directly affects the space of functions that may be realized by a network, and while no choice is expressively efficient w.r.t. another, we show in this work that interconnecting networks with different dilations leads to expressive efficiency, and by this demonstrate that connectivity indeed bears the potential to enhance the expressiveness of deep networks. Our analysis follows several recent works utilizing tensor decompositions for theoretical studies of deep learning (see for example Janzamin et al. (2015); Sedghi and Anandkumar (2016)), and in particular, builds on the equivalence between hierarchical tensor decompositions and convolutional networks established in Cohen et al. (2016b) and Cohen and Shashua (2016). We show that with dilated convolutional networks, the choice of dilations throughout a network corresponds to determination of the mode (dimension) tree underlying the respective decomposition.", "startOffset": 9, "endOffset": 2212}, {"referenceID": 2, "context": "(2016b); Cohen and Shashua (2016); Poggio et al. (2015); Mhaskar et al. (2016)) have focused on the architectural feature of depth, showing instances where deep networks are expressively efficient w.r.t. shallow ones. This theoretical focus is motivated by the vast empirical evidence supporting the importance of depth (see LeCun et al. (2015) for a survey of such results). However, it largely overlooks an additional architectural feature that in recent years is proving to have great impact on the performance of deep networks \u2013 connectivity. Nearly all state of the art networks these days (e.g. Szegedy et al. (2015); He et al. (2015); Huang et al. (2016b,a)) deviate from the simple feed-forward approach, running layers in parallel with various connectivity (split/merge) schemes. Whether or not this relates to expressive efficiency remains to be an open question. A specific family of deep networks gaining increased attention in the deep learning community is that of dilated convolutional networks. These models form the basis of the recent WaveNet (van den Oord et al. (2016)) and ByteNet (Kalchbrenner et al. (2016)) architectures, which provide state of the art performance in audio and text processing tasks. Dilated convolutional networks are typically applied to sequence data, and consist of multiple succeeding convolutional layers, each comprising non-contiguous filters with a different dilation (distance between neighboring elements). The choice of dilations directly affects the space of functions that may be realized by a network, and while no choice is expressively efficient w.r.t. another, we show in this work that interconnecting networks with different dilations leads to expressive efficiency, and by this demonstrate that connectivity indeed bears the potential to enhance the expressiveness of deep networks. Our analysis follows several recent works utilizing tensor decompositions for theoretical studies of deep learning (see for example Janzamin et al. (2015); Sedghi and Anandkumar (2016)), and in particular, builds on the equivalence between hierarchical tensor decompositions and convolutional networks established in Cohen et al. (2016b) and Cohen and Shashua (2016). We show that with dilated convolutional networks, the choice of dilations throughout a network corresponds to determination of the mode (dimension) tree underlying the respective decomposition. We then define the notion of a mixed tensor decomposition, which blends together multiple mode trees, effectively creating a large ensemble of hybrid trees formed from all possible combinations. Mixed tensor decompositions correspond to mixed dilated convolutional networks, i.e. mixtures formed by connecting intermediate layers of different dilated convolutional networks. This allows studying the expressive properties of such mixtures using mathematical machinery from the field of tensor analysis. We fully analyze a particular case of dilated convolutional arithmetic circuits, showing that a single connection between intermediate layers already leads to an almost quadratic expressive efficiency, which in large-scale settings typically makes the difference between a model that is practical and one that is not. An experiment on TIMIT speech recognition dataset (Garofolo et al. (1993)) demonstrates the gain brought forth by mixing different networks, showing that interconnectivity can indeed boost the performance of dilated convolutional networks.", "startOffset": 9, "endOffset": 3302}, {"referenceID": 2, "context": "In Cohen and Shashua (2016) a generalization of the tensor product is defined, by replacing multiplication with a general operator g(\u00b7).", "startOffset": 3, "endOffset": 28}, {"referenceID": 8, "context": "The viewpoint we adopt is actually a concrete special case of a more abstract algebraic viewpoint of tensor analysis, as presented for example in Hackbusch (2012). We limit ourselves to this concrete viewpoint since it suffices for our needs and is easier to grasp.", "startOffset": 146, "endOffset": 163}, {"referenceID": 15, "context": "Dilated Convolutional Networks Convolutional networks (LeCun and Bengio (1995)) are the cornerstone of modern deep learning, and have played a critical role in its resurgence.", "startOffset": 55, "endOffset": 79}, {"referenceID": 15, "context": "Since the work of Krizhevsky et al. (2012), nearly all state of the art systems for image and video processing, in both academia and industry, are heavily based on convolutional networks (see for example Szegedy et al.", "startOffset": 18, "endOffset": 43}, {"referenceID": 15, "context": "Since the work of Krizhevsky et al. (2012), nearly all state of the art systems for image and video processing, in both academia and industry, are heavily based on convolutional networks (see for example Szegedy et al. (2015); Taigman et al.", "startOffset": 18, "endOffset": 226}, {"referenceID": 15, "context": "Since the work of Krizhevsky et al. (2012), nearly all state of the art systems for image and video processing, in both academia and industry, are heavily based on convolutional networks (see for example Szegedy et al. (2015); Taigman et al. (2014); He et al.", "startOffset": 18, "endOffset": 249}, {"referenceID": 10, "context": "(2014); He et al. (2015); Karpathy et al.", "startOffset": 8, "endOffset": 25}, {"referenceID": 10, "context": "(2014); He et al. (2015); Karpathy et al. (2014); Long et al.", "startOffset": 8, "endOffset": 49}, {"referenceID": 10, "context": "(2014); He et al. (2015); Karpathy et al. (2014); Long et al. (2015)).", "startOffset": 8, "endOffset": 69}, {"referenceID": 10, "context": "(2014); He et al. (2015); Karpathy et al. (2014); Long et al. (2015)). In their basic form, convolutional networks consist of successive layers, each comprising convolutions with multiple filters followed by point-wise activation (non-linearity), which in turn is followed by spatial pooling (decimation). Recently, an alternative form of convolutional networks has emerged \u2013 dilated convolutional networks. These models are obtained by removing spatial pooling and introducing non-contiguity to convolutional filters. Although they have been used for more conventional image processing tasks (e.g. Yu and Koltun (2015)), arguably the most significant merit of dilated convolutional networks is that they thrive in application domains previously outside the realm of convolutional networks.", "startOffset": 8, "endOffset": 620}, {"referenceID": 10, "context": "(2014); He et al. (2015); Karpathy et al. (2014); Long et al. (2015)). In their basic form, convolutional networks consist of successive layers, each comprising convolutions with multiple filters followed by point-wise activation (non-linearity), which in turn is followed by spatial pooling (decimation). Recently, an alternative form of convolutional networks has emerged \u2013 dilated convolutional networks. These models are obtained by removing spatial pooling and introducing non-contiguity to convolutional filters. Although they have been used for more conventional image processing tasks (e.g. Yu and Koltun (2015)), arguably the most significant merit of dilated convolutional networks is that they thrive in application domains previously outside the realm of convolutional networks. The WaveNet model recently developed by Google (van den Oord et al. (2016)) is based on dilated convolutions applied to raw audio, and provides state of the art text-to-speech results, as well as promising phoneme recognition (speech classification) performance.", "startOffset": 8, "endOffset": 866}, {"referenceID": 10, "context": "(2014); He et al. (2015); Karpathy et al. (2014); Long et al. (2015)). In their basic form, convolutional networks consist of successive layers, each comprising convolutions with multiple filters followed by point-wise activation (non-linearity), which in turn is followed by spatial pooling (decimation). Recently, an alternative form of convolutional networks has emerged \u2013 dilated convolutional networks. These models are obtained by removing spatial pooling and introducing non-contiguity to convolutional filters. Although they have been used for more conventional image processing tasks (e.g. Yu and Koltun (2015)), arguably the most significant merit of dilated convolutional networks is that they thrive in application domains previously outside the realm of convolutional networks. The WaveNet model recently developed by Google (van den Oord et al. (2016)) is based on dilated convolutions applied to raw audio, and provides state of the art text-to-speech results, as well as promising phoneme recognition (speech classification) performance. The following ByteNet model (Kalchbrenner et al. (2016)) applies dilated convolutional networks to raw textual characters, delivering state of the art character-level language modeling, as well as excellent character-level machine translation results at a fraction of the run time required by competing methods.", "startOffset": 8, "endOffset": 1110}, {"referenceID": 22, "context": "Different choices of g(\u00b7) lead to different convolutional operators, for example g(a, b) := max{a + b, 0} leads to standard convolution followed by rectified linear activation (ReLU, Nair and Hinton (2010)), whereas g(a, b) = a\u00b7b gives rise to what is known as a convolutional arithmetic circuit (Cohen et al.", "startOffset": 183, "endOffset": 206}, {"referenceID": 3, "context": "Different choices of g(\u00b7) lead to different convolutional operators, for example g(a, b) := max{a + b, 0} leads to standard convolution followed by rectified linear activation (ReLU, Nair and Hinton (2010)), whereas g(a, b) = a\u00b7b gives rise to what is known as a convolutional arithmetic circuit (Cohen et al. (2016b)).", "startOffset": 297, "endOffset": 318}, {"referenceID": 2, "context": "Therefore, strictly speaking, the baseline decomposition is a generalized tensor decomposition, as defined in Cohen and Shashua (2016). To conclude this subsection, we relate the material above to prior works in the literature, and highlight our contributions in the text to come.", "startOffset": 110, "endOffset": 135}, {"referenceID": 2, "context": "Therefore, strictly speaking, the baseline decomposition is a generalized tensor decomposition, as defined in Cohen and Shashua (2016). To conclude this subsection, we relate the material above to prior works in the literature, and highlight our contributions in the text to come. The first work to formalize the correspondence between convolutional networks and hierarchical tensor decompositions was Cohen et al. (2016b), in which only convolutional arithmetic circuits (convolutional networks with product pooling and linear activations) were considered.", "startOffset": 110, "endOffset": 423}, {"referenceID": 2, "context": "Therefore, strictly speaking, the baseline decomposition is a generalized tensor decomposition, as defined in Cohen and Shashua (2016). To conclude this subsection, we relate the material above to prior works in the literature, and highlight our contributions in the text to come. The first work to formalize the correspondence between convolutional networks and hierarchical tensor decompositions was Cohen et al. (2016b), in which only convolutional arithmetic circuits (convolutional networks with product pooling and linear activations) were considered. Cohen and Shashua (2016) later generalized the correspondence to account for other types of convolutional networks (e.", "startOffset": 110, "endOffset": 583}, {"referenceID": 2, "context": "Therefore, strictly speaking, the baseline decomposition is a generalized tensor decomposition, as defined in Cohen and Shashua (2016). To conclude this subsection, we relate the material above to prior works in the literature, and highlight our contributions in the text to come. The first work to formalize the correspondence between convolutional networks and hierarchical tensor decompositions was Cohen et al. (2016b), in which only convolutional arithmetic circuits (convolutional networks with product pooling and linear activations) were considered. Cohen and Shashua (2016) later generalized the correspondence to account for other types of convolutional networks (e.g. ones with ReLU activation and max or average pooling) as well. The baseline decomposition above (eq. 2) \u2013 a hierarchical tensor decomposition characterizing the baseline dilated convolutional network (fig. 1), is essentially a direct outcome of the formulation presented in Cohen and Shashua (2016). Our contributions begin in the next subsection, where we establish a correspondence between hierarchical decompositions over general mode trees, and dilated convolutional networks with different dilations.", "startOffset": 110, "endOffset": 978}, {"referenceID": 3, "context": "We focus on this special case since it allows the use of a plurality of algebraic tools for theoretical analysis, while at the same time corresponding to models showing promising results in practice (see for example Cohen et al. (2016a); Sharir et al.", "startOffset": 216, "endOffset": 237}, {"referenceID": 3, "context": "We focus on this special case since it allows the use of a plurality of algebraic tools for theoretical analysis, while at the same time corresponding to models showing promising results in practice (see for example Cohen et al. (2016a); Sharir et al. (2016)).", "startOffset": 216, "endOffset": 259}, {"referenceID": 31, "context": "1), with architectural parameters similar to those used in WaveNet (van den Oord et al. (2016)), to classify individual phonemes in the TIMIT acoustic speech corpus (Garofolo et al.", "startOffset": 76, "endOffset": 95}, {"referenceID": 7, "context": "(2016)), to classify individual phonemes in the TIMIT acoustic speech corpus (Garofolo et al. (1993)).", "startOffset": 78, "endOffset": 101}, {"referenceID": 9, "context": "We split the data into train and validation sets in accordance with Halberstadt (1998), and as advised by Lee and Hon (1989), mapped the 61 possible phoneme labels into 39 and an additional \u201cgarbage\u201d label.", "startOffset": 68, "endOffset": 87}, {"referenceID": 9, "context": "We split the data into train and validation sets in accordance with Halberstadt (1998), and as advised by Lee and Hon (1989), mapped the 61 possible phoneme labels into 39 and an additional \u201cgarbage\u201d label.", "startOffset": 68, "endOffset": 125}, {"referenceID": 14, "context": "was Caffe toolbox (Jia et al. (2014)), and we used Adam optimizer (Kingma and Ba (2014)) for training (with default hyper-parameters \u2013 \u03b21 = 0.", "startOffset": 19, "endOffset": 37}, {"referenceID": 14, "context": "was Caffe toolbox (Jia et al. (2014)), and we used Adam optimizer (Kingma and Ba (2014)) for training (with default hyper-parameters \u2013 \u03b21 = 0.", "startOffset": 19, "endOffset": 88}], "year": 2017, "abstractText": "Expressive efficiency is a concept that allows formally reasoning about the representational capacity of deep network architectures. A network architecture is expressively efficient with respect to an alternative architecture if the latter must grow super-linearly in order to represent functions realized by the former. A well-known example is the exponential expressive efficiency of depth, namely, that in many cases shallow networks must grow exponentially large in order to represent functions realized by deep networks. In this paper we study the expressive efficiency brought forth by the architectural feature of connectivity, motivated by the observation that nearly all state of the art networks these days employ elaborate connection schemes, running layers in parallel while splitting and merging them in various ways. A formal treatment of this question would shed light on the effectiveness of modern connectivity schemes, and in addition, could provide new tools for network design. We focus on dilated convolutional networks, a family of deep models gaining increased attention, underlying state of the art architectures like Google\u2019s WaveNet and ByteNet. By introducing and studying the concept of mixed tensor decompositions, we prove that interconnecting dilated convolutional networks can lead to expressive efficiency. In particular, we show that a single connection between intermediate layers can already lead to an almost quadratic gap, which in large-scale settings typically makes the difference between a model that is practical and one that is not.", "creator": "LaTeX with hyperref package"}}}