{"id": "1703.06902", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "A Comparison of deep learning methods for environmental sound", "abstract": "Environmental sound detection is a challenging application of machine learning because of the noisy nature of the signal, and the small amount of (labeled) data that is typically available. This work thus presents a comparison of several state-of-the-art Deep Learning models on the IEEE challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 challenge task and data, classifying sounds into one of fifteen common indoor and outdoor acoustic scenes, such as bus, cafe, car, city center, forest path, library, train, etc. In total, 13 hours of stereo audio recordings are available, making this one of the largest datasets available. We perform experiments on six sets of features, including standard Mel-frequency cepstral coefficients (MFCC), Binaural MFCC, log Mel-spectrum and two different large- scale temporal pooling features extracted using OpenSMILE. On these features, we apply five models: Gaussian Mixture Model (GMM), Deep Neural Network (DNN), Recurrent Neural Network (RNN), Convolutional Deep Neural Net- work (CNN) and i-vector. Using the late-fusion approach, we improve the performance of the baseline 72.5% by 15.6% in 4-fold Cross Validation (CV) avg. accuracy and 11% in test accuracy, which matches the best result of the DCASE 2016 challenge. With large feature sets, deep neural network models out- perform traditional methods and achieve the best performance among all the studied methods. Consistent with other work, the best performing single model is the non-temporal DNN model, which we take as evidence that sounds in the DCASE challenge do not exhibit strong temporal dynamics.", "histories": [["v1", "Mon, 20 Mar 2017 18:11:47 GMT  (250kb,D)", "http://arxiv.org/abs/1703.06902v1", "5 pages including reference"]], "COMMENTS": "5 pages including reference", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["juncheng li", "wei dai", "florian metze", "shuhui qu", "samarjit das"], "accepted": false, "id": "1703.06902"}, "pdf": {"name": "1703.06902.pdf", "metadata": {"source": "CRF", "title": "A COMPARISON OF DEEP LEARNING METHODS FOR ENVIRONMENTAL SOUND DETECTION", "authors": ["Juncheng Li", "Wei Dai", "Florian Metze", "Shuhui Qu", "Samarjit Das"], "emails": ["junchenl@cs.cmu.edu,", "wdai@cs.cmu.edu,", "fmetze@cs.cmu.edu,", "shuhuiq@stanford.edu,", "samarjit.das@us.bosch.com"], "sections": [{"heading": null, "text": "Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg, Hamburg"}, {"heading": "2.1. Dataset", "text": "We use the dataset of the IEEE Challenge on Detection and Classification of Acoustic Scenes and Events [7], and we also use Xiv: 170 3.06 902v 1 [cs.S D] 20 Mar 201 7use the evaluation setup of the competition. The training dataset contains 15 different indoor and outdoor locations (labels), a total of 9.75 hours of recording (1170 files) and 8.7 GB in WAV format (dual channel, sample rate: 44100 Hz, precision: 24 bit, duration: 30 sec each). We perform a quadruple CV for model selection and parameter adjustment. The evaluation dataset (390 files) contains the same audio classes as the training dataset, with a total of 3.25 hours of recording and 2.5 GB in the same WAV format."}, {"heading": "2.2. Features", "text": "We create six sets of features using audio signal processing methods: 1. Monaural and Binaural MFCC: Same as the winning solution of the DCASE Challenge 2016 [3]. We take 23 melting frequency coefficients (excluding the 0th) over the window length of 20 ms. We expand the feature with first and second order differences using a 60 ms window, resulting in a 61-dimensional vector. We also calculate the MFCC right, left, and channel difference (BiMFCC). 2. Smile983 & Smile6k: We use OpenSmile [8] to generate MFCC, Fourier transformations, zero crossover rate, energy, and pitch, etc. We also calculate first and second order features that result in 6573 features. We select 983 features recommended by domain experts to create the 983 dim function. Note that this is a much larger feature than the M3. We use the M3."}, {"heading": "2.3. Models and Hyperparameter Tuning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.3.1. Guassian Mixture Models (GMMs)", "text": "We use GMMs provided by the DCASE Challenge Committee [7] as the base system for acoustic scene detection. Each audio clip is presented as a bag of acoustic features extracted from audio segments, and for each class label, a GMM is trained on this bag of acoustic features, using only audio clips from this class."}, {"heading": "2.3.2. i-vector Pipeline", "text": "The Universal Background Model (UBM) is a GMM with 256 components trained on the development dataset using the BiMFCC function. The mean supervector M of the GMM can be broken down as follows: M = m + T \u00b7 y, where m is a vector independent of the audio scenario and T \u00b7 y is an offset. The low-dimensional (400-dim) subspace vector y is a scene-dependent vector and it is a latent variable relative to the normal state. The i vector w is a maximum a posteriori probability estimate (MAP) of y. We use the Kaldi toolkit [11] to calculate the T-matrix and perform a linear discript analysis (LDA)."}, {"heading": "2.3.3. Deep Neural Networks (DNNs)", "text": "Multi-layer perception has recently been successfully applied to speech recognition and audio analysis and performs better than GMM [12]. Here we have tried various hyperparameters, including depth (2-10 layers), number of hidden units (256-1024), failure rate (0-0.4), regulator (L1, L2) and various optimization algorithms (stochastic gradient descent, Adam [13], RMSProp [14], Adagrad [15]), batch normalization [16], etc. All deep models we have tried in the next two sections are tuned by cross-validation (CV) to achieve their best performance."}, {"heading": "2.3.4. Recurrent Neural Networks (RNNs)", "text": "Bidirectional architectures generally work better than unidirectional ones. We have tried both LSTM [17] and GRU [18] bidirectional layers. Our network has only 2 layers (one layer per direction) due to the convergence time and limited improvements compared to deeper RNN models."}, {"heading": "2.3.5. Convolutional Neural Networks (CNNs)", "text": "Recently, CNNs have been applied to speech recognition using spectrogram functions [20] and achieve state-of-the-art speech recognition performance. We use architectures similar to the VGG network [21] to keep the number of model parameters low. Input is the popular fluted linear units (relu) for modeling nonlinearity. In addition, we found that dense layers in the ground do not help, but only slow the calculation, so we do not include them in most experiments. Failure layers significantly improve performance, which is consistent with CNN's behavior on natural images. Overall, CNNs take significantly longer to train than RNNNs and DNNs based on shaft layers. Table 1 shows an example of the architectures of all DL models described above."}, {"heading": "2.4. Pipeline & System Configuration", "text": "For each audio clip (train and test), our processing pipeline consists of: 1) applying the various transformations (Section 2.2) to each audio clip to extract the feature representations; 2) treating each feature as a training example for non-time models such as GMMs; for temporal models such as RNs, we treat a sequence of features as a training example; 3) At test time, we apply the same pipeline as training and break up the audio clip as multiple instances, and the probability of a class label for a test audio clip is the sum of the predicted class probability for each segment. The class with the highest predicted probability is the predicted label for the test audio clip. We train our deep learning models using the Keras library [4], which is based on Theano [22] and TensorFlow and uses 4 Titan X GPUs on a 128 GB memory, Intel Core i7 node."}, {"heading": "2.5. Late Fusion", "text": "The bottom line is that we sort the models by performance that exceeds a predefined accuracy threshold."}, {"heading": "4.1. GMM & I-Vectors", "text": "In other words, we are dealing with a number of countries in which people are able to determine for themselves what they want and what they do not want. (...) We are dealing with a system of this kind. (...) We are dealing with a system of this kind. (...) We are dealing with a system in which people are able to determine for themselves what they have to do. (...) We are dealing with a system in which people are able to determine for themselves. (...) We are dealing with a system in which people are able to determine for themselves what they have to do. (...) We have not done it. \"(...) We have done it.\" (... \"......\" \")\" We have done it. \""}], "references": [{"title": "Natural sound archives: past, present and future", "author": ["R. Ranft"], "venue": "Anais da Academia Brasileira de Ci\u0142ncias, vol. 76, pp. 2, 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Recurrence quantification analysis features for auditory scene classification", "author": ["G. Roma", "W. Nogueira", "P. Herrera", "R. de Boronat"], "venue": "DCASE Challenge, Tech. Rep, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "CP-JKU submissions for DCASE-2016: a hybrid approach using binaural i-vectors and deep convolutional neural networks", "author": ["H. Eghbal-Zadeh", "B. Lehner", "M. Dorfer", "G. Widmer"], "venue": "Tech. Rep., DCASE2016 Challenge, September 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Polyphonic sound event detection using multi label deep neural networks.\u201d 2015 international joint conference on neural networks", "author": ["E. Cakir"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Sound event detection in real life recordings using coupled matrix factorization of spectral representations and class activity annotations.", "author": ["A. Mesaros"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Acoustic scene recognition with deep neural networks (DCASE challenge 2016)", "author": ["W. Dai", "J. Li", "P. Pham", "S. Das", "S. Qu"], "venue": "Tech. Rep., DCASE2016 Challenge, September 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "DCASE2016 baseline system", "author": ["T. Heittola", "A. Mesaros", "T. Virtanen"], "venue": "Tech. Rep., DCASE2016 Challenge, September 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Opensmile: The munich versatile and fast open-source audio feature extractor", "author": ["F. Eyben", "M. W\u00f6llmer", "B. Schuller"], "venue": "Proceedings of the 18th ACM International Conference on Multimedia, New York, NY, USA, 2010, MM \u201910, pp. 1459\u20131462, ACM.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "librosa: 0.4.1", "author": ["B. McFee", "M. McVicar", "C. Raffel", "D. Liang", "O. Nieto", "E. Battenberg", "J. Moore", "D. Ellis", "R. YAMAMOTO", "R. Bittner", "D. Repetto", "P. Viktorin", "J.F. Santos", "A. Holovaty"], "venue": "Oct. 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Eigenvoice modeling with sparse training data", "author": ["P. Kenny", "G. Boulianne", "P. Dumouchel"], "venue": "IEEE transactions on speech and audio processing, vol. 13, no. 3, pp. 345\u2013 354, 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "N. Goel", "M. Hannemann", "Y. Qian", "P. Schwarz", "G. Stemmer"], "venue": "IEEE 2011 workshop, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G.E. Hinton"], "venue": "CoRR, vol. abs/1303.5778, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, vol. abs/1412.6980, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Equilibrated adaptive learning rates for non-convex optimization", "author": ["Y.N. Dauphin", "H. d. Vries", "Y."], "venue": "Cambridge, MA, USA, 2015, NIPS\u201915, pp. 1504\u20131512, MIT Press.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 12, no. 2011, pp. 2121\u20132159, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "CoRR, vol. abs/1502.03167, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, vol. abs/1412.3555, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Opinion mining with deep recurrent neural networks", "author": ["O. Irsoy", "C. Cardie"], "venue": "2014, EMNLP, Citeseer.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep speech: Scaling up endto-end speech recognition", "author": ["A.Y. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates", "A.Y. Ng"], "venue": "CoRR, vol. abs/1412.5567, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, vol. abs/1409.1556, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints, vol. abs/1605.02688, May 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Ensemble selection from libraries of models.\u201dProceedings of the twenty-first international", "author": ["R. Caruana"], "venue": "con-ference on Machine learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Smoothing and Differentiation of Data by Simplified Least Squares Procedures", "author": ["A. Savitzky", "M.J.E. Golay"], "venue": "Anal. Chem., vol. 36, no. 8, pp. 1627\u20131639, July 1964.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1964}, {"title": "End-to-end continuous speech recognition using attention-based recurrent NN: first results", "author": ["J. Chorowski", "D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, vol. abs/1412.1602, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for acoustic modeling of raw time signal in lvcsr", "author": ["P. Golik", "Z. T\u00fcske", "R. Schl\u00fcter", "H. Ney"], "venue": "Interspeech, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The prospect of human-like sound understanding could open up a wide range of applications, including intelligent machine state monitoring using acoustic information, acoustic surveillance, cataloging and information retrieval applications such as search in audio archives [1] as well as audio-assisted multimedia content search.", "startOffset": 272, "endOffset": 275}, {"referenceID": 1, "context": "For example, the winning solutions by [2][3] for DCASE challenge 2013 and 2016, extracts MFCC and i-vectors, and they both used other deeper models for temporal relation analysis.", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "For example, the winning solutions by [2][3] for DCASE challenge 2013 and 2016, extracts MFCC and i-vectors, and they both used other deeper models for temporal relation analysis.", "startOffset": 41, "endOffset": 44}, {"referenceID": 3, "context": "There are several studies using DL in sound event detection [4][5].", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": "There are several studies using DL in sound event detection [4][5].", "startOffset": 63, "endOffset": 66}, {"referenceID": 5, "context": "In this paper, we present a comparison of the most successful and complementary approaches to sound event detection on DCASE, which we implemented on top of our evaluation system [6] in a systematic and consistent way.", "startOffset": 179, "endOffset": 182}, {"referenceID": 6, "context": "We use the dataset from the IEEE challenge on Detection and Classification of Acoustic Scenes and Events [7], and we also ar X iv :1 70 3.", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "Monaural and Binaural MFCC: Same as the winning solution in the DCASE challenge 2016 [3].", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "Smile983 & Smile6k: We use OpenSmile [8] to generate MFCC, Fourier transforms, zero crossing rate, energy, and pitch, among others.", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "LogMel: We use LibROSA [9] to compute the log MelSpectrum, and we use the same parameters as the MFCC setup.", "startOffset": 23, "endOffset": 26}, {"referenceID": 6, "context": "We use the GMMs provided by the DCASE challenge committee [7] as the baseline system for acoustic scene recognition.", "startOffset": 58, "endOffset": 61}, {"referenceID": 9, "context": "We replicate the i-vector [10] pipeline from [3].", "startOffset": 26, "endOffset": 30}, {"referenceID": 2, "context": "We replicate the i-vector [10] pipeline from [3].", "startOffset": 45, "endOffset": 48}, {"referenceID": 10, "context": "We use the Kaldi Toolkit [11] to compute T matrix and perform Linear Discrimant Analysis (LDA).", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "Multi-layer perception has recently been successfully applied to speech recognition and audio analysis and shows superior performance compared to GMMs [12].", "startOffset": 151, "endOffset": 155}, {"referenceID": 12, "context": "4), regularizer (L1, L2), and various optimization algorithms(stochastic gradient descent, Adam [13], RMSProp [14], Adagrad [15]), batch normalization [16], etc.", "startOffset": 96, "endOffset": 100}, {"referenceID": 13, "context": "4), regularizer (L1, L2), and various optimization algorithms(stochastic gradient descent, Adam [13], RMSProp [14], Adagrad [15]), batch normalization [16], etc.", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "4), regularizer (L1, L2), and various optimization algorithms(stochastic gradient descent, Adam [13], RMSProp [14], Adagrad [15]), batch normalization [16], etc.", "startOffset": 124, "endOffset": 128}, {"referenceID": 15, "context": "4), regularizer (L1, L2), and various optimization algorithms(stochastic gradient descent, Adam [13], RMSProp [14], Adagrad [15]), batch normalization [16], etc.", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "We tried both LSTM [17] and GRU [18] bidirectional layers.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "We tried both LSTM [17] and GRU [18] bidirectional layers.", "startOffset": 32, "endOffset": 36}, {"referenceID": 18, "context": "Our network only has 2 layers (one direction a layer) due to convergence time and limited improvement from deeper RNN models [19].", "startOffset": 125, "endOffset": 129}, {"referenceID": 19, "context": "Lately, CNNs has been applied to speech recognition using spectrogram features [20] and achieve state-of-the-art speech recognition performance.", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "We employ architectures similar to the VGG net [21] to keep the number of model parameters small.", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "We train our deep learning models with the Keras library [4] built on Theano [22] and TensorFlow, using 4 Titan X GPUs on a 128GB memory, Intel Core i7 node.", "startOffset": 57, "endOffset": 60}, {"referenceID": 21, "context": "We train our deep learning models with the Keras library [4] built on Theano [22] and TensorFlow, using 4 Titan X GPUs on a 128GB memory, Intel Core i7 node.", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "We test with random forest, extremely randomized trees, Adaboost, gradient tree boosting, weighted average probabilities and other model selection methods in the late fusion [23].", "startOffset": 174, "endOffset": 178}, {"referenceID": 2, "context": "2% test accuracy, which is competitve with the winning solution in the DCASE challenge [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 5, "context": "confusion matrix from [6]", "startOffset": 22, "endOffset": 25}, {"referenceID": 23, "context": "If we apply a Savitzky-Golay smoothing function [24] which acts like a low-pass filter on each neuron\u2019s vector (61-dim).", "startOffset": 48, "endOffset": 52}, {"referenceID": 24, "context": "Meanwhile, there could be a potential gain from incorporating attention-based RNN [25] here to tackle those event-rich audio scenes based on audio events.", "startOffset": 82, "endOffset": 86}, {"referenceID": 25, "context": "Our finding is consistent with prior work on speech data [26].", "startOffset": 57, "endOffset": 61}], "year": 2017, "abstractText": "Environmental sound detection is a challenging application of machine learning because of the noisy nature of the signal, and the small amount of (labeled) data that is typically available. This work thus presents a comparison of several state-of-the-art Deep Learning models on the IEEE challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2016 challenge task and data, classifying sounds into one of fifteen common indoor and outdoor acoustic scenes, such as bus, cafe, car, city center, forest path, library, train, etc. In total, 13 hours of stereo audio recordings are available, making this one of the largest datasets available. We perform experiments on six sets of features, including standard Mel-frequency cepstral coefficients (MFCC), Binaural MFCC, log Mel-spectrum and two different largescale temporal pooling features extracted using OpenSMILE. On these features, we apply five models: Gaussian Mixture Model (GMM), Deep Neural Network (DNN), Recurrent Neural Network (RNN), Convolutional Deep Neural Network (CNN) and i-vector. Using the late-fusion approach, we improve the performance of the baseline 72.5% by 15.6% in 4-fold Cross Validation (CV) avg. accuracy and 11% in test accuracy, which matches the best result of the DCASE 2016 challenge. With large feature sets, deep neural network models outperform traditional methods and achieve the best performance among all the studied methods. Consistent with other work, the best performing single model is the non-temporal DNN model, which we take as evidence that sounds in the DCASE challenge do not exhibit strong temporal dynamics.", "creator": "LaTeX with hyperref package"}}}