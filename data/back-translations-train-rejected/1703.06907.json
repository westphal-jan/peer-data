{"id": "1703.06907", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World", "abstract": "Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to $1.5$cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.", "histories": [["v1", "Mon, 20 Mar 2017 18:17:25 GMT  (3579kb,D)", "http://arxiv.org/abs/1703.06907v1", "8 pages, 7 figures. Submitted to 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2017)"]], "COMMENTS": "8 pages, 7 figures. Submitted to 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2017)", "reviews": [], "SUBJECTS": "cs.RO cs.LG", "authors": ["josh tobin", "rachel fong", "alex ray", "jonas schneider", "wojciech zaremba", "pieter abbeel"], "accepted": false, "id": "1703.06907"}, "pdf": {"name": "1703.06907.pdf", "metadata": {"source": "CRF", "title": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World", "authors": ["Josh Tobin", "Rachel Fong", "Alex Ray", "Jonas Schneider", "Wojciech Zaremba", "Pieter Abbeel"], "emails": ["josh@openai.com", "woj}@openai.com", "pieter@openai.com"], "sections": [{"heading": null, "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "II. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Object detection and pose estimation for robotics", "text": "Object recognition and pose estimation for robotics is a well-studied problem in the literature (see e.g. [4], [5], [6], [10], [44], [50], [54]). Newer approaches typically involve offline construction or learning a 3D model of objects in the scene (e.g. a complete 3D mesh model [44] or a 3D representation of metric features [5]. At test time, features from the test data (e.g. the Scale-Invariant Feature Transform [SIFT] features [12] or color-coincidence histograms [10]) are compared with the 3D models (or features from the 3D models).For example, a non-linear black box optimization algorithm can be used to minimize the re-projection error of the SIFT points from the object model and the 2D points from the test image [4]."}, {"heading": "B. Domain adaptation", "text": "The computer vision community addressed the problem of adapting vision-based models trained in a source domain to a previously invisible target domain (see e.g. [9], [14], [15], [19], [23], [25], [51]). A variety of approaches were proposed, including retraining the model in the target domain (e.g. [52]), adapting the weights of the model based on the statistics of the source and target domains (e.g. [22]), learning invariant features between domains (e.g. [47]), and learning to assign them from the target domain to the source domain (e.g. [43])."}, {"heading": "C. Bridging the reality gap", "text": "In fact, most of them are able to play by the rules that they have set themselves in order to play by the rules that they have played by."}, {"heading": "III. METHOD", "text": "Considering some interesting objects {si} i, our goal is to train an object detector d (I0) that maps a single monocular camera image I0 to the cartesian coordinates {xi, yi, zi)} i of each object. In addition to the interesting objects, our scenes sometimes contain distractor objects that must be ignored by the network. Our approach is to train a deep neural network in simulation using domain randomization. The rest of this section describes the specific domain randomization and the methods for forming neural networks that we use."}, {"heading": "A. Domain randomization", "text": "The purpose of domain randomization is to provide enough simulated variability in training time so that at test time the model is able to generalize to real-world data. We randomize the following aspects of the domain for each sample used during training: \u2022 Number and shape of distractor objects on the table \u2022 Position and texture of all objects on the table \u2022 Textures of the table, floor, skybox and robot \u2022 Position, orientation and field of vision of the camera \u2022 Number of lights in the scene \u2022 Position, orientation and spectral properties of the lights \u2022 Type and height of random noise added to images Since we use a single monocular camera image to estimate object positions, we fix the height of the table in the simulation, effectively creating a 2D estimation task. Random textures are selected from the following viewpoints: (a) A random RGB value (A random Rb) between two RGB serves."}, {"heading": "B. Model architecture and training", "text": "We are parameterizing our object detector with a deep convolutionary neural network. In particular, we are using a modified version of the VGG-16 architecture [39] shown in Figure 2. We chose this architecture because it handles a variety of computer vision tasks well and has a wide availability of pre-trained weights. We are using the standard VGG convolution layers, but we are using smaller, fully connected layers of sizes 256 and 64 and do not use a dropout. In most of our experiments, we are using weights obtained through pre-training on ImageNet to initialize the revolutionary layers that we thought were indispensable for transmission. In practice, we have found that initializing random weights works just as well in most cases. We are training the detector by stochastic drop in weight on the L2 loss between the object positions estimated by the network and the true object positions created by Adam using the optimizer [17]."}, {"heading": "IV. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Experimental Setup", "text": "We evaluated our approach by training object detectors for each of eight geometric objects. We constructed mesh representations for each object to be displayed in the simulator. Each training sample consisted of (a) a rendered image of the object and one or more distractors (also from the set of geometric objects) on a simulated tabletop and (b) a label corresponding to the Cartesian coordinates of the object's mass center in the space frame. For each experiment, we performed a small hyperparameter search, evaluating combinations of two learning rates (1e \u2212 4 and 2e \u2212 4) and three lot sizes (25, 50 and 100). We reported on the performance of the best network. The goals of our experiments are: (a) evaluation of the localization accuracy of our trained de-tectors in the real world, including the presence of distractor objects and partial closures. (b) Assess which elements of our detectors are the most important to achieve in order to perform the transmission tasks."}, {"heading": "B. Localization accuracy", "text": "In order to evaluate the accuracy of the learned detectors in the real world, we have taken 480 webcam images of one or more geometric objects on a table at a distance of 70 cm to 105 cm from the camera. Camera position remains constant in all images. We have not checked on the lighting conditions or the rest of the scene around the table (for example, all images contain a part of the robot and tape and wires on the floor).1Categories for which the best final performance was achieved for detectors that have been retrained from scratch. We have measured the ground truth positions for a single object per image by aligning the object on a grid on the tabletop. Each of the eight geometric objects has 60 labeled images in the dataset: 20 with the object alone on the table, 20 in which one or more distractor objects are present on the table, and 20 in which the object is partially covered by another object on the tabletop. Table I summarizes the performance of our test set on the table together with 20 models on the table, one or several objects are also present in the object, one object on the table with 20 or several objects."}, {"heading": "C. Ablation study", "text": "To evaluate the importance of various factors in our training method, we evaluated the sensitivity of the algorithm in the following ways: \u2022 Number of training images \u2022 Number of textures seen in training \u2022 Use of random noise in pre-processing \u2022 Presence of distractions in training \u2022 Use of upstream weights We found that the method is at least reasonably sensitive to all factors except the use of random notes. Figure 4 shows the sensitivity to the number of training samples used for pre-formed models and models that are completely new."}, {"heading": "D. Robotics experiments", "text": "To demonstrate the potential of this technique for transferring robotic behavior that we learned in the simulation to the real world, 3Note that the total number of textures is higher than the number of training examples in some of these experiments, since each scene has many surfaces, each with its own texture. We evaluated the ability to use our object recognition networks to locate an object in disarray and perform a prescribed understanding. [41] To test the robustness of our method for discrepancies in object distribution between training and test time, we evaluated the ability to detect the detected object in 20 increasingly confusing scenes using positions estimated by the detector and commercially available motion planning software. [41] To test the robustness of our method for discrepancies in object distribution between training and test time, some of our test images included distractors placed in orientations that were not seen during the training (e.g. a hexagonal prism that was placed on its side and placed on an object we found to be successful in the pipeline]."}, {"heading": "V. CONCLUSION", "text": "We have shown that an object detector trained only in simulation can achieve high accuracy in the real world to perform gripping in the muddle. Future work will examine how this technique is reliable and effective enough to perform tasks requiring contact manipulation or higher precision. Future directions that could improve the accuracy of object detectors trained by domain randomization include: \u2022 Use of higher resolution camera images \u2022 Optimize the model architecture choice4Each of the models compared was tested with 20,000 training programs examples5https: / / sites.google.com / view / domainrandomization / \u2022 Introduce additional forms of texture, lighting, and rendering randomization into simulation and training on more data \u2022 Include multiple camera points, stereo vision, or depth information \u2022 Combining domain randomization with domain adaptaptionDomain randomization is a promising behavioral feedback direction for research in the real world."}, {"heading": "A. Randomly generated samples from our method", "text": "Figure 7 shows a selection of images used during the training for the object detectors detailed in the paper."}], "references": [{"title": "Using inaccurate models in reinforcement learning", "author": ["Pieter Abbeel", "Morgan Quigley", "Andrew Y Ng"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Reinforcement learning for pivoting task", "author": ["Rika Antonova", "Silvia Cruciani", "Christian Smith", "Danica Kragic"], "venue": "arXiv preprint arXiv:1703.00472,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "The ycb object and model set: Towards common benchmarks for manipulation research", "author": ["Berk Calli", "Arjun Singh", "Aaron Walsman", "Siddhartha Srinivasa", "Pieter Abbeel", "Aaron M Dollar"], "venue": "In Advanced Robotics (ICAR), 2015 International Conference on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Object recognition and full pose registration from a single image for robotic manipulation", "author": ["Alvaro Collet", "Dmitry Berenson", "Siddhartha S Srinivasa", "Dave Ferguson"], "venue": "In Robotics and Automation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "The moped framework: Object recognition and pose estimation for manipulation", "author": ["Alvaro Collet", "Manuel Martinez", "Siddhartha S Srinivasa"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Efficient multi-view object recognition and full pose estimation", "author": ["Alvaro Collet", "Siddhartha S Srinivasa"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Efficient reinforcement learning for robots using informative simulated priors", "author": ["Mark Cutler", "Jonathan P How"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Reinforcement learning with multi-fidelity simulators", "author": ["Mark Cutler", "Thomas J Walsh", "Jonathan P How"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Learning with augmented features for heterogeneous domain adaptation", "author": ["Lixin Duan", "Dong Xu", "Ivor Tsang"], "venue": "arXiv preprint arXiv:1206.4660,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Object recognition and pose estimation using color cooccurrence histograms and geometric modeling", "author": ["Staffan Ekvall", "Danica Kragic", "Frank Hoffmann"], "venue": "Image and Vision Computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Deep predictive policy training using reinforcement learning", "author": ["Ali Ghadirzadeh", "Atsuto Maki", "Danica Kragic", "M\u00e5rten Bj\u00f6rkman"], "venue": "arXiv preprint arXiv:1703.00727,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "What and where: 3d object recognition with accurate pose", "author": ["Iryna Gordon", "David G Lowe"], "venue": "In Toward category-level object recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Learning invariant feature spaces to transfer skills with reinforcement learning", "author": ["Abhishek Gupta", "Coline Devin", "YuXuan Liu", "Pieter Abbeel", "Sergey Levine"], "venue": "ICLR 2017,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Lsda: Large scale detection through adaptation", "author": ["Judy Hoffman", "Sergio Guadarrama", "Eric Tzeng", "Ronghang Hu", "Jeff Donahue", "Ross Girshick", "Trevor Darrell", "Kate Saenko"], "venue": "In Neural Information Processing Symposium (NIPS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Efficient learning of domain-invariant image representations", "author": ["Judy Hoffman", "Erik Rodner", "Jeff Donahue", "Trevor Darrell", "Kate Saenko"], "venue": "arXiv preprint arXiv:1301.3224,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "3d simulation for robot arm control with deep q-learning", "author": ["Stephen James", "Edward Johns"], "venue": "arXiv preprint arXiv:1609.03759,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Learning omnidirectional path following using dimensionality reduction", "author": ["J Zico Kolter", "Andrew Y Ng"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["Brian Kulis", "Kate Saenko", "Trevor Darrell"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Endto-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Revisiting batch normalization for practical domain adaptation", "author": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "venue": "arXiv preprint arXiv:1603.04779,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Learning transferable features with deep adaptation networks", "author": ["Mingsheng Long", "Yue Cao", "Jianmin Wang", "Michael I Jordan"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Three-dimensional object recognition from single two-dimensional images", "author": ["David G Lowe"], "venue": "Artificial intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1987}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "arXiv preprint arXiv:0902.3430,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "A selfsupervised learning system for object detection using physics simulation and multi-view pose estimation", "author": ["Chaitanya Mitash", "Kostas E Bekris", "Abdeslam Boularias"], "venue": "arXiv preprint arXiv:1703.03347,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2017}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Ensemblecio: Full-body dynamic motion planning that transfers to physical humanoids", "author": ["Igor Mordatch", "Kendall Lowrey", "Emanuel Todorov"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "How useful is photo-realistic rendering for visual learning", "author": ["Yair Movshovitz-Attias", "Takeo Kanade", "Yaser Sheikh"], "venue": "In Computer Vision\u2013 ECCV 2016 Workshops,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Description and recognition of curved objects", "author": ["Ramakant Nevatia", "Thomas O Binford"], "venue": "Artificial Intelligence,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1977}, {"title": "Learning deep object detectors from 3d models", "author": ["Xingchao Peng", "Baochen Sun", "Karim Ali", "Kate Saenko"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Depthsynth: Real-time realistic synthetic data gen-  eration from cad models for 2.5 d recognition", "author": ["Benjamin Planche", "Ziyan Wu", "Kai Ma", "Shanhui Sun", "Stefan Kluckner", "Terrence Chen", "Andreas Hutter", "Sergey Zakharov", "Harald Kosch", "Jan Ernst"], "venue": "arXiv preprint arXiv:1702.08558,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2017}, {"title": "Epopt: Learning robust neural network policies using model ensembles", "author": ["Aravind Rajeswaran", "Sarvjeet Ghotra", "Sergey Levine", "Balaraman Ravindran"], "venue": "arXiv preprint arXiv:1610.01283,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Playing for data: Ground truth from computer games", "author": ["Stephan R Richter", "Vibhav Vineet", "Stefan Roth", "Vladlen Koltun"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["Andrei A Rusu", "Matej Vecerik", "Thomas Roth\u00f6rl", "Nicolas Heess", "Razvan Pascanu", "Raia Hadsell"], "venue": "arXiv preprint arXiv:1610.04286,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "cad) 2 RL: Real single-image flight without a single real image", "author": ["Fereshteh Sadeghi", "Sergey Levine"], "venue": "arXiv preprint arXiv:1611.04201,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael I Jordan", "Philipp Moritz"], "venue": "In ICML,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views", "author": ["Hao Su", "Charles R Qi", "Yangyan Li", "Leonidas J Guibas"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "From virtual to reality: Fast adaptation of virtual object detectors to real domains", "author": ["Baochen Sun", "Kate Saenko"], "venue": "In BMVC,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Unsupervised crossdomain image generation", "author": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "venue": "arXiv preprint arXiv:1611.02200,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "A textured object recognition pipeline for color and depth image data", "author": ["Jie Tang", "Stephen Miller", "Arjun Singh", "Pieter Abbeel"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Emanuel Todorov", "Tom Erez", "Yuval Tassa"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "Adapting deep visuomotor representations with weak pairwise constraints", "author": ["Eric Tzeng", "Coline Devin", "Judy Hoffman", "Chelsea Finn", "Pieter Abbeel", "Sergey Levine", "Kate Saenko", "Trevor Darrell"], "venue": "In Workshop on the Algorithmic Foundations of Robotics (WAFR),", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "Deep domain confusion: Maximizing for domain invariance", "author": ["Eric Tzeng", "Judy Hoffman", "Ning Zhang", "Kate Saenko", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1412.3474,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "Superhuman performance of surgical tasks by robots using iterative learning from human-guided demonstrations", "author": ["Jur Van Den Berg", "Stephen Miller", "Daniel Duckworth", "Humphrey Hu", "Andrew Wan", "Xiao-Yu Fu", "Ken Goldberg", "Pieter Abbeel"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "Real-time visual tracking of 3d objects with dynamic handling of occlusion", "author": ["Patrick Wunsch", "Gerd Hirzinger"], "venue": "In Robotics and Automation,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1997}, {"title": "Cross-domain video concept detection using adaptive svms", "author": ["Jun Yang", "Rong Yan", "Alexander G Hauptmann"], "venue": "In Proceedings of the 15th ACM international conference on Multimedia,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2007}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2014}, {"title": "Preparing for the unknown: Learning a universal policy with online system identification", "author": ["Wenhao Yu", "C Karen Liu", "Greg Turk"], "venue": "arXiv preprint arXiv:1702.02453,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2017}, {"title": "Detection and localization  of multiple objects", "author": ["Stefan Zickler", "Manuela M Veloso"], "venue": "In Humanoid Robots,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2006}], "referenceMentions": [{"referenceID": 25, "context": "Learning in simulation is especially promising for building on recent results using deep reinforcement learning to achieve human-level performance on tasks like Atari [27] and robotic control [21], [38].", "startOffset": 167, "endOffset": 171}, {"referenceID": 19, "context": "Learning in simulation is especially promising for building on recent results using deep reinforcement learning to achieve human-level performance on tasks like Atari [27] and robotic control [21], [38].", "startOffset": 192, "endOffset": 196}, {"referenceID": 35, "context": "Learning in simulation is especially promising for building on recent results using deep reinforcement learning to achieve human-level performance on tasks like Atari [27] and robotic control [21], [38].", "startOffset": 198, "endOffset": 202}, {"referenceID": 25, "context": "It often requires hundreds of thousands or millions of samples [27], which could take thousands of hours to collect, making it impractical for many applications.", "startOffset": 63, "endOffset": 67}, {"referenceID": 5, "context": ", [6], [5], [44]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": ", [6], [5], [44]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 40, "context": ", [6], [5], [44]).", "startOffset": 12, "endOffset": 16}, {"referenceID": 34, "context": "Although previous work demonstrated the ability to perform robotic control using a neural network pretrained on ImageNet and fine-tuned on randomized rendered pixels [37], this paper provides the first demonstration that domain randomization can be useful for robotic tasks requiring precision.", "startOffset": 166, "endOffset": 170}, {"referenceID": 3, "context": ", [4], [5], [6], [10], [44], [50], [54]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": ", [4], [5], [6], [10], [44], [50], [54]).", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": ", [4], [5], [6], [10], [44], [50], [54]).", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": ", [4], [5], [6], [10], [44], [50], [54]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 40, "context": ", [4], [5], [6], [10], [44], [50], [54]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 45, "context": ", [4], [5], [6], [10], [44], [50], [54]).", "startOffset": 29, "endOffset": 33}, {"referenceID": 49, "context": ", [4], [5], [6], [10], [44], [50], [54]).", "startOffset": 35, "endOffset": 39}, {"referenceID": 40, "context": ", a full 3D mesh model [44] or a 3D metric feature representation [5]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 4, "context": ", a full 3D mesh model [44] or a 3D metric feature representation [5]).", "startOffset": 66, "endOffset": 69}, {"referenceID": 11, "context": ", Scale-Invariant Feature Transform [SIFT] features [12] or color co-occurrence histograms [10]) are matched with the 3D models (or features from the 3D models).", "startOffset": 52, "endOffset": 56}, {"referenceID": 9, "context": ", Scale-Invariant Feature Transform [SIFT] features [12] or color co-occurrence histograms [10]) are matched with the 3D models (or features from the 3D models).", "startOffset": 91, "endOffset": 95}, {"referenceID": 3, "context": "For example, a black-box nonlinear optimization algorithm can be used to minimize the re-projection error of the SIFT points from the object model and the 2D points in the test image [4].", "startOffset": 183, "endOffset": 186}, {"referenceID": 5, "context": "Most successful approaches rely on using multiple camera frames [6] or depth information [44].", "startOffset": 64, "endOffset": 67}, {"referenceID": 40, "context": "Most successful approaches rely on using multiple camera frames [6] or depth information [44].", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "some success with only monocular camera images [4].", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": ", [9], [14], [15], [19], [23], [25], [51]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 13, "context": ", [9], [14], [15], [19], [23], [25], [51]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": ", [9], [14], [15], [19], [23], [25], [51]).", "startOffset": 13, "endOffset": 17}, {"referenceID": 18, "context": ", [9], [14], [15], [19], [23], [25], [51]).", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": ", [9], [14], [15], [19], [23], [25], [51]).", "startOffset": 25, "endOffset": 29}, {"referenceID": 23, "context": ", [9], [14], [15], [19], [23], [25], [51]).", "startOffset": 31, "endOffset": 35}, {"referenceID": 46, "context": ", [9], [14], [15], [19], [23], [25], [51]).", "startOffset": 37, "endOffset": 41}, {"referenceID": 47, "context": ", [52]), adapting the weights of the model based on the statistics of the source and target domains (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 20, "context": ", [22]), learning invariant features between domains (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 43, "context": ", [47]), and learning a mapping from the target", "startOffset": 2, "endOffset": 6}, {"referenceID": 39, "context": ", [43]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": "Researchers in the reinforcement learning community have also studied the problem of domain adaptation by learning invariant feature representations [13], adapting pretrained networks [35], and other methods.", "startOffset": 149, "endOffset": 153}, {"referenceID": 12, "context": "See [13] for a more complete treatment of domain adaptation in the reinforcement learning literature.", "startOffset": 4, "endOffset": 8}, {"referenceID": 15, "context": "Though using realistic RGB rendering alone has had limited success for transferring to real robotic tasks [16], incorporating realistic simulation of depth information can allow models trained on rendered images to transfer reasonably well to the real world [32].", "startOffset": 106, "endOffset": 110}, {"referenceID": 30, "context": "Though using realistic RGB rendering alone has had limited success for transferring to real robotic tasks [16], incorporating realistic simulation of depth information can allow models trained on rendered images to transfer reasonably well to the real world [32].", "startOffset": 258, "endOffset": 262}, {"referenceID": 32, "context": "Combining data from high-quality simulators with other approaches like fine-tuning can also reduce the number of labeled samples required in the real world [34].", "startOffset": 156, "endOffset": 160}, {"referenceID": 6, "context": "It is often faster to fine-tune a controller learned in simulation than to learn from scratch in the real world [7], [18].", "startOffset": 112, "endOffset": 115}, {"referenceID": 17, "context": "It is often faster to fine-tune a controller learned in simulation than to learn from scratch in the real world [7], [18].", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "In [11], the authors use a variational autoencoder trained on simulated data to encode trajectories of motor outputs corresponding to a desired behavior type (e.", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "[36] explore using the progressive network", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "In [46], the authors explore learning a correspondence between domains that allows the real images to be mapped into a space understood by the model.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "require reward functions or labeled data, which can be difficult to obtain in the real world, Mitash and collaborators [26] explore pretraining an object detector using realistic rendered images with randomized lighting from 3D models to bootstrap an automated learning learning process that does", "startOffset": 119, "endOffset": 123}, {"referenceID": 0, "context": ", [1] and [8]) to surgical robotics (e.", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": ", [1] and [8]) to surgical robotics (e.", "startOffset": 10, "endOffset": 13}, {"referenceID": 44, "context": ", [48]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 26, "context": "In the context of physics adaptation, Mordatch and collaborators [28] show that training a policy on an ensemble of dynamics models can make the controller robust to modeling error and improve transfer to a real robot.", "startOffset": 65, "endOffset": 69}, {"referenceID": 1, "context": "Similarly, in [2], the authors train a policy to pivot a tool held in the robot\u2019s gripper in a simulator with randomized friction and action delays, and find that it works in the real world and is robust to errors in estimation of the system parameters.", "startOffset": 14, "endOffset": 17}, {"referenceID": 48, "context": "[53] use a model trained on varied physics to perform system identification using online trajectory data, but their approach is not shown to succeed in the real world.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] explore different training strategies for learning from an ensemble of models, including adversarial training and adapting the ensemble distribution using data from the target domain, but also do not demonstrate successful real-world transfer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": ", [30], [24]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 22, "context": ", [30], [24]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 37, "context": "estimation [40] and object detection [42], [29].", "startOffset": 11, "endOffset": 15}, {"referenceID": 38, "context": "estimation [40] and object detection [42], [29].", "startOffset": 37, "endOffset": 41}, {"referenceID": 27, "context": "estimation [40] and object detection [42], [29].", "startOffset": 43, "endOffset": 47}, {"referenceID": 29, "context": "In [31], the authors find that by pretraining a network on ImageNet and fine-tuning on synthetic data created from", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "Sadeghi and Levine\u2019s work [37] is the most similar to our own.", "startOffset": 26, "endOffset": 30}, {"referenceID": 41, "context": "We render images using the MuJoCo Physics Engine\u2019s [45] built-in renderer.", "startOffset": 51, "endOffset": 55}, {"referenceID": 36, "context": "In particular, we use a modified version the VGG-16 architecture [39] shown in Figure 2.", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "We train the detector through stochastic gradient descent on the L2 loss between the object positions estimated by the network and the true object positions using the Adam optimizer [17].", "startOffset": 182, "endOffset": 186}, {"referenceID": 4, "context": "Even with over-fitting, the accuracy is comparable at a similar distance to the translation error in traditional techniques for pose estimation in clutter from a single monocular camera frame [5] that use higher-resolution images.", "startOffset": 192, "endOffset": 195}, {"referenceID": 2, "context": "To test the performance of our object detectors on realworld objects with non-uniform textures, we trained an object detector to localize a can of Spam from the YCB Dataset [3].", "startOffset": 173, "endOffset": 176}], "year": 2017, "abstractText": "Bridging the \u2018reality gap\u2019 that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.", "creator": "LaTeX with hyperref package"}}}