{"id": "1703.06914", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Applying Deep Machine Learning for psycho-demographic profiling of Internet users using O.C.E.A.N. model of personality", "abstract": "In the modern era, each Internet user leaves enormous amounts of auxiliary digital residuals (footprints) by using a variety of on-line services. All this data is already collected and stored for many years. In recent works, it was demonstrated that it's possible to apply simple machine learning methods to analyze collected digital footprints and to create psychological profiles of individuals. However, while these works clearly demonstrated the applicability of machine learning methods for such an analysis, created simple prediction models still lacks accuracy necessary to be successfully applied to practical needs. We have assumed that using advanced deep machine learning methods may considerably increase the accuracy of predictions. We started with simple machine learning methods to estimate basic prediction performance and moved further by applying advanced methods based on shallow and deep neural networks. Then we compared prediction power of studied models and made conclusions about its performance. Finally, we made hypotheses how prediction accuracy can be further improved. As result of this work, we provide full source code used in the experiments for all interested researchers and practitioners in corresponding GitHub repository. We believe that applying deep machine learning for psychological profiling may have an enormous impact on the society (for good or worse) and providing full source code of our research we hope to intensify further research by the wider circle of scholars.", "histories": [["v1", "Tue, 7 Mar 2017 09:27:21 GMT  (814kb,D)", "https://arxiv.org/abs/1703.06914v1", "arXiv admin note: text overlap witharXiv:1207.0580by other authors"], ["v2", "Wed, 5 Jul 2017 12:16:24 GMT  (814kb,D)", "http://arxiv.org/abs/1703.06914v2", null]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1207.0580by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.CY", "authors": ["iaroslav omelianenko"], "accepted": false, "id": "1703.06914"}, "pdf": {"name": "1703.06914.pdf", "metadata": {"source": "CRF", "title": "Applying Deep Machine Learning for psycho-demographic profiling of Internet users using O.C.E.A.N. model of personality", "authors": ["Iaroslav Omelianenko"], "emails": ["yaric@newground.com.ua"], "sections": [{"heading": null, "text": "In fact, it is so. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it. (it.) It is. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it.) is. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it.) It. (it. (it. (it.) It. (it.) is. (it. (it. (it.) It. (it.) It. (it. (it.) It. (it. (it. (it.) It. (it. (it.) It. (it.) It. (it. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it.) It. (it. (it. (it.) It. (it.) It. (it. (it. (it. (it.) It. It. (it.) It. (it.) is. It. (it. (it. It. (it. It.) is. (it. (it. (it. It."}, {"heading": "Dimensionality reduction with SVD", "text": "After two previous steps, the resulting, user-friendly, sparse matrix still has a significant number of features per data sample: 8,523 feature columns. To make it even more maintainable, we have considered applying singular value substitution [Golub, G. H. and Reinsch, C. 1970], which represents self-decomposition-based methods and projects a set of data points into a series of dimensions. As mentioned in [Kosinski et. al, 2016], reducing the dimensionality of the data corpus has a number of advantages: \u2022 With reduced feature space, we can use fewer data samples, since most machine learning analysis algorithms require that the number of data samples exceed the number of features (input variables) \u2022 It reduces the risk of overmatch and increases the statistical power of the results \u2022 It eliminates multicollinity and redundancy in the data group by significantly reducing the number of attributes (and related to the data corpable)."}, {"heading": "Factor rotation analysis", "text": "The method of factor rotation analysis can be used to further simplify SVD dimensions and increase their interpretability by mapping the original multidimensional space into a new, rotated space. Rotation approaches can be orthogonal (i.e., generate uncorrelated dimensions) or oblique (i.e. allow correlations between rotated dimensions). In this work during data preprocessing, we applied one of the most popular orthogonal rotation methods - varimax. It minimizes both the number of dimensions associated with each variable and the number of variables associated with each dimension, thereby improving the interpretability of the data by human analysts. For more details on rotation techniques see [Abdi, H., 2003].3 Regression analyses There are a wealth of methods developed to create prediction models for machine learning, which are suitable for analyzing large data sets using pre-calculated learning methods [Vectales, Vectales, Vectales, Vectales, Vectales, Vectales, Vectales, Vectales, Vectales, Vectales, Vectales, Vectales, Vecalgorithms, Vectales, Vectales, Vectales, Vectales, Vectales, Vectales, Vectales, Vectales, Vectales, and Vectales, Vectales, Vectales, Vectales)."}, {"heading": "Linear regression analysis", "text": "Linear regression is an approach to modelling the relationship between a continuous scalar dependent variable y and one or more explanatory (or independent) variables called X. The case of an explanatory variable is referred to as simple linear regression. For more than one explanatory variable, the process is referred to as multiple linear regression [David A. Freedman, 2009]. In linear regression, relationships are modeled using a linear predictor function y = \u0418TX, whose unknown model parameters are calculated from the input data. Such models are called linear models [Hilary L. Seal, 1967]. We used linear regression to create predictive models for the analysis of six continuously dependent variables in a given data body: age, openness, conscientiousness, extroversion, agreeableness, and neuroticism."}, {"heading": "Logistic regression analysis", "text": "Logistic regression is a regression model in which the dependent variable is categorical [David A. Freedman, 2009]. It measures the relationship between the categorical dependent variable and one or more independent variables by calculating probabilities using a logistic function \u03c3 (x) = 11 + e \u2212 x, the cumulative logistic distribution [Rodriguez, G., 2007]. We considered only specialized binary logistic regression, because categorical dependent variables in our data corpus (Gender and Political Views) are binominal, i.e. have only two possible types, \"0\" and \"1.\""}, {"heading": "Cross-Validation", "text": "In k-fold cross-validation, the original sample is randomly divided into k-sized sub-samples of the same size. Of the k-sample, a single sub-sample is retained as validation data for testing the model, and the remaining k \u2212 1 sub-samples are used as training data. The cross-validation process is then repeated k-times (the folds), with each of the k-sample being used exactly once as validation data. k results from the folds can then be averaged to achieve a single estimation. k results from the folds can then be averaged. The advantage of this method is that all observations are used for both training and validation, and each observation is used exactly once for validation. 10-fold cross-validation is often used, but generally k remains an indeterminate parameter [Kohavi, Ron, 1995]."}, {"heading": "Dimensionality reduction", "text": "To reduce the number of features (input variables) in the data corpus, multiple value decomposition (SVD) followed by Varimax factor rotation analysis was used. The number of Varimax rotated single value decomposition measurements (K) has a significant impact on the accuracy of model forecasts. To find an optimal number of SVD dimensions, we performed an analysis of the relationships between K and the accuracy of model forecasts by using a series of regression models for different values of K. Then, we plotted the accuracy of regression models against the selected number of K SVD dimensions. Typically, the accuracy of prediction grows rapidly within lower ranges of K and can begin when the number of dimensions decreases. K selection values marking the end of rapid growth of prediction accuracy values provides the decent interpretation capability of input datopics. Generally, the learning machine values are better when machine-specific results are obtained with more specific K."}, {"heading": "The three-layer Deep Learning Network Architecture Evaluation", "text": "Our experiments with deep learning networks started with a simple DNN architecture consisting of two hidden layers with ReLU activation and dropout after each hidden layer with a probability of 0.5. The experimental network graph shown in Figure 7. We started with the learning rate \u03b3 = 0.0001, which resulted in the best prediction performance for flat ANN and experiment series to estimate the optimal value of K-SVD dimensions. Optimal predictive accuracy of the DNN model was achieved with K = 256 SVD dimensions and two hidden layers of [512, 256] units correspondingly. Similar predictive accuracy can be achieved with K = 1024 SVD dimensions, and [2048, 1024] units per layer with a learning rate of 10 \u2212 5. But we did not consider later sets of hyperparameters due to their additional computational overhead, while statistically delivering the same results as before."}, {"heading": "The four-layer Deep Learning Network Architecture Evaluation", "text": "This year it is more than ever before."}], "references": [{"title": "Mining Big Data to Extract Patterns and Predict Real-Life Outcomes", "author": ["Kosinski et"], "venue": "Psychological Methods 2016,", "citeRegEx": "et.,? \\Q2016\\E", "shortCiteRegEx": "et.", "year": 2016}, {"title": "Tracking the digital footprints of personality", "author": ["R. Lambiotte", "M. Kosinski", "R. 2014] Lambiotte"], "venue": "Proceedings of the Institute of Electrical and Electronics Engineers,", "citeRegEx": "Lambiotte et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lambiotte et al\\.", "year": 2014}, {"title": "The International Personality Item Pool and the future of public-domain personality measures", "author": ["L.R. Goldberg", "J.A. Johnson", "H.W. Eber", "R. Hogan", "M.C. Ashton", "C.R. Cloninger", "H.G. Gough"], "venue": "[Goldberg", "citeRegEx": "Goldberg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2006}, {"title": "Singular value decomposition and least squares solutions", "author": ["G.H. Golub", "Reinsch", "G.H.C. 1970] Golub", "C. Reinsch"], "venue": "Numerische Mathematik,", "citeRegEx": "Golub et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Golub et al\\.", "year": 1970}, {"title": "Factor rotations in factor analyses", "author": ["H. Abdi"], "venue": null, "citeRegEx": "Abdi,? \\Q2003\\E", "shortCiteRegEx": "Abdi", "year": 2003}, {"title": "Deep learning. Manuscript in preparation", "author": ["Goodfellow et al", "2016] Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Probabilistic Graphical Models. Stanford University. Retrieved from http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course= ProbabilisticGraphicalModels", "author": ["Daphne Koller"], "venue": null, "citeRegEx": "Koller,? \\Q2012\\E", "shortCiteRegEx": "Koller", "year": 2012}, {"title": "Linear Regression Analysis: Theory and Computing, World Scientific, pp", "author": ["Yan", "Su", "2009] Xin Yan", "Xiao Gang Su"], "venue": null, "citeRegEx": "Yan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2009}, {"title": "Statistical Models: Theory and Practice", "author": ["David A. Freedman", "2009] David A. Freedman"], "venue": null, "citeRegEx": "Freedman and Freedman,? \\Q2009\\E", "shortCiteRegEx": "Freedman and Freedman", "year": 2009}, {"title": "ROCR: Visualizing classifier performance", "author": ["Sing", "Sander", "Beerenwinkel", "Lengauer", "T. 2005] Sing", "O. Sander", "N. Beerenwinkel", "T. Lengauer"], "venue": "in R. Bioinformatics,", "citeRegEx": "Sing et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sing et al\\.", "year": 2005}, {"title": "The frequency distribution of the product moment correlation coefficient in random samples of any size draw from non-normal universes", "author": ["A.K. Gain"], "venue": "[Gain,", "citeRegEx": "Gain,? \\Q1951\\E", "shortCiteRegEx": "Gain", "year": 1951}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "author": ["Kohavi", "Ron"], "venue": "Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence", "citeRegEx": "Kohavi and Ron,? \\Q1995\\E", "shortCiteRegEx": "Kohavi and Ron", "year": 1995}, {"title": "Singular value decomposition and its visualization", "author": ["Zhang", "Marron", "Shen", "Zhu", "L. 2007] Zhang", "J. Marron", "H. Shen", "Z. Zhu"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Zhang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2007}, {"title": "mice: Multivariate Imputation by Chained Equations in R", "author": ["van Buuren", "Groothuis-Oudshoorn", "2011] Stef van Buuren", "Karin"], "venue": "Groothuis-Oudshoorn", "citeRegEx": "Buuren et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Buuren et al\\.", "year": 2011}, {"title": "Neural Networks for Pattern Recognition, Oxford University Press, Inc", "author": ["Christopher M. Bishop", "1995] Christopher M. Bishop"], "venue": null, "citeRegEx": "Bishop and Bishop,? \\Q1995\\E", "shortCiteRegEx": "Bishop and Bishop", "year": 1995}, {"title": "Learning Deep Architectures for AI", "author": ["Bengio", "Yoshua"], "venue": "Foundations and Trends in Machine Learning", "citeRegEx": "Bengio and Yoshua,? \\Q2009\\E", "shortCiteRegEx": "Bengio and Yoshua", "year": 2009}, {"title": "Neural network studies", "author": ["Tetko", "Livingstone", "Luik", "1995] Tetko", "I. V", "D.J. Livingstone", "A.I. Luik"], "venue": "J. Chem. Inf. Comput. Sci", "citeRegEx": "V. et al\\.,? \\Q1995\\E", "shortCiteRegEx": "V. et al\\.", "year": 1995}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "G. et al", "2012] Hinton", "Geoffrey E", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov"], "venue": null, "citeRegEx": "E. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "E. et al\\.", "year": 2012}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["Nair", "Hinton", "2010] Vinod Nair", "Geoffrey Hinton"], "venue": "ICML. Retrieved from http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Deep Sparse Rectifier Neural Networks. JMLR W&CP 15:315-323 Retrieved from http://jmlr.org/proceedings/papers/v15/glorot11a", "author": ["Glorot", "Bordes", "Bengio", "2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": null, "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba", "2014] Diederik P. Kingma", "Lei Jimmy Ba"], "venue": null, "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Do Deep Nets Really Need to be Deep", "author": ["Ba", "Caruana", "2014] Lei Jimmy Ba", "Rich Caruana"], "venue": null, "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Optimization by Simulated Annealing", "author": ["Kirkpatrick et al", "1983] S. Kirkpatrick", "C.D. Gelatt Jr.", "M.P. Vecchi"], "venue": "Science, 13 May 1983:", "citeRegEx": "al. et al\\.,? \\Q1983\\E", "shortCiteRegEx": "al. et al\\.", "year": 1983}], "referenceMentions": [{"referenceID": 10, "context": "In this research we have considered following metrics: \u2022 the accuracy of prediction model applied to the continuous dependent variable will be measured as Pearson product-moment correlation [Gain, 1951] \u2022 the accuracy of prediction model applied to the bi-nominal dependent variable will be measured as area under the receiver-operating characteristic curve coefficient (AUC) [Sing, Sander, Beerenwinkel, Lengauer, 2005] Before executing models make sure that data corpus already preprocessed as described in Subsection: \"Construction of sparse users-likes matrix and matrix trimming\" When data corpus is ready, the following command can be executed to start linear/logistic regression models building and its predictive performance evaluation (run command from terminal in the project\u2019s root directory): $ R s c r i p t .", "startOffset": 190, "endOffset": 202}], "year": 2017, "abstractText": "In the modern era, each Internet user leaves enormous amounts of auxiliary digital residuals (footprints) by using a variety of on-line services. All this data is already collected and stored for many years. In recent works, it was demonstrated that it\u2019s possible to apply simple machine learning methods to analyze collected digital footprints and to create psycho-demographic profiles of individuals. However, while these works clearly demonstrated the applicability of machine learning methods for such an analysis, created simple prediction models still lacks accuracy necessary to be successfully applied for practical needs. We have assumed that using advanced deep machine learning methods may considerably increase the accuracy of predictions. We started with simple machine learning methods to estimate basic prediction performance and moved further by applying advanced methods based on shallow and deep neural networks. Then we compared prediction power of studied models and made conclusions about its performance. Finally, we made hypotheses how prediction accuracy can be further improved. As result of this work, we provide full source code used in the experiments for all interested researchers and practitioners in corresponding GitHub repository. We believe that applying deep machine learning for psycho-demographic profiling may have an enormous impact on the society (for good or worse) and provides means for Artificial Intelligence (AI) systems to better understand humans by creating their psychological profiles. Thus AI agents may achieve the human-like ability to participate in conversation (communication) flow by anticipating human opponents\u2019 reactions, expectations, and behavior. By providing full source code of our research we hope to intensify further research in the area by the wider circle of scholars.", "creator": "LaTeX with hyperref package"}}}