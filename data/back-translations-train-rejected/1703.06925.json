{"id": "1703.06925", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "Black-Box Optimization in Machine Learning with Trust Region Based Derivative Free Algorithm", "abstract": "In this work, we utilize a Trust Region based Derivative Free Optimization (DFO-TR) method to directly maximize the Area Under Receiver Operating Characteristic Curve (AUC), which is a nonsmooth, noisy function. We show that AUC is a smooth function, in expectation, if the distributions of the positive and negative data points obey a jointly normal distribution. The practical performance of this algorithm is compared to three prominent Bayesian optimization methods and random search. The presented numerical results show that DFO-TR surpasses Bayesian optimization and random search on various black-box optimization problem, such as maximizing AUC and hyperparameter tuning.", "histories": [["v1", "Mon, 20 Mar 2017 19:00:18 GMT  (490kb)", "http://arxiv.org/abs/1703.06925v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hiva ghanbari", "katya scheinberg"], "accepted": false, "id": "1703.06925"}, "pdf": {"name": "1703.06925.pdf", "metadata": {"source": "CRF", "title": "Black-Box Optimization in Machine Learning with Trust Region Based Derivative Free Algorithm", "authors": ["Hiva Ghanbari", "Katya Scheinberg"], "emails": ["<hiva.ghanbari@gmail.com>,", "<katyascheinberg@gmail.com>."], "sections": [{"heading": null, "text": "ar Xiv: 170 3.06 925v 1 [cs.L G] 20 Mar 201 7Derivative Free Optimization (DFO-TR) Method for directly maximizing the Area Under Receiver Operating Characteristic Curve (AUC), a non-smooth, noisy function. We show that AUC is a smooth function when the distributions of positive and negative data points obey a common normal distribution. The practical performance of this algorithm is compared with three prominent Bayean optimization methods and random search. The presented numerical results show that DFO-TR exceeds Bayesian optimization and random search for various black box optimization problems such as maximizing AUC and hyperparameter tuning."}, {"heading": "1. Introduction", "text": "Most machine learning methods (ML) are based on optimization tools for conducting training. Typically, these models are built so that at least stochastic estimates of the course can be calculated; for example, when the optimization of smallest squares or logistical loss of a neural network is shown on a given dataset. Lately, however, with the increasing need to tune hyperparameters of ML models, black box optimization methods have been considered in a significant way. These methods do not rely on explicit gradient calculation, but assume that only functional values can be calculated, usually with noises. There are two relatively independent directions of research for black box optimization methods (BO, 1994 et al.). There are two relatively independent approaches to research for Black-Bayesian optimization methods (BO) -popular methods of optimization (BO, 1994 et al.), predominantly popular in the community, ML-derived optimization and DFO."}, {"heading": "2. Algorithmic Framework of DFO-TR", "text": "In fact, it is not that this is a way in which people in a country in which they are able to survive themselves and others. (...) It is not that they are able to survive themselves. (...) It is not that they are able to survive themselves. (...) It is not that they are able to survive themselves. (...) It is not that they are able to survive themselves. (...) It is not that they are able to survive themselves. (...) It is that they are not able to survive themselves. (...) It is not that they are able to survive themselves. (...) It is not that they are able to survive themselves. (...) It is not that they are able to survive themselves. (...) It is not that they are able to survive themselves. (...) It is not that they are able to survive themselves. (...) It is not that they are able to survive themselves. (...) It is not that they are able to survive themselves. (...) It is not that they are able to survive themselves."}, {"heading": "3. AUC function and its expectation", "text": "In this section we define the AUC function of a linear classifier wTx and show that under certain assumptions about the data distribution ratios, their expected values with respect to the distribution are smooth. First, we assume that we have two predefined sets, each of which is obtained from a particular linear classifier wTx, the corresponding AUC value, a (noisy) nonsmooth deterministic function, is obtained as (Mann & Whitney, 1947) FAUC (w) = 1: 1 Iw (x + i \u2212 j) N + N \u2212 N (1), where Iw is defined an indicator function, the asIw (x + i, x \u2212 j) if wTx \u2212 j."}, {"heading": "4. Bayesian Optimization versus DFO-TR", "text": "Bayesian optimization framework, as outlined in Algorithm 2, but as DFO-TR framework works by constructing a (probabilistic) modelM (w) of the true function f by using functional values previously calculated by the algorithm. Next iterate wk is calculated by optimizing a capture function, aM, which represents a trade-off between minimizing the model and improving the model by examining areas where f (w) has not been sampled. Different Bayesian optimization algorithms use different models and different capture functions, for example, expected improvement (M. Schonlau & Jones, 1998) on the best observed functional value is a popular capture function in literature. The main advantage and difficulty of BO methods is that the capture function can have a complex structure, and must be globally optimized on each iteration. For example, the algorithm in Brochu et Jones, 2010 uses a derivative function (REDerivative Jones)."}, {"heading": "5. Numerical Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Optimizing Smooth, NonConvex Benchmark Functions", "text": "In this section we compare the performance of DFO-TR and Bayesian optimization algorithms to optimize three nonconvex smooth benchmark functions. We compare the precision \u2206 fopt with the global optimal value known and calculated according to a certain number of function evaluations.Algorithm 1 is implemented in Python 2.7.11 1. We start with the zero vector as the starting point. Furthermore, the radius of the trust region is initialized as \u2206 0 = 1 and the initial interpolation set has d + 1 random members. The parameters are selected as \u03b70 = 0.001 = 0.75, \u03b8 = 10, \u03b31 = 0.98 and \u03b32 = 1.5. We have used the hyperparameter optimization library HPOlib 2 to improve the experi-1 https: / / github.com / TheClimateCorporation / dfo algorithm 2."}, {"heading": "5.2. Optimizing AUC Function", "text": "In this section we compare the performance of DFOTR and the three Bavarian optimization algorithms, TPE, SMAC and SPEARMINT, to enable the optimization of AUC of a linear classification. While we are able to optimize the objective, in practice we have a limited amount of data that we are able to calculate as it appears in the table. Essentially, this means that we expect the goal to be optimized to a certain accuracy."}, {"heading": "5.2.1. STOCHASTIC VERSUS DETERMINISTIC DFO-TR", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we will be able, that we will be able, that we will be able, that we will be able, and that we will be able, that we will be able, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in a position, to put ourselves in the position we are in."}, {"heading": "5.3. Hyperparameter Tuning of Cost-Sensitive RBF-Kernel SVM", "text": "We look at the tuning parameters of an RBF kernel, cost-sensitive, SVM, with the frame 2 regularization parameters \u03bb, kernel width \u03b3, and positive class cost cp. Thus, in this setting, we compare the performance of a DFO-TR, Table 6. Comparison DFO-TR vs. random search algorithm. Data DFO-TR Random SearchAUC num. fevals AUC num. fevalsfourclass 0,835 \u00b1 0.019 100 0.836 \u00b1 0.017 100 svmguide1 0.839 \u00b1 0.004 200 svmguide1 0.988 \u00b1 0.004 100 0.965 \u00b1 0.009 200 Diabetes 0.829 \u00b1 0.041 100 0.783 \u00b1 0.038 \u00b1 0.045 200 Shuttle 0.990 \u00b1 0.004 \u00b1 0.008 \u00b1 0.008 \u00b1 0.004 \u00b1 0.008 \u00b1 0.004 \u00b1 0.088 \u00b1 0.008 \u00b1 0.009 \u00b1 0.003 \u00b1 0.003 \u00b1 0.003 0.003 0.003 0.003 0.003 0.003 0.003 0.003 0.003 0.003 -0.003"}, {"heading": "6. Conclusion", "text": "In this paper, we demonstrate that model-based derivative free optimization is a better alternative to Bayean optimization for some machine learning black box optimization tasks. We rely on an existing convergent stochastic trust region framework to provide the theoretical basis for the chosen algorithm, and we demonstrate the efficiency of a hands-on implementation of DFO-TR to optimize AUC functionality versus linear classifiers, hyperparameter tuning, and other benchmark problems."}], "references": [{"title": "Algorithms for hyper-parameter optimization", "author": ["J. Bergstra", "R. Bardenet", "Y. Bengio", "B. Kegl"], "venue": "In Proc. of NIPS-11,", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "Stochastic derivative-free optimization using a trust region framework", "author": ["S.C. Billups", "J. Larson"], "venue": "J. Computational Optimization and Apps,", "citeRegEx": "Billups and Larson,? \\Q2016\\E", "shortCiteRegEx": "Billups and Larson", "year": 2016}, {"title": "Convergence rate analysis of a stochastic trust region method for nonconvex optimization", "author": ["J. Blanchet", "C. Cartis", "M. Menickelly", "K. Scheinberg"], "venue": null, "citeRegEx": "Blanchet et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blanchet et al\\.", "year": 2016}, {"title": "The use of the area under the ROC curve in the evaluation of machine learning algorithms", "author": ["A.P. Bradley"], "venue": "Pattern Recognition,", "citeRegEx": "Bradley,? \\Q1997\\E", "shortCiteRegEx": "Bradley", "year": 1997}, {"title": "AUC maximizing support vector learning", "author": ["U. Brefeld", "T. Scheffer"], "venue": null, "citeRegEx": "Brefeld and Scheffer,? \\Q2005\\E", "shortCiteRegEx": "Brefeld and Scheffer", "year": 2005}, {"title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. de Freitas"], "venue": null, "citeRegEx": "Brochu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "Efficient AUC optimization for classification", "author": ["T. Calders", "S. Jaroszewicz"], "venue": "Discovery in Databases, Warsaw,", "citeRegEx": "Calders and Jaroszewicz,? \\Q2007\\E", "shortCiteRegEx": "Calders and Jaroszewicz", "year": 2007}, {"title": "Stochastic optimization using a trust-region method and random models", "author": ["R. Chen", "M. Menickelly", "K. Scheinberg"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Recent progress in unconstrained nonlinear optimization without derivatives", "author": ["A.R. Conn", "K. Scheinberg", "Ph. L. Toint"], "venue": "Mathematical Programming,", "citeRegEx": "Conn et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Conn et al\\.", "year": 1997}, {"title": "Introduction To Derivative-Free Optimization", "author": ["A.R. Conn", "K. Scheinberg", "L.N. Vicente"], "venue": "Society for Industrial and Applied Mathematics. Philadelphia,", "citeRegEx": "Conn et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Conn et al\\.", "year": 2009}, {"title": "AUC optimization vs. error rate minimization", "author": ["C. Cortes", "M. Mohri"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Cortes and Mohri,? \\Q2004\\E", "shortCiteRegEx": "Cortes and Mohri", "year": 2004}, {"title": "Towards an empirical foundation for assessing bayesian optimization of hyperparameters", "author": ["K. Eggensperger", "M. Feurer", "F. Hutter", "J. Bergstra", "J. Snoek", "H.H. Hoos", "K.L. Brown"], "venue": "In NIPS Workshop on Bayesian Optimization in Theory and Practice,", "citeRegEx": "Eggensperger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Eggensperger et al\\.", "year": 2013}, {"title": "Learning decision trees using the area under the ROC curve", "author": ["C. Ferri", "P. Flach", "J. Hernandez-Orallo"], "venue": null, "citeRegEx": "Ferri et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ferri et al\\.", "year": 2002}, {"title": "The meaning and use of the area under a receiver operating characteristic (ROC) curve", "author": ["J.A. Hanley", "B.J. McNeil"], "venue": null, "citeRegEx": "Hanley and McNeil,? \\Q1982\\E", "shortCiteRegEx": "Hanley and McNeil", "year": 1982}, {"title": "Optimizing area under the ROC curve using gradient descent", "author": ["A. Herschtal", "B. Raskutti"], "venue": null, "citeRegEx": "Herschtal and Raskutti,? \\Q2004\\E", "shortCiteRegEx": "Herschtal and Raskutti", "year": 2004}, {"title": "Sequential model-based optimization for general algorithm configuration", "author": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "In Proc. of LION-5,", "citeRegEx": "Hutter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2011}, {"title": "Non-stochastic best arm identification and hyperparameter optimization", "author": ["Jamieson", "Kevin", "Talwalkar", "Ameet"], "venue": null, "citeRegEx": "Jamieson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2015}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "In ACM SIGKDD, pp", "citeRegEx": "Joachims,? \\Q2006\\E", "shortCiteRegEx": "Joachims", "year": 2006}, {"title": "Lipschitzian optimization without the lipschitz constant", "author": ["D.R. Jones", "C.D. Perttunen", "B.E. Stuckman"], "venue": "J. Optimization Theory and Apps,", "citeRegEx": "Jones et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Jones et al\\.", "year": 1993}, {"title": "Efficient global optimization of expensive black-box functions", "author": ["D.R. Jones", "M. Schonlau", "W.J. Welch"], "venue": "J. Global Optimization,", "citeRegEx": "Jones et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Jones et al\\.", "year": 1998}, {"title": "Hyperband: A novel bandit-based approach to hyperparameter optimization", "author": ["L. Li", "K. Jamieson", "G. DeSalvo", "A. Rostamizadeh", "A. Talwalkar"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Evolving neural networks with maximum AUC for imbalanced data classification", "author": ["X. Lu", "K. Tang", "X. Yao"], "venue": null, "citeRegEx": "Lu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2010}, {"title": "Global versus local search in constrained optimization of computer models", "author": ["M. Schonlau", "W.J. Welch", "D.R. Jones"], "venue": "In New Developments and Applications in Experimental Design,", "citeRegEx": "Schonlau et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Schonlau et al\\.", "year": 1998}, {"title": "On a test whether one of two random variables is stochastically larger than the other", "author": ["H.B. Mann", "D.R. Whitney"], "venue": "Ann. Math. Statist,", "citeRegEx": "Mann and Whitney,? \\Q1947\\E", "shortCiteRegEx": "Mann and Whitney", "year": 1947}, {"title": "Application of bayesian approach to numerical methods of global and stochastic optimization", "author": ["J. Mockus"], "venue": "J. Global Optimization,", "citeRegEx": "Mockus,? \\Q1994\\E", "shortCiteRegEx": "Mockus", "year": 1994}, {"title": "Benchmarking derivative-free optimization algorithms", "author": ["J.J. More", "S.M. Wild"], "venue": "SIAM J. Optim,", "citeRegEx": "More and Wild,? \\Q2009\\E", "shortCiteRegEx": "More and Wild", "year": 2009}, {"title": "Least Frobenius norm updating of quadratic models that satisfy interpolation conditions", "author": ["M.J.D. Powell"], "venue": "Mathematical Programming,", "citeRegEx": "Powell,? \\Q2004\\E", "shortCiteRegEx": "Powell", "year": 2004}, {"title": "Margin-based ranking and an equivalence between adaboost and rankboost", "author": ["C. Rudin", "R.E. Schapire"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rudin and Schapire,? \\Q2009\\E", "shortCiteRegEx": "Rudin and Schapire", "year": 2009}, {"title": "Self-correcting geometry in model-based algorithms for derivative-free unconstrained optimization", "author": ["K. Scheinberg", "Toint", "Ph. L"], "venue": "SIAM J. Optim,", "citeRegEx": "Scheinberg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Scheinberg et al\\.", "year": 2010}, {"title": "ASTRODF: A class of adaptive sampling trust-region algorithms for derivative-free simulation optimization", "author": ["S. Shashaani", "F.S. Hashemi", "R. Pasupathy"], "venue": null, "citeRegEx": "Shashaani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shashaani et al\\.", "year": 2015}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "In Proc. of NIPS-12,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Hinge rank loss and the area under the ROC curve", "author": ["H. Steck"], "venue": "In ECML, Lecture Notes in Computer Science,", "citeRegEx": "Steck,? \\Q2007\\E", "shortCiteRegEx": "Steck", "year": 2007}, {"title": "The multivariate normal distribution", "author": ["Y.L. Tong"], "venue": "Springer Series in Statistics,", "citeRegEx": "Tong,? \\Q1990\\E", "shortCiteRegEx": "Tong", "year": 1990}, {"title": "Optimizing classifier performance via approximation to the wilcoxon-mann-witney statistic", "author": ["L. Yan", "R. Dodier", "M.C. Mozer", "R. Wolniewicz"], "venue": "Proceedings of the Twentieth Intl. Conf. on Machine Learning,", "citeRegEx": "Yan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2003}, {"title": "Online AUC maximization", "author": ["P. Zhao", "S.C.H. Hoi", "R. Jin", "T. Yang"], "venue": "In Proceedings of the 28th ICML,", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 24, "context": "There are two relatively independent directions of research for black-box optimization\u2013Bayesian Optimization (BO) (Mockus, 1994; Brochu et al., 2010), predominantly popular in the ML community, and derivative free optimization (DFO) (Conn et al.", "startOffset": 114, "endOffset": 149}, {"referenceID": 5, "context": "There are two relatively independent directions of research for black-box optimization\u2013Bayesian Optimization (BO) (Mockus, 1994; Brochu et al., 2010), predominantly popular in the ML community, and derivative free optimization (DFO) (Conn et al.", "startOffset": 114, "endOffset": 149}, {"referenceID": 9, "context": ", 2010), predominantly popular in the ML community, and derivative free optimization (DFO) (Conn et al., 2009)\u2013popular in the optimization community.", "startOffset": 91, "endOffset": 110}, {"referenceID": 20, "context": "Recently, the BO efficiency has been called into question in comparison with a simple random search (Li et al., 2016), whose iterations require nothing, but function evaluations.", "startOffset": 100, "endOffset": 117}, {"referenceID": 3, "context": "Various results has been reported in terms of comparing the AUC value as a performance measure of a classifier versus the usual prediction accuracy, that is the total percentage of misclassified examples (Bradley, 1997; Ferri et al., 2002; Brefeld & Scheffer, 2005; Lu et al., 2010).", "startOffset": 204, "endOffset": 282}, {"referenceID": 12, "context": "Various results has been reported in terms of comparing the AUC value as a performance measure of a classifier versus the usual prediction accuracy, that is the total percentage of misclassified examples (Bradley, 1997; Ferri et al., 2002; Brefeld & Scheffer, 2005; Lu et al., 2010).", "startOffset": 204, "endOffset": 282}, {"referenceID": 21, "context": "Various results has been reported in terms of comparing the AUC value as a performance measure of a classifier versus the usual prediction accuracy, that is the total percentage of misclassified examples (Bradley, 1997; Ferri et al., 2002; Brefeld & Scheffer, 2005; Lu et al., 2010).", "startOffset": 204, "endOffset": 282}, {"referenceID": 3, "context": "In particular, in (Bradley, 1997), AUC is used to evaluate the performance of some ML algorithms such as decision trees, neural networks, and some statistical methods, where, experimentally, it is shown that AUC has advantages over the accuracy.", "startOffset": 18, "endOffset": 33}, {"referenceID": 33, "context": "In (Yan et al., 2003; Herschtal & Raskutti, 2004; Calders & Jaroszewicz, 2007), various smooth nonconvex approximation of AUC has been optimized by the gradient descent method.", "startOffset": 3, "endOffset": 78}, {"referenceID": 17, "context": "Alternatively, a ranking loss, which is defined as 1\u2212AUC value, is minimized approximately, by replacing it with the pairwise margin loss, such as exponential loss, logistic loss, and hinge loss (Joachims, 2006; Steck, 2007; Rudin & Schapire, 2009; Zhao et al., 2011), which results in a convex problem.", "startOffset": 195, "endOffset": 267}, {"referenceID": 31, "context": "Alternatively, a ranking loss, which is defined as 1\u2212AUC value, is minimized approximately, by replacing it with the pairwise margin loss, such as exponential loss, logistic loss, and hinge loss (Joachims, 2006; Steck, 2007; Rudin & Schapire, 2009; Zhao et al., 2011), which results in a convex problem.", "startOffset": 195, "endOffset": 267}, {"referenceID": 34, "context": "Alternatively, a ranking loss, which is defined as 1\u2212AUC value, is minimized approximately, by replacing it with the pairwise margin loss, such as exponential loss, logistic loss, and hinge loss (Joachims, 2006; Steck, 2007; Rudin & Schapire, 2009; Zhao et al., 2011), which results in a convex problem.", "startOffset": 195, "endOffset": 267}, {"referenceID": 9, "context": "In this work, we apply a variant of a model-based trust region derivative free method, called DFO-TR, (Conn et al., 2009) to directly maximize the AUC function over a set of linear classifiers, without using any hyperparameters.", "startOffset": 102, "endOffset": 121}, {"referenceID": 24, "context": "In terms of required number of objective function evaluations, Bayesian optimization methods are considered to be some of the most efficient techniques (Mockus, 1994; Jones et al., 1998; Brochu et al., 2010) for black-box problems of low effective dimensionality.", "startOffset": 152, "endOffset": 207}, {"referenceID": 19, "context": "In terms of required number of objective function evaluations, Bayesian optimization methods are considered to be some of the most efficient techniques (Mockus, 1994; Jones et al., 1998; Brochu et al., 2010) for black-box problems of low effective dimensionality.", "startOffset": 152, "endOffset": 207}, {"referenceID": 5, "context": "In terms of required number of objective function evaluations, Bayesian optimization methods are considered to be some of the most efficient techniques (Mockus, 1994; Jones et al., 1998; Brochu et al., 2010) for black-box problems of low effective dimensionality.", "startOffset": 152, "endOffset": 207}, {"referenceID": 5, "context": "In theory, Bayesian optimizationmethods seek global optimal solution, due to their sampling schemes, which trade-off between exploitation and exploration (Brochu et al., 2010; Eggensperger et al., 2013).", "startOffset": 154, "endOffset": 202}, {"referenceID": 11, "context": "In theory, Bayesian optimizationmethods seek global optimal solution, due to their sampling schemes, which trade-off between exploitation and exploration (Brochu et al., 2010; Eggensperger et al., 2013).", "startOffset": 154, "endOffset": 202}, {"referenceID": 5, "context": "Then, by using this model, the subsequent configurations of the parameters will be selected (Brochu et al., 2010) by optimizing an acquisition function derived from the model.", "startOffset": 92, "endOffset": 113}, {"referenceID": 15, "context": "We compare DFO-TR with SMAC (Hutter et al., 2011), SPEARMINT (Snoek et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 30, "context": ", 2011), SPEARMINT (Snoek et al., 2012), and TPE (Bergstra et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 0, "context": ", 2012), and TPE (Bergstra et al., 2011), which are popular Bayesian optimization algorithms based on different types of model.", "startOffset": 17, "endOffset": 40}, {"referenceID": 7, "context": "We also discuss the convergence properties of DFO-TR and its stochastic variant (Chen et al., 2015), and argue that these results apply to optimizing expected AUC value, when it is a smooth function.", "startOffset": 80, "endOffset": 99}, {"referenceID": 8, "context": "Model-based trust region DFO methods (Conn et al., 1997; Powell, 2004) have been proposed for a class of optimization problems of the formminw\u2208Rd f(w), when computing the gradient and the Hessian of f(w) is not possible, either because it is unknown or because the computable gradient is too noisy to be of use.", "startOffset": 37, "endOffset": 70}, {"referenceID": 26, "context": "Model-based trust region DFO methods (Conn et al., 1997; Powell, 2004) have been proposed for a class of optimization problems of the formminw\u2208Rd f(w), when computing the gradient and the Hessian of f(w) is not possible, either because it is unknown or because the computable gradient is too noisy to be of use.", "startOffset": 37, "endOffset": 70}, {"referenceID": 9, "context": "If the function is smooth, then such information is usually constructed by building an interpolation or regression model of f(w) using a set of points for which function value is (approximately known) (Conn et al., 2009).", "startOffset": 201, "endOffset": 220}, {"referenceID": 9, "context": "Extensive convergence analysis of these methods over smooth deterministic functions have been summarized in (Conn et al., 2009).", "startOffset": 108, "endOffset": 127}, {"referenceID": 29, "context": "Recently, several variants of trust region methods have been proposed to solve stochastic optimization problem minw E\u03be[f(w, \u03be)], where f(w, \u03be) is a stochastic function of a deterministic vector w \u2208 R and a random variable \u03be (Shashaani et al., 2015; Billups & Larson, 2016; Chen et al., 2015).", "startOffset": 224, "endOffset": 291}, {"referenceID": 7, "context": "Recently, several variants of trust region methods have been proposed to solve stochastic optimization problem minw E\u03be[f(w, \u03be)], where f(w, \u03be) is a stochastic function of a deterministic vector w \u2208 R and a random variable \u03be (Shashaani et al., 2015; Billups & Larson, 2016; Chen et al., 2015).", "startOffset": 224, "endOffset": 291}, {"referenceID": 7, "context": "In particular, in (Chen et al., 2015), a trust region based stochastic method, referred to STORM (STochastic Optimization with Random Models), is introduced and shown to converge, almost surely, to a stationary point ofE\u03be[f(w, \u03be)], under the assumption thatE\u03be[f(w, \u03be)] is smooth.", "startOffset": 18, "endOffset": 37}, {"referenceID": 2, "context": "Moreover, in recent work (Blanchet et al., 2016) a convergence rate of this method has been analyzed.", "startOffset": 25, "endOffset": 48}, {"referenceID": 7, "context": "For general convergent framework of STORM, we refer the reader to (Chen et al., 2015).", "startOffset": 66, "endOffset": 85}, {"referenceID": 9, "context": "When applied to deterministic smooth functions, this algorithm converges to a local solution (Conn et al., 2009), but here we apply it to a nonsooth function which can be viewed as a noisy version of a smooth function (as argued in the next section).", "startOffset": 93, "endOffset": 112}, {"referenceID": 32, "context": "Proof 1 The proof can be found in (Tong, 1990).", "startOffset": 34, "endOffset": 46}, {"referenceID": 32, "context": "Proof 2 The proof can be found in (Tong, 1990).", "startOffset": 34, "endOffset": 46}, {"referenceID": 5, "context": "For example, the algorithm in (Brochu et al., 2010) uses deterministic derivative free optimizer DIRECT (Jones et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 18, "context": ", 2010) uses deterministic derivative free optimizer DIRECT (Jones et al., 1993) to maximize the acquisition function.", "startOffset": 60, "endOffset": 80}, {"referenceID": 7, "context": "In order to further improve efficiency of DFO-TR, we observe that STORM framework and theory (Chen et al., 2015) suggests that noisy function evaluations do not need to be accurate far away from the optimal solution.", "startOffset": 93, "endOffset": 112}], "year": 2017, "abstractText": "In this work, we utilize a Trust Region based Derivative Free Optimization (DFO-TR) method to directly maximize the Area Under Receiver Operating Characteristic Curve (AUC), which is a nonsmooth, noisy function. We show that AUC is a smooth function, in expectation, if the distributions of the positive and negative data points obey a jointly normal distribution. The practical performance of this algorithm is compared to three prominent Bayesian optimization methods and random search. The presented numerical results show that DFO-TR surpasses Bayesian optimization and random search on various blackbox optimization problem, such as maximizing AUC and hyperparameter tuning.", "creator": "LaTeX with hyperref package"}}}