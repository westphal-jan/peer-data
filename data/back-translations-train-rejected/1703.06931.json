{"id": "1703.06931", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "Learning Correspondence Structures for Person Re-identification", "abstract": "This paper addresses the problem of handling spatial misalignments due to camera-view changes or human-pose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or human-pose variation in individual images. We further introduce a global constraint-based matching process. It integrates a global matching constraint over the learned correspondence structure to exclude cross-view misalignments during the image patch matching process, hence achieving a more reliable matching score between images. Finally, we also extend our approach by introducing a multi-structure scheme, which learns a set of local correspondence structures to capture the spatial correspondence sub-patterns between a camera pair, so as to handle the spatial misalignments between individual images in a more precise way. Experimental results on various datasets demonstrate the effectiveness of our approach.", "histories": [["v1", "Mon, 20 Mar 2017 19:17:14 GMT  (9268kb,D)", "https://arxiv.org/abs/1703.06931v1", "accepted by IEEE Trans. Image Processing. arXiv admin note: text overlap witharXiv:1504.06243"], ["v2", "Wed, 26 Apr 2017 12:31:28 GMT  (9268kb,D)", "http://arxiv.org/abs/1703.06931v2", "IEEE Trans. Image Processing, vol. 26, no. 5, pp. 2438-2453, 2017. The project page for this paper is available atthis http URLarXiv admin note: text overlap witharXiv:1504.06243"], ["v3", "Thu, 27 Apr 2017 16:15:30 GMT  (9268kb,D)", "http://arxiv.org/abs/1703.06931v3", "IEEE Trans. Image Processing, vol. 26, no. 5, pp. 2438-2453, 2017. The project page for this paper is available atthis http URLarXiv admin note: text overlap witharXiv:1504.06243"]], "COMMENTS": "accepted by IEEE Trans. Image Processing. arXiv admin note: text overlap witharXiv:1504.06243", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.MM", "authors": ["weiyao lin", "yang shen", "junchi yan", "mingliang xu", "jianxin wu", "jingdong wang", "ke lu"], "accepted": false, "id": "1703.06931"}, "pdf": {"name": "1703.06931.pdf", "metadata": {"source": "CRF", "title": "Learning Correspondence Structures for Person Re-identification", "authors": ["Weiyao Lin", "Yang Shen", "Junchi Yan", "Mingliang Xu", "Jianxin Wu", "Jingdong Wang", "Ke Lu"], "emails": ["shenyang1715}@sjtu.edu.cn).", "jcyan@sei.ecnu.edu.cn).", "iexumingliang@zzu.edu.cn)", "wujx2001@nju.edu.cn).", "jingdw@microsoft.com).", "luk@ucas.ac.cn)."], "sections": [{"heading": null, "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "II. RELATED WORKS", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "III. OVERVIEW OF THE APPROACH", "text": "The framework of our approach is illustrated in Fig. 2. During the training process, which is described in detail in Section V, we present a boosting-based process to learn the correspondence structure between the target camera pair. During the prediction phase, which is described in detail in Section IV, we use the correspondence structure to evaluate the correlations between the probe image and each gallery image and to find the optimal one-to-one mapping between the fields and according to the matching score. ReID results are achieved by evaluating gallery images according to their matching scores."}, {"heading": "IV. PERSON RE-ID VIA CORRESPONDENCE STRUCTURE", "text": "This section introduces the concept of the correspondence structure, shows the scheme of calculating the correlation of patches based on the correspondence structure, and finally presents the method of patchwise mapping to calculate the appropriate score between the probe image and a gallery image."}, {"heading": "A. Correspondence structure", "text": "The correspondence structure, \u0435A, B, encodes the spatial correspondence distribution between a camera pair, A and B. In our problem, we assume a discrete distribution, which is a series of patchwise matching probabilities, \u0435A, B = {Pi, B} NAi = 1, where NA is the number of patches of an image in camera A. Pi, B = {Pi1, Pi2,...., PiNB} describes the correspondence distribution in an image of camera B for the ith patch xi of an image taken by camera A, where NB is the number of patches of an image in B. The images of the correspondence distribution are in the upper right corner of Fig. 2.The definition of the correspondence probabilities in the correspondence structure depends only on one camera pair and are independent of specific images. In the correspondence structure, it is possible that a patch in camera A is strongly correlated to several patches in camera B in order to deal with human variations and local visual changes."}, {"heading": "B. Patch correlation", "text": "In view of a probe image U in camera A and a gallery image V in camera B, the patch-wise correlation between U and V, C (xi, yj) (xi, yj) (xi, yj, yj) is calculated both from the correspondence structure between two cameras and from the visual characteristics: C (xi, yj) = \u03bbTc (Pij) \u00b7 log \u03a6 (fxi, fyj; xi, yj). (1) Here are xi and yj ith and jth patches in images U and V; fxi and fyj are the characteristic vectors for xi and yj, and Pij is the correspondence structure between U and V. (Pij) \u03bbTc (Pij) = 1 if Pij > Tc, and 0 otherwise, and Tc = 0.05 is a threshold. (fxi, fyj; yj; yj) is the correspondence structure controlled similarity between xi and yj [kxi]."}, {"heading": "C. Patch-wise mapping", "text": "After reaching the correlation strength C (xi, yj) improved by the alignment, we can find a best-matched patch in image V for each patch in U and calculate the final image-matching score from it. To calculate C (xi, yj) of the tested image pairs < Td}, we only look at the potential matching patches within a search range R (xi) in order to calculate the selected patch result. That is, R (xi) = {yj | d (yi, yj) < Td}, where yi's co-located4 (a) (b) Figure 3. Patch-matching result (a) by locating the greatest correlation strength C (xi, yj) for each patch and (b) by using a global constraint. The red dashed lines point to the final patch-matching results and the coloured continuous lines that represent the matching probabilities in the corresponding scale in the colour-yale-scale-best-scale-scale-scale-best-scale-scale-scale-structure."}, {"heading": "V. CORRESPONDENCE STRUCTURE LEARNING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Objective function", "text": "In view of a series of test images {U\u03b1} of camera A and their corresponding cross-sectional images {V\u03b2} of camera B in the training set, we learn the optimal correspondence between cameras A and B, so that the correct image of correspondence in terms of correspondence values precedes the wrong image of correspondence. The formulation reads: min-A, B-U\u03b1 R (V\u03b1, V\u03b1) (as calculated from Eq.4) is the image of correspondence between U\u03b1 and V\u03b1 \"(V\u03b2 A, B), (5) in which V\u03b2\" is the correct image of correspondence of the test image U\u03b1."}, {"heading": "B. Boosting-based learning", "text": "A binary mapping structure is similar to the correspondence structure, except that it simply uses 0 or 1 instead of matching probabilities to indicate the connectivity or linkage between patches, cf. 4a. It can be regarded as a simplified version of the correspondence structure, containing rough information about the correspondence structure of cross views. Since binary mapping structures only contain simple connectivity information between patches, their optimal solutions for individual test images are traceable. Therefore, by searching for the optimal binary mapping structure for different test images and using it, we can gradually update the correspondence structure, appropriate cross-view correspondence patterns can be achieved. The entire boosting-based learning process can be described in the following steps, which are summarized in Algorithm 1.Finding the optimal binary mapping structure."}, {"heading": "VI. EXTENDING THE APPROACH WITH MULTIPLE CORRESPONDENCE STRUCTURES", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution."}, {"heading": "VII. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Datasets and Experimental Settings", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "B. Results of using a single correspondence structure", "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own if they are not able to survive on their own."}, {"heading": "C. Results of using multiple correspondence structures", "text": "In this year it is so far that it will be able to put itself at the forefront, namely in the way in which it was the case in the past: in the way in which it was in the past, in the time in which it was to do, in the time in which it was to do, in the time in which it was to do, in the time in which it was to do, in the time in which it was to do, in the time in which it was to do, in the time in which it was to do, in the time in which it is to do, in the time in which it is to do, in the time in which it is to do, in the time in which it is to do, in the time in which it is to do, in the time in which it is to do, in the time in which it is to do, in the time in which it is to do, in the time in the case in which it is to do, in the case in the case in the case in which it is to do, in the time in the case in the case in which it is to do, in the time in the case in the case in the case in which it is to do it is to do, in the time in the case in the case in the case in which it is to do it is to do, in the case in the case in the case in the case in the case in the case in which it is to do it is to do it, in the case in the case in the case in the case in the case in the case in the case in the case in the case in which it is to do it is to do it is to do it is to do it is to do it, in the case in the case in the case in the case in the case in the case in the case in the case in the case in the case in the case in which it is to do it is to do it is to do it is to do it is to do it is to do it, in the case in the case in the case in the case in the case in the case in the case in the case in the case in the case in the case in the case of do it is to do it is to do it is to do it is to do it is to do it is to do it, in the it is to do it, in the case in the case in the"}, {"heading": "D. Comparing with the state-of-art methods", "text": "We compare our positions with those of the other parties."}, {"heading": "E. Time complexity & Effects of different parameter values", "text": "As a matter of fact, most of them are able to move without being able to move, most of them are able to move, most of them are able to move, most of them are able to move, most of them are able to move, most of them are able to move, most of them are able to move, most of them are able to move, most of them are able to move, most of them are able to move, most of them are able to move."}, {"heading": "VIII. CONCLUSION", "text": "In this paper, we propose a novel framework for solving the problem of spatial misalignment in person Re-ID. Our framework consists of three key components: 1) introducing the idea of correspondence structure and learning this structure through a novel crank method to adapt to arbitrary camera configurations; 2) a limited global matching step to control patch misalignment between images due to local ambiguity; 3) a multi-structure strategy for more precise handling of spatial misalignments. Extensive experimental benchmark results show that our approach is state-of-the-art."}], "references": [{"title": "Person reidentification with correspondence structure learning", "author": ["Y. Shen", "W. Lin", "J. Yan", "M. Xu", "J. Wu", "J. Wang"], "venue": "IEEE Intl. Conf. Comp. Vision, 2015, pp. 3200\u20133208.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Salient color names for person re-identification", "author": ["Y. Yang", "J. Yang", "J. Yan", "S. Liao", "D. Yi", "S.Z. Li"], "venue": "Eur. Conf. Comp. Vision, 2014, pp. 536\u2013551.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Person re-identification using semantic color names and rankboost", "author": ["C.-H. Kuo", "S. Khamis", "V. Shet"], "venue": "IEEE Winter Conf. Application of Computer Vision, 2013, pp. 281\u2013287.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning invariant color features for person re-identification", "author": ["R. Varior", "G. Wang", "J. Lu", "T. Liu"], "venue": "IEEE Trans. Image Process., 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Viewpoint invariant pedestrian recognition with an ensemble of localized features", "author": ["D. Gray", "H. Tao"], "venue": "Eur. Conf. Comp. Vision, 2008, pp. 262\u2013275.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Person re-identification by efficient impostor-based metric learning", "author": ["M. Hirzer", "P.M. Roth", "H. Bischof"], "venue": "IEEE Conf. Advanced Video and Signal-based Surveillance, 2012, pp. 203\u2013208.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Large scale metric learning from equivalence constraints", "author": ["M. Kostinger", "M. Hirzer", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "IEEE Comp. Vision and Patt. Recog., 2012, pp. 2288\u20132295.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Person re-identification using kernel-based metric learning methods", "author": ["F. Xiong", "M. Gou", "O. Camps", "M. Sznaier"], "venue": "Eur. Conf. Comp. Vision, 2014, pp. 1\u201316.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Mahalanobis distance learning for person re-identification", "author": ["P.M. Roth", "M. Hirzer", "M. K\u00f6stinger", "C. Beleznai", "H. Bischof"], "venue": "Person Re-Identification. Springer, 2014, pp. 247\u2013267.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalable person re-identification: A benchmark", "author": ["L. Zheng", "L.-Y. Sheng", "L. Tian", "S.-J. Wang", "J.-D. Wang", "Q. Tian"], "venue": "IEEE Intl. Conf. Comp. Vision, 2015, pp. 1116\u20131124.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Similarity learning on an explicit polynomial kernel feature map for person re-identification", "author": ["D.-P. Chen", "Z.-J. Yuan", "G. Hua", "N.-N. Zheng", "J.-D. Wang"], "venue": "IEEE Comp. Vision and Patt. Recog., 2015, pp. 1565\u20131573.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Human identity recognition in aerial images", "author": ["O. Oreifej", "R. Mehran", "M. Shah"], "venue": "IEEE Comp. Vision and Patt. Recog., 2010, pp. 709\u2013 716.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "A generalized emd with body prior for pedestrian identification", "author": ["L. Ma", "X. Yang", "Y. Xu", "J. Zhu"], "venue": "Journal of Visual Communication and Image Representation, vol. 24, no. 6, pp. 708\u2013716, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised salience learning for person re-identification", "author": ["R. Zhao", "W. Ouyang", "X. Wang"], "venue": "IEEE Comp. Vision and Patt. Recog., 2013, pp. 3586\u20133593.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Dense invariant feature based support vector ranking for cross-camera person re-identification", "author": ["S. Tan", "F. Zheng", "L. Liu", "J. Han", "L. Shao"], "venue": "IEEE Trans. Circuits Syst. Video Technol., pp. 687\u2013691, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "On-the-fly feature importance mining for person re-identification", "author": ["C. Liu", "S. Gong", "C.C. Loy"], "venue": "Patt. Recog., vol. 47, no. 4, pp. 1602\u20131615, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Person re-identification over camera networks using multi-task distance metric learning", "author": ["L. Ma", "X. Yang", "D. Tao"], "venue": "IEEE Trans. Image Process., vol. 23, no. 8, pp. 3656\u20133670, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Relevance metric learning for person re-identification by exploiting listwise similarities", "author": ["J. Chen", "Z. Zhang", "Y. Wang"], "venue": "IEEE Trans. Image Process., vol. 24, no. 12, pp. 4741\u20134755, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a discriminative null space for person re-identification", "author": ["L. Zhang", "T. Xiang", "S. Gong"], "venue": "IEEE Comp. Vision and Patt. Recog., 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint learning of single-image and cross-image representations for person reidentification", "author": ["F.-Q. Wang", "W.-M. Zuo", "L. Lin", "D. Zhang", "L. Zhang"], "venue": "IEEE Comp. Vision and Patt. Recog., 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Bit-scalable deep hashing with regularized similarity learning for image retrieval and person re-identification", "author": ["R. Zhang", "L. Lin", "R. Zhang", "W. Zuo", "L. Zhang"], "venue": "IEEE Trans. Image Process., vol. 24, no. 12, pp. 4766\u20134779, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep ranking for person reidentification via joint representation learning", "author": ["S.-Z. Chen", "C.-C. Guo", "J.-H. Lai"], "venue": "IEEE Trans. Image Process., 2016.  16", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "An improved deep learning architecture for person re-identification", "author": ["E. Ahmed", "M. Jones", "T.-K. Marks"], "venue": "IEEE Comp. Vision and Patt. Recog., 2015, pp. 3908\u20133916.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep feature learning with relative distance comparison for person re-identification", "author": ["S.-Y. Ding", "L. Lin", "G.-R. Wang", "H.-Y. Chao"], "venue": "Patt. Recog., vol. 48, no. 10, pp. 2993\u20133003, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Person reidentification by multi-channel parts-based cnn with improved triplet loss function", "author": ["D. Cheng", "Y. Gong", "S. Zhou", "J. Wang", "N. Zheng"], "venue": "IEEE Comp. Vision and Patt. Recog., 2016, pp. 1335\u2013 1344.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep feature representations with domain guided dropout for person re-identification", "author": ["T. Xiao", "H. Li", "W. Ouyang", "X. Wang"], "venue": "IEEE Comp. Vision and Patt. Recog., 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "An enhanced deep feature representation for person re-identification", "author": ["S. Wu", "Y.-C. Chen", "X. Li", "A.-C. Wu", "J.-J. You", "W.-S. Zheng"], "venue": "IEEE Winter Conf. App. of Comp. Vision, 2016, pp. 1\u20138.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Human re-identification by matching compositional template with cluster sampling", "author": ["Y. Xu", "L. Lin", "W.-S. Zheng", "X. Liu"], "venue": "IEEE Intl. Conf. Comp. Vision, 2013, pp. 3152\u20133159.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Person re-identification by salience learning", "author": ["R. Zhao", "W. Ouyang", "X. Wang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Custom pictorial structures for re-identification", "author": ["D. Cheng", "M. Cristani", "M. Stoppa", "L. Bazzani", "V. Murino"], "venue": "British Machive Vision Conference, 2011, pp. 1\u20136.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised learning of generative topic saliency for person re-identification", "author": ["H. Wang", "S. Gong", "T. Xiang"], "venue": "British Machive Vision Conference, 2014, pp. 1\u201311.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Person reidentification via learning visual similarity on corresponding patch pairs", "author": ["H. Sheng", "Y. Huang", "Y. Zheng", "J. Chen", "Z. Xiong"], "venue": "Intl. Conf. Knowledge Science, Engineering and Management, 2015, pp. 787\u2013798.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Person re-identification using deformable patch metric learning", "author": ["S. Bak", "P. Carr"], "venue": "IEEE Winter Conf. App. of Comp. Vision, 2016, pp. 1\u20139.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Improving person reidentification by viewpoint cues", "author": ["S. Bak", "S. Zaidenberg", "B. Boulay"], "venue": "IEEE Conf. Advanced Video and Signal Based Surveillance, 2014, pp. 175\u2013180.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantics-aware deep correspondence structure learning for robust person re-identification", "author": ["Y. Zhang", "X. Li", "L. Zhao", "Z. Zhang"], "venue": "Intl. Joint Conf. Artificial Intelligence, 2016, pp. 3545\u20133551.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "A novel visual word cooccurrence model for person re-identification", "author": ["Z.-M. Zhang", "Y.-T. Chen", "V. Saligrama"], "venue": "Eur. Conf. Comp. Vision Workshops, 2014, pp. 122\u2013133.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Prism: Person re-identification via structured matching", "author": ["Z.-M. Zhang", "V. Saligrama"], "venue": "IEEE Trans. Circuits and Systems for Video Technology, pp. 1\u201313, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "The generalized patchmatch correspondence algorithm", "author": ["C. Barnes", "E. Shechtman", "D.B. Goldman", "A. Finkelstein"], "venue": "Eur. Conf. Comp. Vision, 2010, pp. 29\u201343.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Mirror representation for modeling view-specific transform in person re-identification", "author": ["Y.-C. Chen", "W.-S. Zheng", "J. Lai"], "venue": "Intl. Joint Conf. Artificial Intelligence, 2015, pp. 3402\u20133408.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "The hungarian method for the assignment problem", "author": ["H.W. Kuhn"], "venue": "Naval Research Logistics Quarterly, vol. 2, no. 1-2, pp. 83\u201397, 1955.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1955}, {"title": "Evaluating appearance models for recognition, reacquisition, and tracking", "author": ["D. Gray", "S. Brennan", "H. Tao"], "venue": "Performance Evaluation of Tracking and Surveillance (PETS), vol. 3, no. 5, 2007.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2007}, {"title": "Shape and appearance context modeling", "author": ["X. Wang", "G. Doretto", "T. Sebastian", "J. Rittscher", "P. Tu"], "venue": "IEEE Intl. Conf. Comp. Vision, 2007, pp. 1\u20138.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2007}, {"title": "3dpes: 3d people dataset for surveillance and forensics", "author": ["D. Baltieri", "R. Vezzani", "R. Cucchiara"], "venue": "ACM workshop Human gesture and behavior understanding, 2011, pp. 59\u201364.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Person re-identification with multi-level adaptive correspondence models", "author": ["S.-C. Shia", "C.-C. Guoa", "J.-H. Lai", "S.-Z. Chen", "X.-J. Hua"], "venue": "Neurocomputing, vol. 168, pp. 550\u2013559, 2015.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Pedestrian parsing via deep decompositional network", "author": ["L. Ping", "X.-G. Wang", "X.-O. Tang"], "venue": "IEEE Intl. Conf. Comp. Vision, 2013, pp. 2648\u20132655.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "Libsvm: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Trans. Intell. Syst. Tech., vol. 2, pp. 1\u201327, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning descriptors for object recognition and 3d pose estimation", "author": ["P. Wohlhart", "V. Lepetit"], "venue": "IEEE Comp. Vision and Patt. Recog., 2015, pp. 3041\u20133048.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Human pose estimation using body parts dependent joint regressors", "author": ["M. Dantone", "J. Gall", "C. Leistner", "L.-V. Gool"], "venue": "IEEE Comp. Vision and Patt. Recog., 2013.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2013}, {"title": "Inferring user image search goals under the implicit guidance of users", "author": ["Z. Lu", "X. Yang", "W. Lin", "H. Zha", "X. Chen"], "venue": "IEEE Trans. Circuits and Systems for Video Technology, vol. 24, no. 3, pp. 394\u2013406, 2014.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning locally-adaptive decision functions for person verification", "author": ["Z. Li", "S. Chang", "F. Liang", "T.S. Huang", "L. Cao", "J.R. Smith"], "venue": "IEEE Comp. Vision and Patt. Recog., 2013, pp. 3610\u20133617.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "PCCA: A new approach for distance learning from sparse pairwise constraints", "author": ["A. Mignon", "F. Jurie"], "venue": "IEEE Comp. Vision and Patt. Recog., 2012, pp. 2666\u20132672.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "The dynamic hungarian algorithm for the assignment problem with changing costs", "author": ["G.A. Mills-Tettey", "A. Stentz", "M.B. Dias"], "venue": "Carnegie Mellon University, 2007.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 1, "context": ", cameras that are non-overlapping and different from the probe image\u2019s camera) [2], [3], [4].", "startOffset": 80, "endOffset": 83}, {"referenceID": 2, "context": ", cameras that are non-overlapping and different from the probe image\u2019s camera) [2], [3], [4].", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": ", cameras that are non-overlapping and different from the probe image\u2019s camera) [2], [3], [4].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "The basic idea of this paper appeared in our conference version [1].", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "However, most existing works [2], [3], [4], [5], [6], [7],", "startOffset": 29, "endOffset": 32}, {"referenceID": 2, "context": "However, most existing works [2], [3], [4], [5], [6], [7],", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "However, most existing works [2], [3], [4], [5], [6], [7],", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "However, most existing works [2], [3], [4], [5], [6], [7],", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "However, most existing works [2], [3], [4], [5], [6], [7],", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "However, most existing works [2], [3], [4], [5], [6], [7],", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "[8], [9], [10], [11] focus on handling the overall appearance variations between images, while the spatial misalignment among images\u2019 local patches is not addressed.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[8], [9], [10], [11] focus on handling the overall appearance variations between images, while the spatial misalignment among images\u2019 local patches is not addressed.", "startOffset": 5, "endOffset": 8}, {"referenceID": 9, "context": "[8], [9], [10], [11] focus on handling the overall appearance variations between images, while the spatial misalignment among images\u2019 local patches is not addressed.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "[8], [9], [10], [11] focus on handling the overall appearance variations between images, while the spatial misalignment among images\u2019 local patches is not addressed.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "Although some patch-based methods [12], [13], [14], [15] address the spatial misalignment problem by decomposing images into patches", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "Although some patch-based methods [12], [13], [14], [15] address the spatial misalignment problem by decomposing images into patches", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "Although some patch-based methods [12], [13], [14], [15] address the spatial misalignment problem by decomposing images into patches", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "Although some patch-based methods [12], [13], [14], [15] address the spatial misalignment problem by decomposing images into patches", "startOffset": 52, "endOffset": 56}, {"referenceID": 1, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 91, "endOffset": 94}, {"referenceID": 2, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 106, "endOffset": 109}, {"referenceID": 15, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 5, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 206, "endOffset": 209}, {"referenceID": 6, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 211, "endOffset": 214}, {"referenceID": 7, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 216, "endOffset": 219}, {"referenceID": 8, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 221, "endOffset": 224}, {"referenceID": 16, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 226, "endOffset": 230}, {"referenceID": 17, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 232, "endOffset": 236}, {"referenceID": 18, "context": "Most of them focus on developing suitable feature representations about humans\u2019 appearance [2], [3], [4], [5], [16], or finding proper metrics to measure the cross-view appearance similarity between images [6], [7], [8], [9], [17], [18], [19].", "startOffset": 238, "endOffset": 242}, {"referenceID": 19, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 79, "endOffset": 83}, {"referenceID": 23, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 85, "endOffset": 89}, {"referenceID": 24, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 91, "endOffset": 95}, {"referenceID": 25, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 97, "endOffset": 101}, {"referenceID": 26, "context": "reliability of feature representations or similarity metrics [20], [21], [22], [23], [24], [25], [26], [27].", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 104, "endOffset": 108}, {"referenceID": 12, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 116, "endOffset": 120}, {"referenceID": 28, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 122, "endOffset": 126}, {"referenceID": 29, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 128, "endOffset": 132}, {"referenceID": 30, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 134, "endOffset": 138}, {"referenceID": 31, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 140, "endOffset": 144}, {"referenceID": 32, "context": "In order to address the spatial misalignment problem, some patch-based methods are proposed [28], [12], [15], [13], [14], [29], [30], [31], [32], [33] which decompose images into patches and perform an online patch-level matching to exclude patch-wise misalignments.", "startOffset": 146, "endOffset": 150}, {"referenceID": 27, "context": "In [28], [30], [32], a human body in an image is first parsed into semantic parts (e.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "In [28], [30], [32], a human body in an image is first parsed into semantic parts (e.", "startOffset": 9, "endOffset": 13}, {"referenceID": 31, "context": "In [28], [30], [32], a human body in an image is first parsed into semantic parts (e.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "In [12], Oreifej et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "[13] introduce a body prior constraint to avoid mismatching between distant patches, the problem is still not well addressed, especially for the mismatching between closely located patches.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "saliency-based approaches [14], [29], [33], [34] are recently proposed, which estimate the saliency distribution relationship between images and utilize it to control the patch-wise matching process.", "startOffset": 26, "endOffset": 30}, {"referenceID": 28, "context": "saliency-based approaches [14], [29], [33], [34] are recently proposed, which estimate the saliency distribution relationship between images and utilize it to control the patch-wise matching process.", "startOffset": 32, "endOffset": 36}, {"referenceID": 32, "context": "saliency-based approaches [14], [29], [33], [34] are recently proposed, which estimate the saliency distribution relationship between images and utilize it to control the patch-wise matching process.", "startOffset": 38, "endOffset": 42}, {"referenceID": 33, "context": "saliency-based approaches [14], [29], [33], [34] are recently proposed, which estimate the saliency distribution relationship between images and utilize it to control the patch-wise matching process.", "startOffset": 44, "endOffset": 48}, {"referenceID": 32, "context": "Following the similar line, Bak and Carr [33] introduce a deformable model to obtain a set of weights to guide the patch matching process.", "startOffset": 41, "endOffset": 45}, {"referenceID": 33, "context": "[34] further", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Comparatively, the matching weights in most of the saliency-based approaches [29], [33] are only controlled by patches in the probe-image (probe patch).", "startOffset": 77, "endOffset": 81}, {"referenceID": 32, "context": "Comparatively, the matching weights in most of the saliency-based approaches [29], [33] are only controlled by patches in the probe-image (probe patch).", "startOffset": 83, "endOffset": 87}, {"referenceID": 34, "context": "In [35],", "startOffset": 3, "endOffset": 7}, {"referenceID": 35, "context": "In [36], [37], image-wise similarities are measured by kernelized visual word co-occurrence matrices where a latent spatial kernel is integrated to handle spatial displacements.", "startOffset": 3, "endOffset": 7}, {"referenceID": 36, "context": "In [36], [37], image-wise similarities are measured by kernelized visual word co-occurrence matrices where a latent spatial kernel is integrated to handle spatial displacements.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "2 can be achieved by many off-the-shelf methods [14], [29], [38].", "startOffset": 48, "endOffset": 52}, {"referenceID": 28, "context": "2 can be achieved by many off-the-shelf methods [14], [29], [38].", "startOffset": 54, "endOffset": 58}, {"referenceID": 37, "context": "2 can be achieved by many off-the-shelf methods [14], [29], [38].", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "In this paper, we extract Dense SIFT and Dense Color Histogram [14] from each patch and utilize the KISSME metric [7] as the basic distance metric to compute \u03a6ij(fxi , fyj ).", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "In this paper, we extract Dense SIFT and Dense Color Histogram [14] from each patch and utilize the KISSME metric [7] as the basic distance metric to compute \u03a6ij(fxi , fyj ).", "startOffset": 114, "endOffset": 117}, {"referenceID": 32, "context": "Note that in order to achieve better performance [33], we learn multiple metrics so as to adaptively model the local patch-", "startOffset": 49, "endOffset": 53}, {"referenceID": 7, "context": "replacing KISSME with the kLFDA [8] and KMFA-R\u03c72 [39] metrics to demonstrate the effectiveness of our approach.", "startOffset": 32, "endOffset": 35}, {"referenceID": 38, "context": "replacing KISSME with the kLFDA [8] and KMFA-R\u03c72 [39] metrics to demonstrate the effectiveness of our approach.", "startOffset": 49, "endOffset": 53}, {"referenceID": 39, "context": "To address this problem, we introduce a global one-to-one mapping constraint and solve the resulting linear assignment task [40] to find the best matching:", "startOffset": 124, "endOffset": 128}, {"referenceID": 39, "context": "3 can be solved by the Hungarian method [40].", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "Based on the above process, we can calculate the image matching scores \u03c8 between a probe image and all gallery images in a cross-view camera, and rank the gallery images accordingly to achieve the final Re-ID result [13].", "startOffset": 216, "endOffset": 220}, {"referenceID": 13, "context": "searching ranges (from 26 to 32) by adjacency-constrained search [14], and then select the one which minimizes the rank order of V\u03b1\u2032 and use it as the approximated optimal binary mapping structure M\u03b1.", "startOffset": 65, "endOffset": 69}, {"referenceID": 40, "context": "(b)-(c): Examples of the correspondence structures learned by our approach for the VIPeR [41] data.", "startOffset": 89, "endOffset": 93}, {"referenceID": 41, "context": "P (M\u03b1) = R\u0303n(M\u03b1) \u2211 M\u03b3\u2208\u0393k R\u0303n(M\u03b3) is the prior probability for binary mapping structure M\u03b1, where R\u0303n(M\u03b1) is the CMC score at rank n [42] when using M\u03b1 as the correspondence structure to perform person Re-ID over the", "startOffset": 132, "endOffset": 136}, {"referenceID": 13, "context": "average appearance similarity [14], [7] between patches xi and yj over all correct match image pairs in the training set.", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "average appearance similarity [14], [7] between patches xi and yj over all correct match image pairs in the training set.", "startOffset": 36, "endOffset": 39}, {"referenceID": 40, "context": "(a, d, g, j): The correspondence structures learned by our approach (with the KISSME metric) for the VIPeR [41], PRID 450S [9], 3DPeS [43], and SYSU-sReID [44] datasets, respectively (the correspondence structure for our ROAD dataset is shown in Figure 1).", "startOffset": 107, "endOffset": 111}, {"referenceID": 8, "context": "(a, d, g, j): The correspondence structures learned by our approach (with the KISSME metric) for the VIPeR [41], PRID 450S [9], 3DPeS [43], and SYSU-sReID [44] datasets, respectively (the correspondence structure for our ROAD dataset is shown in Figure 1).", "startOffset": 123, "endOffset": 126}, {"referenceID": 42, "context": "(a, d, g, j): The correspondence structures learned by our approach (with the KISSME metric) for the VIPeR [41], PRID 450S [9], 3DPeS [43], and SYSU-sReID [44] datasets, respectively (the correspondence structure for our ROAD dataset is shown in Figure 1).", "startOffset": 134, "endOffset": 138}, {"referenceID": 43, "context": "(a, d, g, j): The correspondence structures learned by our approach (with the KISSME metric) for the VIPeR [41], PRID 450S [9], 3DPeS [43], and SYSU-sReID [44] datasets, respectively (the correspondence structure for our ROAD dataset is shown in Figure 1).", "startOffset": 155, "endOffset": 159}, {"referenceID": 41, "context": "P\u0302 (mst|M\u03b1) = R\u0303n(mst) \u2211 mhg\u2208M\u03b1 R\u0303n(mhg) , (11) where R\u0303n(mst) is the rank-n CMC score [42] when only using a single link mst as the correspondence structure to perform Re-ID.", "startOffset": 87, "endOffset": 91}, {"referenceID": 44, "context": "More specifically, for a pair of test images (a probe image and a gallery image), we first parse the human pose information for both images [45] and classify them into their closest pose groups,", "startOffset": 140, "endOffset": 144}, {"referenceID": 44, "context": "Specifically, we first utilize semantic region parsing [45] to obtain head and hair regions from a test image.", "startOffset": 55, "endOffset": 59}, {"referenceID": 45, "context": "mation of head and hair regions, and apply a linear SVM [46] to classify this feature vector into one of the pose groups.", "startOffset": 56, "endOffset": 60}, {"referenceID": 46, "context": "ticated pose estimation methods [47], [48], [34] can be included to handle pose information parsing under more complicated scenarios.", "startOffset": 32, "endOffset": 36}, {"referenceID": 47, "context": "ticated pose estimation methods [47], [48], [34] can be included to handle pose information parsing under more complicated scenarios.", "startOffset": 38, "endOffset": 42}, {"referenceID": 33, "context": "ticated pose estimation methods [47], [48], [34] can be included to handle pose information parsing under more complicated scenarios.", "startOffset": 44, "endOffset": 48}, {"referenceID": 48, "context": "Dividing pose groups & pose group pairs automatically, where we first cluster training images [49] in each camera into a set of pose groups (using the same feature vector as described in point 1), and then form camera-", "startOffset": 94, "endOffset": 98}, {"referenceID": 48, "context": "Note that in order to automatically decide the number of clusters, we utilize a spectral clustering scheme [49] which is able to find the optimal cluster number by integrating a clustering-coherency metric to evaluate the results under different cluster numbers.", "startOffset": 107, "endOffset": 111}, {"referenceID": 40, "context": "The VIPeR dataset [41] is a commonly used dataset which contains 632 image pairs for 632 pedestrians, as in Figures 4a-4c and 8d.", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "The PRID 450S dataset [9] consists of 450", "startOffset": 22, "endOffset": 25}, {"referenceID": 42, "context": "The 3DPeS dataset [43] is comprised of 1012", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "For images from the same camera group, we simply utilize adjacency-constrained search [14] to find patch-wise mapping and calculate the image matching", "startOffset": 86, "endOffset": 90}, {"referenceID": 43, "context": "The SYSU-sReID dataset [44] contains 502 individual pairs taken by two disjoint cameras in a campus environment, as in Fig.", "startOffset": 23, "endOffset": 27}, {"referenceID": 4, "context": "[5], [8], [2] and perform experiments under 50%-training and 50%-testing.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[5], [8], [2] and perform experiments under 50%-training and 50%-testing.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "[5], [8], [2] and perform experiments under 50%-training and 50%-testing.", "startOffset": 10, "endOffset": 13}, {"referenceID": 13, "context": "1) Patch matching performances: We compare the patch matching performance of three methods: (1) The adjacencyconstrained search method [14], [29], [15] which finds a best matched patch for each patch in a probe image (probe patch) by searching a fixed neighborhood region around the", "startOffset": 135, "endOffset": 139}, {"referenceID": 28, "context": "1) Patch matching performances: We compare the patch matching performance of three methods: (1) The adjacencyconstrained search method [14], [29], [15] which finds a best matched patch for each patch in a probe image (probe patch) by searching a fixed neighborhood region around the", "startOffset": 141, "endOffset": 145}, {"referenceID": 14, "context": "1) Patch matching performances: We compare the patch matching performance of three methods: (1) The adjacencyconstrained search method [14], [29], [15] which finds a best matched patch for each patch in a probe image (probe patch) by searching a fixed neighborhood region around the", "startOffset": 147, "endOffset": 151}, {"referenceID": 6, "context": "Moreover, in order to further evaluate the effectiveness of our correspondence structure learning process, we also show the learned correspondence structures when using different distance metrics (KISSME [7], kLFDA [8], and KMFAR\u03c72 [39]) to measure the patch-wise similarity (cf.", "startOffset": 204, "endOffset": 207}, {"referenceID": 7, "context": "Moreover, in order to further evaluate the effectiveness of our correspondence structure learning process, we also show the learned correspondence structures when using different distance metrics (KISSME [7], kLFDA [8], and KMFAR\u03c72 [39]) to measure the patch-wise similarity (cf.", "startOffset": 215, "endOffset": 218}, {"referenceID": 38, "context": "Moreover, in order to further evaluate the effectiveness of our correspondence structure learning process, we also show the learned correspondence structures when using different distance metrics (KISSME [7], kLFDA [8], and KMFAR\u03c72 [39]) to measure the patch-wise similarity (cf.", "startOffset": 232, "endOffset": 236}, {"referenceID": 41, "context": "ID results by the standard Cumulated Matching Characteristic (CMC) curve [42] which measures the correct match rates within different Re-ID rank ranges.", "startOffset": 73, "endOffset": 77}, {"referenceID": 4, "context": "The evaluation protocols are the same as [5].", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "Comparison of correspondence structures for the Road dataset when using different distance metrics (KISSME [7], kLFDA [8] and KMFA-R\u03c72 [39]) to measure the patch-wise similarity.", "startOffset": 107, "endOffset": 110}, {"referenceID": 7, "context": "Comparison of correspondence structures for the Road dataset when using different distance metrics (KISSME [7], kLFDA [8] and KMFA-R\u03c72 [39]) to measure the patch-wise similarity.", "startOffset": 118, "endOffset": 121}, {"referenceID": 38, "context": "Comparison of correspondence structures for the Road dataset when using different distance metrics (KISSME [7], kLFDA [8] and KMFA-R\u03c72 [39]) to measure the patch-wise similarity.", "startOffset": 135, "endOffset": 139}, {"referenceID": 6, "context": "Tables I\u2013V show the CMC results of different methods under three distance metrics: KISSME [7], kLFDA [8], and KMFA-R\u03c72 [39] (note that we use Dense SIFT and Dense Color Histogram features [14] for KISSME & kLFDA metrics, and use RGB, HSV, YCbCr, Lab, YIQ, 16 Gabor texture", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "Tables I\u2013V show the CMC results of different methods under three distance metrics: KISSME [7], kLFDA [8], and KMFA-R\u03c72 [39] (note that we use Dense SIFT and Dense Color Histogram features [14] for KISSME & kLFDA metrics, and use RGB, HSV, YCbCr, Lab, YIQ, 16 Gabor texture", "startOffset": 101, "endOffset": 104}, {"referenceID": 38, "context": "Tables I\u2013V show the CMC results of different methods under three distance metrics: KISSME [7], kLFDA [8], and KMFA-R\u03c72 [39] (note that we use Dense SIFT and Dense Color Histogram features [14] for KISSME & kLFDA metrics, and use RGB, HSV, YCbCr, Lab, YIQ, 16 Gabor texture", "startOffset": 119, "endOffset": 123}, {"referenceID": 13, "context": "Tables I\u2013V show the CMC results of different methods under three distance metrics: KISSME [7], kLFDA [8], and KMFA-R\u03c72 [39] (note that we use Dense SIFT and Dense Color Histogram features [14] for KISSME & kLFDA metrics, and use RGB, HSV, YCbCr, Lab, YIQ, 16 Gabor texture", "startOffset": 188, "endOffset": 192}, {"referenceID": 38, "context": "features [39] for KMFA-R\u03c72 metric).", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "3) The AC+global method can be viewed as an extended version of the adjacency-constrained search method [14], [29], [15] plus a global constraint.", "startOffset": 104, "endOffset": 108}, {"referenceID": 28, "context": "3) The AC+global method can be viewed as an extended version of the adjacency-constrained search method [14], [29], [15] plus a global constraint.", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "3) The AC+global method can be viewed as an extended version of the adjacency-constrained search method [14], [29], [15] plus a global constraint.", "startOffset": 116, "endOffset": 120}, {"referenceID": 6, "context": "5) The improvement of our approach is coherent on different distance metrics (KISSME [7], kLFDA [8], and", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "5) The improvement of our approach is coherent on different distance metrics (KISSME [7], kLFDA [8], and", "startOffset": 96, "endOffset": 99}, {"referenceID": 38, "context": "KMFA-R\u03c72 [39]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 42, "context": "Note that since the number of pedestrians for each pose is small in the 3DPeS [43] dataset, which is insufficient to construct reliable local correspondences, we focus on evaluating our multi-structure strategy on the other four datasets: VIPeR [41], PRID 450S [9], Road, and SYSU-sReID [44].", "startOffset": 78, "endOffset": 82}, {"referenceID": 40, "context": "Note that since the number of pedestrians for each pose is small in the 3DPeS [43] dataset, which is insufficient to construct reliable local correspondences, we focus on evaluating our multi-structure strategy on the other four datasets: VIPeR [41], PRID 450S [9], Road, and SYSU-sReID [44].", "startOffset": 245, "endOffset": 249}, {"referenceID": 8, "context": "Note that since the number of pedestrians for each pose is small in the 3DPeS [43] dataset, which is insufficient to construct reliable local correspondences, we focus on evaluating our multi-structure strategy on the other four datasets: VIPeR [41], PRID 450S [9], Road, and SYSU-sReID [44].", "startOffset": 261, "endOffset": 264}, {"referenceID": 43, "context": "Note that since the number of pedestrians for each pose is small in the 3DPeS [43] dataset, which is insufficient to construct reliable local correspondences, we focus on evaluating our multi-structure strategy on the other four datasets: VIPeR [41], PRID 450S [9], Road, and SYSU-sReID [44].", "startOffset": 287, "endOffset": 291}, {"referenceID": 2, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 113, "endOffset": 116}, {"referenceID": 8, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 125, "endOffset": 128}, {"referenceID": 28, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 139, "endOffset": 143}, {"referenceID": 14, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 154, "endOffset": 158}, {"referenceID": 49, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 166, "endOffset": 170}, {"referenceID": 17, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 176, "endOffset": 180}, {"referenceID": 7, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 188, "endOffset": 191}, {"referenceID": 7, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 197, "endOffset": 200}, {"referenceID": 22, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 207, "endOffset": 211}, {"referenceID": 19, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 217, "endOffset": 221}, {"referenceID": 36, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 229, "endOffset": 233}, {"referenceID": 25, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 246, "endOffset": 250}, {"referenceID": 32, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 257, "endOffset": 261}, {"referenceID": 18, "context": "We also compare our results with the state-of-the-art methods on different datasets: RankBoost [3], LF [4], JLCF [4], KISSME [9], SalMatch [29], DSVR FSA [15], svmml [50], ELS [18], kLFDA [8], MFA [8], IDLA [23], JLR [20], PRISM [37], DG-Dropout [26], DPML [33], LOMO [19],", "startOffset": 268, "endOffset": 272}, {"referenceID": 38, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 18, "endOffset": 22}, {"referenceID": 34, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 29, "endOffset": 33}, {"referenceID": 31, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 42, "endOffset": 46}, {"referenceID": 24, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 55, "endOffset": 59}, {"referenceID": 26, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 103, "endOffset": 106}, {"referenceID": 5, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 140, "endOffset": 143}, {"referenceID": 49, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 151, "endOffset": 155}, {"referenceID": 7, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 161, "endOffset": 164}, {"referenceID": 7, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 172, "endOffset": 175}, {"referenceID": 38, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 195, "endOffset": 199}, {"referenceID": 26, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 209, "endOffset": 213}, {"referenceID": 50, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 245, "endOffset": 249}, {"referenceID": 7, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 257, "endOffset": 260}, {"referenceID": 49, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 268, "endOffset": 272}, {"referenceID": 7, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 278, "endOffset": 281}, {"referenceID": 7, "context": "Mirror-KMFA(R\u03c72 ) [39], DCSL [35], deCPPs [32], MCPCNN [25], and FNN [27] on the VIPeR dataset; KISSME [9], EIML [6], SCNCD [2], SCNCDFinal [2], svmml [50], MFA [8], kLFDA [8], Mirror-KMFA(R\u03c72 ) [39], and FNN [27] on the PRID 450S dataset; PCCA [51], rPCCA [8], svmml [50], MFA [8], and kLFDA [8] on the 3DPeS dataset;", "startOffset": 293, "endOffset": 296}, {"referenceID": 13, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 9, "endOffset": 13}, {"referenceID": 49, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 46, "endOffset": 49}, {"referenceID": 13, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 84, "endOffset": 88}, {"referenceID": 49, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 96, "endOffset": 100}, {"referenceID": 7, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 117, "endOffset": 120}, {"referenceID": 43, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 128, "endOffset": 132}, {"referenceID": 33, "context": "eSDC-knn [14], svmml [50], MFA [8], and kLFDA [8] on the Road dataset; and eSDC-knn [14], svmml [50], MFA [8], kLFDA [8], MLACM [44], and TA+W [34] on the SYSUsReID dataset.", "startOffset": 143, "endOffset": 147}, {"referenceID": 33, "context": "Note that since the rotation and orientation parameters used in the TA+W method [34] are derived from camera calibration", "startOffset": 80, "endOffset": 84}, {"referenceID": 33, "context": ", guarantee that [34]\u2019s performance will not be degraded due to improper rotation and orientation parameters).", "startOffset": 17, "endOffset": 21}, {"referenceID": 13, "context": ", adding multiple Re-ID results together to achieve a higher Re-ID result) [14], [29], [4], we also show a fusion result of our approach (svmml+Proposed+multi-manu-KMFA) and compare it with the other fusion results in Table VI.", "startOffset": 75, "endOffset": 79}, {"referenceID": 28, "context": ", adding multiple Re-ID results together to achieve a higher Re-ID result) [14], [29], [4], we also show a fusion result of our approach (svmml+Proposed+multi-manu-KMFA) and compare it with the other fusion results in Table VI.", "startOffset": 81, "endOffset": 85}, {"referenceID": 3, "context": ", adding multiple Re-ID results together to achieve a higher Re-ID result) [14], [29], [4], we also show a fusion result of our approach (svmml+Proposed+multi-manu-KMFA) and compare it with the other fusion results in Table VI.", "startOffset": 87, "endOffset": 90}, {"referenceID": 22, "context": ", IDLA [23], JLR [20], and DG-Dropout [26] in Ta-", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": ", IDLA [23], JLR [20], and DG-Dropout [26] in Ta-", "startOffset": 17, "endOffset": 21}, {"referenceID": 25, "context": ", IDLA [23], JLR [20], and DG-Dropout [26] in Ta-", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": "for Re-ID, it can easily accommodate future advances by combining with the deep-neural-network-based features (such as [27]) without changing the core protocols.", "startOffset": 119, "endOffset": 123}, {"referenceID": 33, "context": "3) Since the TA+W method [34] creates multiple weight matrices for pedestrians with different orientations, it can be viewed as another version of using mul-", "startOffset": 25, "endOffset": 29}, {"referenceID": 33, "context": "However, our approach (proposed+multi-manu) still outperforms the TA+W method [34] (cf.", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "Rank 1 5 10 20 30 RankBoost [3] 23.", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "7 JLCF [4] 26.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "1 - KISSME [9] 27 - 70 83 SalMatch [29] 28.", "startOffset": 11, "endOffset": 14}, {"referenceID": 28, "context": "1 - KISSME [9] 27 - 70 83 SalMatch [29] 28.", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "3 - - DSVR FSA [15] 29.", "startOffset": 15, "endOffset": 19}, {"referenceID": 49, "context": "9 svmml [50] 30.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "1 ELS [18] 31.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "7 kLFDA [8] 32.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "9 MFA [8] 32.", "startOffset": 6, "endOffset": 9}, {"referenceID": 22, "context": "6 IDLA [23] 34.", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "5 - - JLR [20] 35.", "startOffset": 10, "endOffset": 14}, {"referenceID": 36, "context": "0 - - PRISM [37] 36.", "startOffset": 12, "endOffset": 16}, {"referenceID": 3, "context": "2 JLCF-fusing [4] 38.", "startOffset": 14, "endOffset": 17}, {"referenceID": 25, "context": "0 - DG-Dropout [26] 38.", "startOffset": 15, "endOffset": 19}, {"referenceID": 32, "context": "6 - - - DPML [33] 41.", "startOffset": 13, "endOffset": 17}, {"referenceID": 18, "context": "5 LOMO [19] 42.", "startOffset": 7, "endOffset": 11}, {"referenceID": 38, "context": "1 Mirror-KMFA(R\u03c72 ) [39] 43.", "startOffset": 20, "endOffset": 24}, {"referenceID": 34, "context": "DCSL [35] 44.", "startOffset": 5, "endOffset": 9}, {"referenceID": 28, "context": "6 - svmml+SalMatch [29] 44.", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "4 - - deCPPs [32] 44.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "8 - deCPPs+MER [32] 47.", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": "8 - MCP-CNN [25] 47.", "startOffset": 12, "endOffset": 16}, {"referenceID": 26, "context": "3 FNN [27] 51.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "9 LOMO-fusing [19] 51.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "Rank 1 5 10 20 30 KISSME [9] 33 - 71 79 EIML [6] 35 - 68 77 SCNCD [2] 41.", "startOffset": 25, "endOffset": 28}, {"referenceID": 5, "context": "Rank 1 5 10 20 30 KISSME [9] 33 - 71 79 EIML [6] 35 - 68 77 SCNCD [2] 41.", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "Rank 1 5 10 20 30 KISSME [9] 33 - 71 79 EIML [6] 35 - 68 77 SCNCD [2] 41.", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "4 SCNCDFinal [2] 41.", "startOffset": 13, "endOffset": 16}, {"referenceID": 49, "context": "8 svmml [50] 42.", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "2 MFA [8] 44.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "9 kLFDA [8] 46.", "startOffset": 8, "endOffset": 11}, {"referenceID": 38, "context": "4 Mirror-KMFA(R\u03c72 ) [39] 55.", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "FNN [27] 66.", "startOffset": 4, "endOffset": 8}, {"referenceID": 49, "context": "Rank 1 5 10 20 30 svmml [50] 34.", "startOffset": 24, "endOffset": 28}, {"referenceID": 50, "context": "5 PCCA [51] 41.", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "4 rPCCA [8] 47.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "9 MFA [8] 48.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "8 kLFDA [8] 54.", "startOffset": 8, "endOffset": 11}, {"referenceID": 25, "context": "4 DG-Dropout [26] 56.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "Rank 1 5 10 20 30 eSDC-knn [14] 52.", "startOffset": 27, "endOffset": 31}, {"referenceID": 49, "context": "8 svmml [50] 57.", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "6 MFA [8] 58.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "3 kLFDA [8] 59.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "Rank 1 5 10 20 30 KISSME [9] 28.", "startOffset": 25, "endOffset": 28}, {"referenceID": 49, "context": "0 svmml [50] 30.", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "3 MFA [8] 32.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "4 kLFDA [8] 33.", "startOffset": 8, "endOffset": 11}, {"referenceID": 43, "context": "0 MLACM [44] 32.", "startOffset": 8, "endOffset": 12}, {"referenceID": 33, "context": "2 \u2013 TA-W [34] 43.", "startOffset": 9, "endOffset": 13}, {"referenceID": 51, "context": "This implies that our testing process can be further optimized by replacing Hungarian method with other more efficient algorithms [52].", "startOffset": 130, "endOffset": 134}, {"referenceID": 41, "context": "Figures 12a to 12b show the rank-1 CMC scores [42] of", "startOffset": 46, "endOffset": 50}], "year": 2017, "abstractText": "This paper addresses the problem of handling spatial misalignments due to camera-view changes or humanpose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or humanpose variation in individual images. We further introduce a global constraint-based matching process. It integrates a global matching constraint over the learned correspondence structure to exclude cross-view misalignments during the image patch matching process, hence achieving a more reliable matching score between images. Finally, we also extend our approach by introducing a multi-structure scheme, which learns a set of local correspondence structures to capture the spatial correspondence sub-patterns between a camera pair, so as to handle the spatial misalignments between individual images in a more precise way. Experimental results on various datasets demonstrate the effectiveness of our approach. The project page for this paper is available at http://min.sjtu.edu.cn/lwydemo/personReID.htm", "creator": "LaTeX with hyperref package"}}}