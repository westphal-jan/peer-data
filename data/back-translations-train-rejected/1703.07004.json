{"id": "1703.07004", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "The Use of Autoencoders for Discovering Patient Phenotypes", "abstract": "We use autoencoders to create low-dimensional embeddings of underlying patient phenotypes that we hypothesize are a governing factor in determining how different patients will react to different interventions. We compare the performance of autoencoders that take fixed length sequences of concatenated timesteps as input with a recurrent sequence-to-sequence autoencoder. We evaluate our methods on around 35,500 patients from the latest MIMIC III dataset from Beth Israel Deaconess Hospital.", "histories": [["v1", "Mon, 20 Mar 2017 23:30:40 GMT  (992kb,D)", "http://arxiv.org/abs/1703.07004v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["harini suresh", "peter szolovits", "marzyeh ghassemi"], "accepted": false, "id": "1703.07004"}, "pdf": {"name": "1703.07004.pdf", "metadata": {"source": "CRF", "title": "The Use of Autoencoders for Discovering Patient Phenotypes", "authors": ["Harini Suresh"], "emails": ["psz@mit.edu", "mghassem@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "However, most decisions will be able to make decisions about when patients need to be treated with specific interventions. [1] The high quality of treatment depends on a robust understanding of the underlying acoustics. [2]"}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Autoencoders", "text": "Previous work has tried to use autoencoders on random 30-day patches of input vectors to learn basic patient phenotypes [9], and has shown that they can extract relevant and useful features about patients. It is also valuable to consider signals that occur throughout a patient's stay, as there are often long delays between the expression of relevant signals [10]. Sequence autoencoders take measured signals one step at a time into a layer of LSTM cells (LongShort Term Memory) and produce a fixed-length embedding. This embedding is then used as an input into another layer of LSTM cells, which attempt to predict the original input sequence. LSTM cells are used for their ability to effectively model time series data of varying lengths and capture long-term dependencies. LSTMs have reached the state of the art by using the natural language processing results in many translation machines."}, {"heading": "3 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Features", "text": "Since 2001, the MIMIC database has been built and maintained by the Laboratory of Computational Physiology at the Massachusetts Institute of Technology, Beth Israel Deaconess Medical Center and Philips Healthcare with the support of the National Institute of Biomedical Imaging and Bioinformatics. It contains general patient information (ICD-9 codes, demographics, room tracking), physiological signals (vital metrics, SAPS), medicines (IV drugs, vendor order intake data), laboratory tests (chemistry, imaging), fluid balance (discharge summary, nursing progress reports), physiological signals (vital metrics, SAPS), medicines (IV drugs, vendor order intake data), laboratory tests (chemistry, imaging) and notes (discharge summary, nursing progress reports)."}, {"heading": "3.2 Autoencoders", "text": "We are testing the ability of a simple auto encoder with a single hidden layer, an auto encoder with two hidden layers, and a sequence encoder to reconstruct the input (Figure 1). We are also comparing the performance of these models over inputs of different interval lengths, in particular 4, 16, 32, and 64 hours. We are training on mini batches of 128 samples with early stop to determine the number of epochs. In fixed input auto encoders, we concatenate all 30 features for each hour during the specified interval length. We are using an embedding size that corresponds to the total number of input values divided by 10 to achieve a compression factor of 10x. All hidden layers use a ReLU activation function, and the output layer uses a sigmoid activation function."}, {"heading": "4 Results", "text": "We evaluate the performance of each individual autoencoder by using the mean square error (MSE) between the predicted value sequence and the true value sequence. At all interval lengths, the single-layer autoencoder with one LSTM layer achieves a lower MSE than the fixed-length single-layer autoencoder, but differs from the fixed-length two-layer autoencoder (Figure 2). We also show that the reconstruction of input time series with autoencoders is relatively robust compared to layers in population subgroups. We run the autoencoders at intervals of 32 hours with patient subgroups stratified by the care unit. MSEs are higher than when the autoencoders are trained on the entire patient population, but in all cases less than 0.08, although the training sets are much smaller (Figure 3). On these smaller subgroups of patients, the sequence autoencoder data seem to be better at generating with smaller amounts of autocoders and in better condition in all cases."}], "references": [{"title": "Critical care-where have we been and where are we going? Critical Care. 2013;17(Suppl 1):S2", "author": ["Vincent JL"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Deep computational phenotyping.", "author": ["Che", "Zhengping"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Dynamically modeling patient\u2019s health state from electronic medical records: a time series approach", "author": ["KL Caballero Barajas", "R. Akella"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Unfolding physiological state: Mortality modelling in intensive care units", "author": ["M Ghassemi", "T Naumann", "F Doshi-Velez"], "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Caution when using prognostic models: a prospective comparison of 3 recent prognostic models.", "author": ["Nassar", "Antonio Paulo"], "venue": "Journal of critical care", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Assessment of six mortality prediction models in patients admitted with severe sepsis and septic shock to the intensive care unit: a prospective cohort study.", "author": ["Arabi", "Yaseen"], "venue": "Critical care", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Blood pressure targets for vasopressor therapy: a systematic review", "author": ["F D\u2019Aragon", "EP Belley-Cote", "MO Meade"], "venue": "Shock", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A multivariate timeseries modeling approach to severity of illness assessment and forecasting in ICU with sparse, heterogeneous clinical data.", "author": ["Ghassemi", "Marzyeh"], "venue": "Proceedings of the...AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Computational phenotype discovery using unsupervised feature learning over noisy, sparse, and irregular clinical data.", "author": ["Lasko", "Thomas A", "Joshua C. Denny", "Mia A. Levy"], "venue": "PloS one", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Learning to Diagnose with LSTM Recurrent Neural Networks.", "author": ["Lipton", "Zachary C"], "venue": "arXiv preprint arXiv:1511.03677", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate.", "author": ["Bahdanau", "Dzmitry", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems.\" arXiv preprint", "author": ["Wen", "Tsung-Hsien"], "venue": "APA", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Show and tell: A neural image caption generator.", "author": ["Vinyals", "Oriol"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Semi-supervised sequence learning.", "author": ["Dai", "Andrew M", "Quoc V. Le"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II): A public-access intensive care unit database.", "author": ["Saeed", "Mohammed", "Mauricio Villarroel", "Andrew T. Reisner", "Gari Clif- ford", "Li-Wei Lehman", "George Moody", "Thomas Heldt", "Tin H. Kyaw", "Benjamin Moody", "Roger G. Mark"], "venue": "DOI: 10.1097/CCM.0b013e31820a92c6", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "With the rapid rise in Electronic Health Records (EHRs) available for analysis, data-driven models are well-suited to aid physicians in making decisions about when patients should be treated with or weaned off certain interventions [1].", "startOffset": 232, "endOffset": 235}, {"referenceID": 1, "context": "Achieving high-quality care depends on a robust understanding of the patient\u2019s underlying acuity throughout time [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "Traditional measures of acuity are often based on mortality evaluated at a single endpoint [3, 4], or on static scores such as SAPS that don\u2019t take into account evolving clinical information [5, 6].", "startOffset": 91, "endOffset": 97}, {"referenceID": 3, "context": "Traditional measures of acuity are often based on mortality evaluated at a single endpoint [3, 4], or on static scores such as SAPS that don\u2019t take into account evolving clinical information [5, 6].", "startOffset": 91, "endOffset": 97}, {"referenceID": 4, "context": "Traditional measures of acuity are often based on mortality evaluated at a single endpoint [3, 4], or on static scores such as SAPS that don\u2019t take into account evolving clinical information [5, 6].", "startOffset": 191, "endOffset": 197}, {"referenceID": 5, "context": "Traditional measures of acuity are often based on mortality evaluated at a single endpoint [3, 4], or on static scores such as SAPS that don\u2019t take into account evolving clinical information [5, 6].", "startOffset": 191, "endOffset": 197}, {"referenceID": 6, "context": "The efficacy of interventions can drastically vary from patient to patient, and unnecessarily administering an intervention can be harmful and expensive [7].", "startOffset": 153, "endOffset": 156}, {"referenceID": 7, "context": "Previously, multitask gaussian processes have been tested for modelling patient acuity but only in Traumatic Brain Injury (TBI) patients [8] or only using longitudinal billing data [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 1, "context": "Previously, multitask gaussian processes have been tested for modelling patient acuity but only in Traumatic Brain Injury (TBI) patients [8] or only using longitudinal billing data [2].", "startOffset": 181, "endOffset": 184}, {"referenceID": 8, "context": "Previous work has attempted to use autoencoders on random 30-day patches of input vectors to learn underlying patient phenotypes [9], and has shown that they can extract relevant and useful features about patients.", "startOffset": 129, "endOffset": 132}, {"referenceID": 9, "context": "It is also valuable to consider signals that occur throughout a patient\u2019s stay, since there are often long delays between when relevant signals are expressed [10].", "startOffset": 158, "endOffset": 162}, {"referenceID": 10, "context": "LSTMs have achieved state-of-the-art results in many different natural language processing applications from machine translation [11] to dialogue systems [12] to image captioning [13].", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "LSTMs have achieved state-of-the-art results in many different natural language processing applications from machine translation [11] to dialogue systems [12] to image captioning [13].", "startOffset": 154, "endOffset": 158}, {"referenceID": 12, "context": "LSTMs have achieved state-of-the-art results in many different natural language processing applications from machine translation [11] to dialogue systems [12] to image captioning [13].", "startOffset": 179, "endOffset": 183}, {"referenceID": 13, "context": "They were recently used as an initialization step for recurrent neural networks for text classification [14], but have not been applied to the clinical space.", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "Features were extracted from the Multiparameter Intelligent Monitoring in Intensive Care (MIMIC III) Database [15].", "startOffset": 110, "endOffset": 114}], "year": 2017, "abstractText": "We use autoencoders to create low-dimensional embeddings of underlying patient phenotypes that we hypothesize are a governing factor in determining how different patients will react to different interventions. We compare the performance of autoencoders that take fixed length sequences of concatenated timesteps as input with a recurrent sequence-to-sequence autoencoder. We evaluate our methods on around 35,500 patients from the latest MIMIC III dataset from Beth Israel Deaconess Hospital.", "creator": "LaTeX with hyperref package"}}}